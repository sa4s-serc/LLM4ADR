{"prompt":"## Context and Problem Statement\\nUsing response model directly in UI layer causes to create lots of god object, prevent flexibility and increase dependency with each layer.\\n","completion":"## Decision\nIn the context of creating multiple model classes for network and ui layers facing concern of creating god objects and limiting the ability to modularize the application we decided to use multiple model classes and neglected using a single model class, to achieve flexibility and fewer dependencies.\\n"}
{"prompt":"## Context\\nPACE contains a number of independently developed and released projects with separate source and build pipelines.\\nCoordination of documentation from projects using three different primary languages (MATLAB, Python, C) with varying release cycles will create tight coupling between the projects and their builds.\\n","completion":"## Decision\nEach project will maintain its own GitHub pages documentation that will be updated with the project's build-release-deploy pipeline.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in\\n[documenting architecture decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n"}
{"prompt":"## Context\\nThe developer should be able to define, if an association has to be considered or skipped during the cloning of an entity.\\n","completion":"## Decision\nThe current clone behavior is controlled by the `Shopware\\Core\\Framework\\DataAbstractionLayer\\Field\\Flag\\CascadeDelete` flag.\\nAll associations which are marked with this flag are considered in the clone process.\\nWe will add an optional parameter to the flag constructor to disable this behavior.\\nI already added this flag to the following associations:\\n* `product.productReviews`\\n* This association is already overwritten by the administration\\n* `product.searchKeywords`\\n* Will be indexed by the product indexer, so we can skip this association in the clone process\\n* `product.categoriesRo`\\n* Will be indexed by the product indexer, so we can skip this association in the clone process\\nAn example looks like the following:\\n```\\n(new OneToManyAssociationField('searchKeywords', ProductSearchKeywordDefinition::class, 'product_id'))\\n->addFlags(new CascadeDelete(false)),\\n```\\n"}
{"prompt":"## Context\\nUsing [selectors](https:\/\/medium.com\/@matthew.holman\/what-is-a-redux-selector-a517acee1fe8) is a best practice for Redux-applications to derive information from a minimal state while keeping the `mapStateToProps` and  `mapDispatchToProps` functions small.\\nI will use this pattern, because I made good experience with them on our work-project.\\nI am considering to use the [Reselect](https:\/\/github.com\/reduxjs\/reselect) for this, because it seems to be the \"industry-standard\".\\nArguments for using Rselect:\\n- Reselect is very popular and has a lot of documentation on the official pages and on StackOverflow\\n- Reselect handles memoisation\\n- Adding reselect now is not much effort, because there are no selectors yet. Adding it later will require touching all selectors again.\\nArguments against using Reselect:\\n- Selectors can also be written as plain functions to achieve the same separation of state and derived data\\n- An additional library adds to the size of the bundle\\n- The state of the tacboard is not that complicate, I don't expect memoisation to become a problem soon (but who knows?)\\n- Many of the selectors can not be cachedefficiently, because they depend on component-props (e.g. whether a field is empty or not requires the field-index), see [Reselect-Docs](https:\/\/github.com\/reduxjs\/reselect#accessing-react-props-in-selectors)\\n","completion":"## Decision\nI will use Reselect to rewrite the selectors. The only real downside seems to be the increased package-size, but that is not a big concern at this point.\\nIt seems more important to avoid having to rewrite everything later. There also is a way to get memoization for selectors that use component props (see [Reselect-Docs](https:\/\/github.com\/reduxjs\/reselect#sharing-selectors-with-props-across-multiple-component-instances)).\\n"}
{"prompt":"## Context\\nIn [this issue](https:\/\/github.com\/raft-tech\/TANF-app\/issues\/609) we propose tasks to automate the provisioning of Cloud.gov-brokered services via our CI pipeline.\\nThis project plans to use Cloud.gov-brokered S3 buckets for TANF data file storage (sensitive PII data) and for Django Admin static assets (public), and a Cloud.gov-brokered RDS PostgreSQL service. Currently, there are no scripts to provision and secure these S3 buckets and RDS instance, and no dev documentation on how to deploy them, secure them, and verify that they are configured correctly. Additionally, this same initial provisioning must be done again for all additional target environments.\\n","completion":"## Decision\n[Terraform](https:\/\/www.terraform.io\/) is a tool for building, changing, and versioning infrastructure safely and efficiently, and was proposed as a solution for managing our persistent Cloud.gov-brokered infrastructure. We can closely model what is done in [another HHS project](https:\/\/github.com\/HHS\/Head-Start-TTADP) and create per-environment infrastructure configurations which are leveraged by Terraform in CircleCI, with environment-specific settings read directly from Cloud.gov during our CI process. Note that this workflow was a [recommendation from Cloud.gov](https:\/\/www.youtube.com\/watch?v=86wfgNK_0o4), and they themselves use Terraform to provision their own infrastructure.\\n"}
{"prompt":"## Context\\nWe are expecting to have to run a large number of containers across several servers\\nand in different environments. Given this, we think that we need a Container\\nOrchestration tool and are selecting between Kubernetes, Docker Swarm and Mesos.\\n","completion":"## Decision\nWe have selected to use Kubernetes as our container orchestration platform. This is due\\nto it being the choice that the JASMIN team have made and also that it has established\\nitself as a clear industry favourite.\\n"}
{"prompt":"## Context\\nFields in the JSON structures used by the Discord API can be optional, nullable, or both\\n(see [here](https:\/\/discordapp.com\/developers\/docs\/reference#nullable-and-optional-resource-fields)).\\nThis means that we can not treat `Optional.empty()` and `null` the same.\\nAdding a `@Nullable` annotation to an `Optional<T>` field is not handled well by Immutables.\\nIt will not generate the methods need to make using such a field in a builder 'nice'.\\nImmutables supports [custom encodings](https:\/\/immutables.github.io\/encoding.html) for types.\\nIt does not allow detecting the encoding to use via annotation.\\nIt requires that the encoding be compiled before the main compilation stage.\\n","completion":"## Decision\nWe will create a `Nullable<T>` class, similar in functionality to `Optional<T>`.\\nWe will create an encoding for that class, making the builder easier to use for\\nfields of that type.\\n"}
{"prompt":"## Context\\nPACE is a collection of programs written in various languages including Matlab\\nand Python, but the aim is that *users* should be able to use either Python\\n*or* Matlab to run PACE programs.\\nMatlab has [built-in functionality](https:\/\/www.mathworks.com\/help\/matlab\/call-python-libraries.html)\\nto call Python programs however the reverse is not true - principally because\\nMatlab requires a (paid-for) license to use.\\nThus to allow Python users to call the current Matlab PACE codes\\n([Horace](https:\/\/github.com\/pace-neutrons\/Horace\/)\/[Herbert](https:\/\/github.com\/pace-neutrons\/Herbert\/))\\nit should either be translated to Python or C++ and a wrapper to Python and Matlab created,\\nor the Matlab Compiler Runtime (MCR) toolbox should be used to \"compile\"\\n`horace`\/`herbert` for distribution as a Python package, as detailed\\n[here](..\/..\/python_interface\/design\/01_pace_python_high_level_discussion.md).\\nUsing the MCR would also allow users to use Horace without a Matlab license,\\nwhilst enabling us to leave the Horace code mainly in the Matlab language.\\n","completion":"## Decision\nThe decision was made to use a compiled Matlab library for Python users to\\nrun PACE.\\nThis is because it was judged to be unfeasible to translate the `horace`\/`herbert`\\ncodebase to C\/C++.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nThis application will need to present a user interface based on the result of multiple API calls.\\nThe majority of client side developers at the Guardian have some React experience, and it is succesfully used in several projects.\\n","completion":"## Decision\nWe will build the front end in React.\\n"}
{"prompt":"## Context and Problem Statement\\n**What type of arichitecture is to be used ?**\\nConsidered options:\\n1. Monolith\\n2. Modular Monolith\\n3. Microservices\\nDrivers:\\n* Project is realized as GreenField\\n* A small development team (~10)\\n* Little experience in implementing distributed software\\n* Load - (50-150 req\/sec) - not so much\\n","completion":"## Decision\nOption no 2 - Modular Monolith.\\nArchitecture suitable for teams with little experience in implementing distributed systems.\\nIt gives the possibility of gradual migration to distributed. Suitable for  greenfield systems, where high variation in requirements is expected.\\narchitecture.\\n"}
{"prompt":"## Context and Problem Statement\\nCurrently, ADR files follow this format: `NNNN-adr-title.md`, with NNNN being an incremental number from `0000` to `9999`.\\nIt causes an issue during a `git merge` when two developers have created a new ADR on their respective branch.\\nThere is a conflict because [an ADR number must be unique](20200926-use-the-adr-number-as-its-unique-id.md).\\n","completion":"## Decision\nFrom now on, we won't use ADR numbers anymore.\\nAn ADR will be uniquely identified by its slug (ie. its filename without the extension), and its filename will have the following format: `YYYYMMDD-adr-title.md`, with `YYYYMMDD` being the date of creation of the file.\\nAs a result, there won't have conflicts anymore and the files will still be correctly sorted in the IDE thanks to the date.\\nFinally, the ADRs will be sorted with these rules (ordered by priority):\\n1. By Date field, in the markdown file (if present)\\n2. By Git creation date (does not follow renames)\\n3. By file creation date if no versioned yet\\n4. By slug\\nThe core library is responsible for sorting.\\n"}
{"prompt":"## Context\\nThe GRPC default message size of 4 mb currently causing a bottleneck between cc-route-syncer and copilot. As our message sizes increased with scale this prevents us from sending messages to copilot.\\n","completion":"## Decision\nWe have decided to reduce the message size by enabling GRPC's GZIP compression between cc-route-syncer and copilot.\\n"}
{"prompt":"## Context\\nWikibase-related code is found in various places and under different degrees of isolation.\\nParts of the PHP code, especially early \/ fundamental parts like the data model and its serialization,\\nare independent libraries in separate Git repositories, distributed via Composer.\\nSome JavaScript code, such as the new Termbox,\\nis also developed independently, and typically included as a Git submodule.\\nOn the other hand, much of Wikibase is included directly in the Wikibase Git repository,\\noften as part of the monolithic \u201cLib\u201d component.\\nPart of the goal of the Decoupling Hike (June to August 2020) has been to develop a strategy for refactoring this \u201cLib\u201d component.\\nWe found that the separate libraries have obvious benefits thanks to being stand-alone, separate components,\\nbut also have downsides for development:\\nit is inconvenient to have to develop the libraries in separate repository,\\nand cumbersome to get the changes back into the main Wikibase Git repository.\\nAt best, a submodule pointer needs to be updated;\\nat worst, the library needs to publish a new release, which then needs to make its way into [mediawiki\/vendor][].\\nWe investigated a \u201cmonorepo\u201d approach in [T254920][],\\nand propose that it offers the best of both worlds.\\nMonorepos are also used by others, including the Symfony PHP framework\\n([blog post][Symfony blog post], [talk video][Symfony talk video]).\\n","completion":"## Decision\nWikibase.git will become a monorepo,\\ncontaining not just the overall MediaWiki extension code\\nbut also the code and history of libraries that can stand on their own.\\nChanges to those libraries become immediately effective within Wikibase,\\nbut are also published separately.\\nWhere possible, sections of Wikibase Lib will be extracted into separate libraries.\\nDependencies on MediaWiki are removed\\n(or replaced with suitable MediaWiki libraries, e.g. [wikimedia\/timestamp][] instead of `wfTimestamp()`).\\nTheir source code is moved from subdirectories of `lib\/includes\/` and `lib\/tests\/phpunit\/` into a subdirectory of `lib\/packages\/`\\n(adjusting the paths in extension `AutoloadNamespaces` and elsewhere),\\nnamed after the prospective Composer package name,\\nand a `composer.json` file is added there.\\nThe [git filter-repo][] tool can then be used to extract a subset of the Wikibase Git history with only the changes relevant to the new library;\\nthis new read-only repository can be used as the VCS source of the Composer package,\\nand is automatically updated through a GitHub action (see e.g. `.github\/workflows\/filterChanges.yml`).\\nThe Decoupling Hike team demonstrated these steps for the [wikibase\/changes][] library;\\nsee [T256058][] and related tasks for details.\\nFormerly stand-alone libraries will be merged into the Wikibase Git repository.\\nTheir history will be preserved, and they will also be extracted into separate Git repositories again,\\nusing the [git filter-repo][]-based process outlined above.\\nWe expect that it will be possible to produce identical Git hashes,\\nmaking this migration transparent to other users of the libraries\u2019 Git repositories \u2013\\nthe repositories will simply no longer be the main source of truth.\\nThe Decoupling Hike team has not done this for any existing library.\\n"}
{"prompt":"# Context\\nSee [https:\/\/github.com\/azavea\/raster-foundry\/issues\/1253](https:\/\/github.com\/azavea\/raster-foundry\/issues\/1253) for feature requirements\\nWe need to provide a way for users to request shipments or exports of imagery from our system in GeoTIFF and other formats, and we need to be able to deliver the finalized export to users.\\nWe need to be able to deliver exports of large areas at high resolution, and we need to provide users with some indication of the progress of their requested exports.\\nAt a very high level, no matter what architecture we choose, the workflow is probably going to look something like this:\\n1. User makes an export request via the API\\n2. Generate an export configuration based on the User's request\\n3. Render output (and report status)\\n4. Delivery (and report status)\\n","completion":"## Decision\nWe will focus our immediate development efforts on providing a solution for users who want to export RGB imagery from Projects that already contain RGB bands, with little analysis or alteration aside from color correction and mosaicking. This approach will constrain the scope of development and allow us to iterate quickly on addressing a concrete problem faced by real users.\\nPut another way, we are explicitly deferring implementation of export functionality for the following use cases:\\n* False-color composites\\n* Hyper-spectral or non-RGB data sources\\n* Projects whose Scenes don't all have the same available bands and resolution\\n* Tool outputs\\n* Arbitrary Tool nodes\\nHowever, we expect that we will need to cover these deferred cases sooner rather than later, so we will design our data model in a way that we believe will allow us to easily expand the export functionality going forward. More details on this design are given below.\\nAdditionally, in order to avoid making decisions about functionality that we have limited information about, some further aspects of the functionality outlined above are not addressed in this ADR. These are noted at the end of the Decision section.\\nNote: Airflow 1.8 has an early version of a REST API that may allow us to trigger DAG Runs directly from the user-facing API endpoint, which would eliminate the need for a periodic task to scan for pending exports and thereby reduce latency. However, this feature is in its early stages, so we will wait until it is more mature before we utilize it.\\n### 3. Render output\\nThis will be a Spark job will which take the export configuration (or relevant portions of it), render the necessary output, and store the result on S3. The details of how this job functions or where it should run are not defined here.\\n### 4. Delivery\\nBecause the result of the export task will be stored on S3 in a consistent format, each delivery mechanism can be structured as a process that copies data from S3 and makes it available at a target. The three targets that we know of are: internal S3, accessible via HTTP, external S3, and DropBox. Internally, we will store this information in a database record so that users can have multiple AWS and DropBox credentials tied to their account. The authentication details to external accounts will need to be treated as sensitive information (that is, the same as passwords).\\n#### Internal S3\\nThe delivery process will need to make the export result available via HTTP. This could be done in a number of ways, such as moving it to a specific S3 bucket or making a database record that links the export result with a publicly-accessible URL. This delivery option will also need to include rate-limiting and metering. The exact details of this functionality are left undefined for now.\\n#### External S3\\nThe delivery process will require an S3 bucket and prefix where files should be stored, and AWS credentials to an external AWS account with sufficient permissions to write to that location. These parameters should be provided by the user. The delivery process will need to authenticate with the provided credentials, and then copy the result of the export process from our S3 into the provided bucket and prefix location.\\n#### DropBox\\nThe delivery process will require credentials to a DropBox account, as well as a location within that DropBox. Functionality will be similar to the external S3 delivery mechanism: the delivery process will authenticate with the provided credentials and then attempt to copy the export results from our S3 to the provided DropBox account. Because DropBox accounts have limited storage, this delivery mechanism will need to pre-verify that enough space is available to store the export result, and will need to handle errors that may arise from a user's DropBox running out of space.\\n### 5. Status updates\\nWe will need to provide the user with status updates about the progress of their export. Consistent with our existing status update mechanisms, we will express this as an enumeration of statuses on the export records in the database. This enumeration is expected to look something like the following (subject to change):\\n* PENDING\\n* The export needs to have an Airflow DAG kicked off for it.\\n* SUBMITTED\\n* The export has a DAG but has not been processed further.\\n* RENDERING\\n* The export is being rendered.\\n* DELIVERING\\n* The export has been rendered and is being delivered.\\n* SUCCESS\\n* The export has been delivered successfully and is complete.\\n* ERROR\\n* The export has encountered an error and has failed to complete.\\nWe will structure status updates as separate Airflow tasks wherever possible. This is because update mechanisms that are too tightly coupled to the task they report on can often be brought down with it when it fails, causing incorrect reporting. By keeping the status tasks separate from the tasks which do the \"real\" work, we can ensure that reporting is based on Airflow's view of task status, which we expect to be most often correct. An example DAG structure is given below:\\n![example DAG structure](diagrams\/0017-figure-02.png)\\nDecisions about structure for the following features are left undefined until we gather more information about usage and desired functionality:\\n* Rate limiting and metering for downloads\\n* We don't know how much detail is necessary and this could carry high engineering costs depending on the solution chosen.\\n* User interface for allowing users to select different bands for export, false-color composites\\n* This is not anticipated to be a priority of our early users and we don't have enough information about the use cases here to develop a clear path forward for implementing this in the backend.\\n* Handling for Projects that contain Scenes with different sets of bands and\/or different resolutions.\\n* Same as previous.\\n# Consequences\\nWrapping all exports inside of Tools is likely to require greater upfront development effort than exporting Projects directly would have. However, as described above, we believe that this approach will pay significant dividends in terms of development effort and robustness later.\\nStoring status updates in an enumeration makes it more difficult for us to store extra metadata about export status, and it prevents us from storing fine-grained statuses such as percent completion. It is simpler to implement, however, and more flexible solutions are also more complex. We believe the simplicity of an enumeration-based solution will make it straightforward for us to extend the status update functionality once a concrete use case arises.\\nBecause this is a complex feature being developed largely without direct user feedback (so far), we have deferred architecting a number of aspects of this functionality until we gather more real-world usage information. This reduces the risk that we will expend development resources on features that users don't need. By deferring implementation, we expose ourselves to the possibility that we may have to develop these features under time pressure at some point in the future, but this is balanced by the fact that if such a situation does occur, we will have a much clearer understanding of the necessary functionality at that point than we do now.\\n"}
{"prompt":"## Context\\nWe have a number of internal services that need to have\\ninternal only service records. This allows clients to connect\\nwithin their own environment or stack without needing a fully\\nqualified domain name.\\nThese records should not be visible outside the local environment or stack.\\n","completion":"## Decision\nWe've bought a domain name that will not be connected to the rest of the public\\nDNS system. This will be used by our internal services.\\nThis domain will *not* have DNS records added to it.\\n"}
{"prompt":"## Context\\nPreviously Connaisseur only supported the configuration of a single notary, where all signature data had to reside in. Unfortunately this is rather impractical, as one doesn't create all signatures for all images one uses in a cluster. There is a need to access signature data from multiple places, like in a setup where most images come from a private registry + notary and some from Docker Hub and their notary.\\nThere is also the problem that a single notary instance could use multiple root keys, used for creating the signatures, like in the case of Docker Hub. Connaisseur also only supports a single root key to be trust pinned, thus making it impractical.\\nThat's why the decision was made to support more than one notary and multiple keys per notary, which leads to the question how the new configuration should look like. This also has implications on the notary health check, which is important for Connaisseur's own readiness check.\\nProblem: Might get inconsistent, should list ordering in python get shuffled around\\n#### Option 2 (Explicit default)\\nOne of the notary configuration will be given a `default` field, which marks it as the default value.\\nProblem: No real problems here, just an extra field that the user has to care about.\\n#### Option 3 (Mandatory Notary)\\nThe `notary` (and potentially `key`) field is mandatory for the image policy.\\nProblem: Creates configuration overhead if many image policies use the same notary\/key combination.\\n#### Option 4 (Default name)\\nIf no `notary` or `key` are given in the image policy, it is assumed that one of the elements in the notary list or key list has `name: \"default\"`, which will then be taken. Should the assumption be wrong, an error is raised.\\n### Choice 3\\nPreviously, the readiness probe for connaisseur also considered the notary's health for its own status. With multiple notary instances configured, this behavior changes.\\n#### Option 1 (Ignore Notary)\\nThe readiness probe of Connaisseur will no longer be dependent on any notary health checks. The are completely decoupled.\\nProblem: No knowledge that Connaisseur will automatically fail because of an unreachable notary, before one tries to deploy an image.\\n#### Option 2 (Health check on all)\\nIn order for connaisseur to be ready, all configured notaries must be healthy and reachable.\\nProblem: A single unreachable notary will \"disable\" Connaisseur's access to all others.\\n#### Option 3 (Log Notary status)\\nA mix of option 1 and 2, whereas the readiness of Connaisseur is independent of the notaries health check, but they are still being made, so unhealthy notaries can be logged.\\nProblem: At what interval should be logged?\\n","completion":"## Decision\n### Choice 1\\nOption 1 was chosen, to keep configurational duplication at a minimum.\\n### Choice 2\\nOption 4 was chosen. If more than one notary configuration or key within a configuration are present, one of those can be called \"default\" (setting the `name` field). That way it should be obvious enough, which configuration or key will be used, if not further specified within the image policy, while keeping configuration effort low.\\n### Choice 3\\nOption 3 was chosen. Notary and Connaisseur will be completely decoupled, with Connaisseur logging all notaries it can't reach. This way Connaisseur can still be operational, even with all notaries being unreachable. Otherwise Connaisseur would have blocked even images that were allowlisted. This is a breaking change, but we agreed that it is better as it allows e.g. deployments for which the respective image policy specifies `verify: false`.\\n"}
{"prompt":"## Context\\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\\n","completion":"## Decision\nThe application will provide an info page.\\n"}
{"prompt":"## Context and Problem Statement\\nWe'd like to have a permanent URL for the experimenter docs hub that is:\\n- Memorable\\n- Stable\\n- Ideally, part of the rest of our data information architecture\\n","completion":"## Decision\nWe will use a custom domain for now (experimenter.info) until a decision is made on the general organization of data.mozilla.org in the future, at which time we will update the URL to fit in with the rest of our data documentation.\\n"}
{"prompt":"## Context and Problem Statement\\nCollections will be key in the Decentraland ecosystem. To bring more scalability and reduce costs the L2 will be seen as the main chain, and L1, as the gateway for collectors to use\/buy\/sell tokens in other platforms.\\nL1 could be seen as Ethereum mainnet and L2 any other sidechain EVM compatible as Matic or xDai.\\nEach collection follows the ERC721 standard. Every token transferred between layers will keep a reference of itself in the main chain:\\n- Collection address in L2. E.g: `0x2dac71c8c8a4b9547b53c1e1838152ca3277ce76`\\n- Token Id in L2. E.g: `1`\\n- Token URI in L2 (URI used for Decentraland and other platforms to know how the token looks like). E.g: `https:\/\/peer.decentraland.org\/lambdas\/collections\/standard\/erc721\/0x2dac71c8c8a4b9547b53c1e1838152ca3277ce76\/0`\\nThis document presents 2 alternatives on how to manage the collections between L1 & L2.\\n","completion":"## Decision\n#### Alternative 1\\n#### Pros\\n- N\/A\\n#### Cons\\n- Moving the first token from a collection to L1, involves creating a contract using the factory (gas cost).\\n#### Implications\\n- Keep the same address and token ids between layers.\\n- Users will need to send a transaction for each collection to authorize the usage of tokens.\\n- Third party marketplaces (OpenSea) will not know without any effort if the contract is an official Decentraland collection in L1.\\n#### Alternative 2 \u2705\\n##### Pros\\n- Users will need to send only one transaction per marketplace to authorize the usage of tokens.\\n- We can use the unique collection as a valid and the official one for Decentraland in L1.\\n- Automatic and instant availability in marketplaces with the whitelisted collection.\\n- Fewer moving parts\\n##### Cons\\n- N\/A\\n##### Implications\\n- Collection addresses and token Ids will not be the same in L2 and L1.\\n# Open questions\\n- **Do the token URI changes in L2?**\\nToken URI of L2 is stored in the main collection in L1\\n```yaml\\nOpenSea (mainnet): opensea.com\/mainnet\/tokens\/{collection_l1}\/{hash(collection_l1_1, token)}\\nOpenSea (L2): opensea.com\/matic\/tokens\/{collection_l2_1}\/{token}\\n```\\n```python\\n# L2\\ncollection_l2_1.tokenURI(token_1) = https:\/\/peer.decentraland.org\/collection_l2_1\/token_1\\nmapping(tokenId => string) tokenURI\\n# L1\\ncollection_l1.tokenURI(hash(collection_l2_1, token_1)) = https:\/\/peer.decentraland.org\/collection_l2_1\/token_1\\nmapping(tokenId => string) tokenURI\\n```\\n- **Alt 1. Is it necessary to burn the token when L1->L2?**\\nIt could be not done. It is a common practice.\\n- **Alt 1. We could have the same collection address in L2 and L1**\\nThat was the idea.\\n- **How do we validate allowed\/denied items when moving from L2 to L1?**\\nValidate if the collection was created by factory.\\nTBD.\\n- **How do we prevent abuse on meta-transactions paid by DCL?**\\n.\\n- **What do we do if Matic is no _longer viable_ or _moving_ to another chain?**\\nWe can restore the state from a blockchain snapshot and copy the contracts.\\nUsers may need to manually send the tokens as a result of the migration.\\nThere are coordination risks with third parties (open sea selling items in the middle of the migration).\\nEnforce pause mechanisms? Upgradable contracts?\\n- **Who pays the tx fees when L2->L1?**\\nAs it is today, the user pays the TX.\\n- **How does the approval flow affect any of the options in L2?**\\n> If DAO is in L2 and it is cheap it would be good to vote everything.\\n_Reviewers will have to review several orders of magnitude of collections in L2_\\n# Participants\\n- Esteban Ordano\\n- Ignacio Mazzara\\n- Marcelo Alaniz\\n- Nicolas Santangelo\\n- Juan Cazala\\n- Agustin Mendez\\nDate: 2020-10-13\\n"}
{"prompt":"## Context\\nThe issue motivatina# [short title of solved problem and solution]\\nWhat is the issue that we're seeing that is motivating this decision or change?\\n* Status: [proposed | rejected | accepted | deprecated | ... | superseded by [ADR-0005](0005-example.md)] <!-- optional -->\\n* Deciders: [list everyone involved in the decision] <!-- optional -->\\n* Date: [YYYY-MM-DD when the decision was last updated] <!-- optional -->\\nTechnical Story: [description | ticket\/issue URL] <!-- optional -->\\n## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\n","completion":"## Decision\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | ... | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, ...]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, ...]\\n* ...\\nWhat is the change that we're proposing and\/or doing?\\nThe change that we're proposing or have agreed to implement.\\n"}
{"prompt":"## Problems with the current approach\\n- The `sub` attribute might be opaque and not usable between systems, e.g. when sending entitlements to other systems\\n- Different login methods may have different formats for external user identifiers\\n- Different login methods might use conflicting values for claims\\n- E.g. `foo@csc.fi` via HAKA or CSC SSO (TODO: do we know of cases like this?)\\n- A single user might have multiple external identifiers (TODO: do we know of cases like this?)\\n","completion":"## Decision\nWe'll add internal random user ids to REMS. This internal user id will\\nbe the key that users are referred to within REMS. The user's external\\nid will be stored in the attributes JSON blob.\\nThis will allow us more flexibility in the future when identity\\nrequirements and use cases change, and might also make all sorts of\\nmigrations easier (since internal user ids don't need to be touched).\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nSince the service launched, candidates have been able to enter details about their academic and other\\nrelevant qualifications using free text answers for most questions.\\nWe\u2019ve seen that free-text answers can often lead to candidates submitting applications with spelling errors, which may\\nreduce their chances of being offered an interview.\\nThere is also some complexity around entering GCSE standard equivalent qualifications; there are a number of different\\nEnglish GCSE exams, and science GCSEs can be awarded in a number of different ways.\\nThere are two key differences between English and Science GCSEs\\n- for English GCSEs they have different certificates and are considered separate qualifications (eg. English Language and English Literature)\\n- a candidate can enter an 'other English GCSE' qualification and give it a name, eg. Cockney Rhyming Slang.\\nWe considered two different options for how we store this data in the database\\n- as separate records, one for each GCSE qualification\\n- as a single record with the different qualifications stored in a JSON blob in a `structured_grades` column\\nPart of the reason for this problem is that the English GCSE form is a single page but is gathering information about\\nmultiple qualifications.\\n### Separate records\\n#### Pros\\n- The database structure more closely reflects the domain structure of having multiple records for multiple\\nqualifications\\n#### Cons\\n- Increased code complexity. The `ApplicationForm` would need to have multiple English GCSEs, and anywhere that uses\\nthe English GCSE data would need to be refactored to handle multiple GCSEs. The grade controller would also have to build\\nthe `EnglishGCSEGradeForm` from multiple qualifications\\n- No obvious way to store the name of \"other English GCSEs\" eg. Cockney Rhyming Slang. One idea was to introduce a new\\nfield to capture this data\\n### JSON blob in a single record\\n#### Pros\\n- The logic for serialising and deserialising the GCSEs to JSON can be encapsulated inside the form object.\\nSo it has a smaller footprint and impact on the codebase\\n- Using a JSON blob we can easily store the name of \"other English GCSEs\" eg. Cockney Rhyming Slang\\n```\\n{\\nsubject: 'english',\\nstructured_grades: { 'English Language': 'E', 'English Literature': 'E', 'Cockney Rhyming Slang: 'A*'},\\nlevel: 'gcse'\\n}\\n```\\n#### Cons\\n- Database structure doesn't reflect domain structure\\n","completion":"## Decision\nWe decided to go for the JSON blob approach\\n- Within the candidate interface, whenever we reference English GCSEs we always discuss them together, rather than\\none at a time, so there is little reason to treat them as separate from that point of view\\n- We didn't feel that domain logic should necessarily dictate the structure of information in the database, if\\nit meant introducing extra code complexity\\n"}
{"prompt":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\\n","completion":"## Decision\n* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\\n"}
{"prompt":"## Context\\nThe stream encoder component defined in this project is designed to subscribe\\nto the MQTT broker provided by SmartCitizen. This broker is used to support\\nSmartCitizen's existing platform and will not be modified specifically for\\nDECODE.\\nEvents published via the broker look like this:\\n```json\\n{\\n\"data\": [\\n{\\n\"recorded_at\": \"2018-11-01T13:59:06Z\",\\n\"sensors\": [\\n{\\n\"id\": 10,\\n\"value\": 0\\n},\\n{\\n\"id\": 10,\\n\"value\": 0\\n},\\n{\\n\"id\": 14,\\n\"value\": 0\\n},\\n{\\n\"id\": 55,\\n\"value\": 29.1\\n},\\n{\\n\"id\": 56,\\n\"value\": 36.64\\n},\\n{\\n\"id\": 16,\\n\"value\": 41.4\\n}\\n]\\n}\\n]\\n}\\n```\\nThat is we receive a timestamp for the event, and then an array of readings\\ncomprising an `id` (which is a numeric id that relates to specific sensor type\\nas described by SmartCitizen) and a numeric value.\\nNot included in the received event are the following fields:\\n* The location of the device\\n* The exposure of the device (indoor, outdoor)\\n* The name, description or units of received sensor measurements\\nSensor id values are described by SmartCitizen via an API endpoint:\\nhttps:\/\/api.smartcitizen.me\/v0\/sensors\\nThis returns JSON that looks like this:\\n```json\\n[\\n{\\n\"id\": 3,\\n\"uuid\": \"ac284ba3-e2fc-4795-b2b1-530b32a9b05b\",\\n\"parent_id\": null,\\n\"name\": \"DHT22\",\\n\"description\": \"A digital temperature and humidity sensor. It uses a capacitive humidity sensor and a thermistor to measure the surrounding air, and spits out a digital signal on the data pin (no analog input pins needed)\",\\n\"unit\": null,\\n\"created_at\": \"2015-02-02T18:14:15Z\",\\n\"updated_at\": \"2015-02-02T18:14:15Z\",\\n\"measurement\": null\\n},\\n{\\n\"id\": 20,\\n\"uuid\": \"4a2e9c80-748c-44a3-b400-8824f50d19cd\",\\n\"parent_id\": null,\\n\"name\": \"MiCS4514\",\\n\"description\": \"Gas Sensor\",\\n\"unit\": null,\\n\"created_at\": \"2015-02-02T18:31:50Z\",\\n\"updated_at\": \"2015-02-02T18:31:50Z\",\\n\"measurement\": null\\n}\\n]\\n```\\nSo it is this JSON that encodes all metadata about each sensor including names,\\ndescriptions and units.\\n### Constraints\\n### Options\\nWe identified the following options:\\n* The encoder could encrypt the data exactly as received to the encrypted\\ndatastore, meaning that when the receiving dashboard fetches and decrypts the\\ndata it will have to make a request to the SmartCitizen API to convert the\\nnumerical sensor IDs into something more meaningful to know how to represent\\nthe data.\\nThis fetching of data from the SmartCitizen API could happen every time the\\ndashboard scrapes data from the encrypted datastore, or it could be updated\\nto include a local copy of the sensor ID mappings.\\n* The encoder could enrich the data, so that the data that is then written to\\nthe datastore has locations, and sensor metadata added directly to the file\\nmeaning that the receiver can immediately understand the data in order to\\nrender it appropriately.\\nNote that again here we could add this enriching metadata dynamically on\\nevery request, by fetching from the SmartCitizen API, or by caching a local\\ncopy of the sensor mappings such that this information is already known to\\nthe encoder.\\n","completion":"## Decision\nWe will enrich the data within the encoder using a cached copy of\\nSmartCitizen's sensor JSON.\\nReasoning:\\n* Removes any coupling between the final receipient of the data and\\nSmartCitizen. This avoids consumers having to build knowledge of the\\nSmartCitizen API into their systems, and would be more scalable if other data\\nsources were ever added.\\n* Sensor types included by SmartCitizen do not change frequently (last update\\nwas 2016, so 2 years ago at time of writing).  Because of this low update\\nfrequency there is likely to be little issue with having to update the local\\ncached copy frequently.\\nThe output JSON we will publish will look like this:\\n```json\\n{\\n\"longitude\": 2.234,\\n\"latitude\": 54.213,\\n\"exposure\": \"INDOOR\",\\n\"recordedAt\": \"2018-11-01T15:06:23Z\",\\n\"userUid\": \"abc-123-fbd\",\\n\"sensors\": [\\n{\\n\"sensorId\": 29,\\n\"name\": \"MEMS Mic\",\\n\"description\": \"MEMS microphone with envelope follower sound pressure sensor (noise).\",\\n\"unit\": \"dBC\",\\n\"type\": \"SHARE\",\\n\"value\": 64.252\\n},\\n{\\n\"sensorId\": 14,\\n\"name\": \"BH1730FVC\",\\n\"description\": \"Digital Ambient Light Sensor\",\\n\"unit\": \"Lux\",\\n\"type\": \"BIN\",\\n\"bins\": [5,10,50,500],\\n\"values\": [0,1,0,0,0]\\n},\\n{\\n\"sensorId\": 4,\\n\"name\": \"HPP828E031\",\\n\"description\": \"Temperature\",\\n\"unit\": \"\u00baC\",\\n\"type\": \"MOVING_AVG\",\\n\"interval\": 900,\\n\"value\": 18.2\\n}\\n]\\n}\\n```\\n"}
{"prompt":"## Context\\nCompletely delete a file is not an easy thing to do in invenio. To achieve this goal we tried to delete as many info as possible.\\n","completion":"## Decision\nDelete a record means delete it from the disk, from the database tables and not indexing it anymore. We decided to delete it from all the tables except `record_metadata_version`\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"Context\\n=======\\nThe alpha initially took the approach of starting with a vanilla set of cloud\\nfoundry manifests, and merging new values into it using spiff. This became\\ndifficult to reason about, and cf-release was forked because it was easier than\\noverriding necessary values using spiff. However, the confusing spiff hierarchy\\nremained.\\nDecision\\n========\\nWe will create our own set of manifests based on those in cf-release.\\nWe will modify these as required.\\nWe will use spruce to merge a series of files into the yml required by cloud\\nfoundry\\nWe will name the files with a numeric prefix and rely on shell globbing to\\ndetermine the merge order rather than listing the merge order in the\\nbuild-manifest script.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe will need to define an upgrade path for our cloud foundry manifests - making\\nsure we pull in the required values from the new release manifests into ours.\\n","completion":"## Decision\n========\\nWe will create our own set of manifests based on those in cf-release.\\nWe will modify these as required.\\nWe will use spruce to merge a series of files into the yml required by cloud\\nfoundry\\nWe will name the files with a numeric prefix and rely on shell globbing to\\ndetermine the merge order rather than listing the merge order in the\\nbuild-manifest script.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe will need to define an upgrade path for our cloud foundry manifests - making\\nsure we pull in the required values from the new release manifests into ours.\\n"}
{"prompt":"## Context\\nLerna was being a pain; not working as described in the docs. It seems a bit old and crufty, and was built before yarn workspaces were a thing.\\n","completion":"## Decision\nSwitch to [yarn workspaces](https:\/\/classic.yarnpkg.com\/en\/docs\/workspaces\/). We may also use [mono](https:\/\/github.com\/enzsft\/mono) or [rush](https:\/\/rushjs.io) to add extra features for managing a monorepo.\\n"}
{"prompt":"## Context and Problem Statement\\nService developers seeking to use play-frontend to build their frontend microservices need to follow around\\n12 separate integration steps involving two separate Scala libraries: play-frontend-govuk and play-frontend-hmrc. Failure to\\nperform any one of these steps correctly, or making changes that break any of this configuration,\\nresults in failed builds, incorrectly styled components or broken Javascript-enabled functionality. These steps involve:\\n* Adding play-frontend-govuk and play-frontend-hmrc as SBT dependencies.\\n* Setting up an SBT asset pipeline to compile and optimise the CSS and Javascript into bundles suitable for production.\\n* Adding multiple asset routing rules.\\n* Adding the boilerplate SCRIPT and LINK tags to the HTML page template to link to the assets at runtime.\\n* Adding Javascript to initialising the GOV.UK and HMRC components.\\nFollowing investigation, we discovered that the integration process could be drastically simplified\\nby providing head and script helpers that would link to pre-compiled and optimised asset bundles from\\nthe hmrc-frontend webjar, subsuming the govuk-frontend assets and taking responsibility\\nfor component initialisation. This move would:\\n* simplify the migration path from assets-frontend\/play-ui\/govuk-template to play-frontend\\n* particularly benefit teams lacking an embedded full time frontend developer\\n* not radically change the way microservices integrate with play-frontend\\n* still allow teams to create custom components and add their own custom CSS where needed\\n* continue to allow for local development without reliance on networked dependencies\\nOverall, the total number of integration steps would be reduced from 12 to 6.\\nFacing these considerations, should we therefore\\n* change the pre-compiled hmrc-frontend bundles to also include govuk-frontend and to initialise all components\\n* add the pre-compiled and optimised CSS and Javascript bundles into the published hmrc-frontend npm package and\\n* create Twirl helper components to render the HTML snippets necessary to link to them?\\n## Decision Drivers\\n* The integration difficulties teams have reported over the past 12 months.\\n* The problematic nature of diagnosing build issues in frontend microservices due to the requirement for a deep\\nknowledge of both Scala and frontend build tooling, which is rare.\\n* The pressing need to migrate services away from assets-frontend for reasons of accessibility, security and sustainability.\\n* That services do not generally need extensive service-specific custom CSS. UI components are heavily standardised by GDS.\\n* That services do not generally need extensive service-specific Javascript. The mantra of progressive\\nenhancement demands services build without Javascript if possible.\\n* That teams should be free to create custom components with custom CSS where there is a genuine need, without relying\\non other teams. Historically denying teams this ability arguably led to many of the problems with assets-frontend.\\n* That teams should continue to be able to develop locally without relying on networked dependencies.\\n","completion":"## Decision\n* The integration difficulties teams have reported over the past 12 months.\\n* The problematic nature of diagnosing build issues in frontend microservices due to the requirement for a deep\\nknowledge of both Scala and frontend build tooling, which is rare.\\n* The pressing need to migrate services away from assets-frontend for reasons of accessibility, security and sustainability.\\n* That services do not generally need extensive service-specific custom CSS. UI components are heavily standardised by GDS.\\n* That services do not generally need extensive service-specific Javascript. The mantra of progressive\\nenhancement demands services build without Javascript if possible.\\n* That teams should be free to create custom components with custom CSS where there is a genuine need, without relying\\non other teams. Historically denying teams this ability arguably led to many of the problems with assets-frontend.\\n* That teams should continue to be able to develop locally without relying on networked dependencies.\\nChosen option: \"Add pre-compiled assets with auto-initialisation\", because this is a major step forward in improving\\nthe ease of use of play-frontend and goes a long way to address many of the issues\\nusers have experienced.\\nThe change to the S3 assets is not of concern due to the fact that the existence of these\\nassets was never publicised or fully documented and we have seen only a tiny number of requests for these assets\\nin server logs.\\n### Positive Consequences\\n* A radically simplified integration process for Scala developers.\\n* Fewer moving parts in frontend microservices and reduced boilerplate.\\n* No changes needed by teams wanting to continue to consume the original un-compiled assets.\\n* Users of the npm package should not be affected by this change as it adds files only.\\n* Simplified usage outside Scala microservices e.g. in outage\/waiting pages.\\n* Bundling govuk-frontend and hmrc-frontend together means there is no longer the possibility for the use\\nof incompatible versions of govuk-frontend with hmrc-frontend.\\n* Best practice is to initialise all GOV.UK and HMRC components rather than\\ncherry picking to avoid the risk of breaking accessibility features. Doing this automatically\\nmeans there is one less thing for teams to worry about getting right.\\n### Negative Consequences\\n* Teams using the S3-distributed hmrc-frontend assets will no longer have the option to separately initialise the\\nGOVUK and HMRC components. However, they will retain access to window.GOVUKFrontend and window.HMRCFrontend if\\nneeded for initialisation of dynamically injected content.\\n* On their next upgrade, teams using the S3-distributed hmrc-frontend assets will need to remove the SCRIPT tag referencing the\\ngovuk-frontend bundle and any calls to GOVUKFrontend.initAll() or HMRCFrontend.initAll().\\n* The distribution build task diverges from the pattern set by govuk-frontend. In govuk-frontend, the distributable does not\\nauto-initialise the govuk components. This may affect developers consuming hmrc-frontend using git rather than via the npm package.\\n"}
{"prompt":"## Context\\nThe quality of information included in our pull requests varies greatly which can lead to code reviews which take longer and are harder for the person to understand the considerations, outcomes and consquences of a series of changes.\\nA couple of recent projects have found a GitHub pull request template to have been a positive change. Prompting what pull request descriptions should include has lead to better documented changes that have been easier to review on the whole.\\n","completion":"## Decision\nInclude a basic pull request template for GitHub so that every pull request prompts every author to fill it out.\\n"}
{"prompt":"## Context\\nProviding shell access to a node in AWS is time consuming to set up in a secure manner.\\n","completion":"## Decision\nUse Session Manager to establish shell connections to the client instance.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nUsing Yarn Workspaces as decided in [ADR-0003](.\/0003-use-yarn-workspaces.md) turned out to be impractical and produced too many problems.\\n","completion":"## Decision\nWe're going to switch to [projen](https:\/\/github.com\/projen\/projen).\\nRelated to that we're going to switch to npm 7 and drop yarn.\\n"}
{"prompt":"## Context\\nSince its debut, EdgeX has had a configuration seed service (config-seed) that, on start of EdgeX, deposits configuration for all the services into Consul (our configuration\/registry service).  For development purposes, or on  resource constrained platforms, EdgeX can be run without Consul with services simply reading configuration from the filesystem.\\nWhile this process has nominally worked for several releases of EdgeX, there has always been some issues with this extra initialization process (config-seed), not least of which are:\\n- race conditions on the part of the services, as they bootstrap, coming up before the config-seed completes its deposit of configuration into Consul\\n- how to deal with \"overrides\" such as environmental variable provided configuration overrides. As the override is often specific to a service but has to be in place for config-seed in order to take effect.\\n- need for an additional service that is only there for init and then dies (confusing to users)\\nNOTE - for historical purposes, it should be noted that config-seed only writes configuration into the configuration\/registry service (Consul) once on the first start of EdgeX.  On subsequent starts of EdgeX, config-seed checks to see if it has already populated the configuration\/registry service and will not rewrite configuration again (unless the --overwrite flag is used).\\nThe design\/architectural proposal, therefore, is:\\n- removal of the config-seed service (removing cmd\/config-seed from the edgex-go repository)\\n- have each EdgeX micro service \"self seed\" - that is seed Consul with their own required configuration on bootstrap of the service.  Details of that bootstrapping process are below.\\n### Command Line Options\\nAll EdgeX services support a common set of command-line options, some combination of which are required on startup for a service to interact with the rest of EdgeX. Command line options are not set by any configuration.  Command line options include:\\n- --configProvider or -cp (the configuration provider location URL - prefixed with `consul.` - for example:  `-cp=consul.http:\/\/localhost:8500`)\\n- --overwrite or -o (overwrite the configuration in the configuration provider)\\n- --file or -f (the configuration filename - configuration.toml is used by default if the configuration filename is not provided)\\n- --profile or -p (the name of a sub directory in the configuration directory in which a profile-specific configuration file is found. This has no default. If not specified, the configuration file is read from the configuration directory)\\n- --confdir or -c (the directory where the configuration file is found - .\/res is used by default if the confdir is not specified, where \".\" is the convention on Linux\/Unix\/MacOS which means current directory)\\n- --registry or -r (string indicating use of the registry)\\nThe distinction of command line options versus configuration will be important later in this ADR.\\nTwo command line options (-o for overwrite and -r for registry) are not overridable by environmental variables.\\nNOTES: Use of the --overwrite command line option should be used sparingly and with expert knowledge of EdgeX; in particular knowledge of how it operates and where\/how it gets its configuration on restarts, etc.  Ordinarily, --overwrite is provided as a means to support development needs.  Use of --overwrite permanently in production enviroments is highly discouraged.\\n### Configuration Initialization\\nEach service has (or shall have if not providing it already) a local configuration file.  The service may use the local configuration file on initialization of the service (aka bootstrap of the service) depending on command line options and environmental variables (see below) provided at startup.\\n**Using a configuration provider**\\nWhen the configuration provider _is_ specified, the service will call on the configuration provider (Consul) and check if the top-level (root) namespace for the service exists.  If configuratation at the top-level (root) namespace exists, it indicates that the service has already populated its configuration into the configuration provider in a prior startup.\\nIf the service finds the top-level (root) namespace is already populated with configuration information it will then read that configuration information from the configuration provider under namespace for that service (and ignore what is in the local configuration file).\\nIf the service finds the top-level (root) namespace is not populated with configuration information, it will read its local configuration file and populate the configuration provider (under the namespace for the service) with configuration read from the local configuration file.\\nA configuration provider can be specified with a command line argument (the -cp \/ --configProvider) or environment variable (the EDGEX_CONFIGURATION_PROVIDER environmental variable which overrides the command line argument).\\n> NOTE:  the environmental variables are typically uppercase but there have been inconsistencies in environmental variable casing (example:  edgex_registry).  This should be considered and made consistent in a future major release.\\n**Using the local configuration file**\\nWhen a configuration provider _isn't_ specified, the service just uses the configuration in its local configuration file.  That is the service uses the configuration in the file associated with the profile, config filename and config file directory command line options or environmental variables.  In this case, the service does not contact the configuration service (Consul) for any configuration information.\\nNOTE:  As the services now self seed and deployment specific changes can be made via environment overrides, it will no longer be necessary to have a Docker profile configuration file in each of the service directories (example:  https:\/\/github.com\/edgexfoundry\/edgex-go\/blob\/master\/cmd\/core-data\/res\/docker\/configuration.toml).  See Consequences below.  It will still be possible for users to use the profile mechanism to specify a Docker configuration, but it will no longer be required and not the recommended approach to providing Docker container specific configuration.\\n### Overrides\\nEnvironment variables used to override configuration always take precedence whether configuration is being sourced locally or read from the config provider\/Consul.\\nNote - this means that a configuration value that is being overridden by an environment variable will always be the source of truth, even if the same configuration is changed directly in Consul.\\nThe name of the environmental variable must match the path names in Consul.\\nNOTES:\\n- Environmental variables overrides remove the need to change the \"docker\" profile in the res\/docker\/configuration.toml files - Allowing removal of 50% of the existing configuration.toml files.\\n- The override rules in EdgeX between environmental variables and command line options may be counter intuitive compared to other systems.  There appears to be no standard practice.  Indeed, web searching \"Reddit & Starting Fights Env Variables vs Command Line Args\" will layout the prevailing differences.\\n- Environment variables used for configuration overrides are named by prepending the the configuration element with the configuration section inclusive of sub-path, where sub-path's \".\"s are replaced with underscores. These configuration environment variable overrides must be specified using camel case.  Here are two examples:\\n~~~~~\\nRegistry_Host  for\\n[Registry]\\nHost = 'localhost'\\nClients_CoreData_Host for\\n[Clients]\\n[Clients.CoreData]\\nHost = 'localhost'\\n~~~~~\\n- Going forward, environmental variables that override command line options should be all uppercase.\\nAll values overriden get logged (indicating which configuration value or op param and the new value).\\n","completion":"## Decision\nThese features have been implemented (with some minor changes to be done) for consideration here:  https:\/\/github.com\/edgexfoundry\/go-mod-bootstrap\/compare\/master...lenny-intel:SelfSeed2.  This code branch will be removed once this ADR is approved and implemented on master.\\nThe implementation for self-seeding services and environmental overrides is already implemented (for Fuji) per this document in the application services and device services (and instituted in the SDKs of each).\\n"}
{"prompt":"## Context\\nBecause this project is going to be delivered as a Beta project by the end of the year, it will be likely that its needs will grow as feedback from real users begins to filter in, as new ideas are conceived, new platforms are required, and as sources of funding are renewed. In order to accommodate these concerns, CHIP needs a reliable and fast UI\/UX that can be easily maintained.\\n","completion":"## Decision\nElm is a pure functional language for building reliable web apps with great performance and no runtime exceptions.\\n"}
{"prompt":"## Context\\nAvailability of systems can be improved by reducing the number of dependencies. Each additional dependency comes with the risk of that service breaking, causing issues to our system. On the other hand, 3rd party services can be leveraged to quickly build and apply managed services at low cost, compared to creating and running these services by ourselves. Therefore, the right balance between dependency on and benefitting from external services is important.\\nDuring run the changes in 3rd party dependencies do not change. Therefore, caching or copying the functionality of these dependencies is possible in many cases. In build and development environments, the dependencies on 3rd party components and services change more often. Therefore, it is oftentimes not opportune to invest in becoming independent on these components and services.\\n","completion":"## Decision\nDuring runtime, we will only depend on resources services from the Google Cloud Platform.\\n"}
{"prompt":"## Context\\nHaving a fixed structure for a project has may advantages, limiting spread of files across multiple folders and contraining locations to known places. THere is an advantage is letting a folder strucute emerge oganically, but also a large risk, as things can break when low-level file locations change, necesitating logs of bug fixing and refactoring. Having a rigid initial structure canb lead to later restrictions, or imposed complexity.\\n","completion":"## Decision\nThe following folder strucure is adopted:\\n.\\n\u251c\u2500\u2500 app\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 controllers\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 models\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 public\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 css\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 img\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 js\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 routes\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 views\\n\u251c\u2500\u2500 docs\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 adr\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 misc\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 project_artifacts\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 templates\\n\u251c\u2500\u2500 node_modules\\n\u251c\u2500\u2500 test\\n\u2514\u2500\u2500 local_only\\n**Update** Removed folders originally specified that were found to not be required during project development: 'log' and 'utility'\\n"}
{"prompt":"## Context\\nBecause of we want to know what is happening in some circumstances, such as granting roles, we introduced message log.\\n","completion":"## Decision\nIn this way every time a successful or unsuccessful operation is done it is recorded.\\n"}
{"prompt":"## Context\\nBuilding the installer requires choosing an appropriate infrastructure as code tool that allows configuring AWS EKS.\\n","completion":"## Decision\nWe can either use vanilla CloudFormation, Terraform or CDK.\\nWe choose CDK. CDK seems like having the most traction right now, it's easy to develop with.\\n"}
{"prompt":"## Context\\nWe are starting to build a beta of the 'Apply for postgraduate teacher training' service. This service will have 5 distinct user groups, who will have their own interface in the service.\\n1. Candidates are people who would like to become a teacher.\\n2. Provider users work for an organisation that provides teacher training, like a school or university\\n3. Vendors make software that universities use for administration\\n4. Referees will be provided by candidates and the service will ask them to provide a reference\\n5. Users within DfE need information about the performance of the service and solve problems\\nFor the architecture of the system, we've discussed 2 options: a microservice architecture and a monolith.\\nIn the microservice architecture, we might split up the service into 3 or more applications - for example an API, a frontend for candidates and a frontend for the provider users. In the monolithic architecture, all components live together in a single application.\\n","completion":"## Decision\nWe've chosen to go for a monolith. After discussing the options, we hypothesise that the microservice architecture for this project does not provide many of the [benefits often associated with microservice approach](https:\/\/rubygarage.org\/blog\/advantages-of-microservices).\\nFor example:\\n- Teams can work independently: we anticipate that the team size of this project will be limited (less than 20), so communication is less of a problem\\n- Independent scaling: traffic for this application will be such that the different components will see similar spikes, so independent scaling is not as useful\\n- Reuse of components: we do not have the need to reuse components in other parts of the organisation\\nNote that we do use other services and APIs to provide functionality - for example, we use [GOV.UK Notify to send emails](https:\/\/www.notifications.service.gov.uk\/), and we'll likely use the [Find API to fetch course data](https:\/\/github.com\/DFE-Digital\/manage-courses-backend).\\nUsing a monolith approach will make the project easier to run for developers and easier to deploy.\\nHowever, there are problems with a monolith as well. In particular, it can lead to an application that is hard to understand because of a wild growth of classes and files. Additionally, the tests on a large monolith may become slow over time, causing frustration and slowdown in development.\\n"}
{"prompt":"## Context\\nTo perform benchmarking tests, I want to set some resources capable to generate\\na high volume of requests against the cluster.\\nTo do this, I could use several tools, like JMeter, Garret, or Taurus.\\nBut I like Blazemeter's Taurus over all because it is very simple to setup and\\nexecute tests.\\nWith Taurus you can create JMeter tests, or use other tools like Selenium, in\\norder to execute and collect results.\\nThen It generates a report that can be easily imported in a JMeter tool or\\nopen it in a data analysis tool like jupyter notebook to draw new diagrams.\\n","completion":"## Decision\nCreate an instance with an easy test (Just request the keys and the metrics).\\n"}
{"prompt":"## Context\\nWe need clearing naming for each layer of the Use an LPA service.\\nThere are 3 layers:\\n- A front service layer through which Actors will access the service.\\n- A front service layer through which Viewers will access the service.\\n- A backend service shared by the two front services which will provide data access and some domain logic.\\n","completion":"## Decision\nThe 3 services will be called:\\n- Actor Front\\n- Viewer Front\\n- Api\\n"}
{"prompt":"## Context\\nAt the time of writing we have explorers, emitters and receivers as three main flow components.\\nThey resemble CR and U from the well-known [CRUD](https:\/\/en.wikipedia.org\/wiki\/Create,_read,_update_and_delete).\\nMost APIs are CRUD or [BREAD](http:\/\/paul-m-jones.com\/post\/2008\/08\/20\/bread-not-crud\/) based and therefore match the three named flow components.\\nFor now, emitting and receiving entities can be also used differently as this \"just\" sends data from one portal node and is received by another portal node.\\nEmitters and receivers could send commands instead of entities.\\nAs previously mentioned we do not have a deletion flow component.\\nA receiver could receive an entity with a custom deletion command with any previous version of HEPTAconnect.\\nThis is discouraged but possible.\\nWe have already seen implementations, that receive data but don't write anything to the API the portal resembles.\\nThis is a misuse that is similar to described scenario above.\\nLooking at the other existing flow components we also have webhooks and status reporters.\\nThese are not related to CRUD at all, so we are not limited to CRUD.\\n","completion":"## Decision\nReceivers are not meant to do everything, when it is about receiving a command.\\nReceivers are meant to be used for entities only.\\nGrouping explorers, emitters, receivers and \"deleters\" into a single CRUD flow component enforces structures that probably don't benefit APIs, that do not fall into this pattern.\\nGrouping flow components is not helpful when we do not know the possible groups in beforehand and therefore can't be done right.\\nEvery other transfer needs a new flow component.\\nAs routes connect emitters and receivers they need to learn how to decide which flow components to use on a route.\\nThis is described in a [different ADR](.\/2021-10-30-route-capabilities.md).\\n"}
{"prompt":"## Problem Statement\\n### Why Distributed Tracing?\\n> Observability is defined as the ability of the internal states of a system to\\nbe determined by its external outputs.\\nThere are several established techniques for gaining observability into complex\\nsoftware systems.\\n* Logging\\n* helpful for known knowns\\n* e.g. \"I want to know when a specific condition is reached\"\\n* Metrics\\n* helpful for assessing known unknowns\\n* e.g. \"How many requests per second was the system handling last Tuesday?\"\\n* Distributed Tracing\\n* helpful for learning about unknown unknowns\\n* e.g. \"What was the execution context for User X that caused their\\ninteraction to timeout last Tuesday?\"\\nSome of the benefits of distributed tracing, as outlined in\\n[this](https:\/\/petabridge.com\/blog\/why-use-distributed-tracing\/) article are:\\n* radically improves developer productivity and output\\n* works across multiple applications, programming languages, and transports\\n* improve time to market\\n* facilitates excellent cross-team communication and cooperation\\nHere are several example scenarios or questions that distributed tracing can\\nhelp answer.\\n* As a new engineer on the team, I want to understand how many separate systems\\nare involved when a certain user type logs in and the first page is rendered.\\n* As an operations engineer, I want to know how many SQL queries are executed\\nfor a given endpoint or interaction.\\n* As a product manager, I want to know if a new feature is being used by a\\ncertain cohort of users on a regular basis.\\n* As an engineer, I want to prove that an optimization I wrote is effective\\nin a production environment.\\n* As a load tester, after I have shown that a problem exists, I want to\\nunderstand how the system is interacting so I can debug and fix the issue.\\n### ADR Goals and Anti-goals\\n* Goal: Choose which set of libraries to use at callsites (across programming\\nlanguages) within the MilMove codebase, which will be used to generate\\ndistributed tracing data\\n* Anti-goal: Committing to a specific \"backend\", i.e. platform or service for\\ngathering, exploring, and displaying trace information\\n* Sub-goal: Leaving open as many options as possible for the backend\\n","completion":"## Decision\n* Chosen Alternative: _Use OpenTelemetry_\\n* OpenTelemetry is an emerging industry standard\\n* vendors find benefit of being in the OpenTelemetry ecosystem because they\\nno longer have to create or support instrumentation libraries in an ever\\ngrowing array of languages, i.e. as soon as language library exists for\\nOpenTelemetry, the vendors automatically become available to support that\\ngiven language.\\n* OpenTelemetry is vendor agnostic\\n* tracing information can be sent to hosted services (e.g. Honeycomb.io, AWS\\nX-Ray, etc) or self-hosted Open Source implementations (e.g. Zipkin, Jaeger,\\netc)\\n* if left unconfigured, OpenTelemetry instrumentation calls default to\\nlightweight\/noop executions\\n* OpenTelemetry has well-maintained libraries for the languages used in the\\nlayers of the MilMove project\\n* i.e. Go (back-end); JavaScript (front-end); Python (load testing); etc\\n* Easily swappable back-ends\\n* e.g. could choose a local Docker version of OpenZipkin for an all-local\\ndevelopment environment\\n* e.g. can use Honeycomb.io in the experimental commercial-cloud hosted\\nenvironment\\n* e.g. can swap in AWS X-Ray for use in GovCloud hosted environments\\n* Cons\\n* as an abstraction layer, OpenTelemetry may prohibit usage of vendor-\\nspecific capabilities\\n* some OpenTelemetry libraries and tools may trail their vendor-supported\\ncounterparts\\n* instrumentation for tracing may be a vector for performance overhead\\n"}
{"prompt":"## Context\\nThis is a fast growing system and justin-time scaling is essential. Also, it allows buying cheaper instances for services without overall degradation.\\nProper and trustworthy monitoring is the basis for making any decision about infrastructure changes (i.e. scaling and extracting services), proper monitoring should support business decisions and should act architecture fitness functions.\\nWe'd like to use a system that helps us get the full picture of correlated business events, connected services and amount of consumed resources.\\n","completion":"## Decision\nEven though ELK stack or Graphana are the cheapest options for FoodFarmacy to go with, we decided to choose DataDog, because it requires minimal integration without maintenance (contrary to ELK Stack or Graphana). The convenience of an all-in-one alerting and monitoring tool for $15 USD per month is just too difficult to argue with. Finally, choosing an open-source solution we would have to appoint 0.2-0.5 time of a developer \/ administrator to take care of the monitoring.\\n"}
{"prompt":"## Context\\nBecause we are building an [event-driven framework], we need to think about how to handle callback functions. We want to use them without causing threads\/functions to block unexpectedly while executing callbacks.\\n","completion":"## Decision\nWe will dispatch callbacks to a global dispatch queue which will execute them as processing time is available.\\n"}
{"prompt":"## Context\\nAs we rely more heavily on generated schemas for validation, undoing\\ndestructive actions like deciding or creating a previously-retired split\\ncould result in deviation between the schema and production, meaning\\nlocal\/prod disparities in the form of missing variants in the schema\\nfile.\\nWhile this is not ideal, it's not the end of the world. We'd like to\\nlimit the impact of this scenario by merging in information from prior\\nmigrations if available.\\nThis stance attempts to balance the desire for correctness with the\\nability to have schemas that don't grow indefinitely (by deleting retired\\nsplits), as well as migration files that can be culled after they reach\\na certain age to cut down on cognitive overhead for app maintainers.\\n","completion":"## Decision\nWhen reviving a retired split, the variants of the last creation of that\\nsplit will be merged into the set of variants reflected in the new\\nschema.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nThis application need a user authentication and a fine granularity for roles.\\n","completion":"## Decision\nI decided to choose IdentityServer 4 because it is a mature, use by many people and well documented project.\\nIt implement OpenID connect and support external authentication (Google, Facebook, ...)\\n"}
{"prompt":"## Context and Problem Statement\\nTo deliver BFI's IIIF Universal Viewer auditing platform, custom\\ndeliverables must be produced to support the serving of the Universal\\nViewer, and an underlying API which records audit events and persists\\nthem into a database.\\nThis ADR provides options on technologies and frameworks which can be\\nused to produce these deliverables.\\n## Decision Drivers\\n* Ability for Digirati to rapidly deliver the solution.\\n* Long-term support and stability of the technology and framework\\nchoices.\\n* Ease of deployment across different environments \/ operating systems.\\n* Robustness and performance of the solution.\\n* Any technology and framework should be open source.\\n","completion":"## Decision\n* Ability for Digirati to rapidly deliver the solution.\\n* Long-term support and stability of the technology and framework\\nchoices.\\n* Ease of deployment across different environments \/ operating systems.\\n* Robustness and performance of the solution.\\n* Any technology and framework should be open source.\\nKotlin is selected as the solution owing to Digirati's vast experience\\nwith the JVM, and when combined with Spring will make this the\\ndeliverables straightforward.\\nKotlin does not have the boilerplate cost associated with Java, and as a\\nresult internal there should be fewer barriers in identifying and\\nunderstanding the domain specific code required for the deliverables.\\nIn addition, since Spring handles significant amounts of the heavy\\nlifting, we do not expect to consume time writing code that is not\\nconcerned entirely with handling these audit events.\\n### Positive Consequences\\n* Faster development cycle for the parts of the auditing platform which\\nrequire custom code.\\n* Spring can provide both the Universal Viewer assets and the API,\\nunifying everything under one service and keeping authentication\\nsimple\\n* The deployment is straightforward, whether using a containerisation or\\ndeploying directly onto a bare metal server.\\n### Negative Consequences\\n* Lack of JVM \/ Kotlin experience within internal BFI teams.\\n"}
{"prompt":"## Context\\nTo ensure a clear separation as regards what logic resides within CDH and ETL related services where there exists feed specific\\nrequirements during onboarding and processing of feed files, the following issues where considered:\\n- Will such logic require specialized implementation across feeds?\\n- Are such logic generic and require one time implementation or will require continous change\/update?\\n- What are the benefits of moving such logic into ETL instead of CDH.\\n","completion":"## Decision\nThe most important point agreed on was that CDH will remain focused on defined object and data models as possible and CDH will\\nrun with the expectation that all inputs received are completed. This means CDH should not have domain specific knowledge in regards\\nspecific intricacies about how specific feeds are reconcilied into complete Positions, Transactions and Ticker data.\\nSuch specificity will reside within the ETL service and be housed based on each feed parsing logic within the ETL service.\\nThe benefits of such a system is that only ETL needs to change to accomodate new and changing requirements of old and new feeds\\nensuring the final result is always consistent to march what the CDH service requires.\\n"}
{"prompt":"## Context\\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer\/simpler riglet flavor\\nand put newer approaches to the test.\\n","completion":"## Decision\n* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n"}
{"prompt":"## Context\\nAs described in [ADR #12](0012-matlab-python-wrapper.md),\\nPACE aims to provide both a Python and a Matlab user interface (UI) for all its constituent programs.\\nIn addition, these programs should be able to interact with each other regardless of the user's environment.\\nThat ADR decided that the Matlab UI for Brille and Euphonic (which currently only have Python UIs)\\nwould use an interface library called [`light_python_wrapper`](https:\/\/github.com\/pace-neutrons\/light_python_wrapper).\\nA meeting between the developers of these programs on Jan 7 2021 also agreed the implementation details given here.\\nIn addition, the meeting also decided that any computation or logic required for interfacing Brille\/Euphonic\\nwith other PACE programs should be written in *Python* so as to be accessible to both the Matlab and Python UI\\nwithout the need for compiled Matlab.\\n","completion":"## Decision\n* Any computation required for use with Horace or other Matlab programs would be implemented in *Python*.\\n* This Python code would be distributed as a separate module, to be installable using `pip`.\\n* The Matlab UI itself would be a pure interface layer (very little computation)\\nand be provided mainly by `light_python_wrapper`.\\n* `light_python_wrapper` will be moved to a separate repository, and used as a submodule in\\n[`horace_euphonic_interface`](https:\/\/github.com\/pace-neutrons\/horace-euphonic-interface)\\nand [`brillem`](https:\/\/github.com\/brille\/brillem).\\n* The continuous integration (CI) system would be updated to pull in the submodule for tests\\n* The CI system would also be used to build a [\"Matlab Toolbox\"](https:\/\/uk.mathworks.com\/help\/matlab\/creating-help.html)\\nfor distribution which would include `light_python_wrapper` (so users do not have to install it separately).\\n* Using this Matlab Toolbox `mltbx` file and uploading it to the github release allows it to be\\n[automatically published](https:\/\/www.mathworks.com\/matlabcentral\/about\/fx\/#Why_GitHub) by the File Exchange.\\n"}
{"prompt":"## Context\\n### Definitions\\n* **Function**: A JavaScript function\\n* **Service Function**: A complete function that takes messages from a\\nmessaging service\\n* **Topic**: A name within a messaging service that any service can listen to\\n* **Message**: A message that is distributed to topic listeners\\n* **Command**: A message indicating a user action\\nThe first task implementation required a service function that takes a command, transforms the data, and then writes that data to Firebase. In order to make\\nsure that other tasks know about the data change the change then needs to be\\nbroadcast on to another topic.\\nThe above means that the function has two responsibilities, writing the data\\nand broadcasting a new message. Writing the tests for such a service function\\nshowed that mocking two targets is hard and causes complexity.\\n","completion":"## Decision\nOption 2, use 2 services. We will name these *business* services and *repository*\\nservices.\\n"}
{"prompt":"## Context\\nSupport for ingestion of JPEG2000 images was added to accommodate\\nscanned map records for which we cannot find original TIFFs. The VIPS library,\\nused to generate pyramidal tiffs and thumbnails, can't read JP2s\\ndirectly, so a temporary intermediate TIFF was generated using the OpenJPEG `opj_decompress`\\ncommand. This command uses a large amount of memory and will eventually\\ncrash the host server if several JP2s are decompressed simultaneously.\\n","completion":"## Decision\n1. Derivative generation functionality for JPEG2000 images was removed.\\n"}
{"prompt":"## Context\\n[PRD](https:\/\/docs.google.com\/document\/d\/1Cflct0Y-44IRUVw_5-NptcnNSX1UgAPBiXqoXHq22io\/edit)\\nUsers in large districts reported that the Simple app was running very slow, making the app near-unusable.\\nThe slowdown was caused by the volume of patient data synced to the user\u2019s phone. We realised that the amount of data\\nbeing stored on the device had to be reduced for better long-term performance.\\nCurrently we sync the entire district's records to a user's phone. Some of the large districts have upto 50,000 patients,\\nwhich can amount to 400-500 MB of data. On lower-end phones we noticed the app started slowing down when the DB size grew beyond 250 MB.\\nA district typically has between 1-20 blocks. From trends in IHCI, we found it's uncommon for patients to visit facilities across blocks.\\nPatients that have a BP taken in more than 1 block is around 2%, with the exceptions of:\\nSindhudurg (9.8%), Hoshiarpur (5.3%), Bathinda (3.1%).\\nThis means that we can sync only a block's data to the user's phone and be reasonably confident about finding patients on the app.\\n","completion":"## Decision\n- The server will sync records from the user's block instead of the entire district.\\nSpecifically the following patients will be synced:\\n- patients that registered at a facility in the same block,\\n- patients that are assigned to a facility in the same block, and\\n- patients that have an appointment scheduled at a facility in the same block.\\n- All other sync resources will return records belonging to these patients only.\\n- The sync mechanism should support the ability to adjust the sync radius to any sync region.\\nThis is important in case we need to change the kind of records that are synced to the app in the future.\\nSee the [wiki entry on Region level sync](..\/wiki\/adjusting-sync-boundaries.md) for how it works.\\n### On the app\\n- Users can continue selecting any facility in their district when switching facilities.\\n- Users can continue selecting any facility in their district when scheduling a patient\u2019s next visit or preferred facility.\\n- It is possible that a patient will visit a facility outside their block and their record will not be found on the user\u2019s device. In this case the user should\\n- Scan the patient\u2019s BP passport if they have one.\\n- Register the patient again, as if they were new. Make sure to attach their existing BP passport to the registration.\\n- The duplicate patient records will be merged by the Simple team later.\\n"}
{"prompt":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n","completion":"## Decision\nWe will use Dependabot to monitor dependency updates.\\n"}
{"prompt":"## Context\\n*This section describes the forces at play, including technological, political, social, and project local.\\nThese forces are probably in tension, and should be called out as such. The language in this section is value-neutral.\\nIt is simply describing facts.*\\nShould the application be implimented as a monolith or microservices.  While a monolith approach would work for this simple\\napplication, CAH builds sophisticated, complex applications composed of multiple logical components.\\n","completion":"## Decision\n*This section describes our response to these forces. It is stated in full sentences, with active voice.\\n\"We will ...\"*\\nGiven these facts, a multi-repository microservices approach will be used\\nSCM Repos:\\n- UI\\n- Microservice #1\\n- Microservice #2\\n- Microservice #3\\n"}
{"prompt":"## Context\\nThe rig defined at [Bookit Infrastructure](https:\/\/github.com\/buildit\/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\\nThe only way to capture that is via branches which can be hard to discover.\\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\\n","completion":"## Decision\nCreate a digitalrig-metal-aws repo (https:\/\/github.com\/buildit\/digitalrig-metal-aws) that demonstrates all of the options and latest features of the AWS Bare Metal Rig and removes any Bookit specific wording\/concepts.\\n"}
{"prompt":"## Context\\nThe normal behavior of a module is to be self-contained, meaning that its code and all its dependencies are stored in the module's directory, including `vendor` directory and autoloader.\\nWhen developing and building PrestaShop, native modules aren't downloaded from marketplace, they are required using composer.\\nIn difference with the normal behavior described above, a module installed using composer will have its autoloader and dependencies merged into the core's and placed in the core's `vendor` directory.\\nThis can prove problematic for native modules:\\n- When the module is uninstalled, both its dependencies and autoloader are left behind in the core.\\n- If the module is upgraded using the marketplace sources, the dependencies are now available twice: once in the core and once in the module.\\nAlternate systems to avoid having modules leak code into the core have been proved unpractical:\\n- Including modules via [Composer script handler](https:\/\/github.com\/PrestaShop\/composer-script-handler) or git clone is too slow.\\n- Using submodules would be a step backwards.\\n- Retrieving modules from the marketplace would be slow as well.\\n","completion":"## Decision\n1. [The module managment system must be changed to be fully based on composer](https:\/\/github.com\/PrestaShop\/PrestaShop\/issues\/12586). This will require major changes in the marketplace and will have to be analyzed for feasibility.\\n2. In the meantime, we will keep using composer to include native modules.\\n3. Native modules must prepend their autoloader.\\n4. To avoid leaving dependencies in the core, no composer dependencies are to be added to native modules until step 1 has been resolved.\\n- Because of Decision No. 4, we may face technical difficulties in the future.\\n"}
{"prompt":"## Context\\nThe goal of this project is to build out a small, demonstration CQRS and Event Sourced application.\\nTest Driven Development is an excellent way to keep the development cycle short and on track whilst naturally keeping test coverage high.\\nThis project is also a learning tool that is helping me get familiar with the Go programming language.\\n","completion":"## Decision\nAllow the tests to drive the development of this application.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context\\nThe choice of using spring-boot is not related to any specific purpose. It just happened to be lying around.\\n","completion":"## Decision\nOpenCHS server will use Spring boot\\n"}
{"prompt":"## Context\\nWe feel the need to define that every config folder should contain at least one dataflow diagram.\\n","completion":"## Decision\nWe decided that every config folder should contain at least one dataflow diagram.\\n### Dataflow Diagram\\nEvery config with a cloudbuild should contain a dataflow diagram. This diagram contains the dataflow of the GCP project the config is for. It is also possible to have multiple diagrams if there are multiple dataflows. [Here](https:\/\/github.com\/vwt-digital-config\/snb-link2-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) and [here](https:\/\/github.com\/vwt-digital-config\/snb-tmob-problm-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) examples of dataflow diagrams can be found.\\nThe diagrams are made via the python package [diagrams](https:\/\/github.com\/mingrammer\/diagrams).\\n"}
{"prompt":"## Context\\nIn order to be more community-oriented, we should adopt a process to have a structured way to have open architectural decisions.\\nUsing an Architectural Decision Records-based process as a support of discussion on the developers mailing-lists.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](https:\/\/web.archive.org\/web\/20190824074401\/http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nEach ADR will be discussed on the Apache James' developers mailing-list before being accepted.\\nFollowing [Apache Decision Making process](https:\/\/community.apache.org\/committers\/decisionMaking.html), we provide the following possible status, with their associated meaning:\\n- `Proposed`: The decision is being discussed on the mailing list.\\n- `Accepted (lazy consensus)` : the architecture decision was proposed on the mailing list, and a consensus emerged from people involved in the discussion on the mailing list.\\n- `Accepted (voted)` : the architecture undergo a voting process.\\n- `Rejected` : Consensus built up against that proposal.\\n"}
{"prompt":"## Context\\nThe SAML profile used by the eIDAS scheme requires that SAML messages are sent directly to the receiving proxy node via a user's browser. This introduces a host of security concerns, especially when we take the history of SAML exploits and XML security in general into account. Due to many of the attacks being centred around breaking XML parsing, the main concerns raised were of SAML being parsed by a public-facing service or the service responsible for communicating with the hardware security module (HSM). This suggests we can't implement the proxy node with a monolithic architecture.\\n","completion":"## Decision\nThe proxy node is going to be split out into a series of microservices, each with a very limited set of responsibililties. Kubernetes ensures that sensitive microservices do not run on the same node. An attacker is limited in the damage they can do by compromising a single service and should be unable to compromise a vital service using maliciously constructed XML.\\n### Gateway\\nPublic internet-facing service hosting the SAML endpoints and responsible for managing the user session. No parsing of SAML is carried out by this service.\\n### eIDAS SAML Parser (ESP)\\nA small service solely responsible for parsing eIDAS SAML AuthnRequests and returning useful information in JSON format to the gateway for session management purposes. The ESP is also responsible for consuming and validating the requesting member state's connector node metadata and passing the encryption certificate back for use by the translator (via the user's session).\\n### Verify Service Provider (VSP)\\nAn application provided by Verify to build Verify SAML AuthnRequests and parse SAML Responses from the Verify Hub to return a user's attributes in JSON format.\\n### Translator\\nConstructs the eIDAS SAML response destined for the requesting member state's connector node. The message is signed using the HSM and encrypted for the connector.\\n### Message flow\\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u250c\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2510                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                             \u250c\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2510\\n\u2502Connector\u2502                         \u2502Gateway\u2502                \u2502ESP\u2502          \u2502VSP\u2502                 \u2502Translator\u2502                             \u2502Hub\u2502          \u2502Logs\u2502\\n\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518                         \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518                \u2514\u2500\u252c\u2500\u2518          \u2514\u2500\u252c\u2500\u2518                 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                             \u2514\u2500\u252c\u2500\u2518          \u2514\u2500\u252c\u2500\u2500\u2518\\n\u2502 1 [Browser] SAML(eIDAS Request)  \u2502                      \u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502                      \u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u25022 JSON(eIDAS Request) \u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502 3 JSON(useful info)  \u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502<\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502                 4    \u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502       5 JSON(Verify Request)        \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502<\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502                      \u2502          6 [Browser] SAML(Verify Request)                                   \u2502              \u2502\\n\u2502                                  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502                                     \u2502 7 <user hash>\u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502                                     \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502                      \u2502          8 [Browser] SAML(Verify Response)                                  \u2502              \u2502\\n\u2502                                  \u2502<\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502                   9 JSON(Verify Response)                    \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>                                     \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u250210 JSON(Verify Response)\u2502                                     \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502<\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                                     \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502   11 JSON(user data)   \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>                                     \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502                   12 <user hash>    \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502 13 <sign Response> \u250c\u2500\u2500\u2500\u2510            \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502 <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502HSM\u2502            \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502                    \u2514\u2500\u252c\u2500\u2518            \u2502              \u2502\\n\u2502                                  \u2502                   14 JSON(eIDAS Response)                    \u2502                                     \u2502              \u2502\\n\u2502                                  \u2502<\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                      \u2502              \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502                      \u2502              \u2502              \u2502\\n\u250215 [Browser] SAML(eIDAS Response) \u2502                      \u2502              \u2502                        \u2502                      \u2502              \u2502              \u2502\\n\u2502<\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502                      \u2502              \u2502                        \u2502                      \u2502              \u2502              \u2502\\n\u2502                                  \u2502                      \u2502              \u2502                        \u2502                                     \u2502              \u2502\\n"}
{"prompt":"## Context\\nA clean architecture depends inwards, putting I\/O concerns at its boundary\/adapter layer. DynamoDB has an object model that allows us to mark up an entity with persistence information.\\nBy adding this persistence information to the entity, in the form of attributes, we couple the application to a persistence concern--how do we persist these entities to DynamoDB, which would be redundant if we changed how we stored the entity.\\nIn addition, we have to add converter classes to the Application layer, as the attribute markup needs to know how to persist a 'custom' field type, which contains information on DynamoDB stores values.\\nThe alternative is to use a DTO, marked up with DynamoDB attributes in the boundary\/adapter layer, and then use automapper in the ports\/interactors layer to map to and from DTOs.\\n","completion":"## Decision\nAllow markup on the entity and put converters in the application layer. As we 'own' our own tables in Dynamo the risk of coupling to them is low, and the use of attributes is orthogonal to the code, allowing them to be easily removed if we were to change DB. This is tradeed-off against the cost of having to implement the DTO and autommapper.\\n"}
{"prompt":"## Context\\nSome Content Items are written in different languages, so [Publishing-api][1] will return the `content_id` along with all locales assigned to the Content Item.\\n","completion":"## Decision\nFocus on content written in English.\\nThe main reason is that we would need different algorithms and libraries to make our application consistent among all the languages \/ locales.\\nIf this is a real need, we will support it in future iterations of the Data Warehouse.\\n### Benefits:\\nThis makes the codebase simpler.\\n"}
{"prompt":"## Context\\nThis system needs storage in the data store for every VPN it deals with.\\nSince it shall be used to analyse IPsec tunnels it could create the needed storage on the fly when interesting traffic reaches the system.\\nThis would make it easier to get started with a new VPN but it would make the system vulnerable to resource exhaustion.\\nAdditionally it could be triggered with spoofed source addresses to send traffic to arbitrary systems.\\n","completion":"## Decision\nFor every VPN gateway that this system shall interact with, at least the storage in the data store has to be initialized.\\n"}
{"prompt":"## Context\\nIn the request builder you can currently call methods like `filter` and `top` multiple times.\\nIn this document we discuss possibilities to unify this the behavior when this is done.\\n","completion":"## Decision\nWe decided to use option `A` and `B` depending on the methods.\\nFor the OData related options we will use option `A`.\\nFor configuration related builder methods we will use the verbs `add` or `set` to make clear if it is option `A` or `B`.\\nThe relevant builder methods (the one containing arguments) are listed in the table below including the decision on the option:\\n| class                           | methods                                                                                                                                                                              |\\n| ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| request-builder-base            | withCustomHeader ( refactor to addCustomHeaders B)<br>withCustomQueryParameters (refactor to addCustomQueryParameters B)<br>withCustomServicePath (A rename to setCustomServicePath) |\\n| get-all-request-builder-base    | select (A) <br>orderBy (A) <br>top (A) <br>skip (A)                                                                                                                                  |\\n| get-all-request-builder-v2\/v4   | filter (A)<br>expand(v4)                                                                                                                                                             |\\n| get-by-key-request-builder-base | select (A refactoring move to base)                                                                                                                                                  |\\n| get-by-key-request-builder-v4   | expand(A)                                                                                                                                                                            |\\n| delete-request-builder-v2\/v4    | setVersionIdentifier (A)                                                                                                                                                             |\\n| update-request-builder-base     | requiredFields (refactor setRequiredFields A)<br>ignoredFields (refactor setRequiredFields A)<br>withCustomVersionIdentifier (refactor align with setVersionIdentifier A)            |\\n"}
{"prompt":"## Context\\n","completion":"## Decision\nTBD\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context and Problem Statement\\nIn many use cases, we have a lot of data that needs to be accesed on almost a project-wide (or module-wide) scope.\\nThis need brought us to define a standarized approach for data storage.\\nThis approach should cover the following use cases:\\n* Maintenance ease: new fields should be easy to add, modify and retrieve.\\n* Need to know when specific data points change to react accordingly.\\n* Cyclic assembly references prevention. Data as a leaf node of the dependency graph.\\n* Easy to mock.\\nIn the current state, this kind of necessity is covered by a few static classes wrapped around Unity `ScriptableObjects`.\\nEach of field of the static class is a variant of a custom `BaseVariable` type.\\nThe `BaseVariable` wraps around a value and gives us `OnChange` events.\\n```csharp\\npublic static class CommonScriptableObjects\\n{\\nprivate static Vector3Variable playerUnityPositionValue;\\npublic static Vector3Variable playerUnityPosition => GetOrLoad(ref playerUnityPositionValue, \"ScriptableObjects\/PlayerUnityPosition\");\\nprivate static Vector3Variable playerWorldPositionValue;\\npublic static Vector3Variable playerWorldPosition => GetOrLoad(ref playerWorldPositionValue, \"ScriptableObjects\/PlayerWorldPosition\");\\nprivate static Vector3Variable playerUnityEulerAnglesValue;\\npublic static Vector3Variable playerUnityEulerAngles => GetOrLoad(ref playerUnityEulerAnglesValue, \"ScriptableObjects\/PlayerUnityEulerAngles\");\\nprivate static Vector3Variable playerUnityToWorldOffsetValue;\\n...\\n}\\n```\\nUsing `ScriptableObject` instances like this has the following boons:\\n- As they are serialized assets, they can be referenced from scenes, another assets, etc.\\n- We can look up their value from Unity editor to debug.\\n- When working with artists and designers, we can give them a tool to parametrize certain values.\\nHowever, we are looking at these downsides too:\\n- Serialized assets means that each field is going to take space on our Unity bundle. Small space, but still.\\n- Having hardcoded paths on the static class. Not very maintainable as moving assets around can break the code.\\n- Having to wrap every access in a `GetOrLoad` call that internally uses `Resources.Load`. This has a bit of overhead that we could prevent.\\n","completion":"## Decision\nWe choose to be boundary agnostic because this will give us the versatility of using the best approach for any situation. We discussed that in case of going for POCO and needing an SO later, the need refactoring work would be very low.\\nBy choosing to deliberately use a data store pattern, we can name it accordingly and define the system responsibilities in a more direct way than we have now.\\nAlso, we have the added benefit of a more cohesive design between Kernel and Unity, because Kernel already has the redux data store concept.\\n"}
{"prompt":"## Context\\nE2E tests ensure the functional correctness in an e2e environment in order to make sure Dapr works with the user code deployments. The tests will be run before \/ after PR is merged or by a scheduler.\\nDapr E2E tests require the test infrastructure in order to not only test Dapr functionalities, but also show these test results in a consistent way. This document will decide how to bring up the test cluster, run the test, and report the test results.\\n","completion":"## Decision\n### Test environments\\nAlthough Dapr is designed for multi cloud environments, e2e tests will be run under Kubernetes environments for now. We will support two different options to run e2e tests with local machine and CI on the pre-built Kubernetes cluster.\\n* **Local machine**. contributors or developers will use [Minikube](https:\/\/github.com\/kubernetes\/minikube) to validate their changes and run new tests before creating Pull Request.\\n* **Continuous Integration**. E2E tests will be run in the pre-built [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) before\/after PR is merged or by a scheduler. Even if we will use [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) in our test infrastructure, contributors should run e2e tests in any  RBAC-enabled Kubernetes clusters.\\n### Bring up test cluster\\nWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3rd party test frameworks, such as ginkgo, gomega.\\n### CI\/CD and test result report for tests\\nMany Kubernetes-related projects use [Prow](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/prow), and [Testgrid](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/testgrid) for Test CI, PR, and test result management. However, we will not use them to run Dapr E2E tests and share the test result since we need to self-host them on Google cloud platform.\\nInstead, Dapr will use [Azure Pipeline](https:\/\/azure.microsoft.com\/en-us\/services\/devops\/pipelines\/) to run e2e tests and its [test report feature](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/test\/review-continuous-test-results-after-build?view=azure-devops) without self-hosted CI and test report services. Even contributors can get their own azure pipelines accounts **for free** without self-hosting them.\\n"}
{"prompt":"## Context\\nTo make it easy for users to interact with our API we need a frontend that can be used by modern devices.\\n","completion":"## Decision\nThe frontend will be build with [React](https:\/\/reactjs.org\/) and [Snowpack](https:\/\/www.snowpack.dev) will be used as build tool.\\nThe application will be styled with [Tailwind CSS](https:\/\/tailwindcss.com)\\nWe also considered the following alternative solutions:\\n* Vanilla HTML would work for the scope of this application.\\n* Vanilla CSS would work, but take more time to generate nice layouts.\\n"}
{"prompt":"## Context\\nIn the light client (or any other client), the user may want to **subscribe to\\na subset of transactions** (rather than all of them) using `\/subscribe?event=X`. For\\nexample, I want to subscribe for all transactions associated with a particular\\naccount. Same for fetching. The user may want to **fetch transactions based on\\nsome filter** (rather than fetching all the blocks). For example, I want to get\\nall transactions for a particular account in the last two weeks (`tx's block time >= '2017-06-05'`).\\nNow you can't even subscribe to \"all txs\" in Tendermint.\\nThe goal is a simple and easy to use API for doing that.\\n![Tx Send Flow Diagram](img\/tags1.png)\\n","completion":"## Decision\nABCI app return tags with a `DeliverTx` response inside the `data` field (_for\\nnow, later we may create a separate field_). Tags is a list of key-value pairs,\\nprotobuf encoded.\\nExample data:\\n```json\\n{\\n\"abci.account.name\": \"Igor\",\\n\"abci.account.address\": \"0xdeadbeef\",\\n\"tx.gas\": 7\\n}\\n```\\n### Subscribing for transactions events\\nIf the user wants to receive only a subset of transactions, ABCI-app must\\nreturn a list of tags with a `DeliverTx` response. These tags will be parsed and\\nmatched with the current queries (subscribers). If the query matches the tags,\\nsubscriber will get the transaction event.\\n```\\n\/subscribe?query=\"tm.event = Tx AND tx.hash = AB0023433CF0334223212243BDD AND abci.account.invoice.number = 22\"\\n```\\nA new package must be developed to replace the current `events` package. It\\nwill allow clients to subscribe to a different types of events in the future:\\n```\\n\/subscribe?query=\"abci.account.invoice.number = 22\"\\n\/subscribe?query=\"abci.account.invoice.owner CONTAINS Igor\"\\n```\\n### Fetching transactions\\nThis is a bit tricky because a) we want to support a number of indexers, all of\\nwhich have a different API b) we don't know whenever tags will be sufficient\\nfor the most apps (I guess we'll see).\\n```\\n\/txs\/search?query=\"tx.hash = AB0023433CF0334223212243BDD AND abci.account.owner CONTAINS Igor\"\\n\/txs\/search?query=\"abci.account.owner = Igor\"\\n```\\nFor historic queries we will need a indexing storage (Postgres, SQLite, ...).\\n### Issues\\n- https:\/\/github.com\/tendermint\/tendermint\/issues\/376\\n- https:\/\/github.com\/tendermint\/tendermint\/issues\/287\\n- https:\/\/github.com\/tendermint\/tendermint\/issues\/525 (related)\\n"}
{"prompt":"## Context\\nWe need to create a riglet for our new twig project so that we practice what we preach.\\n","completion":"## Decision\nWe will use the AWS Bare Metal Riglet from bookit-infrastructure as a starting point for our riglet.  We will keep the previous twig-riglet and create a new twig-infrastructure project\/repo.\\nTechnologies:\\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\\n* Deployment Mechanism: Docker images\\n* Build: CodePipeline, with Jenkins as an eventual target\\n"}
{"prompt":"## Context\\nResearchers may require methods to search for a particular Whale Observation or set of Whale Observations during their\\nstudies. For the program to accomplish this task two search methods may require implementation.\\n","completion":"## Decision\nCreate `getByDate` and `getById` methods in ObservationRepository.\\n`getByDate` uses the Observation iterator to compare Observation objects `sightingTime` (Date) to the date passed to the\\nmethod and creates an array list of matching objects. This function uses an iterator rather than `Collections.binarySearch()`\\nto reduce the runtime given that there may exist multiple objects of a similar date.\\n`getById` uses `Collections.binarySearch()` to search and possibly return the object with the `ObservationId` matching\\nthe long passed to the method.\\nCollection.sort()` used in `getById` implements the abstract strategy of the `Comparator` interface using the concrete\\nstrategy `compareByDate` and returns an integer referring to the order of the objects.\\n"}
{"prompt":"## Context\\nMessages are denormalized in Cassandra in order to:\\n- access them by their unique identifier (messageId), for example through the JMAP protocol\\n- access them by their mailbox identifier and Unique IDentifier within that mailbox (mailboxId + uid), for example\\nthrough the IMAP protocol\\nHere is the table organisation:\\n- `messageIdTable` Holds mailbox and flags for each message, lookup by mailbox ID + UID\\n- `imapUidTable` Holds mailbox and flags for each message, lookup by message ID\\nFailures during the denormalization process will lead to inconsistencies between the two tables.\\nThis can lead to the following user experience:\\n```\\nBOB receives a message\\nThe denormalization process fails\\nBOB can read the message via JMAP\\nBOB cannot read the message via IMAP\\nBOB marks a message as SEEN\\nThe denormalization process fails\\nThe message is SEEN in JMAP\\nThe message is UNSEEN in IMAP\\n```\\n### Current operations\\n- Adding a message:\\n- (CassandraMessageMapper) First reference the message in `messageIdTable` then in `imapUidTable`.\\n- (CassandraMessageIdMapper) First reference the message in `imapUidTable` then in `messageIdTable`.\\n- Deleting a message:\\n- (CassandraMessageMapper) First delete the message in `imapUidTable` then in `messageIdTable`.\\n- (CassandraMessageIdMapper) Read the message metadata using `imapUidTable`, then first delete the message in\\n`imapUidTable` then in `messageIdTable`.\\n- Copying a message:\\n- (CassandraMessageMapper) Read the message first, then first reference the message in `messageIdTable` then\\nin `imapUidTable`.\\n- Moving a message:\\n- (CassandraMessageMapper) Logically copy then delete. A failure in the chain migh lead to duplicated message (present\\nin both source and destination mailbox) as well as different view in IMAP\/JMAP.\\n- (CassandraMessageIdMapper) First reference the message in `imapUidTable` then in `messageIdTable`.\\n- Updating a message flags:\\n- (CassandraMessageMapper) First update conditionally the message in `imapUidTable` then in `messageIdTable`.\\n- (CassandraMessageIdMapper) First update conditionally the message in `imapUidTable` then in `messageIdTable`.\\n","completion":"## Decision\nAdopt `imapUidTable` as a source of truth. Because `messageId` allows tracking changes to messages accross mailboxes\\nupon copy and moves. Furthermore, that is the table on which conditional flags updates are performed.\\nAll writes will be performed to `imapUidTable` then performed on `messageIdTable` if successful.\\nWe thus need to modify CassandraMessageMapper 'add' + 'copy' to first write to the source of truth (`imapUidTable`)\\nWe can adopt a retry policy of the `messageIdTable` projection update as a mitigation strategy.\\nUsing `imapUidTable` table as a source of truth, we can rebuild the `messageIdTable` projection:\\n- Iterating `imapUidTable` entries, we can rewrite entries in `messageIdTable`\\n- Iterating `messageIdTable` we can remove entries not referenced in `imapUidTable`\\n- Adding a delay and a re-check before the actual fix can decrease the occurrence of concurrency issues\\nWe will expose a webAdmin task for doing this.\\n"}
{"prompt":"## Context\\nThe welcome and the sign in journey currently bounce our user to pages controlled by Auth0. Auth0 give us 2 options called 'experiences' in the admin area under 'Universal Login':\\n1. Classic - requires Javascript to be enabled in the browser\\n2. New - no Javascript required\\nhttps:\/\/auth0.com\/docs\/universal-login\/new\\nThe [service standard does not mention failing or passing an assessment if the service doesn't work without JS](https:\/\/www.gov.uk\/service-manual\/service-assessments\/pre-july-2019-digital-service-standard) the guidance I can find is from the [service manual which advises that we should use progressive enhancement](https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancement) to ensure that when a user doesn't have JS enabled, the service remains functional.\\n","completion":"## Decision\nUse Auth0 in all environments without requiring Javascript.\\n"}
{"prompt":"## Context\\nWe want to add relational database.\\n","completion":"## Decision\nWe will use PostgreSQL. It's a mature database with handy features like JSON\\nand hstore data types. It's fully ACID compliant including schema changes. It\\nhas very good support in Django's ORM.\\nAnother popular option is MySQL\/MariaDB. But because it has a major bug `#28727`\\n(10 years since it has been reported and it's still not fixed) breaking ACID in\\nschema changes it can't be used for any serious project.\\n"}
{"prompt":"## Context and Problem Statement\\nIn order to maintain the serverless nature of the project, a DynamoDB table was\\nchosen as main persistence mechanism, which means that a schedule is primarily\\nstored in DynamoDB and then distributed to other components, which in turn\\ngoes back to DynamoDB to update state. So, DynamoDB holds both state and\\nhistorical data.\\nThe problem here is that both ~~warmer~~ `task-1minute-enqueuer` and ~~poller~~ `task-1minute-sqs2sns` will concur by Dynamo resources and probably will be throttled (it's easy to reproduce this behavior only by setting Dynamo's read and write capacity to 1 and trying to send some hundreds of schedules while some other are ~~moving from *WARM* state~~ being enqueued in delayer queue).\\n## Decision Drivers\\n*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\n","completion":"## Decision\n*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\nTake the decision for the use of the DynamoDB introduced a new concept for the entire architecture: the layered concern.\\nThe `delayer-aws` solution aims to provide a way to schedule future operations reliably. It's not part of this system store or ingest or even present information about these schedules. In this sense, the use of DynamoDB is needed only because there's a need of store schedules that could not be posted in delayer queue, and there's only 2 options for those records: or they are in the delayer queue, or they're not. That's why the \"state\" field is needed, but it will not hold the *entire* lifecycle of a schedule.\\nWith this in mind, we realize that all 3 options will be considered, but in different contexts:\\n-   Present data of scheduler is not `delayer-aws`'s concern, but it will be needed. So all the data events should be published by `delayer-aws` to be consumed by another \"view\" platform - this is a kind of *event driven approach*.\\n-   In this sense, if another system will ingest all of this published data, state and historical data will be stored in different persistent stores;\\n### How it should work\\nWhen a schedule is inserted, if their timeframe was greater then the timeframe of the delayer queue, the schedule is stored in DynamoDB with a *currentStatus* marked as `NEW` and a TTL of 1 hour after schedule time.\\nWhen `task-1minute-enqueuer` runs and finds this scheduler, it will be updated to `PROCESSED`. After that, no more updates are made in DynamoDB.\\nFor each event that occurs with this schedule, a message should be posted in the correspondent topic.\\n### Don't use state as a field in database\\nThe state of a schedule is noted by currentStatus\/currentState attribute in\\nDynamoDB. The proposal of this optional is to treat state as part of the\\ngeneral flow of application:\\n1.  When scheduled, it's COLD\\n1.  When *warmer* puts it on SQS, it's WARM\\n1.  When *poller* publishes it on SNS is DOING\\n1.  When *actions* execute\\n*   Good, simplified update policy\\n*   Good, reduced DynamoDB costs\\n*   Bad, no information about past events\\n"}
{"prompt":"## Context\\nOther work within the same domain has created a JSON file containing all GP practices that need to be searched and exposed via ES. This datafile was originally created to be loaded into MongoDB. Whilst Elastic Search (ES) can import the JSON document in the existing format it takes considerable time. Re-shaping the data to that required by the bulk import API of ES means the import can happen in a matter of seconds rather than many minutes otherwise.\\n","completion":"## Decision\nWe will use the existing data file but reshape the data specifically to suit the needs of ES.\\n"}
{"prompt":"## Context\\nWe want to provide metrics about the status of a monitored service.\\n","completion":"## Decision\nWe will use _Dropwizard_ for creating the metrics.\\n"}
{"prompt":"## Context\\nFood Farmacy works with external individual users and depends on reputation. In such an environment the time and accuracy of resolving users' complaints are essential. Thus, there should be a way to store all intermediate changes that are used. Keeping only the final state of a domain entity won't help to understand how we end up with a specific state.\\nThe ordering system going to evolve as the company grows and not all aspects clear at the beginning. There should be an option for graceful migration from one data model and data completeness to another.\\nUser engagement is one of the business goals and there should be data for analysis of a user's actions and behaviors to build a better model for new user engagements.\\n","completion":"## Decision\nThe event sourcing approach gives us a full history of changes and can help investigate issues and users' complaints. With the support of EventSource storage, the main issue with synchronizing updates and read models is manageable.\\n"}
{"prompt":"Context\\n=======\\nWe needed to decide where to terminate TLS connections for public and tenant\\nfacing endpoints and how to manage the corresponding private keys.\\nWe had previously decided to only support HTTPS to both deployed applications\\nand Cloud Foundry endpoints.\\nAt the time of writing there were 4 endpoints to consider:\\n- Deployed applications (gorouter). Accessed by the public.\\n- CF API. Accessed by tenants.\\n- UAA. Accessed by tenants.\\n- Loggregator. Accessed by tenants.\\n- SSH proxy. In theory accessed by tenants, but not working in our environment.\\nWe had an existing credentials store suitable for storing the private keys at\\nrest. Only a small number of engineers within the team can access; the same\\nones that can make IAM changes using our account-wide terraform config.\\nPlacing ELBs in front of public-facing services is an architectural pattern\\nadvised by Amazon [in order to reduce attack\\nsurface](https:\/\/d0.awsstatic.com\/whitepapers\/DDoS_White_Paper_June2015.pdf).\\nSpecifically they advise that it helps withstand volumetric Denial of Service\\nattacks; the ELB handles TCP connections and therefore the responsibility for\\nhandling DDOS at Layer 4 and below resides with the ELB team.\\nWe did a spike, where we attempted to place everything public-facing or\\ntenant-facing behind ELBs. We found that:\\n- In HTTP mode the ELBs do not support web sockets. This is known to break\\nloggregator, which relies on them for log streaming. It would also prevent\\ntenants from using web sockets within their applications.\\n- When the ELB is in TCP mode, we have no way of communicating the client IP\\naddress to the downstream service. Practical consequences of this would be\\nthat tenants would be unable to see in their logs who is using their service or\\ndo any access control based on client IP address.\\nIn attempting to solve the second problem, we explored some options:\\n- ELB has support for the [Proxy\\nProtocol](http:\/\/www.haproxy.org\/download\/1.5\/doc\/proxy-protocol.txt), but\\nunfortunately none of the downstream services, such as gorouter, support it. It\\nseemed simple to add support to gorouter.\\n- We could introduce another intermediary proxy such as HAProxy, which\\nunderstands the proxy protocol and adds or appends to an `X-Forwarded-For`\\nheader with the client IP address as provided via the proxy protocol.\\nDecision\\n========\\nWe decided to:\\n- use the ELB to terminate TLS\\n- use the ELB in TCP mode\\n- submit proxy protocol support to gorouter\\n- use S3 logging to ensure we have the IP addresses of clients using the CF\\nendpoint\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n- We played a [spike to investigate setting X-Forwarded-For\\ncorrectly](https:\/\/www.pivotaltracker.com\/projects\/1275640\/stories\/116619465)\\nwhich produced an [upstream PR to add proxy protocol support to\\ngorouter](https:\/\/github.com\/cloudfoundry\/gorouter\/pull\/126) and [another to introduce\\nX-Forwarded-Proto headers](https:\/\/github.com\/cloudfoundry\/gorouter\/pull\/127)\\n- As an interim measure until gorouter gained support, [we added an\\nintermediate HAProxy to introduce `X-Forwarded-For` and `X-Forwarded-Proto`\\nheaders](https:\/\/www.pivotaltracker.com\/story\/show\/116309951).\\n","completion":"## Decision\n========\\nWe decided to:\\n- use the ELB to terminate TLS\\n- use the ELB in TCP mode\\n- submit proxy protocol support to gorouter\\n- use S3 logging to ensure we have the IP addresses of clients using the CF\\nendpoint\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n- We played a [spike to investigate setting X-Forwarded-For\\ncorrectly](https:\/\/www.pivotaltracker.com\/projects\/1275640\/stories\/116619465)\\nwhich produced an [upstream PR to add proxy protocol support to\\ngorouter](https:\/\/github.com\/cloudfoundry\/gorouter\/pull\/126) and [another to introduce\\nX-Forwarded-Proto headers](https:\/\/github.com\/cloudfoundry\/gorouter\/pull\/127)\\n- As an interim measure until gorouter gained support, [we added an\\nintermediate HAProxy to introduce `X-Forwarded-For` and `X-Forwarded-Proto`\\nheaders](https:\/\/www.pivotaltracker.com\/story\/show\/116309951).\\n"}
{"prompt":"## Context and Problem Statement\\nShould `Extended` support geometry shaders?\\n## Decision Drivers <!-- optional -->\\n* Metal does not support geometry shaders.\\n* Performance of geometry shaders implementations are not consistent accross hardware vendors.\\n* Performance problems when geometry shaders are generating primitives that are to be stored in slower access mediums off chip.\\n* Another stage in the graphics pipeline which is competing for resources which could effectively be used somewhere else such as the vertex or fragment stage.\\n* The practical function of geometry shaders can effectively be done instead using vertex shaders with advanced techniques, compute shaders, tesselation, or instancing.\\n* MonoGame does not support geometry shaders resulting in some, if not most, developers who are not unfamiliar with how geometry shaders work or even their purpose.\\n","completion":"## Decision\n* Metal does not support geometry shaders.\\n* Performance of geometry shaders implementations are not consistent accross hardware vendors.\\n* Performance problems when geometry shaders are generating primitives that are to be stored in slower access mediums off chip.\\n* Another stage in the graphics pipeline which is competing for resources which could effectively be used somewhere else such as the vertex or fragment stage.\\n* The practical function of geometry shaders can effectively be done instead using vertex shaders with advanced techniques, compute shaders, tesselation, or instancing.\\n* MonoGame does not support geometry shaders resulting in some, if not most, developers who are not unfamiliar with how geometry shaders work or even their purpose.\\nNone taken yet.\\n"}
{"prompt":"## Context\\nAs described in ADR0001, we are intending to build a new Marain API providing a standardized schema for common entities used in line-of-business applications.\\nThis API is logically separated into three, as described by [the service definition files here.](https:\/\/github.com\/marain-dotnet\/Marain.LineOfBusiness\/tree\/master\/Solutions\/ServiceDefinitions\/Prototyping):\\n- Organizational units and their data and relationships\\n- People and their data and relationships\\n- The relationships between organizational units and people\\nFor the initial implementation we have a number of decisions to make:\\n### Level of separation of APIs\\nHere we have three choices.\\n1. Implement the three APIs as completely separated services, each with their own underlying storage. This would give the most flexibility in implementation, but would also be the most complex. It would likely require an element of data synchronization between the services, as some level of denormalization would be necessary to make the APIs useful in their own right.\\n2. Implement the three APIs as a single service, sharing a common store. This would be the simplest solution, but would be a difficult decision to reverse at a later date should it become necessary.\\n3. Implement the three APIs as separate services but use common storage. This is a balance between the first two options, providing a simple initial implementation but making it relatively straightforward to separate out the three services at a later date if required.\\n### Underlying storage selection\\nAs always, we need to determine the most appropriate storage mechanism for the APIs. To some degree, this will depend on the level of separation decided upon.\\nHowever, since a large part of these APIs is about describing relationships between entities, a graph model is the obvious choice. If we do completely separate the three APIs, then it will make sense to use a graph database for the organizational unit and person APIs; the API that holds the relationship between organizational units and people could be either a standard document database or a graph database.\\n","completion":"## Decision\n### We will implement separate services over a single data store\\nThis option gives us a simple path to v1 of the service but leaves us the option of further separating the APIs in the future.\\n### We will use a Graph database to implement storage\\nAs described above, this is the obvious choice for an API of this nature.\\n"}
{"prompt":"## Context\\nin [ADR#4](0004-cookies-encryption.md) we decided to use session cookies but\\ndidn't address the problem of sessions lasting an infinitely long time if the\\nuser doesn't ever close their browser (all tabs).\\nA user could abandon their registration and then return months or years later to register again and we'd have their information pre-populated. This will be extremely disconcerting for the user.\\n","completion":"## Decision\nUsing middleware, add an extra key to the session cookie called max-age. If the middleware sees a cookie with a max-age < now then it will delete that cookie. We can set to a long time like 4 hours for now and reduce as we continue to learn about our users' behaviour.\\n"}
{"prompt":"## Context\\nWhen HapiJS plugins are registered they expect an object like this:\\n```JavaScript\\n{\\nname: 'plugin-name',\\nversion: '1.0.0',\\nregister: (server, options) => {}\\n}\\n```\\nThe problem with `export default` is that it exports an object whose property `default` is the value of the exported object. This makes my convention of having the `index.ts` register the plugin not work because HapiJS still uses CommonJs and won't get the `default` value E.g.\\n```JavaScript\\nimport plugin from '.\/plugin';\\nexport default {\\nplugin,\\n}\\n```\\nSwitching between `module.exports` and `export default` will make using my packages tricky. It won't be clear when to use CommonJS or ES6 modules so I'll pick one and stick with it.\\nSo let's use `module.exports` instead of `export default`.\\n","completion":"## Decision\nRejected because if I try and import a module that uses `module.exports` Typescript shows an error that the file `is not a module`. The implication of this is that I have to replace [Glue][hapi-glue] to compose the API.\\n"}
{"prompt":"## Context\\nIn order to wrap the FlowKit toolkit in a single HTTP API, an HTTP framework is required. There are a variety of options for this purpose, e.g. Flask and Django.\\nBoth Flask & Django offer a significant plugin ecosystem, and are 'battle-tested'. However, both are on the heavy side and are bound by legacy design. Of the two, Flask has much less boilerplate overhead.\\nAn alternative option is Quart, which is considerably newer. Quart is compatible with the Flask ecosystem plugin, and built to follow the newer ASGI standard. It is lightweight, offers impressive performance, and takes full advantage of the recent addition of asyncio to Python.\\nQuart also supports websockets, which, while not an immediate priority, are likely to be very useful for future, more dynamic iterations of the API.\\nShould Quart become defunct, the close mapping to the Flask API provides a low-impact exit.\\n","completion":"## Decision\nThe FlowKit API will make use of the Quart framework.\\n"}
{"prompt":"## Context\\nWe need users to be able to browse and search the datasets within the platform so that they can find what they need, and we can break down data siloes. There are various different ways to implement this.\\n## Decision Drivers\\n- Ease of set up and maintenance\\n- Features and user experience\\n- Ability to ingest metadata from the data lake\\n- Cost\\n","completion":"## Decision\n- Ease of set up and maintenance\\n- Features and user experience\\n- Ability to ingest metadata from the data lake\\n- Cost\\nWe have decided to use an DataHub, an open source tool, as our data catalogue tool because:\\n- We have been able to successfully set it up in our AWS environment without much difficulty [please add here]\\n- It has the basic features we require to catalogue datasets (e.g. a range of metadata available which users can add to, search and browse functionality) as well as additional functionality that may enable us to catalogue data pipelines, dashboards, models etc in future.\\n- Users responded positively to it in user research.\\n- It is capable of ingesting metadata from Hive\/AWS Glue and thus our data lake.\\n- It is open-source and therefore cost is limited.\\n- It has an active community where we have an opportunity to influence its future development.\\n"}
{"prompt":"## Context\\nPCMT is intended to be an open and free software product that's aligned with\\nthe [Open Source Definition][osd] and the\\n[Principles for Digital Development][pdd].  To this end PCMT needs to release\\nthe software and documentation under appropriate software licenses.\\nWhile PCMT has it's pick of open-source licenses, our starting point of Akeneo\\ncomes with the OSL v3 license, a copy-left license.\\n","completion":"## Decision\n- We will abide by the Akeneo PIM Community Edition's [OSL v3][osl3] software\\nlicense. We will include the License and Copyright notice from Akeneo in the\\nroot of every source repository which distributes Akeneo PIM CE.\\n- We will license PCMT software (that which isn't a contribution to Akeneo)\\nunder the [Non-Profit OSL v3][nposl3].  We will include a copy of this license\\nin the root of every source repository created.  We will mark every file\\ncreated with this license notice.\\n- We will license all PCMT documentation and non-software materials under\\n[CC BY-SA 4.0][cc-by-sa]. We will include this mark with all documentation\\nand materials.\\n- We will copyright all software and documentation, that's not a contribution to\\nAkeneo, to [VillageReach][villagereach].  We will include this notice in every\\nfile created.\\n- We will require that all contributions to PCMT be licensed under the\\naforementioned licenses and copyright assigned to VillageReach.  We will\\nnote this in a contribution guide available at the root of the PCMT project.\\n"}
{"prompt":"## Context\\nWe have actors &lt;0001-actors&gt;. And\\nvideos &lt;0002-videos-languages&gt;, with their auxiliary files, that\\nwe'll call for now \"content\". We want this content to be organized for\\nactors to manage\/view them.\\n","completion":"## Decision\nVideos are grouped in playlists, which is a Django model named\\n`Playlist`. A playlist belongs to an organization (`Organization` model\\ndefined in actors &lt;0001-actors&gt;) and is created by someone who can\\nbe a manager of this organization, or an author who belongs to this\\norganization. This link is materialized by an `author` field on the\\n`Playlist` model, a `ForeignKey` to the `User` model.\\nThe manager can allow many actors to manage a playlist, so there is a\\n`ManyToMany` field on the `Playlist` model, named `editors`, pointing to\\nthe `User` model. And instead of relying on the hidden model created by\\nDjango when creating a `ManyToMany`, we'll define this model and use it\\nvia the `through` argument, to be able to add more rights constraints\\nlater if needed.\\nThe author of the playlist is automatically added to this list of\\neditors. And can be removed from it by a manager, still staying marked\\nas the author, but being the author itself doesn't involve any rights.\\nA playlist can be duplicated by a manager, and if it stays in the same\\norganization, the manager can clear or keep the list of editors. If it\\nis to be duplicated in another organization, the list of editors will be\\ncleared of actors not belonging to the new organization, and the manager\\nwill still be able to clear it all or keep the remaining editors.\\nWhen duplicated, a new instance of `Playlist` is created, with a link to\\nthe original playlist, keeping the author. We do the same for each\\ninstances of the `Video` linked to this playlist, but we will still\\npoint to the same files (videos\/audios\/subtitles...) on the hosting\\nprovider, to keep cost manageable.\\nAnd finally, there is a flag named `is_public` on the playlist, that can\\nbe toggled by a manager, to tell if the playlist can be viewed by anyone\\nor only by people who were granted access to it. This kind of access is\\nnot in the scope of this document.\\n"}
{"prompt":"## Context\\nUsers (RPs) will need to provide some private keys to sign AuthnRequests and\\ndecrypt Response Assertions.\\nThey will need to provide these to the verify-service-provider in some, reasonably\\nsecure way. Different users may have different opinions on how best to do this.\\n","completion":"## Decision\nInitially we'll use files for this.\\nWe chose not to use environment variables because they're visible to other processes.\\nWe chose not to use a more complicated solution because it would be more complicated.\\n"}
{"prompt":"## Context\\nWe need to advertise a meaningful history of changes to the Dogma API\\nspecification for both application developers and engine developers.\\nThe types of changes that have been made should be clearly identified, with\\nspecial attention drawn to changes that are not backwards compatible.\\n### Proposals\\n- Maintain a `CHANGELOG.md` as per the recommendations of [Keep a Changelog]\\n- Additionally, begin changelog entries that describe a BC break with `**[BC]**`\\n- Periodically tag releases, using [semantic versioning]\\n","completion":"## Decision\nA changelog will be maintained as per [Keep a Changelog]. Unreleased changes\\nshould be added to the changelog as they are made.\\nGit tags will be named according to the rules of [semantic versioning].\\nAdditionally, tag names are to be prefixed with a `v` as required by [Go modules].\\n"}
{"prompt":"## Context\\nClients can have operations running on multiple DataSHIELD servers concurrently.\\n","completion":"## Decision\nWe need to support asynchronous requests in the R client. We implemented it using completable futures.\\nWe must keep the last execution result for each R session until it gets retrieved or until a new execution is started.\\n"}
{"prompt":"## Context\\nWe want to guard against out-of-date dependencies, especially those with security issues.\\n","completion":"## Decision\nUse github's [dependabot](https:\/\/dependabot.com\/) to track dependencies and generate\\npull requests to stay up to date.\\n"}
{"prompt":"## Context\\nThere are three different versions of flutter that you can build out: Stable, Beta and Dev. Each version will come with various trade-offs.\\nCurrently, there has been a missed stable release, and Beta is required to build iOS correctly.\\n","completion":"## Decision\nWe are using the beta channel.  Once some of the fixes land in stable, we may decide to revisit this decision.\\n"}
{"prompt":"## Context\\nA unified security model for Holochain applications:\\n* Each zome must be able to represent and enforce its own security modeling because that is the appropriate place to do so. (Push the intelligence to the edges.)\\n* Developers must be able to build in granualar specificity and revokability of access to functions and entries.\\n* We must be able to distinguish between application security model, and architectural and code security model.  I.e. what is the security model application developers build into their apps, and the security model of Conductor\/Core, etc.  E.g. we have to ensure that zome calls aren't subject to replay attacks in general, and also allow zome developers to declare and create security policies in specific.\\n","completion":"## Decision\nHolochain will use a variant of the [capabilities](https:\/\/en.wikipedia.org\/wiki\/Capability-based_security) security model. Holochain DNA instances will grant revokable, cryptographic capability tokens which are shared as access credentials. Appropriate access credentials must be used to access to functions and private data.\\nThis enables us to use a single security pattern for:\\n- connecting end-user UIs,\\n- calls across zomes within a DNA,\\n- bridging calls between different DNAs,\\n- and providing selective users of a DNA the ability to query private entries on the local chain via send\/receive.\\nEach capability grant gets recorded as a private entry on the grantor's chain, and are validated against for every zome function call.\\n"}
{"prompt":"## Context\\nAll of our launch configs require that an Amazon Machine Image (AMI) be\\nspecified. How the AMI ID is specified is the point of debate.\\nCurrently we use the `aws_ami` data source to determine the newest AMI ID\\npublished by Ubuntu within a major version and use that in our launch\\nconfigurations. This ensures we're always using newest version of Ubuntu\\nwhen we spin up a new set of machines. On the downside, as this can change\\nwithout our intervention, our terraform runs sometimes want to change more than\\nwe would expect:\\n```\\nimage_id: \"ami-2944as50\" => \"ami-10c9asa9\" (forces new resource)\\n```\\nIn some cases _this is fine_ but due to the legacy nature of our deployments\\nit causes issues with machines that need a number of manual deployment steps\\napplied to them. Examples of these include the app servers and the puppet\\nmaster.\\nThe other alternative is to specify a specific image name. This has the benefit that\\nwe know exactly which version we deploy and it cannot change without our intervention.\\n","completion":"## Decision\nWe override the `instance_ami_filter_name` parameter of the `node_group` module in all\\nthe projects. In the data directory, there is a common variable that sets the default\\nvalue of the parameter for an environment. This can be customised per project in the\\nproject tfvars file.\\n"}
{"prompt":"## Context\\nWe would like to achieve high availability of Chef Automate. HA support for Postgresql & Elasticsearch would require considerable time and effort and likely be very difficult to tailor to individual customer needs. Rather than focus on building out HA support for our backend data services, which do not add much value to the product, we would instead prefer to add support for external data sources. This allows us to focus on building value into the product and allow customers to use existing cloud-based data services or in-house clusters.\\n","completion":"## Decision\n### Requirements\\n* Initial deployment of Chef Automate with external data stores should require minimal user configuration.\\n* Migrations from a single-node install to external data stores should require minimal downtime and operator intervention.\\n* Backup and Restore operations must work when external data stores are in use.\\n### External Elasticsearch\\n#### Deployment\\nDeployment with external Elasticsearch should remain the same as the existing deployment options, only additional configuration in the initial configuration will be required. Domain services that access Elasticsearch will continue to so by binding to the `automate-elasticsearch` package. When their habitat configuration files are rendered they include the IP address and port. To support external Elasticsearch, we keep all of the existing logic the same, however, when external mode is configured the `automate-elasticsearch` package will not start an instance of Elasticsearch, it\u2019ll create an instance `nginx` which listens on the same address and port but proxies to the upstream external Elasticsearch nodes. This approach has a few properties that are desirable:\\n* Existing domain applications do not have to care if the Elasticsearch cluster is local, external, has many nodes or one, or if it requires special signing or security.\\n* If the external cluster requires xpack security we can use the configuration they\u2019ve provided, along with `nginx`, to transparently perform mutual TLS with the xpack cert and key.\\n* If the external cluster is AWS Elasticsearch Service, we can use the `nginx` proxy to transparently sign outgoing requests with AWS access key and secret key that are provided (or resolved).\\n#### Migration\\nThere are two ways a user might want to migrate their data from a single instance deployment of Chef Automate to an external cluster.\\nIn cases where the user has control over the cluster configuration, they can join the `automate-elasticsearch` service to their external cluster and configure the automate-elasticsearch node for no shards. As soon a shard relocation has completed the user could stop their Chef Automate installation, remove the `automate-elasticsearch` service from the cluster, and patch their Chef Automate config with the external cluster configuration.\\nFor clusters in AWS's Elasticsearch Service (or Elastic\u2019s Elastic Cloud service), the user would likely need to:\\n* Configure their Chef Automate cluster to use S3 backups\\n* Create a full backup (which will create S3 snapshot repositories in their desired bucket)\\n* Create a new Elasticsearch domain in the cloud providers service\\n* Configure their Elasticsearch domain to use the same S3 bucket as the Chef Automate cluster\\n* Use the `_snapshot` API of their Elasticsearch domain to restore the latest snapshot of each Chef Automate snapshot repository\\n* Configure the Chef Automate install to use their AWS Elasticsearch domain\\n#### Backup\\nBackups should more or less work transparently if the user provides the backup repository type that the `es-sidecar-service` will use when creating the snapshot repositories. If they\u2019re using the AWS Elasticsearch Service we\u2019ll also need the `role_arn` that the repository plugin will assume to take snapshots.\\n#### Restore\\nRestoration should work the same, though we might need to add more s3 flags to support the `role_arn`.\\n### External Postgresql\\n#### Deployment\\nDeployment with external Postgresql should remain the same as the existing deployment options. Additional configuration in the initial configuration will be required. Domain services would be accessed in a similar manner to Elasticsearch, using the bind information from the `automate-postgresql` service. When external Postgresql is configured, services could still bind the `automate-postgresql` service, but instead of actually running `postgresql` it would a `postgresql` proxy to then external Postgresql FQDN's. We would configure the proxy to handle any Postgresql authn\/authz transparently and require mTLS to the proxy from the upstream clients.\\nNote: We haven't yet determined what software we'd like to use as the Postgresql proxy, but we've identified a few options that we'll investigate further during implementation and planning.\\n#### Migration\\nMigration from an existing Chef Automate install to an external Postgresql cluster can be achieved in several ways depending on target cluster:\\n* The user could create a backup and manually restore the database .sql files that are contained in the backup repository.\\n* We could build a utility that restores backup databases to a remote Postgresql cluster. We\u2019d have the user create a backup and then restore it to their remote cluster as part of the migration.\\n* It\u2019s possible that a user could even configure a remote cluster to replicate from the `automate-postgresql` service. The user could wait for replication to catch up, stop Chef Automate, promote a read replica in their cluster to primary, and then update the configuration for external Postgresql.\\n#### Backup\\nTo support on-premises Postgresql clusters it makes sense to keep the default backup strategy consistent with the current backup implementation, which involves taking a dump of each database and storing the SQL file in a backup repository. RDS\/Aurora support creating their own snapshots. We could eventually integrate with this feature for these customers.\\n#### Restore\\nRestoration of a database dump would work mostly the same. We\u2019d restore the database SQL against the remote database. If we integrated with RDS\/Aurora we could leverage their snapshot restoration ability. RDS supports restoring to an exact point in time. As our backup ID\u2019s are an exact point in time we might be able to utilize such a feature if we decided to build that integration.\\n"}
{"prompt":"## Context\\n`__decorate`, `__metadata` and `__param` may contain more than half of total generated js in a large enterprise project.\\nImprove the performance will benefit large project.\\n","completion":"## Decision\nRe-Implement the above function with better performance. By remove the comparison inside loop.\\n"}
{"prompt":"## Context\\nPreviously, the [Driver base class](..\/components\/core\/driver.md) interacted with the [Driver Registry](..\/components\/core\/driver_registry.md). This eliminated work that was needed from the user's side, but it also complicated the base class and introduced a dependency of drivers on the registry.\\n","completion":"## Decision\nThe Driver Registry dependency will be removed from the Driver base class. Drivers will instead be manually registered in the hardware platform.\\nThis comes with a few related changes:\\n- There is no more global driver registry instance - it's part of the hardware platform, and the virtual platform interfaces just forward the call to the hardware platform\\n- The virtual hardware platform base class takes a driver registry as a template parameter, which allows us to defer the selection of our driver registry until we declare one in the platform.\\n- From the hardware-platform's perspective, we don't care if we are using static or dynamic registry creation. We can't have our hardware platform deciding that it needs dynamic memory, when our platform is set up to use no dynamic memory.\\n- The virtual platform base class will be adjusted to forward the calls to the hardware platform APIs\\n- There is no more driver registry global singleton (in the virtual platform), because we don't need to access the registry *before* the hardware platform is created. Now, the instance is kept in the virtual hardware platform.\\n"}
{"prompt":"## Context\\nOur team created new Core Tracker Interface for tracking events. For consistency we should replace and use CoreTrackable instead of Legacy Trackable Interface.\\n","completion":"## Decision\nEvery new events must use CoreTrackable. Also every new event should use new Tracker approach.\\n"}
{"prompt":"## Context\\nWhether to start with a Microservices or not. There are 2 schools each advocating for the exact opposites.\\n### 1. [Don\u2019t start with a monolith](https:\/\/martinfowler.com\/articles\/dont-start-monolith.html) - Stefan Tilkov\\n> Microservices\u2019 main benefit, in my view, is enabling parallel development by establishing a hard-to-cross boundary between different parts of your system. By doing this, you make it hard \u2013 or at least harder \u2013 to do the wrong thing: Namely, connecting parts that shouldn\u2019t be connected, and coupling those that need to be connected too tightly. In theory, you don\u2019t need microservices for this if you simply have the discipline to follow clear rules and establish clear boundaries within your monolithic application; in practice, I\u2019ve found this to be the case only very rarely\\n### 2. [MonolithFirst](https:\/\/martinfowler.com\/bliki\/MonolithFirst.html) - Martin Fowler\\n> The first reason for this is classic Yagni. When you begin a new application, how sure are you that it will be useful to your users? It may be hard to scale a poorly designed but successful software system, but that's still a better place to be than its inverse. As we're now recognizing, often the best way to find out if a software idea is useful is to build a simplistic version of it and see how well it works out. During this first phase you need to prioritize speed (and thus cycle time for feedback), so the premium of microservices is a drag you should do without.\\n> The second issue with starting with microservices is that they only work well if you come up with good, stable boundaries between the services - which is essentially the task of drawing up the right set of BoundedContexts. Any refactoring of functionality between services is much harder than it is in a monolith. But even experienced architects working in familiar domains have great difficulty getting boundaries right at the beginning. By building a monolith first, you can figure out what the right boundaries are, before a microservices design brushes a layer of treacle over them\\n","completion":"## Decision\nWe believe that in the early stages, when domain is still being explored and the project is small, monolith is the\\neasiest and safest way to start.\\nWe also acknowledge that as the project complexity and demand for shorter time-to-market increases, microservices\\narchitecture becomes more and more valuable.\\nTherefore, we decide to start with the monolith, with the strict boundaries between the modules, in order to benefit from\\nthe ease of initial development and enable ourselves to evolve towards the microservices in the future\\n"}
{"prompt":"## Context\\nMany k8s operators use Golang, but none of us working on this project are proficient with Golang.\\nAnother option was Rust, which could be a useful learning exercise.\\nPython is used in FIAAS, so we have some experience using it for an operator.\\n","completion":"## Decision\nWe will use Python for Kafkarator.\\n"}
{"prompt":"## Context\\nThe body, headers, attachments of the mails are stored as blobs in a blob store.\\nIn order to save space in those stores, those blobs are de-duplicated using a hash of their content.\\nTo attain that the current blob store will read the content of the blob before saving it, and generate its id based on\\na hash of this content. This way two blobs with the same content will share the same id and thus be saved only once.\\nThis makes the safe deletion of one of those blobs a non-trivial problem as we can't delete one blob without ensuring\\nthat all references to it are themselves deleted. For example if two messages share the same blob, when we delete\\none message there is at the time being no way to tell if the blob is still referenced by another message.\\n","completion":"## Decision\nTo solve this, we will propose a simple two steps algorithms to provide a background deduplication job.\\nThe **first step** consists in building a bloom filter using the entities referencing the blobs.\\nIn the **second step** we iterate over blobs and check in the bloom filter to predict if they are referenced on not.\\n**Bloom filters** are probabilistic data structures. Here the reference prediction can produce false positives: we might\\nskip some non referenced blobs that should have been garbage collected. However, the associated probability can be tuned\\nand by adding a salt we can ensure subsequent runs will have different sets of false positives and thus that all blobs is\\neventually garbage collected.\\nTo avoid concurrency issues, where we could garbage collect a blob at the same time a new reference to it appear,\\na `reference generation` notion will be added. The de-duplicating id of the blobs which before where constructed\\nusing only the hash of their content,  will now include this `reference generation` too. To avoid synchronization\\nissues, the `generation` will be time based.\\nSo only blobs belonging to the `reference generation` `n-2` will be eligible for garbage collection to avoid\\nconcurrency issues, and allow for a clock skew.\\nFinally, we wish to offer the opportunity to configure, and reconfigure, the `generation` duration. In order to do so,\\nwe introduce a `generation family` part in the blobId. Incremented by the administrator on each configuration changes on\\nthe generation duration it allows avoiding conflicts in generations getting the same number before and after the change:\\nall blobIds with a different family are considered belonging to a distinct generation ready to be garbage collected. This\\nallows arbitrary changes in the generation duration.\\n"}
{"prompt":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","completion":"## Decision\nThe change that we're proposing or have agreed to implement.\\n"}
{"prompt":"## Context\\nCurrently I'm capable of using two languages to develop what I'm intending to do - Ruby and Javascript. It didn't make sense to use both, since functionality is not that big. The main thing I wanted to check was related to presentation layer, so it kinda limited my options as well.\\n","completion":"## Decision\nUse Javascript to develop proof of concept.\\n"}
{"prompt":"# Context\\nAkvo Lumen is a system that allows the user to connect to different _data\\nsources_ and import the data. After that user must be able to transform\\nit (clean, aggregate, etc).\\nWe have been looking at different open source ETL frameworks like:\\n* [Pentaho Data\\nIntegration](http:\/\/www.pentaho.com\/product\/data-integration)\\n* [Clover ETL](http:\/\/www.cloveretl.com\/)\\n* [Onyx Platform](http:\/\/www.onyxplatform.org\/about.html)\\n* [Bubbles](http:\/\/bubbles.databrewery.org\/)\\n* [Kiba ETL](http:\/\/www.kiba-etl.org\/)\\nSome of them provide a GUI to build transformations, others require\\ncoding.\\nThere are other ones that are based on Hadoop ecosystem, which is really\\ntoo much for our current needs:\\n* [Luigi](https:\/\/luigi.readthedocs.org\/en\/stable\/)\\n* [Oozie](https:\/\/oozie.apache.org\/)\\n* [Azkaban](https:\/\/azkaban.github.io\/)\\n# Decision\\nBased on the skills of the team (Clojure expertise) and the fact Clojure\\nexcels at data transformation. We have decided that a small ad-hoc\\nfunctions for handling the import and transformation is enough for our\\ncurrent needs.\\nDepending on requirements we'll use a scheduling library like\\n[Quarzite](https:\/\/github.com\/michaelklishin\/quartzite) for scheduling\\nimports.\\n# Status\\nAccepted\\n# Consequences\\n* The current approach will be to create ad-hoc functions to handle\\nimports _(extract)_ from different data sources\\n* If we need a HA setup, there is an easy transition to Onyx Platform.\\nSee the\\n[conversation](https:\/\/gist.github.com\/iperdomo\/7af984b9f32c117678de) with Onyx author\\n","completion":"## Decision\nBased on the skills of the team (Clojure expertise) and the fact Clojure\\nexcels at data transformation. We have decided that a small ad-hoc\\nfunctions for handling the import and transformation is enough for our\\ncurrent needs.\\nDepending on requirements we'll use a scheduling library like\\n[Quarzite](https:\/\/github.com\/michaelklishin\/quartzite) for scheduling\\nimports.\\n# Status\\nAccepted\\n# Consequences\\n* The current approach will be to create ad-hoc functions to handle\\nimports _(extract)_ from different data sources\\n* If we need a HA setup, there is an easy transition to Onyx Platform.\\nSee the\\n[conversation](https:\/\/gist.github.com\/iperdomo\/7af984b9f32c117678de) with Onyx author\\n"}
{"prompt":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","completion":"## Decision\nThe change that we're proposing or have agreed to implement.\\n"}
{"prompt":"## Context\\nSince configuring self-hosted runners is commonly automated via scripts, the labels need to be able to be created during configuration.  The runner currently registers the built-in labels (os, arch) during registration but does not accept labels via command line args to extend the set registered.\\nSee Issue: https:\/\/github.com\/actions\/runner\/issues\/262\\nThis is another version of [ADR275](https:\/\/github.com\/actions\/runner\/pull\/275)\\n","completion":"## Decision\nThis ADR proposes that we add a `--labels` option to the `config`, which could be used to add custom additional labels to the configured runner.\\nFor example, to add a single additional label the operator could run:\\n```bash\\n.\/config.sh --labels mylabel\\n```\\n> Note: the current runner command line parsing and envvar override algorithm only support a single argument (key).\\nThis would add the label `mylabel` to the runner, and enable users to select the runner in their workflow using this label:\\n```yaml\\nruns-on: [self-hosted, mylabel]\\n```\\nTo add multiple labels the operator could run:\\n```bash\\n.\/config.sh --labels mylabel,anotherlabel\\n```\\n> Note: the current runner command line parsing and envvar override algorithm only supports a single argument (key).\\nThis would add the label `mylabel` and `anotherlabel` to the runner, and enable users to select the runner in their workflow using this label:\\n```yaml\\nruns-on: [self-hosted, mylabel, anotherlabel]\\n```\\nIt would not be possible to remove labels from an existing runner using `config.sh`, instead labels would have to be removed using the GitHub UI.\\nThe labels argument will split on commas, trim and discard empty strings.  That effectively means don't use commas in unattended config label names.  Alternatively, we could choose to escape commas but it's a nice to have.\\n"}
{"prompt":"## Context\\nRODA uses Papertrail for logging. Our Papertrail account has a log data\\ntransfer limit of 50 MB. Extending this limit means moving to another tier\\non Papertrail's platform, which we would prefer to avoid.\\n","completion":"## Decision\n- Reduce the default logging level on production to `info` instead of `debug`\\n- Use the [lograge gem](https:\/\/github.com\/roidrage\/lograge) to turn Rails'\\ndefault multiline logs into a single line, without losing information or\\ncontext\\n"}
{"prompt":"## Context\\nIn order for the Cosmos SDK to implement the [IBC specification](https:\/\/github.com\/cosmos\/ics), modules within the SDK must have the ability to introspect recent consensus states (validator sets & commitment roots) as proofs of these values on other chains must be checked during the handshakes.\\n","completion":"## Decision\nThe application MUST store the most recent `n` headers in a persistent store. At first, this store MAY be the current Merklised store. A non-Merklised store MAY be used later as no proofs are necessary.\\nThe application MUST store this information by storing new headers immediately when handling `abci.RequestBeginBlock`:\\n```golang\\nfunc BeginBlock(ctx sdk.Context, keeper HistoricalHeaderKeeper, req abci.RequestBeginBlock) abci.ResponseBeginBlock {\\ninfo := HistoricalInfo{\\nHeader: ctx.BlockHeader(),\\nValSet: keeper.StakingKeeper.GetAllValidators(ctx), \/\/ note that this must be stored in a canonical order\\n}\\nkeeper.SetHistoricalInfo(ctx, ctx.BlockHeight(), info)\\nn := keeper.GetParamRecentHeadersToStore()\\nkeeper.PruneHistoricalInfo(ctx, ctx.BlockHeight() - n)\\n\/\/ continue handling request\\n}\\n```\\nAlternatively, the application MAY store only the hash of the validator set.\\nThe application MUST make these past `n` committed headers available for querying by SDK modules through the `Keeper`'s `GetHistoricalInfo` function. This MAY be implemented in a new module, or it MAY also be integrated into an existing one (likely `x\/staking` or `x\/ibc`).\\n`n` MAY be configured as a parameter store parameter, in which case it could be changed by `ParameterChangeProposal`s, although it will take some blocks for the stored information to catch up if `n` is increased.\\n"}
{"prompt":"## Context\\nWe wanted to explore Azure's KeyVault capabilities to serve as a CA for services deployed on the strategic platform at HMCTS, that's to issue the certificates services may need for authentication purposes and compare to our previous findings when .\\n","completion":"## Decision\nThe first difference that shows up when comparing Azure's KeyVault (KV) with Hashicorp's Vault (as per ADR 0004) is that the Azure offering is a self hosted service provided by Microsoft whereas Hashicorp's vault would have been an IaaS based solution hosted inside a private VNET. This is an important difference since currently the only way to interact with KV is by connecting to a public endpoint.\\nAlthough KV makes its REST endpoints public (SSL encrypted) it is important to highlight that the only way to interact with this service is by first authenticating against Azure's Active Directory service which, in combination with KV's access policy capabilities, provides a fairly granular control with regards to which information and which KV's can be accessed by an app\/service_principal.\\nThe KV service supports a couple of methods to create certificates including self-signed as described [here](https:\/\/docs.microsoft.com\/en-us\/rest\/api\/keyvault\/create-a-certificate). The KV REST API provides support for an extensive number of Certificate related operations including the most basic use cases HMCTS might need to cover in CNP currently. The following is a list of the certificate related operations:\\n#### Certificate operations\\nThe Azure Key Vault REST API supports the following operations on certificates.\\n* Create a certificate\\n* Import a certificate\\n* List versions of a certificate\\n* List certificates\\n* Get a certificate\\n* Delete a certificate\\n* Update a certificate\\n* Merge a certificate\\n#### Certificate management operations\\nThese REST operations are for the management of certificate operations associated with a Key Vault certificate.\\n* Delete certificate operation\\n* Get certificate operation\\n* Update certificate operation\\n#### Certificate policy operations\\nThe following operations are available on a certificate policy:\\n* Get a certificate policy\\n* Update a certificate policy\\n#### Soft-delete operations\\nThe soft-delete feature supports these operations for deleted certificates:\\n* Get deleted certificate\\n* Get deleted certificates\\n* Purge deleted certificate\\n* Recover deleted certificate\\n#### Certificate Issuers\\nYou can do the following with certificate issuers in a key vault:\\n* Set a certificate issuer\\n* Get a certificate issuer\\n* Update a certificate issuer\\n* Delete a certificate issuer\\n* Get certificate issuers\\n#### Certificate Contacts\\nYou can do the following with certificate contacts:\\n* Get certificate contacts\\n* Set certificate contacts\\n* Delete certificate contacts\\n### Interacting with the KeyVault API\\nInteracting with the KeyVault REST API is a 2 step process. First one must obtain a valid authentication token that can subsequently be used to conduct any of the KeyVault operations listed above providing your service principal have the right access policies for the targeted vault. To illustrate the process here are some sample API calls to obtain a token and then List the certificates in a keyvault called `danvaultpoc`:\\n* We first request an access token. For this you'll need some information about your service_principal, specifically the `TENANT_ID`, `CLIENT_ID` and `CLIENT_SECRET` values:\\n```shell\\n$ curl -X \"POST\" \"https:\/\/login.microsoftonline.com\/$ARM_TENANT_ID\/oauth2\/token\" -H \"Content-Type: application\/x-www-form-urlencoded\" --data-urlencode \"client_id=$ARM_CLIENT_ID\" --data-urlencode \"grant_type=client_credentials\" --data-urlencode \"client_secret=$ARM_CLIENT_SECRET\" --data-urlencode \"resource=https:\/\/vault.azure.net\" | jq -r .access_token\\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\\nDload  Upload   Total   Spent    Left  Speed\\n100  1425  100  1279  100   146   6350    724 --:--:-- --:--:-- --:--:--  6363\\neyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6IjJLVmN1enFBaWRPTHFXU2FvbDd3Z0ZSR0\\n< output truncated >\\n```\\n* Once we have obtain the token it is possible to conduct operations like those listed above against an specific vault. Here we show how to get a list of certificates stored in a vault (`danvaultpoc`):\\n```shell\\n$ curl -H \"Authorization: Bearer <put your token here>\" https:\/\/dankvaultpoc.vault.azure.net\/certificates?api-version=2016-10-01 | jq\\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\\nDload  Upload   Total   Spent    Left  Speed\\n100   234  100   234    0     0    253      0 --:--:-- --:--:-- --:--:--   253\\n{\\n\"value\": [\\n{\\n\"id\": \"https:\/\/dankvaultpoc.vault.azure.net\/certificates\/cert1\",\\n\"x5t\": \"1h6bRfbhRdsrDvJfF3S5jVU_gbk\",\\n\"attributes\": {\\n\"enabled\": true,\\n\"nbf\": 1507645296,\\n\"exp\": 1539181896,\\n\"created\": 1507645896,\\n\"updated\": 1507645896\\n}\\n}\\n],\\n\"nextLink\": null\\n}\\n```\\n"}
{"prompt":"## Context and Problem Statement\\nThis is part of a body of work necessary to support the use of Nimbus within web applications.\\nThe current Nimbus SDK is written in such a way that it supports client-oriented experimentation \u2014 experiments are downloaded, evaluated, and stored on the client, and a feature store is exposed with the experiment branches applied.\\nIn previous decisions (not in this repository), we've already decided that in order to support web experimentation the Nimbus SDK will need to be updated to be stateless, to support a more statically defined set of helper methods, and to have additional support for Python.\\nUltimately, the problem we're trying to solve can be boiled down to one question \u2014 how can we update the Nimbus SDK to support web applications while continuing to support the existing clients?\\nAs an example of what the Cirrus API might look like, we can likely expect endpoints to perform the following:\\n* Enroll a user into available experiment(s)\\n* This would return the enrolled experiments as well as the feature values given the enrollments\\n* Fetch the default feature values\\n* Fetch the feature manifest\\n* Fetch a specific feature value given enrolled experiments\\n## Decision Drivers\\n* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\n","completion":"## Decision\n* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\nWe have decided to move forward with option number 2, \"Cargo features, one library\".\\nThis option, like the others mentioned, meets the key decision drivers.\\nWe believe using this option will be the most maintainable long-term, despite the added complexity of using cargo features.\\nIn addition, implementing this option has a similarly short timeline and amount of work necessary as compared to the \"Cargo features, separate library\" option, without the additional overhead of more complex typing.\\n### Positive Consequences\\n* Meets all decision drivers.\\n* Small amount of work necessary with a very limited amount of code churn.\\n### Negative Consequences\\n* It will be difficult to draw boundaries between Cirrus and Nimbus code.\\n* We could run into unexpected issues with UniFFI, as multiple UDL files in a single crate has not yet been tested.\\n2. [Cargo Features](https:\/\/doc.rust-lang.org\/cargo\/reference\/features.html)\\n2. Examples\\n1. [Multi-module typing example](https:\/\/github.com\/jeddai\/application-services\/pull\/1\/files#diff-02305e8e02a7900352e67be1fd2eef0b5a7c7cf91f4cc3e4559668c124d34e88R11-R19)\\n"}
{"prompt":"## Context\\nSNS is a service originally designed for notifications (e.g. smartphone push notifications) rather as a message queue. Its configuration needs to be tuned for our use case.\\nIt's important to keep the channel as easy to use as possible, without additional complications.\\n","completion":"## Decision\nUse [RawMessageDelivery](https:\/\/docs.aws.amazon.com\/sns\/latest\/dg\/large-payload-raw-message.html) on all SQS-to-SNS subscriptions.\\n"}
{"prompt":"## Context\\nActivation service fails a lot and that lead to starting online threebot without a wallet\\n","completion":"## Decision\nAdd option to activate the 3bot wallet via custom activation wallet on the deployer in case threefold service activation fails and pass the secret in the secret env\\n"}
{"prompt":"## Context\\nA well-architected AWS setup includes backups for non-transient resources like RDS\/Aurora, DynamoDB, EBS etc. so superwerker should protect users by enabling backups automatically.\\nSince superwerker prefers the usage of native AWS services, AWS Backup is used.\\n","completion":"## Decision\n- Backups of resources are created daily for 30 days\\n- The default AWS Backup vault is used per AWS account\\n- For convenience, we auto-create the [Default Service Role for AWS Backup](https:\/\/docs.aws.amazon.com\/aws-backup\/latest\/devguide\/iam-service-roles.html#default-service-roles)\\n- Snapshots are currently not protected via integrity protection SCP, only AWS Backup role should delete snapshots. This is descoped for now. #120\\n- Use AWS Organizations backup policies to enable backups across the entire AWS Organization\\n- We use service-managed stack sets to roll out the required resources across the AWS Organization.\\n- AWS Backup does not currently support backing up all resources of a particular type in an AWS account. Either ARNs or tags have to be specified. Workaround:\\n- Set a resource tag `superwerker:backup` to valid values of `daily` or `none`. Resources with tag set to `daily` are backed up daily defined by the backup policy. AWS Organizations tag policies to enforce `superwerker:backup` is set to valid values.\\n- Since [tag enforcement has no effect on resources that are created without tags](https:\/\/docs.aws.amazon.com\/organizations\/latest\/userguide\/orgs_manage_policies_tag-policies-enforcement.html), we set up an AWS Config Rule `required-tags` to check if tags have been set on backup-eligible resources; if not set, set tags via AWS Config Rules Remediation\\n- Organizational Config Rules or Conformance Packs can be used to roll out AWS Config rules and\/or remediation, but not to roll out the required IAM roles, so:\\n- We use CloudFormation StackSets for rolling out required IAM roles in sub-accounts.\\n- We use AWS Config organizational conformance packs to roll out AWS Config rules and remediation since rolling out AWS Config rules via StackSets would lead to the following race condition: once a new sub-account is enrolled, the stack set would immediately start to roll out AWS Config rules to the new sub-account, but this would fail since the AWS Config recorder would not yet have been created by Control Tower.\\n- Om the other hand, conformance packs wait until the AWS config recorder is created before rolling out AWS Config rules and remediation\\n- Conformance packs provide integrity protection of AWS config rules and remediation configs out of the box.\\n- Trade-offs:\\n- We are aware that conformance packs need an additional S3 bucket, and overall they imitate \/ reimplement parts of CloudFormation StackSets.\\n- The conformance pack tries to enable Config rules in all sub-accounts. Existing sub-accounts must have an enabled AWS config recorder, at least at superwerker installation time, otherwise the installation would fail.\\n- We don't use AWS Config organizational rules because they don't support rolling out the remediation.\\n- No support for cross-region \/ cross-account backups (though we know the AWS features exist) currently to keep it simple.\\n- Since EFS is currently [not supported by AWS Config](https:\/\/docs.aws.amazon.com\/config\/latest\/developerguide\/resource-config-reference.html), it's currently not automatically tagged and thus not automatically backed up\\n- Since AWS Config Rule with compliance resource type `AWS::RDS::DBCluster` and source identifier `REQUIRED_TAGS` [are currently not supported](https:\/\/docs.aws.amazon.com\/config\/latest\/developerguide\/required-tags.html), RDS Clusters are currently not automatically tagged and thus not automatically backed up.\\n"}
{"prompt":"## Context\\nTerraform requires a `backend` for state file storage. Most existing Terraform\\nmodules within LIC have this `backend` set in the Terrform configuration in\\n`main.tf`.\\nHaving the `backend` configured in the module means that consumers can't\\noverwrite the configuration. When building and testing modules it is useful to\\nbe able to use a \"local\" `backend`.\\n","completion":"## Decision\nWe have not set the `backend` configuration in the `main.tf`.\\n"}
{"prompt":"## Context\\nThere are several sites across the Guardian (identity-frontend, membership-frontend) that started life with a developer experience that emulated the dotcom frontend, before diverging. The duplication of effort required to keep these codebases in step with frontend was presumably too large an investment, and as a result the developer experience deteriorated and the consistency was lost.\\nOne way we could prevent this happening again is to allow teams to build their own sites in the dotcom-rendering repo. They would share the same toolchain, code style config, bundling configuration and developer experience. This would reduce duplication and help maintain consistency and best practices across projects.\\n","completion":"## Decision\nWe will allow teams to build their own websites under the `sites\/` directory.\\n"}
{"prompt":"## Context\\nThe solution has to be portable and lightweight and work without special infrastructure.\\n","completion":"## Decision\nDocker-compose is used as multi-container solution.\\n"}
{"prompt":"## Context\\nChanges in behavior of `init` and `uninstall` on Self Hosted mode for. Discussed in this [issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n","completion":"## Decision\n* Calling `dapr init` will\\n* Install `daprd` binary in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Set up the `dapr_placement`, `dapr_redis` and `dapr_zipkin` containers.\\n* Create the default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Create the default components configurations for `pubsub.yaml`, `statestore.yaml` and `zipkin.yaml` in the default `components` folder.\\n* Create a default configuration file in `$HOME\/.dapr\/config.yaml` for Linx\/MacOS and `%USERPROFILE%\\.dapr\\config.yaml` for Windows for enabling tracing by default.\\n* Calling `dapr init --slim` will\\n* Install the binaries `daprd` and `placement` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Create an empty default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Calling `dapr uninstall` will\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker dapr_placement if Docker is installed.\\n* Calling `dapr uninstall --all`\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker containers dapr_placement, dapr_redis and dapr_zipkin if Docker is installed.\\n* Remove the default folder `$HOME\/.dapr` in Linux\/MacOS and `%USERPROFILE%\\.dapr` in Windows.\\n* CLI on the init command will fail if a prior installtion exists in the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* **There will no longer be an option for `--install-path` during init or during uninstall.**\\n* The `dapr` CLI by default will expect the `daprd` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows. The command `dapr run` will not expect the `daprd` binary to be in the `PATH` variable, it will launch the binary from the default path.\\n"}
{"prompt":"**Context**\\nIn order to make sure we are retrieving the expected status and object responses from our API, we need to add an integration test layer in order to ensure the API is working as a single unit.\\n**Decision**\\nWe gonna use the built-in Spring test module and [RestAsured](http:\/\/rest-assured.io\/) in order to make request to the API in a more human readable way.\\n**Status**\\nAccepted\\n**Consequences**\\nTo execute the integration tests we just need to execute the `intTest` task configured in our `build.gradle` file.\\nThe main context is loaded from the Core application, in the future this can be configured in a own context loader for integration test.\\n","completion":"## Decision\nWe gonna use the built-in Spring test module and [RestAsured](http:\/\/rest-assured.io\/) in order to make request to the API in a more human readable way.\\n**Status**\\nAccepted\\n**Consequences**\\nTo execute the integration tests we just need to execute the `intTest` task configured in our `build.gradle` file.\\nThe main context is loaded from the Core application, in the future this can be configured in a own context loader for integration test.\\n"}
{"prompt":"## Context\\nA full test suite run may take > 10 minutes. Given a particular failure, at the end2end level we may encounter many different failures as consequences: for example, a missing file in the publication process may lead to errors in downstream services that rely on it.\\nMany tests also use polling, waiting for a success condition like a 200 response.\\n","completion":"## Decision\nFail a test that detects something is wrong as soon as possible, without triggering additional checks or commands on the system under test.\\n"}
{"prompt":"## Context\\nAs part of my research into using a GeoTrellis REST service, I'm spending some time learning about raster operations. The aim's to gain a clearer sense of what kinds of operations are available and the occasions in which they might be useful\\nThis ADR aims to summarize some of the map algebra ops catalogued in Dana Tomlin's book *GIS and Cartographic Modeling* as preparation for deciding what API endpoints to implement.\\nThe book segments these operations into three subsets\\n- Local operations, which return values for cells based on the values of the same cell in different rasters\\n- Focal operations, which return values for cells in relation to other cells within a defined \"neighborhood\"\\n- Zonal operations, which return values for cells in reation to other cells within a defined \"zone\" (e.g. a watershed);\\n#### Local Operations\\n> Each generates a new map layer on which every location is set to a value computed as a specified function of the location's value(s) on one or more existing map layers.\\nGeoTrellis package: [geotrellis.raster.mapalgebra.local](https:\/\/geotrellis.github.io\/scaladocs\/latest\/#geotrellis.raster.mapalgebra.local.package)\\n##### Operations\\n*Local Calculation*: Calculate new cell value as an arithmetic function of the same cells in other rasters.\\n*Local Classification*: Calculate new cell value by classifying values of the same cells in other rasters.\\n*Local Combination*: Calculate new cell value based on combinations of values from other rasters.\\n*Local Majority*: Calculate new cell value based on the values occurring most frequently in other rasters.\\n*Local Maximum*: Calculate new cell value based on the highest value for the same cells in other rasters.\\n*Local Mean*: Calculate new cell value based on the average value of the same cells in other rasters.\\n*Local Minimum*: Calculate new cell value based on the lowest value for the same cells in other rasters.\\n*Local Minority*: Calculate new cell value based on the values occurring least frequently in other rasters.\\n*Local Variety*: Calculate new cell value indicating the number of dissimilar values for the same cells in other rasters.\\n#### Focal Operations\\n> generates a new map layer on which every location is set to a value computed as a specified function of the values, distances, and\/or directions of neighboring locations. Certain of these operations apply to neighborhoods that can extend well beyond the immediate vicinity of each location, with neighborhood distances measured in terms of physical separation, travel costs, or lines of sight.\\nGeoTrellis package: [geotrellis.raster.mapalgebra.focal](https:\/\/geotrellis.github.io\/scaladocs\/latest\/#geotrellis.raster.mapalgebra.focal.package)\\n##### *Spreading phase*\\n> all distances from each neighborhood focus are to be measured as an accumulation of costs associated with location-to-location movement.\\n##### *Radiating phase*\\n> distances from each neighborhood focus are to be measured over unobstructed lines of sight.\\n##### Operations on Extended Neighborhoods\\n*Focal Bearing*: Calculate cell value indicating the direction of the nearest non-null\/-0 cell location in neighborhood.\\n*Focal Classification*: Return new cell value which indicate the combination of zones in neighborhood.\\n*Focal Combination*: Calculate new cell value which indicates the combination of zones occurring in neighborhood cells.\\n*Focal Distribution*: Calculate new cell value indicating the \"inverse-distance-weighted-average\" of all other neighborhood cells.\\n*Focal Insularity*: Return new cell value which \"uniquely matches\" the values assigned to other cells in the same neighborhood which are also in the same zone.\\n*Focal Interpolation*: Return new cell value indicating the \"inverse-square-distance-weighted-average\" of all other neighborhood cells.\\n*Focal Majority*: Return new cell value indicating the cell value occurring most frequently in neighborhood.\\n*Focal Maximum*: Return new cell value indicating the maximum value of all neighborhood cells.\\n*Focal Mean*: Calculate new cell value indicating the average of all neighborhood cells.\\n*Focal Minimum*: Return new cell value indicating the minimum value of all neighborhood cells.\\n*Focal Minority*: Return new cell value indicating the value occurring least frequently in neighborhood.\\n*Focal Neighbor*: Return new cell value indicating the value of the nearest non-null-value neighborhood cell.\\n*Focal Percentage*: Return new cell value indicating the percentage of neighborhood cells with values equal to the original cell.\\n*Focal Percentile*: Return new cell value indicating what percentage of neighborhood cells have values less than the original cell.\\n*Focal Proximity*: Return new cell value indicating the distance to the nearest non-null cell? New value is \"treated as a measure of distance already accumulated at that location when new distances are calculated.\"\\n*Focal Ranking*: Calculate new cell value indicating how many zones in neighborhood with lower values than the original cell.\\n*Focal Sum*: Calculate new cell value by summing all neighborhood cells.\\n*Focal Variety*: Return new cell value indicating the number of zones in neighborhood.\\n##### Operations on Immediate Neighborhoods\\n*Focal Area*: Calculate new cell value indicating the area of whatever portion of an \"areal condition\" is represented by that cell when a first raster's projected onto another raster.\\n*Focal Aspect*: Calculate new cell value indicating the compass direction of steepest descent for a plane inferred from the surface layer values of that location and any adjacent neighbors that share its first layer value.\"\\n*Focal Drainage*: Return a new cell value indicating which cells \"lie upstream on a surface inferred from the surface layers of all locations in the same first layer zone.\"\\n*Focal Frontage*: Calculate new cell value indicating \"the length\" of the boundaries formed by whatever portion of an areal condition is represented by that location\" on a first raster when it's projected onto another raster.\\n*Focal Gradient*: Calculate new cell value indicating the slope of a plane inferred from the original value of the cell and adjacent neighbors with the same value.\\n*Focal Length*: Return new cell value indicating the length \"of whatever portion of a lineal condition is represented by the location's original value when projected\" onto another raster's values.\\n*Focal Linkage*: Calculate a new cell value indicating the \"type of form\" from cell and neighborhood.\\n*Focal Partition*: Return new cell value indicating the \"areal boundary\" of upper, upper right, and right neighbors.\\n*Focal Volume*: Return new cell value indicating the \"surficial volume beneath whatever portion of an areal condition\" corresponds to the first raster value when projected onto a second raster.\\n#### Zonal Operations\\n> generates a new map layer on which every location is set to a value computed as a specified function of the values from one existing map layer associated with all locations in a common zone on another existing map layer.\\nGeoTrellis package: [geotrellis.io.mapalgebra.zonal](https:\/\/geotrellis.github.io\/scaladocs\/latest\/#geotrellis.raster.mapalgebra.zonal.package)\\n##### Operations\\n*Zonal Classification*: Explicitly assign new cell value as a combination of values occurring in both first and second raster.\\n*Zonal Combination*: Calculate new cell value which indicates the combination of values in one raster also present in a second raster.\\n*Zonal Majority*: Calculate a new cell value indicating which values in one raster occur most often in a second raster.\\n*Zonal Maximum*: Calculate new cell value indicating which values in one raster occur most frequently in a second raster.\\n*Zonal Maximum*: Calculate new cell value indicating the highest value from a first raster within a second raster zone.\\n*Zonal Mean*: Calculate new cell values indicating the average first raster value present in a second raster zone.\\n*Zonal Minimum*: Calculate new cell value indicating the minimum first raster value in a second raster zone.\\n*Zonal Minority*: Calculate new cell value indicating the value from a first raster which occurs least frequently in a second raster.\\n*Zonal Percentage*: Calculate new cell value indicating the cell count in a second raster zone whose values are equal to the values in the first raster.\\n*Zonal Percentile*: Calculate new cell value indicating the cell count in a second raster zone whose values have lower values in the first raster.\\n*Zonal Ranking*: Calculate new cell value indicating the number of zones in the second raster which have lower values in the first raster.\\n*Zonal Sum*: New cell value sums the first raster values present in a second raster zone.\\n*Zonal Variety*: New cell value indicates the count of zones in the first raster which also occur in the second raster.\\n","completion":"## Decision\nAfter reading through this catalog and checking the GeoTrellis docs, I'm going to attempt to include these endpoints for these three operations as I'm implementing the API for [this issue](https:\/\/github.com\/azavea\/geotrellis-collections-api-research\/issues\/5):\\n- [Local variety](https:\/\/github.com\/locationtech\/geotrellis\/blob\/master\/raster\/src\/main\/scala\/geotrellis\/raster\/mapalgebra\/local\/Variety.scala)\\n- [Focal standard deviation](https:\/\/github.com\/locationtech\/geotrellis\/blob\/master\/raster\/src\/main\/scala\/geotrellis\/raster\/mapalgebra\/focal\/StandardDeviation.scala)\\n- [Zonal histogram](https:\/\/github.com\/locationtech\/geotrellis\/blob\/master\/raster\/src\/main\/scala\/geotrellis\/raster\/mapalgebra\/zonal\/ZonalHistograms.scala)\\nWhile these GeoTrellis ops don't map precisely onto those catalogued above, they're documented GeoTrellis capabilities.\\nAlong with those endpoints for those ops, I'm going to try writing two additional endpoints:\\n- an endpoint to generate PNG tiles with color ramps as [documented here](http:\/\/geotrellis.readthedocs.io\/en\/latest\/guide\/rasters.html)\\n- an endpoint to write data out as a GeoTIFF which seems to be available via the [GeoTiffWriter module](https:\/\/github.com\/locationtech\/geotrellis\/blob\/master\/raster\/src\/main\/scala\/geotrellis\/raster\/io\/geotiff\/writer\/GeoTiffWriter.scala)\\nHere are five API endpoints I'm going to implement:\\n| Path | Service |\\n| --- | --- |\\n| [\/localvariety](http:\/\/localhost:7000\/localvariety) | Returns local variety results |\\n| [\/focalstandarddeviation](http:\/\/localhost:7000\/focalstandarddeviation) | Returns focal standard deviation results |\\n| [\/zonalhistogram](http:\/\/localhost:7000\/zonalhistogram) | Returns zonal histogram results |\\n| [\/pngtile](http:\/\/localhost:7000\/pngtile) | Returns a PNG tile fit to the shape |\\n| [\/geotiff](http:\/\/localhost:7000\/geotiff) | Returns a GeoTIFF for the drawn shape along with Raster data |\\nEach will accept GeoJSON POSTed from the client.\\n"}
{"prompt":"## Context\\nCurrently we have an Nginx Ingress controller deployed in the same namespace\\nas the Datalabs application. This means that when multiple instances of\\nDatalabs are deployed to a single cluster (such as prod and test), ingress\\ncontrollers are deployed to each of these namespaces.\\nAs we are in the process of implementing multiple project functionality within\\nDatlabs, there is now a need for an ingress controller than can fulfill ingress\\nrules across all namespaces. This also will move control of ingress to be a\\nplatform service and not part of the deployment of the application itself,\\nwhich will aid to decouple Datalabs into being more of a standalone\\napplication.\\n","completion":"## Decision\nWe have decided to deploy a single Nginx Ingress Controller into the\\nkube-system namespace that will handle the ingress rules for the entire\\ncluster.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nThere is a need for a scalable structure of the SDK documentation. Current documentation includes a lot of non-related SDK material, is difficult to maintain and hard to follow as a user.\\nIdeally, we would have:\\n- All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\\n- All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\\n","completion":"## Decision\nRe-structure the `\/docs` folder of the SDK github repo as follows:\\n```\\ndocs\/\\n\u251c\u2500\u2500 README\\n\u251c\u2500\u2500 intro\/\\n\u251c\u2500\u2500 concepts\/\\n\u2502   \u251c\u2500\u2500 baseapp\\n\u2502   \u251c\u2500\u2500 types\\n\u2502   \u251c\u2500\u2500 store\\n\u2502   \u251c\u2500\u2500 server\\n\u2502   \u251c\u2500\u2500 modules\/\\n\u2502   \u2502   \u251c\u2500\u2500 keeper\\n\u2502   \u2502   \u251c\u2500\u2500 handler\\n\u2502   \u2502   \u251c\u2500\u2500 cli\\n\u2502   \u251c\u2500\u2500 gas\\n\u2502   \u2514\u2500\u2500 commands\\n\u251c\u2500\u2500 clients\/\\n\u2502   \u251c\u2500\u2500 lite\/\\n\u2502   \u251c\u2500\u2500 service-providers\\n\u251c\u2500\u2500 modules\/\\n\u251c\u2500\u2500 spec\/\\n\u251c\u2500\u2500 translations\/\\n\u2514\u2500\u2500 architecture\/\\n```\\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\\n- `README`: Landing page of the docs.\\n- `intro`: Introductory material. Goal is to have a short explainer of the SDK and then channel people to the resource they need. The [sdk-tutorial](https:\/\/github.com\/cosmos\/sdk-application-tutorial\/) will be highlighted, as well as the `godocs`.\\n- `concepts`: Contains high-level explanations of the abstractions of the SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\\n- `clients`: Contains specs and info about the various SDK clients.\\n- `spec`: Contains specs of modules, and others.\\n- `modules`: Contains links to `godocs` and the spec of the modules.\\n- `architecture`: Contains architecture-related docs like the present one.\\n- `translations`: Contains different translations of the documentation.\\nWebsite docs sidebar will only include the following sections:\\n- `README`\\n- `intro`\\n- `concepts`\\n- `clients`\\n`architecture` need not be displayed on the website.\\n"}
{"prompt":"## Context\\nSince the initial decision to rebuild NOMIS and the Fix & Go infrastructure in Azure we have learned a few things and come to reevaluate our choice. Some context:\\n- NOMIS, OASys, CAFM, and the rest of Fix & Go resides in Azure under the control and maintenance of the DSO team.\\n- Database, WebLogic and application installation and management is provided by the Sheffield Studio DBA team.\\n- Fix & Go has network connectivity to a few key environments, particularly the Quantum network and PSN.\\n- Uncertainty around IR35 status and contractual renegotiations, as well as a shift in team focus and priorities means we have lost several members of the team. Unfortunately there's a reasonable chance we'll lose more people in the next few months.\\n- The Delius migration is now live, so the infrastructure code is tested and used in production.\\n- Robin helped deliver the Delius DevOps tooling, and has deep knowledge of how it works and where it could benefit us.\\n- Recent spikes demonstrated that:\\n+ We can spin up an environment in AWS very quickly \u2013 within 3 days we had automated builds and deployment for a simple Oracle\/WebLogic stack. While NOMIS wasn't functional (Oracle Forms still needs to be built regardless of AWS\/Azure), the supporting infrastructure was \"production quality\".\\n+ The Packer images and Ansible is largely reusable in Azure \u2013 again with a few days we had a simple Oracle\/WebLogic stack, albeit without the production-quality supporting infrastructure and networking. The Terraform configurations would need to be rewritten for Azure, as they are provider-specific.\\n+ While NOMIS closely resembles a portion of Delius, the latter is significantly more complex.\\n","completion":"## Decision\nWe will continue to build in Azure, and lean more heavily on Azure's native offerings to offset some of the cost of _not_ using the Delius team's AWS code.\\nThis decision is based on a couple of things (as well as the million more nuanced things):\\n1. Reducing the risk of knowledge silos, knowledge loss (bus\/lottery factor), and lowering the barrier to entry for new team members is really important given the current challenges around IR35 and the makeup of our team. Adding a new cloud provider increases the complexity of our systems, even with the potential support we could get from the Delius migration team.\\n2. Reducing our time to deliver DR in production. We already have network connectivity to Quantum, remote desktop infrastructure, etc. in Azure, and replicating sufficient services in AWS to allow a migration of production is likely to be a significant undertaking involving a number of external stateholders.\\n"}
{"prompt":"## Context\\n[Kubernetes offers a bunch of strategies for authentication](https:\/\/kubernetes.io\/docs\/reference\/access-authn-authz\/authentication\/#authentication-strategies). The most appropriate strategies are x509 certificates, webhook tokens and OpenID Connect (OIDC) Tokens.\\nUsing x509 certificates would allow us to authenticate to a cluster without having to pass additional `--oidc-*` flags to the Kubernetes masters. This is something that is often hard or impossible to change on managed clusters. However, it isn't clear that managed clusters that we'd be able authenticate using certificates anyway. For example, EKS seems to only allow IAM for authentication.\\nRenewal of short-lived x509 certificates would require regenerating, revalidating and resigning a certificate as there is no equivalent refresh token as with [OIDC tokens](https:\/\/auth0.com\/docs\/tokens\/refresh-token\/current). This is not part of `kubectl`.\\nWebhook tokens require additional configuration in the apiserver (`--authentication-token-webhook-*` flags and a configuration file). They allow a supplied token to be validated by an external service and kubernetes apiserver expects a certain payload in the response containing the user properties. An application to generate a token for a given user (following authentication with an identity provider such as GitHub) will need to be written. If expiry of tokens is a requirement this will also need to be managed. [Guard](https:\/\/github.com\/appscode\/guard) was looked at in evaluating flows for webhook tokens.\\nOIDC tokens require passing `--oidc-*` arguments to the Kubernetes apiservers. However, the refreshing of short-lived tokens is [natively supported by `kubectl`](https:\/\/kubernetes.io\/docs\/reference\/access-authn-authz\/authentication\/#using-kubectl).\\n","completion":"## Decision\nWe will use OIDC.\\nWebhooks force a permanent inclusion of additional configuration, potentially additional application code to be maintained and don't offer the flexibility and security of OIDC tokens. X509 certificates come with all the complexities of managing certificates but offer no additional security or flexibility over OIDC.\\n"}
{"prompt":"## Context\\nAs documented in [ADR 0014](0014-access-the-delius-api-via-ndh.md) we are\\ngoing to access the Delius API via the NOMIS data hub using TLS mutual auth,\\nmanaged on our side by a sidecar container.\\nWe need to find a way of testing our auth setup - in particular that it only\\nallows access to the Delius API from our allocation manager. The NDH only\\nexists in production, so we need to decide how to test this setup and\\ndeployment of the sidecar in our other environments (at the moment we only\\nhave staging and production). We want staging to be as similar as possible to\\nproduction so that we're testing changes in a realistic environment before\\npushing them to production.\\n","completion":"## Decision\nWe will set up a separate namespace on the cloud platform for a Delius staging\\nenvironment.\\nWe will deploy the NDH-side mutual auth container into that environment, along\\nwith the Delius API with a dataset constructed to match T3 NOMIS data.\\nInitially our main concern is testing the mutual auth setup so instead of the\\nDelius API we could deploy a simple HTTP server if that's easier in the short\\nterm.\\nWe will deploy the sidecar in our staging environment and connect to our Delius\\nstaging environment from it.\\n"}
{"prompt":"## Context\\nAs part of the 'transfer summary' page, the user will see a 'consignment reference' to identify their consignment (which can be useful if a user needs to contact us about a specific consignment).\\nThe user will also be able to see the progress of multiple consignments on the 'transfer history' page, which will use the consignment reference to identify separate transfers.\\nThe TDR system already generates a UUID when a consignment is created, but this can be difficult for users to read or work with.\\nUtilising the `SeriesID` was considered to show the users, but one of the long-term goals for TDR is to remove the concepts of series', so introducing the user to these before removing them did not seem sensible.\\nDiscussions were also had about using the transferring body\/code to identify a consignment, but was decided against due to potential for these to change.\\nThe TDR team decided that it would be helpful to generate a separate reference for users to use to refer to consignments using a base25 alphabet.\\nWith the introduction of the base25, the TDR backend will also be able to use either the UUID or consignment reference to look up consignments, but users will exclusively see the consignment reference within TDR pages (though TDR **URLs** will still show the consignment UUID within, since these will be used by the TDR backend)\\nWe came to the decision that we liked the format:\\n`TDR-YEAR-BASE25REFERENCE`\\n`TDR` - since the consignment will be transferred using the TDR service\\n`YEAR` - year the consignment created (not completed) - as transfers may take weeks\/months rather than a set time period, so we do not know when a consignment will be completed.\\n`BASE25REFERENCE` - created once the consignment is assigned an incremental ID, which will be encoded to a base25 alpha-numeric code.\\nAn example of this could be:\\n`TDR-2020-MTB`\\nWhere `MTB` is the base25 encoding of the id number `6000`\\nBase25 was specifically chosen due to its ability to represent numeric identifiers through alpha-numeric codes, this is especially pertinent when it comes to longer numeric identifiers, as base25 can represent this in a more succinct and easy to read manner.\\nIt also allows for specific letters and numbers to be removed from the list of possible characters, limiting the possibility that a reference can be created that is un-intentionally offensive.\\nBase25 is also used within other TNA projects (referred to as Generated Catalogue Reference base25 or GCRb25), and TDR utilising it will keep references to a similar standard to those used within different projects.\\nThough we are not creating references to replace the references created by other services, they will continue to create their own that may be similar to the ones created by TDR.\\n","completion":"## Decision\nTo create the format of `TDR-YEAR-BASE25REFERENCE` we have made the following decisions:\\nThe PostgreSQL sequence is much more customisable to our needs and can be started from any specified value, which would be very helpful should any tables be dropped from the database. We would use this to create a bigInt incremental ID, with no-cycle to prevent it rolling back to the min-value once the max-value is reached.\\nUtilising the Omega library is a good way to start working on the encoding. If the project changes (like any open-source project can) we have the option to use the 2014 DRI code if needed.\\n"}
{"prompt":"## Context\\nEnsure good code quality with ease for change, integration and error correction.\\n","completion":"## Decision\nIt will be used the [PHPUnit](https:\/\/phpunit.de\/) as testing framework.\\n"}
{"prompt":"## Context\\nMartin Fowler: \"Event Sourcing ensures that all changes to application state are stored as a sequence of events. Not just can we query these events, we can also use the event log to reconstruct past states, and as a foundation to automatically adjust the state to cope with retroactive changes.\"\\nEvent sourcing persists the state of a business entity such an Order or a Customer as a sequence of state-changing events. Whenever the state of a business entity changes, a new event is appended to the list of events. Since saving an event is a single operation, it is inherently atomic. The application reconstructs an entity\u2019s current state by replaying the events.\\nA system is eventsourced when\\n- the single source of truth is a persisted history of the system\u2019s events;\\n- and that history is taken into account for enforcing constraints on new events.\\n","completion":"## Decision\nAll data on the ODH is available as a series of events. Events are published on the ODH topics and applications can process these events by creating a subscription on the topic.\\n"}
{"prompt":"## Context\\nAs any kind of server James needs to provide some level of consistencies.\\nStrong consistency can be achieved with Cassandra by relying on LightWeight transactions. This enables\\noptimistic transactions on a single partition key.\\nUnder the hood, Cassandra relies on the PAXOS algorithm to achieve consensus across replica allowing us\\nto achieve linearizable consistency at the entry level. To do so, Cassandra tracks consensus in a system.paxos\\ntable. This `system.paxos` table needs to be checked upon reads as well in order to ensure the latest state of the ongoing\\nconsensus is known. This can be achieved by using the SERIAL consistency level.\\nExperiments on a distributed James cluster (4 James nodes, having 4 CPU and 8 GB of RAM each, and a 3 node Cassandra\\ncluster of 32 GB of RAM, 8 CPUs, and SSD disks) demonstrated that the system.paxos table was by far the most read\\nand compacted table (ratio 5).\\nThe table triggering the most reads to the `system.paxos` table was the `acl` table. Deactivating LWT on this table alone\\n(lightweight transactions & SERIAL consistency level) enabled an instant 80% throughput, latencies reductions\\nas well as softer degradations when load breaking point is exceeded.\\n","completion":"## Decision\nRely on `event sourcing` to maintain a projection of ACLs that do not rely on LWT or SERIAL consistency level.\\nEvent sourcing is thus responsible for handling concurrency and race conditions as well as governing denormalization\\nfor ACLs. It can be used as a source of truth to re-build ACL projections.\\nNote that the ACL projection tables can end up being out of synchronization from the aggregate but we still have a\\nnon-questionable source of truth handled via event sourcing.\\n"}
{"prompt":"## Context\\nSince the first version of the service, fields which are associated with a matched offender have been stored with the defendant record. This includes fields such as ```probationStatus```, ``CRN`` and ```suspendedSentenceOrder```.\\nIt has become apparent that this has had several negative effects\\n* since we often have the same defendant appearing in multiple cases, there is a challenge to keep those offender-based fields consistent\\n* mixing of concepts around defendant and offender is confusing, a more natural model to adopt would be to have a separate ```Offender``` and have that as an optional relation to the ```Defendant```\\n* performance impacts\\n","completion":"## Decision\nIt has been decided to move the offender based fields from the Defendant to a new Offender entity at the level of the repository. This includes the following fields\\n* crn\\n* previouslyKnownTerminationDate\\n* suspendedSentenceOrder\\n* breach\\n* preSentenceActivity\\n* awaitingPsr\\n* probationStatus\\nCRN will be unique in the Offender table. Ultimately, all of these fields will be removed from the Defendant, except for CRN which will remain to operate as the foreign key field.\\nNote that whilst PNC is related to an Offender, we will keep it in Defendant because it is sent in the LIBRA and Common Platform feeds and we need to keep the original value as it was supplied.\\nREST-based interfaces will not be altered to ensure that clients will not need to change.\\n"}
{"prompt":"## Context\\nBased on ADRs and developer need, we need to create a system that is light weight, fully managed and to integrate into.\\n","completion":"## Decision\nTo run in a fully managed AWS cloud environment using Terraform to manage it.\\n"}
{"prompt":"## Context and Problem Statement\\n[Describe the context and problem statement, for example, in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n","completion":"## Decision\n* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. For example, only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best].\\n### Positive Consequences <!-- optional -->\\n* [For example, improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [For example, compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n"}
{"prompt":"## Context\\n[ICS 26 - Routing Module](https:\/\/github.com\/cosmos\/ics\/tree\/master\/spec\/ics-026-routing-module) defines a function [`handlePacketRecv`](https:\/\/github.com\/cosmos\/ics\/tree\/master\/spec\/ics-026-routing-module#packet-relay).\\nIn ICS 26, the routing module is defined as a layer above each application module\\nwhich verifies and routes messages to the destination modules. It is possible to\\nimplement it as a separate module, however, we already have functionality to route\\nmessages upon the destination identifiers in the baseapp. This ADR suggests\\nto utilize existing `baseapp.router` to route packets to application modules.\\nGenerally, routing module callbacks have two separate steps in them,\\nverification and execution. This corresponds to the `AnteHandler`-`Handler`\\nmodel inside the SDK. We can do the verification inside the `AnteHandler`\\nin order to increase developer ergonomics by reducing boilerplate\\nverification code.\\nFor atomic multi-message transaction, we want to keep the IBC related\\nstate modification to be preserved even the application side state change\\nreverts. One of the example might be IBC token sending message following with\\nstake delegation which uses the tokens received by the previous packet message.\\nIf the token receiving fails for any reason, we might not want to keep\\nexecuting the transaction, but we also don't want to abort the transaction\\nor the sequence and commitment will be reverted and the channel will be stuck.\\nThis ADR suggests new `CodeType`, `CodeTxBreak`, to fix this problem.\\n","completion":"## Decision\n`PortKeeper` will have the capability key that is able to access only the\\nchannels bound to the port. Entities that hold a `PortKeeper` will be\\nable to call the methods on it which are corresponding with the methods with\\nthe same names on the `ChannelKeeper`, but only with the\\nallowed port. `ChannelKeeper.Port(string, ChannelChecker)` will be defined to\\neasily construct a capability-safe `PortKeeper`. This will be addressed in\\nanother ADR and we will use insecure `ChannelKeeper` for now.\\n`baseapp.runMsgs` will break the loop over the messages if one of the handlers\\nreturns `!Result.IsOK()`. However, the outer logic will write the cached\\nstore if `Result.IsOK() || Result.Code.IsBreak()`. `Result.Code.IsBreak()` if\\n`Result.Code == CodeTxBreak`.\\n```go\\nfunc (app *BaseApp) runTx(tx Tx) (result Result) {\\nmsgs := tx.GetMsgs()\\n\/\/ AnteHandler\\nif app.anteHandler != nil {\\nanteCtx, msCache := app.cacheTxContext(ctx)\\nnewCtx, err := app.anteHandler(anteCtx, tx)\\nif !newCtx.IsZero() {\\nctx = newCtx.WithMultiStore(ms)\\n}\\nif err != nil {\\n\/\/ error handling logic\\nreturn res\\n}\\nmsCache.Write()\\n}\\n\/\/ Main Handler\\nrunMsgCtx, msCache := app.cacheTxContext(ctx)\\nresult = app.runMsgs(runMsgCtx, msgs)\\n\/\/ BEGIN modification made in this ADR\\nif result.IsOK() || result.IsBreak() {\\n\/\/ END\\nmsCache.Write()\\n}\\nreturn result\\n}\\n```\\nThe Cosmos SDK will define an `AnteDecorator` for IBC packet receiving. The\\n`AnteDecorator` will iterate over the messages included in the transaction, type\\n`switch` to check whether the message contains an incoming IBC packet, and if so\\nverify the Merkle proof.\\n```go\\ntype ProofVerificationDecorator struct {\\nclientKeeper ClientKeeper\\nchannelKeeper ChannelKeeper\\n}\\nfunc (pvr ProofVerificationDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (Context, error) {\\nfor _, msg := range tx.GetMsgs() {\\nvar err error\\nswitch msg := msg.(type) {\\ncase client.MsgUpdateClient:\\nerr = pvr.clientKeeper.UpdateClient(msg.ClientID, msg.Header)\\ncase channel.MsgPacket:\\nerr = pvr.channelKeeper.RecvPacket(msg.Packet, msg.Proofs, msg.ProofHeight)\\ncase chanel.MsgAcknowledgement:\\nerr = pvr.channelKeeper.AcknowledgementPacket(msg.Acknowledgement, msg.Proof, msg.ProofHeight)\\ncase channel.MsgTimeoutPacket:\\nerr = pvr.channelKeeper.TimeoutPacket(msg.Packet, msg.Proof, msg.ProofHeight, msg.NextSequenceRecv)\\ncase channel.MsgChannelOpenInit;\\nerr = pvr.channelKeeper.CheckOpen(msg.PortID, msg.ChannelID, msg.Channel)\\ndefault:\\ncontinue\\n}\\nif err != nil {\\nreturn ctx, err\\n}\\n}\\nreturn next(ctx, tx, simulate)\\n}\\n```\\nWhere `MsgUpdateClient`, `MsgPacket`, `MsgAcknowledgement`, `MsgTimeoutPacket`\\nare `sdk.Msg` types correspond to `handleUpdateClient`, `handleRecvPacket`,\\n`handleAcknowledgementPacket`, `handleTimeoutPacket` of the routing module,\\nrespectively.\\nThe side effects of `RecvPacket`, `VerifyAcknowledgement`,\\n`VerifyTimeout` will be extracted out into separated functions,\\n`WriteAcknowledgement`, `DeleteCommitment`, `DeleteCommitmentTimeout`, respectively,\\nwhich will be called by the application handlers after the execution.\\n`WriteAcknowledgement` writes the acknowledgement to the state that can be\\nverified by the counter-party chain and increments the sequence to prevent\\ndouble execution. `DeleteCommitment` will delete the commitment stored,\\n`DeleteCommitmentTimeout` will delete the commitment and close channel in case\\nof ordered channel.\\n```go\\nfunc (keeper ChannelKeeper) WriteAcknowledgement(ctx Context, packet Packet, ack []byte) {\\nkeeper.SetPacketAcknowledgement(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence(), ack)\\nkeeper.SetNextSequenceRecv(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitment(ctx Context, packet Packet) {\\nkeeper.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitmentTimeout(ctx Context, packet Packet) {\\nk.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\nif channel.Ordering == types.ORDERED [\\nchannel.State = types.CLOSED\\nk.SetChannel(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), channel)\\n}\\n}\\n```\\nEach application handler should call respective finalization methods on the `PortKeeper`\\nin order to increase sequence (in case of packet) or remove the commitment\\n(in case of acknowledgement and timeout).\\nCalling those functions implies that the application logic has successfully executed.\\nHowever, the handlers can return `Result` with `CodeTxBreak` after calling those methods\\nwhich will persist the state changes that has been already done but prevent any further\\nmessages to be executed in case of semantically invalid packet. This will keep the sequence\\nincreased in the previous IBC packets(thus preventing double execution) without\\nproceeding to the following messages.\\nIn any case the application modules should never return state reverting result,\\nwhich will make the channel unable to proceed.\\n`ChannelKeeper.CheckOpen` method will be introduced. This will replace `onChanOpen*` defined\\nunder the routing module specification. Instead of define each channel handshake callback\\nfunctions, application modules can provide `ChannelChecker` function with the `AppModule`\\nwhich will be injected to `ChannelKeeper.Port()` at the top level application.\\n`CheckOpen` will find the correct `ChennelChecker` using the\\n`PortID` and call it, which will return an error if it is unacceptable by the application.\\nThe `ProofVerificationDecorator` will be inserted to the top level application.\\nIt is not safe to make each module responsible to call proof verification\\nlogic, whereas application can misbehave(in terms of IBC protocol) by\\nmistake.\\nThe `ProofVerificationDecorator` should come right after the default sybil attack\\nresistent layer from the current `auth.NewAnteHandler`:\\n```go\\n\/\/ add IBC ProofVerificationDecorator to the Chain of\\nfunc NewAnteHandler(\\nak keeper.AccountKeeper, supplyKeeper types.SupplyKeeper, ibcKeeper ibc.Keeper,\\nsigGasConsumer SignatureVerificationGasConsumer) sdk.AnteHandler {\\nreturn sdk.ChainAnteDecorators(\\nNewSetUpContextDecorator(), \/\/ outermost AnteDecorator. SetUpContext must be called first\\n...\\nNewIncrementSequenceDecorator(ak),\\nibcante.ProofVerificationDecorator(ibcKeeper.ClientKeeper, ibcKeeper.ChannelKeeper), \/\/ innermost AnteDecorator\\n)\\n}\\n```\\nThe implementation of this ADR will also create a `Data` field of the `Packet` of type `[]byte`, which can be deserialised by the receiving module into its own private type. It is up to the application modules to do this according to their own interpretation, not by the IBC keeper.  This is crucial for dynamic IBC.\\nExample application-side usage:\\n```go\\ntype AppModule struct {}\\n\/\/ CheckChannel will be provided to the ChannelKeeper as ChannelKeeper.Port(module.CheckChannel)\\nfunc (module AppModule) CheckChannel(portID, channelID string, channel Channel) error {\\nif channel.Ordering != UNORDERED {\\nreturn ErrUncompatibleOrdering()\\n}\\nif channel.CounterpartyPort != \"bank\" {\\nreturn ErrUncompatiblePort()\\n}\\nif channel.Version != \"\" {\\nreturn ErrUncompatibleVersion()\\n}\\nreturn nil\\n}\\nfunc NewHandler(k Keeper) Handler {\\nreturn func(ctx Context, msg Msg) Result {\\nswitch msg := msg.(type) {\\ncase MsgTransfer:\\nreturn handleMsgTransfer(ctx, k, msg)\\ncase ibc.MsgPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handlePacketDataTransfer(ctx, k, msg, data)\\ncase ibc.MsgTimeoutPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handleTimeoutPacketDataTransfer(ctx, k, packet)\\n\/\/ interface { PortID() string; ChannelID() string; Channel() ibc.Channel }\\n\/\/ MsgChanInit, MsgChanTry implements ibc.MsgChannelOpen\\ncase ibc.MsgChannelOpen:\\nreturn handleMsgChannelOpen(ctx, k, msg)\\n}\\n}\\n}\\nfunc handleMsgTransfer(ctx Context, k Keeper, msg MsgTransfer) Result {\\nerr := k.SendTransfer(ctx,msg.PortID, msg.ChannelID, msg.Amount, msg.Sender, msg.Receiver)\\nif err != nil {\\nreturn sdk.ResultFromError(err)\\n}\\nreturn sdk.Result{}\\n}\\nfunc handlePacketDataTransfer(ctx Context, k Keeper, packet Packet, data PacketDataTransfer) Result {\\nerr := k.ReceiveTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ TODO: Source chain sent invalid packet, shutdown channel\\n}\\nk.ChannelKeeper.WriteAcknowledgement([]byte{0x00}) \/\/ WriteAcknowledgement increases the sequence, preventing double spending\\nreturn sdk.Result{}\\n}\\nfunc handleCustomTimeoutPacket(ctx Context, k Keeper, packet CustomPacket) Result {\\nerr := k.RecoverTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ This chain sent invalid packet or cannot recover the funds\\npanic(err)\\n}\\nk.ChannelKeeper.DeleteCommitmentTimeout(ctx, packet)\\n\/\/ packet timeout should not fail\\nreturn sdk.Result{}\\n}\\nfunc handleMsgChannelOpen(sdk.Context, k Keeper, msg MsgOpenChannel) Result {\\nk.AllocateEscrowAddress(ctx, msg.ChannelID())\\nreturn sdk.Result{}\\n}\\n```\\n"}
{"prompt":"## Context\\nMoJ Cloud Platform team has decided to use [kubernetes for container management platform](https:\/\/github.com\/ministryofjustice\/cloud-platform\/blob\/master\/architecture-decision-record\/004-use-kubernetes-for-container-management.md) following the outcome of MOJ Digital's approach to infrastructure management. The team needed the below features for the infrastructure management:\\n- An universal authentication mechanism to manage users without depending on the cloud provider\\n- Able to customize the control plane of kubernetes for MOJ requirement on Pod security\\n- Able to integrate external tools easily\\n- Able to manage and configure the control plane for any custom changes\\n","completion":"## Decision\nThere are several leading cloud providers who provide managed production-ready kubernetes cluster:\\n- Amazon Elastic Kubernetes Service (Amazon EKS)\\n- Azure Kubernetes Service (AKS)\\n- Google Kubernetes Engine (GKE)\\nWe decided to host our cluster on AWS because our service team has good development experience working with AWS services. This made it easier for teams to migrate to the kubernetes platform\\nWe decided to manage the kubernetes cluster ourselves rather than using EKS mainly for the below reasons:\\n- When the time MOJ needed to build the kubernetes, Amazon EKS was still in the Alpha stage and was not production ready. Also Amazon EKS require to use IAM for user authentication which will be an overhead for managing users of service teams\\n- Kubernetes(k8s) allows to authenticate using OIDC and therefore it was easy to manage the authentication externally using Auth0\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n"}
{"prompt":"## Context\\nEach event recorded by Pottery (aka \"shard\") is a short piece of text.  Maybe with links, or basic formatting.\\nPottery is used on the command line, so shards need to be generated by Unix command line tools, readable on the terminal and edited in a text editor.\\n","completion":"## Decision\nThe content of each shard will be in Markdown format.\\n"}
{"prompt":"## Context\\nWe have a large amount of data which is needed to run the Terraform projects.\\nSome of the data we need to use is sensitive (e.g. RDS passwords). It is considered best practice to keep sensitive information encrypted in private repositories. This means that access can be controlled and the that the data will remain secure, even if it is accidentally published.\\n","completion":"## Decision\nThe `terraform\/data` directory will be split out into its own repository that will be kept private and all sensitive data within it will be encrypted using [sops](https:\/\/github.com\/mozilla\/sops).\\n"}
{"prompt":"## Context\\nThere are quite a few options when choosing a platform and infrastructure patterns for managing applications. Across Mozilla teams, services & applications are already overwhelmingly containerized using Docker. We can deploy these containerized applications onto a number of options, including but not limited to multiple Kubernetes services or self-hosted routes.\\n","completion":"## Decision\n* We will support Dockerized applications primarily, with some flexibility given to application components that greatly benefit from serverless architectures.\\n* Dockerized applications are deployed to Kubernetes, defaulting to a Web SRE shared applications clusters if not requiring their own Kubernetes cluster.\\n* We will use AWS' Elastic Kubernetes Service to manage our Kubernetes clusters, unless there is a documented reason for a particular project not to.\\n* We will deploy our EKS clusters using Terraform & our internal Terraform module, unless there is a documented reason for a particular project not to.\\n* Any divergences from the above decisions must be clearly documented in the project's or application's service documentation.\\n"}
{"prompt":"## Context and Problem Statement\\nMaking a node wrapper for Mozilla Deepseepch, in `deepspeech-node-wrapper`.\\nUsing Mozilla node `deepspeech` module and their example [`DeepSpeech\/examples\/nodejs_wav`](https:\/\/github.com\/mozilla\/DeepSpeech-examples\/tree\/r0.6\/nodejs_wav) as a starting point.\\nThe question this ADR explore is, how to package the the STT models in this npm module?\\n## Decision Drivers <!-- optional -->\\n- Easy to reason around\\n- Avoid adding large binaries to git repository\\n- Avoid adding large binaries to NPM\\n- ease of use and setup\\n- considerate of slow internet connections\\n- \u2026 <!-- numbers of drivers can vary -->\\n","completion":"## Decision\n- Easy to reason around\\n- Avoid adding large binaries to git repository\\n- Avoid adding large binaries to NPM\\n- ease of use and setup\\n- considerate of slow internet connections\\n- \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\nLeaning torwards option 3.\\n<!-- ### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n-->\\n"}
{"prompt":"## Context\\nThe Engaging Crowds project (UK collaboration with The National Archives, Royal Museum Greenwich, Royal Botanical Gardens Edinburgh) involves building an indexing tool to examine levels of user engagement and agency. The indexing tool will be built on the new classifier in the front-end monorepo (FEM). This tool will allow volunteers to select a subject-set or a subject to classify on. All 3 Project Builder projects being built to test the use of the indexing tool will require Dropdown tasks. The Dropdown task is not currently in the Front-end monorepo. We have two options to support the requirements  of these projects: using the existing dropdown task available in Panoptes-Front-End (PFE), the legacy single-page front-end app and build the new indexing tool in the classifier in the FEM or build both the dropdown task and the indexing tool in the FEM. Building the dropdown task in the new classifier in the FEM is something we need to do eventually anyway and we have the opportunity to evaluate how the existing dropdown task functions and how we may want to change it.\\nKnown issues for the dropdown task in PFE include:\\n- Long selection lists, particularly if there are multiple for cascading select options, can create massive task objects on the workflow resource resulting in slow loading and browser performance and slow exports.\\n- The annotations are machine readable unique identifier strings to support cascading dropdowns. Machine readable annotations make analyzing the post-classification extraction and aggregation more complicated, particularly because of workflow versioning and translations. Caesar does not store the workflow task contents and so project owners have to reference the original workflow task by version in an export to get meaningful aggregations for actual study.\\n- The dropdown task also allows for user submitted values which essentially adds in a text task into the dropdown task. The annotation includes a boolean which, if false, lets the aggregation code know that this annotation should no longer aggregate as a dropdown task, but as a text task.\\n### Sample task and annotation JSON from PFE dropdown task\\n**PFE dropdown task structure**\\n``` json\\n{\\n\"T0\":{\\n\"help\":\"\",\\n\"next\":\"T1\",\\n\"type\":\"dropdown\",\\n\"selects\":[\\n{\\n\"id\":\"070b610fbf5d9\",\\n\"title\":\"Favourite colour\",\\n\"options\":{\\n\"*\":[\\n{\\n\"label\":\"Red\",\\n\"value\":\"hashed-value-R\"\\n}\\n]\\n},\\n\"required\":false,\\n\"allowCreate\":false\\n}\\n],\\n\"instruction\":\"Choose your favourite things\"\\n}\\n}\\n```\\n**PFE dropdown task annotation structure**\\n```json\\n{\\n\"annotations\":[\\n{\\n\"task\":\"T0\",\\n\"value\":[\\n{\\n\"value\":\"hashed-value-R\",\\n\"option\":true\\n}\\n]\\n}\\n]\\n}\\n```\\n**PFE dropdown task translation strings**\\n```json\\n{\\n\"tasks.T0.help\": \"Pick a colour from the menu.\",\\n\"tasks.T0.instruction\": \"Choose your favourite things.\",\\n\"tasks.T0.selects.0.title\": \"Favourite colour\",\\n\"tasks.T0.selects.0.options.*.0.label\": \"Red\",\\n\"tasks.T0.selects.0.options.*.1.label\": \"Blue\"\\n}\\n```\\n","completion":"## Decision\nWe will develop a simplified dropdown task in the new classifier in the FEM. Creating a simple dropdown task contains the following functionality:\\n- Limited dropdown list options of a minimum (4) and of a maximum (20) number of options (justification: if less than 4, this can be a single choice task using radio buttons)\\n- No cascading, dependencies, or effect on other select inputs\\n- No free-text entry on the dropdown. It will be recommended to project builders use the new [workflow steps](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/docs\/arch\/adr-05.md) feature to have a dropdown task and text task in a single step.\\nMore complex dropdown tasks will be built in the future as separate task types based on analysis of actual usage in PFE. These will include:\\n- Cascading dropdowns for\\n- Locations\\n- Custom (TBD)\\n- Date picker\\n- Possibly a text input with validation rather than dropdowns\\n- Asynchronous loading long lists by text input search\\n### Proposed task and annotation JSON structure examples\\n**Task**\\n``` json\\n{\\n\"T0\":{\\n\"help\": \"\",\\n\"type\": \"dropdown-simple\",\\n\"options\":[\\n\"Red\",\\n\"Blue\",\\n\"Green\",\\n\"Yellow\"\\n],\\n\"required\": false,\\n\"allowCreate\": false,\\n\"instruction\": \"Choose your favourite colour\"\\n}\\n}\\n```\\n**Annotation**\\n```json\\n{\\n\"annotations\": [\\n{\\n\"task\":\"T0\",\\n\"value\": {\\n\"value\": 1,\\n\"option\": true\\n}\\n}\\n]\\n}\\n```\\n"}
{"prompt":"## Context\\nWe hereby define as an artifact compiled artifact that external people consumes. This includes:\\n- libraries\\n- Mail servers\\n- Extensions for James Mail Servers\\n- Command line tools\\nWe designate as a feature an optional, opt-in behaviour of a James server that can be configured by\\nuser willing to rely on it.\\nJames as a project delivers several artifacts, and features. In order for project users to better\\nunderstand the underlying quality of the artifact they use, as well as the level of risk associated,\\nwe need to better define some quality levels.\\n","completion":"## Decision\nFor a given artifact or feature, by **mature** we mean that:\\n- *interfaces* in components need a contract test suite\\n- *interfaces* have several implementations\\n- *implementation* of these interfaces need to pass this contract test suite which provides unit tests\\n- Decent integration tests coverage is needed\\n- Performance tests need to be conducted out\\n- Quality Assurance with external clients needs to be conducted out\\n- known existing production deployments\/usages\\n- usable documentation\\nThis is the maximum quality level delivered by the James project. Users should feel confident using these\\nartifacts or features.\\nBy **experimental** we designate an artifact or feature not matching yet the above requirements. However some\\nactive contributors are willing to raise the quality level of this component, and eventually make it\\nmature. Or at least are willing to support users.\\nUsers should have low expectations regarding experimental artifacts or features. They are encouraged to contribute to them\\nin order to raise its quality.\\nBy **unsupported** we mean that an artifact or feature do not match most of the *mature* quality conditions. Active\\ncontributors do not feel confident delivering support for it. This artifact or feature might be deprecated and\\nremoved from future James releases. Users are strongly encouraged to contribute to the artifact development.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context\\nThe service receives case messages from the SQS queue `court-case-matcher-queue`. These messages, being produced from an SNS subscription, are embedded in JSON containing metadata about the message. It is possible to remove this metadata through configuration of AWS, thereby allowing for easier processing of the case itself in court-case-matcher or to retain it.\\nThe metadata includes the following fields\\n* `messageId`\\n* `topicArn`\\n* `timestamp`\\n","completion":"## Decision\nWe have decided to retain the message metadata. There is no immediate use for the fields at but the cost of processing is low and there's a possibility that `messageId` will be useful for tracing.\\n### Links\\nDescription of the terraform field for enabling delivery of the message without metadata.\\nhttps:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/sns_topic_subscription#raw_message_delivery\\n"}
{"prompt":"## Context\\nWe need to lint our Ruby code, the team has previous experience working with\\nRuboCop.\\n","completion":"## Decision\nWe will use RuboCop for linting our Ruby code\\n"}
{"prompt":"## Context\\nWe have just enough features in the repo to see the shape it will need to take.\\n","completion":"## Decision\nWe'll aim our refactoring towards a hexagonal architecture, using commands and events as the way to communicate with the core domain.\\nSomething like this:\\n![diagram](.\/0008-hexagonal-architecture.jpg)\\nKey to the diagram:\\n- Each box represents at least one yarn\/npm package.\\n- Arrows, where drawn, indicate a dependency.\\n- Green indicates where we've already created and are building this package.\\n- Amber indicates where we have code for this, but it still needs to be factored out into this design.\\n- Hand-drawn lines indicate a bounded context.\\nOther agreements:\\n- Communication into \/ out of the core domain will be via commands and domain events.\\n- Commands going into the domain will all be asynchronous. The application layer (client port) can abstract this away by watching for domain events that indicate when the command has completed.\\n- We won't use event sourcing for the domain models, but we will probably persist domain events in a log.\\n- The client port will mediate queries to go straight into git, e.g. for streams of files.\\n- The web may keep its own cache of read models, shaped in the representation needed for its API. Hence the separate bounded context.\\n"}
{"prompt":"## Context\\nThe [NHS's Syndicated Content](http:\/\/www.nhs.uk\/aboutNHSChoices\/professionals\/syndication\/Pages\/Webservices.aspx)\\nholds information about GP surgeries.\\nSeveral of Connecting to Services applications need to make use of GP data, including the\\n[GP Finder](https:\/\/github.com\/nhsuk\/gp-finder) and [Profiles](https:\/\/github.com\/nhsuk\/profiles) projects.\\nThe Syndication API is not intended for direct access in production environments.\\nThe Syndication API returns data in XML, and information is spread across several subpages,\\ni.e. overview, services, facilities.\\n","completion":"## Decision\nThe syndication XML API should be scraped nightly to create a local copy of the data.\\nWhat were multiple pages for a practice on the Syndication API will be merged into a single record per practice and\\nconverted into the Connecting to Services teams preferred format, JSON.\\n"}
{"prompt":"## Context\\nSitting directly on the `\/find-a-psychological-therapies-service` path is not in keeping with the organisations plans for\\ninformation architecture going forward. When the service moves from beta.nhs.uk to nhs.uk we want\\nit to sit on path consistent with other finder services.\\n","completion":"## Decision\nThe site root will be changed to be `\/service-search\/find-a-psychological-therapies-service`\\n"}
{"prompt":"## Context\\nIt's hard to test components in isolation. Other components are almost always also tested, which is intended because it's the nature of end-to-end tests being workflow-based.\\nThere are currently three E2E projects that are maintained separately. There are a lot of duplicated commands and different variations of them.\\n","completion":"## Decision\nWe'll merge all cypress e2e projects of platform into a single project.\\nThe projects will be merged by\\n- creating new project `E2E` in `tests\/E2E`\\n- moving storefront tests to `tests\/E2E\/cypress\/integration\/storefront`\\n- moving administration tests to `tests\/E2E\/cypress\/integration\/administration`\\n- moving recovery tests to `tests\/E2E\/cypress\/integration\/recovery`\\n- moving the new package test scenarios to `tests\/E2E\/cypress\/integration\/scenarios`\\n- merging the commands.js files and removing duplicate code\\n- merging the setup code\\n- merging fixtures\\n- use automatic cleanup in global setup instead of manual calls to `cleanUpPreviousState` in admin tests\\n"}
{"prompt":"## Context\\nWe run unit-e as an open source project. At the beginning of the project the git\\nrepositories were kept private to the initial team in order to first get to a\\nstate were it makes sense to get a broader audience involved. With the start of\\nthe testnet, reflected in the [0.1\\nmilestone](https:\/\/github.com\/dtr-org\/unit-e\/milestone\/11), we are reaching this\\nstate. So we will open the initial set of repos and make them public on GitHub.\\nWe have mostly worked on the repositories as if they would already be public, so\\nwe should be in a good shape there, but it's still important to be clear about\\nwhat will be made public because it changes the visibility of our work and we\\nhave a chance for a final review of what will be made public.\\nThe purpose of this ADR is twofold:\\nFirst, it's meant to make it clear on which repositories we have agreed to make\\npublic in the first step.\\nSecond, it's supposed to get the awareness and buy-in from the whole team what\\nwill be made public. So we can use the review process of this ADR to get the\\napproval of everybody that the repositories are good to be made public.\\n","completion":"## Decision\nWe will make the following GitHub projects public on the date of the opening:\\n* https:\/\/github.com\/dtr-org\/unit-e\\n* https:\/\/github.com\/dtr-org\/uips\\n* https:\/\/github.com\/dtr-org\/unit-e-clonemachine\\n* https:\/\/github.com\/dtr-org\/docs.unit-e.io\\n* https:\/\/github.com\/dtr-org\/unit-e-project\\nThe exact date of the opening is not subject of this ADR. This will be decided\\nby the team when everything is ready.\\nEach member of the team will approve this ADR to signal that they are fine with\\nopening the listed repositories, and that they are not aware of anything in\\nthese repos which should not be published.\\n"}
{"prompt":"## Context\\nWe use [Sentry](https:\/\/sentry.io) to provide centralised JavaScript error logging for the project and content pages apps. Sentry supports versioned releases, and GitHub integration, so that fixes can be linked to releases and new issues can be linked to potentially bad commits.\\n","completion":"## Decision\nThe monorepo will be versioned in Sentry, using the git commit SHA to version a release. New releases are deployed to staging on each push to master. A release is finalised and deployed to production when the production-release tag is updated to point to that release. [#1599](https:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/1599) and [#1601](https:\/\/github.com\/zooniverse\/front-end-monorepo\/pull\/1601) implement this using GitHub actions.\\n"}
{"prompt":"## Context\\nCurrently the `Commit` structure contains a lot of potentially redundant or unnecessary data.\\nIt contains a list of precommits from every validator, where the precommit\\nincludes the whole `Vote` structure. Thus each of the commit height, round,\\ntype, and blockID are repeated for every validator, and could be deduplicated,\\nleading to very significant savings in block size.\\n```\\ntype Commit struct {\\nBlockID    BlockID `json:\"block_id\"`\\nPrecommits []*Vote `json:\"precommits\"`\\n}\\ntype Vote struct {\\nValidatorAddress Address   `json:\"validator_address\"`\\nValidatorIndex   int       `json:\"validator_index\"`\\nHeight           int64     `json:\"height\"`\\nRound            int       `json:\"round\"`\\nTimestamp        time.Time `json:\"timestamp\"`\\nType             byte      `json:\"type\"`\\nBlockID          BlockID   `json:\"block_id\"`\\nSignature        []byte    `json:\"signature\"`\\n}\\n```\\nThe original tracking issue for this is [#1648](https:\/\/github.com\/tendermint\/tendermint\/issues\/1648).\\nWe have discussed replacing the `Vote` type in `Commit` with a new `CommitSig`\\ntype, which includes at minimum the vote signature. The `Vote` type will\\ncontinue to be used in the consensus reactor and elsewhere.\\nA primary question is what should be included in the `CommitSig` beyond the\\nsignature. One current constraint is that we must include a timestamp, since\\nthis is how we calculuate BFT time, though we may be able to change this [in the\\nfuture](https:\/\/github.com\/tendermint\/tendermint\/issues\/2840).\\nOther concerns here include:\\n- Validator Address [#3596](https:\/\/github.com\/tendermint\/tendermint\/issues\/3596) -\\nShould the CommitSig include the validator address? It is very convenient to\\ndo so, but likely not necessary. This was also discussed in [#2226](https:\/\/github.com\/tendermint\/tendermint\/issues\/2226).\\n- Absent Votes [#3591](https:\/\/github.com\/tendermint\/tendermint\/issues\/3591) -\\nHow to represent absent votes? Currently they are just present as `nil` in the\\nPrecommits list, which is actually problematic for serialization\\n- Other BlockIDs [#3485](https:\/\/github.com\/tendermint\/tendermint\/issues\/3485) -\\nHow to represent votes for nil and for other block IDs? We currently allow\\nvotes for nil and votes for alternative block ids, but just ignore them\\n","completion":"## Decision\nDeduplicate the fields and introduce `CommitSig`:\\n```\\ntype Commit struct {\\nHeight  int64\\nRound   int\\nBlockID    BlockID      `json:\"block_id\"`\\nPrecommits []CommitSig `json:\"precommits\"`\\n}\\ntype CommitSig struct {\\nBlockID  BlockIDFlag\\nValidatorAddress Address\\nTimestamp time.Time\\nSignature []byte\\n}\\n\/\/ indicate which BlockID the signature is for\\ntype BlockIDFlag int\\nconst (\\nBlockIDFlagAbsent BlockIDFlag = iota \/\/ vote is not included in the Commit.Precommits\\nBlockIDFlagCommit                    \/\/ voted for the Commit.BlockID\\nBlockIDFlagNil                       \/\/ voted for nil\\n)\\n```\\nRe the concerns outlined in the context:\\n**Timestamp**: Leave the timestamp for now. Removing it and switching to\\nproposer based time will take more analysis and work, and will be left for a\\nfuture breaking change. In the meantime, the concerns with the current approach to\\nBFT time [can be\\nmitigated](https:\/\/github.com\/tendermint\/tendermint\/issues\/2840#issuecomment-529122431).\\n**ValidatorAddress**: we include it in the `CommitSig` for now. While this\\ndoes increase the block size unecessarily (20-bytes per validator), it has some ergonomic and debugging advantages:\\n- `Commit` contains everything necessary to reconstruct `[]Vote`, and doesn't depend on additional access to a `ValidatorSet`\\n- Lite clients can check if they know the validators in a commit without\\nre-downloading the validator set\\n- Easy to see directly in a commit which validators signed what without having\\nto fetch the validator set\\nIf and when we change the `CommitSig` again, for instance to remove the timestamp,\\nwe can reconsider whether the ValidatorAddress should be removed.\\n**Absent Votes**: we include absent votes explicitly with no Signature or\\nTimestamp but with the ValidatorAddress. This should resolve the serialization\\nissues and make it easy to see which validator's votes failed to be included.\\n**Other BlockIDs**: We use a single byte to indicate which blockID a `CommitSig`\\nis for. The only options are:\\n- `Absent` - no vote received from the this validator, so no signature\\n- `Nil` - validator voted Nil - meaning they did not see a polka in time\\n- `Commit` - validator voted for this block\\nNote this means we don't allow votes for any other blockIDs. If a signature is\\nincluded in a commit, it is either for nil or the correct blockID. According to\\nthe Tendermint protocol and assumptions, there is no way for a correct validator to\\nprecommit for a conflicting blockID in the same round an actual commit was\\ncreated. This was the consensus from\\n[#3485](https:\/\/github.com\/tendermint\/tendermint\/issues\/3485)\\nWe may want to consider supporting other blockIDs later, as a way to capture\\nevidence that might be helpful. We should clarify if\/when\/how doing so would\\nactually help first. To implement it, we could change the `Commit.BlockID`\\nfield to a slice, where the first entry is the correct block ID and the other\\nentries are other BlockIDs that validators precommited before. The BlockIDFlag\\nenum can be extended to represent these additional block IDs on a per block\\nbasis.\\n"}
{"prompt":"## Context\\nUnbalanced metabolites and enzyme concentrations are boundary conditions for our ODE\\nmodel. Experimental conditions are defined with respect to these values and drains,\\nwhich are already defined as priors. Therefore, our prior knowledge about the\\nmetabolic phenotype is defined as what is measured about the boundary conditions. This\\ndecision aims to shift the measurements of the enzymes and unbalanced metabolites from\\nthe likelihood evaluations to the prior information.\\nThe benefit of treating priors in this way is that we define a prior on the phenotype\\nrather than all possible phenotypes. However, boundary conditions that are unmeasured\\nare still considered using weakly informative priors (read: within biologically relevant\\nboundaries).\\n","completion":"## Decision\nUnbalanced metabolites and enzyme concentrations can also be considered as prior distributions\\nrather than likelihood evaluations.\\n"}
{"prompt":"## Context\\nIt is necessary for breadcrumb navigation to be supported and configured in a consistent way across the application\\n","completion":"## Decision\nThe recommended solution is to use the `loaf` library - which is already used in several other DfE repositories as the go-to solution for breadcrumb management:\\n- https:\/\/github.com\/DFE-Digital\/academy-transfers-frontend\\n- https:\/\/github.com\/DFE-Digital\/get-into-teaching-app\\n- https:\/\/github.com\/DFE-Digital\/npd-find-and-explore\\n"}
{"prompt":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","completion":"## Decision\nThe change that we're proposing or have agreed to implement.\\n"}
{"prompt":"## Context\\nThis repository could potentially contain multiple solutions that consist of multiple projects. For this, a build system is needed that is capable of creating multiple assemblies or executables. Some of those projects may require more advanced tooling because of the need to target multiple platforms or because of the requirement of a different dev stack (eg front end development). Thus it could be interesting to look at some build tools. Tools I considered using:\\n- Makefile\\n- Fake\\n- Cake\\n- Nuke\\n- Npm\\n- Yarn\\n- Webpack\\n","completion":"## Decision\nThe idea is to keep things simple and easy to use by leveraging build tooling provided by dotnet. This means that `csproj` will be leveraged as much as possible. Any TypeScript related tools should be called via MSBuild targets. This makes sure that it's easy to get up and running via a CLI or an IDE.\\nFor the front-end and TypeScript related projects, the decision is made to use `Yarn` to manage and install packages and `Webpack` to bundle any assets. Yarn installs and Webpack builds should be mainly called from MSBuild.\\nA general Makefile will be provided for Unix environments to ease CLI based development. This way many dotnet CLI commands can be orchestrated together. The plan is to only provide this for Unix based environments no such efforts will be made to add a higher level orchestration for Windows.\\nWhen extra commands are needed to bootstrap certain parts of the application those should then be thoroughly documented in the `README.md` file. We should try to avoid this as much as possible and when needed try to put measures in place to move away from this extra step.\\n"}
{"prompt":"## Context\\nIn the light client (or any other client), the user may want to **subscribe to\\na subset of transactions** (rather than all of them) using `\/subscribe?event=X`. For\\nexample, I want to subscribe for all transactions associated with a particular\\naccount. Same for fetching. The user may want to **fetch transactions based on\\nsome filter** (rather than fetching all the blocks). For example, I want to get\\nall transactions for a particular account in the last two weeks (`tx's block time >= '2017-06-05'`).\\nNow you can't even subscribe to \"all txs\" in Tendermint.\\nThe goal is a simple and easy to use API for doing that.\\n![Tx Send Flow Diagram](img\/tags1.png)\\n","completion":"## Decision\nABCI app return tags with a `DeliverTx` response inside the `data` field (_for\\nnow, later we may create a separate field_). Tags is a list of key-value pairs,\\nprotobuf encoded.\\nExample data:\\n```json\\n{\\n\"abci.account.name\": \"Igor\",\\n\"abci.account.address\": \"0xdeadbeef\",\\n\"tx.gas\": 7\\n}\\n```\\n### Subscribing for transactions events\\nIf the user wants to receive only a subset of transactions, ABCI-app must\\nreturn a list of tags with a `DeliverTx` response. These tags will be parsed and\\nmatched with the current queries (subscribers). If the query matches the tags,\\nsubscriber will get the transaction event.\\n```\\n\/subscribe?query=\"tm.event = Tx AND tx.hash = AB0023433CF0334223212243BDD AND abci.account.invoice.number = 22\"\\n```\\nA new package must be developed to replace the current `events` package. It\\nwill allow clients to subscribe to a different types of events in the future:\\n```\\n\/subscribe?query=\"abci.account.invoice.number = 22\"\\n\/subscribe?query=\"abci.account.invoice.owner CONTAINS Igor\"\\n```\\n### Fetching transactions\\nThis is a bit tricky because a) we want to support a number of indexers, all of\\nwhich have a different API b) we don't know whenever tags will be sufficient\\nfor the most apps (I guess we'll see).\\n```\\n\/txs\/search?query=\"tx.hash = AB0023433CF0334223212243BDD AND abci.account.owner CONTAINS Igor\"\\n\/txs\/search?query=\"abci.account.owner = Igor\"\\n```\\nFor historic queries we will need a indexing storage (Postgres, SQLite, ...).\\n### Issues\\n- https:\/\/github.com\/tendermint\/tendermint\/issues\/376\\n- https:\/\/github.com\/tendermint\/tendermint\/issues\/287\\n- https:\/\/github.com\/tendermint\/tendermint\/issues\/525 (related)\\n"}
{"prompt":"## Context\\nSince the initial version of the pubsub, there's been a number of issues\\nraised: [#951], [#1879], [#1880]. Some of them are high-level issues questioning the\\ncore design choices made. Others are minor and mostly about the interface of\\n`Subscribe()` \/ `Publish()` functions.\\n### Sync vs Async\\nNow, when publishing a message to subscribers, we can do it in a goroutine:\\n_using channels for data transmission_\\n```go\\nfor each subscriber {\\nout := subscriber.outc\\ngo func() {\\nout <- msg\\n}\\n}\\n```\\n_by invoking callback functions_\\n```go\\nfor each subscriber {\\ngo subscriber.callbackFn()\\n}\\n```\\nThis gives us greater performance and allows us to avoid \"slow client problem\"\\n(when other subscribers have to wait for a slow subscriber). A pool of\\ngoroutines can be used to avoid uncontrolled memory growth.\\nIn certain cases, this is what you want. But in our case, because we need\\nstrict ordering of events (if event A was published before B, the guaranteed\\ndelivery order will be A -> B), we can't publish msg in a new goroutine every time.\\nWe can also have a goroutine per subscriber, although we'd need to be careful\\nwith the number of subscribers. It's more difficult to implement as well +\\nunclear if we'll benefit from it (cause we'd be forced to create N additional\\nchannels to distribute msg to these goroutines).\\n### Non-blocking send\\nThere is also a question whenever we should have a non-blocking send.\\nCurrently, sends are blocking, so publishing to one client can block on\\npublishing to another. This means a slow or unresponsive client can halt the\\nsystem. Instead, we can use a non-blocking send:\\n```go\\nfor each subscriber {\\nout := subscriber.outc\\nselect {\\ncase out <- msg:\\ndefault:\\nlog(\"subscriber %v buffer is full, skipping...\")\\n}\\n}\\n```\\nThis fixes the \"slow client problem\", but there is no way for a slow client to\\nknow if it had missed a message. We could return a second channel and close it\\nto indicate subscription termination. On the other hand, if we're going to\\nstick with blocking send, **devs must always ensure subscriber's handling code\\ndoes not block**, which is a hard task to put on their shoulders.\\nThe interim option is to run goroutines pool for a single message, wait for all\\ngoroutines to finish. This will solve \"slow client problem\", but we'd still\\nhave to wait `max(goroutine_X_time)` before we can publish the next message.\\n### Channels vs Callbacks\\nYet another question is whether we should use channels for message transmission or\\ncall subscriber-defined callback functions. Callback functions give subscribers\\nmore flexibility - you can use mutexes in there, channels, spawn goroutines,\\nanything you really want. But they also carry local scope, which can result in\\nmemory leaks and\/or memory usage increase.\\nGo channels are de-facto standard for carrying data between goroutines.\\n### Why `Subscribe()` accepts an `out` channel?\\nBecause in our tests, we create buffered channels (cap: 1). Alternatively, we\\ncan make capacity an argument and return a channel.\\n","completion":"## Decision\n### MsgAndTags\\nUse a `MsgAndTags` struct on the subscription channel to indicate what tags the\\nmsg matched.\\n```go\\ntype MsgAndTags struct {\\nMsg interface{}\\nTags TagMap\\n}\\n```\\n### Subscription Struct\\nChange `Subscribe()` function to return a `Subscription` struct:\\n```go\\ntype Subscription struct {\\n\/\/ private fields\\n}\\nfunc (s *Subscription) Out() <-chan MsgAndTags\\nfunc (s *Subscription) Canceled() <-chan struct{}\\nfunc (s *Subscription) Err() error\\n```\\n`Out()` returns a channel onto which messages and tags are published.\\n`Unsubscribe`\/`UnsubscribeAll` does not close the channel to avoid clients from\\nreceiving a nil message.\\n`Canceled()` returns a channel that's closed when the subscription is terminated\\nand supposed to be used in a select statement.\\nIf the channel returned by `Canceled()` is not closed yet, `Err()` returns nil.\\nIf the channel is closed, `Err()` returns a non-nil error explaining why:\\n`ErrUnsubscribed` if the subscriber choose to unsubscribe,\\n`ErrOutOfCapacity` if the subscriber is not pulling messages fast enough and the channel returned by `Out()` became full.\\nAfter `Err()` returns a non-nil error, successive calls to `Err() return the same error.\\n```go\\nsubscription, err := pubsub.Subscribe(...)\\nif err != nil {\\n\/\/ ...\\n}\\nfor {\\nselect {\\ncase msgAndTags <- subscription.Out():\\n\/\/ ...\\ncase <-subscription.Canceled():\\nreturn subscription.Err()\\n}\\n```\\n### Capacity and Subscriptions\\nMake the `Out()` channel buffered (with capacity 1) by default. In most cases, we want to\\nterminate the slow subscriber. Only in rare cases, we want to block the pubsub\\n(e.g. when debugging consensus). This should lower the chances of the pubsub\\nbeing frozen.\\n```go\\n\/\/ outCap can be used to set capacity of Out channel\\n\/\/ (1 by default, must be greater than 0).\\nSubscribe(ctx context.Context, clientID string, query Query, outCap... int) (Subscription, error) {\\n```\\nUse a different function for an unbuffered channel:\\n```go\\n\/\/ Subscription uses an unbuffered channel. Publishing will block.\\nSubscribeUnbuffered(ctx context.Context, clientID string, query Query) (Subscription, error) {\\n```\\nSubscribeUnbuffered should not be exposed to users.\\n### Blocking\/Nonblocking\\nThe publisher should treat these kinds of channels separately.\\nIt should block on unbuffered channels (for use with internal consensus events\\nin the consensus tests) and not block on the buffered ones. If a client is too\\nslow to keep up with it's messages, it's subscription is terminated:\\nfor each subscription {\\nout := subscription.outChan\\nif cap(out) == 0 {\\n\/\/ block on unbuffered channel\\nout <- msg\\n} else {\\n\/\/ don't block on buffered channels\\nselect {\\ncase out <- msg:\\ndefault:\\n\/\/ set the error, notify on the cancel chan\\nsubscription.err = fmt.Errorf(\"client is too slow for msg)\\nclose(subscription.cancelChan)\\n\/\/ ... unsubscribe and close out\\n}\\n}\\n}\\n### How this new design solves the current issues?\\n[#951] ([#1880]):\\nBecause of non-blocking send, situation where we'll deadlock is not possible\\nanymore. If the client stops reading messages, it will be removed.\\n[#1879]:\\nMsgAndTags is used now instead of a plain message.\\n### Future problems and their possible solutions\\n[#2826]\\nOne question I am still pondering about: how to prevent pubsub from slowing\\ndown consensus. We can increase the pubsub queue size (which is 0 now). Also,\\nit's probably a good idea to limit the total number of subscribers.\\nThis can be made automatically. Say we set queue size to 1000 and, when it's >=\\n80% full, refuse new subscriptions.\\n"}
{"prompt":"## Context\\nFunction as a Service (FaaS) is the main capability inside Kyma. This DR presents a market research and a detailed analysis of both Kubeless and fission.\\n","completion":"## Decision\nThe decision is to use Kubeless as the FaaS solution.\\n"}
{"prompt":"## Context\\nAs we want to base inter-modular communication on asynchronous communication in the form of event-driven architecture, we need some \"events bus\" to do that.\\n","completion":"## Decision\nSolution number 1 - In Memory Events Bus<\/br>\\nAt that moment we don't see more advanced integration scenarios in our system than simple publish\/subscribe scenario. We decided to follow the simplest scenario and if it will be necessary - move to more advanced.\\n"}
{"prompt":"## Context\\nOn the Cloud Platform, there is a need to implement various policies to safeguard our tenant applications and to enforce best practices.\\nKubernetes offers various mechanisms that cover some of our needs (eg.: `ResourceQuotas` to prevent resource exhaustion and `PodSecurityPolicies` to enforce non-root containers) but there are other areas for which there is no builtin solution. However, kubernetes implements a Dynamic Admission Control API which introduces [admission webhooks][admission-control]. This API provides an easy way with which to expand on the existing admission controllers (built in the apiserver).\\nOur immediate need was to prevent users from reusing hostnames in `Ingresses`. Although our ingress controller prevents hijacking of hostnames, it does so silently and furthermore, this is not a documented behaviour. Therefore, we decided that the user should not be allowed to reuse hostnames already defined in other `Ingresses` and receive a useful error message if they try to do that.\\n","completion":"## Decision\nWe explored a number of existing solutions in the open source community, as well as the possibility of implementing our own and we also discussed the issue with other organisations that use kubernetes before reaching a conclusion.\\nEventually we decided to introduce the [Open Policy Agent][open-policy-agent]:\\n- It is a generic framework for building and enforcing policies (whereas most other existing implementations were designed around specific problems)\\n- The policies are defined in a declarative, high-level language\\n- It is designed for cloud-native environments\\n- It provides a kubernetes integration\\n- It provides a way by which to unit test the policies\\n- The project is adopted by CNCF\\nAlthough the project is still in alpha and very likely to change in the near future, we decided that it is stable enough for our needs and worth adopting even at these early stages, since the benefits outweigh the cost.\\n"}
{"prompt":"## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state\/model.\\n","completion":"## Decision\nWe will assert against the session in the provider which is accessible through the context. (the BananaContext instance, not a React Context)\\n"}
{"prompt":"## Context\\nApplications need to be configured differently depending on where they are running. For example, the backend running locally will have different configuration then the backend running in production.\\n","completion":"## Decision\nWe will use environment variables to configure applications.\\n"}
{"prompt":"## Context\\n1. The current implementation goes against a convention where global configuration files are prefixed with a period\\n2. I want to be able to do writebacks to the configuration, an exact example would be `dev add repo ${REPO_URL}`, and the current implementation makes it messy because the configuration files can be anywhere.\\n","completion":"## Decision\nI have made the decision for configuration files to look like: `\\.dev(\\.[a-zA-Z\\-\\_\\.]+)?\\.yaml` and for the application to only search in the user home directory and current working directory. The `includes` property will be removed.\\n"}
{"prompt":"## Context\\nWe need to decide whether command messages can be routed directly to processes.\\n","completion":"## Decision\nWe have decided to disallow this behavior - processes may only handle events\\nand timeout messages.\\nIf we were to allow processes to accept commands directly it may be tempting to\\nimplement domain idempotency in the process instead of in an aggregate where\\nsuch logic belongs.\\nFurthermore, it's easier to allow commands to be routed to processes in a future\\nversion than it is to remove it once it's in use.\\n"}
{"prompt":"## Context and Problem Statement\\nWe need a central observability platform where we ingest all our logs, metrics and traces so that ops, devs and devops can analyze performance, reliability and uptime. We can try to build and host such a platform on our own or use SaaS providers like DataDog.\\n## Decision Drivers\\n- Reliability\\n- Feature-richness\\n- Cost\\n- Maintenance\\n- Development\\n- Vendor lock level\\n","completion":"## Decision\n- Reliability\\n- Feature-richness\\n- Cost\\n- Maintenance\\n- Development\\n- Vendor lock level\\nSaaS solution (DataDog) is a clear winner due to its hastle-free service usage, continuous improvement as well as its competitive pricing.\\n### Positive Consequences\\n- Easy access to observability for devs and devops\\n- Well known integration target for third-party services\\n- Monitoring setup is well known in the industry\\n- No special knowledge about setup of the plaftorm\\n### Negative Consequences\\nNone.\\n"}
{"prompt":"## Context and Problem Statement <!-- required -->\\nCurrently we are using JHOVE 1.x to generate voluminous technical metadata for every file of every object accessioned in SDR, and we do not use most of this metadata. This is problematic especially for large & many files: we cannot currently accessioning books with many pages because the technical metadata robot consumes all system memory which causes the virtual machine to kill the JHOVE process. We believe that only a small subset of the JHOVE output will ever be useful to SDR consumers.  Note: SMPL content ships with its own metadata typically from MediaInfo rather than JHOVE.\\n## Decision Drivers <!-- optional -->\\n* Cannot accession large files (objects > 1GB or so)\\n* Cannot accession objects with many pages, such as books\\n* Blocker for Google Books project\\n* Causes extreme delays accessioning other content\\n","completion":"## Decision\n* Cannot accession large files (objects > 1GB or so)\\n* Cannot accession objects with many pages, such as books\\n* Blocker for Google Books project\\n* Causes extreme delays accessioning other content\\n**Preferred** (by Infrastructure Team) option: option 2, because:\\n* Option 1 is preventing us from accessioning books and other large objects, which is unacceptable to SDR customers\\n* Option 3 is an unsound preservation strategy and does not meet SDR user needs\\n* Option 4 has already been pursued a number of times already, and there's only so much we can toss at the worker machines\\n* Option 5 has been rejected as a general deployment strategy for now\\nThus, option 2 is the only option that currently meets the department's and its customers' needs.\\nAs part of this work, we will move forward with a two-prong strategy in order to resolve the tension between the need to come up with a sound, community-oriented preservation practice and the need to accession large-scale content now.\\nIn the short-term, we will come up with a short list of technical metadata attributes that will be extracted from all files and from all files of certain types. We will convene a \u201ctechnical metadata strike team\u201d in short order that will review attributes being used in Samvera and make recommendations based thereupon. The aim is for this group to finalize their recommendations in advance of the January 2020 Google Books work cycle.\\nIn parallel, we will pursue a longer-term effort for determining what an ideal, community-oriented strategy is for doing this work building on best practices (which are currently murky\/non-emergent). Along with this longer-term work, we will look into how to support on-demand regeneration of technical metadata so that we can iterate on the short-term work in the prior bullet.\\n"}
{"prompt":"## Context\\nSloth machine applications need to support any type of processor architectures, that can be loaded and chosen by users when running the applications. The architecture with which to run the virtual machine will be selected at launch time: for example, a command-line application could be used like:\\n```\\n$ sm --arc=sma program_file\\n```\\n","completion":"## Decision\nWe will support adding new architectures as mods, or user-side plugins, that are provided by the user without needing to rebuild the application.\\nWe won't provide integration tests with real code targeting specific architectures in any Sloth Machine suite, because they become actually independent from the architecture, and as such they cannot know about any specific architecture, not event in tests, where we might just use stubs instead. Integration tests for architectures must be included in the specific architecture packages.\\n### Local architectures\\nThe local architecture loader, which is selected by an adapter to load architectures from local files, will load an architecture object from the module file.\\nThis architecture will implement the `<Architecture>` domain interface, thus providing a `getInterpreter(<System>)` method, taking a System to return an Interpreter for that specific architecture.\\nThe module file must be named `lib`, and must be located directly inside a directory called with the name of the architecture.\\nThe architecture module is only allowed to depend on the `smf`, and no other external packages, because the environment where the architecture module is loaded is controlled by the suite loading it, for example the Sloth Machine virtual machine.\\n"}
{"prompt":"## Context and Problem Statement\\nplay-frontend-hmrc relies on a webjar for [hmrc\/hmrc-frontend](https:\/\/www.github.com\/hmrc\/hmrc-frontend)\\npublished to www.webjars.org. This has a number of drawbacks:\\n* publishing is a manual process\\n* it can take many hours to complete\\n* webjars has been known to be down and HMRC has no support arrangements with www.webjars.org\\nThe main impact of the above is an excessive lead time for making improvements in the\\nunderlying hmrc-frontend library available in production via play-frontend-hmrc.\\nBearing the above in mind, and the fact that HMRC has its own repository for open artefacts, replacing\\nBintray, should we:\\n* automate the creation of the webjars within our own deployment pipelines with no dependency\\non webjars.org\\n* publish the resulting webjars to this repository automatically?\\nNote, this decision only addresses the creation and publishing of the hmrc-frontend webjar, not the\\nwebjar for [alphagov\/govuk-frontend](https:\/\/www.github.com\/alphagov\/govuk-frontend), which is\\ncurrently a dependency for [hmrc\/play-frontend-govuk](https:\/\/www.github.com\/hmrc\/play-frontend-govuk).\\n## Decision Drivers\\n* The need to make improvements and upgrades to hmrc-frontend\\navailable in play-frontend-hmrc quickly.\\n* The increasing user base of play-frontend-hmrc, and accelerating demand for new features and\\nimprovements.\\n* The hardship, frustration and toil the current manual process is causing the team.\\n* The need to keep things simple and avoidance of creating new repositories unnecessarily due to\\nthe overhead of maintaining those repositories\\n* The testing advantages of being able to build and deploy the hmrc-frontend webjar locally\\n* Parity between the hmrc-frontend NPM package and the webjar.\\n","completion":"## Decision\n* The need to make improvements and upgrades to hmrc-frontend\\navailable in play-frontend-hmrc quickly.\\n* The increasing user base of play-frontend-hmrc, and accelerating demand for new features and\\nimprovements.\\n* The hardship, frustration and toil the current manual process is causing the team.\\n* The need to keep things simple and avoidance of creating new repositories unnecessarily due to\\nthe overhead of maintaining those repositories\\n* The testing advantages of being able to build and deploy the hmrc-frontend webjar locally\\n* Parity between the hmrc-frontend NPM package and the webjar.\\nChosen option: option 2 because it solves the core issue and enables local testing without introducing\\nadditional dependencies.\\n### Existing architecture\\n<img alt=\"Existing architecture\" src=\"0009-webjars-existing.png\" width=\"450\">\\n### To be architecture\\n<img alt=\"To be architecture\" src=\"0009-webjars-tobe.png\" width=\"450\">\\n### Positive Consequences\\n* Webjars are available instantaneously after a new version of hmrc-frontend is released\\n* It is now possible to locally test changes to hmrc-frontend in conjunction with Scala microservices\\nwithout needing to publish to NPM or webjars.org first, reducing the risk that flawed components are released into\\nproduction.\\n* Lead times for making improvements to hmrc-frontend available in production are reduced.\\n* Maintaining play-frontend-hmrc is a less frustrating process.\\n* We have more control over the metadata attached to the webjars published. For example, at the moment, the webjars\\nproduced indicate webjars.org as the developer in the POM files.\\n* There are fewer external dependencies and moving parts.\\n### Negative Consequences\\n* We have an additional moving part to maintain ourselves.\\n"}
{"prompt":"## Context\\nDeploying hosted 3Bot takes a lot of time because of cloning and installation time.\\nIt can make use of the new flist build by the CI to be always up to date and reduce the installation time.\\n","completion":"## Decision\nUpdate the flist with the up to date one\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nPostgres can autoincrement identifiers for database tables. By default this is an integer.\\n","completion":"## Decision\nWe decided to use [ULID](https:\/\/github.com\/ulid\/spec)'s for all database id's.  Serial integers present potential problems with scaling and concurrency. Unlike UUID's ULID's are lexicographically (i.e., alphabetically) sortable as the first 48 bits of the identifier contain a UNIX timestamp.\\n"}
{"prompt":"## Context\\n* We are building a web app that has one official url that uses query params\\n* The url is \/search?q=search-term\\n* We believe that future endpoints will be added, or more complicated search requests\\n","completion":"## Decision\n* We will use Bidi to parse the routes.\\n"}
{"prompt":"## Context\\nI have written and seen a lot of code where it was not clear what rules apply, where to find a certain\\nfile, how to write the import correctly is it \"render-page.js\" or \"render-pages.js\"\\nand bugs caused by those missing definitions. IDE support and types have definitely improved that over the\\nyears. Still I struggle from time to time.\\nI just renamed a file from \"tidbit\/render-pages.js\" to \"tidbit\/render-page.js\" (the new name is **singular**)\\neven though this file contains multiple page-render functions. But if I will start naming a file\\ndepending of how many page-render functions it contains I will have more work renaming when functionality\\nevolves and will introduce bugs\\n(I know I have tests to prevent that, but they are not almighty either). I want one way to do it.\\nFor files I want one rule how they are named. That rule applies always and esp. in doubt.\\n","completion":"## Decision\n**Use the singular for file names** for as much consistency as possible.\\nAs usual really good reasons for exceptions exist, but ask yourself many times if the exception is\\nworth dropping consistency for.\\n"}
{"prompt":"## Context\\nA question was raised where a system was being developed with Microsoft SQLServer. The solution does not require SQLServer and it would be straightforward to port to [PostgreSQL](https:\/\/www.postgresql.org\/).\\nFurther investigation, in slack, gave answers of:\\n> _\"Hi, PostgreSQL always unless there's a very good reason you specifically need SQL Server\"_\\n> _\"Hi, as we follow open standard, we always recommend free open source as a first choice unless there is a good reason not to use open source.\"_\\n","completion":"## Decision\nFree and open source software will be selected as a first choice, unless there is a good reason not to use open source.\\nExample: PostgreSQL instead of Microsoft SQLServer.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context\\nSo I'm arranging the project, and I'm trying to think about how to make a development environment include the right versions of aws-cli, Serverless, and other tools. Docker is the way I've done that recently, so let's try it.\\nIf I make Lambda functions, for instance, I can use a Docker container to create a local execution environment running AWS's same Python or Node version, without interfering with my own system's configuration.\\nIf I make a CI\/CD pipeline, the same Docker container can be used to perform tests and deployments.\\n","completion":"## Decision\nI will use Docker to create an environment for working on the cloud provisioning project.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context\\nDigiDeps uses a Postgres database to store persistent information. Since the project was first set up, we have been using an Amazon RDS hosted instance of Postgres for this. However, this hosting option lacks scalability so we have to pay for a full database host 24\/7 for each environment.\\nWe have several environments which do not need to be highly available. This includes \"ephemeral\" development environments, the \"main\" environment used in CI, and the training environment. We do not need to run a database for these environments outside of working hours, and often inside of them too.\\n","completion":"## Decision\nWe will use Amazon Aurora Serverless for environments which do not need to always be on. Aurora automatically scales with usage, including pausing completely if the database isn't in use.\\n"}
{"prompt":"## Context\\nThe landing page need a logo for the name. This could be design by hand but this requires skill and time or an online service could be used.\\n","completion":"## Decision\nGenerate the logo with [launchaco.com](https:\/\/www.launchaco.com\/logo\/editor).\\n"}
{"prompt":"## Context\\nOne of the aims of the Datalabs project is to provide the climate science community easy\\naccess to the compute capabilities of JASMIN. In order to do this we need to find a way\\nto distribute computationally expensive calculations across a cluster and need to explore\\nthe available options for doing this.\\n","completion":"## Decision\nWe have decided to use [Apache Spark](https:\/\/spark.apache.org\/) as our first option for\\ndistributed compute. It has established itself as the successor to Hadoop and provides\\nstrong integration with interactive notebook technologies such as Jupyter and Zeppelin.\\nSpark also provides support for multiple languages including Scala, Java, Python and R.\\nThis makes is a flexible platform that should appeal to many users.\\nAdditionally, Spark provides Streaming and Machine Learning capabilities which may be of\\ninterest later in the project.\\n"}
{"prompt":"## Context\\nFor data such as variants, there can be a very large number of joined objects, so we need to provide a count rather than showing the objects. This could then be used as a link to retrieve the list of objects separately.\\n","completion":"## Decision\nThis has been implemented as part of JoinMergeSort, triggered by a special `count` field e.g.\\n`http:\/\/localhost:8080\/api\/genes\/query?query={\"name\":\"BRCA2\",\"genome\":\"homo_sapiens\"}&fields=[\"name\",\"description\",{\"variants\":[\"count\"]}]`\\nyields:\\n```\\n{\\n\"name\": \"BRCA2\",\\n\"description\": \"breast cancer 2 [Source:HGNC Symbol;Acc:HGNC:1101]\",\\n\"id\": \"ENSG00000139618\",\\n\"variants\": {\\n\"count\": 9337\\n}\\n}\\n```\\n"}
{"prompt":"## Context\\nWe need to decide the format and semantics of a command type within the actor system.\\n","completion":"## Decision\nWe will use a regular class for a command. The data of a command is added as properties with public getters and setters. A command should be handled as an immutable type.\\n"}
{"prompt":"## Context\\nTo ensure scalability, reliability and performance, microservice architecture should be used\\n","completion":"## Decision\nAll agreed with this decision\\n"}
{"prompt":"## Context\\nThe backend has Lambda functions run by an API gated with an API key and a usage plan. It is currently reading sample data from a Dynamo table. I have made progress toward storing data in AWS from the client.\\nNext, each user needs to its own pool of data. That means I need a notion of users. I need some piece of the backend to authenticate someone and vend a token that they can send to the API to get access to a bucket of data.\\nMy ideal is to sign in with GitHub, Google, or some other identity provider and get a token. AWS has a service called Cognito that does a few things, and I don't necessarily understand all of those things, but I think it does the thing I want.\\n","completion":"## Decision\nI will try to use Cognito to authenticate users.\\n"}
{"prompt":"### Context\\nThe application runs well when using Panoptes for subject retrieval and Caesar for classification counts. Although the touch table has never lost internet connection (using WiFi) and internet connection on the museum floor is quite reliable, we should be prepared for internet outages, however abrupt or short. Although Caesar works well as a new service, we should be prepared for a situation where Caesar is unresponsive.\\n### Decision\\nWe will use a local database in the case our external dependencies (Panoptes and Caesar) are unresponsive. The local database will be responsible for holding subject information from Panoptes and classification counts from Caesar. Furthermore, we will hold subject images locally so we will not have to wait for a potentially large array of subject images returning from the api.\\nWe will also use a classification queue similar to the one used in PFE, whereas a new classification is kept in a queue if unable to be created through the Panoptes api (eg. in the case of lost internet connection).\\n### Status\\nAccepted\\n### Consequences\\nThis will require a good amount of overhead to choose new technologies and write the logic to keep subjects locally. This will introduce a new part of the app to maintain while making sure the table has the same subjects locally as well as on the project builder. Things could get hairy on the table if a keen eye isn't kept on keeping the local database and subject set up to date.\\nMy main worry is that using a local database will add another layer of complexity to the app that will make it a bit more difficult for a new developer to maintain the app. In this case, we will have to rely on strong documentation to ensure this doesn't happen.\\n_In Retrospect:_ Overall, I'm very satisfied with this decision. The app really only _needs_ internet connection to create a classification. Also, the app should perform well even if local subject images aren't available. The table will keep track of classification counts correctly in the case Caesar is down. Caesar is mainly used to provide accurate classification counts in case the table counts get out of sync. I strongly recommend using a local database on any future development.\\n","completion":"## Decision\nWe will use a local database in the case our external dependencies (Panoptes and Caesar) are unresponsive. The local database will be responsible for holding subject information from Panoptes and classification counts from Caesar. Furthermore, we will hold subject images locally so we will not have to wait for a potentially large array of subject images returning from the api.\\nWe will also use a classification queue similar to the one used in PFE, whereas a new classification is kept in a queue if unable to be created through the Panoptes api (eg. in the case of lost internet connection).\\n### Status\\nAccepted\\n### Consequences\\nThis will require a good amount of overhead to choose new technologies and write the logic to keep subjects locally. This will introduce a new part of the app to maintain while making sure the table has the same subjects locally as well as on the project builder. Things could get hairy on the table if a keen eye isn't kept on keeping the local database and subject set up to date.\\nMy main worry is that using a local database will add another layer of complexity to the app that will make it a bit more difficult for a new developer to maintain the app. In this case, we will have to rely on strong documentation to ensure this doesn't happen.\\n_In Retrospect:_ Overall, I'm very satisfied with this decision. The app really only _needs_ internet connection to create a classification. Also, the app should perform well even if local subject images aren't available. The table will keep track of classification counts correctly in the case Caesar is down. Caesar is mainly used to provide accurate classification counts in case the table counts get out of sync. I strongly recommend using a local database on any future development.\\n"}
{"prompt":"## Context\\nTo protect against data loss, a backup of all storage components should be implemented. As all data components are specified in the [data catalog](0012-data-catalog-specifies-all-data-components.md), automated backup can be implemented from this specification.\\n","completion":"## Decision\nWe will implemented automated backup based on the [data catalog](0012-data-catalog-specifies-all-data-components.md) for each component that stores data.\\n"}
{"prompt":"## Context\\nAcceptance tests aim to test the application behaviors as a whole from a consumer point of view.\\nThose tests must be understandable by any user, either technical or not and help documenting the provided features.\\nDefining new acceptance tests must be easy, through reusable step definitions.\\nApplication sources must be self-contained, including the acceptance tests definition and implementation, so that\\nacceptance tests can be run during the development lifecycle in a [Behavior-driven development](https:\/\/en.wikipedia.org\/wiki\/Behavior-driven_development)\\napproach.\\n","completion":"## Decision\n[Cucumber](https:\/\/cucumber.io\/) will be used to describe and execute acceptance tests in `menu-generation` application.\\n"}
{"prompt":"## Context and Problem Statement\\nMicrosoft Dynamics CRM has been proposed as the system to manage contacts, accounts & permits (cases) for the CCP prototype.\\nUsers should be able to recognise that the service is provided by SEPA (styling) and the interface should be sufficiently customisable\\nto support a good user experience.\\n## Decision Drivers\\n* Data is stored in the CRM so all interactions must be compatible with the data entities available.\\n* A user-centered design approach requires the technical solution to support custom journeys and styling as much as possible.\\n","completion":"## Decision\n* Data is stored in the CRM so all interactions must be compatible with the data entities available.\\n* A user-centered design approach requires the technical solution to support custom journeys and styling as much as possible.\\n[Option 1] A custom web application, combining a modern reactive frontend framework with a backend API gateway to interact with Dynamics and Azure services via restful webs APIs\\nwould provide the best support for optimising the user journey.\\nAvoiding the use of a hosted web application (MS Portals), by decoupling the frontend, reduces the reliance on a single platform.\\n### Positive Consequences\\n* Complete control over the user interface and user journey.\\n* SEPA can reuse & build upon shared web patterns & components.\\n### Negative Consequences\\n* Additional overhead required for maintenance, testing and deployment.\\n"}
{"prompt":"## Context\\nIn [ADR031](ADR034-one-service-operator-different-resource-kinds.md) we have\\ndecided that we will be building an service operator for postgres.\\nWe have came to an impasse in the discussion over whether we build separate\\nservice operators for separate services - i.e. a single service operator for\\nPostgres, a single one for queues, etc. - or whether we should build a single\\nservice operator that provisions access to all the services we need.\\n","completion":"## Decision\nWe will make a single service operator for multiple services, and will be able\\nto split out parts as necessary if the considerations change over time. We\\nshould keep an eye on its security, its scope, and AWS Service Operator\\nupstream activity.\\n"}
{"prompt":"## Context\\nThe teams looking after a cluster need visibility of key metrics in order that they can ensure reliability and diagnose issues.\\n[Prometheus](https:\/\/prometheus.io) and [Alertmanager](https:\/\/prometheus.io\/docs\/alerting\/alertmanager\/) are open-source systems monitoring and alerting tools and a graduated from the [Cloud Native Computing Foundation][CNCF].\\nA [kubernetes operator is available for Prometheus](https:\/\/github.com\/coreos\/prometheus-operator) that provides tight integration with the kubernetes API and minimal configuration required from service teams.\\nReliability Engineering has standardised on [Prometheus](https:\/\/prometheus.io) to enable platform observability.\\n[CNCF]: https:\/\/www.cncf.io\/announcement\/2018\/08\/09\/prometheus-graduates\/\\n","completion":"## Decision\nWe will use [Prometheus](https:\/\/prometheus.io\/) and [Alertmanager](https:\/\/prometheus.io\/docs\/alerting\/alertmanager\/) managed by the [Prometheus Operator](https:\/\/github.com\/coreos\/prometheus-operator) for metrics in line with the standard [Reliability Engineering approach to Metrics and Alerting](https:\/\/reliability-engineering.cloudapps.digital\/monitoring-alerts.html).\\n"}
{"prompt":"## Context\\nWe started with waiting code spread throughout the different layers of the DockerComposition stack:\\n* DockerComposition\\n* Container\\n* DockerPort\\nThis makes our custom waits depend on specific functionality rather than general functionality that external devs can also use. Now we're making all `waitForService` calls go through just a couple of API calls with flexible Healthchecks this is going to bite us.\\n","completion":"## Decision\nWait code (for the purposes of powering `waitForService`) does not go in the Composition stack. Instead, waits depend on external observations of this stack.\\n"}
{"prompt":"## Context\\n[ElasticSearch](https:\/\/www.elastic.co\/products\/elasticsearch) is a Java based search engine built on top of technologies like Lucene and Solr. It is open source software which can be freely [downloaded](https:\/\/github.com\/elastic\/elasticsearch) and installed on a number of operating systems. It is also freely available from elastic.co as a Docker image and as a subscription based SaaS offering. The SaaS option is discounted in ADR [0002](0002-use-docker-image-for-elastic.md).\\n","completion":"## Decision\nWe will use the official Docker image provided by elastic.co.\\n"}
{"prompt":"## Context\\nA couple of client projects need infrastructure to process GridLabD jobs.\\nThe goal would be that other projects could either include this project as its\\nown service in their infrastructure, or incorporate it directly into their\\nFlask-based project.\\nThese kinds of jobs are long running, and produce artifacts that each project\\nmay store in different ways (but primarily S3).\\n","completion":"## Decision\nCreate a Flask Rest service, coupled with Celery for job processing.\\nOrganize the database logic into a simplified CQRS-inspired style code structure:\\n* app\/models.py contain all sql models.\\n* app\/services.py contain all db commands that modify database state.\\n* app\/queries.py contain all db queries to the database.\\nUse the structure of the [presence-account-service](https:\/\/github.com\/PresencePG\/presence-account-service) project as a reference.\\n"}
{"prompt":"## Context\\nExisting self-hosted infrastructure at GDS has been managed in code\\nusing tools like puppet, but in a somewhat ad hoc way with each team\\ndoing things differently, little sharing of code, and much reinvention\\nof wheels.  We would like to learn about other ways of deploying\\ninfrastructure which encourage consistency: in terms of code\\nartifacts, configuration methods, and such like.\\nSystems such as Kubernetes and Amazon ECS are coalescing around Docker\\nas a standard for packaging software and managing configuration.\\n","completion":"## Decision\nWe will build our initial prometheus beta in Amazon ECS, and assess\\nhow effective it is.  We will review this decision once we have learnt\\nmore about both prometheus and ECS.\\n"}
{"prompt":"## Context\\nThe Kubernetes Probe Resource Structure would allow adding multiple types of Healthchecks at once. This would be:\\n* exec\\n* httpGet\\n* tcpSocket\\nHowever currently v1.18 v1.20 when one tries to add more than one Type, like:\\n```\\nreadinessProbe:\\nexec:\\ncommand:\\n- \"\/usr\/bin\/sh\"\\n- \"-c\"\\n- \"echo Hello World\"\\nhttpGet:\\nhost: localhost\\nport: 80\\ntcpSocket:\\nhost: localhost\\nport: 80\\n```\\nkubernetes will not validate the Resource with following message:\\n```\\n# pods \"test-probes\" was not valid:\\n# * spec.containers[0].readinessProbe.httpGet: Forbidden: may not specify more than 1 handler type\\n# * spec.containers[0].readinessProbe.tcpSocket: Forbidden: may not specify more than 1 handler type\\n```\\n","completion":"## Decision\nIt is decided to go with the same logic and ensure that only one type will be accepted, so the Operator is not supposed to handle the case more than one Types are given.\\nFurther it follows the same logic and will use and check HTTP first than TCP last. Exec healthchecks don't make sense and will be ignored completly\\n"}
{"prompt":"## Context\\nFuture projects using the `front-end-monorepo` will require a video player. Some projects may simply need a video to play. Some projects will require users to interact with a video file; such as make annotations on the video to mark locations or item sizes. The video player must be highly customizable, able to be used in a React code base project and support all major browsers.\\n","completion":"## Decision\nWe will implement `react-player` [Github](https:\/\/github.com\/CookPete\/react-player).\\nDemo Page: [Demo](https:\/\/cookpete.com\/react-player\/)\\nThe lengthy list of attributes (props) and Callback Props makes `react-player` a great choice for developers who need customization. Many of these attributes are simply booleans. For example, to play a video, pass in `playing={boolean}`. Full list of [Props](https:\/\/github.com\/CookPete\/react-player#props)\\nCompared to the native HTML video player, `react-player` makes it easy to customize styling so the player looks the same across different browsers.\\nMaking `react-player` responsive is easy by targeting pre-defined classNames.\\n[Responsiveness](https:\/\/github.com\/cookpete\/react-player#responsive-player)\\nOne of the biggest wins of using `react-player` is ease-of-use. This will reduce developer time and reduce the amount of custom methods in our code base.\\n### Media\\n`react-player` is a React component for playing a variety of URLs, including file paths, YouTube, Facebook, Twitch, SoundCloud, Streamable, Vimeo, Wistia, Mixcloud, and DailyMotion. [Supported Media](https:\/\/github.com\/CookPete\/react-player#supported-media)\\nAlthough `react-player` supports a number of externally hosted videos, we will only support video files uploaded to the Zooniverse platform.\\nFile types will be validated to ensure the files are mp4.\\n### Customization\\nA wide range of [Props](https:\/\/github.com\/CookPete\/react-player#props) can be passed in to control playback and react to events.\\nPlease read through the [Github](https:\/\/github.com\/CookPete\/react-player) for a full list of features.\\n"}
{"prompt":"## Context\\nWe need to decide which front-end web framework to use for the Datalabs application.\\nThe choices we evaluated were [React.js](https:\/\/reactjs.org\/) and\\n[Angular](https:\/\/angular.io\/).\\n","completion":"## Decision\nWe have decided to use the React framework for the following reasons:\\n* Preferred the \"batteries not included\" approach of React vs the \"batteries\\nincluded\" approach of Angular.\\n* Better development tooling available for React.\\n* Larger community for React.\\n* The team had previous experience with React.\\n"}
{"prompt":"## Context and Problem Statement\\n`semplate` is intended to provide functionality to java developers so that they can build tools that create, use and\/or manage documents that are written in markdown. As such, it should be easily to integrate it with programs written in Java.\\n","completion":"## Decision\nChosen option is to create as a standard Java JAR library\\nThe option to create a microservice was rejected as these costs of hosting it could not be met. However, this option has not been excluded as additional to a standard Java JAR.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nWe need a mechanism for highlighting security vulnerabilities in our code before it reaches production environments\\n","completion":"## Decision\nUse the [Brakeman](https:\/\/brakemanscanner.org\/) static security analysis tool to find vulnerabilities in development and test\\n"}
{"prompt":"## Context and Problem Statement\\nHow do we want to organise work in branches and how should changes be released? How should different branches be continuously deployed for QA?\\n## Decision Drivers\\n- We need to have confidence in our releases.\\n- We want more structured releases while we're still getting our footing in a shared monorepo.\\n- We need simplicity and clear [takt time](https:\/\/kanbanize.com\/continuous-flow\/takt-time) so different teams can plan for what is going out the door from them.\\n- It should work well with our agile work environment.\\n","completion":"## Decision\n- We need to have confidence in our releases.\\n- We want more structured releases while we're still getting our footing in a shared monorepo.\\n- We need simplicity and clear [takt time](https:\/\/kanbanize.com\/continuous-flow\/takt-time) so different teams can plan for what is going out the door from them.\\n- It should work well with our agile work environment.\\nChosen option: \"OneFlow\" because it provides a single eternal branch with well structured releases.\\nWe'll implement OneFlow with these details:\\n- Release branches are set up the Monday after each sprint. This is sometimes called release trains, where features line up for different release trains.\\n- Release and quality managers from each team are responsible for reviewing and approving releases.\\n- Releases apply to all apps in the monorepo.\\n- Releases are versioned like this: `{cycle}.{sprint}.{hotfix}`. So version 3.1.2 is the release after cycle 3, sprint 1 with two hot fixes applied.\\n- Feature branches are merged using \"Squash and merge\", so they can be easily reverted.\\n- There are two ways to build larger features.\\n- If the feature is isolated and not likely to cause conflicts, they can stay on long-living feature branches until they are ready to be released.\\n- If the feature touches many parts of the codebase, it can be useful to merge changes more often but hide the feature in production with feature flags.\\n- If a project needs to deploy updates outside of the sprint rhythm, they should use hotfix branches.\\n### Future strategy\\nWith time, we expect to build up better testing capabilities which gives us more confidence in the health of our monorepo. Then we can move quicker, with a simpler GitHub Flow branching strategy and continuous delivery into production.\\n### Hosting environments\\nWe'll set up continuous delivery to different hosting environments:\\n| Environment | Git source            | Databases\/services | Features |\\n| ----------- | --------------------- | ------------------ | -------- |\\n| sandbox     | feature branch        | Test               | All      |\\n| dev         | main                  | Test               | All      |\\n| staging     | main                  | Prod               | All      |\\n| pre-prod    | release\/hotfix branch | Prod               | Finished |\\n| prod        | latest release tag    | Prod               | Finished |\\nWe'll probably start with dev, staging, pre-prod and prod environments, since feature branch deployments are more dynamic and difficult to manage.\\n"}
{"prompt":"## Context\\nDeploying to the cloud is a fundamental part of increasing software delivery\\nfrequency.  However once the frequency is increased, it becomes increasingly\\nneeded to manage the provisioning and configuration of those cloud resources\\nlest a step is forgotten, a security vulnerability is found, or an instance is\\ncorrupted.  Following patterns found in software development, such as managing\\ninfrastructure as code (IaC) and tracking changes in source control, are\\nwidely regarded as good practice today.\\nMany cloud providers have bespoke tooling that helps manage the creation of\\nresources on their infrastructure.  While this tooling is hugely powerful, the\\ncontext of PCMT is that our cloud resources should be kept as simple as\\npossible, in part due to our constraint to allow for on-prem deployments.\\nFurther an organization that chooses to deploy PCMT as a SaaS may not use the\\nsame cloud provider as the one the PCMT project uses.\\n","completion":"## Decision\nWe will use Terraform to provision cloud resources:  compute instances,\\nstorage, DNS, etc.\\n"}
{"prompt":"## Context and Problem Statement\\nFetchers are failing.\\nThe reason why they are failing needs to be investigated.\\n* Claim 1: Knowing the URL which was used to query the fetcher eases debugging\\n* Claim 2: Somehow logging the URL eases debugging (instead of showing it in the debugger only)\\nHow to properly log the URL used for fetching?\\n## Decision Drivers\\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\n","completion":"## Decision\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\nChosen option: \"Separate URL creation\", because comes out best \\(see below\\).\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nWe need to turn on STRICT mTLS for all components on the mesh. However, some\\ncomponents are currently incompatible with this mode.\\nCAPI is incompatible because it uses an init container to run migrations. This\\ninit container comes up before the sidecar, so it is unable to establish an mTLS\\nconnection with the capi database. This causes the init container to fail and\\nprevents capi from coming up. See [this\\nissue](https:\/\/github.com\/cloudfoundry\/capi-k8s-release\/issues\/12) in capi.\\nLog-cache is incompatible because it is configured to establish its own tls\\nconnection, which is incompatible with the mTLS the sidecars are attempting to\\nestablish.\\n","completion":"## Decision\nWe have provided configuration workarounds in the form of Policies, that were\\nplaced in the cf-for-k8s repo to be owned by the respective teams that manage\\nthe troublesome components.\\n[Pull Request](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/pull\/35)\\n"}
{"prompt":"## Context\\nThis decision is being made during Alpha. It also comes at a time that the Digital organisation within DfE are building developer capability and want the members of the developer community to readily switch between services. Hence a default preferred language and framework is desired, Digital are coalescing around Ruby and the associated Rails framework.\\nRails is a development tool which gives web developers a framework, providing structure for all the code they write. The Rails framework helps developers to build websites and applications, because it abstracts and simplifies common repetitive tasks.\\nRails is written in Ruby, the programming language which is also used alongside Rails. Ruby is to Rails as PHP is to Symfony and Zend, or as Python is to Django. The appeal of Ruby to developers lies in the elegance and terseness of the language.\\n","completion":"## Decision\nThe decision is to align with the central Digital preference for Ruby and Rails.\\nKey reasons for choosing Ruby and Rails are:-\\n* Alignment with a large portion of the Government Digital Service sites and services, including cross service development skills within DfE. [Here](https:\/\/dfe-digital.github.io\/technical-guidance\/guides\/default-technology-stack\/#the-ruby-stack) is the DfE Technical Guidance around this.\\n* Optimisation around developer productivity and lower costs.\\n* It was created specifically for building web applications.\\n* Numerous code libraries (Gems) which provide free, open-source code for developers to fulfil specific needs.\\n* A very strong community with a great sense of collaboration and support.\\n* It is robust and high-quality. All Ruby developers are encouraged to follow a set of community-supported coding conventions and standards, which in turn helps produce better code, and thereby high quality digital products and services.\\n* It\u2019s conventions make it much easier for developers to move between Rails services, which will tend to use the same structure and coding practices.\\n* A strong focus on testing, with many good testing frameworks.\\n* It emphasises RESTful application design. REST (REpresentational State Transfer) is a style of software architecture based around a client-server relationship. It encourages a logical structure which can be exposed as an API (Application Programing Interface) which can be used to expose platform functionality (as desired for this service).\\n* Is an intuitive and easy-to-read language. It uses fewer characters than other languages allowing for intuitive, natural language, coding rather than having to use lots of special characters. The flip side is that reading code is made simpler, allowing for quick on-boarding to a service and peer reviewing for example.\\n* Enables rapid development. Due to many of the above points, such as readily available code libraries specifically designed for websites and apps, an active community which promotes high-quality development, and code that is simple to read and write then using Rails can achieve rapid development with high-quality results.\\nReasons why you might not choose Ruby and Rails are:-\\n* Not all hosting can support Rails - but since we are choosing where to host our service we can ensure this isn\u2019t an issue.\\n* Java and PHP are more widely used and there are more developers in these languages. However there is still a strong market in Ruby developers and the very extensive open-source libraries available mean a lot can be achieved by a few developers.\\n* Performance and Scalability. Rails is not as fast as Java or C, but it is fast enough for this service and will horizontally scale out when needed. There are plenty of high-profile organisations relying on Rails, examples include AirBnB, Yellow Pages, Groupon, Channel 5, GitHub, Shopify and Gov.uk.\\n"}
{"prompt":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum.\\n","completion":"## Decision\nWe will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n"}
{"prompt":"## Context\\nThe blockchain reactor is responsible for two high level processes:sending\/receiving blocks from peers and FastSync-ing blocks to catch upnode who is far behind. The goal of [ADR-40](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-040-blockchain-reactor-refactor.md) was to refactor these two processes by separating business logic currently wrapped up in go-channels into pure `handle*` functions. While the ADR specified what the final form of the reactor might look like it lacked guidance on intermediary steps to get there.\\nThe following diagram illustrates the state of the [blockchain-reorg](https:\/\/github.com\/tendermint\/tendermint\/pull\/3561) reactor which will be referred to as `v1`.\\n![v1 Blockchain Reactor Architecture\\nDiagram](https:\/\/github.com\/tendermint\/tendermint\/blob\/f9e556481654a24aeb689bdadaf5eab3ccd66829\/docs\/architecture\/img\/blockchain-reactor-v1.png)\\nWhile `v1` of the blockchain reactor has shown significant improvements in terms of simplifying the concurrency model, the current PR has run into few roadblocks.\\n- The current PR large and difficult to review.\\n- Block gossiping and fast sync processes are highly coupled to the shared `Pool` data structure.\\n- Peer communication is spread over multiple components creating complex dependency graph which must be mocked out during testing.\\n- Timeouts modeled as stateful tickers introduce non-determinism in tests\\nThis ADR is meant to specify the missing components and control necessary to achieve [ADR-40](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-040-blockchain-reactor-refactor.md).\\n","completion":"## Decision\nPartition the responsibilities of the blockchain reactor into a set of components which communicate exclusively with events. Events will contain timestamps allowing each component to track time as internal state. The internal state will be mutated by a set of `handle*` which will produce event(s). The integration between components will happen in the reactor and reactor tests will then become integration tests between components. This design will be known as `v2`.\\n![v2 Blockchain Reactor Architecture\\nDiagram](https:\/\/github.com\/tendermint\/tendermint\/blob\/584e67ac3fac220c5c3e0652e3582eca8231e814\/docs\/architecture\/img\/blockchain-reactor-v2.png)\\n### Fast Sync Related Communication Channels\\nThe diagram below shows the fast sync routines and the types of channels and queues used to communicate with each other.\\nIn addition the per reactor channels used by the sendRoutine to send messages over the Peer MConnection are shown.\\n![v2 Blockchain Channels and Queues\\nDiagram](https:\/\/github.com\/tendermint\/tendermint\/blob\/5cf570690f989646fb3b615b734da503f038891f\/docs\/architecture\/img\/blockchain-v2-channels.png)\\n### Reactor changes in detail\\nThe reactor will include a demultiplexing routine which will send each message to each sub routine for independent processing. Each sub routine will then select the messages it's interested in and call the handle specific function specified in [ADR-40](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-040-blockchain-reactor-refactor.md). The demuxRoutine acts as \"pacemaker\" setting the time in which events are expected to be handled.\\n```go\\nfunc demuxRoutine(msgs, scheduleMsgs, processorMsgs, ioMsgs) {\\ntimer := time.NewTicker(interval)\\nfor {\\nselect {\\ncase <-timer.C:\\nnow := evTimeCheck{time.Now()}\\nschedulerMsgs <- now\\nprocessorMsgs <- now\\nioMsgs <- now\\ncase msg:= <- msgs:\\nmsg.time = time.Now()\\n\/\/ These channels should produce backpressure before\\n\/\/ being full to avoid starving each other\\nschedulerMsgs <- msg\\nprocessorMsgs <- msg\\nioMesgs <- msg\\nif msg == stop {\\nbreak;\\n}\\n}\\n}\\n}\\nfunc processRoutine(input chan Message, output chan Message) {\\nprocessor := NewProcessor(..)\\nfor {\\nmsg := <- input\\nswitch msg := msg.(type) {\\ncase bcBlockRequestMessage:\\noutput <- processor.handleBlockRequest(msg))\\n...\\ncase stop:\\nprocessor.stop()\\nbreak;\\n}\\n}\\nfunc scheduleRoutine(input chan Message, output chan Message) {\\nschelduer = NewScheduler(...)\\nfor {\\nmsg := <-msgs\\nswitch msg := input.(type) {\\ncase bcBlockResponseMessage:\\noutput <- scheduler.handleBlockResponse(msg)\\n...\\ncase stop:\\nschedule.stop()\\nbreak;\\n}\\n}\\n}\\n```\\n"}
{"prompt":"## Context\\nFor the Go based prototype we used IPFS' Kademlia DHT implementations with edits.\\nSince we are switching over to Rust, we can't easily continue using that code base.\\nMore importantly, there are too many Holochain specific additions to a vanilla Kademlia DHT, as well as other possible implementations of achieving entry resilience, routing, our forthcoming World Model, CRDT handling, gossip and our graph properties via linking, so it does not make sense to change existing implementations but instead roll our own.\\n","completion":"## Decision\nWe will build our own DHT implementation in Rust from scratch.\\n"}
{"prompt":"## Context\\nWe aim to:\\n- Make it easier to understand the codebase and its status\\n- Reduce the number of meetings to handover information across teams\\n- Facilitate team rotations across GOV.UK\\n","completion":"## Decision\nTrack architectural decision that impact the status of the CPM, [following a lightweight format: ADR][1]\\n"}
{"prompt":"## Context and Problem Statement\\n[Handlebars](https:\/\/www.npmjs.com\/package\/mustache), an extension of [Mustache](https:\/\/www.npmjs.com\/package\/mustache) ([see the differences](https:\/\/github.com\/handlebars-lang\/handlebars.js#differences-between-handlebarsjs-and-mustache)) used in `fxa-auth-server` email templates, can accommodate very limited logic in its templates which has been a pain point for Firefox Accounts engineers and discussion around using a different templating system [began here](https:\/\/github.com\/mozilla\/fxa\/issues\/4627). While converting our emails to a more modern templating solution, there is an opportunity to evaluate what stack would be the most ideal for FxA emails beyond a proposed templating solution. This includes evaluating our CSS options, improving how we can preview emails in various states, and our localization tools, and the best approach for landing new templates in production.\\n","completion":"## Decision\n### Templating and Styling\\n- Option A - Continue to use Mustache and ad-hoc inline styles\\n- Option B - Use React server-side to generate static HTML templates with TailwindCSS\\n- Option C - Use EJS and MJML, using CSS options offered by MJML\\n### Email previewing\\n- Option A - Continue to use the `write-emails` command\\n- Option B - Use Storybook\\n- Option C - Use Mailtrap\\n---\\nFurthermore, there are a few **other decisions** worth noting that won\u2019t necessarily have pros\/cons lists:\\n- How to handle generating plaintext versions\\n- Transitioning to Fluent over GetText for localization\\n- Plans around integration involving feature flagging and the QA process\\n### Templating and Styling\\nChosen option: \"Option C - Use EJS and MJML, using CSS options offered by MJML\", because:\\n- HTML email has a lot of quirks - MJML shifts the burden of maintaining solutions for these off of FxA engineers now and in the future\\n- While we use React and Tailwind in other parts of FxA, React is heavy for an email solution since no state is involved, component reuse across FxA would likely be very minimal, and it involves a more complex build setup than EJS\\n- MJML helps significantly with responsiveness in emails, reducing time spent developing templates and making future email redesigns easier\\n### Email Previewing\\nChosen option: \"Option B - Use Storybook\", because:\\n- It provides us the flexibility to preview all of the different email states together\\n- We have a Storybook deployment already in place so hooking it up for `fxa-auth-server` will ensure consistency\\n- Easy to test and perform QA without needing to touch the codebase\\n- Support for CSS (whether it's plain old CSS, TailwindCSS or something else)\\nNote: Along with using storybook to view the various states of the templates, we are also planning on continued support of [Maildev](https:\/\/www.npmjs.com\/package\/maildev) since it has applications beyond just previewing emails.\\n### Other\\n**Plaintext files**: We'll use [html-to-text](https:\/\/github.com\/html-to-text\/node-html-to-text) to automatically generate plaintext versions of templates rendered to HTML. There may be a scenario in which the automatically-generated plaintext version does not look exactly how we'd like, in which case we can look into exporting both an MJML _and_ plaintext version of the email from the template file.\\n**Localization**: We will upgrade the localization tool from GetText to [Fluent](https:\/\/github.com\/projectfluent\/fluent.js) since it's preferred by the l10n team and other FxA front-ends are using it. With our chosen templating option we can make use of Fluent's [`@fluent\/dom`](https:\/\/github.com\/projectfluent\/fluent.js\/tree\/master\/fluent-dom) package.\\n**Integration & QA**: During development templates will be marked as being part of a release group. This could be in the form of a mapped list of template names or some variable associated with the template file. Each group will have a corresponding environment variable flag. When this release group\u2019s flag is enabled any templates that fall under it will be used when generating an email template in that environment; until a template's corresponding release group is enabled it is not used and the old\/current template will continue to be served. Release groups will be initially enabled in staging until QA has had an opportunity to thoroughly test and evaluate each template in the group, after which they can be incrementally enabled in production. To give us the most flexibiity we can add a new auth-server configuration value that can control which users email are supported and which email templates are supported for mjml. The auth-server can then expose a feature flag method to check this value and then render the correct template.\\n### Positive Consequences\\n- Using MJML abstracts HTML email peculiarities away and handles responsiveness for us\\n- Allows us to move away from inline CSS during development, and improves style reusability and consistency in template files\\n- Using EJS allows us to write templates using JavaScript, removing the complexities of custom syntaxes like JSX and Mustache\\n### Negative Consequences\\n- The Storybook setup will be much more complex than with a React and Tailwind solution\\n- Introduces new dependencies, and MJML introduces a small learning curve\\n- Our email templates have been battle-tested over the years, and this change could introduce potential new bugs across various clients\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nWhen creating a piece of infrastructure it is implied that this code would be able to be run in several environments. To illustrate this, there are three environments created, dev, uat, and prod. There will be one and only one\\nterraform module and each environment will feed it with different parameters to create distinct instances.\\n","completion":"## Decision\nThe terraform code is made multi environment and multi account through there are better tools to do that with, like terragrunt, which is outside the scope of the PoS.\\nThe credentials are assumed to be in the credentials file. the author is aware that the are more secure ways of doing this , but as the PoS needs to be easily reproducible the author has chosen to make it easy to use by dropping in the credential files in the home directory.\\nThe credentials are that of an administrator, there has been no attempt made to create assumed roles as would be done in production systems, as the goal in this case is speed.\\n"}
{"prompt":"## Context\\nIstio's service mesh capabilites are facilitated via sidecars injected into\\nworkload pods. These sidecars run an Istio-patched version of Envoy that is tied\\nto the version of Istio that injects them.\\nTypically when new versions of Istio are released, new versions of the sidecars\\nare released as well. Istio has been good so far about supporting older versions\\nof sidecars that were deployed before Istio was upgraded, but it is still\\n[documented best practice](https:\/\/istio.io\/latest\/docs\/setup\/upgrade\/) to roll\\nall the pods after an Istio upgrade.\\nAs an additional constraint, the operators of cf-for-k8s clusters expect to be\\nable to perform upgrades in one `kapp deploy`, with no post-install hooks or\\nother bash scripts. This limits our options considerably. See this [Slack\\nthread](https:\/\/cloudfoundry.slack.com\/archives\/CH9LF6V1P\/p1592521879117400) on\\nthat constraint.\\n","completion":"## Decision\nWe will use the kubernetes\\n[Job](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/job\/) resource\\nto run the kubectl command needed to roll workload pods, after waiting for the\\nnew Istio control plane to be up and healthy.\\nTo that end, we will add the necessary minimal `ServiceAccounts` and `Roles`\\nneeded to list resources in the `istio-system` namespace, and restart resources\\nin the configured workload namespace. We will also build and maintain a\\ncontainer image that contains the Job's logic.\\nAll istio components will be tagged with their Istio version so that the job can\\npositively determine that the correct version of control plane components are\\nalive and healthy. We will also name the job according to it's Istio version, so\\nthat we can take advantage of `Jobs` inherent immutability in cases where a\\ncf-for-k8s upgrade does not contain a new Istio version (pushing the same job\\nagain will not cause it to rerun, preventing workloads from rolling\\nunnecessarily). Subsequent jobs will clean up previous ones.\\n"}
{"prompt":"## Context\\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence..\\n","completion":"## Decision\n[Transit Gateway Route Analyzer](https:\/\/docs.aws.amazon.com\/vpc\/latest\/tgw\/route-analyzer.html) is an AWS tool allowing the analysis of routes in Transit Gateway Route tables. It analyzes the routing path between a specified source and destination, and returns information about the connectivity between components. It is useful in validating and troubleshooting configuration. As such, it could be used to assess the desired state for transit gateway route table configuration, providing feedback on issues.\\n"}
{"prompt":"## Context\\nOne of the goals for the CSSI grant is to be able to represent JSON data as subjects for classification in the classifier. We initially accomplished this separately for the Planet Hunters: TESS project by building a d3.js subject viewer for its specific use case (see: [ADR 8](adr-08.md)). We would like to expand this concept to be more generalizable and modular to be able to be used by other research projects that have JSON data to classify.\\n","completion":"## Decision\n### Changes from the TESS LCV\\nThe TESS LCV is built only for their use case and is not configurable. It is hard coded to expect brush stroke annotation for the classifications, zoom only in the x-axis direction, and for only one data series. We will build a generally configurable plots so that other projects can have the flexibility they need. The new plot viewers will be modular so that it can be placed into a composite, complex subject viewer as needed.\\nPreviously, the TESS LCV was built using d3.js, however, mixing d3 and react can be dangerous. The decision at the time was to use d3 because of the custom requirements needed for the TESS LCV and the react + d3 libraries were too opinionated to be used for our needs. The library d3 is also difficult to write tests for because of its chaining API. For this reason, the original TESS LCV is largely untested.\\nSince then, a library called [vx](https:\/\/vx-demo.now.sh\/) containing reusable low-level visualization react component that uses the d3 math utilities, has become more mature to start using. This fits our needs to have the DOM solely rendered by React, but still has the usefulness of a mature library like d3 to do calculations as needed. The new plots will be built using vx.\\nThe long term goal is to swap the TESS LCV over to the new `ScatterPlotViewer`, however, this means adding support for brush annotations which will be investigated at a later time.\\n### ScatterPlotViewer\\nThe scatter plot will be built with support to configure:\\n- Multiple data series\\n- Customizable data series colors to represent information as needed\\n- Pan and zoom in both axes directions or constrainable\\n- Axis inversion\\n- Customizable axis label\\n- Customizable number of axis ticks and direction\\n- Customizable margin and padding for the plot area\\n### BarChartViewer\\nA bar chart plot will be built with support to configure:\\n- Multiple data series\\n- Customizable data series colors to represent information as needed\\n- Labels for axes and for individual bars\\n### VariableStarViewer\\nThe scatter plot and the bar chart together along with the `SingleImageViewer` and a few additional controls will be a complex composite viewer built as the `VariableStarViewer`. The `VariableStarViewer` will have its own control bar that has a toggle for axis inversion, period, data series visibility, and phase focus. Each scatter plot will be individually pan and zoomable.\\n### DataImageViewer\\n_Note: Naming still TBD_\\nThis will be a complex composite consisting of a scatter plot and a single image. We may want to support up to N images, but this is still TBD. The initial build will be just the single scatter plot and single image.\\n### Future plots\\nThere may be requests to build more plot types like a line plot or map plot that renders GeoJSON. We will continue to evaluate our usage of vx at that time and ideally will continue to use it.\\n"}
{"prompt":"## Context\\nCassandra maintains a per mailbox projection for message count and unseen message count.\\nAs with any projection, it can go out of sync, leading to inconsistent results being returned to the client, which is not acceptable.\\nHere is the table organisation:\\n- `mailbox` Lists the mailboxes\\n- `messageIdTable` Holds mailbox and flags for each message, lookup by mailbox ID + UID\\n- `imapUidTable` Holds mailbox and flags for each message, lookup by message ID and serves as a source of truth\\n- `mailboxCounters` Holds messages count and unseen message count for each mailbox.\\nFailures during the denormalization process will lead to inconsistencies between the counts and the content of `imapUidTable`\\nThis can lead to the following user experience:\\n- Invalid message count can be reported in the Mail User Agent (IMAP & JMAP)\\n- Invalid message unseen count can be reported in the Mail User Agent (IMAP & JMAP)\\n","completion":"## Decision\nImplement a webadmin exposed task to recompute mailbox counters.\\nThis endpoints will:\\n- List existing mailboxes\\n- List their messages using `messageIdTable`\\n- Check them against their source of truth `imapUidTable`\\n- Compute mailbox counter values\\n- And reset the value of the counter if needed in `mailboxCounters`\\n"}
{"prompt":"## Context\\nOur current certificate structure is based on ERC-721 non-fungible tokens. This presents an issue when a part of a certificate's volume has to be transferred to another owner.\\nIn cases like these, we currently \"split\" the certificate into 2 smaller certificates, and then transfer one of the certificates to the new owner, and leave the smarthubal certificate to the smarthubal owner - deprecating the old certificate.\\nThis approach is not ideal, so we started looking into better ways of changing owners for smaller parts of the certificates.\\n","completion":"## Decision\nWe decided to use the [ERC-1888](https:\/\/github.com\/ethereum\/EIPs\/issues\/1888) Certificate structure so that we can comply and work on standardizing Certificates.\\n"}
{"prompt":"## Context\\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence.\\n","completion":"## Decision\n[IaC network tester](https:\/\/aws.amazon.com\/blogs\/networking-and-content-delivery\/integrating-network-connectivity-testing-with-infrastructure-deployment\/) is a programmatic wrapper around the [AWS Reachability Analyzer](https:\/\/docs.aws.amazon.com\/vpc\/latest\/reachability\/what-is-reachability-analyzer.html). It supports automated executions of the Reachability Analyzer, with feedback indicating if the network connectivity test was successful or not. Such a tool could allow us to automatically test network connectivity and take actions on the results.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context and Problem Statement\\nSDR is evolving into an (increasing) set of (largely) single-purpose services, with communication between these services occurring via synchronous HTTP. This architecture is fragile as it scales under load and as the number of services increases because:\\n* Each service must have knowledge about many other services.\\n* Each service depends on many other services to perform its job; any of these services being unavailable can bring down the service or hamper performance.\\n* Synchronous HTTP can have a performance penalty for operations in which a response is not required (e.g., reporting events).\\n* The Stanford network is notoriously unreliable (especially at night); significant coding effort is required to account for this.\\nIn addition, there are already places within SDR were we have re-invented a message broker and\/or jerry-rigged around not having a message broker (e.g., polling via HTTP to determine if a long-running job has completed).\\nIntroducing a message broker to support asynchronous communication where appropriate will allow decoupling of services, making SDR more resilient and scalable.\\n","completion":"## Decision\nTBD.\\n### Positive Consequences\\n* Increased ability to scale number of services.\\n* Increased ability to scale under load.\\n* Increased resilience to service failures.\\n* Provide mechanism for notification of the completion of long-running jobs.\\n### Negative Consequences\\n* Message broker must be supported by Ops.\\n* Services become dependent on the Message Broker. (However, Message Brokers are typically designed to be depended on.)\\n* The evolution work to begin using the Message Broker.\\n"}
{"prompt":"## Context\\nBecause we're deprecating fully-loaded local TestTrack server instances\\nin favor of the testtrack CLI, client-side validations are the only way\\nof ensuring that migrations will apply cleanly in production.\\nWe're entering a world where developers will likely not have local\\ncopies of all the app repositories that might contribute splits to\\nTestTrack's configuration fully updated at all times, making validating\\nsplit names locally across apps impossible and undesirable to attempt.\\nSo we're seeking to find a balance that can validate as much as we can\\nwhile accepting the fact that cross-app split dependencies won't be\\nvalidatable locally.\\n","completion":"## Decision\nfeature_completions and remote_kills of splits defined by other apps\\nwill not be validated, but you'll have to opt out in one of a few ways\\nin order to skip validation:\\n* Choose a split name prefixed with another app's name (a new-style\\nsplit), which indicates that validation of split presence is\\nimpossible.\\n* Specify a legacy non-prefixed split name with the `--no-prefix`\\noption.  Non-prefixed splits names will not be validated for presence\\nin the schema because they are not obviously tied to any app in\\nparticular, so even though a non-prefixed split might belong to our app,\\nit's not a certainty, and impossible to validate.\\n* Specify that you know that a split name doesn't appear in the schema\\nand you want to write a migration referring to it anyway via the\\n`--force` option. This is important in the case of\\ncreating\/modifying\/destroying remote kills or feature_completions for\\nretired splits which no longer appear in the schema file due to their\\nretirement.\\n"}
{"prompt":"## Context\\nSince the API version for a given resource is unpredictable, we need a way to specify the api version on a request by request basis. We also need to make sure this remains backwards and forward compatible in case versions are updated before the library is.\\n","completion":"## Decision\nAn operation should default it's API version to the latest version, but allow it to be overridden by passing in a `api_version` argument.\\n"}
{"prompt":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to `await` a `Task` running on any node of the cluster.\\n","completion":"## Decision\n* Broadcast `Event`s in `RabbitMQ`\\n"}
{"prompt":"## Context\\nWe need a way of running database migrations to the verify event store database.\\n","completion":"## Decision\nWe have chosen to use [Flyway](https:\/\/flywaydb.org) since the team has some experince with it and it seems like a simple and lightweight option.\\n"}
{"prompt":"## Context and Problem Statement\\nWe want to support most of the existing Code Review systems. Monocle queries should be agnostic regarding the data source.\\n","completion":"## Decision\nChoosen option: \"A crawler by data source and an unique and generic data schema\".\\nBecause, the addition of a data source support must not have any impact on the queries, the CLI or the WEB UI. The DB schema will be complete enough to fill the basic requirements of each source but will not implement specificities. The terminilogy will be generic for instance a Pull Request and a Review (Gerrit) will be called a \"Change\".\\n### Database objects\\n* Change: an object that describe the status of a Pull Request or Gerrit Review. The Change object will be attached attributes such as: the creation data, the author, the amount of commits within the Change, the changed files list, the title, ...\\n* ChangeCreatedEvent: an object that describe the creation event of a Change\\n* ChangeCommentedEvent: an object that describe a comment event on a given Change\\n* ChangeReviewedEvent: an object that describe a review event on a given Change\\n* ChangeCommitPushedEvent: an object that describe a commit push event on a given Change\\n* ChangeCommitForcePushedEvent: an object that describe a commit force push event on a given Change\\n* ChangeMergedEvent: an object that describe a Change merge event\\n* ChangeAbandonedEvent: an object that describe a Change abandoned event\\n"}
{"prompt":"## Context\\nWe maintain a lot of [terraform modules] which teams use to manage AWS\\nresources for their namespaces.\\nIn the past, lettings different namespaces use different versions of terraform\\nmodules has caused problems because there has not always been a clear upgrade\\npath from a module that is several versions old, to the latest version.\\nIn these cases, it would have been easier to have upgraded every namespace when\\nthe significant change was made to the module, because at that point in time\\nthe relevant knowledge and understanding are fresh. Leaving it until later\\nmakes the whole process much more difficult.\\n","completion":"## Decision\nWe decided to ensure that all namespaces are always using the latest version of\\nevery module.\\n"}
{"prompt":"## Context\\nMoving towards a suite of multiple applications, we need a library of shared Angular components that can be reused across the different apps, all with the same design language and a consistent api.\\n","completion":"## Decision\n- We will build a library of atomic ui components that will be used throughout the applications.\\n- Developers should be able to easily import only the components they need and use them in their applications.\\n- Components should be well tested and isolated in a way that doesn't break the host application's code.\\n"}
{"prompt":"## Context\\nWe need to give increasing numbers of suppliers access to the service, in order to deliver the framework transition plan. In November, we need to transition one or more frameworks with very large (1,000s) supplier user bases. Routine account management activities like changing the account email addresses, resetting passwords, and requesting new accounts, cannot be handled manually within the delivery team at this scale. Users need to be able to self-serve these activities, and internal users in the operational team need to be able to administer and support them without additional engineer involvement.\\nA permanent CCS-wide access management solution has not yet been agreed upon, and is therefore not ready for integration.\\n","completion":"## Decision\nWe will use the paid tier of Auth0 to support this scale of suppliers.\\n"}
{"prompt":"## Context\\nThe desire to define formally and unambiguously what are the component parts of a Marain instance. (An 'instance' is one deployed set of services operating in isolation from any other instance. We maintain a dev instance separate from any production use. Where customers use Marain services but are not using endjin's hosted production instance, they have their own instance.)\\n","completion":"## Decision\nThe `Marain.Instance` repo (this repo) includes a master service list, `Solutions\/MarainServices.jsonc`. This JSON (with comments) file contains an entry for each service that can be part of a Marain instance. This gives a name to the service (e.g. `Marain.Tenancy`) and identifies the GitHub project in which the service is defined. It also defines an API prefix, for use in scenarios where all services are made available behind a single API management layer\u2014the API prefix indicates what the first part of the URL should be at the API gateway for accessing the relevant service.\\nWhereas `MarainServices.jsonc` is common to all instances, each instance also defines a manifest. This determines whether particular services are deployed to a particular instance, and if so which version.\\n"}
{"prompt":"## Context\\nPanoptes-Front-End's drawing tools largely developed from previous custom projects and\/or were added one at a time to support a specific project. Because of this, several inconsistencies have been discovered in downsteam analysis and aggregation. To explain the inconsistencies, a few definitions are needed:\\n- RHC: A right handed coordinate system, this is defined as a system where positive angles rotate an object from the +x axis to the +y axis with angle=0 along the +x axis\\n- LHC: A left handed coordinate system, this is defined as a system where positive angles rotate an object from the +x axis to the -y axis with angle=0 along the +x axis\\n- Domain: The range of values a number can take [ or ] is inclusive, ( or ) is exclusive.\\n- Upper origin: The point 0, 0 is in the upper left of the plot\\n- Lower origin: The point 0, 0 is in the lower left of the plot\\nThe inconsistencies comprise of:\\n- The browser SVG coordinate systems use _RHC_ with an _upper origin_ resulting in positive angles rotating clockwise. Most plotting software (R, Python, Matlab) are _RHC_ with a _lower origin_ resulting in positive angles rotating counter-clockwise.\\n- The position of origin has been inconsistent between tools which has an effect on the final annotation too. Most use the center x, y point, but some don't\\n- Some of the drawing tools use _LHC_\\n- Some tools' annotation use `angle` some use `rotation`\\n- It's unclear when the x, y annotation refers to the center point of the shape\\n- It's unclear when the x, y annotation is being used as the point of rotation\\nSome of the mark annotation models have a few other issues as well:\\n- Some shapes have default values which an create bias. For example, the ellipse has a default axis ratio of 0.5 and many volunteers have left the default creating a bias ([comment](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/500#issuecomment-516788821))\\n- The freehand drawing tool has peformance impact on the browser as the drawing is being created and with the job to create classification exports as well. This is because the current annotation consists of every single x, y point created\\n","completion":"## Decision\nThe shape's mark annotation models should change for consistency and improved post-classification analysis in the following ways:\\n- The annotation should use the mathematical standard of _RHC_ with a domain of `[-180, 180]` for consistent angle calculation\\n- The annotation model should use `angle` for naming rotation angles. This replaces usage of `rotation`.\\n- The annotation model should replace `x` and `y` with `x_center` and `y_center` for shapes where that is applicable\\n- The exceptions are non-shapes like point, line, and transcription line tools, and non-symmetric shapes like fan.\\n- Shape clustering in aggregation is always done with the center\\n- All rotations should be defined about `x_center` and `y_center` point. If the rotation cannot be defined around the center point, then the point used should be clearly recorded in the annotation as `x_rotation` and `y_rotation`\\n- Conditional logic in component code can be avoided by using a Mobx-State-Tree's computed view functions to get either `x_center` or `x_rotation` mapped to `mark.x`. Code example:\\n```js\\nconst CircleModel = types\\n.model('CircleModel', {\\nx_center: types.optional(types.number, 0),\\ny_center: types.optional(types.number, 0),\\nradius: types.maybe(types.number),\\nangle: types.maybe(types.number)\\n})\\n.views(self => ({\\nget coords { \/\/ this is naming is reusing what's already been done with Point and Line for consistency.\\nreturn {\\nx: self.x_center,\\ny: self.y_center\\n}\\n}\\n}))\\n```\\n```js\\n\/\/ non-symmetrical shapes like fan use x_rotation, y_rotation\\nconst FanModel = types\\n.model('FanModel', {\\nx_rotation: types.optional(types.number, 0),\\ny_rotation: types.optional(types.number, 0),\\nradius: types.maybe(types.number),\\nangle: types.maybe(types.number),\\nspread: types.maybe(types.number)\\n})\\n.views(self => ({\\nget coords { \/\/ this is naming is reusing what's already been done with Point and Line for consistency.\\nreturn {\\nx: self.x_rotation,\\ny: self.y_rotation\\n}\\n}\\n}))\\n```\\n- Default values should be removed wherever possible. We will replace these with project builder configurable values set in the project builder lab when the tools are setup.\\n- The parameters will _not_ have default values suggested by us. If the parameters are not set in the lab, then when attempting to use that drawing tool in the classifier, the classifier should display an error message that the tool is not fully setup yet. The lab should also prompt for inputing a value.\\n- The lab should include instructions and a warning about the biasing effect.\\n- The tools that have defaults are ellipse, rotate rectangle, fan.\\n- The freehand drawing tools mark annotation will be a string of the SVG's path and it will be the responsibility of post-classification analysis to convert this to usable x,y points. We will include a sample script in the `DataDigging` repo for project owners to reference on how to do this. Aggregation in Caesar will have to be updated to do the conversion first.\\n- `tool` will change to `toolIndex` to clarify it is referring to the index of the input from the task area. The `toolIndex` is useful to distinguish between multiple instances of the same tool tip in a drawing task.\\n- Drawing annotations and drawing tool marks will have a `taskType` and `toolType` attribute added that map to an enumeration of the type of task or tool like `drawing` or `point`, `ellipse`, etc respectively. This enables the aggregation for Caesar code to auto-configure which extractor to use without checking data types ([comment](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/823#issuecomment-493896524)).\\n- Certain annotation models may have internal properties used denoted by a preceding underscore. These properties will be removed with the classification complete action. We will remove them to help prevent confusion by project owners in downstream analysis.\\n"}
{"prompt":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n","completion":"## Decision\nWe will use Dependabot to monitor dependency updates.\\n"}
{"prompt":"## Context\\n[Protobuf](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3)\\nseralization is not unique (i.e. there exist a practically unlimited number of\\nvalid binary representations for a protobuf document)<sup>1<\/sup>. For signature\\nverification in Cosmos SDK, signer and verifier need to agree on the same\\nserialization of a SignDoc as defined in\\n[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting the\\nserialization. This document describes a deterministic serialization scheme for\\na subset of protobuf documents, that covers this use case but can be reused in\\nother cases as well.\\n","completion":"## Decision\nThe following encoding scheme is proposed to be used by other ADRs.\\n### Scope\\nThis ADR defines a protobuf3 serializer. The output is a valid protobuf\\nserialization, such that every protobuf parser can parse it.\\nNo maps are supported in version 1 due to the complexity of defining a\\nderterministic serialization. This might change in future. Implementations must\\nreject documents containing maps as invalid input.\\n### Serialization rules\\nThe serialization is based on the\\n[protobuf 3 encoding](https:\/\/developers.google.com\/protocol-buffers\/docs\/encoding)\\nwith the following additions:\\n1. Fields must be serialized only once in ascending order\\n2. Extra fields or any extra data must not be added\\n3. [Default values](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3#default)\\nmust be omitted\\n4. `repeated` fields of scalar numeric types must use\\n[packed encoding](https:\/\/developers.google.com\/protocol-buffers\/docs\/encoding#packed)\\nby default.\\n5. Variant encoding of integers must not be longer than needed.\\nWhile rule number 1. and 2. should be pretty straight forward and describe the\\ndefault behaviour of all protobuf encoders the author is aware of, the 3rd rule\\nis more interesting. After a protobuf 3 deserialization you cannot differentiate\\nbetween unset fields and fields set to the default value<sup>2<\/sup>. At\\nserialization level however, it is possible to set the fields with an empty\\nvalue or omitting them entirely. This is a significant difference to e.g. JSON\\nwhere a property can be empty (`\"\"`, `0`), `null` or undefined, leading to 3\\ndifferent documents.\\nOmitting fields set to default values is valid because the parser must assign\\nthe default value to fields missing in the serialization<sup>3<\/sup>. For scalar\\ntypes, omitting defaults is required by the spec<sup>4<\/sup>. For `repeated`\\nfields, not serializing them is the only way to express empty lists. Enums must\\nhave a first element of numeric value 0, which is the default<sup>5<\/sup>. And\\nmessage fields default to unset<sup>6<\/sup>.\\nOmitting defaults allows for some amount of forward compatibility: users of\\nnewer versions of a protobuf schema produce the same serialization as users of\\nolder versions as long as newly added fields are not used (i.e. set to their\\ndefault value).\\n### Implementation\\nThere are three main implementation strategies, ordered from the least to the\\nmost custom development:\\n- **Use a protobuf serializer that follows the above rules by default.** E.g.\\n[gogoproto](https:\/\/pkg.go.dev\/github.com\/gogo\/protobuf\/gogoproto) is known to\\nbe compliant by in most cases, but not when certain annotations such as\\n`nullable = false` are used. It might also be an option to configure an\\nexisting serializer accordingly.\\n- **Normalize default values before encoding them.** If your serializer follows\\nrule 1. and 2. and allows you to explicitly unset fields for serialization,\\nyou can normalize default values to unset. This can be done when working with\\n[protobuf.js](https:\/\/www.npmjs.com\/package\/protobufjs):\\n```js\\nconst bytes = SignDoc.encode({\\nbodyBytes: body.length > 0 ? body : null, \/\/ normalize empty bytes to unset\\nauthInfoBytes: authInfo.length > 0 ? authInfo : null, \/\/ normalize empty bytes to unset\\nchainId: chainId || null, \/\/ normalize \"\" to unset\\naccountNumber: accountNumber || null, \/\/ normalize 0 to unset\\naccountSequence: accountSequence || null, \/\/ normalize 0 to unset\\n}).finish();\\n```\\n- **Use a hand-written serializer for the types you need.** If none of the above\\nways works for you, you can write a serializer yourself. For SignDoc this\\nwould look something like this in Go, building on existing protobuf utilities:\\n```go\\nif !signDoc.body_bytes.empty() {\\nbuf.WriteUVarInt64(0xA) \/\/ wire type and field number for body_bytes\\nbuf.WriteUVarInt64(signDoc.body_bytes.length())\\nbuf.WriteBytes(signDoc.body_bytes)\\n}\\nif !signDoc.auth_info.empty() {\\nbuf.WriteUVarInt64(0x12) \/\/ wire type and field number for auth_info\\nbuf.WriteUVarInt64(signDoc.auth_info.length())\\nbuf.WriteBytes(signDoc.auth_info)\\n}\\nif !signDoc.chain_id.empty() {\\nbuf.WriteUVarInt64(0x1a) \/\/ wire type and field number for chain_id\\nbuf.WriteUVarInt64(signDoc.chain_id.length())\\nbuf.WriteBytes(signDoc.chain_id)\\n}\\nif signDoc.account_number != 0 {\\nbuf.WriteUVarInt64(0x20) \/\/ wire type and field number for account_number\\nbuf.WriteUVarInt(signDoc.account_number)\\n}\\nif signDoc.account_sequence != 0 {\\nbuf.WriteUVarInt64(0x28) \/\/ wire type and field number for account_sequence\\nbuf.WriteUVarInt(signDoc.account_sequence)\\n}\\n```\\n### Test vectors\\nGiven the protobuf definition `Article.proto`\\n```protobuf\\npackage blog;\\nsyntax = \"proto3\";\\nenum Type {\\nUNSPECIFIED = 0;\\nIMAGES = 1;\\nNEWS = 2;\\n};\\nenum Review {\\nUNSPECIFIED = 0;\\nACCEPTED = 1;\\nREJECTED = 2;\\n};\\nmessage Article {\\nstring title = 1;\\nstring description = 2;\\nuint64 created = 3;\\nuint64 updated = 4;\\nbool public = 5;\\nbool promoted = 6;\\nType type = 7;\\nReview review = 8;\\nrepeated string comments = 9;\\nrepeated string backlinks = 10;\\n};\\n```\\nserializing the values\\n```yaml\\ntitle: \"The world needs change \ud83c\udf33\"\\ndescription: \"\"\\ncreated: 1596806111080\\nupdated: 0\\npublic: true\\npromoted: false\\ntype: Type.NEWS\\nreview: Review.UNSPECIFIED\\ncomments: [\"Nice one\", \"Thank you\"]\\nbacklinks: []\\n```\\nmust result in the serialization\\n```\\n0a1b54686520776f726c64206e65656473206368616e676520f09f8cb318e8bebec8bc2e280138024a084e696365206f6e654a095468616e6b20796f75\\n```\\nWhen inspecting the serialized document, you see that every second field is\\nomitted:\\n```\\n$ echo 0a1b54686520776f726c64206e65656473206368616e676520f09f8cb318e8bebec8bc2e280138024a084e696365206f6e654a095468616e6b20796f75 | xxd -r -p | protoc --decode_raw\\n1: \"The world needs change \\360\\237\\214\\263\"\\n3: 1596806111080\\n5: 1\\n7: 2\\n9: \"Nice one\"\\n9: \"Thank you\"\\n```\\n"}
{"prompt":"## Context\\nIn the original \"caia\" implementation, denied items were resubmitted as part\\nof every submission to CaiaSoft, as long as the item was included in the list\\nfrom Aleph.\\nAs denied items were typically being denied multiple times, this was causing\\nredundant entries to appear in the CaiaSoft interface. So it was decided not\\nresubmit denied items.\\nAfter further consideration, there was a concern that never resubmitting\\ndenied items could result in patron requests getting lost. So it was decided\\nthat denied items should be resubmitted after a sufficient interval (such as\\n7 days) as long as the denied item is still in the list provided by Aleph.\\n","completion":"## Decision\nModify the application so that items denied by CaiaSoft are resubmitted if\\nthey are still being reported by Aleph after a configurable \"wait interval\".\\n"}
{"prompt":"## Context\\nIn order to support building highly secure, robust and interoperable blockchain\\napplications, it is vital for the Cosmos SDK to expose a mechanism in which arbitrary\\nevidence can be submitted, evaluated and verified resulting in some agreed upon\\npenalty for any misbehavior committed by a validator, such as equivocation (double-voting),\\nsigning when unbonded, signing an incorrect state transition (in the future), etc.\\nFurthermore, such a mechanism is paramount for any\\n[IBC](https:\/\/github.com\/cosmos\/ics\/blob\/master\/ibc\/2_IBC_ARCHITECTURE.md) or\\ncross-chain validation protocol implementation in order to support the ability\\nfor any misbehavior to be relayed back from a collateralized chain to a primary\\nchain so that the equivocating validator(s) can be slashed.\\n","completion":"## Decision\nWe will implement an evidence module in the Cosmos SDK supporting the following\\nfunctionality:\\n- Provide developers with the abstractions and interfaces necessary to define\\ncustom evidence messages, message handlers, and methods to slash and penalize\\naccordingly for misbehavior.\\n- Support the ability to route evidence messages to handlers in any module to\\ndetermine the validity of submitted misbehavior.\\n- Support the ability, through governance, to modify slashing penalties of any\\nevidence type.\\n- Querier implementation to support querying params, evidence types, params, and\\nall submitted valid misbehavior.\\n### Types\\nFirst, we define the `Evidence` interface type. The `x\/evidence` module may implement\\nits own types that can be used by many chains (e.g. `CounterFactualEvidence`).\\nIn addition, other modules may implement their own `Evidence` types in a similar\\nmanner in which governance is extensible. It is important to note any concrete\\ntype implementing the `Evidence` interface may include arbitrary fields such as\\nan infraction time. We want the `Evidence` type to remain as flexible as possible.\\nWhen submitting evidence to the `x\/evidence` module, the concrete type must provide\\nthe validator's consensus address, which should be known by the `x\/slashing`\\nmodule (assuming the infraction is valid), the height at which the infraction\\noccurred and the validator's power at same height in which the infraction occurred.\\n```go\\ntype Evidence interface {\\nRoute() string\\nType() string\\nString() string\\nHash() HexBytes\\nValidateBasic() error\\n\/\/ The consensus address of the malicious validator at time of infraction\\nGetConsensusAddress() ConsAddress\\n\/\/ Height at which the infraction occurred\\nGetHeight() int64\\n\/\/ The total power of the malicious validator at time of infraction\\nGetValidatorPower() int64\\n\/\/ The total validator set power at time of infraction\\nGetTotalPower() int64\\n}\\n```\\n### Routing & Handling\\nEach `Evidence` type must map to a specific unique route and be registered with\\nthe `x\/evidence` module. It accomplishes this through the `Router` implementation.\\n```go\\ntype Router interface {\\nAddRoute(r string, h Handler) Router\\nHasRoute(r string) bool\\nGetRoute(path string) Handler\\nSeal()\\n}\\n```\\nUpon successful routing through the `x\/evidence` module, the `Evidence` type\\nis passed through a `Handler`. This `Handler` is responsible for executing all\\ncorresponding business logic necessary for verifying the evidence as valid. In\\naddition, the `Handler` may execute any necessary slashing and potential jailing.\\nSince slashing fractions will typically result from some form of static functions,\\nallow the `Handler` to do this provides the greatest flexibility. An example could\\nbe `k * evidence.GetValidatorPower()` where `k` is an on-chain parameter controlled\\nby governance. The `Evidence` type should provide all the external information\\nnecessary in order for the `Handler` to make the necessary state transitions.\\nIf no error is returned, the `Evidence` is considered valid.\\n```go\\ntype Handler func(Context, Evidence) error\\n```\\n### Submission\\n`Evidence` is submitted through a `MsgSubmitEvidence` message type which is internally\\nhandled by the `x\/evidence` module's `SubmitEvidence`.\\n```go\\ntype MsgSubmitEvidence struct {\\nEvidence\\n}\\nfunc handleMsgSubmitEvidence(ctx Context, keeper Keeper, msg MsgSubmitEvidence) Result {\\nif err := keeper.SubmitEvidence(ctx, msg.Evidence); err != nil {\\nreturn err.Result()\\n}\\n\/\/ emit events...\\nreturn Result{\\n\/\/ ...\\n}\\n}\\n```\\nThe `x\/evidence` module's keeper is responsible for matching the `Evidence` against\\nthe module's router and invoking the corresponding `Handler` which may include\\nslashing and jailing the validator. Upon success, the submitted evidence is persisted.\\n```go\\nfunc (k Keeper) SubmitEvidence(ctx Context, evidence Evidence) error {\\nhandler := keeper.router.GetRoute(evidence.Route())\\nif err := handler(ctx, evidence); err != nil {\\nreturn ErrInvalidEvidence(keeper.codespace, err)\\n}\\nkeeper.setEvidence(ctx, evidence)\\nreturn nil\\n}\\n```\\n### Genesis\\nFinally, we need to represent the genesis state of the `x\/evidence` module. The\\nmodule only needs a list of all submitted valid infractions and any necessary params\\nfor which the module needs in order to handle submitted evidence. The `x\/evidence`\\nmodule will naturally define and route native evidence types for which it'll most\\nlikely need slashing penalty constants for.\\n```go\\ntype GenesisState struct {\\nParams       Params\\nInfractions  []Evidence\\n}\\n```\\n"}
{"prompt":"## Context\\nDOS returns internally generated unique identifiers for files it ingests.\\nThe URLs are roughly of the form (at the time of this ADR, plain integer identifiers are returned):\\nhttps:\/\/dos-stage.mitlib.net\/file?id=123e4567-e89b-12d3-a456-556642440000\\nOnce files are ingested, the identifiers are stored in ArchivesSpace.\\nThis may result in a situation where if URLs to files need to change (e.g., if\\nthe AWS bucket name for DOS changes), all links in ArchivesSpace need to be updated for\\nall ingested files.\\nAlthough DOS can be made to generate persistent identifies, it is desirable to externalize\\nthis functionality for reuse and portability considerations.\\nThe Handle System can be used for this purpose. Handles are already being used in\\nDome and DSpace, and there is organizational familiarity with the system.\\n","completion":"## Decision\nHandle System will be used to generate persistent links. DOS will use the\\nHandle server API to generate handles. Handles will be returned when the\\nrelevant DOS end point is invoked.\\n"}
{"prompt":"## Context and Problem Statement\\nThe markdown body of ADRs cannot be used as is, because:\\n- Links between ADRs have to be replaced with correct URLs\\n- Header (status, date, deciders etc...) has to be rendered with specific components\\n## Decision Drivers <!-- optional -->\\n- Potential future development of a VSCode extension\\n","completion":"## Decision\n- Potential future development of a VSCode extension\\nChosen option: \"Option 2: the core API is responsible (with MDX)\".\\nBecause if we develop the VSCode extension, it is better to add more business logic into the core package, and it is better tested.\\n### Positive Consequences <!-- optional -->\\n- The metadata in the header is simply removed\\n### Negative Consequences <!-- optional -->\\n- Each UI package will have to implement its own Header component\\n"}
{"prompt":"## Context\\nWe want to build the framework components to work with both dynamic and static memory allocation schemes.\\n","completion":"## Decision\nRather than duplicating implementations, we will use Template Metaprogramming to use a single structure which supports both static and dynamic memory allocation. Classes can be templated with a size parameter. This size is evaluated to determine the underlying data structure that will be used. If the size is equal to 0, dynamic memory is assumed. Otherwise, static memory allocation will be used.\\n"}
{"prompt":"## Context\\nThe front end needs to provide a rich user experience, be modular and performant with large community support.\\n","completion":"## Decision\nReact.js is used as the frontend framework for building the new website. React.js is used with TypeScript, which is a superset of JavaScript.\\n"}
{"prompt":"##Context\\nWe need to choose the database for our CardOperations. The database needs to store data in persistent form but it's not required that data is stored across sessions.\\nApplication is a prototype, so it should be good for a quick setup.\\n","completion":"## Decision\nWe decided to go with H2 database for its ease of use, easy setup and familiarity.\\n"}
{"prompt":"## Context\\nTraditionally we provisioned our own redis machines in a non-clustered state.\\nAWS provide Elasticache which has a Redis engine, and can be configured to be clustered.\\nWe should consider provisioning Elasticache to replace our provisioned Redis instances.\\n","completion":"## Decision\nWe are using Elasticache instead of provisioning our own Redis instances.\\n"}
{"prompt":"## Context\\nThe diego ActualLRP syncing model as currently implemented will fetch all LRPs\\nacross all diego cells at a specified time interval (at the time of writing 10\\nseconds). As the ActualLRP count grows on a cloudfoundry deployment this could\\nimpact the performance of the BBS (large response sets coming back).\\n","completion":"## Decision\nWe want to use the [Event package](https:\/\/github.com\/cloudfoundry\/bbs\/blob\/master\/doc\/events.md)\\nto get the event stream for each ActualLRP. We will also use a bulk sync every\\n60 seconds to catch any events that were missed.\\n"}
{"prompt":"## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage\/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n","completion":"## Decision\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n[Option 1] SEPA should look to integrate Azure Active Directory with Dynamics to provide a shared identity 'federation', where backstage (SEPA) users are able to access Dynamics data via the WebAPI & perform CRUD operations using their existing single sign on login.\\nDynamics users would therefore match the users within the Active Directory, and SEPA would require a Dynamics license for each SEPA user.\\nIf SEPA's Active Directory is managed on premises, Azure AD connect can be used to auto sync existing Active Directory accounts to the Azure setup.\\n### Positive Consequences\\n* Reuse of existing SEPA logins\\n* Better backstage user experience\\n### Negative Consequences\\n* If Azure Active Directory connect is used to sync accounts, this would need to be monitored & managed by SEPA.\\n"}
{"prompt":"## Context\\nIn order to check each of the possible hands a person can make with the\\navailable cards, we'll need to generate those hands. This is a common problem\\nwith established open source implementations, there is no need to re-implement\\nit.\\nThe npm package `js-combinatorics` is very popular (almost 20k weekly\\ndownloads), has no external dependencies, and has an api to do exactly what I'll\\nneed:\\n> Combinatorics.permutation(ary, nelem)\\n> Creates a generator which generates the permutation of ary with nelem\\n> elements. When nelem is ommited, ary.length is used.\\nAlso, since the instructions ask for \"production\" level code, it's worth noting\\nthat js-combinatorics is licensed with the MIT license.\\n","completion":"## Decision\nUse [js-combinatorics](https:\/\/github.com\/dankogai\/js-combinatorics) instead of\\nwriting something bespoke to generate permutations.\\n"}
{"prompt":"## Context\\nCurrrently the routing in the UI is both complex and very inflexible. This causes a number of issues\\nevery time a new page type is added.\\n### Current format\\nThe current complete route format is defined as\\n```\\n\/questionnaire\/:questionnaireId\/:sectionId?\/:pageId?\/:confirmationId?\/:tab\\n```\\nThe regular expressions defining each of the optional ids is the same if the app received the URL\\n`\/questionnaire\/1\/2\/design`. Then `1` is always interpreted as a `questionnaireId` and `2` is always\\ninterpreted as a `sectionId` even if they weren't. So, for a page to be found a section must always\\nbe provided so for the URL `\/questionnaire\/1\/2\/3\/design`, `3` would be interpreted as a pageId.\\n### URL controlling layout\\nWe render different components based on which URL is encountered. Currently controls:\\n1. The main editor layout (combination of the final id and tab)\\n1. The properties panel (combination of the final id and tab)\\n1. Which tab is highlighted (the whole URL must match)\\n1. Which option in the left hand panel to highlight (the whole URL must match)\\n### Centralised URL management\\nCurrently all URL structures are defined in one grand URL and managed through\\n`eq-author\/src\/utils\/UrlUtils.js`. This file is both complex and changes here can have lots of\\nunintended consequences.\\n### URL used by all requests\\nThe URL is parsed at request time to generate a header that is sent with every request to the server\\nthis header is called `questionnaireId` and is used to determine which questionnaire to load and\\nchange or read from for the request.\\n### Route definition\\nCurrently the route is only defined in `eq-author\/src\/App\/QuestionnaireDesignPage\/index.js` which\\nincludes routes for lots of entity types and their tabs which is very confusing. The component also\\nhas a lot of other logic in it for controlling the left hand navigation.\\n### Issues\\n1. It is impossible to add new entity types. If you wanted to add a new entity that does not sit\\nwithin a section (e.g. survey introduction) then there is no way for the current URL format to\\nsupport this.\\n1. Generating the correct URL for the left hand panel requires the URL to maintain the tab if the\\nentity supports the tab location. This is complex and error prone and currently does not always work\\nas intended e.g. navigating from page routing to a section always to take you the first section.\\n1. After the switch to generating UUIDs for the ids of entities the current structure for pages\\nresults in long and complex urls e.g. `\/questionnaire\/95bb38ae-daf1-49c4-93fc-a87edeee7733\/afee8bd0-042b-4350-aaa1-775ac760a0b6\/f8e62d4a-2798-4524-ad39-57ca9ae5166a\/design`\\n","completion":"## Decision\n### 1. New URL format\\nThe URL format will become `\/q\/:questionnaireId:\/:entityType\/:entityId\/:tab`. This means\\nthat is can easily be changed for new entity types.\\n| Entity Type | Example design URL |\\n|---|---|\\n| Section  | `\/q\/1\/section\/1\/design`  |\\n| Page  | `\/q\/1\/page\/1\/design`  |\\n| Question confirmation  | `\/q\/1\/question-confirmation\/1\/design`  |\\n| Survey introduction  | `\/q\/1\/introduction\/1\/design`  |\\n"}
{"prompt":"## Context\\nWe've been running with the cell provisioning policy in ADR017 since February.\\nWe haven't ever run out of cell capacity, but we've observed that there's\\nexcess capacity that we're paying for.\\nAt the time that we wrote ADR017 we had fewer tenants and an individual\\ntenant's quota was a much greater proportion of total memory size. In other\\nwords a single tenant could conceivably use a greater proportion of our excess\\ncapacity.\\nCells are still deployed across 3 AZs.\\nWe still don't have a way to autoscale the number of cells to meet demand, so\\nwe need to ensure that we have surplus capacity for when we're not around.\\nCells are almost completely uncontended; we're not experiencing CPU or disk I\/O\\ncontention and not all the cell memory is being used.\\nOver the 3 month period from 1st August - 1st November\\n- Usable memory (free + cached + buffered) is running between 77% and 92% of total cell memory\\n- The maximum increase in memory usage over an exponentially smoothed average from a week previously was 36%\\n- We're running at about 10% of our total container capacity\\n- container usage has peaked at about 20% above the previous weeks\\n- Average CPU usage is about 10%. We see daily peaks of 80%\\n- reps think that about 50% of the capacity of cells is used\\n- the largest amount that rep's allocated memory increased week on week was 55%\\n","completion":"## Decision\nOur objectives are:\\nState | Expected behaviour\\n------|-------------------\\n# All cells operational | Enough capacity to allow some but not all tenants to scale up to their full quota. The amount of excess capacity required should be enough to accommodate the fluctuations we can expect over a 3 day period (weekend + reaction time)\\n# While CF being deployed | As above: enough capacity to allow some tenants to scale up to their full quota\\n# One availability zone failed\/degraded | Enough capacity to maintain steady state app usage. Not guaranteed to be able to scale apps up.\\n# More than one AZ failed | The system is not expected to have sufficient capacity to host all running apps.\\nTo achieve this we need to start basing our capacity planning on current memory\\noccupied by processes on cells, rather than the sum of all quotas given to\\nusers. We will define alerts for capacity planning purposes, the in-hours\\nsupport person is expected to respond by adjusting the number of cells.\\nWe want to ensure that cells have some headroom above a smoothed\\naverage:\\n- to allow some headroom for increases in the memory consumed by apps.\\n- to allow buffering and caching to occur and not adversely impact application\\nperformance.\\nFrom our data analysis (see context) the amount of memory consumed by apps\\ncan reach about 36% over a week-ago's smoothed average. We round up to 40% to\\ninclude buffering\/caching.\\nIf an AZ fails, we need enough capacity remaining to host all our apps. The\\nfailed AZ's apps are evenly divided amongst the surviving AZs. Because we have\\ntwo remaining AZs, each surviving AZ will have 1.5x as many apps running.\\nBecause we want 40% headroom, we'll want 1.4 (headroom) x 1.5 (evacuated apps)\\ncurrent usage. This is about 2x actual memory consumed by processes on cells.\\nTherefore we need to start alerting when the memory occupied by processes on\\ncells is above 50%, when suitably smoothed to avoid noise \/ small spikes\\ncausing frequent alarms.\\nCPU usage is assumed to be a linear relation of memory usage and we will have a\\nsimilar alert defined when it exceeds 50% on cells.\\nIn addition to wanting the cells to not run short on memory, we also want\\ntenants to be able to scale apps up and down when all AZs are functional. In\\norder to ensure this, we need to allow for a ~50% increase in requested memory,\\nwhich means alerting when all the reps have a cumulative remaining capacity of\\n~33%, when smoothed to avoid false alarms.\\nWe also need enough container capacity to allow tenants to scale apps up and\\ndown and deploy new apps. We should alert when we're using > 80% of the sum of\\nour reps' container capacity. Again, this should be smoothed to ensure that\\nshort lived fluctuations in usage don't cause unnecessary alerts.\\nIt is likely that patterns such as the fluctuation in memory use over a week\\nmay change over time. We should review this decision after 6 months.\\n"}
{"prompt":"## Context and Problem Statement\\nThe newest version of the Marketplace supports various sources for it's data.\\nWe need to agreggate the blockchain, multiple graphs and data from our partners, which themselves are represented as API requests to different servers.\\nWe don't only need to show the data to the user, but we should also provide different ways to interact with it, mainly buying and selling assets. We should also allow the user to filter the data in different ways.\\nTo do this, assets from different sources have their own version of what it means to filter them, and we might also need to interact with different contracts, depending on the operation.\\n","completion":"## Decision\nWe choose option 2 because it gives us many advantages over centralizing the logic in the front-end, like:\\n- having a server other parts of Decentraland can access\\n- making the front-end lighter and simpler, to be able to deliver the webpage faster to the user\\n- providing caching for problematic endpoints if necessary\\nbut allowing us to keep parts that are Marketplace-centric in the front end, like the interaction with partners and with contracts.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\n`service-stac` will be accepting machine-to-machine communication and will have an admin interface for operations\/debugging. Authentication methods for this two use cases need to be defined.\\n","completion":"## Decision\nMachine-to-machine communication will be using token authentication, access to the admin interface will be granted with usernames\/passwords managed in the Django admin interface. At a later stage, this might be changed to a more advanced authentication scheme.\\n"}
{"prompt":"## Context\\nAfter reviewing and reworking the aggregate API in\\n[ADR-16](0016-automatic-aggregate-creation.md) and\\n[ADR-17](0017-recreate-aggregate-after-destruction.md) we have conducted a\\nsimilar review of the process API in an effort to both simplify the API and\\nimprove consistency between the aggregate and process APIs.\\n","completion":"## Decision\n- Remove `ProcessEventScope.Begin()`.\\n- Remove `Process[Event|Timeout]Scope.HasBegun()`.\\n- Remove `Process[Event|Timeout]Scope.Root()`.\\n- Pass the process root directly to `ProcessMessageHandler.Handle[Event|Timeout]()`.\\n- Allow `Process[EventTimeout]Scope.End()` to be called at any time.\\n- Allow `Process[EventTimeout]Scope.ExecuteCommand()` and `ScheduleTimeout()` to\\nbe called at any time. Doing so should \"negate\" any prior call to `End()` as\\nthough it were never called.\\n- Routing a command message to a process instance causes that instance to begin.\\n"}
{"prompt":"## Context\\nWe need to determine a place to host our new release artifacts (tarballs). Currently releases are being served by the Extension Distributor and the release branches of the git repositories.\\n","completion":"## Decision\n- Wikibase release artifacts will be hosted on the WMF-controlled domain https:\/\/releases.wikimedia.org\/.\\n"}
{"prompt":"## Context\\nA SCRUM-based agile devlopment workflow would benefit from a central KANBAN board to keep track of userstories that have been written, are in progress, and are complete. This will help identify the sprint backlog, and the current focus of the sprint. Labels could be used to indicate size\/priority\/difficuly or value to the project, to help calculate the sprint velocity and determine what can get done inside a single sprint.\\n","completion":"## Decision\nUsing the GitHib Project page with a single project for the repo, and using Issues labelled as User Stories, with columns for \"To Do\", \"In progress\", and \"Completed\".  We can leverage some of the automatic rules in Git to help automate some of the completetion of tasks ties to Milestones for each sprint:\\nhttps:\/\/github.com\/witseie-elen4010\/2019-005-project\/projects\/1\\n"}
{"prompt":"## Context\\nCurrently, there's a little bit of pain related to serializing JS `Date` objects in NextJS. This especially comes into play on the `\/blog` page (where we sort by date) and the `blog\/[slug]` pages (where we display the last updated date).\\nUsing a custom serializer allows parsing and sending of rich JS data structures, like `Date`, `Map`, and `Set` without having to do all of the parsing manually.\\n`superjson` is used internally by `blitzjs`\u2014which I really like\u2014so I'm thinking that should be a good choice to start with. In addition, it has a Babel transform that makes this seamless and noninvasive to the codebase.\\n","completion":"## Decision\nI will add `superjson` as a dependency to serialize & deserialize initial props that have rich JS data structures.\\n"}
{"prompt":"## Context\\nWe need to be able to pull in these components to other apps as one or more NPM packages. There are pros and cons to a single-package approach versus one package per component.\\n### One Package Per Component\\nPros:\\n- Minimum dependencies pulled in with each component used. You don't download or package anything you aren't using.\\n- When you need to pull in a fix to one component, you aren't unintentionally changing the behavior of a hundred other components.\\nCons:\\n- How do we track dependencies per component but also install them so that the whole Style Guide app can run as one?\\n- Each component package has to be installed by developers as it is needed\\n- Publishing them will be tricky, especially if there are any interdependencies. Lerna + semantic-release may help, but the typical Lerna repo structure may not be ideal with the context of the Style Guide app we have here.\\n### A Single Package Exporting All Components\\nPros:\\n- Install a single package and you get access to them all in your app\\n- Much simpler to publish vs multiple packages\\n- Easier for people working on the style guide repo to understand.\\nCons:\\n- Every time you bump the dependency version of the component package, you'll pull in potentially changed versions of every component in your app.\\n- Extra work will be required to ensure that bundled JavaScript does not include any components that an app does not use.\\n### What about the style guide\\nThere is a further complication here, which is \"how does the style guide release cycle relate to the release cycle of the components within it, and how does the style guide represent multiple versions of components as they change?\"\\n","completion":"## Decision\nThe complexity of setting up, maintaining, and understanding a one-package-per-component approach, even with the help of tools like Lerna, is a very strong con. So if we assume that a single package is our preference, let's look at how we might mitigate the cons of that apprach.\\nThe first con, pulling in unwanted component changes, is MAJOR. This causes a lot of headaches for a lot of people. There are a few things we can do to avoid it:\\n- Every component change that changes the appearance of a component in any way should be exported as a new component. The whole component folder can be copied into a `v2` folder. That way, any app that pulls in an updated package will not see any changes until they change `import { Button } from \"@reactioncommerce\/components\/v1` to `import { Button } from \"@reactioncommerce\/components\/v2`\\n- No React component in this library will import any other React component in this library. All components that a component needs to render will be passed in as props or children.\\n- Every component will have Jest snapshot testing, which will give a clue to developers and reviewers that a component's appearance may have changed. Part of the review process can be deciding whether the changes are visual and whether they require splitting off a new version of the component (i.e. are considered \"breaking\" changes). There is the possibility of automating this even more through automated screen shot comparison.\\nThis approach also answers the question of how the style guide will show all versions of the component. Since all versions will remain in the repo in versioned folder names, they'll just naturally appear.\\nThe second con of a single package can be avoided by the following suggestion:\\n- Apps using the component package should set up a \"tree shaking\" solution to avoid bundling any components they do not import.\\n"}
{"prompt":"## Context and Problem Statement\\nWe would like to create a reactive Dashboard for the xebiKart. As a leaf component we will be noticed of event occurred on other component, like the car.\\nIn order to be reactive, we must find a way to notify the front of event come from back.\\n## Decision Drivers\\n* An open source standart\\n* Esay to use in Javascript or in a backend side\\n* Limit change to done on network\\n","completion":"## Decision\n* An open source standart\\n* Esay to use in Javascript or in a backend side\\n* Limit change to done on network\\nChosen option: \"[WebSocket]\", because it's a standart provide by HTTP2, supported in major language, easy to provide with network.\\n"}
{"prompt":"## Context and Problem Statement\\nVolleyManagement.SPA provides a way for users to interact. We need to select a framework which will be used to develop.\\n## Decision Drivers\\n* TypeScript support\\n* Unit Test support\\n* Skill availability\\n* UI components availability\\n","completion":"## Decision\n* TypeScript support\\n* Unit Test support\\n* Skill availability\\n* UI components availability\\nChosen option: Angular, because skill availability is better.\\n### Positive Consequences\\n* Easier to get work done as it will be easier to find person eager to contribute\\n### Negative Consequences\\n* Angular has a reputation of being complex. As a workaround we can always build Vue.JS app later\\n"}
{"prompt":"## Context\\nWe have some data that we don't want to appear in the logs (user passwords are\\na good example). Currently, they are passed around as strings, that could be\\nprinted out by mistake in a log entry (see https:\/\/github.com\/status-im\/status-react\/issues\/4053)\\n","completion":"## Decision\nTo minimize the risk of leaking passwords through logs, we should not pass\\npasswords as strings in our codebase. We introduced a new type `MaskedData` in\\n`status-im.utils.security`.\\nWe use `(security\/mask-data <data to hide>` to wrap sensitive data into this\\ntype and then use `(security\/unmask <masked-data>)` to get the plaintext back.\\nIt is important to keep that sensitive data masked as much as possible, until\\nyou need the plaintext to pass to the extenral APIs.\\nExample:\\n```clojure\\n(println (security\/mask-data \"my-plaintext-password\")) ;; Outputs \"******\"\\n(println (security\/unmask (security\/mask-data \"my-plaintext-password\"))) ;; Outputs \"my-plaintext-password\"\\n```\\n"}
{"prompt":"## Context\\nAs part of the functional requirements, we need to have a means of managing the content through an intuitive web\\ninterface. There are a couple of options to achieve this\\n### 1. As a \"traditional\" web application in the MVC architectural style, embedded within the backend (monolith)\\n#### Pros\\n- Server-side rendering sometimes can provide better user experience since content is immediately visible once\\nthe page is loaded\\n- Compatibility issues between the frontend and backend can be detected at the earlier stage\\n#### Cons\\n- Tools and libraries for the backend and frontend are mixed in the same project, making it a bit messy and mentally harder to grasp\\n- Strong coupling between web layer and backend discourages us to design general-purpose API for other types of potential consumers\\n- Provides limited level of interactivity\\n### 2. As a Single Page Application in the MVVM architectural style, packaged and deployed separately from the backend\\n#### Pros\\n- Frontend is more decoupled from backend technologies since they interact with each other via API\\n- Backend API can be potentially used by other consumers too: CLI, Native & Mobile (as long as it is general-purpose)\\n- Frontend and backend can be delivered independently (and hence faster) from each other\\n- Provides greater level of interactivity\\n#### Cons\\n- More pipelines need to be maintained in CI\\n- Compatibility issues between the frontend and backend may be detected later, during the integration stage\\n- Complete client-side rendering may degrade the user experience a bit\\n","completion":"## Decision\nWe will go for the SPA approach\\n"}
{"prompt":"## Context\\nAs a result of running the feature-compare process a bunch of files are produced. These files are essentially a _diff_ between what the operator started out with, that is the dump \/ extract he \/ she started working from, and what they ended up with, after having edited shapes and associated data in their editor. Some of those files have a `_ALL.DBF|_ALL.SHP` suffix. These files are the files we base our logic on to validate and translate to an _internal_ change request. Most notably, these files have an extra column called `RECORDTYPE` which, as the name suggests, classifies a record as either being `IDENTICAL`, `ADDED`, `MODIFIED` or `REMOVED`. Next to that most of the files contain, per record, the equivalent of a primary key that identifies a row. This primary key is used in other files as a foreign key to reference a record sitting in another file. Records with a `RECORDTYPE` of `ADDED` will use a temporary identifier (a really big number that we assume is free to use) to make this work (see [004-temporary-and-permanent-identifiers.md](004-temporary-and-permanent-identifiers.md) for how that works). One gotcha is that the primary keys are not always unique, that is, they can appear multiple times in the `*_ALL.DBF|*_ALL.SHP` files, once for each record type. A common scenario is a modification represented as a removal and an addition record.\\nFor the `WEGSEGMENT_ALL.DBF` file, things are more complicated ... next to having a `WS_OIDN` column act as primary key it has a `EVENTIDN` column acting as an alternative primary key in some cases. In case the `RECORDTYPE` is `ADDED` and the `EVENTIDN` has a value differing from `0`, the `WS_OIDN` column refers to an existing road segment and the `EVENTIDN` column refers to its new representation. In such a case, other files refer to a road segment by the value found in the `EVENTIDN`, not by the value in `WS_OIDN`. Alas, such is life ...\\n","completion":"## Decision\nModifying a road segment involves data from `WEGSEGMENT_ALL.DBF`, `WEGSEGMENT_ALL.SHP` and `ATTRIJSTROKEN_ALL.DBF`, `ATTWEGBREEDTE_ALL.DBF`, `ATTWEGVERHARDING_ALL.DBF` - that is, when it is represented as an _internal_ change request command. Each of those `.DBF` files contains a `RECORDTYPE` column. As such, a road segment could be marked as identical in `WEGSEGMENT_ALL.DB`, yet it's lanes, width and \/ or surfaces could be marked as a mixture of modified, removed, added, identical. This is the reason why a road segment that is identical is appended as a `provisional` change to the list of translated changes. Because we're not sure, just yet, that it is an actual change. The order in which these files are translated causes a `provisional` change, if warranted, to be promoted to an actual change.\\nWhy go thru all this trouble? Well, lanes, widths, and surfaces are tightly coupled to the geometry of a road segment such that it makes sense to capture them as a holistic change rather than as individual fragmented changes. There's still a bigger debate to be had about which pieces of data change together ...\\n"}
{"prompt":"## Context\\nA decision needs to be made on where the pipeline should be executed. The initial prototype has been built using [Github] Actions for it's ease of use and made it possible to quickly set up a repository and full access to a framework that allows us to execute our build steps, testing and publish artifacts. To this day very few technical downsides has been identified by using github to run the pipeline.\\nAs there is a desire to run the pipeline on Wikimedia Foundation's infrastructure an investigation into the possibility to use [PipelineLib] has concluded some drawbacks that makes this a less ideal candidate. [PipelineLib] is backed by [Blubber] which is a piece of software for generating opinionated Dockerfiles that are suited to run tests on and eventually end up in a production environment.\\nLimitations of PipelineLib and Blubber include:\\n- Does currently not support publishing tarballs (could potentially be done by injecting jenkins credentials that publish in a separate container)\\n- Does not support using base images other than those on docker-registry.wikimedia.org\\n- [Blubber] does not provide root access inside the container\\n- Does not support docker-in-docker with the builder container having access to the host docker daemon.\\n- Mainly supporting ubuntu-flavoured image building (apt is not a thing in alpine)\\nUsing WMF's Jenkins has also been considered as an alternative to WMF's PipelineLib for building non-docker artefacts.\\nAnother option would be running the pipeline on a WMDE controlled [Toolforge] VPS with some kind of CI framework or an even simpler implementation for triggering the pipeline to start. If this release process should be fully automated, some additional mechanism would then also be required to move the artifacts to the desired hosting server.\\nSome key properties of considered options are summarized in the table below:\\n| Infrastructure | Owner\/Provider | Cost of introduction | Cost of maintenance | Trusted and Secure? | WMDE can modify\/update | Has some native tooling for docker images | Has some way to build non-docker artifacts | Restriction on the source of software run on the infrastructure? |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| Github | Github\/Microsoft | low | low | no (1) | yes | yes | yes | none, everything from the internet that can be run in a container? |\\n| PipelineLib | WMF | medium-high (2) | medium (3) | yes | yes but with limitations (4) | yes | no (right now) | Only things hosted somewhere on WMF infrastructure (gerrit, phabricator diffusion, etc). Does not support using base images other than those on docker-registry.wikimedia.org. Mainly supporting ubuntu-flavoured image building (apt is not a thing in alpine) |\\n| Wikimedia Jenkins | WMF | medium-high | medium | yes | yes but with limitations (4) | no | yes | Only things hosted somewhere on WMF infrastructure (gerrit, phabricator diffusion, etc) |\\n| some WMDE infrastructure (5) | WMDE | high (6) | medium-high | yes | yes | no | yes | none, everything from the internet that can be run in a container? |\\n1. Not a complete random vendor but a third party\\n2. Will likely be different from existing WMF things, there will require some effort and negotiations with the WMF\\n3. Per likely difference from existing WMF infrastructure, there will be an effort required to maintained WMDE's \"custom\" elements\\n4. It is to be expected that WMDE staff will have limited permissions and will to a degree rely on WMF staff\\n5. e.g. running some custom solution on a VPS or containerised infrastructure either from a commercial provider or on Wikimedia Cloud Services\\n6. Even though we would expect WMDE office IT help, and\/or use the specilalized service provider, there would be significant effort on WMDE Engineering team to set up all elements of infrastructure\\n","completion":"## Decision\nAs the infrastructure that is already existing, and has the least technical limitations, Github Actions will be used to run Wikibase release pipeline. To minimize binding to a specific infrastructure, Wikibase release pipeline will be implemented as a set of Docker container images, intended to be run on the end infrastructure.\\n"}
{"prompt":"## Context\\nPython 2 support will be discontinued in 2020. Python 3.6 is the first version\\nto natively support f-strings which are sweet.\\n","completion":"## Decision\nWe make an early decision to only support Python 3.6 and above.\\n"}
{"prompt":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n","completion":"## Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\nWe are going to use Layered Architecture to help us solve those problems. In a nutshell, Layered Architecture breaks our system into four layers:\\n1. **Presentation:** This layer is responsible for showing the user all the elements for interaction.\\n2. **Application:** This layer is responsible for handling user interactions and processing them accordingly.\\n3. **Domain:** This layer is accountable for your core business. It contains all business logic to control your entities and required resources.\\n4. **Infrastructure:** As the name suggests, this layer is responsible for handling your infrastructure: interacting with databases, handling memory, and so forth.\\nThis pattern is like a Russian doll. If you drill-down inside a microservice from the presentation layer, it can have their Presentation, Application, Domain, and Infrastructure likewise.\\nA layer can also only talk with the layer below it (except for the Infrastructure layer). For example, the Presentation layer can only interact with the Application layer and the Infrastructure layer. And so forth.\\n![Diagram explaining the Layered Architecture hierarchy](..\/assets\/0004-layered-architecture\/hierarchy-diagram.png)\\n"}
{"prompt":"## Context\\nAs part of participation in the [Mapping Manuscript Migrations (MMM)][mmm] Linked Data project, the SDBM needed to export its data in RDF format for aggregation in a unified set of data from the project's three contributing organizations.\\n[mmm]: http:\/\/mappingmanuscriptmigrations.org \"Mapping Manuscript Migrations project site\"\\nAt the time the export was built a single unified data model had not been agreed upon.\\n","completion":"## Decision\nThe project decided to build a SPARQL endpoint built on Apache Jena. Since there was no target data model, the SDBM was exported to a custom namespace using a simple, direct mapping from SDBM model attributes to RDF. RabbitMQ messenger queue and a listener interface (`interface.rb`) to push updates from the SDBM to Jena. Also a simple SPARQL query interface was added for SDBM users.\\n"}
{"prompt":"## Context\\nThe decision records must be stored in a plain text format:\\n* This works well with version control systems.\\n* It allows the tool to modify the status of records and insert\\nhyperlinks when one decision supercedes another.\\n* Decisions can be read in the terminal, IDE, version control\\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\n","completion":"## Decision\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\\nDecisions will be formatted nicely and hyperlinked by the\\nbrowsers of project hosting sites like GitHub and Bitbucket.\\nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convert\\nthe decision records into HTML or PDF.\\n"}
{"prompt":"Context\\n=======\\nWe want to serve [HSTS\\nheaders](https:\/\/en.wikipedia.org\/wiki\/HTTP_Strict_Transport_Security) for all\\nHTTPS requests to the apps domains, but it will safeguard existing users from\\nbeing MITMed over insecure connections and it will improve the user experience\\nwhen they click on a hostname that doesn't have a protocol.\\n(Note that without pre-loading in browsers this won't help first time users,\\nbut that is out of context)\\nWe want to leave open the option of able to override these headers from\\nthe tenant application if they wish.\\nThis feature requires conditionally process and modify the request headers.\\nThere are several possible implementations:\\n1. Implement the logic in the `gorouter` itself: `gorouter` shall process\\nand add the header if required, by:\\n* Supporting the specific HSTS headers, and allowing configure some\\nsort of behaviour and default value.\\n* Allow inject any additional header if they are missing.\\nBut [current `gorouter` implementation](https:\/\/github.com\/cloudfoundry\/gorouter\/commit\/0d475e57b1742c42ba6d98d1ed853edc9f709893)\\ndoes not support any of these features, which require being added.\\n2. Add some intermediate proxy (for example nginx, haproxy) in front of\\nthe go-routers and after the ELB.\\n3. Implement it in a external CDN in front of PaaS origin (PaaS LB entry point):\\nAll the commercial CDN have the capacity to add additionally headers\\nconditionally.\\n4. AWS ELB: They do not support this logic and will not in the short term.\\nIn consequence they cannot be used to solve this problem.\\nDecision\\n========\\nWe do not want to add any additional logic in the CDN, as they will\\nbe an optional part of the platform and we will try to keep as simple\\nas possible.\\nWe consider that the optional solution would be implement this logic in\\nthe `gorouter`, but that requires some development effort and a PR being merged\\nupstream.\\nBecause that we will implement, in the short term, the second option: a proxy\\nin front of the `gorouter`.\\n* We will implement [HAproxy](http:\/\/www.haproxy.org\/) in front of the go router.\\n* Ha-proxy is the default LB solution for the official CF distribution.\\n* It is really powerful and has good support.\\n* Enough features to cover our needs.\\n* It will be setup colocated with the `gorouter`, proxying directly to\\nlocalhost.\\n* We will do SSL termination in HAProxy, and plain text to `gorouter`. This\\nis OK as the two services are colocated in the same VM.\\n* We will reuse the code from [official haproxy job from cf-release](https:\/\/github.com\/cloudfoundry\/cf-release\/tree\/master\/jobs\/haproxy),\\nalthough we will have to fork it to add additional settings in the\\nhaproxy configuration.\\nFuture work:\\n* We will implement and propose a PR to add logic in go-router to allow\\ndefine additional headers.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n### Positive\\n* We will be able to easily add more logic to rewrite the HTTP communication\\nto the applications using HAProxy.\\n* HAProxy SSL termination has better performance than `gorouter`, although\\nthis has a low impact because ELB is terminating the end user connections\\nand using keep alive connections to the gorouter\/haproxy.\\n* HAProxy supports web-sockets and does HTTP multiplexing.\\n* We can implement HTTP => HTTPS redirect in HAProxy.\\n### Negative\\n* Adds some additional latency to every request.\\n* We have to maintain our custom haproxy release.\\n* Another moving part to monitor and take into account.\\n### See Also\\n[ADR012](ADR012-haproxy-healthcheck\/)\\n","completion":"## Decision\n========\\nWe do not want to add any additional logic in the CDN, as they will\\nbe an optional part of the platform and we will try to keep as simple\\nas possible.\\nWe consider that the optional solution would be implement this logic in\\nthe `gorouter`, but that requires some development effort and a PR being merged\\nupstream.\\nBecause that we will implement, in the short term, the second option: a proxy\\nin front of the `gorouter`.\\n* We will implement [HAproxy](http:\/\/www.haproxy.org\/) in front of the go router.\\n* Ha-proxy is the default LB solution for the official CF distribution.\\n* It is really powerful and has good support.\\n* Enough features to cover our needs.\\n* It will be setup colocated with the `gorouter`, proxying directly to\\nlocalhost.\\n* We will do SSL termination in HAProxy, and plain text to `gorouter`. This\\nis OK as the two services are colocated in the same VM.\\n* We will reuse the code from [official haproxy job from cf-release](https:\/\/github.com\/cloudfoundry\/cf-release\/tree\/master\/jobs\/haproxy),\\nalthough we will have to fork it to add additional settings in the\\nhaproxy configuration.\\nFuture work:\\n* We will implement and propose a PR to add logic in go-router to allow\\ndefine additional headers.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n### Positive\\n* We will be able to easily add more logic to rewrite the HTTP communication\\nto the applications using HAProxy.\\n* HAProxy SSL termination has better performance than `gorouter`, although\\nthis has a low impact because ELB is terminating the end user connections\\nand using keep alive connections to the gorouter\/haproxy.\\n* HAProxy supports web-sockets and does HTTP multiplexing.\\n* We can implement HTTP => HTTPS redirect in HAProxy.\\n### Negative\\n* Adds some additional latency to every request.\\n* We have to maintain our custom haproxy release.\\n* Another moving part to monitor and take into account.\\n### See Also\\n[ADR012](ADR012-haproxy-healthcheck\/)\\n"}
{"prompt":"## Context\\nWithin GDS product teams operate autonomously and tech-ops\\nis relatively new. There is no central administration of\\nAWS. There is a shared authentication model but it is not\\nused everywhere.\\nTeams are busy with their own mission work and don't have\\ntime allocated to standardisation or security work.\\nTo get things built in a client AWS account we need to:\\n* negotiate time from the product team\\n* provide them with the code to build the resources\\n* code review with them to reassure them that it's OK to\\nrun our code in their environments\\n* repeat for each team we engage\\nWe want to make it easy and low-risk for a team to engage\\nwith us.\\nIf there were centralised management of all accounts, the\\nsensible way to do this would be by configuration the\\nconfig service with a set of config rules administered\\nand maintained centrally by Cyber Security.\\nThese rules would be largely triggered by CloudTrail\\nevents notifying us of compliance as soon as resouces were\\ncreated or changed.\\nHowever this approach would involve quite a large number\\nof resources deployed into each client AWS account which\\nmeans a longer and costlier engagement process.\\nThe simplest way to do this is by provisioning an IAM role\\nand associated policy. That way we can work from the\\nprinciple of least privilege, granting us only the access\\nwe need to run our audit making it low risk.\\nBy providing the role and policy definition in a Terraform\\nmodule we can also ensure that it requires little work by\\nclient account holders.\\nThe advantage of the IAM mechanism is that since we're\\nquerying the configuration of the user account directly\\nvia the API it is very easy to tell whether our service\\nhas been tampered with - the only thing that can change\\nis the definition of our policy which we can check.\\nIn the config service model there are more moving parts\\nwithin the client account which could be switched off,\\naltered or intercepted.\\n","completion":"## Decision\nCreate a terraform module which creates an IAM role and\\nassociated policy granting primarily read-only access\\n(Get, List, Describe API calls) working from the\\nprinciple of least privilege.\\n"}
{"prompt":"## Context\\nApplications need to be configured differently depending on where they are running. For example, the backend running locally will have different configuration then the backend running in production.\\n","completion":"## Decision\nWe will use environment variables to configure applications.\\n"}
{"prompt":"## Context and Problem Statement\\nThe [material-ui](https:\/\/material-ui.com) library is considered \"legacy\". In FDS, we\\naim to provide component coverage so that in the future, consumers will no longer need to\\nrely on material-ui.\\nThe FDS node module however, still provides a `mui-theme` theming object for the\\nmaterial-ui library.\\n","completion":"## Decision\nRemoved `mui-theme` from FDS.\\n### Negative Consequences <!-- optional -->\\n- Consumers will need to handle the breaking change. Consumers are now responsible for\\ndefining their own material-ui base theme.\\n"}
{"prompt":"## Context\\nThis document describes NATS Headers from the perspective of clients. NATS\\nheaders allow clients to specify additional meta-data in the form of headers.\\nNATS headers are similar to\\n[HTTP Headers](https:\/\/tools.ietf.org\/html\/rfc7230#section-3.2) with some important differences.\\nAs with HTTP headers:\\n- Each header field consists of a field name followed by a\\ncolon (`:`), optional leading whitespace, the field value, and optional\\ntrailing whitespace.\\n- No spaces are allowed between the header field name and colon.\\n- Field value may be preceded or followed by optional whitespace.\\n- The specification may allow any number of strange things like comments\/tokens\\netc.\\n- The keys can repeat.\\nMore specifically from [rfc822](https:\/\/www.ietf.org\/rfc\/rfc822.txt) Section\\n3.1.2:\\n> Once a field has been unfolded, it may be viewed as being composed of a\\n> field-name followed by a colon (\":\"), followed by a field-body, and terminated\\n> by a carriage-return\/line-feed. The field-name must be composed of printable\\n> ASCII characters (i.e., characters that have values between 33. and 126.,\\n> decimal, except colon). The field-body may be composed of any ASCII\\n> characters, except CR or LF. (While CR and\/or LF may be present in the actual\\n> text, they are removed by the action of unfolding the field.)\\n### Unique to NATS Headers\\n###### Version header\\nInstead of an HTTP method followed by a resource, and the HTTP version (`GET \/ HTTP\/1.1`),\\nNATS provides a string identifying the header version (`NATS\/X.x`),\\ncurrently 1.0, so it is rendered as `NATS\/1.0\u240d\u240a`.\\n###### Case preserving\\nNATS treats application headers as a part of the message _payload_ and is agnostic to the\\napplication use-case between publishers and subscribers; therefore, NATS headers are _case preserving_.\\nThe server will not change the case in message conveyance, the publisher's case will be preserved.\\nAny case sensitivity in header interpretation is the responsibility of the application and client participants.\\n> Note: This is _different_ from HTTP headers which declare\/define that web server and user-agent participants should ignore case.\\nWith above caveats, please refer to the\\n[specification](https:\/\/tools.ietf.org\/html\/rfc7230#section-3.2) for information\\non how to encode\/decode HTTP headers.\\n### Enabling Message Headers\\nThe server that is able to send and receive headers will specify so in it's\\n[`INFO`](https:\/\/docs.nats.io\/nats-protocol\/nats-protocol#info) protocol\\nmessage. The `headers` field if present, will have a boolean value. If the\\nclient wishes to send headers, it has to enable it must add a `headers` field\\nwith the `true` value in its\\n[`CONNECT` message](https:\/\/docs.nats.io\/nats-protocol\/nats-protocol#connect):\\n```\\n\"lang\": \"node\",\\n\"version\": \"1.2.3\",\\n\"protocol\": 1,\\n\"headers\": true,\\n...\\n```\\n### Publishing Messages With A Header\\nMessages that include a header have a `HPUB` protocol:\\n```\\nHPUB SUBJECT REPLY 23 30\u240d\u240aNATS\/1.0\u240d\u240aHeader: X\u240d\u240a\u240d\u240aPAYLOAD\u240d\u240a\\nHPUB SUBJECT REPLY 23 23\u240d\u240aNATS\/1.0\u240d\u240aHeader: X\u240d\u240a\u240d\u240a\u240d\u240a\\nHPUB SUBJECT REPLY 48 55\u240d\u240aNATS\/1.0\u240d\u240aHeader1: X\u240d\u240aHeader1: Y\u240d\u240aHeader2: Z\u240d\u240a\u240d\u240aPAYLOAD\u240d\u240a\\nHPUB SUBJECT REPLY 48 48\u240d\u240aNATS\/1.0\u240d\u240aHeader1: X\u240d\u240aHeader1: Y\u240d\u240aHeader2: Z\u240d\u240a\u240d\u240a\u240d\u240a\\nHPUB <SUBJ> [REPLY] <HDR_LEN> <TOT_LEN>\\n<HEADER><PAYLOAD>\\n```\\n#### NOTES:\\n- `HDR_LEN` includes the entire serialized header, from the start of the version\\nstring (`NATS\/1.0`) up to and including the \u240d\u240a before the payload\\n- `TOT_LEN` the payload length plus the HDR_LEN\\n### MSG with Headers\\nClients will see `HMSG` protocol lines for `MSG`s that contain headers\\n```\\nHMSG SUBJECT 1 REPLY 23 30\u240d\u240aNATS\/1.0\u240d\u240aHeader: X\u240d\u240a\u240d\u240aPAYLOAD\u240d\u240a\\nHMSG SUBJECT 1 REPLY 23 23\u240d\u240aNATS\/1.0\u240d\u240aHeader: X\u240d\u240a\u240d\u240a\u240d\u240a\\nHMSG SUBJECT 1 REPLY 48 55\u240d\u240aNATS\/1.0\u240d\u240aHeader1: X\u240d\u240aHeader1: Y\u240d\u240aHeader2: Z\u240d\u240a\u240d\u240aPAYLOAD\u240d\u240a\\nHMSG SUBJECT 1 REPLY 48 48\u240d\u240aNATS\/1.0\u240d\u240aHeader1: X\u240d\u240aHeader1: Y\u240d\u240aHeader2: Z\u240d\u240a\u240d\u240a\u240d\u240a\\nHMSG <SUBJECT> <SID> [REPLY] <HDR_LEN> <TOT_LEN>\\n<PAYLOAD>\\n```\\n- `HDR_LEN` includes the entire serialized header, from the start of the version\\nstring (`NATS\/1.0`) up to and including the \u240d\u240a before the payload\\n- `TOT_LEN` the payload length plus the HDR_LEN\\n","completion":"## Decision\nImplemented and merged to master.\\n"}
{"prompt":"## Context\\n[context]: #context\\nAll contents of *kartevonmorgen.org* are stored in an SQLite database, i.e. a single file\\non the server. Currently this file is backed up manually, every few days or sometimes\\nweeks. The backup is stored both on the server and on private, external storage media.\\n","completion":"## Decision\n[decision]: #decision\\n- A daily backup is generated automatically, compressed and stored on the server in a dedicated folder\\n- The backup folder is synchronized periodically (daily\/weekly?) via rsync or syncthing with external storage\\n- NTH: Old backups are selectively deleted, e.g. only te most recent 30 daily backups\\nare kept, for previous month only the last daily backup ist kept.\\n"}
{"prompt":"# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which language to write the data\\nAPI for the Performance Platform.\\nGDS has experience in running Ruby (Rails\/Sinatra) and Scala apps in\\nproduction. Choosing one of these (Ruby) would allow us to rotate people\\nacross GDS in and out of the Performance Platform team.\\nBut we have some excellent Python developers in GDS, who\\ndevelop in Ruby at work. There is a community here that we expect would be\\ninterested in working in their preferred language, so choosing Python\\nmight be an way of encouraging rotation (people would not have to leave\\nthe organisation to try a new thing) and diversity.\\nWe have:\\n- lots of people here that have operated Python applications in\\nproduction\\n- knowledge about how to architect and write Python applications\\n- an easy step to deploying Python in production\\n# Decision\\nWe will write the data API in Python.\\n# Status\\nAccepted.\\n# Consequences\\nWe will have to write (Capistrano or Fabric?) code to deploy a Python\\napplication.\\n","completion":"## Decision\nWe will write the data API in Python.\\n# Status\\nAccepted.\\n# Consequences\\nWe will have to write (Capistrano or Fabric?) code to deploy a Python\\napplication.\\n"}
{"prompt":"## Context\\nNot everything that has a style has a component in\\n[`lbh-frontend`](https:\/\/github.com\/LBHackney-IT\/lbh-frontend). We build the\\n`lbh-frontend` dependency into our distributables, making it hard for importers\\nto gain access to the stylesheets without duplicating some styles (see ADR 11).\\nReact components insulates importers of this library from the internal\\nimplementation details of how those components work. We want to continue to do\\nthat, and not require users to understand the styling hierarchy and class names.\\n","completion":"## Decision\nWhen components don't exist in `lbh-frontend` and it doesn't make sense to add\\nthem there, we will create new components in this library.\\n"}
{"prompt":"## Context\\nVisual Studio 2017 has been released for roughly a year and includes improvements to C++ standards conformance. It is C++14\\nfeature complete and has many C++17 features.\\n","completion":"## Decision\nAs part of the maintenance cycle after release 3.13 we will move the Windows compilers forward to\\nVisual Studio 2017 in a step towards requiring C++14 as a minimum standard.\\n"}
{"prompt":"## Context\\nThere are certain pieces of configuration which do not change often but need to be easy to change for colleagues who are not developers. Some examples include:\\n- the document types that the API recognises and their metadata (like ther ID, title and description)\\n- the council services which the API recognises and their metadata (like their name and google group ID).\\nThe choice came down to either storing these configurations in the database, or storing them in text files.\\n","completion":"## Decision\nUse text files for this configuration.\\n"}
{"prompt":"## Context and Problem Statement\\nisland.is will be maintaining and publishing content from many different government agencies and institutions. Their technical skill may vary a great deal, the content skill may also be lacking, therefore it is paramount for the system to be user friendly and intuitive.\\nAgencies and institutions should have enough autonomy with regards to editing content they are responsible for, to minimise the manual labour required by the island.is editors.\\nWhich CMS system would best suit the needs of island.is?\\n## Decision Drivers\\n- Content needs to be editable by non technical users\\n- Content needs to be accessible across multiple domains and platforms\\n- Setup should be simple for developers new to the project\\n- The system should manage flexible content structures to limit systems impact on design\\n- The system should be user friendly and easy to use for a non technical person\\n- The system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute\\n","completion":"## Decision\n- Content needs to be editable by non technical users\\n- Content needs to be accessible across multiple domains and platforms\\n- Setup should be simple for developers new to the project\\n- The system should manage flexible content structures to limit systems impact on design\\n- The system should be user friendly and easy to use for a non technical person\\n- The system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute\\nDevs narrowed the choice down to two options Contentful and Contentstack.\\nBoth systems meet the required featureset.\\nA decision from management was made to use Contentful.\\nContentful is deemed to have a larger presence in the Icelandic dev community.\\nContentful is also believed to have a stronger funding base.\\nContentful is already implemented in some of our projects.\\n"}
{"prompt":"## Context\\n* The application will require persistent storage for storing LPA ownership, granted access, and possibly user credentials\\n* Current MoJ strategy is to use managed services where possible\\n","completion":"## Decision\nUse DynamoDB for persistent storage\\n"}
{"prompt":"## Context and Problem Statement\\nWe need to provide a client software to consume the Monocle web API. This client software must be easily consumable via a WEB browser.\\n","completion":"## Decision\nChosen option: \"React\".\\nBecause it is a today standard. It provides a wide variety of libraries to ease building s complex UI. The React community is pretty large and lot of doc resources are available. Also as that library is popular, it might benefit the project in terms of external contributions.\\n"}
{"prompt":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\\n","completion":"## Decision\n* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\\n"}
{"prompt":"## Context\\nWhilst most created evidence of malicious behavior is self evident such that any individual can verify them independently there are types of evidence, known collectively as global evidence, that require further collaboration from the network in order to accumulate enough information to create evidence that is individually verifiable and can therefore be processed through consensus. [Fork Accountability](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/spec\/consensus\/light-client\/accountability.md) has been coined to describe the entire process of detection, proving and punishing of malicious behavior. This ADR addresses specifically what a light client amnesia attack is and how it can be proven and the current decision around handling light client amnesia attacks. For information on evidence handling by the light client, it is recommended to read [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md).\\n### Amnesia Attack\\nThe schematic below explains a scenario where an amnesia attack can occur such that two sets of honest nodes, C1 and C2, commit different blocks.\\n![](..\/imgs\/tm-amnesia-attack.png)\\n1. C1 and F send PREVOTE messages for block A.\\n2. C1 sends PRECOMMIT for round 1 for block A.\\n3. A new round is started, C2 and F send PREVOTE messages for a different block B.\\n4. C2 and F then send PRECOMMIT messages for block B.\\n5. F later on creates PRECOMMITS for block A and combines it with those from C1 to form a block\\nThis forged block can then be used to fool light clients trying to verify it. It must be stressed that there are a few more hurdles or dimensions to the attack to consider.For a more detailed walkthrough refer to Appendix A.\\n","completion":"## Decision\nThe decision surrounding amnesia attacks has both a short term and long term component. In the long term, a more sturdy protocol will need to be fleshed out and implemented. There is already draft documents outlining what such a protocol would look like and the resources it would require (see references). Prior revisions however outlined a protocol which had been implemented (See Appendix B). It was agreed that it still required greater consideration and review given it's importance. It was therefore discussed, with the limited time frame set before 0.34, whether the protocol should be completely removed or if there should remain some logic in handling the aforementioned scenarios.\\nThe latter of the two options meant storing a record of all votes in any height with which there was more than one round. This information would then be accessible for applications if they wanted to perform some off-chain verification and punishment.\\nIn summary, this seemed like too much to ask of the application to implement only on a temporary basis, whilst not having the domain specific knowledge and considering such a difficult and unlikely attack. Therefore the short term decision is to identify when the attack has occurred and implement the detector algorithm highlighted in [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md) but to not implement any accountability protocol that would identify malicious validators and allow applications to punish them. This will hopefully change in the long term with the focus on eventually reaching a concrete and secure protocol with identifying and dealing with these attacks.\\n"}
{"prompt":"## Context\\nWe have various log sources:\\n* The EKS control plane (audit logs, api service, scheduler, control-manager)\\n* VPC Flow logs\\n* Load Balancer\\n* Pod\/Application logs\\n* CloudHSM\\nAll of these with the exception of the Pod\/Application logs are stored by AWS in [CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/).\\nWe would like a single storage location for indexing and search our logs for auditing and debugging purposes.\\nGDS currently have several common storage locations for logs:\\n* Logit.io (a SaaS ELK stack provider)\\n* Self hosted ELK stacks\\n* CloudWatch\\n* Splunk\\nOptions:\\n### Option 1:\\nWe could ship the Cloudwatch logs to logit.io using AWS lambda and ship the Pod\/Application logs to Logit.io using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Logit.io instance and would end up duplicating a large number of the logs in both CloudWatch and Logit.io\\n### Option 2:\\nWe could host a dedicate ELK stack (either in cluster or from AWS's managed offering) and ingest logs from both Pods and CloudWatch into the ELK stack. Managing ELK stacks has been a maintenance burden at GDS previously and this would require duplicating logs already stored in CloudWatch.\\n### Option 3:\\nWe could ship the Pod\/Application logs to CloudWatch using [fluentd](https:\/\/www.fluentd.org\/) and expose CloudWatch insights interface to users of the platform\\n### Option 4:\\nWe could ship the CloudWatch logs to Splunk using AWS lambda and ship the Pod\/Application logs to Splunk using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Splunk instance and would end up duplicating a large number of the logs in both CloudWatch and Splunk.\\n","completion":"## Decision\nWe will use [fluentd](https:\/\/www.fluentd.org\/) to ship pod\/application logs to [AWS CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/) to aggregate all platform\/application logs to avoid double spending on log storage.\\n"}
{"prompt":"Context\\n=======\\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\\n```\\nMemory reserved by orgs: 368640 MB (360 GB)\\nMemory reserved by apps: 107108 MB (104 GB)\\nMemory actually used by apps: 32868 (32 GB)\\n```\\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\\nDecision\\n========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n","completion":"## Decision\n========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n"}
{"prompt":"## Context\\n> This section contains all the context one needs to understand the current state, and why there is a problem. It should be as succinct as possible and introduce the high level idea behind the solution.\\n","completion":"## Decision\n> This section explains all of the details of the proposed solution, including implementation details.\\n> It should also describe affects \/ corollary items that may need to be changed as a part of this.\\n> If the proposed change will be large, please also indicate a way to do the change to maximize ease of review.\\n> (e.g. the optimal split of things to do between separate PR's)\\n"}
{"prompt":"## Context\\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\\n","completion":"## Decision\nWe will use a mixture on NPM scripts and [Gulp](https:\/\/gulpjs.com) to run our tasks. NPM scripts give us the core installation and start tasks as well as the ability to run Gulp tasks. Gulp, written in javascript, is very extensible and will allow us to have complete control over compilation and assembly of the applications assets.\\n"}
{"prompt":"## Context\\nWe need to select a testing framework to use for testing node.js applications. There are\\nmany choices in this space including [Jasmine](https:\/\/jasmine.github.io\/) and\\n[Jest](https:\/\/facebook.github.io\/jest\/).\\n","completion":"## Decision\nWe have decided to use Jest as our testing framework as it provides mocking and\\nexpectation functions which would have to be provided separately to Jasmine. Additionally\\nthe snapshot testing mechanism can be used to simplify certain types of testing and is\\nparticularly beneficial to front end unit testing.\\n"}
{"prompt":"## Context\\nSome of the data that is required by the application is only needed in specific modules. Till now, `redux` has been relied on heavily and most of times for good reason. Some data, however, is only needed in specific parts of the application, but is still stored in the global store or is kept in a reducer on a per-component basis.\\nDifferent parts of the application have their own saga, reducer, actions and selectors which makes the application more difficult to understand, error prone and maintenance harder to keep up.\\nStoring all data in the global store requires a lot of (duplicate) boilerplate code, tests and mocking.\\n","completion":"## Decision\nThe structure of the application's state needs to reflect the data that is globally required. If specific data is only needed in specific parts of the application, that application's part should provide the data through a reducer and a context provider and not make use of the global (`redux`) store.\\nEssentially, the entire application's state can be provided by a context provider, but for now we'll take the bottom-up approach and gradually refactor and introduce the reducer\/context approach in favour of `redux` complexity.\\n"}
{"prompt":"## Context\\nWe want to run the smallest number of pre-production environments which give us\\nuseful feedback on changes to our applications before we deploy them to\\nproduction.\\nWe think that as we integrate with more systems (NOMIS, Delius, OASys etc) we\\nare likely to need more pre-production environments of our own in order to test\\nagainst pre-production environments of those systems. It's unlikely that data\\nwill match up well across systems in those environments so we may need to\\ncreate matching data in them ourselves in order to test our applications.\\nWe don't know yet how many pre-production environments would be useful for us\\nto have in this context.\\nWe suspect that challenges around data quality and how quickly records are\\nmatched will only be clear in production data, so we're keen to start working\\nwith production systems as soon as we can. However only NOMIS has available\\nAPIs in production, and working with only one system is less likely to reveal\\nthe scale of those challenges.\\nSo far we've set up [one environment for our new applications](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments\/tree\/1afcd91536201415b868ccebcaf1aeb8ecc2d339\/namespaces\/cloud-platform-live-0.k8s.integration.dsd.io\/offender-management-staging)\\non the new cloud platform, called Staging.\\nIt's straightforward and quick to set up further environments as we need them,\\nbut there's no way at the moment to share config between cloud platform\\nenvironments so there is a code maintenance cost to having many environments.\\nWe still need to finish setting up authentication on our applications as a\\nminimum before we start using production APIs to other systems. There's little\\nbenefit in us having a Production environment ourselves until we can do that -\\nwe're a long way from having real users.\\nSetting up authentication means that we need to pick a NOMIS pre-production\\nenvironment to use from our Staging environment, because we're going to use the\\nNOMIS OAuth2 server (see [ADR 0011](0011-use-nomis-oauth-server2-for-allocation-api-authentication.md)).\\nOur team already have comprehensive access to the T3 NOMIS environment from our\\nwork on Visit someone in prison. That environment contains anonymised data,\\nwhich is sufficient for our needs at this stage. All the NOMIS-based services\\nwe need are running there. It's commonly used as a development environment\\n(rather than staging) by other services.\\n","completion":"## Decision\nWe will start out with one shared Staging environment for our new applications.\\nWe will use the T3 NOMIS environment from that environment to start with.\\n"}
{"prompt":"## Context\\nWe need a webserver. Obvious choices between spray (or akka-http) or Play. Given we're doing very little that uses the\\nfull power of play, I'd normally have gone with akka-http, but this reactjs tutorial started with Play, so we stuck with it\\nfor the spik\\n","completion":"## Decision\nPlay\\n"}
{"prompt":"## Context\\nA couple of client projects need a simple authentication system. One of the\\nprojects already uses Flask and Postgres, while another is in the design phase.\\nIn the short term we want to have a minimal and functional authentication system\\nimplemented as soon as possible.\\nIn the very long term we hope that this implementation would be reused many\\ntimes and to have easily customized drop-in libraries for Flask projects.\\n","completion":"## Decision\nCreate a minimal reference authentication implementation that uses Flask and\\nPostgres. Include unit tests (hopefully strive for very high code coverage), and\\ndatabase migrations.\\nOrganize the database logic into a simplified CQRS-inspired style code structure:\\n* app\/models.py contain all sql models.\\n* app\/services.py contain all db commands that modify database state.\\n* app\/queries.py contain all db queries to the database.\\nDelay any features that aren't in our current project requirements. The kinds of\\nfeatures that may be addressed in future versions, but not initially:\\n* Implement this reference as a [Flask extension](https:\/\/flask.palletsprojects.com\/en\/1.1.x\/extensiondev\/), and possibly add to the PyPi public repositories.\\n* Create a React reference implementation.\\n* password resets\\n* email account verification\\n* permissions\\n* multi factor authentication\\n* OAuth\\n* Expiring tokens\\n* Revoking\/blacklisting tokens\\n* prevent re-use of old passwords\\n"}
{"prompt":"## Context\\nAs we continue to improve and evolve Chef\u2019s product architecture, we are in a position where there is a need to evolve the process itself of how architectural decisions get made and to expand the participants and seek input from a wider team.\\nThe important goals we are trying to accomplish via this process are:\\n1. More transparency and participation from everyone - including:\\n1. Proposing new architectural changes \/ patterns\\n2. An opportunity for anyone interested to chime in with their thoughts\\n3. Having this decision be known to all parts of the organization for future reference\\n2. A more streamlined decision making process and tracking\\n3. Clarity on who makes decisions\\nBelow is the process we are going to follow going forward.\\n","completion":"## Decision\nThe following is the process:\\n1. <Optional step> If you have a topic that should be considered for an architecture discussion, write a google document describing the need for the decision, the options evaluated, the recommended decision, etc\\n* The purpose of this document is to give enough context and information to proceed with the discussion.\\n* Note that there is no prescribed format for this document at this time. Just make sure that all relevant information that you can think of is captured\\n* While we want to get as much information written down as possible, please don't get blocked by trying to document everything you can think of. This is really meant to get the conversation started\\n2. Once some initial discussions have happened and you are ready to bring this to the architecture team, follow this [ADR process][1] and create a new ADR with status Proposed\\n3. Create a PR for the topic in the A2 repository.\\n* We are limiting this process to A2 for the time being.\\n4. Drop a link to the PR in the #platform-architecture slack channel so that everyone becomes aware\\nWe will automate this soon\\n5. The architecture team will then discuss all topics that have come in over the period of the week in the next architecture meeting\\n* An agenda for each week\u2019s topics will be posted a day before so people can come prepared\\n6. The person requesting a review will be invited to the meeting to present their topic and discuss.\\n* This will be an open meeting and anyone who has input and  wants to join are welcome to the meeting\\n7. The architecture team and others are expected to come in having reviewed the content and ready with their questions and thoughts\\n8. We will discuss the topic and hopefully reach a decision right in that meeting.\\n9. If further discussions are required, we will indicate that as next steps and reconvene at the earliest for follow ups\\n10. If there is no clear decision or if there are differing opinions, the engineering directors (Christoph Hartmann, Seth Falcon, Sudhir Reddy) will be responsible for resolving these and helping the team with a decision\\n11. NOTE that if a particular topic does not align with company or engineering strategy or is something not even worth considering, it is the Directors\u2019 responsibility to indicate and articulate this as early in the process as feasible.\\n12. Here is a link to the [ADR process][1] set forth to document decisions\\n13. There are a lot of good practices documented here on [decision making][2] which we\u2019ll use as appropriate over time.\\n[1]: https:\/\/github.com\/chef\/automate\/blob\/master\/dev-docs\/adr\/adr-2018-08-15.md\\n[2]: https:\/\/medium.com\/@barmstrong\/how-we-make-decisions-at-coinbase-cd6c630322e9\\n"}
{"prompt":"## Context and Problem Statement\\nI have to select a platform where I implement my event driven architecture as well as my machine learning. While the hurdle is lower with Google as it offer more ML serverless, most Biotech companies currently use AWS.\\n## Decision Drivers <!-- optional -->\\n* Learning materials and community\\n* Fit with Biotech requirements\\n* Cost\\n* Easy of use\\n","completion":"## Decision\n* Learning materials and community\\n* Fit with Biotech requirements\\n* Cost\\n* Easy of use\\nChosen option: \"AWS\", because I want to get started and therefore the basics are enough, further I can use it at work and the cost for learning are low.\\nPositive Consequences: <!-- optional -->\\n* I have lower cost and use it at work\\n* I have the stack that is searched by biotech startups\\nNegative consequences: <!-- optional -->\\n* Later I will have to check GCP\\n* I can not use it for private projects\\n"}
{"prompt":"## Context\\nA prior distribution needs to be specified to accurately calculate probabilities. The prior can\\nbe calculated in many different ways. Hopefully this decision reflects the least surprising\\nresults for the user.\\n","completion":"## Decision\nThe prior will be calculated as follows:\\n* if -p specified on command line: calculate a Poisson distribution with the specified lambda\\n* if -f specified on command line: The user has specified a root distribution. Use that. Issue a warning if the user has ALSO specified a Poisson lambda\\n* if -i specified on command line: Estimate a Poisson distribution from the families provided.\\n* Otherwise, use a uniform distribution. Issue a warning as this is not a very reasonable prior.\\n"}
{"prompt":"## Context and Problem Statement\\nExperiment analysis for the Nimbus experimentation platform occurs in a separate project called Jetstream. Jetstream delivers results to Experimenter, which must parse these results based on the expected format of Jetstream\u2019s output. However, Experimenter is forced to make assumptions about what it expects in the results, increasing the potential for buggy behavior during both ingestion and on the Results UI for Experimenter. Currently, various schemas are defined in a few different places for different purposes, but one schema definition shared between Experimenter and Jetstream would allow for validation of outputs from Jetstream while removing the need for Experimenter to make assumptions about the data it is receiving from Jetstream. This document describes the decisions around how and where that schema will be defined.\\n## Decision Drivers\\n* Single schema definition shared by Experimenter and Jetstream\\n* No functional changes to Jetstream workflow\\n* Jetstream can (and should) use the schema definition in forming\/validating outputs\\n* Process for making schema changes\\n* Simplify Experimenter ingestion of analysis results\\n","completion":"## Decision\n* Single schema definition shared by Experimenter and Jetstream\\n* No functional changes to Jetstream workflow\\n* Jetstream can (and should) use the schema definition in forming\/validating outputs\\n* Process for making schema changes\\n* Simplify Experimenter ingestion of analysis results\\nChosen option: **Option 1B**: define the schema using Pydantic, and put this definition in the `experimenter` repository. **Option 1** Pydantic was chosen because it meets the needs of the solution, and fits best with the language and approaches in use by the relevant systems. **Option B** `experimenter` repo was chosen as the location for the schema because it represents the closest thing to a monorepo (or at least somewhat consolidated location for shared content), and any schemas shared among Nimbus systems will likely be used in the `experimenter` repo.\\n### Overview of Solution Integration\\n![Architecture of chosen solution, showing location of schema and how it works with existing systems](.\/images\/jetstream-schema-arch.png)\\nSee below for more info on the options and their pros\/cons.\\n### Positive Consequences\\n* Ability to generate and ingest analysis results in a structured way\\n* Nimbus repository as central source of truth\\n* Nimbus repository may become home to Jetstream in the future\\n* Other Nimbus schemas can live in the same place\\n* Aligns with FastAPI approach used for ongoing Cirrus work\\n* Easily generate schema definitions for different scenarios\\n* Pydantic allows us to automatically generate bindings and publish new versions to external repositories (e.g., PyPi for use by Jetstream)\\n* Django, Typescript, JSON\\n* Publishing the schema as a library handles concerns about schema changes and synchronization\\n* Schema versioning is basically free via the published library\\n* Ability to reference older versions of the schema and tag results with the relevant version\\n### Negative Consequences\\n* Could be confusing to have the schema definition not live where the data it defines is generated\\n* And the definition does not live with the data\\n* Need to consume a new dependency in order to use the schema\\n### Prior Art\\nThis solution is similar in practice to how [Protocol Buffers](https:\/\/protobuf.dev\/overview\/) (also known as Protobuf) are used to document schemas. For example, Google uses them extensively as a data format that provides typed schemas and serialization for structured data. Our solution could be used in a similarly broad fashion, and Protobufs would be applicable for our use-case, but Pydantic provides more familiarity to a team and organization with much greater usage of Python (in particular, both Experimenter and Jetstream).\\n"}
{"prompt":"## Context\\nAs the Endpoint ressource has two controlling structures.\\n1. Endpoint Reconciler\\n2. Prober (ProbeManager and according Workers)\\n","completion":"## Decision\nThere is the need to seperate concerns in that resource. Therefore:\\n### Endpoint Reconciler\\nIs responsible for modifying:\\n* ObjectMeta\\n* General Structure for Subsets\\n* It makes sure exactly the same IPs in the Externalservice ressource exist also in the Endpoint\\n* If not it will add the missing IPs always to the NotReadyAddresses List\\n### Prober\\nThe Prober package is soley responsible for moving Addresses from Addresses to NotReadyAddresses and vice versa.\\n"}
{"prompt":"# Context\\nWe need to store the imported data from the _Connect_ process in an\\nhomogeneous way for easy retrieval.\\n# Decision\\nAfter checking how other tools work (like Kibana) it seems that\\nElasticSearch is a good choice for storing the imported data.\\nElasticSearch has its own DSL for building complex queries and we can\\nreuse that part.\\n# Status\\nSuperseded. See [ADR-004](ADR-004.md)\\n# Consequences\\n* We need to know how to run and maintain a ElasticSearch cluster\\n* We need to address the security of the cluster ourselves. Elastic\\nhas a security plugin but is not open source.\\nExamples:\\n- http:\/\/floragunn.com\/searchguard\\n- http:\/\/keycloak.github.io\/docs\/userguide\/keycloak-server\/html\/proxy.html\\n","completion":"## Decision\nAfter checking how other tools work (like Kibana) it seems that\\nElasticSearch is a good choice for storing the imported data.\\nElasticSearch has its own DSL for building complex queries and we can\\nreuse that part.\\n# Status\\nSuperseded. See [ADR-004](ADR-004.md)\\n# Consequences\\n* We need to know how to run and maintain a ElasticSearch cluster\\n* We need to address the security of the cluster ourselves. Elastic\\nhas a security plugin but is not open source.\\nExamples:\\n- http:\/\/floragunn.com\/searchguard\\n- http:\/\/keycloak.github.io\/docs\/userguide\/keycloak-server\/html\/proxy.html\\n"}
{"prompt":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","completion":"## Decision\nThe change that we're proposing or have agreed to implement.\\n"}
{"prompt":"## Context\\nVersioning APIs can be controversial.  Ideally, APIs don't need to change, or be additive so as not to break backwards compatibility.  Occasionally, some changes require bigger changes.\\nWe will do our best to be open w\/ what we accept and strict w\/ what we return\\nThere are lots of ways an API can change:\\n* Breaking changes across the entire API (all endpoints change from REST to GraphQL for example)\\n* Breaking changes at a resource level (\/v1\/booking) needs to change\\n* Breaking changes in behavior\\nThis decision currently applies to the 1st bullet.\\n","completion":"## Decision\nAll API endpoints will start with the version of the API (e.g. \/v1\/resource).  We don't anticipate this to change.\\n"}
{"prompt":"## Context\\nThe type of data representing elements in consumers may differ from the type of data native to prosemirror-elements elements. This is the case at the Guardian, where our document format (flexible-model) has an element definition that doesn\u2019t correspond with the elements we define with prosemirror-elements.\\nThis is always likely to be the case \u2013 there\u2019s usually a difference between data as it\u2019s represented in a model, and data represented in forms designed to feed that model.\\nAs a result, currently prosemirror-elements has a `transformIn` and `transformOut` API, that allows consumers to include the transformation between external and internal types in their element.\\nAs we worked with this API, a few concerns were raised:\\n- It makes our types harder to reason about, both in terms of writing them, and in terms of reasoning about them as a consumer when things go wrong.\\n- Reasoning about partial representations of external data, and hydrating to their complete or partial internal counterparts felt complex.\\n- Coupling the definition of our element with a particular set of data requirements for a particular consumer felt arbitrary.\\nUltimately, these concerns led us to explore alternatives to this approach.\\n","completion":"## Decision\nWe decided to remove the transformers API, and make it the consumer\u2019s responsibility to do this work.\\n"}
{"prompt":"## Context\\nReact can be implemented from scratch, using the `create-react-app` CLI, or by using a 3rd-party framework such as Next.js.\\n","completion":"## Decision\nWe will re-start the scaffold.\\nWe will use Next.js as the basis for this project.\\nWe will extend and customise Next.js to fit our more bespoke requirements.\\n"}
{"prompt":"## Context\\nData Engineers are building performance dashboards for Snippets using the\\nmetrics we collect from Firefox Telemetry. Telemetry pings include only basic\\ninformation about the Snippet, like Snippet ID.\\nFor better to understand and more complete dashboards, we want to enhance the\\nTelemetry received information with more Snippet metadata, like campaign\\ninformation, included URL, main message used and others.\\nTo achieve this we will export the metadata from the Snippets Service in a\\nmachine readable format and make the file available in a Cloud Storage Provider.\\nThen Data Engineers will import the metadata and combine them Telemetry data in\\nunified dashboards.\\nGitHub Issue: [#887](https:\/\/github.com\/mozmeao\/snippets-service\/issues\/887)\\n","completion":"## Decision\n- Export in CSV format.\\n- Create a cron job to export and upload resulting file to S3.\\n- The job will run daily, on early morning UTC hours.\\n- The job will be monitored using Dead Man's Snitch and report to the usual\\nnotification channels that project developers follow.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nPart of the Raster Foundry processing workflow requires a source imagery ingest into a GeoTrellis Catalog. GeoTrellis depends on Apache Spark to parallelize work during an ingest, and that work typically occurs on an Apache Spark enabled compute cluster.\\nPrior iterations of Raster Foundry attempted ingests with an Apache YARN managed compute cluster via Amazon Elastic MapReduce (EMR) on a per-request basis. Unfortunately, that didn't perform well due to the overhead of bootstrapping an EMR cluster.\\nOur goal for the this iteration aims to keep some Spark cluster components active at all times through a combination of spot pricing and usage based cluster auto-scaling. This approach should help minimize cluster bootstrapping durations and keep cost-incurring cluster resources at a minimum.\\n","completion":"## Decision\nGiven that we are still early in Raster Foundry's product development, the desire to keep costs low and ship quickly carry significant weight. In order to meet those objectives, but still minimize cluster bootstrapping overhead, the first pass at having Spark enabled resources on standby will consist of a shared (across staging and production) Amazon EMR cluster.\\nThe overall Raster Foundry infrastructure will be split across three Amazon Virtual Private Clouds (VPCs):\\n- Staging (`10.0.0.0\/18`, 16382 IPs)\\n- Spark (`10.0.64.0\/18`, 16382 IPs)\\n- Production (`10.0.128.0\/18`, 16382 IPs)\\nThe `Staging` and `Production` VPCs will be peered with the `Spark` VPC to allow direct private network access, if necessary. Regardless, public facing APIs such as the Amazon EMR's Step API, or the Raster Foundry API can be used to pass information between systems.\\nDespite having thought long and hard about the consequences, I suspect that the section below does not cover all of the issues that lie ahead of us. The hope is that at least these core ideas remain in future iterations of the Spark execution environment:\\n- Shared cluster resources between environments\\n- Isolated environments at the VPC level\\n- Build on top of the Amazon EMR platform\\n- Use spot pricing and auto-scaling aggressively to keep costs low\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nOur back-end needs to store state somehow.\\nTraditional Object-Relational systems store the current state of the application as the source of truth.\\nWhile these are simple to build, they are less conducive to changes in the way data is stored in the long run.\\nSystems built under our current shareholders have shown to be very prone to change over time, which gives us the need for systems that are conducive to change.\\nOur experience with our previous system under these shareholders, `Wegwijs`, has shown to be very permissive to change. `Wegwijs` had rather simple data manipulations, with the difficulty of the system lying in the presentation of this data.\\nWe built `Wegwijs` using event sourcing, which uses a record of all changes as its source of truth. These events can then be used to create different representations of the data using `projections`.\\nDue to this, we were able to capture the manipulations easily, while still allowing the stakeholders to change their mind about the presentation of this data.\\n","completion":"## Decision\nWe will use event sourcing the single source of truth for our application.\\n"}
{"prompt":"## Context\\nTo access and modify a user's articles stored in Pocket, they have to give us permission. Once permission has been granted, we have to be careful not to leak this privillaged access to malicious parties.\\nASP.NET Core has [Data Protection APIs](https:\/\/docs.microsoft.com\/en-us\/aspnet\/core\/security\/data-protection\/introduction) for managing the encryption and decryption of data. These APIs requires the private key data to be persisted in a location where it will not be destroyed when the application is restarted.\\n","completion":"## Decision\nWe will use the encrypted ID of a PocketAccount entity as an access token, representing the right to access a particular user's articles.\\nWe will encrypt the IDs of Pocket articles returned in API responses, using the ASP.NET Core Data Protection APIs.\\nWe will store the private key material in the database.\\n"}
{"prompt":"Context\\n=======\\nWe use [Bosh](https:\/\/bosh.io\/) to create and manage our cloudfoundry deployment on AWS.\\nTo deploy software, Bosh needs certain binary dependencies available.\\nThese are known as bosh [releases](https:\/\/bosh.io\/docs\/release.html).\\nBefore this decision, we usually built and uploaded releases to Bosh as part of our [concourse](https:\/\/concourse.ci\/) pipeline.\\nOccasionally, we would manually build a release, store it on GitHub, and point Bosh to it there.\\n### Building Bosh Releases\\nWe investigated different approaches to creating bosh releases, in particular\\n* Multiple pipelines created dynamically using [branch manager](https:\/\/github.com\/alphagov\/paas-concourse-branch-manager)\\n* A single pipeline using [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource)\\nThe work on these spikes was recorded in\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/115142265\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/128937731\\nDecision\\n========\\nWe will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe must gradually migrate all our Bosh release builds to their own build pipelines.\\nWe will need separate jobs to build from master - this already has a proof of concept in the spike.\\nWe may have to add additional config in projects we fork to allow us to create final builds.\\n","completion":"## Decision\n========\\nWe will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe must gradually migrate all our Bosh release builds to their own build pipelines.\\nWe will need separate jobs to build from master - this already has a proof of concept in the spike.\\nWe may have to add additional config in projects we fork to allow us to create final builds.\\n"}
{"prompt":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n","completion":"## Decision\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n"}
{"prompt":"## Context\\neLife has several microservices (and larger projects) written in PHP.\\nIn order to provide language version consistency across projects we need to get a consensus on which versions we are going to support.\\nThere is an exception, `crm` not being upgraded on PHP 7 (using 5.6 instead) but supporting it.\\nStaying up to date with the major and minor versions of PHP is important:\\n- to be able to use new features and libraries (nice to have)\\n- to keep working on a version that receives bug fixes and security updates (must have)\\nAll infrastructure is currently based on:\\n- Ubuntu 14.04 (doesn't have PHP 7.x by default)\\n- Ubuntu 16.04 (does have PHP 7.0 by default)\\n- a popular [PPA](https:\/\/launchpad.net\/~ondrej\/+archive\/ubuntu\/php) filling in the blanks, supporting 7.0, 7.1, and 7.2.\\n- official [PHP Docker images](https:\/\/hub.docker.com\/_\/php\/) supporting 5.6, 7.0, 7.1, and 7.2.\\nPHP 7.0 has ceased active support, but has [security support](http:\/\/php.net\/supported-versions.php) until 2018-12-03.\\n","completion":"## Decision\nWe will use PHP 7.0 on all existing and new PHP projects bar exceptions that do not support it (`crm` if needed).\\nWe will upgrade to PHP 7.1, PHP 7.2 and similar minor versions as a concerted effort on all libraries and projects, before the end of the security support.\\n"}
{"prompt":"## Context\\nOur queries are mostly bounded to a mailbox or a user. We can easily\\nlimit the number of ElasticSearch nodes involved in a given query by\\ngrouping the underlying documents on the same node using a routing key.\\nWithout a routing key, each shard needs to execute the query. The coordinator\\nneeds also to be waiting for the slowest shard.\\nUsing the routing key unlocks significant throughput enhancement (proportional\\nto the number of shards) and also a possible high percentile latency enhancement.\\nAs most requests are restricted to a single coordination, most search requests will\\nhit a single shard, as opposed to non routed searches which would have hit each shards\\n(each shard would return the number of searched documents, to be ordered and limited\\nagain in the coordination node). This allows to be more linearly scalable.\\n","completion":"## Decision\nEnable ElasticSearch routing.\\nMessages should be indexed by mailbox.\\nQuota Ratio should be indexed by user.\\n"}
{"prompt":"## Context\\nWe decided to make the jump from React to Preact. Main advantages was reducing the bundle size, with minimal changes to the architecture.\\nPreact had been quite good at keeping up with React features, which was one of the main [previous concerns.](https:\/\/github.com\/guardian\/dotcom-rendering\/blob\/main\/docs\/architecture\/003-react.md)\\nPreact uses a lib called [`preact\/compat`](https:\/\/preactjs.com\/guide\/v10\/switching-to-preact) which allows us to not have a major refactoring to embrace React.\\n","completion":"## Decision\nUsing webpack aliases we can use `preact\/compat` to intercept `react` and `react-dom` imports. We should keep writing components as if we are using react in order to make React compatibility an easy transition if needed.\\n`preact\/compat` add only 2kb extra to preact bundle, but even this has enabled us to go from ~93kb to ~66kb in our react bundle.\\n"}
{"prompt":"## [Context](https:\/\/docs.google.com\/document\/d\/1ayJlohp2-s49c1_oypDE1bAfgYa5shR0I1GM6KRzDNA\/edit)\\nLibero projects may span a single product or possibly more, but they live in a single namespace of the [Libero Github organization](https:\/\/github.com\/libero). Hence the projects need to:\\n- be attributable to the originating product\\n- avoid clashes in naming (e.g. many `article-store` projects)\\n","completion":"## Decision\n- Name repositories that cover multiple products without prefixes (e.g. `infrastructure`, `community`)\\n- Name internal and\/or product-specific repositories with prefix for specific product (e.g. `reviewer-submission`, `my-product-storybook`)\\n- Name generic repositories with no prefix (e.g. `media-type`)\\n- Name forked repositories with the original name, no matter how bad (e.g. [`elifelibero-avada-child-theme`](https:\/\/github.com\/libero\/elifelibero-avada-child-theme), [`texture`](https:\/\/github.com\/libero\/texture))\\n- Name Docker images following Github repository names (e.g. [`reviewer-submission`](https:\/\/hub.docker.com\/r\/liberoadmin\/reviewer-submission))\\n- Name NPM packages using `@libero` followed by Github repository names (e.g. [`@libero\/event-bus`](https:\/\/www.npmjs.com\/package\/@libero\/event-bus)\\n"}
{"prompt":"## Context\\ncljdoc's deployment story has been simplistic but effective. To recap:\\n- During CI a zip file is pushed to S3 that contains all files to run the application\\n- On the live server there is systemd service that will download an archive and run it. The\\nversion of the downloaded archive is specified via a file on the server.\\nUpdating simply required updating a file on the server and restarting the service.\\nThe issue with this approach however is that every time a new release was pushed to the server\\nthe restart of the systemd service would incur up to a minute of downtime. While this generally\\nisn't a huge deal it discourages certain development practices that may be desirable such as\\nContinuous Deployment.\\nOur existing deployment setup (and tools used by it) are poorly equipped to handle this kind of\\ndeployment scenario. A large amount of bash scripts would be required to start a new cljdoc server,\\nwait for it to become available, update nginx's upstream port, and kill old cljdoc server instances\\nin a repeatable, automated manner. Likely these bash scripts would be error-prone and turn into\\nsomething nobody likes to touch.\\n","completion":"## Decision\nImplement a *canary deploy* mechanism for the cljdoc server application using\\n[Nomad](https:\/\/nomadproject.io) and [Traefik](https:\/\/traefik.io).\\nWhile both of these tools are probably aimed at much more complex workloads they provide the\\nfollowing benefits over the existing systemd\/Nginx setup:\\n- Automatic SSL certificates via Lets Encrypt\\n- Declarative specification of jobs and their desired update semantics\\n- APIs to schedule new jobs and cycle old\/new deployments\\n- Health checks to verify new deployments work as expected\\n- Machine images become much simpler since they only need Nomad, Consul, Docker\\nThis simplifies a lot of cljdoc's operational tasks while also enabling Continuous Deployment.\\n"}
{"prompt":"## Context\\nSvelte offers a very interesting `readable` API that could work with this library.\\nAn implementation might look similar to this:\\n```ts\\nexport function readable<T>(initialValue: T, updater?: Updater<T>) {\\nconst value = ref(initialValue);\\nupdater?.((newValue: T) => {\\nvalue.current = newValue;\\n});\\nreturn {\\nget current() {\\nreturn value.current;\\n},\\n};\\n}\\n```\\nThe idea behind it is that it would provide a readonly way to having changing content. Similar to what an Observable would provide.\\nOne of the major questions, however, is whether this API would be beneficial or whether we should aim for something else.\\n","completion":"## Decision\nWhile the `readable` API on its own offers a very nice functionality, it does not add enough to make up for the required learning effort as it does not blend in\\nwell enough with the framework.\\n"}
{"prompt":"### Context\\nThere has been interest from other teams using LRUD that the ability to _cancel_ a movement as it's happening would be useful.\\nFor example, focussing on a node and pressing down would ordinarily take you to the specified node, but perhaps the developer wants to run some business logic at that point that would mean that _actually we don't want the move to happen._\\n### Decision\\nWe have decided to implement this feature in LRUD. It will be useful for the specific team that requested the feature, and as further discussions have happened, all interested parties agree that there are valid use cases in many different scenarios for such a feature.\\n### Status\\nApproved\\n### Consequences\\nIt makes LRUD internally more complex (and alongside that makes the final bundle larger too). However, it is only marginally increasing the bundle size, and we feel the complexity is managable and well understood.\\n### Further Reading\\n- [Github issue discussing topic of cancellable movement](https:\/\/github.com\/bbc\/lrud\/issues\/25)\\n","completion":"## Decision\nWe have decided to implement this feature in LRUD. It will be useful for the specific team that requested the feature, and as further discussions have happened, all interested parties agree that there are valid use cases in many different scenarios for such a feature.\\n### Status\\nApproved\\n### Consequences\\nIt makes LRUD internally more complex (and alongside that makes the final bundle larger too). However, it is only marginally increasing the bundle size, and we feel the complexity is managable and well understood.\\n### Further Reading\\n- [Github issue discussing topic of cancellable movement](https:\/\/github.com\/bbc\/lrud\/issues\/25)\\n"}
{"prompt":"## Context\\nIn order to satisfy the [RA-5](https:\/\/nvd.nist.gov\/800-53\/Rev4\/control\/RA-5)\\ncontrol around vulnerability scanning, we wish to scan all user-uploaded files\\nwith a malware detection service. We want to satisfy the following requirements.\\n1. Scanning can be done asyncronously so as not to negatively impact the user experience.\\n2. Scanning should be loosely coupled to main application to allow for more resiliance and fault tolerance.\\n3. Scanning should be retried if malware detection service is unavailable.\\n4. Scanning should run on a seperate instance to prevent a negative impact to the user experience.\\n","completion":"## Decision\nWe will use redis as a queue and build a worker node which will take jobs from the queue, send them to the malware detection service and then update the database with the scan results.\\n"}
{"prompt":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to decide on a Helm Chart Repository that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Helm Charts. When we looked for a solution, we considered that this is going to be a public-facing customer repository. We also took into consideration the need for a custom domain for it.\\nThe criterias we looked for when we investigated each option are: if it uses AWS and Quay, if it is a PaaS\/Cloud Storage solution, if the project is mature enough for our use case, if it is a personal project or if it is maintained by a company and if it requires IT support (if it does can it be avoided by altering the pipeline).  We considered that authentication and SSL is not needed.\\nWe had several options that we considered. The full list of pros and cons for each option can be found [here](https:\/\/issues.alfresco.com\/jira\/secure\/attachment\/97743\/DEPLOY-150%20Helm%20Chart%20Repos.xlsx). More comments on the investigation we have done can be found [here](https:\/\/issues.alfresco.com\/jira\/browse\/DEPLOY-150).\\n","completion":"## Decision\nWe will use Github Pages to store the Helm Charts. The reasons why we elected this solution are: it is a cloud storage solution, it is mature project that has been actively mantained, doesn't require IT support and it offers an easy, well-known pipeline.\\n"}
{"prompt":"## Context\\n* Minio already has a UI and an S3 API that allow administration of files and buckets.\\n* There are existing client libraries for the S3 API.\\n* It is nontrivial to proxy large file uploads through the armadillo server.\\n","completion":"## Decision\nThe Armadillo client will be written as a high-level library on top of an existing S3 API.\\n"}
{"prompt":"## Context\\nIn order to support fast block-level district editing by users, it will be necessary to push a large amount of static data up to the client-side. It may initially appear that we will be able to get all the data we need from the vector tiles, but given that block-level vector tiles need to be zoom-level restricted (due to the abundance of features at lower zoom levels), there are some operations that require additional information. For example, say a user loads a set of districts where districts split blockgroups (a common situation). If a user wants to select a split blockgroup to move from one district to another, it won't be possible to calculate changes in demographic data on the client-side for the changed districts without having the appropriate block-level vector tiles available.\\nThe solution to this problem is to load static block-level data for demographics on the client-side. Another piece of static data that's needed is a mapping of which blocks belong to geounits in higher geolevels. For example, we need to know which blocks belong to each blockgroup, so we'll be able to collect their underlying demographic data to perform calculations. For a more detailed conversation about how this data will be used on the front-end when a user makes a selection on the map, see [this](https:\/\/github.com\/PublicMapping\/district-builder\/pull\/18#issuecomment-572671925) conversation, particularly the answer to the third question.\\n","completion":"## Decision\nThrough testing, we've determined that in order to efficiently store large amounts of static data in client-side memory, we will need to make use of [typed arrays](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Typed_arrays). The reduced memory footprint and efficient storage mechanics make typed arrays ideal for storing a large amount of fixed integer arrays, and from our testing, the lack of sluggishness was immediately apparent, even with a large amount of data loaded.\\nIn order to make use of typed arrays, we need to stick with the theme of [ADR 02](.\/adr-02-district-data-storage.md), and make use of an internally consistent geolevel ordering across all static data files. The demographic static data will be an array that contains an element for each base geounit (e.g. block), whose value is set to the demographic value for the demographic type in question. That means there will be one file for each demographic type. Similarly, the geolevel static data will be an array that contains an element for each base geounit whose value is set to the index of the geounit for the geolevel type in question. Again, there will be one file for each non-base geolevel. Take counties and blocks for an example. There are 3 counties in Delaware, and the idea is to create a flat array that will specify which blocks belong to which counties. So we need to create a file that is as long as the number of blocks, where each element will either be 0, 1, or 2, depending on which county that belongs to. All of these files will have the same length, since they are all mappings of geounits contained in the base geolevel.\\nSince the data is all static, we will be able to know in advance the information contained within these files, and will be able to programatically make decisions about what kinds of typed arrays to use upon creation. For most files, we will need to use `Uint16Array`, which can store values from 0 to 65535, but in some situations we'll be able to use `Uint8Array`, which can store values from 0 to 255, and is half the size. Unlikely, but if a situation ever demands the need for even larger numbers, we can use `Uint32Array`, which expands to 4294967295.\\nWe've tested the ability to both store data files in such a format, as well as read them back in, on both the server-side and client-side, and there are no problems. On the client-side, the `fetch` function supports specifying that the input data is an `ArrayBuffer`, and then the proper typed array can be constructed without any overhead.\\n"}
{"prompt":"## Context\\nOur user facing applications will need to authenticate users.\\nIn the long-term, we want to use the standard user sign-on services in CCS for\\neach appropriate user group.\\nFor suppliers, this is the [Supplier Registration Service (SRS)][service-srs].\\nFor CCS staff it is their G-Suite account.\\nUsing a single authentication service will reduce administration overheads\\nbecause we won't have to manage a JML (Joiners Movers Leavers) process. For\\nusers it will also mean they don't have to remember yet another username and\\npassword combination.\\nHowever, we want to get moving quickly, and integration with SRS\/CCS G-Suite\\nwill take time. For now, we want a simple sign-in service that we can integrate\\nwith quickly, to enable us to test and iterate the application.\\n","completion":"## Decision\nWe will use [Auth0][service-auth0] for authentication on a temporary basis.\\nAuth0 is an authentication service that uses the OAuth protocol. It provides\\nand simple integration path, and is free for several thousand active users.\\nWe acknowledge that we are incurring technical debt by not integrating\\nimmediately with SRS and G-Suite, however we believe that this will let us move\\nmore quickly during development.\\nThis decision will also allow us to gain a better understanding of the user\\nneeds around authentication before a decision around the final authentication\\nsolution is made.\\nWe will replace Auth0 with the proper authentication service later in the\\ndevelopment process.\\n"}
{"prompt":"## Context\\nI come back to the katas and my formatting style I currently use has\\nchanged, but it makes no sense to adapt all katas in here every time.\\nThese katas are a long-term thingy, they live for a long time, so this\\nshould be defined and settled once and for all.\\n","completion":"## Decision\nA kata looks like the following:\\n```\\ndescribe('`Reflect` basics', function() {     \/\/ The test description is a proper English sentence, so it starts upper case.\\ndescribe('Reflect is special, it is different to e.g. `Object`', function() {\\nit('it`s of type object', function() {\\n\/\/\/\/ const expectedType = 'not a function!';\\nconst expectedType = 'object';\\nassert.equal(typeof Reflect, expectedType);\\n});\\n});\\n});\\n\/\/ the file ends with an empty line\\n```\\nThere are no unnecessary spaces and new lines, also not before the assert.\\nIn some rare cases where a new line really helps readability it's ok, but it should\\nbe the exception.\\nAll lines should be as short as possible.\\nOverall, most important be consistent!\\n"}
{"prompt":"## Context\\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\\nand the server side code (which will be a dropwizard app).\\nDepending on how the users want to run the service provider we may need\\ndifferent security solutions.\\n","completion":"## Decision\nIf possible users can talk to the service provider on the loopback (127.0.0.1)\\nIf that doesn't work for some reason then they can use the dropwizard config\\nto set up basic auth or tls or something.\\nSee http:\/\/www.dropwizard.io\/1.1.0\/docs\/manual\/configuration.html#connectors\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"Context\\n-------\\nThere are three basic ways to lay out controller nodes in a kubernetes\\ncluster:\\n1.  A single master is the easiest way to do it, but if that node is\\never unavailable, then the entire cluster is unavailable.\\n2.  A highly-available group of at least three controller nodes that\\nalso run etcd.\\n3.  Two highly-available groups: one of at least three controller nodes,\\nand the other of at least three etcd nodes.\\nThe kubernetes website has [a detailed rundown][1], but it boils down to\\nthis: high availability complicates the set-up significantly, and\\nseparating etcd from the control plane complicates it yet more.\\nHowever, if we want to allocate a lot of resources to high availability\\nin our kubernetes clusters, then etcd is a nice seam to further divide\\nresponsibility as we scale up the count of control nodes.\\n[1]: https:\/\/kubernetes.io\/docs\/setup\/independent\/ha-topology\/\\nDecision\\n--------\\nWe will start with the middle path of minimal (but extant) high\\navailability. Low availability would fail to grant us a lot of what\\nmakes kubernetes worthwhile, but we're just not in a place where we're\\nready to commit the extra resources to get the highest availability\\npossible.\\nConsequences\\n------------\\nIf we start growing our pool of stacked controller nodes beyond 5, then\\nwe should consider revisiting this decision.\\n","completion":"## Decision\n--------\\nWe will start with the middle path of minimal (but extant) high\\navailability. Low availability would fail to grant us a lot of what\\nmakes kubernetes worthwhile, but we're just not in a place where we're\\nready to commit the extra resources to get the highest availability\\npossible.\\nConsequences\\n------------\\nIf we start growing our pool of stacked controller nodes beyond 5, then\\nwe should consider revisiting this decision.\\n"}
{"prompt":"## Context\\nWe need a database in order to store the data provided by the user in the different forms.\\n","completion":"## Decision\nTo reduce the amount of integrations and complexity, we've decided to use one of the databases that Firebase offers.\\nUnder the Firebase umbrella are Real-time database and Firestore which are similar in many features.\\nGoogle recommends Firestore for new developers, as it is built upon \u201cthe successes of Real-time database\u201d. Also, for our purposes, Firestore is a better choice because:\\n- It offers offline support for iOS, Android and Web clients whereas real-time DB supports only iOS and Android clients.\\n- The one-big-JSON-tree structure of real-time can become difficult to maintain as the DB grows. Document-based Firestore offers better organisation provisions using collections.\\n- Scaling on firestore is automatic whereas scaling on real-time database requires sharding.\\n"}
{"prompt":"### Context\\nThis is the beginning on the project and for the moment all the commit are done on the master branch\\n### Decision\\nUse Gitflow to handle code source and project management.\\n### Consequences\\nInstall the gitflow plugin, explain how to contribute to the project with gitflow in the how to contribute documentation file.\\n","completion":"## Decision\nUse Gitflow to handle code source and project management.\\n### Consequences\\nInstall the gitflow plugin, explain how to contribute to the project with gitflow in the how to contribute documentation file.\\n"}
{"prompt":"## Context and Problem Statement\\nGraphics files have to take a consistent file name\\n## Decision Drivers <!-- optional -->\\n* Easy to process by Jekyll\\n* No WTFs at the creators\\n","completion":"## Decision\n* Easy to process by Jekyll\\n* No WTFs at the creators\\nChosen option: \"Dasherization\", because\\n* clear separation of parts of the name\\n* consistent to other URLs (which are typically lowercase)\\n"}
{"prompt":"## Context\\nWith the goal of producing services (e.g., REST APIs) to support the UniProt website, a programming\\nlanguage should be chosen so that the requirements can be implemented. The choice will\\nleverage the skills of the team of developers working on this project.\\n","completion":"## Decision\nThis particular UniProt REST API project will use Java, and currently version 8.\\nEven though Java 11 is already available, many companies still have not made the leap from 8 to 9\\ndue to the possible amount of work required. In future, we do seek to upgrade to Java 8+.\\n"}
{"prompt":"## Context\\nIn general, we have decided to rewrite the admin UI as we port link-sf into the new Link Platform project. This is because we intend to make a number of improvements on its reliability and UX consistency.\\nWe considered using the [Admin on rest](https:\/\/github.com\/marmelab\/admin-on-rest) library which would give us a lot of data handling for free, including a REST client and validation. However, when we use [Zendesk Garden](https:\/\/garden.zendesk.com\/) we will also get validation styles, and then will be able to a) control how we handle data ourselves, b) support another one of Zendesk's open source projects, and c) get the nicer styles provided by Garden which are also reflective of the Zendesk brand.\\n","completion":"## Decision\nAdmin pages will be rewritten using React, Zendesk Garden, and a REST client to be determined. We will use Create React App to scaffold the directory. Its contents will be separated into folders for each stateful component.\\n"}
{"prompt":"## Context\\n1. Team members know little about last iteration and don\u2019t know which part is good, bad or need improve;\\n2. New team members know nothing about the team\u2019s history and don\u2019t know the best practices for this team;\\n3. We need learn from what we done.\\n","completion":"## Decision\n### Description\\nAfter each iteration we'll spend one hour discussing relevant topics we add during the iteration in our retrospective board.\\nThe idea is to spend quality time and get the most value out of this meeting, therefore each team member is encouraged to bring discussion topics as well as suggestion for improvement.\\n### Preparation\\n* The SM marks the current iteration as finished, moving all the unfinished tasks to the new iteration;\\n* Check the dashboard and analyze our metrics for the last finished iteration. Find out all problematic tasks with `low Flow Efficiency` or `low Quality Score`;\\n* Safety check;\\n* The SM remind team members in team channel at least 10 minutes before starting the meeting;\\n* Everyone is available;\\n* Everyone is encouraged to prepared the `Good`,`Bad` and `Need improve` cards;\\n* Something that went well or wrong;\\n* Any type of improvement suggestion you might think of. Regarding the process, the code base, the collaboration etc.\\n* Add description\/comments to existing ones;\\n* Votes(optional);\\n### Process\\n* Discuss problematic tasks that have bad metrics, in order to understand how we can improve;\\n* Identify the root cause of the problem;\\n* Come up with ideas about how we could have done it better, so that we can learn from our mistakes;\\n* The SM prioritizes the topics, based on team member's votes (optional);\\n* Follow up on the previous iteration's discussion (optional);\\n* Follow up on existing action items;\\n* During the meeting the SM records the outcomes and action items in our retrospective board;\\n* At the end of the meeting we should save 10 min for a brief overview of the past iteration(recently completed features, features in progress, planed features, velocity graphs and user story vs bugs graphs)\\n### Notice\\n* Starts on time and all the team members must participate;\\n* Discuss the accumulated cards in our Backlog, 5 minutes\/card;\\n* Each topic is time-boxed. However after the 5min timer expires, if everyone agrees we need further discussion, we restart the timer one more time.\\n* Anybody can stop the current discussion if it's clear we're going nowhere;\\n* We keep the retrospective meetings within 1 hour.\\n"}
{"prompt":"## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n","completion":"## Decision\nWe will use Prometheus to monitor and alert on the state of the application.\\n"}
{"prompt":"## Context\\nSDK modules currently use the `KVStore` interface and `Codec` to access their respective state. While\\nthis provides a large degree of freedom to module developers, it is hard to modularize and the UX is\\nmediocre.\\nFirst, each time a module tries to access the state, it has to marshal the value and set or get the\\nvalue and finally unmarshal. Usually this is done by declaring `Keeper.GetXXX` and `Keeper.SetXXX` functions,\\nwhich are repetitive and hard to maintain.\\nSecond, this makes it harder to align with the object capability theorem: the right to access the\\nstate is defined as a `StoreKey`, which gives full access on the entire Merkle tree, so a module cannot\\nsend the access right to a specific key-value pair (or a set of key-value pairs) to another module safely.\\nFinally, because the getter\/setter functions are defined as methods of a module's `Keeper`, the reviewers\\nhave to consider the whole Merkle tree space when they reviewing a function accessing any part of the state.\\nThere is no static way to know which part of the state that the function is accessing (and which is not).\\n","completion":"## Decision\nWe will define a type named `Value`:\\n```go\\ntype Value struct {\\nm   Mapping\\nkey []byte\\n}\\n```\\nThe `Value` works as a reference for a key-value pair in the state, where `Value.m` defines the key-value\\nspace it will access and `Value.key` defines the exact key for the reference.\\nWe will define a type named `Mapping`:\\n```go\\ntype Mapping struct {\\nstoreKey sdk.StoreKey\\ncdc      *codec.LegacyAmino\\nprefix   []byte\\n}\\n```\\nThe `Mapping` works as a reference for a key-value space in the state, where `Mapping.storeKey` defines\\nthe IAVL (sub-)tree and `Mapping.prefix` defines the optional subspace prefix.\\nWe will define the following core methods for the `Value` type:\\n```go\\n\/\/ Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\\nfunc (Value) Get(ctx Context, ptr interface{}) {}\\n\/\/ Get and unmarshal stored data, return error if not exists or cannot unmarshal\\nfunc (Value) GetSafe(ctx Context, ptr interface{}) {}\\n\/\/ Get stored data as raw byte slice\\nfunc (Value) GetRaw(ctx Context) []byte {}\\n\/\/ Marshal and set a raw value\\nfunc (Value) Set(ctx Context, o interface{}) {}\\n\/\/ Check if a raw value exists\\nfunc (Value) Exists(ctx Context) bool {}\\n\/\/ Delete a raw value value\\nfunc (Value) Delete(ctx Context) {}\\n```\\nWe will define the following core methods for the `Mapping` type:\\n```go\\n\/\/ Constructs key-value pair reference corresponding to the key argument in the Mapping space\\nfunc (Mapping) Value(key []byte) Value {}\\n\/\/ Get and unmarshal stored data, noop if not exists, panic if cannot unmarshal\\nfunc (Mapping) Get(ctx Context, key []byte, ptr interface{}) {}\\n\/\/ Get and unmarshal stored data, return error if not exists or cannot unmarshal\\nfunc (Mapping) GetSafe(ctx Context, key []byte, ptr interface{})\\n\/\/ Get stored data as raw byte slice\\nfunc (Mapping) GetRaw(ctx Context, key []byte) []byte {}\\n\/\/ Marshal and set a raw value\\nfunc (Mapping) Set(ctx Context, key []byte, o interface{}) {}\\n\/\/ Check if a raw value exists\\nfunc (Mapping) Has(ctx Context, key []byte) bool {}\\n\/\/ Delete a raw value value\\nfunc (Mapping) Delete(ctx Context, key []byte) {}\\n```\\nEach method of the `Mapping` type that is passed the arugments `ctx`, `key`, and `args...` will proxy\\nthe call to `Mapping.Value(key)` with arguments `ctx` and `args...`.\\nIn addition, we will define and provide a common set of types derived from the `Value` type:\\n```go\\ntype Boolean struct { Value }\\ntype Enum struct { Value }\\ntype Integer struct { Value; enc IntEncoding }\\ntype String struct { Value }\\n\/\/ ...\\n```\\nWhere the encoding schemes can be different, `o` arguments in core methods are typed, and `ptr` arguments\\nin core methods are replaced by explicit return types.\\nFinally, we will define a family of types derived from the `Mapping` type:\\n```go\\ntype Indexer struct {\\nm   Mapping\\nenc IntEncoding\\n}\\n```\\nWhere the `key` argument in core method is typed.\\nSome of the properties of the accessor types are:\\n- State access happens only when a function which takes a `Context` as an argument is invoked\\n- Accessor type structs give rights to access the state only that the struct is referring, no other\\n- Marshalling\/Unmarshalling happens implicitly within the core methods\\n"}
{"prompt":"## Context\\nWith the first alpha version of our API released and the first vendor integrations on the the horizon, we need a plan around minor and major revisions to our API. Our API baseurl is already versioned ('\/api\/v1') but it was suggested we should explore 'extension' mechanisms to the API so that we are able to release previews of API changes to vendors or supply custom fields to specific vendors, if needed, without creating problems for everyone else. There is some concern that the tools or libraries vendors may use to integrate with our API break when they encounter a new\/unknown attribute, instead of just ignoring it.\\nThe main idea explored was giving vendors the ability to request optional fields as URL query parameters (e.g. ```\/?extensions=key1,key2```), which would ensure:\\n- only vendors who explicitly requested optional fields would receive them, and\\n- the set of optional fields requested would always stay the same, unless vendors update the URL they use for their API requests\\nTherefore, we tried to come up with a plan which would provide such functionality while consistent with the API choices made so far.\\nAn ideal 'extensions' mechanism would provide:\\n- an update path for API changes,\\n- which does not obstruct API use for vendors who expect payloads never to change,\\n- while allows enough customisation for vendors with more agile teams\\n### Findings\\nAlthough our API is not advertised as a [JSON:API](https:\/\/jsonapi.org\/), it follows many conventions of the spec, so we tried to plan an 'extensions' mechanism consistent with JSON:API guidelines. JSON:API allows the use of ```\/?fields=``` and ```\/?include=```.\\nHowever, the JSON:API spec contains:\\n> If a client requests a restricted set of fields for a given resource type, an endpoint MUST NOT include additional fields in resource objects of that type in its response.\\nand\\n> A resource object\u2019s attributes and its relationships are collectively called its \u201cfields\u201d.\\nThis would work fine for vendors not specifying ```\/?fields=``` but would result in very long URLs for vendors requesting optional fields, as they would need to specify all fields they need (unless we use something like a non-standard ```\/?fieldset=``` grouping).\\nWe could, in theory, use ```\/?include=``` to specify groups of attributes to be included in a separate part of the response (JSON:API provides an ```included:``` section), but strictly speaking these extra fields are not really additional resources. There would also be some complexity around mapping data between the main attributes block and the ```included:``` section, even more for one-to-many associations\/lists, especially in the absence of ids.\\nIf we wanted to proceed with either ```\/?fields```= or ```\/?include=```, we would probably need to consider changing the presentation of resources to make use of the ```relationships:``` and ```included:``` blocks, as is good JSON:API practice. This, in effect, would mean we start advertising our API as a JSON:API one, as parsing relationships and associations properly would then require the use of JSON:API consumer library, for practical purposes. JSON:API libraries exist in many languages and ecosystems, but their level of maturity varies.\\n#### Pros\\n- implementing an extensions API would allow us to pre-release features to specific vendors\\n- agile vendors could optimise their requests, by requesting exactly the fields that they need\\n- we would not need to increment our API version with every minor field addition\\n#### Cons\\n- it is unclear what the best approach for adding this extensions mechanism is (```\/?fields=``` vs ```\/?include=``` vs ```\/?fieldset=```, plus do we merge fields in main attributes block or do we add to the ```included:``` section)\\n- if we were to follow JSON:API recommendations and put associations in ```relationships:``` and ```included:``` blocks, it would be easier to add ```\/?fields=``` and ```\/?included=``` support, but this would make our responses hard to parse by vendors that do not have JSON:API libraries.\\n- implementing such an 'extensions' mechanism may be overkill for a small number of vendors and minor API revisions a year\\n- an 'extensions' mechanism could, actually, be introduced as part of a minor release at any point, does not need to be implemented now\\n","completion":"## Decision\nWe have decided to not to implement an 'extensions' mechanism at this point because there is no immediate need and we are likely to make a better choice of implementation when and if such a need does arise. If additional fields need to be released, we shall take the traditional route of a minor version release, ensuring no breaking changes are included in the release. The current advertised API versioning strategy can be found here: [https:\/\/apply-for-postgraduate-teacher-training-tech-docs.cloudapps.digital\/#versioning](https:\/\/apply-for-postgraduate-teacher-training-tech-docs.cloudapps.digital\/#versioning)\\n"}
{"prompt":"## Context\\nWe need a mechanism to capture and manage application errors. Without an\\nintegration our debugging options are to access a console on live environments\\nand try to replicate (something we want to minimise) or by looking through\\ninformation provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision\\nsince. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the\\ntransition to business as usual. Due to this we will have a handful of projects\\nusing Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment.\\nDalmatian-<project> and paas-<project> both exist. There also exists both\\npermutations for the same project as we transition. We have used ROLLBAR_ENV to\\nmanage this before so making it explicit will hopefully make it clearer how it\\ncan be changed.\\n","completion":"## Decision\nUse Rollbar to collect and manage our application errors.\\n"}
{"prompt":"## Context\\nThe \"Fundraising Frontend Content\" repository is a git repository where the Fundraising Department can make edits to the translated messages and texts of the Fundraising Application. Those changes get deployed automatically, independently from the code deployments.\\nThe [`wmde19` skin](008_Client_Side_Rewrite.md) uses client-side rendering and the [Vue i18n](https:\/\/kazupon.github.io\/vue-i18n\/) plugin for translating messages. There are several possibilities to get the translated strings into the client-side code:\\n1. Importing it directly in JavaScript, with an `import` statement. This requires a continuous delivery pipeline that creates a new client-side code bundle on every content change.\\n2. Loading it asynchronously when the client-side code loads. This has the benefit of working out of the box, but the drawback of an additional HTTP request.\\n3. Reading the file on the server side and putting its contents in a HTML [data attribute](https:\/\/developer.mozilla.org\/en-US\/docs\/Learn\/HTML\/Howto\/Use_data_attributes) where the bootstrap code will read it an inject it into the i18n plugin.\\n","completion":"## Decision\nSince we don't have the engineering resources to create a continuous delivery pipeline, only options 2 and 3 remain. We choose the data attribute method for performance reasons: We want one less HTTP request and the size of the messages is acceptable: At the time of writing this ADR, it's 30K uncompressed, 7K compressed.\\nWe want to keep the message size down and implement the server-side code in a way that allows for splitting the messages into \"common\" and page-specific bundles.\\n"}
{"prompt":"## Context\\nI'm having second thoughts on shipping examples as one of the next main things for cljdoc. Second to a [clojure.spec integration](https:\/\/github.com\/cljdoc\/cljdoc\/issues\/67) it would be a huge feature but I'm no longer sure it's impact will be proportional.\\nMany people suggested examples as a feature for cljdoc but I think this might be more due to their familiarity with them from sites like clojuredocs.org \u2014 rather than library authors actively looking for ways to add structured examples to their libraries.\\nVarious hypothesis that would support the addition of examples haven't been tested or verified sufficiently:\\n- Are examples useful for libraries? Their usefulness is mostly proven with regards to standard library functions which are tiny in scope and mostly operate on plain data. Libraries often require more complex setup.\\n- Will library authors add examples to their libraries?\\n- Is a community-repository necessary? This has been mostly introduced to allow adding examples to Clojure itself.\\n","completion":"## Decision\nDon't ship examples just yet.\\ncljdoc is great because library authors don't have to do anything to get great looking documentation, let's keep it that way and focus on broader adoption, i.e. features that deliver value without requiring extra work from library authors.\\n"}
{"prompt":"## Context\\nWe need some form of background framework to do asynchronous tasks such as sending emails, creating accounts, and possibly some long import\/export operations.\\nWe'll be evaluating for\\n- Priority or multiple queues\\n- Retriable jobs\\n- Scheduling\\n- Persistent queues\\n- Realtime dashboards\\n### Which Library?\\nOur use cases aren't particularly complicated.  Our main concerns are ease of integration and reliability.  In order of popularity from [The Ruby Toolbox](https:\/\/www.ruby-toolbox.com\/categories\/Background_Jobs), let's take a look.\\n#### Sidekiq\\nhttps:\/\/github.com\/mperham\/sidekiq\\nSidekiq is the most popular Ruby job running framework by a fair margin.  It is well maintained and time tested.  It supports Ruby 2.0+ and Rails 3.2+, and all state is maintained in Redis.\\nThe feature set ticks all the boxes:\\n- [x] Priority or multiple queues\\n- [x] Retriable jobs\\n- [x] Scheduling\\n- [x] Persistent queues\\n- [x] Realtime dashboards\\n#### Resque\\nhttps:\/\/github.com\/resque\/resque\\nWe use Resque here at Zendesk so we have plenty of domain knowledge.  Similarly to Sidekiq, Ruby 2.0+ and 3+ are supported, and Redis is the state store.\\nFeatures include:\\n- [x] Priority or multiple queues\\n- [x] Retriable jobs\\n- [x] Scheduling\\n- [x] Persistent queues\\n- [x] Realtime dashboards\\n#### Delayed Job\\nhttps:\/\/github.com\/collectiveidea\/delayed_job\\nDelayedJob inspired the creation of Resque, and has most of the same features.  It's main shortcomings are around its resiliency and throughput since state is stored in the database using ActiveRecord.  It has Ruby 2+ and Rails 3+ support.\\nIt has:\\n- [x] Priority or multiple queues\\n- [x] Retriable jobs\\n- [x] Scheduling\\n- [x] Persistent queues\\n- [ ] Realtime dashboards\\n### Sucker Punch\\nhttps:\/\/github.com\/brandonhilkert\/sucker_punch\\nSucker Punch is perhaps the simplest of the frameworks we're exploring.  Unlike the previous options, Sucker Punch stores all of its state in memory, meaning there's no external dependency for it to work.  Ruby 2+ is supported.\\nThe features:\\n- [x] Priority or multiple queues\\n- [ ] Retriable jobs\\n- [x] Scheduling\\n- [ ] Persistent queues\\n- [ ] Realtime dashboards\\n","completion":"## Decision\nWe will use DelayedJob as our background job framework.\\nWhile Sidekiq and Resque both offer more features, and Sucker Punch is extremely light weight, DelayedJob provides us a great set of features with no extra dependencies since we already have a database.  It integrates seamlessly with ActiveRecord, and should scale to meet our needs.\\nTo quote directly from [the Resque documentation](https:\/\/github.com\/resque\/resque):\\n> If you're doing Rails development, you already have a database and ActiveRecord. DelayedJob is super easy to setup and works great. GitHub used it for many months to process almost 200 million jobs.\\n[Here's another great reference I used for some comparisons.](https:\/\/scoutapm.com\/blog\/which-ruby-background-job-framework-is-right-for-you)\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nTo ensure the RODA application has the highest availability, we want\\nto have confidence that the application is resourced properly in times of\\nlow and high usage.\\n","completion":"## Decision\ndxw have used the Skylight gem in many other projects and there is a Skylight\\naccount available for use. We have prior knowledge of using Skylight in\\nother projects.\\nSkylight provides a web interface and graphing so we can more easily identify\\npain points in our application which will cause it to become less performant\\nunder high load. It can identify slow-running parts of the service down to the\\ncontroller action level, and indicate where - for example - excessive database\\ncalls or slow queries are causing degradation in the user's experience.\\nCommon alternatives to Skylight are New Relic and DataDog. We do have some\\nexperience with DataDog at dxw, but we had more confidence within the team with\\nusing Skylight.\\n"}
{"prompt":"## Context and Problem Statement\\nThis proposal explains the raison d'\u00eatre for the *global operator*. The *global operator* is an operator that contains controllers for resources that span more than one deployment.\\nAdditionally, this proposal outlines deploying an operator per namespace that is responsible for managing namespace-local resources, such as individual Elasticsearch clusters and Kibana nodes.\\n## Decision Drivers\\n### Prior state\\nAs of when this proposal was written, a single stack operator was running in the \"stack-operators-system\" namespace:\\nAll controllers watch all the namespaces for CRDs we define and reconcile them within the namespace they are defined in.\\n### Motivation\\nWe want to be able to address the following concerns:\\n- Security: RBAC does not allow us to limit the resources we may be watching based on their labels or annotations so our operator has to have a very wide set of permissions (practically close to full admin on all K8s cluster resources)\\n- All controllers watch a lot of K8s API resources in all namespaces. This has multiple downsides:\\n- Unnecessary load put on K8s apiserver as we'll be watching resources that are not related to our use case (e.g listening to all Services, ConfigMaps, Secrets etc). We can expect some improvements from controller-runtime in this area, but the time-frame is not clear.\\n- Single point of failure: if the operator crashes, not only the triggering resources will be affected, but an entire region.\\n- Slowly responding clusters \/ clusters with a large cluster state may negatively affect performance \/ resource consumption.\\n- Resource usage as number of deployed resources grow is difficult to ascertain a-priori.\\n- To upgrade the version of the operator, all operations need to go down temporarily (this may be mitigated at least somewhat by running multiple operators and introduce leader election between them).\\n- May or may not be fine: No ability to slowly roll changes out, needs to be all or nothing.\\n","completion":"## Decision\n### Prior state\\nAs of when this proposal was written, a single stack operator was running in the \"stack-operators-system\" namespace:\\nAll controllers watch all the namespaces for CRDs we define and reconcile them within the namespace they are defined in.\\n### Motivation\\nWe want to be able to address the following concerns:\\n- Security: RBAC does not allow us to limit the resources we may be watching based on their labels or annotations so our operator has to have a very wide set of permissions (practically close to full admin on all K8s cluster resources)\\n- All controllers watch a lot of K8s API resources in all namespaces. This has multiple downsides:\\n- Unnecessary load put on K8s apiserver as we'll be watching resources that are not related to our use case (e.g listening to all Services, ConfigMaps, Secrets etc). We can expect some improvements from controller-runtime in this area, but the time-frame is not clear.\\n- Single point of failure: if the operator crashes, not only the triggering resources will be affected, but an entire region.\\n- Slowly responding clusters \/ clusters with a large cluster state may negatively affect performance \/ resource consumption.\\n- Resource usage as number of deployed resources grow is difficult to ascertain a-priori.\\n- To upgrade the version of the operator, all operations need to go down temporarily (this may be mitigated at least somewhat by running multiple operators and introduce leader election between them).\\n- May or may not be fine: No ability to slowly roll changes out, needs to be all or nothing.\\nSuperseded by [005](https:\/\/github.com\/elastic\/cloud-on-k8s\/blob\/main\/docs\/design\/0005-configurable-operator.md).\\n### Positive Consequences <!-- optional -->\\n+ Can limit what each operator can do based on namespace (trivial through RBAC).\\n+ Can attribute resource usage of the operator to namespaces \/ customers \/ projects.\\n+ Enables rolling out updates in a controlled manner.\\n### Negative Consequences <!-- optional -->\\n- Introduces more than one operator, complicating deployment and debugging.\\n- Controllers in the global operator, such as the CCR controller still need to be scaled on a number-of-clusters \/ associations basis (but it does not need to connect to individual ES clusters).\\n"}
{"prompt":"## Context\\nWe will be storing sensitive council data within S3 and therefore need to restrict access to this data based on the\\ndepartment the user belongs to.\\n","completion":"## Decision\nIn order to limit access, we propose to store all S3 buckets in a single AWS account. Users accessing this account\\ndirectly will have little or no access to the owned S3 buckets, instead through the infrastructure deployment process\\n(terraform) we will share a partition of the S3 buckets to relevant department accounts.\\nE.g. s3:\/\/s3-bucket\/social-care\/\\* -> Social Care Account\\n"}
{"prompt":"## Context and Problem Statement\\nIs there available tool that is compliant to requirements?\\nCan the tool provide functionality for API Gateway?\\nCan the tool provide functionality for API Development Portal?\\n## Decision Drivers\\n* Vendor lock in for runtime\\n* Open source or not\\n* Installation options\\n* Functional ability\\n* Market presence\\n### Functional ability for decision\\nFunctional ability to consider when evaluating the tool. The following list was taken from [wikipedia page for API Management](https:\/\/en.wikipedia.org\/wiki\/API_management).\\n* Gateway: a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance.\\n* Publishing tools: a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle.\\n* Developer portal\/API store: community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community.\\n* Reporting and analytics: functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs.\\n* Monetization: functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments.\\n","completion":"## Decision\n* Vendor lock in for runtime\\n* Open source or not\\n* Installation options\\n* Functional ability\\n* Market presence\\n### Functional ability for decision\\nFunctional ability to consider when evaluating the tool. The following list was taken from [wikipedia page for API Management](https:\/\/en.wikipedia.org\/wiki\/API_management).\\n* Gateway: a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance.\\n* Publishing tools: a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle.\\n* Developer portal\/API store: community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community.\\n* Reporting and analytics: functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs.\\n* Monetization: functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments.\\nBased on the option to run the API Management tool on premise, the following options have been pinpointed for further analysis.\\nIf open source options are not a requirement, it is suggested to evaluate the following tools.\\n* [Mulesoft Anypoint](https:\/\/www.mulesoft.com\/platform\/api-management)\\n* [Software AG API Management](https:\/\/softwareaggov.com\/products\/integration\/webmethods-api-management\/)\\n* [IBM API Connect](https:\/\/www.ibm.com\/cloud\/api-connect)\\n* [Axway Ampify](https:\/\/www.axway.com\/en\/products\/api-management)\\n* [Tibco Mashery](https:\/\/www.tibco.com\/products\/api-management)\\n* [Nginx Plus](https:\/\/www.nginx.com\/)\\nThese are the tools that are most mature, and do not provide lock in for runtime platform.\\nThey need to be evaluated based on pricing and technical ability.\\nIf true open source is required, it is suggested to evaluate the following tools.\\n* [Kong](https:\/\/konghq.com\/)\\n* [Red Hat 3Scale](https:\/\/www.3scale.net\/)\\n* [Tyk](https:\/\/tyk.io\/)\\n* [WSO2](https:\/\/wso2.com\/api-management\/)\\nThese tools provide open source offering, but in most cases the features for Api Developer Portal and Publishing tools are only part of enterprise offering with subscription.\\nEvaluation is needed for validating if the open source offering of the tool contains what is needed for implementation.\\nWe need to evaluate pricing of enterprise offering against the price of creating\/implementing required pieces, like custom Api Developer Portal.\\nFor all considered tools we need to check what underlying software components are required. For example, data storage, queuing and logging capacity.\\nWe need to take into consideration effort and ability to build Developer portal, compared to customizing the tool offering.\\nDownside for\\n* [Google Apigee Edge](https:\/\/apigee.com\/about\/cp\/open-source-api-management)\\n* [AWS](https:\/\/aws.amazon.com\/api-gateway\/api-management\/)\\n* [Microsoft Azure API Management](https:\/\/azure.microsoft.com\/en-us\/services\/api-management\/)\\nis that they are all platform dependent, to the owners proposed platform with more limited on-premise options. These are all top of the line tools according to capabilities, so\\nif vendor lock was not one of the decision drivers, they would be on evaluation list.\\nOther tools that we looked at did in our opinion lack functionality or other ability for further considered, even though many of them could be considered.\\nNote that the list provided is not all existing Api Management tools, so other options might apply.\\n### Positive Consequences\\n* Api Management tools are listed and grouped based on if they have open source option or not.\\n* This is first phase of decision on what tool could help implementation on Api Management Platform, and narrow down options to consider.\\n### Negative Consequences\\n* Follow up is required on tool that will make the shortlist.\\n"}
{"prompt":"## Context\\nWe must guarantee that the data stored, transferred to and from the cloud services will never leave the borders of the European Union.\\n","completion":"## Decision\nWe decide to deploy our services on the [West Europe](https:\/\/azure.microsoft.com\/en-us\/regions\/) region, located in the Netherlands.\\n"}
{"prompt":"## Context\\nTests against the real system instead of test doubles was proven to be useful detecting bugs and exercising better our\\nsystem.\\n","completion":"## Decision\nAvoid introducing test doubles as much as possible. The only allowed exception is to stub\/simulate external systems we\\ncan't control. Library code should never be stubbed.\\n"}
{"prompt":"## Context and Problem Statement\\nDeprecated annotation usages without any comment or suggestion makes no sense when using existing classes.\\n","completion":"## Decision\nWhen we decide to use deprecated annotation in classes, we need to comment reason and alternative to that implementation.\\n"}
{"prompt":"## Context\\nThere is a desire from users to be able to build docker images as an artifact from their jobs, this creates a security problem because of technical limitations within docker.  This ADR is the result of discovery work to discern whether there are alternative solutions to building docker images securely.  The discovery focussed on evaluating the software product called [Buildah].\\n## Technical Problem\\nAs the workers run within a docker container, they may need to build another docker image as an artifact.  To do so requires that the host servers docker socket be exposed to the worker container.  The docker daemon runs as root and has full control of the host server, meaning malicious code running within the workers docker container could trivially gain access to the host system by mounting (via -v) the host systems root.\\nAs Docker does not support RBAC to control and restrict access to its socket there is no way provided by docker to prevent this privilege escalation method occuring.\\nBuildah solves this problem by being able to build a docker container without the need to use docker and the vulnerable socket.\\n","completion":"## Decision\nBuildah addresses the problems of docker container privilege escalation, is feature rich to support typical use cases (as it is feature compatible with docker), and provides us a means in which to provide docker image artifacts.\\n"}
{"prompt":"Context\\n-------\\nDistrictBuilder is intended to be a primarily front-end application, with a back-end API to persist data and support operations that cannot be feasibly completed in the browser in a performant manner.\\nThe main performance critical operation we know the back-end API will need to support is the merging of geographic areas to form new districts. Through some [research and experimentation](https:\/\/docs.google.com\/document\/d\/1znHcDr1wXQ1vr6cNiI5Y9gl5uwIveVPj659Wpug70Is\/edit?usp=sharing), we have decided to rely upon [TopoJSON](https:\/\/github.com\/topojson\/topojson) to support this operation.\\nTopoJSON is written in JavaScript (there are TopoJSON libraries in other languages, but none support the critical merge operations that make TopoJSON useful for our situation), and in order to be able to use it in a performant manner we must create a Node.js service that can keep the TopoJSON data structure in-memory. Due to familiarity and enthusiasm from the team, we would prefer to use TypeScript for this service over plain JavaScript.\\nAs an additional point in favor of Node.js \/ TypeScript, using TypeScript will enable us to share code with the frontend, which will be particularly helpful as we expect to need to mirror some client-side operations on the backend to support contest submissions validation.\\nIn addition to being able to perform TopoJSON operations, the back-end API will need to be able to provide user authentication (and potentially user registration and management as well) and communicate with a PostgreSQL database to persist data.\\nI evaluated [11 different Node.js web frameworks](https:\/\/docs.google.com\/spreadsheets\/d\/1zLixcL1Xt53iYPkvzDHidg48uyKrevO7i7zj6EiBMBA\/edit?usp=sharing), which generally fell into 3 types:\\n- Minimalist, generally with only routing and minimal HTTP support. Other features are potentially available as middleware.\\n- Express\\n- koa\\n- hapi\\n- Fastify\\n- Next\\n- Fully featured, with support for user authorization & registration, an ORM, database migrations, etc. Typically built *using* one of the more minimalist frameworks under the hood.\\n- Adonis\\n- Feathers\\n- Nest\\n- Meteor\\n- Somewhere in-between, with some extra features built-in, and others only as third-party plugins.\\n- Sails\\n- Loopback\\nFrom these frameworks, I immediately identified a few that won't work well for our purposes:\\n- MeteorJS is a popular and well-documented framework, but it only supports MongoDB for persisting data\\n- Next is primarily focused on server-side rendering of React applications, and does not provide many of the features we would need, nor does it have a large third-party plugin ecosystem to make up for the lack of first-party support.\\nFor the more minimalist frameworks, there are third-party tools we could leverage to make up for missing features.\\nUser registration:\\n- Auth0 can provide this as an external service, and is a service we are familiar with\\n- [Passport](http:\/\/www.passportjs.org\/docs\/)\\nDatabase querying & migrations management\\n- [TypeORM](https:\/\/typeorm.io\/#\/), TypeScript first ORM with support for generating migrations\\n- [Sequelize](https:\/\/sequelize.org\/) more dynamic Javascript-focused ORM, supports migrations but can't generate them\\n- For a more low-level approach, `node-postgres` or KnexJS combined with `node-db-migrate`\\nThe more \"batteries-included\" frameworks tend to use the above libraries (in some cases even allowing swapping in one with another), rather than including their own implementations. This means that when choosing between one of these frameworks or a more minimalist one like Express, what we are really choosing between for the most part is whether to put the pieces together ourselves or whether to use the combination of libraries brought together by a framework like Nest or Feathers.\\nWe could also instead consider using a minimalist framework in a hybrid approach, where we build most of the back-end API in a language\/framework we are familiar with, such as Python & Django, and encapsulate the TopoJSON-specific operations in a microservice. This would allow us to not need to learn a new way of managing database migrations, user authentication or management, etc., but with a cost of greater architectural complexity and a potential performance loss due to communication overheads. For such an approach I think it would make sense to go with Express, as we have familiarity with using it for other microservices on previous projects.\\nDecision\\n--------\\nWe will proceed by using TypeScript with Nest as the framework for the DistrictBuilder backend API.\\nThe documentation for Nest was among the best of the frameworks I investigated, and the [purposeful similarity to Angular](https:\/\/codesandbox.io\/s\/jjo90y00xw?from-embed) will hopefully provide an easier learning curve given the familiarity within the development team with Angular.\\nBy using Nest we won't have to choose a user authentication strategy (it provides an integration with Passport) or database management library (it uses TypeORM by default). Because it is built on-top of Express we can still use Express-middleware if necessary, but we may not have to as there are a number of plugins (https:\/\/github.com\/juliandavidmr\/awesome-nestjs#components--libraries). Nest also provides a dependency injection framework, which is an added bonus.\\n---\\nFeathers was another major contender, and it provides many of the basics we need. Ultimately the choice between them mainly came down to Nest appearing to have better TypeScript support, and the familiarity of the Angular-esque application structure used by Nest.\\nAdonis is seemingly less popular and well-known than either Feathers or Nest, though it did seem to offer a similar feature-set to Nest.\\nThe hybrid\/microservice approach was compelling, but the added architectural complexity is significant, and I don't think our back-end needs really require a framework as complex and all-encompassing as Django, Additionally, the ability to write the backend in a statically-typed language like TypeScript is attractive, which we would be mainly giving up with a microservice-based approach.\\nConsequences\\n------------\\nBy making the decision to build the entire back-end using Node.js, we're taking an approach that hasn't been commonly used at Azavea, where we have typically used a microservice approach when making use of Node.js libraries (such as when using Windshaft for tiling).\\nThe consequences of this decision are primarily in increased time spent learning the new framework. Additionally, by using Node.js instead of a platform we're more familiar with like Python or Scala we'll inevitably encounter situations that require research into how to do things that would be solved by familiar and well-known tools on a platform we're more familiar with.\\nBy choosing to use Nest, we are hopefully reducing the need for further research and decision-making, by providing a default choice for the libraries that we would inevitably need to integrate if we used Express and mix\/matched other libraries to match the feature set provided by Nest.\\n","completion":"## Decision\n--------\\nWe will proceed by using TypeScript with Nest as the framework for the DistrictBuilder backend API.\\nThe documentation for Nest was among the best of the frameworks I investigated, and the [purposeful similarity to Angular](https:\/\/codesandbox.io\/s\/jjo90y00xw?from-embed) will hopefully provide an easier learning curve given the familiarity within the development team with Angular.\\nBy using Nest we won't have to choose a user authentication strategy (it provides an integration with Passport) or database management library (it uses TypeORM by default). Because it is built on-top of Express we can still use Express-middleware if necessary, but we may not have to as there are a number of plugins (https:\/\/github.com\/juliandavidmr\/awesome-nestjs#components--libraries). Nest also provides a dependency injection framework, which is an added bonus.\\n---\\nFeathers was another major contender, and it provides many of the basics we need. Ultimately the choice between them mainly came down to Nest appearing to have better TypeScript support, and the familiarity of the Angular-esque application structure used by Nest.\\nAdonis is seemingly less popular and well-known than either Feathers or Nest, though it did seem to offer a similar feature-set to Nest.\\nThe hybrid\/microservice approach was compelling, but the added architectural complexity is significant, and I don't think our back-end needs really require a framework as complex and all-encompassing as Django, Additionally, the ability to write the backend in a statically-typed language like TypeScript is attractive, which we would be mainly giving up with a microservice-based approach.\\nConsequences\\n------------\\nBy making the decision to build the entire back-end using Node.js, we're taking an approach that hasn't been commonly used at Azavea, where we have typically used a microservice approach when making use of Node.js libraries (such as when using Windshaft for tiling).\\nThe consequences of this decision are primarily in increased time spent learning the new framework. Additionally, by using Node.js instead of a platform we're more familiar with like Python or Scala we'll inevitably encounter situations that require research into how to do things that would be solved by familiar and well-known tools on a platform we're more familiar with.\\nBy choosing to use Nest, we are hopefully reducing the need for further research and decision-making, by providing a default choice for the libraries that we would inevitably need to integrate if we used Express and mix\/matched other libraries to match the feature set provided by Nest.\\n"}
{"prompt":"## Context\\nDevelopers from BioConductors pointed out that the current format of data returned as hashes e.g.\\n```\\nresults:[\\n{id:\"123\", \"name\":\"xyz\", genome:\"homo_sapiens\"},\\n{id:\"456\", \"name\":\"abc\", genome:\"homo_sapiens\"},\\n{id:\"890\", \"name\":\"def\", genome:\"homo_sapiens\"}\\n]\\n```\\nwould be better returned as:\\n```\\nresults:[\\n[\"123\", \"xyz\", \"homo_sapiens\"],\\n[\"456\", \"abc\", \"homo_sapiens\"],\\n[\"890\", \"def\", \"homo_sapiens\"]\\n]\\n```\\nThis would be better for several reasons:\\n* smaller payload\\n* client would probably need to do this (esp for R) so why not do it already\\n* might work better with tabular views (CSV, web interfaces)\\nHowever, this is not a universally desirable result - the gene documents are inherently nested so we would want to support both approaches.\\nThis might be applied at a variety of levels, but is best done as far away from the interactions with ES etc as possible.\\n","completion":"## Decision\nWe will implement this as an option for `FetchService` and `QueryService` endpoints with a defined toggle parameter array=true\/false. The services will then reformat the data accordingly, but the underlying searches will return data in the same format.\\n"}
{"prompt":"### Context\\nWe want to use Lrud as a `tap-static` module, which requires converting to `amd` format. To do so we use `esm-2-amd`, which means we need to distribute lrud as ESM. Currently it's not possible as it's only built to CJS.\\n### Decision\\nThe distribution folder structure will change with two subfolders, `cjs` and `esm`. The type definitions will also be distributed in another subfolder, `types`. Rollup will still be used to create the CJS and ESM format.\\n### Status\\nApproved\\n### Consequences\\nThe final distribution size will increase.\\nThis change won't affect how current users consume lrud, as the package `main` is updated to point to the CJS min file.\\n","completion":"## Decision\nThe distribution folder structure will change with two subfolders, `cjs` and `esm`. The type definitions will also be distributed in another subfolder, `types`. Rollup will still be used to create the CJS and ESM format.\\n### Status\\nApproved\\n### Consequences\\nThe final distribution size will increase.\\nThis change won't affect how current users consume lrud, as the package `main` is updated to point to the CJS min file.\\n"}
{"prompt":"## Context\\nWhen I decided to use AMI's instead of docker containers, I knew I had to\\nuse some tool for configuration management.\\nThis tool would help me to execute configuration tasks.  But as my intention\\nis to create an inmutable system, I don't want to manage changes, just set\\nthe resources.\\nFor doing this I could implement Puppet, Chef, or Ansible.  I have experience\\nwith all of them, but recently I have been working more with Ansible.  And for\\na challenge like this, I prefer something modern, without servers.\\n","completion":"## Decision\nI have used Ansible to configure all the services for the three kind of nodes\\nin the challenge: etcd, monitor, and benchmark.\\n"}
{"prompt":"## Context\\nAdd feature to stream logs from trc and nginx container for better debugging\\n","completion":"## Decision\nUpdate the nginx and trc flists to use zinit and redirect logs to stdout to be streamed from redis\\n"}
{"prompt":"## Context\\n`stentor` needs to pick a config file format.\\nThe options under consideration are yaml and toml.\\nBoth options are human readable and writable,\\nallow for easy parsing of structured data,\\nand are supported by well maintained libraries.\\nyaml has the benefit of being more straightforward to write,\\nespecially for nested structures.\\nHowever, toml is intended for config files,\\nand provides stricter parsing out of the box.\\n","completion":"## Decision\n`stentor` will use toml for its config file.\\n"}
{"prompt":"## Context\\nThe pixel array within an SQW object can be too large to fit into memory.\\nTo avoid running out of memory, the object holding the pixel array can be\\nfile-backed.\\nThis means that only a \"page\" of the pixel array is loaded into memory at any\\none time.\\nTherefore two possible ways to index into the pixel array exist:\\n1. **Absolute index**:\\n_The position of the pixel in the full, file-backed, pixel array_.\\n2. **Relative index**:\\n_The position of the pixel in the currently loaded page of pixel data._\\n","completion":"## Decision\nThere will be two separate APIs for accessing data,\\nthese APIs distinguish between the two types of indexing.\\n1. **Perform absolute indexing  using `get_` methods:**\\nObtaining a subset of pixels or pixel data by absolute index will be\\npossible using a `get_` method.\\nFor example, the following will retrieve pixels 100-200 by absolute index:\\n```matlab\\npixels.get_pixels(100:200)\\n```\\nSimilarly, to retrieve a range of data from particular pixel array fields:\\n```matlab\\npixels.get_data({'signal', 'variance'}, 100:200)\\n```\\nAt time of writing, there are no plans to implement similar `set_`\\nmethods with absolute indexing.\\nIf there becomes a requirement for setters with absolute indexing,\\nthese should follow a similar syntax.\\nFor example, the following should set signal and variance from absolute\\nindex 100 to 200, to zero:\\n```matlab\\nset_data({'signal', 'variance'}, 100:200, zeros(2, 101));\\n```\\n2. **Perform relative indexing using attribute:**\\nObtaining pixel data using an attribute will return just the data for the\\ncurrently cached page.\\nHence, indexing into these attributes will be relative.\\nFor example, the following will retrieve the signal values of pixels 10-20\\nin the currently cached page:\\n```matlab\\npixels.signal(10:20)\\n```\\nSetting of pixel data will be possible using attributes and relative\\nindexing.\\nFor example the following will set pixels 1 to 20 on the current page to\\nzero:\\n```matlab\\npixels.signal(1:20) = 0\\n```\\n"}
{"prompt":"## Context\\nThe current implementation of BaseApp does not allow developers to write custom error handlers during panic recovery\\n[runTx()](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/bad4ca75f58b182f600396ca350ad844c18fc80b\/baseapp\/baseapp.go#L539)\\nmethod. We think that this method can be more flexible and can give SDK users more options for customizations without\\nthe need to rewrite whole BaseApp. Also there's one special case for `sdk.ErrorOutOfGas` error handling, that case\\nmight be handled in a \"standard\" way (middleware) alongside the others.\\nWe propose middleware-solution, which could help developers implement the following cases:\\n* add external logging (let's say sending reports to external services like [Sentry](https:\/\/sentry.io));\\n* call panic for specific error cases;\\nIt will also make `OutOfGas` case and `default` case one of the middlewares.\\n`Default` case wraps recovery object to an error and logs it ([example middleware implementation](#Recovery-middleware)).\\nOur project has a sidecar service running alongside the blockchain node (smart contracts virtual machine). It is\\nessential that node <-> sidecar connectivity stays stable for TXs processing. So when the communication breaks we need\\nto crash the node and reboot it once the problem is solved. That behaviour makes node's state machine execution\\ndeterministic. As all keeper panics are caught by runTx's `defer()` handler, we have to adjust the BaseApp code\\nin order to customize it.\\n","completion":"## Decision\n### Design\\n#### Overview\\nInstead of hardcoding custom error handling into BaseApp we suggest using set of middlewares which can be customized\\nexternally and will allow developers use as many custom error handlers as they want. Implementation with tests\\ncan be found [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/6053).\\n#### Implementation details\\n##### Recovery handler\\nNew `RecoveryHandler` type added. `recoveryObj` input argument is an object returned by the standard Go function\\n`recover()` from the `builtin` package.\\n```go\\ntype RecoveryHandler func(recoveryObj interface{}) error\\n```\\nHandler should type assert (or other methods) an object to define if object should be handled.\\n`nil` should be returned if input object can't be handled by that `RecoveryHandler` (not a handler's target type).\\nNot `nil` error should be returned if input object was handled and middleware chain execution should be stopped.\\nAn example:\\n```go\\nfunc exampleErrHandler(recoveryObj interface{}) error {\\nerr, ok := recoveryObj.(error)\\nif !ok { return nil }\\nif someSpecificError.Is(err) {\\npanic(customPanicMsg)\\n} else {\\nreturn nil\\n}\\n}\\n```\\nThis example breaks the application execution, but it also might enrich the error's context like the `OutOfGas` handler.\\n##### Recovery middleware\\nWe also add a middleware type (decorator). That function type wraps `RecoveryHandler` and returns the next middleware in\\nexecution chain and handler's `error`. Type is used to separate actual `recovery()` object handling from middleware\\nchain processing.\\n```go\\ntype recoveryMiddleware func(recoveryObj interface{}) (recoveryMiddleware, error)\\nfunc newRecoveryMiddleware(handler RecoveryHandler, next recoveryMiddleware) recoveryMiddleware {\\nreturn func(recoveryObj interface{}) (recoveryMiddleware, error) {\\nif err := handler(recoveryObj); err != nil {\\nreturn nil, err\\n}\\nreturn next, nil\\n}\\n}\\n```\\nFunction receives a `recoveryObj` object and returns:\\n* (next `recoveryMiddleware`, `nil`) if object wasn't handled (not a target type) by `RecoveryHandler`;\\n* (`nil`, not nil `error`) if input object was handled and other middlewares in the chain should not be executed;\\n* (`nil`, `nil`) in case of invalid behavior. Panic recovery might not have been properly handled;\\nthis can be avoided by always using a `default` as a rightmost middleware in the chain (always returns an `error`');\\n`OutOfGas` middleware example:\\n```go\\nfunc newOutOfGasRecoveryMiddleware(gasWanted uint64, ctx sdk.Context, next recoveryMiddleware) recoveryMiddleware {\\nhandler := func(recoveryObj interface{}) error {\\nerr, ok := recoveryObj.(sdk.ErrorOutOfGas)\\nif !ok { return nil }\\nreturn sdkerrors.Wrap(\\nsdkerrors.ErrOutOfGas, fmt.Sprintf(\\n\"out of gas in location: %v; gasWanted: %d, gasUsed: %d\", err.Descriptor, gasWanted, ctx.GasMeter().GasConsumed(),\\n),\\n)\\n}\\nreturn newRecoveryMiddleware(handler, next)\\n}\\n```\\n`Default` middleware example:\\n```go\\nfunc newDefaultRecoveryMiddleware() recoveryMiddleware {\\nhandler := func(recoveryObj interface{}) error {\\nreturn sdkerrors.Wrap(\\nsdkerrors.ErrPanic, fmt.Sprintf(\"recovered: %v\\nstack:\\n%v\", recoveryObj, string(debug.Stack())),\\n)\\n}\\nreturn newRecoveryMiddleware(handler, nil)\\n}\\n```\\n##### Recovery processing\\nBasic chain of middlewares processing would look like:\\n```go\\nfunc processRecovery(recoveryObj interface{}, middleware recoveryMiddleware) error {\\nif middleware == nil { return nil }\\nnext, err := middleware(recoveryObj)\\nif err != nil { return err }\\nif next == nil { return nil }\\nreturn processRecovery(recoveryObj, next)\\n}\\n```\\nThat way we can create a middleware chain which is executed from left to right, the rightmost middleware is a\\n`default` handler which must return an `error`.\\n##### BaseApp changes\\nThe `default` middleware chain must exist in a `BaseApp` object. `Baseapp` modifications:\\n```go\\ntype BaseApp struct {\\n\/\/ ...\\nrunTxRecoveryMiddleware recoveryMiddleware\\n}\\nfunc NewBaseApp(...) {\\n\/\/ ...\\napp.runTxRecoveryMiddleware = newDefaultRecoveryMiddleware()\\n}\\nfunc (app *BaseApp) runTx(...) {\\n\/\/ ...\\ndefer func() {\\nif r := recover(); r != nil {\\nrecoveryMW := newOutOfGasRecoveryMiddleware(gasWanted, ctx, app.runTxRecoveryMiddleware)\\nerr, result = processRecovery(r, recoveryMW), nil\\n}\\ngInfo = sdk.GasInfo{GasWanted: gasWanted, GasUsed: ctx.GasMeter().GasConsumed()}\\n}()\\n\/\/ ...\\n}\\n```\\nDevelopers can add their custom `RecoveryHandler`s by providing `AddRunTxRecoveryHandler` as a BaseApp option parameter to the `NewBaseapp` constructor:\\n```go\\nfunc (app *BaseApp) AddRunTxRecoveryHandler(handlers ...RecoveryHandler) {\\nfor _, h := range handlers {\\napp.runTxRecoveryMiddleware = newRecoveryMiddleware(h, app.runTxRecoveryMiddleware)\\n}\\n}\\n```\\nThis method would prepend handlers to an existing chain.\\n"}
{"prompt":"## Context\\nCurrently, we have only implemented one flow when you want to access a service:\\n- You bind the service to the application or create a service key.\\n- This creates a VCAP env variable entry including `clientID` and `clientSecret`\\n- With this information you can ask the XSUAA to issue a token to access the service\\n- Different grant types are available for the token (clientGrant, userGrant....)\\nThis changes slightly when you want to use certificates as a secret to request a token at the XSUAA\\n- Now per default XSUAA accepts x509 as authentication type.\\nYou can configure this in the security descriptor JSON:\\n```JSON\\n{\\n\"xsappname\": \"someName\",\\n\"oauth2-configuration\": {\\n\"credential-types\": [\"binding-secret\", \"x509\"]\\n}\\n}\\n```\\n- You bind the service or create a service key as usual `cf create-service-key myservice myservicekey -c parameters.json` where the parameters.json is:\\n```json\\n{\\n\"credential-type\": \"x509\",\\n\"x509\": {\\n\"key-length\": 2048,\\n\"validity\": 7,\\n\"validity-type\": \"DAYS\"\\n}\\n}\\n```\\n- If you want to support multiple credential-types you have to create multiple service keys.\\n- This binding leads to a `certificate` and `key` property in the VCAP service variables.\\n- You add this certificate to the HTTP client (mTLS) making the call to the XSUAA instead of the `clientSecret` in the payload.\\n- The XSUAA returns a token to access the desired service\\n### Advantages of x509\\nAt first glance it seems like one string in the service keys:\\n```json\\n{\\n\"apiurl\": \"https:\/\/api.authentication.sap.hana.ondemand.com\",\\n\"clientid\": \"someID\",\\n\"clientsecret\": \"someSecret\",\\n\"credential-type\": \"instance-secret\",\\n...\\n}\\n```\\nis replaced by another string (certificate and key):\\n```json\\n{\\n\"apiurl\": \"https:\/\/api.authentication.sap.hana.ondemand.com\",\\n\"certificate\": \"-----BEGIN CERTIFICATE-----...-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----..-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----...-----END CERTIFICATE-----\\n\",\\n\"clientid\": \"someID\",\\n\"credential-type\": \"x509\",\\n\"key\": \"-----BEGIN RSA PRIVATE KEY-----...-----END RSA PRIVATE KEY-----\\n\",\\n...\\n}\\n```\\nHowever, the certificate based approach has some advantages:\\n- The secret is not part of HTTP traffic but the TLS layer.\\nSo if customers switch on the HTTP trace you will not see secrets in the logs\\n- The lifetime of a certificate can be set so that even in the token retrieval you can not use an old stolen secret\\n### Disadvantages of x509\\nWe are still investigating the details, but we are afraid of a higher support load.\\nAssume the following flow:\\n- Customer creates new application using the SDK\\n- Binds XSUAA and destination service to the application\\n- Per default the credentials-type will be `x509` and the XSUAA manages the certificate\\n- Everything works fine initially, but after 7 days the certificate expires.\\n- The support request will go to the SDK, because we throw the error.\\nYou can of course configure the duration of the certificate validity but it seems to be limited to a maximum of one year.\\n### Rollout and Compatibility\\nFor existing bindings and service keys nothing will change.\\nAs of Q3, XSUAA will accept x509 and so will the other services like destination and connectivity.\\nNote that for service keys with `credential-type` x509 there is no `clientsecret` property.\\nAt the current state the SDK will fail for this case.\\nIn Q3 and Q4 the SDK relevant services like XSUAA, destination and connectivity should support x509 and are rolled out internally.\\n### Self Managed Certificates\\nWe have discussed the XSUAA managed flow up to know.\\nIn this case the certificates are created by the XSUAA.\\nBut it is also possible to bring your own certificate when you create the service key:\\n```json\\n{\\n\"credential-type\": \"x509\",\\n\"x509\": {\\n\"certificate\": \"-----BEGIN CERTIFICATE-----...-----END CERTIFICATE-----\",\\n\"ensure-uniqueness\": false,\\n\"certificate-pinning\": true\\n}\\n}\\n```\\nIn this case the `key` property is not part of the VCAP service variables, and you need to bring it from the outside.\\n","completion":"## Decision\nFor the self-managed certificates we have to find a secure way for the customer to pass the private part of the certificate to the application.\\nIn the first version this is not considered.\\nImplementation is easy it is more a security question.\\n### Option A - Extend our current code\\nWe could extend our current code to support the x509 authentication.\\nThis would require the current changes:\\n- Ensure the right service credentials are found in the VCAP (should already be the case).\\n- Investigate the `credential-type` if it is `instance-secret` or `x509`.\\n- Depending on the value use the http client with certificate or clientSecret to get a token.\\nNote that the xsuaa endpoints are different:\\n- X.509: https:\/\/<subdomain>.authentication.cert.<landscape domain>\/oauth\/token\\n- clientSecret: https:\/\/<subdomain>.authentication.<landscape domain>\/oauth\/token\\n### Option B\\nWe change the token retrieval to the [xssec](https:\/\/www.npmjs.com\/package\/@sap\/xssec) library.\\nWe would need to do the following:\\n- Replace our existing call with the lib\\n- Ensure the right service credentials are found in the VCAP (should already be the case)\\n- The lib will take care of evaluation which type the service key is.\\n### Comparison A versus B\\nOption B is the winner.\\nWith the library we are future-proof for other things to come.\\nAlso, from a security point of view we should use the the `xssec` and not a homemade implementation.\\n"}
{"prompt":"## Context\\nES2016 introduced native support for the concept of modules. These are scoped files that expose some public functions. Modules are a way of organizing and sharing code.\\n","completion":"## Decision\nWe will use ES2016 modules to organize and share code. More information can be found here: http:\/\/exploringjs.com\/es6\/ch_modules.html#sec_modules-in-javascript\\n"}
{"prompt":"## Context\\nWe use a completely stateless implementation for the RDS broker, as described in [ADR006](..\/ADR006-rds-broker).\\nSo all the asynchronous operations on RDS instances were relying on executing a unique operation on AWS API, and querying the RDS instance status reported the AWS API.\\nBut to implement the feature of restore from snapshot, we must execute several operations sequentially.\\nThe broker must:\\n1. Start the restore from snapshot, which can take minutes.\\n2. Once finish, update several parameters of the instance (security groups, parameters, etc).\\n3. Once that is finish, reset the master password of the RDS instance.\\n4. Finally reset the passwords of the users previously bind in the original DB.\\nAs the create operation is a asynchronous operation, the Cloud Controller API will periodically request the `LastOperation` endpoint to query the state of the restored instance.\\nThe rds-broker must respond accordingly.\\nThe Cloud Controller API includes logic to ensure the resiliance of a service creation, maintaining the workers that will poll the `LastOperation` until the service is created or there is timeout.\\nTo implement this kind logic, some kind of state must be kept to track the changes on the instance.  Options are:\\n* run a background house-keeping routine. This house-keeping should be resilient to rds-broker restarts and able to work with multiple rds-broker instances..\\n* Use SNS and SQS, by subscribing to the [AWS events from RDS](http:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/UserGuide\/USER_Events.html). This requires a lot of additional work and integration effort.\\n* Store the state in some database or k\/v store.\\n","completion":"## Decision\nWe decided:\\n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/Using_Tags.html) of the instance.\\nWe will add a list of tags for each pending operations to execute.\\n* We make use of `LastOperation` to check the pending operations and perform them, to finally delete the corresponding tag to mark it as done.\\nWe assume that:\\n* all the required operations are either asynchronous in the AWS API (eg. update instance) or quick to execute (e.g. reset bind user passwords)\\n* that update the tags is atomic and synchronous.\\n"}
{"prompt":"## Context\\nAs we are building our new platform on Kubernetes we have already found the need to build quite a few clusters. These have been for a range of purposes including users, testing new ideas (\"sandbox\"), testing new functionality (\"test\"), deploying apps to them (\"non-production\").\\nAs we are still learning we are finding that:\\n1. we need to continue building new clusters for different purposes and\\n2. we often need to test the cluster creation process\\n3. we want to differentiate between clusters that have users on them and those that are for internal testing purposes\\n4. we do not want to differentiate cluster by function (e.g. \"perf-test\", \"sandbox\") or status (\"non-production\").\\nTo make this easier we propose having a naming scheme that makes it easy to understand whether users are on that cluster but makes no other assumptions about what it is used for.\\n","completion":"## Decision\nWe will name all clusters with the following naming scheme:\\n- `live-{n}` for any cluster that have users on them, for instance `live-1`.\\n- `test-{n}` for any cluster that do not have users on them and are used by the cloud platform team only, for instance `test-2`.\\nWe will number the clusters sequentially.\\n"}
{"prompt":"## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov\/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n","completion":"## Decision\nThe GOV.UK PaaS team decided that they were happy to take ownership of the broker, because it requires knowledge of the platform to maintain, and they maintain all the other brokers on the platform.\\n"}
{"prompt":"Context\\n-------\\nBefore starting to commit in the main project, I was looking at various possibilities of writing a brain\/cognitive simulation with GUI.\\nDecision\\n--------\\nPick TornadoFX and Kotlin as main simulation framework.\\nRationalization\\n---------------\\nWith lots of possibilities around, the 2nd place winning one being Simbrain,\\nTornadoFX and Kotlin seemed to be the most modern and, hence, robust and extendable option.\\nFurthermore, reduction of the amount and increase of the readability of code, which the framework provides, are especially important for an open-source project.\\n","completion":"## Decision\n--------\\nPick TornadoFX and Kotlin as main simulation framework.\\nRationalization\\n---------------\\nWith lots of possibilities around, the 2nd place winning one being Simbrain,\\nTornadoFX and Kotlin seemed to be the most modern and, hence, robust and extendable option.\\nFurthermore, reduction of the amount and increase of the readability of code, which the framework provides, are especially important for an open-source project.\\n"}
{"prompt":"## Context\\nCurrent version of clang-format is aging an becoming unavailable.\\n","completion":"## Decision\nWe will move to clang-format 5 and not the bleeding edge version. Visual Studio 2017 will ship with clang-format 5 so it makes sense to standardize\\non this version across the project.\\n"}
{"prompt":"## Context\\nOur current certificate structure is based on ERC-721 non-fungible tokens. This presents an issue when a part of a certificate's volume has to be transferred to another owner.\\nIn cases like these, we currently \"split\" the certificate into 2 smaller certificates, and then transfer one of the certificates to the new owner, and leave the original certificate to the original owner - deprecating the old certificate.\\nThis approach is not ideal, so we started looking into better ways of changing owners for smaller parts of the certificates.\\n","completion":"## Decision\nWe decided to use the [ERC-1888](https:\/\/github.com\/ethereum\/EIPs\/issues\/1888) Certificate structure so that we can comply and work on standardizing Certificates.\\n"}
{"prompt":"## Context\\nThe Observe team is a part of the new platform team, which is building out kubernetes capability in GDS.\\nThere is a long-term goal that teams in GDS should avoid running bespoke infrastructure, specific to that team, so that any infrastructure we run is run in a common way and supportible by many people.\\nWe also have a desire to migrate off ECS.  ECS is painful for running alertmanager because:\\n- ECS doesn't support dropping configuration files in place\\n- ECS doesn't support exposing multiple ports via load balancer for service discovery\\nKubernetes does not have either of these limitations.\\nCurrently, we have a plan to migrate everything to EC2, in order to get away from ECS.  We have quite a bit of outstanding pain from the old way of doing things:\\n- we have two different deploy processes; one using the Makefile and one using the deploy_enclave.sh\\n- we have two different code styles, related to the above\\n- we have two different types of infrastructure\\nWe haven't fully planned out how we would migrate alertmanager to EC2, but we suspect it would involve at least the following tasks:\\n- create a way of provisioning an EC2 instance with alertmanager installed (probably a stock ubuntu AMI with cloud.conf to install software)\\n- create a way of deploying that instance with configuration added (probably a terraform module similar to what we have for prometheus)\\n- actually deploy some alertmanagers to EC2 in parallel with ECS\\n- migrate prometheus to start using both EC2 and ECS alertmanagers in parallel\\n- once we're confident, switch off the ECS alertmanagers\\n- tidy up the old ECS alertmanager code\\nThis feels like a lot of work, especially if our longer-term goal is that we shouldn't run bespoke infrastructure and should instead run in some common way such as the new platform.\\nNevertheless, we could leave alertmanager in ECS but still ease some of the pain by refactoring the terraform code to be the new module-style instead of the old project-and-Makefile style, even if we leave alertmanager itself in ECS.\\n(Prometheus is different: we want to run prometheus the same way that non-PaaS teams such as Verify or Pay run it, so that we can offer guidance to them. The principle is the same: we want to run things the same way other GDS teams run them.)\\n","completion":"## Decision\n1. We will pause any work migrating alertmanager to EC2\\n2. We will run an alertmanager in the new platform, leaving the remaining alertmanagers in ECS\\n3. We will try to migrate as much of nginx out of ECS as possible; in particular, we want paas-proxy to move to the same network (possibly same EC2 instance) as prometheus.\\n4. We will refactor our terraform for ECS to be module-based rather than the old project-and-Makefile style, so that we reduce the different types of code and deployment style.\\n5. We will keep prometheus running in EC2 and not migrate it to the new platform (although new platform environments will each have a prometheus available to them)\\n"}
{"prompt":"## Context\\nBased on the instructions, this is a \"Programming Exercise\" and the solution\\nshould be \"production level\".\\nIt is also in the format of a well specified programming puzzle: the\\n\"Specifications\" section is very precise.\\nIf this were real-world software, I would not ignore invalid inputs and the\\nlike. Even if it's not beautiful, some basic input validation would be expected\\n-- sooner or later someone's going to type in the wrong thing, and a messy stack\\ntrace is not a nice user experience. (On the other hand, if it was a programming\\ncompetition, I would not think twice about believing the specification.)\\nI would also review things like performance requirements (\"You described\\ninputting one game description at a time, is that right? Or will you need to\\nprocess batches quickly?\").\\nFurther, I would want to demo the application before delivering it to check that\\nI haven't made any silly assumptions.\\nHowever, this is neither a programming puzzle nor a client deliverable, it's a\\njob application. My guess is that Darryl isn't interested in rounds of iteration\\nto make sure I've built him just the right thing -- I'm guessing it's more about\\nseeing my process, the code I write, and the sorts of things I'm thinking about.\\nSo, in the interest of time and understanding the essence of this task:\\n","completion":"## Decision\nI will treat this as a programming puzzle and not a client deliverable. I will\\ntake the instructions at face value and forsake the client feedback sessions.\\n"}
{"prompt":"Context\\n-------\\nKubernetes has modified their [definition of annotations](https:\/\/kubernetes.io\/docs\/concepts\/overview\/working-with-objects\/annotations\/) to support separate key prefixes and key names.\\nThis update makes annotation keys behave the same as [label](https:\/\/kubernetes.io\/docs\/concepts\/overview\/working-with-objects\/labels\/) keys.\\nKey prefixes can be up to 253 characters long and must be valid DNS names.\\nKey names can be up to 63 characters long and can contain alphanumeric characters plus some limited punctuation.\\nSee [these docs](http:\/\/v3-apidocs.cloudfoundry.org\/version\/3.72.0\/index.html#labels-and-selectors) for more information on label key requirements.\\nWhen we implemented annotations as part of the [metadata epic](https:\/\/www.pivotaltracker.com\/epic\/show\/4124692) these restrictions did not exist.\\nSince annotations are not queryable on the API, we allowed users to create annotations with keys up to 1000 characters long and we did not limit the allowed character set.\\nAPI clients [such as service brokers](https:\/\/github.com\/openservicebrokerapi\/servicebroker\/issues\/654) would like a consistent interface for annotations in both CF and Kubernetes,\\nso we are planning on adding the concept of label key prefixes\/names to annotations in [#166447964](https:\/\/www.pivotaltracker.com\/story\/show\/166447964).\\nThis was first brought to us in the following Github issue: [#1335](https:\/\/github.com\/cloudfoundry\/cloud_controller_ng\/issues\/1335)\\nThis leaves us with two problems:\\n1. How do we model annotation keys going forward?\\n2. How do we handle existing annotations that do not conform to these restrictions?\\nDecision\\n--------\\n1. Make the table structure for annotations more closely match the schema for labels\\n* Add a `key_prefix` column to annotations\\n* Leave the `key` column name the same for rolling-deployment compatibility, but treat it as if it was `key_name`\\n* Leave the `key` column size the same (1000 characters) for backward compatibility. We will enforce the 63 character limit on the Ruby side in validations.\\n* Do this in separate migrations for each annotation table to avoid deadlocking during the upgrade.\\n* In the Ruby code alias `key` to `key_name` in the methods we use to talk to it.\\n* Create a [time bomb test](https:\/\/github.com\/cloudfoundry\/cloud_controller_ng\/blob\/b6ba5196722728a221034aadad076646f43f5de3\/spec\/support\/deprecation_helpers.rb) to tell us to rename the column in six months\\n1. New annotations will be stored with the key split between `key_prefix` and `key_name` (`key` column)\\n1. Updated annotations will be stored with the key split between `key_prefix` and `key_name` (`key` column)\\n1. Updating annotations that do not meet the new prefix validations will fail with a `422` error\\n1. Existing annotations will continue to be readable until they are updated. Since they are not queryable this should not cause too much overhead.\\nWe **will not** attempt to migrate existing data into this new structure for the following reasons:\\n1. Migrations fail when Cloud Foundry is being upgraded and a failed migration is difficult for operators to recover from. We do not want to risk platform downtime and support incidents.\\n1. The operator upgrading a platform is likely not the one who created (or \"owns\") the resource with the problematic annotations. Lazily migrating annotations when they are updated ensures that the one who owns it gets to make the decision.\\nConsequences\\n------------\\n### Positive Consequences\\n1. We will be aligned with Kubernetes once more. \ud83d\ude42\\n1. We will have a more consistent schema across labels and annotations.\\n1. By separating `key_prefix` from `key_name` we will not preclude ourselves from adding new features that want to use or query on the prefix independently.\\n### Negative Consequences\\n1. Users who were relying on large annotation keys will be upset we took them away from them. \ud83d\ude14\\n1. We will be performing migrations across many different tables, there is still the potential for upgrade issues.\\n1. Lazily updating annotations means some will have the old structure for a long time.\\n","completion":"## Decision\n--------\\n1. Make the table structure for annotations more closely match the schema for labels\\n* Add a `key_prefix` column to annotations\\n* Leave the `key` column name the same for rolling-deployment compatibility, but treat it as if it was `key_name`\\n* Leave the `key` column size the same (1000 characters) for backward compatibility. We will enforce the 63 character limit on the Ruby side in validations.\\n* Do this in separate migrations for each annotation table to avoid deadlocking during the upgrade.\\n* In the Ruby code alias `key` to `key_name` in the methods we use to talk to it.\\n* Create a [time bomb test](https:\/\/github.com\/cloudfoundry\/cloud_controller_ng\/blob\/b6ba5196722728a221034aadad076646f43f5de3\/spec\/support\/deprecation_helpers.rb) to tell us to rename the column in six months\\n1. New annotations will be stored with the key split between `key_prefix` and `key_name` (`key` column)\\n1. Updated annotations will be stored with the key split between `key_prefix` and `key_name` (`key` column)\\n1. Updating annotations that do not meet the new prefix validations will fail with a `422` error\\n1. Existing annotations will continue to be readable until they are updated. Since they are not queryable this should not cause too much overhead.\\nWe **will not** attempt to migrate existing data into this new structure for the following reasons:\\n1. Migrations fail when Cloud Foundry is being upgraded and a failed migration is difficult for operators to recover from. We do not want to risk platform downtime and support incidents.\\n1. The operator upgrading a platform is likely not the one who created (or \"owns\") the resource with the problematic annotations. Lazily migrating annotations when they are updated ensures that the one who owns it gets to make the decision.\\nConsequences\\n------------\\n### Positive Consequences\\n1. We will be aligned with Kubernetes once more. \ud83d\ude42\\n1. We will have a more consistent schema across labels and annotations.\\n1. By separating `key_prefix` from `key_name` we will not preclude ourselves from adding new features that want to use or query on the prefix independently.\\n### Negative Consequences\\n1. Users who were relying on large annotation keys will be upset we took them away from them. \ud83d\ude14\\n1. We will be performing migrations across many different tables, there is still the potential for upgrade issues.\\n1. Lazily updating annotations means some will have the old structure for a long time.\\n"}
{"prompt":"## Context\\nPCMT wants to accomplish a number of goals through the use of versions:\\n1. Convey to stakeholders the initial suitability of the project:\\n* Pre-v1: In development for global stakeholders.\\n* Version 1: Production ready for global stakeholders.\\n* Version 2: Production ready for national stakeholders.\\n1. Leverage semantic versioning to convey to dependents the nature of the\\nrelease.  e.g. is it a patch-release, major new functionality, etc.\\n1. Mark a specific state of the code-base, tied to a released asset.\\n1. Indicate which version of Akeneo that PCMT is derived from.\\n","completion":"## Decision\n1. We will version PCMT by [Semantic Versioning 2.0][semver].  However since\\nDocker tags do not support the use of the `+` symbol, we'll use `-` in\\nits place.\\n1. We will have a build number (from the commit SHA), which is from a CI\\npipeline, that includes a distinct set of commits.\\n1. We will expire and remove old Build numbers, so they need to be pruned\\nregularly.\\n1. We will not publish assets if a pipeline fails, and conversely if a pipeline\\nis re-run, it consists of the same commits (state of the code), and therefore\\nit should have the same build number.\\n1. We will promote build numbers to a showcase or UAT server, manually.\\n1. We will promote build numbers to a release, manually.\\n1. We will keep the semantic version of the system separate from the build\\nnumber, in a file in the SCM.  When an asset is labeled from\\nthe CI pipeline, it'll use the full form by placing the build number in\\nthe build meta-data of the Semantic Version.  e.g. `1.0.0+48af4a30`\\n1. When promoting to a release we will co-label the published assets such that\\none asset will have the build number, and the released asset will not include\\nthat number. e.g. version 1.0.0 might be `image:v1.0.0+SHA` and\\n`image:v1.0.0`, which are equivalent assets.\\n"}
{"prompt":"## Context and Problem Statement\\nWhen executing tests, logback is loaded as logging framework.\\nLogback in testing mode is configured using `logback-test.xml`.\\nIn case logback is not configured, it does not output anything.\\n## Decision Drivers\\n- Ease of use for developers\\n","completion":"## Decision\n- Ease of use for developers\\nChosen option: \"Use same `logback-test.xml` for each sub project\", because\\n- During development, the \"local\" `logback-test.xml` file can be adjusted to the needs\\n- During continuous integration, the output should contain warnings and errors only and not any debug information\\n### Positive Consequences\\n- When modifying `logback-test.xml`, a developer just has to copy it over to the other sub projects without thinking which change to propagate to which sub project.\\n"}
{"prompt":"## Context and Problem Statement\\nIn the problem detection and solving approach by Saatkamp et al., detected problems in a topology are solved by specific algorithms.\\nThese algorithms must know some semantics in order to perform correctly.\\nConcretely: The IPSec algorithm must know some kind of abstract Virtual Machine (VM) Node Type, since it replaces unsecure VMs with secure VMs that open a secure connection on the IP level.\\n","completion":"## Decision\nChosen option: \"Abstract VM Node Type\" since TOSCA allows inheritance and inheritance creates mor semantic meaning.\\nChosen option: \"Secure VMs collected in a special namspace\" since they are special kinds of the \"normal\" VMs, they should inherit from them (and consequently from the abstract VM type mentioned above) to create a meaningful semantics.\\nHowever, instead of creating a special namespace, this should be changed to \"Annotate Secure Types with a Tag\" in near future.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nTraceability of a Docker image to a source code repository is valuable to debug any problem that comes up during testing and deployment.\\nDependency pinning on project images pinning a particular version of the base image they are using is also valuable for build reproducibility.\\n","completion":"## Decision\nWe will tag every new, tested version of an image using the commit SHA value that produced it.\\n"}
{"prompt":"## Context\\nWe want to be aware of CVEs (Common Vulnerabilities and Exposures) before they\\nend up in production, and make sure to block deployments with known high\\nseverity CVEs. Snyk allows us to scan our PRs for CVEs and fail builds if there\\nare any vulnerabilities in the code we've written.\\n","completion":"## Decision\nWe will use Snyk to:\\n- Run scans on PRs provide results and fail builds if any known high severity\\nCVEs are found.\\n- Run scans on main and fail builds on high severity CVEs, posting the results\\nto the Snyk platform for monitoring.\\n- Run nightly scans on the docker image and app dependencies.\\n"}
{"prompt":"## Context\\nThis project is design to explore architecture, so I have to record my decisions.\\n","completion":"## Decision\nFor all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\\nEach ADR will be recorded using [Michael Nygard template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\\n"}
{"prompt":"## Context\\nWe already have partial hydration implemented using react 'islands', a design pattern where we use multiple react apps on the same page, but now we have a requirement to share data between these islands.\\nReact portals would allow us to have one root react app with multiple portals existing within it without the need to hydrate the entire dom. See: https:\/\/reacttraining.com\/blog\/portals-with-context\/\\nThere are two ways to share data between portals. Using props, or react context.\\n### React context\\nUseful because it means all child components can access the context at any point in the tree but adds 'magic'.\\n### Props\\nMore explicit but causes additional prop drilling.\\n","completion":"## Decision\nImplement react portals pattern and use props to pass down shared data.\\nDon't use react context, use pure props instead. This means more prop drilling but we prefer this cost over the complexity of context.\\n"}
{"prompt":"## Context\\n### Mutations\\nMutations in Cassandra take place at a Cassandra *writetime* or timestamp. In normal Cassandra operation, these times\\nare the wall-clock times where a mutation took place. These are used to \"resolve\" conflicts when performing reads;\\nfor a given column, the most recently written value will be read (\"last-write-wins\").\\nAtlasDB columns take the form `(row, column1, column2, value)` where `column2` is the Atlas timestamp. These timestamps\\nare unique - thus, for transactional tables, no mutations to the same column should take place apart from deleting\\na cell that is no longer needed (e.g. because of sweep).\\nIn the context of AtlasDB, reliance on wall-clock time is generally considered unacceptable. Thus, Atlas uses its\\nown timestamps:\\n- a cell written at a start timestamp `TS` is given the Cassandra timestamp `TS`\\n- a sweep sentinel is given the Cassandra timestamp `-1`\\n- a tombstone inserted by deleting a cell with start timestamp `TS` is given the Cassandra timestamp `TS + 1`\\n- a range tombstone from time `0` to `TS` exclusive is given the Cassandra timestamp `TS`\\nFor transactional tables, there are several key invariants to be preserved:\\n- a tombstone or range tombstone that covers a cell with the Cassandra timestamp `TS` must have a Cassandra timestamp\\ngreater than `TS`\\n- a tombstone or range tombstone that covers a sweep sentinel with Cassandra timestamp `TS` must have a Cassandra\\ntimestamp greater than `TS`\\n- the insertion of a fresh sweep sentinel after a tombstone that covers the sentinel, written at timestamp `TS`, must\\nhave a Cassandra timestamp greater than `TS`\\nNotice that the existing system does not satisfy the third invariant.\\nSwitching from `CONSERVATIVE` sweep (writes a sentinel) to `THOROUGH` sweep\\n(which removes sentinels) and back is broken, in that the sentinels written by the conservative sweep after\\nthe reversion of the strategy will not be visible to readers.\\nThings behave differently for non-transactional tables:\\n- most of these tables (`_timestamp`, backup lock, schema mutation lock, schema metadata) are written to at `0`.\\n- the `_metadata` table is written to and read from at wall-clock time.\\n### Compaction\\nCassandra stores its data in multiple SSTables which are periodically compacted together. Compactions can help to\\nclear deleted data and reclaim disk space. This is when tombstones are reclaimed as well.\\nWhen compacting SSTables together, if the most recent value is a tombstone then Cassandra needs to determine whether\\nthe tombstone can safely be reclaimed or not. Cassandra has a first check based on the real-time age of the tombstone\\nand `gc_grace_seconds`, which gives us some protection against zombie data when a failed node recovers.\\nAssuming that check passes, we then look at the minimum write timestamp of SSTables not involved in\\nthe compaction. More precisely, reclaimable tombstones are tombstones where the deletion timestamp is less than the\\nlowest timestamp of\\n1. all other SSTables that include the partition being deleted, and\\n2. any cells within the same partition in memtables\\nOtherwise, we may have live data that the tombstone was covering suddenly become visible again.\\nSince Atlas writes its sweep sentinels at timestamp `-1`, some SSTables or memtables will have a minimum timestamp of\\n`-1` which may prevent compactions on subsets of SSTables from dropping many legitimately reclaimable tombstones.\\n","completion":"## Decision\nChange the timestamps at which sweep sentinels, tombstones and range tombstones are written to be a fresh timestamp\\nfrom the timestamp service. In the case of sentinels and tombstones, a single API call to add a set of garbage\\ncollection sentinels or delete a set of cells makes one call for a fresh timestamp to the timestamp service. Cell write\\ntimestamps are unchanged (so a write at timestamp `TS` still receives a Cassandra timestamp of `TS`). Taking an RPC\\noverhead here is fine, because these operations don't happen on critical path operations (i.e. they don't occur as part\\nof normal client read\/write transactions).\\nThe write pattern for the metadata table is unchanged (still uses wall-clock time). Some of this is because of a\\nchicken and egg problem with embedded users of the timestamp service; additionally, there are existing CLIs that\\nmanipulate the metadata table but don't have ready access to a timestamp service. Also, these tables receive minimal\\nwrites and deletes compared to actual transactional data tables, and are thus less of a concern from a Cassandra\\nstrain\/performance perspective.\\n### Proof of Correctness\/Safety\\nRecall the invariants we seek to preserve:\\n1. a tombstone or range tombstone that covers a cell with the Cassandra timestamp `TS` must have a Cassandra timestamp\\ngreater than `TS`\\n2. a tombstone or range tombstone that covers a sweep sentinel with Cassandra timestamp `TS` must have a Cassandra\\ntimestamp greater than `TS`\\n3. the insertion of a fresh sweep sentinel after a tombstone that covers the sentinel, written at timestamp `TS`, must\\nhave a Cassandra timestamp greater than `TS`\\nA cell being covered with Cassandra timestamp `TS` must also have Atlas start timestamp `TS`. Given that this cell is\\nalready written to the database, a fresh timestamp `TS'` is necessarily greater than `TS`, giving us statement 1.\\nStatements 2 and 3 follow from the guarantees of the timestamp service.\\n### Migrations\/Cutover\\nNotice that even in the presence of values written under the old scheme, our invariants are preserved.\\nStatement 1 holds because cell write timestamps haven't changed (and we can re-use the proof above).\\nFor statements 2 and 3, the timestamps of values we are covering would be `-1` and `deletionTimestamp + 1`;\\nwe know that a fresh timestamp will be at least that (since timestamps aren't given out twice).\\nTimestamps for existing values are not changed, so a major compaction may be necessary before one begins to\\nreap the benefits of better tombstone droppability. This may be especially relevant for larger tables that are marked\\nas `appendHeavyAndReadLight()` (because these use the `SizeTieredCompactionStrategy`) and also for heavier users in\\ngeneral (there may be several `-1` timestamps in SSTables that are not frequently compacted, and thus an issue).\\n"}
{"prompt":"## Context\\nWe want to be able to display more info regarding the status of the nodes connected to the network. A useful feature would be to display if a node is still connected or not and also to see a \"last seen\" timestamp of the nodes.\\n","completion":"## Decision\nIn order for the connection status to be displayed, an endpoint checking the multichain's [getpeerinfo](https:\/\/www.multichain.com\/developers\/json-rpc-api\/) function would be called. This returns the nodes connected at the moment. However, since MultiChain doesn't provide information about when a node was last seen we want to implement this feature in the api. A new stream should be created called `network_log` and every 24h an event would be saved on the stream displaying the addresses of the nodes connected at that moment. All nodes in the network perform the check but only saves it on the stream if there is no event saved for that day. This way, the check is being recorded even if one of the nodes is disconnected but it is also prevents duplicate entries.\\n"}
{"prompt":"## Context\\nUp to the point of writing this ADR, the SIA application aims to service only the municipality of Amsterdam. However, because of the nature of the application, more municipalities have shown an interest in the functionality the application has to offer.\\nThis poses a challenge, because there is a lot in the application that is specific to the municipality of Amsterdam. To sum up, amongst others (in random order):\\n- Docker registry URLs\\n- URLs of API endpoints, machine learning service, Authz service and map server\\n- Nginx configuration\\n- HTML `<title \/>`\\n- [`Sentry` Package](https:\/\/www.npmjs.com\/package\/@sentry\/browser) dependency and configuration\\n- [`Matomo` Package](https:\/\/www.npmjs.com\/package\/@datapunt\/matomo-tracker-js) dependency and configuration\\n- PWA manifest\\n- Logo\\n- Favicon\\n- PWA icons\\n- Hardcoded strings\\n- Main menu items\\n- Theme configuration\\n- Maps settings\\n- [`amsterdam-stijl` Package](https:\/\/www.npmjs.com\/package\/amsterdam-stijl) Package dependency\\n- Package URL and description\\nAll of the above need to be configurable or should be taken out of the equation to be able to publish a white-label version of `signals-frontend`.\\n","completion":"## Decision\nTaking the pros and cons, the application's architecture, the Datapunt infrastructure and upcoming client wishes into account, the decision is made to go for the [Server-side \/ container](#Server-side \/ container) option.\\nThe repository will contain default configuration options so that it can be run locally and still be deployed to (acc.)meldingen.amsterdam.nl without the application breaking.\\nConfiguration is injected in the `index.html` file so it can be read at run-time.\\n"}
{"prompt":"## Context\\nEvery month the service pays out approved claims as part of a payroll process\\nperformed by a third party payroll provider. To perform this function the\\npayroll provider needs to be given the details of the claims and claimants. This\\ndata is highly personal in nature and needs to be shared safely and securely\\nwith the third-party provider.\\n","completion":"## Decision\nThe payroll provider will download the monthly payroll data file directly from\\nthe service\u2019s back-office, which uses DfE's single sign-on service\\n[DfE Sign In](https:\/\/services.signin.education.gov.uk\/) for authentication.\\nThe payroll provider will be set up with their own organisation in DfE Sign In\\nso that they can manage their own user access and be responsible for\\nmovers\/leavers. Their users will only support a specific \u201cPayroll operator\u201d\\nrole. This role will only allow the downloading of the monthly payroll file and\\nnothing else within the back-office.\\nThe monthly payroll file will only be downloadable by the payroll provider\u2019s\\nusers. Other users will not be able to access the file.\\nThe file will only be available as a one-time download; once downloaded, that\\nmonth\u2019s file will no longer be available.\\n"}
{"prompt":"## Context\\nSome mailbox implementations of James store already parsed attachments for faster retrieval.\\nThis attachment storage capabilities are required for two features:\\n- JMAP attachment download\\n- JMAP message search \"attachment content\" criteria\\nOnly Memory and Cassandra backends can be relied upon as a JMAP backend.\\nOther protocols relies on dynamic EML parsing to expose message subparts (IMAP)\\nHere are the POJOs related to these attachments:\\n- **Attachment** : holds an attachmentId, the attachment content, as well as the content type\\n- **MessageAttachment** : composes an attachment with its disposition within a message (cid, inline and name)\\n- **Message** exposes its list of MessageAttachment when it is read with FetchType Full.\\n- **Blob** represents some downloadable content, and can be either an attachment or a message. Blob has a byte array\\npayload too.\\nThe following classes work with the aforementioned POJOs:\\n- **AttachmentMapper** and **AttachmentManager** are responsible of storing and retrieving an attachment content.\\n- **BlobManager** is used by JMAP to allow blob downloads.\\n- Mailbox search exposes attachment content related criteria. These criteria are used by the JMAP protocol.\\nThis organisation causes attachment content to be loaded every time a message is fully read (which happens for instance\\nwhen you open a message using JMAP) despite the fact that it is not needed, as attachments are downloadable through a\\nseparate JMAP endpoint, their content is not attached to the JMAP message JSON.\\nAlso, the content being loaded \"at once\", we allocate memory space to store the whole attachment, which is sub-optimal. We\\nwant to keep the consumed memory low per-message because a given server should be able to handle a high number of messages\\nat a given time.\\nTo be noted that JPA and maildir mailbox implementations do not support attachment storage. To retrieve attachments of a\\nmessage, these implementations parse the messages to extract their attachments.\\nCassandra mailbox prior schema version 4 stored attachment and its metadata in the same table, but from version 5 relies\\non the blobStore to store the attachment content.\\n","completion":"## Decision\nEnforce cassandra schema version to be 5 from James release 3.5.0. This allows to drop attachment management prior version\\n5.\\nWe will re-organize the attachment POJOs:\\n- **Attachment** should hold an attachmentId, a content type, and a size. It will no longer hold the content. The\\ncontent can be loaded from its **AttachmentId** via the **AttachmentLoader** API that the **AttachmentManager**\\nimplements.\\n- **MessageAttachment** : composes an attachment with its disposition within a message (cid, inline and name)\\n- **Blob** would no longer hold the content as a byte array but rather a content retriever (`Supplier<InputStream>`)\\n- **ParsedAttachment** is the direct result of attachment parsing, and composes a **MessageAttachment** and the\\ncorresponding content as byte array. This class is only relied upon when saving a message in mailbox. This is used as\\nan output of `MessageParser`.\\nSome adjustments are needed on class working with attachment:\\n- **AttachmentMapper** and **AttachmentManager** need to allow from an attachmentId to retrieve the attachment content\\nas an `InputStream`. This is done through a separate `AttachmentLoader` interface.\\n- **AttachmentMapper** and **AttachmentManager** need the Attachment and its content to persist an attachment\\n- **MessageManager** then needs to return attachment metadata as a result of Append operation.\\n- **InMemoryAttachmentMapper** needs to store attachment content separately.\\n- **MessageStorer** will take care of storing a message on the behalf of `MessageManager`. This enables to determine if\\nattachment should be parsed or not on an implementation aware fashion, saving attachment parsing upon writes for JPA\\nand Maildir.\\nMaildir and JPA no longer support attachment content loading. Only the JMAP protocol requires attachment content loading,\\nwhich is not supported on top of these technologies.\\nMailbox search attachment content criteria will be supported only on implementation supporting attachment storage.\\n"}
{"prompt":"## Context\\nThe main programming language for the present repository is Python. The Python ecosystem features a rich set of tools and libraries that facilitate developing an industry-standard codebase. Development includes implementation of production and test code in an environment built on linting and formatting tools.\\n","completion":"## Decision\n1. Test-driven development\\n1. Code maintainability\\n1. Code scalability\\n1. Code format consistency\\n1. Testing: `pytest`. Compared to unittest, test code is more concise yet readable, test fixture setup is straightforward, and test assertion output is very clear and helpful for debugging.\\n1. Code formatting: `black`. Developed by the Python Software Foundation themselves. Uncompromising, fast, deterministic.\\n1. Linting: `flake8`. Detection of common code smells. `pylint` tends to be pickier and might hinder rapid initial development.\\n1. Integration with `git`: `pre-commit`. Automatic style checks prior to committing. Orchestration of style check tools.\\n1. Isolated Python development environment: `venv`. Isolation of system and project Python packages in so called 'virtual environments'.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context\\nIn LinShare Flutter application, we can make offline mode files one by one from \"My Space\", \"Received Shares\" or \"Shared Spaces\" with the library [`sqflite`](https:\/\/pub.dev\/packages\/sqflite):\\nSQLite plugin for Flutter. Supports iOS, Android and MacOS.\\n- Support transactions and batches\\n- Automatic version management during open\\n- Helpers for insert\/query\/update\/delete queries\\n- DB operation executed in a background thread on iOS and Android\\nUsage\\n- Data\\n- Document Table: Help save documents when it make available offline in MySpace\\n````\\nTable Document {\\ndocumentId TEXT PRIMARY KEY \/\/ Id of document file\\ndescription TEXT \/\/ Description of document file\\ncreationDate Integer \/\/ Creation date of document file\\nmodificationDate Integer \/\/ Modification date of document file\\nexpirationDate Integer \/\/ Expiration date of document file\\nciphered Integer \/\/ Document file has been set code not?\\nname TEXT \/\/ Name of document file\\nsize Integer \/\/ Size of document file\\nsha256sum TEXT \/\/ SHA of document file\\nhasThumbnail Integer \/\/ Check if the file is downloadable as an thumbnail.\\nshared Integer \/\/ Number shares of document file.\\nmediaType TEXT \/\/ mediaType of document file.\\nlocalPath TEXT \/\/ local storage path of document file.\\n}\\n````\\n- SharedSpace Table: Help save shared space when it contains at least one file make available offline in SharedSpace\\n````\\nTable SharedSpace {\\nsharedSpaceId TEXT PRIMARY KEY,\\nsharedSpaceRoleId TEXT,\\nsharedSpaceRoleName TEXT,\\nsharedSpaceRoleEnable Integer,\\ncreationDate Integer,\\nmodificationDate Integer,\\nname TEXT,\\nnodeType TEXT,\\nquotaId TEXT,\\nversioningParameters Integer\\n}\\n````\\n- WorkGroupNode Table: Help save file\/folder when it contains at least one file make available offline in SharedSpace\\n````\\nTable WorkGroupNode {\\nnodeId TEXT PRIMARY KEY,\\nsharedSpaceId TEXT,\\nparentNodeId TEXT,\\ncreationDate Integer,\\nmodificationDate Integer,\\nname TEXT,\\nnodeType TEXT,\\ndescription TEXT,\\nnameAccount TEXT,\\nmailAccount TEXT,\\nfirstNameAccount TEXT,\\nlastNameAccount TEXT,\\naccountId TEXT,\\naccountType TEXT,\\nsize Integer,\\nmediaType TEXT,\\nhasThumbnail Integer,\\nuploadDate Integer,\\nhasRevision Integer,\\nsha256sum TEXT,\\nlocalPath TEXT\\n}\\n````\\n- Deployment\\n- Example 1:\\n+ Make available file `A.png` in SharedSpace `SP1` has path: `SP1 > A.png`\\n+ The data is stored as follows:\\nInformation of SharedSpace `SP1` will be saved in the SharedSpace table:\\n```\\nSharedSpace {\\nsharedSpaceId = SP1_ID,\\nname = SP1_NAME,\\n...\\n}\\n```\\nInformation of file `A.png` will be saved in the WorkGroupNode table:\\n```\\nWorkGroupNode {\\nnodeId = A_ID,\\nsharedSpaceId = SP1_ID,\\nparentNodeId = null,\\nname = A,\\nnodeType = DOCUMENT\\n...\\n}\\n```\\n- Example 2:\\n+ Make available file `file.png` in folders `B` & `B1` of SharedSpace `SP2` has path: `SP2 > B > B2 > A.png`\\n+ The data is stored as follows:\\nInformation of SharedSpace `SP2` will be saved in the SharedSpace table:\\n```\\nSharedSpace {\\nsharedSpaceId = SP2_ID,\\nname = SP2_NAME,\\n...\\n}\\n```\\nInformation of folder `B` will be saved in the WorkGroupNode table:\\n```\\nWorkGroupNode {\\nnodeId = B_ID,\\nsharedSpaceId = SP2_ID,\\nparentNodeId = null,\\nname = B,\\nnodeType = FOLDER\\n...\\n}\\n```\\nInformation of folder `B1` will be saved in the WorkGroupNode table:\\n```\\nWorkGroupNode {\\nnodeId = B1_ID,\\nsharedSpaceId = SP2_ID,\\nparentNodeId = B_ID,\\nname = B1,\\nnodeType = FOLDER\\n...\\n}\\n```\\nInformation of file `file.png` will be saved in the WorkGroupNode table:\\n```\\nWorkGroupNode {\\nnodeId = file_ID,\\nsharedSpaceId = SP1_ID,\\nparentNodeId = B1_ID,\\nname = file,\\nnodeType = DOCUMENT\\n...\\n}\\n```\\n","completion":"## Decision\nWe decided that LinShare could currently make offline mode a file one by one.\\n"}
{"prompt":"Context\\n=======\\nWhen building pipelines using concourse, we investigated using the [pool\\nresource](https:\/\/github.com\/concourse\/pool-resource) in order to control flow\\nthrough jobs. This was an alternative to the use of the\\n[semver resource](https:\/\/github.com\/concourse\/semver-resource).\\nThese 2 resources are both workarounds to solve the problem of triggering jobs\\nwhen we haven't made changes to a resource.\\nThe problem is that the pool resource relies on write access to a github repo,\\nwhich means we must pass public keys that allow this access into the pipeline\\nand deployed concourse instance - we want to minimise the number of credentials\\nwe pass, and the semver resource relies on AWS credentials that are already\\npassed.\\nDecision\\n========\\nWe will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThis was an investigation into a different approach, so no consequences\\n","completion":"## Decision\n========\\nWe will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThis was an investigation into a different approach, so no consequences\\n"}
{"prompt":"## Context\\nWhen a chain upgrade introduces state-breaking changes inside modules, the current procedure consists of exporting the whole state into a JSON file (via the `simd genesis export` command), running migration scripts on the JSON file (`simd genesis migrate` command), clearing the stores (`simd unsafe-reset-all` command), and starting a new chain with the migrated JSON file as new genesis (optionally with a custom initial block height). An example of such a procedure can be seen [in the Cosmos Hub 3->4 migration guide](https:\/\/github.com\/cosmos\/gaia\/blob\/v4.0.3\/docs\/migration\/cosmoshub-3.md#upgrade-procedure).\\nThis procedure is cumbersome for multiple reasons:\\n* The procedure takes time. It can take hours to run the `export` command, plus some additional hours to run `InitChain` on the fresh chain using the migrated JSON.\\n* The exported JSON file can be heavy (~100MB-1GB), making it difficult to view, edit and transfer, which in turn introduces additional work to solve these problems (such as [streaming genesis](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6936)).\\n","completion":"## Decision\nWe propose a migration procedure based on modifying the KV store in-place without involving the JSON export-process-import flow described above.\\n### Module `ConsensusVersion`\\nWe introduce a new method on the `AppModule` interface:\\n```go\\ntype AppModule interface {\\n\/\/ --snip--\\nConsensusVersion() uint64\\n}\\n```\\nThis methods returns an `uint64` which serves as state-breaking version of the module. It MUST be incremented on each consensus-breaking change introduced by the module. To avoid potential errors with default values, the initial version of a module MUST be set to 1. In the Cosmos SDK, version 1 corresponds to the modules in the v0.41 series.\\n### Module-Specific Migration Functions\\nFor each consensus-breaking change introduced by the module, a migration script from ConsensusVersion `N` to version `N+1` MUST be registered in the `Configurator` using its newly-added `RegisterMigration` method. All modules receive a reference to the configurator in their `RegisterServices` method on `AppModule`, and this is where the migration functions should be registered. The migration functions should be registered in increasing order.\\n```go\\nfunc (am AppModule) RegisterServices(cfg module.Configurator) {\\n\/\/ --snip--\\ncfg.RegisterMigration(types.ModuleName, 1, func(ctx sdk.Context) error {\\n\/\/ Perform in-place store migrations from ConsensusVersion 1 to 2.\\n})\\ncfg.RegisterMigration(types.ModuleName, 2, func(ctx sdk.Context) error {\\n\/\/ Perform in-place store migrations from ConsensusVersion 2 to 3.\\n})\\n\/\/ etc.\\n}\\n```\\nFor example, if the new ConsensusVersion of a module is `N` , then `N-1` migration functions MUST be registered in the configurator.\\nIn the Cosmos SDK, the migration functions are handled by each module's keeper, because the keeper holds the `sdk.StoreKey` used to perform in-place store migrations. To not overload the keeper, a `Migrator` wrapper is used by each module to handle the migration functions:\\n```go\\n\/\/ Migrator is a struct for handling in-place store migrations.\\ntype Migrator struct {\\nBaseKeeper\\n}\\n```\\nMigration functions should live inside the `migrations\/` folder of each module, and be called by the Migrator's methods. We propose the format `Migrate{M}to{N}` for method names.\\n```go\\n\/\/ Migrate1to2 migrates from version 1 to 2.\\nfunc (m Migrator) Migrate1to2(ctx sdk.Context) error {\\nreturn v2bank.MigrateStore(ctx, m.keeper.storeKey) \/\/ v043bank is package `x\/bank\/migrations\/v2`.\\n}\\n```\\nEach module's migration functions are specific to the module's store evolutions, and are not described in this ADR. An example of x\/bank store key migrations after the introduction of ADR-028 length-prefixed addresses can be seen in this [store.go code](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/36f68eb9e041e20a5bb47e216ac5eb8b91f95471\/x\/bank\/legacy\/v043\/store.go#L41-L62).\\n### Tracking Module Versions in `x\/upgrade`\\nWe introduce a new prefix store in `x\/upgrade`'s store. This store will track each module's current version, it can be modelized as a `map[string]uint64` of module name to module ConsensusVersion, and will be used when running the migrations (see next section for details). The key prefix used is `0x1`, and the key\/value format is:\\n```text\\n0x2 | {bytes(module_name)} => BigEndian(module_consensus_version)\\n```\\nThe initial state of the store is set from `app.go`'s `InitChainer` method.\\nThe UpgradeHandler signature needs to be updated to take a `VersionMap`, as well as return an upgraded `VersionMap` and an error:\\n```diff\\n- type UpgradeHandler func(ctx sdk.Context, plan Plan)\\n+ type UpgradeHandler func(ctx sdk.Context, plan Plan, versionMap VersionMap) (VersionMap, error)\\n```\\nTo apply an upgrade, we query the `VersionMap` from the `x\/upgrade` store and pass it into the handler. The handler runs the actual migration functions (see next section), and if successful, returns an updated `VersionMap` to be stored in state.\\n```diff\\nfunc (k UpgradeKeeper) ApplyUpgrade(ctx sdk.Context, plan types.Plan) {\\n\/\/ --snip--\\n-   handler(ctx, plan)\\n+   updatedVM, err := handler(ctx, plan, k.GetModuleVersionMap(ctx)) \/\/ k.GetModuleVersionMap() fetches the VersionMap stored in state.\\n+   if err != nil {\\n+       return err\\n+   }\\n+\\n+   \/\/ Set the updated consensus versions to state\\n+   k.SetModuleVersionMap(ctx, updatedVM)\\n}\\n```\\nA gRPC query endpoint to query the `VersionMap` stored in `x\/upgrade`'s state will also be added, so that app developers can double-check the `VersionMap` before the upgrade handler runs.\\n### Running Migrations\\nOnce all the migration handlers are registered inside the configurator (which happens at startup), running migrations can happen by calling the `RunMigrations` method on `module.Manager`. This function will loop through all modules, and for each module:\\n* Get the old ConsensusVersion of the module from its `VersionMap` argument (let's call it `M`).\\n* Fetch the new ConsensusVersion of the module from the `ConsensusVersion()` method on `AppModule` (call it `N`).\\n* If `N>M`, run all registered migrations for the module sequentially `M -> M+1 -> M+2...` until `N`.\\n* There is a special case where there is no ConsensusVersion for the module, as this means that the module has been newly added during the upgrade. In this case, no migration function is run, and the module's current ConsensusVersion is saved to `x\/upgrade`'s store.\\nIf a required migration is missing (e.g. if it has not been registered in the `Configurator`), then the `RunMigrations` function will error.\\nIn practice, the `RunMigrations` method should be called from inside an `UpgradeHandler`.\\n```go\\napp.UpgradeKeeper.SetUpgradeHandler(\"my-plan\", func(ctx sdk.Context, plan upgradetypes.Plan, vm module.VersionMap)  (module.VersionMap, error) {\\nreturn app.mm.RunMigrations(ctx, vm)\\n})\\n```\\nAssuming a chain upgrades at block `n`, the procedure should run as follows:\\n* the old binary will halt in `BeginBlock` when starting block `N`. In its store, the ConsensusVersions of the old binary's modules are stored.\\n* the new binary will start at block `N`. The UpgradeHandler is set in the new binary, so will run at `BeginBlock` of the new binary. Inside `x\/upgrade`'s `ApplyUpgrade`, the `VersionMap` will be retrieved from the (old binary's) store, and passed into the `RunMigrations` function, migrating all module stores in-place before the modules' own `BeginBlock`s.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.\\nHistorically, the Firefox Accounts codebase has not adhered to a formal CSS structure. This ADR serves to determine how we'll approach our CSS architecture in the Settings Redesign project, evaluating libraries and frameworks to determine which if any will be the best option for the FxA ecosystem. It is part 2 of two [Settings Redesign CSS ADRs](https:\/\/github.com\/mozilla\/fxa\/issues\/5087); part 1, detailing how we'll approach build conventions and variables, [can be found here](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0015-use-css-variables-and-scss.md).\\nConsiderations around class naming conventions, color and measurement standards, interoperability across shared components, and custom configuration options offered by each library to meet Settings Redesign design standards are taken into account. Notably, the new design uses space measurements in increments of 8px and [colors](https:\/\/protocol.mozilla.org\/fundamentals\/color.html) are based in Mozilla Protocol's design system, where a hue's brightness scales in increments of 10.\\n## Decision Drivers\\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\n","completion":"## Decision\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\nChosen options: \"Option B\" with Tailwind CSS for majority styling, and implementation details from \"Option D\" when utility classes don't meet the entire need, because:\\n- Of the options set forth, a utility library provides us with the most flexible yet durable set of tools.\\n- Single-purpose classes are performant and reduce the possibility of overly-complex or convoluted stylesheets.\\n- A utility library is leaner and less opinionated compared to a set of UI components and other options, allowing greater flexibility and reusability across various projects.\\n- Our team has prior experience with Tailwind in particular and newcomers should ramp up quickly with a utility pattern.\\n- Tailwind is highly configurable without being cumbersome, allowing us to modify type and spacing scales, define color ranges, and set up media queries to meet our exact needs.\\n- For cases when we do need to write custom SCSS we will structure our React components to initially rely on utility classes, but allow additional custom styles to be written in an adjacent SCSS file when needed. This is also applicable to components in `fxa-components` where the component can accept a `classes` prop with a list of needed utility classes, and any additional styling can be done in an external SCSS file located where the component was composed as needed (e.g., outside of `fxa-components`). CSS variables can be shared across the Tailwind configuration and in custom SCSS.\\n- Note: class name conventions for the custom SCSS will be declared when the library configuration is setup, as we'd like them to make sense together. Examples will be provided in the `fxa-settings` README at this time as well.\\n"}
{"prompt":"## Context\\nToggles are a useful aspect of Terraform modules that provide the ability to\\nturn on\/off functionality.\\nCapital One have written about\\n[Building Feature Toggles into\\nTerraform](https:\/\/medium.com\/capital-one-tech\/building-feature-toggles-into-terraform-d75806217647)\\n. This build on top of the use of the count and ternary to provide a more robust\\nfeature toggle.\\nAs this module is envisioned to be part of a larger Data Storage Module there is\\na concept where no DynamoDB table may need to be provisioned. In addition, when\\nusing DynamoDB Autoscaling can be a useful feature. Providing the capabilities\\nto turn this on and off based on requirements is also a useful requirement.\\n","completion":"## Decision\nFeature toggles will be leveraged to determine:\\n* If any resources should be created\\n* If a DynamoDB resource should be created\\n* If the Autoscaler should be enabled\\n"}
{"prompt":"## Context\\nWhen creating tests, it helps to have good bean objects. But when you\\nneed a bigger structure of some objects, it's a big thing to create them\\non demand. It would be nice to have a builder pattern implemented for\\nmost objects. This is of course depending on the language and the tools\\nused.\\n","completion":"## Decision\nFor `java`, create a `TestBuilder` for each bean class used in tests.\\nThe `TestBuilder` creates default values for the objects but can have\\nthem easily be overwritten.\\nHere's an example:\\n``` java\\npublic class CustomerTestBuilder {\\nprivate String companyName;\\nprivate Address address;\\npublic CustomerTestBuilder() {\\nthis.companyName = \"ACME Inc.\";\\nthis.address = new AddressTestBuilder().build();\\n}\\npublic CustomerTestBuilder withCompanyName(String companyName) {\\nthis.companyName = companyName;\\nreturn this;\\n}\\npublic CustomerTestBuilder withoutCompanyName() {\\nthis.companyName = null;\\nreturn this;\\n}\\npublic CustomerTestBuilder withAddress(Address address) {\\nthis.address = address;\\nreturn this;\\n}\\npublic CustomerTestBuilder withoutAddress() {\\nthis.address = null;\\nreturn this;\\n}\\npublic Customer build() {\\nCustomer result = new Customer();\\nresult.setCompanyName(this.companyName);\\nresult.setAddress(this.address);\\nreturn result;\\n}\\n}\\n```\\n"}
{"prompt":"## Context\\nWe have a lot of scripts, pipeline definitions, terraform files, yaml files and\\ntemplates which need to define and use variables. We want a consistent\\nconvention for naming these so that, as we write code in multiple,\\ninter-dependent repositories, we can be confident that the names we are using\\nare correct.\\n","completion":"## Decision\nWe will always use snake case (e.g. `foo_bar`) for variable names which appear\\nin terraform\/yaml files and templates.\\n"}
{"prompt":"## Context\\nI wanted to create a website blog using [Markdown][1], a [static site generator][2] and [HapiJS][3] to serve the files. Static site generators I tried, in order, were -\\n### [11ty][4]\\nPros\\n- Easy setup\\n- Predictable static file output\\n- Relatively small codebase\\n- Supporst different template engines\\nCons\\n- The frontmatter parser [didn't work as I expected][7]\\n- Template errors were not reported and would only fail on build\\n- Needs some attention to make production ready e.g. SEO, images, favicons etc\\n### [Gatsby][5]\\nPros\\n- Modern [JAMstack][8] project with React\\n- Batteries included by default - includes helpers for favicons, image loading, SEO etc.\\n- Opportunity to use GraphQL\\nCons\\n- Doesn't output static files - [totally depends on clientside JavaScript][9]\\n- It relies on service workers to cache responses which causes flakey e2e tests and unpredictable behaviour\\n- Developing static files based on API was difficult because of caching and unexpected error handling\\n- Relatively complex project with separate build, develop steps, config, plugins etc\\n### [Hugo][6]\\nPros\\n- Fast and mature static site generator\\n- Small footprint\\nCons\\n- Written in Go - I prefer fullstack JavaScript for this site\\n","completion":"## Decision\nIn the context of using several static site generators each of them had with their own downsides. And facing the concern of having blog that I can easily maintain and customise I decided to build my blog with HapiJS to achieve a smaller blog footprint, predictable behaviour and an opportunity to learn something new. I accept that I'm re-inventing the wheel.\\n"}
{"prompt":"## Context\\nData sources like plain tables (.csv, .tsv, .txt, .json, .xml) and slow\/rate-limited APIs both share the fact that you want to keep the interaction count low.\\nPlain tables need to be parsed and are missing an index for fast navigation wich reduces speed in reading and often uses computation time and memory.\\nRate-limited APIs should not be consumed twice for the same information.\\nWhen they allow retrieving list data similar to single entries that should be preferred to allow for smart rate-limit usage.\\n","completion":"## Decision\n* Allow emission to take place in the same moment as exploration\\n"}
{"prompt":"## Context\\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\\nserver.\\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\\nObject Storage technologies.\\nObject Storage is known to be cheap and yet offers guarantee about durability.\\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\\nIt also contribute negatively to the project build time.\\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\\nour Swift code without dropping Swift support at the same time.\\n","completion":"## Decision\n* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\\n"}
{"prompt":"## Context\\nPIMS requires a database to store all property information.\\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\\nAdditionally it must be supported by Entity Framework Core 3.1.\\n","completion":"## Decision\nOriginally the database generated for the SWU was with PostgreSQL, after further consideration it made more sense to tightly couple both MS-SQL with .NET Core.\\nThis will give us better performance and tighter integration with Entity Framework Core.\\nIt was decided to create a Linux docker container to host the MS-SQL 2019 database.\\n"}
{"prompt":"## Context\\nIn [this issue](https:\/\/github.com\/raft-tech\/TANF-app\/issues\/771) we are investigating the use of pre-signed URLs to determine whether there are security issues with the approach.\\nWe had originally implemented pre-signed URLs for downloading files because the system would need to download the files from S3 and then download them again to the client, using resources on the backend for every download. This would not cause a problem in this stage of development, but when the backend will be charged with parsing data from potentially large files, those system resources would become more precious. Using pre-signed URLs takes the added pressure off of the backend entirely.\\n","completion":"## Decision\nWe believe the use of time\/IP address limited signed URLs is a reasonably secure approach to downloading files from S3. However, we also believe that it may cause issues with our ATO approval as the data is highly sensitive. Furthermore, 18F published a recommendation today, [recommending to not use pre-signed URLs](https:\/\/engineering.18f.gov\/security\/cloud-services\/) for FISMA High projects.\\nIn our investigation we discovered a way that we can [securely download the files from the backend while [streaming the files](https:\/\/github.com\/jschneier\/django-storages\/blob\/master\/storages\/backends\/s3boto3.py#L83) directly from S3 to the client, taking any pressure off of resources needed for parsing files on the backend.\\nIn light of these facts we have decided to shift our efforts to download files from the backend.\\n"}
{"prompt":"## Context\\nRight now, in order to run an elm-spec program, we need to do a few things in a definite order.\\nAt some point, compile the Elm code using the `Compiler` from elm-spec-core. This will\\nwrap the compiled code so that the parts of the program that interact with the outside world\\ncan be easily faked out.\\nThen, to get the program running: First, create an `ElmContext` object. This creates\\nall the fake objects on the `window`\\nobject that the compiled elm code will attempt to reference. Second, evaluate the compiled\\nelm code. It doesn't matter when we compile the elm code, of course, just that it is\\nevaluated in the browser environment *after* we have created a new `ElmContext` in that\\nenvironment.\\nSo, it's a little wild, I guess, that simply instantiating an `ElmContext` modifies the `window`\\nobject and so on.\\nPart of the need for this comes from the fact that the compiled Elm code is wrapped in an IFFE.\\nBut there's no reason why we actually have to do that ...\\nWe've been able to deal with this problem so far because the only things that need to go\\nthrough this process are elm-spec-runner and karma-elm-spec-framework. But with the harness,\\nwe are now asking a test writer to follow this process as well. For that reason, we need to\\nsimplify it so it's not a source of errors.\\n","completion":"## Decision\nWe should change this flow so that we don't need to create an `ElmContext` and evaluate the\\ncompiled Elm code in a particular order.\\nFirst, we will wrap the compiled Elm code in a function that takes an `ElmContext`. Evaluating\\nthis code will still attach the `Elm` object to the window (since we're providing it with a\\nproxy object). But by using a normal function here, we have more control over when the `Elm`\\nobject is loaded.\\nThen, we need to have `ElmContext` store references to all the fakes inside itself -- there's no\\nreal need to store these on the `window` object. So the only things we need to store on the `window`\\nare (1) the function to load Elm -- because this is how the compiled Elm code provides the function;\\nI don't think we can reference it any other way. (2) Data on what the compiler actually\\ntried to do, like what files it tried to compile, the path and all that; we store this on the window\\nso it's available as soon as the compiled code is evaluated -- there may be better ways to do this.\\nAnd (3) the function for loading files, which has to be a function accessible on the window\\nanyway (since that's how Playwright and JSDOM allow us to register a function to be executed in Node).\\nOnce we do this, then it turns out that the test writer doesn't need to create an `ElmContext`\\nat all ... we can have `HarnessController` do that when `prepareHarness` is called. In fact,\\nwe don't even need to bundle anything extra. This will be included in the bundle of tests that\\nget executed in the browser.\\nWe could also change SuiteRunner to create it's own `ElmContext` as well, but it's not necessary.\\n"}
{"prompt":"## Context\\nFeature requests for both summary routes and dynamic route discovery have been proposed so that new routes are automatically discovered and synchronized in order to reduce the risk of new routes being missed.\\n","completion":"## Decision\nProgrammatically making changes to a production routing table is serious business that has the potential to cause network outages.\\nOnly specified routes will be synchronized to prevent unintentional changes.\\n"}
{"prompt":"- [Context](#context)\\n- [Decision](#decision)\\n- [Status](#status)\\n- [More reading](#more-reading)\\n","completion":"## Decision\n- [Status](#status)\\n- [More reading](#more-reading)\\nTo reduce that gap, we've decided to use the Domain-Driven Design technique (as you can see in [ADR#0003](0003-domain-driven-design.md)).\\nWe've divided our domain architecture into two scopes:\\n1. **Model Overview:** As you can see here, an overview of our architecture, considering only the most relevant entities.\\n2. **Local Domain Overview:** Every other file in this `domain` folder. They're only considering their domain, with a more detailed view and exploring their inner objects and considering only external entities that affect them.\\n![Snapshot of last know state of our domain model 2021-05-04](..\/assets\/0015-model-overview\/2021-05-04-diagram.jpg)\\n"}
{"prompt":"## Context\\nCryptpad solution image used in docker was deprecated and flist doesn't make use of volumes\\n","completion":"## Decision\n- Update flist to make use of volumes\\n- Update base image in docker file to be able to maintain\\n"}
{"prompt":"## Context\\n* We have an implementation of a Holochain conductor that can load QML based UIs: https:\/\/github.com\/holochain\/holosqape\\n* We also want to support HTML\/JS\/CSS based UIs\\n* We need to enable pure Web clients to connect to a HoloPort for the Holo use-case\\n* With [ADR 13](https:\/\/github.com\/holochain\/holochain-rust\/blob\/develop\/doc\/architecture\/decisions\/0013-signals-listeners-model-and-api.md)\\nwe defined *app signals*, i.e. events that a Holochain app can push - we need to make sure UIs can receive these as\\npushed notifications from the Holochain node\\n","completion":"## Decision\nStart with: Option 2.\\nThe existing Holochain conductor project HoloSqape is to be extended to provide a **WebSocket interface**\\n(next to the QML based GUI plug-in system) that enables many different kinds of external\\nuser interfaces to connect to the local Holochain node, authenticate, issue zome calls and receive zome signals,\\ndepending on permissions administered by the local user of the node through an admin UI or default settings.\\nA **JavaScript based client library** is to be implemented that makes it easy to use this interface from the context\\nof a web browser, hiding the complexity and wire-protocol of the websocket interface itself, only offering high-level\\nfunctionality such as:\\n* connecting to a local or remote node\\n* authentication (public-key? JWT tokens?)\\n* quering installed\/running apps, the session has access to\\n* issuing zome function calls and receiving results\\n* registering callbacks for zome signals\\n* lifecycle management of the websocket connection\\n![](..\/WebSocket-interface-HoloSqape.png)\\nFollowup with: Option 1 and\/or 4\\nProviding some kind of RESTful interface to Holochain is needed if we are\\nserious about reaching the widest development audience across the most client\\nlanguages\/environments possible.\\nThe good news is that there are many existing examples of realtime providers and\\nlibraries\/frameworks offering both WebSockets and REST APIs (e.g. Twillio, Ably,\\nFeathers, Slack, etc.).\\nLong term: Holo server\/clients and native electron apps\\nWoo!\\n"}
{"prompt":"- [Context](#context)\\n- [Current Design](#current-design)\\n- [Alternative Approaches](#alternative-approaches)\\n- [Prior Art](#prior-art)\\n- [Ethereum](#ethereum)\\n- [Diem](#diem)\\n- [Decision](#decision)\\n- [Detailed Design](#detailed-design)\\n- [CheckTx](#checktx)\\n- [Mempool](#mempool)\\n- [Eviction](#eviction)\\n- [Gossiping](#gossiping)\\n- [Performance](#performance)\\n- [Future Improvements](#future-improvements)\\n- [Consequences](#consequences)\\n- [Positive](#positive)\\n- [Negative](#negative)\\n- [Neutral](#neutral)\\n- [References](#references)\\n","completion":"## Decision\n- [Detailed Design](#detailed-design)\\n- [CheckTx](#checktx)\\n- [Mempool](#mempool)\\n- [Eviction](#eviction)\\n- [Gossiping](#gossiping)\\n- [Performance](#performance)\\n- [Future Improvements](#future-improvements)\\n- [Consequences](#consequences)\\n- [Positive](#positive)\\n- [Negative](#negative)\\n- [Neutral](#neutral)\\n- [References](#references)\\nTo incorporate a priority-based flexible and performant mempool in Tendermint Core,\\nwe will introduce new fields, `priority` and `sender`, into the `ResponseCheckTx`\\ntype.\\nWe will introduce a new versioned mempool reactor, `v1` and assume an implicit\\nversion of the current mempool reactor as `v0`. In the new `v1` mempool reactor,\\nwe largely keep the functionality the same as `v0` except we augment the underlying\\ndata structures. Specifically, we keep a mapping of senders to transaction objects.\\nOn top of this mapping, we index transactions to provide the ability to efficiently\\ngossip and reap transactions by priority.\\n"}
{"prompt":"## Context\\nAs developer, I have a good understanding of clujure language, but not the ecosystem. How persist in db, create api's, secure them, etc.\\n","completion":"## Decision\nThis will be a pet project, the goal is learning clojure ecosystem, not the aplication itself.\\n"}
{"prompt":"## Context\\nWe need to ensure that when asynchronous calls are made that they do not cause issues with database transactions scopes.\\nIn normal circumstances (without @Async) a transaction gets propagated through the call hierarchy from one Spring @Component to the other.\\nHowever, when a @Transactional Spring @Component calls a method annotated with @Async this does not happen.\\nThe call to the asynchronous method is being scheduled and executed at a later time by a task executor and is thus handled as a 'fresh' call.\\n","completion":"## Decision\nWe decided to prevent issues with @Async calls that we only sent primitive values or DTOs rather than a database entity.\\nThis ensures that there is no unexcepted behaviour when performing a lazy-fetch of the database entity.\\n"}
{"prompt":"## Context\\n[ADR-14](0014-apply-historical-events-to-aggregates.md) relaxed the\\nspecification such that `AggregateRoot.ApplyEvent()` implementations were no\\nlonger required to panic with an `UnrecognizedMessage` value when passed an\\nunexpected message type.\\nPrompted by this requirement, we relaxed the requirement for ALL handler\\nmethods, which was likely too broad of a change.\\nSpecifically, unlike when handling a message, the routing methods\\n`AggregateMessageHandler.RouteCommandToInstance()` and\\n`ProcessMessageHandler.RouteEventToInstance()` do not have the option of \"doing\\nnothing\" when passed an unexpected message type.\\n","completion":"## Decision\nReinstate the hard requirement that the handlers MUST panic with\\n`UnexpectedMessage` when asked to route a message type that was not configured\\nas being consumed by that handler.\\n"}
{"prompt":"## Context\\n* We have a Clojure application\\n* We do not know the security requirements for hiding the application\\n* We would rather not pay for hosting\\n","completion":"## Decision\n* We host on Heroku. A free PAAS provider.\\n"}
{"prompt":"## Context\\nAs we continue to solidify our API spec, we need to explicitly define component behaviors in the spec and make sure those are implemented in our implementation. This document captures our decisions on state store behaviors. It's expected that we'll create more of such documents to capture explicit component behavior decisions.\\n","completion":"## Decision\n### Concurrency model\\n* Dapr supports two flavors of optimistic concurrency: first-write wins and last-write wins. First-write wins is implemented through ETag.\\n* User code can express concurrency intention with a *config* annotation attached to a request. See **Config annotation** for details.\\n* Future version of Dapr may support call throttling through application channel.\\n* We'll choose last-write wins as the default.\\n### Consistency model\\n* Dapr supports both eventual consistency and strong consistency.\\n* Actors always use strong consistency.\\n* We'll choose eventual consistency as default for services other than actors.\\n### Actor Transaction\\n* Dapr-compatible Actor state stores shall support ACID transaction.\\n* Dapr doesn't mandate specific transaction isolation level at this point. However, when deemed necessary, we can easily add those to **Config annotation** as needed.\\n### Config annotation\\n* User payload can contain an optional **config** annotation\/element that expresses various constraints and policies to be applied to the call, including:\\n* Concurrency model: first-write or last-write\\n* Consistency model: strong or eventual\\n* Retry policies:\\n* Interval\\n* Pattern: linear, expotential\\n* Circuit-breaker Timeout (before an open circuit-breaker is reset)\\n### State store configuration probe\\n* An Dapr-compatible state store shall provide an endpoint that answers to configuration probe and returns (among others):\\n* Supported concurrency model\\n* Supported consistency model\\n* A state store instance shall return the specific configuration of the current instance.\\n* It's considered out of scope to require state store to dynamically apply new configurations.\\n### Dapr\\n* Update state store API spec to reflect above decisions\\n* Create backlog of issues to implement above decisions\\n"}
{"prompt":"## Context and Problem Statement <!-- required -->\\nSDR is implemented by way of many different discrete services, some of which make network requests of each other to do their work.  For example, at present:\\n* Argo calls Preservation Catalog to retrieve computed checksums for file content, and to retrieve actual preserved file content.\\n* Many different services use dor-services-app for read and write operations on our digital repository objects.\\nOf course, we want to make sure that not just any client on the network can use these services, since access to them should be limited to authorized callers.\\nIn the past, this was accomplished primarily through network access restrictions (e.g., firewall whitelisting IPs of services that should have access, limiting access to clients inside the VPN, etc).  However, sole use of this approach has been deprecated by the industry at large and by Stanford UIT in particular (though it is still an important component of security).\\n## Decision Drivers <!-- optional -->\\n* We want to secure access to our API endpoints.\\n* UIT wants us to secure access to our API endpoints.\\n* We would like an approach that's relatively easy to understand and maintain.\\n","completion":"## Decision\n* We want to secure access to our API endpoints.\\n* UIT wants us to secure access to our API endpoints.\\n* We would like an approach that's relatively easy to understand and maintain.\\nThe infrastructure team came to consensus in a weekly planning meeting that, going forward, we should gate access to API endpoints using JWTs (minted by the service, provided with requests by the client).  This ADR is meant to capture and flesh out that decision.\\n### Positive Consequences <!-- optional -->\\n* More robust and less circumventable than restricting access solely by way of network and firewall configuration.\\n* Does not preclude keeping appropriate firewall restrictions, which should remain part of our security practice.\\n* Should a network re-configuration result in accidental loosening of firewall restrictions, token based authentication provides a robust additional line of access control.\\n* For the ways we've built our applications, token based authentication is at least as easy to implement and maintain as client certificate based authenication, and should provide similarly adequate security.\\n* Minting tokens has become a common approach in industry for authenticating client access to APIs, so there is a wealth of current information and advice available online for this practice.\\n### Negative Consequences <!-- optional -->\\n* More work for developers than solely relying on firewall rules and network configuration (tasks which typically fall to operations and which should happen anyway).\\n"}
{"prompt":"## Context\\nThe `mysql-monitoring-release` comprises several self-contained `src` modules.\\nThe expectation is that each contains a `bin` directory, with a `test` bash\\nscript representing the set of unit tests for a given module.\\nThis approach makes sense, but is leading to some copied linting code.\\n","completion":"## Decision\nWe considered:\\n1. Making a separate module that each of the other modules' tests would source\\n1. Pulling the linting code out of the modules entirely, and putting it in CI\\nBoth options eliminate the redundant, difficult-to-maintain copypasta.\\nWe opted for option 1, as we felt that it enabled the quickest feedback loop on\\ncode formatting, because it still existed inside of the `bin\/test` scripts.\\n"}
{"prompt":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n","completion":"## Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\nAn easy way to improve our communication is by creating templates for both `issues` and `pull requests`. Those templates must be simple, easy to use, and express all required sections for our developers and users.\\nYou can check the suggested templates [in the following folder](..\/assets\/0002-github-template-for-issues-and-pull-requests).\\n"}
{"prompt":"## Context:\\nThe Data Warehouse tracks changes to Content Items through time so, for\\nexample, if we change the organisation attached to a Content Item, we should\\nknow WHEN that change happened and WHAT changed.\\nTracking changes not only applies to other entities associated to the Dimension\\nItems, but also to core attributes like the `title`, `description`,\\n`rendering-app`, or any other attribute. This is the foundation of our Data\\nWarehouse: track changes and track performance at the same time.\\nIn GOV.UK we can identify a Content Item on a unique way by their `content_id`.\\nIt is the combination of a `content_id` and a `locale` what makes a Content\\nItem unique. So on our first iteration of the DW, when we decided to use the\\nContent Item as the `grain`, hoping to be able to track all our changes by\\n`content_id`.\\nShortly after we noticed that most of our metrics are not related to a Content\\nItem but the `base_path` of the item; the is due to some Content Items like\\nGuides or Travel Advice, has multiple `base_paths` and we need to track metrics\\nat that level (sub item). This was a business requirement driven by user\\nresearch.\\nSo we decided to reduce the scope of the `grain` to the `base_path` of the\\nContent Item, which simplified and fulfilled our user needs. It was accepted by\\nthe business that if the `base_path` changes, then we won't be able to track\\nthe series backwards. We decided to postpone dealing with this issue when we\\nhad more information about it.\\nAs of today, we have learned more about our real needs for querying metrics.\\nAggregating metrics through time for a Content Item with a base_path, need to\\nbe done via its `content_id`, and there some edge cases to handle to be\\naccurate. Unfortunately, to address this issues, we need to make our queries\\nmore complex, and the codebase is way more complicated to maintain and reason\\nabout.\\nThe underlying problem is that we don't have a way to track our grain\\n(base_path) through time because all the attributes can be changed, and\\novercoming this limitation using `content_id` and `locales` it is just not\\nworth the effort.\\nIn summary, we would need to provide an efficient way to:\\nGiven a `base_path` in their latest version, select all the metrics through\\ntime, regardless of the changes to the item.\\nAt the very end, what we are asking for is for a real unique identifier for the\\nitem, because we actually don't have it, because the `content_id` is not\\nplaying that role.\\n","completion":"## Decision\nAssociate a unique identifier to each item that is tracked in the dimensions\\nitem.\\nEach `base_path`, which represents the grain of Data Warehouse, will have a\\nunique identifier that will never change. This ID will allow us to perform\\nqueries through time in an efficient way, and also to be accurate with our\\nresults.\\nOn the same go, all the Contnet Items that share a locale will also be uniquely\\nreferenced in the Data Warehouse.\\n### Note\\nWe would not need to add this unique identifier if\\n1. The sub-items for Content with multiple paths have a unique UID for each `base_path`.\\nUnfortunately, to implement this feature we would need to modify each publishing\\napplication; which would impact our immediate delivery, so it needs a wider conversation.\\n2. All the content items that have different locales but same ID have a different\\n`content_id`.\\n"}
{"prompt":"## Context\\n1. \u673a\u5668\u5185\u5b58\u592a\u5c0f\u4e86\uff0c\u9700\u8981\u5347\u7ea7\u4e00\u4e0b\uff1b\\n2. \u6211\u4eec\u4e4b\u524d\u516c\u53f8\u670d\u52a1\u9700\u8981\u7528\u591a\u5c11\u591a\u5c11\uff1b\\n3. Java \u7a0b\u5e8f\u6700\u5927\u6700\u5c0f\u5806\u7684\u6307\u5b9a\u65e0\u6807\u51c6\u3002\\n","completion":"## Decision\n* \u5206\u6790\u4e1a\u52a1\u573a\u666f\uff0c\u660e\u786e\u6211\u4eec\u5230\u5e95\u9700\u8981\u591a\u5c11\u5185\u5b58\uff0c\u5408\u7406\u7684\u9884\u75591-2\u500d\u5185\u5b58\u90fd\u6ca1\u6709\u95ee\u9898\uff1b\\n* \u6211\u4eec\u7684\u4e1a\u52a1\u76ee\u524d\u5206\u522b\u7531\u4e24\u79cd\u8bed\u8a00\u5b9e\u73b0 Python \u53ca Java\uff0cJava \u7a0b\u5e8f\u5bf9\u5185\u5b58\u7684\u4f7f\u7528\u91cf\u975e\u5e38\u5927\u4e14\u76d1\u63a7\u4e0d\u76f4\u89c2\uff0c\u4e3a\u4e86\u5408\u7406\u5206\u914d\u5185\u5b58\uff0c\u6211\u4eec\u91c7\u53d6\u4e86\u4ee5\u4e0b\u5206\u6790\u65b9\u6cd5\u3002\\n* \u5206\u914d\u7684\u6700\u5927\u5185\u5b58\u7528\u5b8c\u540e\uff0cGC \u9891\u7387\u9ad8\u4e14\u7279\u5b9a\u65f6\u95f4\u5185\u65e0\u6cd5\u505a\u5230\u5783\u573e\u56de\u6536\uff0c\u5c06\u4f1a\u5bfc\u81f4 `java.lang.OutOfMemoryError`\uff1b\\n* OOF\uff0cGC \u9891\u7387\u53ca GC \u65f6\u95f4\u662f\u786e\u5b9a heap \u53c2\u6570\u7684\u4e00\u4e2a\u6307\u6807\uff1b\\n* \u7ebf\u4e0a\u4e0d\u53ef\u80fd\u7b49\u5230 OOF \u65f6\uff0c\u624d\u505a\u5347\u7ea7\uff0c\u8fd9\u6837\u76f4\u63a5\u5f71\u54cd\u4e86\u7cfb\u7edf\u7684\u53ef\u7528\u6027\uff0c\u6240\u4ee5\u9488\u5bf9 Java \u7a0b\u5e8f\uff0c\u4e00\u5b9a\u8981\u505a\u597d\u538b\u529b\u6d4b\u8bd5\u6216\u5bf9 JVM \u505a\u597d\u76d1\u63a7\uff08\u901a\u8fc7 Java VisualVM or JConsole\uff09\uff0c\u9274\u4e8e\u6211\u4eec\u5df2\u7528 Newrelic \u6808\uff0c\u5c06\u6709\u5176\u901a\u77e5\u6211\u4eec\u670d\u52a1\u7684\u72b6\u51b5\uff0cMemory Analyzer \u53ef\u4ee5\u505a\u66f4\u8be6\u7ec6\u7684\u53d8\u91cf\u7ea7\u522b\u5185\u5b58\u4f7f\u7528\u67e5\u770b\uff1b\\n* \u5185\u5b58\u7684\u4e0d\u5408\u7406\u4f7f\u7528\u6709\uff1a\u5c06\u5927\u6587\u4ef6\u5b8c\u6574\u52a0\u8f7d\u81f3\u5185\u5b58\uff0c\u5c06\u6574\u4e2a\u8868\u7684\u6570\u636e\u8bfb\u53d6\u81f3\u5185\u5b58\uff1b\\n* \u5185\u5b58\u6cc4\u6f0f\uff1a\u5fae\u89c2\u7684\u6709\u5bf9\u8c61\u7684\u5f15\u7528\u95ee\u9898\uff0c\u5b8f\u89c2\u7684\u6709\u8fde\u63a5\u672a\u91ca\u653e\u7b49\uff1b\\n* Xmx \u8bbe\u7f6e\u540e\uff0c\u5bf9\u7cfb\u7edf\u6765\u8bf4\u662f\u5df2\u4f7f\u7528\u7684\uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u540c\u65f6\u5173\u6ce8\u7cfb\u7edf\u7ea7\u522b\u548c JVM \u7ea7\u522b\u7684\u5185\u5b58\u4f7f\u7528\u60c5\u51b5\u3002\\n![][image-1]\\n"}
{"prompt":"## Context and Problem Statement\\nThe threat modeling approach relies on pairs of threats and mitigations.\\nEach \"threat\" should be referenced by one particular \"mitigation\".\\n","completion":"## Decision\nChosen option: hardcoded namespaces, due to ease of implementation and static nature of the problem\\n### Positive Consequences\\nIn the context of threat modeling multiple different types of threats\/mitigations are not necesaary so a minimal base type that carries the required properties (reference) can be used and extended\\n"}
{"prompt":"## Context\\nOur current update process has several flaws. These lead towards an\\nunreliable autoupdate process.\\nThe existing autoupdate functionality works as follows:\\nOn posix:\\n1. A new version is downloaded into the staging area\\n2. The new version is moved to replace the current binary\\n3. The new binary is exec'ed\\nOn Windows:\\n1. A new version is downloaded into the staging area\\n2. The current exe is moved out of the way\\n3. The new exe is moved into place\\n4. The service is restarted\\nThere are some inherent problems with this design:\\n* Running binaries no longer appear on disk\\n* Files on disk no longer match what was installed via the package manager\\n* On windows, there is a failure condition where the new binary isn't\\nmoved into place, and the next service restart causes a service\\nfailure (https:\/\/github.com\/kolide\/launcher\/issues\/509)\\nThese lead us to needing a new update process.\\n### Assumptions in TUF\\nAdditionally, our implementation must work within the assumptions of\\n[TUF](https:\/\/godoc.org\/github.com\/kolide\/updater\/tuf).\\nTUF has a very simple model. It's designed to notice remote metadata\\nchanges for a single file, download it into a given location, and\\ntrigger into a callback function.\\nBecause it has a single file model, we cannot easily store these by\\nversion.\\nFurthermore, updates happen when the tuf metadata changes. Not when on\\na binary mismatch. This means that if the local `launcher` executable\\nchanges, it will not be refreshed until the tuf metadata\\nchanges. This makes testing somewhat harder.\\n","completion":"## Decision\nInstead of replacing the running binary, we will create a directory to\\nstore updates in. These updates will be launched from there. This new\\nflow will look like:\\n1. a new binary is downloaded into the staging area\\n2. It is moved into the updates directory, and stored by date\\n3. It is spawned from the updates directory\\nThe implementation is documented as part of [\/pkg\/autoupdate], or\\n[godoc](https:\/\/godoc.org\/github.com\/kolide\/launcher\/pkg\/autoupdate)\\n"}
{"prompt":"## Context\\nWe applied the CQRS style (see [ADR #7](0007-use-cqrs-architectural-style.md)), now we need to decide how to handle writing operations (Commands).\\n","completion":"## Decision\nWe will use **Clean Architecture** to handle commands with 4 layers: **API layer**, **Application Service layer**, **Infrastructure layer** and **Domain layer**. <\/br>\\nWe need to add Domain layer because domain logic will be complex and we want to isolate this logic from other stuff like infrastructure or API. Isolation of domain logic supports testing, maintainability and readability.\\n"}
{"prompt":"## Context\\nWhen machines connect to a Puppetmaster, they require a signed certificate to be\\npresent to allow interaction.\\nIf we lose a Puppetmaster (or it is recreated by an Auto Scaling Group) then we\\nwould have to re-sign all the certificates to allow all machines in the\\nenvironment to run Puppet again (ref: https:\/\/docs.puppet.com\/puppet\/3.8\/ssl_regenerate_certificates.html).\\nThis would involve manual intervention to allow machines to connect again\\nand we would be unable to deploy new Puppet code during this period.\\nIdeally we should back up our signed certificates, and a new machine would\\nbe able to restore them when it is recreated.\\n","completion":"## Decision\nWe will defer this until a later date while we're testing this with Integration\\nto concentrate on getting the stack up and running and accept the loss of certicates.\\nAt a later date we will re-visit this approach and implement a suitable solution.\\n"}
{"prompt":"## Context\\n\u5f53\u524d\u6211\u4eec\u7528\u7684\u662f SVN \u505a\u4ee3\u7801\u3001\u4ea7\u54c1\u6587\u6863\u3001UI \u8bbe\u8ba1\u56fe\u7684\u7ba1\u7406\uff0c\u5728\u6b64\u8bf4\u8bf4\u5176\u4e2d\u7684\u4ee3\u7801\u7ba1\u7406\\n1. \u4ee3\u7801\u4ed3\u5e93\u8fc7\u5927\uff0c\u65b0\u5206\u652f\uff0c\u65b0Tag \u4ee3\u7801\u90fd\u662f\u4e00\u4efd\u5b8c\u6574\u7684\u62f7\u8d1d;\\n2. \u5fc5\u987b\u8054\u7f51\u63d0\u4ea4\uff1b\\n3. SVN \u4ee3\u7801\u5408\u5e76\u65b9\u5f0f\u4f4e\u6548\uff0c\u76ee\u524d\u4f7f\u7528 Beyond Compare \u505a\u4ee3\u7801\u5408\u5e76\uff0c\u5206\u652f\u4f7f\u7528\u65b9\u5f0f\u843d\u540e\uff1b\\n4. \u65e0\u6cd5\u5f88\u597d\u7684\u505a code review\uff08\u53ea\u80fd patch \u6216\u7b2c\u4e09\u65b9\u5de5\u5177\uff09\uff1b\\n5. \u9762\u8bd5\u8005\u770b\u5230\u662f\u8fd9\u4e48\u843d\u540e\uff0c\u4ee5\u6b64\u7c7b\u522b\u5176\u4ed6\u6280\u672f\u6808\uff0c\u7efc\u5408\u7406\u89e3\u5c31\u662f\uff0c\u6211\u80fd\u5b66\u5230\u5565\u3002\\n","completion":"## Decision\n\u4f7f\u7528\u5168\u7403\u6700\u6d41\u884c\u7684\u5206\u5e03\u5f0f\u7ba1\u7406\u5de5\u5177 Git \u53ca\u5e73\u53f0 Github\uff0c\u5176\u7279\u70b9\u4e3a\u5206\u5e03\u5f0f\uff0c\u76ee\u5f55\u7ed3\u6784\u7b80\u5355\uff0c\u4ee3\u7801\u65e0\u5197\u4f59\uff0c\u53ef review with PR\u3002\\n### \u65b9\u5f0f\u4e00\uff1a\\ngit svn clone `svn project url`\u00a0-T trunk\\n\u5c06 SVN \u9879\u76ee\u7684 trunk \u8f6c\u4e3a git \u9879\u76ee\u7684 master\uff0c\u4ec5\u4fdd\u7559\u4e86 trunk \u5206\u652f\u7684\u63d0\u4ea4\u8bb0\u5f55\uff0c\u6b64\u65b9\u5f0f\u9002\u7528\u4e8e\u6240\u6709\u4ee3\u7801\u90fd\u89c4\u6574\u5230\u4e86\u4e00\u4e2a\u5206\u652f\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u5176\u4ed6\u5206\u652f\u7684\u63d0\u4ea4\u8bb0\u5f55\u3002\\n### \u65b9\u5f0f\u4e8c\uff1a\\n\u4f7f\u7528\u547d\u4ee4 `https:\/\/github.com\/nirvdrum\/svn2git`\uff0c\u5b83\u53ef\u4ee5\u4fdd\u7559\u6240\u6709branch, tags\uff0c\u4ee5\u53ca\u6240\u6709\u5206\u652f\u7684\u63d0\u4ea4\u5386\u53f2\u3002\\nsvn2git http:\/\/svn.example.com\/path\/to\/repo --trunk trunk --tags tag --branches branch\\ngit push --all origin\\ngit push --tags\\nuse `--revision number` to reduce the commit history.\\n\u76ee\u524d\u751f\u4ea7\u73af\u5883\u4f7f\u7528\u7684 centos \u7248\u672c\u8fc7\u4f4e\uff0c\u5bfc\u81f4 git \u4e5f\u65e0\u6cd5\u5347\u7ea7\u7684\u5904\u7406\u65b9\u6cd5\uff1a\\nyum install http:\/\/opensource.wandisco.com\/centos\/6\/git\/x86\\_64\/wandisco-git-release-6-1.noarch.rpm\\nyum update git\\n"}
{"prompt":"## Context\\nWe want to build a `\"Hello World\"` application using one of the\\nfollowing approaches:\\n* **Containers** (i.e. K8S, ECS, EC2, etc.)\\n* **Serverless** (i.e. AWS Lambda)\\n","completion":"## Decision\nWe will use AWS Lambda for this particular assignment.\\n"}
{"prompt":"## Context\\nIoP had a Mercury\/Connect SDK before this, but it was implemented in Java\/C# and it was not maintainable anymore. We had to decide if we want to maintain somehow or\\nrewrite it from scratch.\\n","completion":"## Decision\nThe Rust language was chosen to be used based on it\u2019s almost C level speed and rusts memory safety.\\nThe language also possesses really good bindings. Basically you can bind any code written in C into Rust.\\nWhile Rust is still in its early years, it\u2019s growing steadily, and it also has a good, stable, and growing community.\\n"}
{"prompt":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n","completion":"## Decision\nImplement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n"}
{"prompt":"## Context\\nHaving a repeatable, predictable way to create and change infrastructure is essential to stability and reliability of applications. One leading candidate to allow writing infrastructure as code is [Terraform](https:\/\/www.terraform.io).\\nOther tools to consider might be Ansible, Puppet or Chef, but they are less suited to modern cloud infrastructure than Terraform as they were developed when running code on VMs was the norm. They are good options for Configuration Management, but less appropriate for managing the infrastructure itself.\\nAmazon CloudFormation is a closer a possibility, but it is only usable on the Amazon stack whereas Terraform can be used to manage any Cloud. This flexibility will allow us to manage infrastructure which span clouds which this project may require (such as backend processing on AWS and frontend APIs on Heroku).\\nBoth CloudFormation and Terraform are good choices for Infrastructure Orchestration, but the AWS only restriction of CloudFormation makes it much less compelling to adopt.\\nSee [3. Follow Twelve Factor methodology](0003-follow-twelve-factor-methodology.md)\\n","completion":"## Decision\nWe will use Terraform to configure our Infrastructure.\\n"}
{"prompt":"## Context\\nSome AIS data sources include NMEA sentences that are not correctly formed. We sometimes see truncated messages, for example. This causes problems when a corrupt message was a fragment. At some point we need to let go of the fragment because its fellow fragments are never going to arrive.\\n","completion":"## Decision\nWe default to abandoning fragmented messages if 8 other messages have arrived since the first fragment. This is configurable, but we chose 8 because there's a limit of 9 fragmented messages in progress at any one time if you use the AIVDM-level group identifiers. It is configurable because systems that provide tag block group identifiers often use larger group IDs (e.g., 4 digits are common), making it possible for more to be in flight at once. But in practice, most message fragments are adjacent, so a large window is usaully unnecessary.\\n"}
{"prompt":"## Context\\nHandles are persistent links that currently point to Dome pages.\\nThe Handles server is embedded in Dome.\\nAn example in [Dome](https:\/\/dome.mit.edu\/handle\/1721.3\/82731).\\nAs handles point to a web page, and as DOS is not meant to expose a web interface for public consumption,\\nthe handles emitted by Dome can continue to point to Dome and the Handle server can remain embedded in Dome,\\nuntil such time when an application other than Dome is in production.\\n","completion":"## Decision\nHandles will not be minted by DOS and the Handle server will remain in Dome.\\n"}
{"prompt":"## Context\\nWhen installing manifests or Helm charts you need to pick a namespace.\\nOften the choice is made in a drive by fashion, leading to inconsistent configuration.\\n","completion":"## Decision\nWe want to install all the shipped components into namespaces grouped by functionality.\\nFor example `external-dns` goes into the namespace `dns`.\\n`fluent-bit` goes into the namespace `logging`.\\n"}
{"prompt":"## Context\\nA common problem developers have experienced with GOV.UK Docker is that the\\n[asset pipeline][] compilation steps can be slow, to the point that it was once\\ncommon that users [were served 504 responses][504-fix] on initial attempts to\\nstart an application. This slowness in GOV.UK Docker appeared to be an\\namplification of [existing performance][govuk-frontend-performance] issues,\\nhowever when GOV.UK apps began upgrading from the end-of-life Ruby Sass to\\nSassc we also began to see problems that pointed to a performance issue\\nwithin GOV.UK Docker.\\nSassc is known to [significantly improve the compilation time][sassc-speed] of\\nSass compared to Ruby Sass. When we updated [Content Publisher][] to Sassc we\\nfound that the [application.scss][] file compiled in 20% of the time (~20s to\\n4s) when running natively on macOS. However when this was run on GOV.UK Docker\\nthe performance was significantly less impressive, compiling in 62% of the\\ntime (~31s to ~19s).\\nWe discovered that the source of this performance issue was the\\n[performance overhead of reading][docker-mac-read] from the\\n[shared mount][govuk-mount] of application files on macOS. In particular we\\nfound that there were directories, `tmp` and `node_modules`, which could be\\nfrequently accessed during Sass compilation, but did not contain\\nfiles that a developer would need to edit during development. Bearing this in\\nmind, we considered a number of options as to how we could improve the\\nperformance:\\n1. Use [Docker volumes][volume] to store non-application code (such as `tmp`\\nand `node_modules`) to benefit from faster I\/O access.\\n2. [Configure GOV.UK Docker to use NFS][nfs-docker] (an alternative driver for\\nshared mounts) for improved I\/O access to shared mounts.\\n3. Use [docker-sync.io](http:\/\/docker-sync.io\/) (a file-syncing daemon) as a\\nmeans for improved I\/O access to shared mounts.\\n[asset pipeline]: https:\/\/guides.rubyonrails.org\/asset_pipeline.html\\n[504-fix]: https:\/\/github.com\/alphagov\/govuk-docker\/pull\/310\\n[govuk-frontend-performance]: https:\/\/github.com\/alphagov\/govuk-frontend\/issues\/1671\\n[sassc-speed]: https:\/\/www.solitr.com\/blog\/2014\/01\/css-preprocessor-benchmark\/\\n[Content Publisher]: https:\/\/github.com\/alphagov\/content-publisher\\n[application.scss]: https:\/\/github.com\/alphagov\/content-publisher\/blob\/2f91d43bcd1eb1f2d30f9d1ff9e556cd4d0de2a6\/app\/assets\/stylesheets\/application.scss\\n[docker-mac-read]: https:\/\/github.com\/docker\/for-mac\/issues\/77\\n[govuk-mount]: https:\/\/github.com\/alphagov\/govuk-docker\/blob\/cb124a3a1d3353e777d4f777d77f03f93415d415\/projects\/content-publisher\/docker-compose.yml#L11\\n[volume]: https:\/\/docs.docker.com\/storage\/volumes\/\\n[nfs-docker]: https:\/\/www.jeffgeerling.com\/blog\/2020\/revisiting-docker-macs-performance-nfs-volumes\\n","completion":"## Decision\nWe decided to mount project-specific Docker volumes on top of the existing\\n`~\/govuk` shared mount, in order to store non-application code (caches or\\ndependencies).\\nWhen trying this on Content Publisher, with the `tmp` directory and\\n`node_modules` directory as Docker volumes, we saw a significant improvement\\nin Sass compilation time on macOS:\\n| Environment                                           | Ruby Sass | Sassc | Decrease |\\n|-------------------------------------------------------|-----------|-------|----------|\\n| Native macOS                                          | ~20s      | ~4s   | 80%      |\\n| GOV.UK Docker (with shared `~\/govuk` mount)           | ~31s      | ~19s  | 38%      |\\n| GOV.UK Docker (with `tmp` and `node_modules` volumes) | ~16s      | ~4s   | 75%      |\\nWe saw less impressive improvements by switching the shared mount to use NFS.\\nThis performed approximately 3 times slower than the Docker volume approach.\\nThis approach was also not desirable as it required configuration on the host\\nmachine to create the NFS directory which would complicate installation and\\nusage of GOV.UK Docker.\\nWe decided not to use docker-sync.io as that required additional dependencies\\nand a large amount of additional configuration. We felt that, given Docker\\nvolumes provided us with near native performance, it would not be worth\\npursuing an unconventional, configuration heavy, approach.\\n"}
{"prompt":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need to have a way to describe the `Task` to be executed and serialize it in order to be able to store it in the `Created` event. Which will be persisted in the Event Store, and will be send in the event bus.\\nAt this point in time a `Task` can contain any arbitrary code. It's not an element of a finite set of actions.\\n","completion":"## Decision\n* Create a `Factory` for one `Task`\\n* Inject a `Factory` `Registry` via a Guice Module\\n* The `Task` `Serialization` will be done in JSON, We will get inspired by `EventSerializer`\\n* Every `Task`s should have a specific integration test demonstrating that serialization works\\n* Each `Task` is responsible of eventually dealing with the different versions of the serialized information\\n"}
{"prompt":"Amends [15. Phoenix Context Organization](0015-phoenix-context-organization.md)\\n## Context\\nThe code organization we agreed upon in [15. Phoenix Context Organization](0015-phoenix-context-organization.md), has become a bit unweildy in practice - specifically the nested schema module names and confusion around function placement within the hierarchy.\\n","completion":"## Decision\nPut schemas inside a `schemas` directory at the top level of the context and place all schemas inside. Flatten other files at the top level alongside the schemas.\\n"}
{"prompt":"## Context\\n### Requirements\\nAs building blocks for the services we're about to develop, we're going to need the following PaaS components:\\n*   A structured data store with automated distribution of data across multiple shards and regions and multi-master capabilities.\\n*   A message bus that supports partitioning, transactions and non destructive reads (peek).\\n*   A framework for building _serverless_ applications and APIs.\\n*   A service for managing push notifications via mobile (both iOS and Android), and browsers.\\n*   A service for delivering emails and text messages.\\n*   Facilities for application log and telemetry aggregation, analysis and monitoring.\\nAll the above components should be fully managed and should\\nprovide elastic and horizontal scalability, latency guarantees,\\n99.99% or greater availability, and a competitive TCO.\\nMoreover, the cloud provider should guarantee that the data\\nstored, transferred to and from the cloud services will never\\nleave the borders of the European Union.\\n### Market analysis\\nHere's a comparison of the players in the public cloud market against the above criteria:\\n| | Amazon Web Services | Google Cloud Platform | Microsoft Azure |\\n|------------------------|-----|-----|-----|\\n| EU data guarantees     | Yes | No  | Yes |\\n| Geo replicated db      | No  | [Spanner](https:\/\/cloud.google.com\/spanner\/) | [CosmosDB](https:\/\/docs.microsoft.com\/en-us\/azure\/cosmos-db\/)  |\\n| Messages \/ events      | [Kinesis](https:\/\/aws.amazon.com\/kinesis\/) \/ [SQS](https:\/\/aws.amazon.com\/sqs\/) | [Pub\/Sub](https:\/\/cloud.google.com\/pubsub\/) | [Queue Storage](https:\/\/azure.microsoft.com\/en-us\/services\/storage\/queues\/) \/ [Service Bus](https:\/\/azure.microsoft.com\/en-us\/services\/service-bus\/) \/ [Event Hubs](https:\/\/azure.microsoft.com\/en-us\/services\/event-hubs\/) |\\n| Serverless framework   | [Lambda](https:\/\/aws.amazon.com\/lambda\/) | [Functions](https:\/\/cloud.google.com\/functions\/) | [Functions](https:\/\/azure.microsoft.com\/en-us\/services\/functions\/) |\\n| Push notifications     | [SNS](https:\/\/aws.amazon.com\/sns\/) | [Firebase Cloud Messaging](https:\/\/firebase.google.com\/docs\/cloud-messaging\/) | [Notification Hubs](https:\/\/azure.microsoft.com\/it-it\/services\/notification-hubs\/) |\\n| Email and SMS delivery | [SNS](https:\/\/aws.amazon.com\/sns\/) | Sendgrid \/ Twilio | Sendgrid \/ Twilio |\\n| Logging and telemetry  | [CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/) | [StackDriver](https:\/\/cloud.google.com\/monitoring\/) | [Application Insights](https:\/\/azure.microsoft.com\/en-us\/services\/application-insights\/) \/ [Log Analytics](https:\/\/azure.microsoft.com\/en-us\/services\/log-analytics\/) |\\nAll the major players stack equally in terms of the richness and\\ncost effectiveness of the cloud offering and in terms of\\nelasticity, scalability and resiliency of the cloud platform.\\nAlmost all the required PaaS services get offered by the major\\nplayers. A notably exception is the geographically distributed\\ndatabase with multi-master capabilities, missing in the AWS\\noffering and recently launched by Google and Microsoft.\\nFinally, all major players guarantee the storage of user data\\nin EU regions but only Google cannot guarantee that the data\\nnever crosses the EU border while getting transferred\\nthrough its global private network.\\n","completion":"## Decision\nWe decide to select Microsoft Azure as our primary cloud provider.\\n"}
{"prompt":"## Context\\nDatasets are made up of observations and dimensions. Each of the options for a\\ndimension have wider definitions and metadata which apply to that option in all\\nscenarios, across multiple datasets. This dimension metadata is stored in APIs.\\nAPIs which hold secure or market sensitive data must meet different NFRs with\\nregards to security and publishing. It is important to understand what data is\\nsubject to these NFRs or not.\\nIt was suggested that hierarchies and code lists may not be considered sensitive,\\nand so the APIs which control access to this data would not be subject to the\\nsame NFRs.\\n","completion":"## Decision\nHierarchies and code lists do not need to be published, and do not need a\\npreview option.\\n"}
{"prompt":"## Context\\nIn general, the application is only interested in data related to previous\\nsteps in the current job, or data from the last successful job that was run.\\nThe amount of data needed by a current job from previous runs is very limited.\\nTo aid in troubleshooting, it is desirable that communications with the servers\\nbe recorded, as well as the results of calculation steps performed by the\\napplication.\\nWhile data from previous runs may have diagnostic value, there is little need\\nto store the data about particular for from long periods of time.\\n","completion":"## Decision\nApplications that need to store data between invocations typically persists\\nthe application state in a database.\\nThe use of files for persisting data was motivated by the desire to provide\\nvisibility into the \"steps\" the application took for a particular job, as well\\nas provide access to the raw data received from and sent to the servers.\\nThe use of files enables easy access to information about a job via tools such\\nas \"vi\" or \"cat\" that are typically installed on a server. Using the job id\\nas part of the filenames for files associated with the job make it relatively\\nstraightforward to the relevant files.\\nThe use of files allows data to be easily cleaned up simply by deleting files\\nin the storage directory older than a particular date.\\nWhile a database could provide similar functionality, it would likely result in\\nadditional development complexity, as well as make it more difficult to\\naccess and purge data. It would also require additional resources to run the\\ndatabase in production. Since very little information is shared between jobs,\\nand preservation of the information of the jobs over long periods of time is\\nnot necessary, the additional complexity a database would bring does not seem\\nworthwhile.\\n"}
{"prompt":"## Context\\nFollowing user research on the prototype of the candidate-facing\\napplication form, the design team decided to offer a Magic Link\\ninstead of a traditional username\/password account creation flow.\\nhttps:\/\/docs.google.com\/presentation\/d\/1_pYQl4oX0-7boqy5D6IqhkoI3QvZjlYrf1qfLZkHfn0\/edit#slide=id.g5f07c7bcc5_1_0\\nThe technical team carried out two spikes around this functionality\\nand discussed their findings with other teams.\\nThe spikes covered\\n- a gem called `passwordless` which offers the Magic Link feature out\\nof the box\\n- customising the popular authentication library `devise` to accept a\\nMagic Link instead of a username and password combination\\n`passwordless` offered a turnkey solution which was well-matched to\\nthe problem. It is a small single-maintainer project without wide\\nadoption: this is harder to feel confident about.\\n`devise` is a very popular battle-tested library, which gave us some\\nconfidence that its authentication system would be sound . However,\\nthe implementation of Magic Links on top of that required some\\nknowledge of how this slightly esoteric gem works.\\nhttp:\/\/blog.plataformatec.com.br\/2019\/01\/custom-authentication-methods-with-devise\/\\nFollowing a meeting with peers the team also considered a DIY approach\\nfollowing the example of the School Experience Prototype team.\\nhttps:\/\/github.com\/DFE-Digital\/schools-experience\/commit\/4d008da0e0edf4a9e47fe6d66eebda827a45f46a\\nThis approach offered complete control over the solution, and gave\\nthat team the flexibility to integrate closely with the DfE Gitis CRM\\nservice which is a requirement for their system. On the other hand,\\nit took time and effort to build and will need to be maintained by\\nthat team.\\nWe analysed our options using a table where each option had a column,\\nand there were three rows: benefits, costs and mitigations.\\nThe conclusion:\\n`passwordless` is good in the short term, but it is a relatively risky\\ndependency to take on, and it will not easily adapt to changing\\nrequirements. We could mitigate this by carefully wrapping it, but\\nwrapping an authentication system is tricky as it touches many parts\\nof the application.\\n`devise` brings a little bit of unwelcome voodoo to our codebase but\\noffers a quick way to deliver the feature with some confidence. We\\ncould mitigate the complexity via documentation, and by helping the\\nteam learn about `devise`. Because the library is so popular we do not\\nconsider learning about it to be a waste of effort, and we're less\\nconcerned than we might otherwise be about adding library-specific\\ncode.\\nThe DIY approach is very appealing, but we do not have time to do it\\nand look after it.\\n","completion":"## Decision\nUse Devise, and brush up the spike branch to implement Magic Links for\\nproduction.\\n"}
{"prompt":"## :dart: Context\\nThe application can be hosted on a private datacenter or on a managed cloud datacenter.\\nChoosing the right hosting model will impact the scalability cost and technology choices for the application.\\n","completion":"## Decision\nThe application will be hosted on Microsoft Azure Cloud datacenter.\\n"}
{"prompt":"# Context\\nAt Akvo we run multiple products on multiple platforms. To be efficient we want\\nto consolidate future application development. We also want the product teams\\nthemselves to own the deployment themselves. There is also few apps \/ services\\nwhich are not products, but helper services. At Akvo we have Open source in our\\nblood, and it's a requirement that tools we use is open source. We want to store\\ncustomers data in Europe, with the possibility to host in specific countries due\\nto regional data laws. We want to go for a solution that gives us the most\\nleverage without being a lock in. There should be an easy migration path from one provider to another. Since we have a limited ops team and multiple\\nproduct teams minimising the dependancy on ops in day to day work is highly\\nregarded.\\nHistorically Akvo have used both apps hosted on VPS using Puppet to application running on Google App Engine. There have been drawback on each of these. We hope the use of containers and a container management tool will be a middle ground which allow future decisions.\\nWe started out using Docker compose on Amazon AWS, and had our eyes on the Amazon container service. After further investigtion Kubernetes started to look very appealing. Main reasons was that Kubernetes is open source. It also have a lot of handy features such as built in secrets management, an appealing network\\nmodel, the notion of deployments. In general Kubernetes is a very active project which we also have some team knowledge about.\\nSince has a hosted Kubernetes solution in Google Container Engine (GKE) and the fact that Akvo runs the Akvo Flow product on Google App Enigne. Google Cloud Platform was the most suitable pick. The Google cloud also provides really good tools for encrypting data. The major drawback compared to AWS is the lack of hosted Postgres. There is however third party solution and hope that Google will start to support Postgres in the future. Also we rather run a Postgres cluster ourselves than a Kubernetes cluster hence GCP seems attractive.\\n# Decision\\n- We will use containers as the base unit for shipping our applications.\\n- We will use Kubernetes to manage containerised applications.\\n- We will use the Google Cloud Platform and their Container Engine (hosted Kubernetes).\\n# Status\\nAccepted\\n# Consequences\\n- Kubernetes deployment files needs to be written CI needs to configured to deploy against Google Container Engine\\n- We need to adopt the notion of cattle instead of pets\\n- New routines between the Ops team and product teams need to be built\\n*This  ADR summarises a longer and more detailed working document*\\n","completion":"## Decision\n- We will use containers as the base unit for shipping our applications.\\n- We will use Kubernetes to manage containerised applications.\\n- We will use the Google Cloud Platform and their Container Engine (hosted Kubernetes).\\n# Status\\nAccepted\\n# Consequences\\n- Kubernetes deployment files needs to be written CI needs to configured to deploy against Google Container Engine\\n- We need to adopt the notion of cattle instead of pets\\n- New routines between the Ops team and product teams need to be built\\n*This  ADR summarises a longer and more detailed working document*\\n"}
{"prompt":"## Context\\nThe requested functionality was to synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event, which would have limited information available to construct the request.\\nOf the available HTTP methods, there isn't a perfect fit for this use case.\\nPUT & PATCH were generally recommended for similar scenarios.\\n","completion":"## Decision\nThe HTTP PATCH method will be used.\\n"}
{"prompt":"## Context\\nSince we do not have the expertise and experience needed to build ui library projects and since we cannot fully anticipate the future needs of such library it's best to follow the path of other libraries and implement it in a similar way.\\n","completion":"## Decision\n- We will copy the (api) design of Angular Material - Angular Material is created and maintained by the Angular team and is a very popular library. It is well tested and very well designed and most importantly has been used by many other teams already. It makes sense to copy their api design and features.\\n- We will try to stay as close as possible to the same design approach chosen by Angular Material for each component.\\n- We will only copy features that we need, we will not implement features we do not need like the ripple effects or support for bidirectionality.\\n- We will not copy paste the code as is without understanding it, we will first study it carefully and then borrow what we need from it.\\n- We will use the CDK as much as possible.\\n"}
{"prompt":"## Context\\nA Vellum node fails whilst a request from Sprout or Dime is outstanding. this will typically result in failure of the request that the Sprout \/ Dime node was processing (with a return code indicating that it should be retried)\\n","completion":"## Decision\nSprout \/ Dime node sends request which is retried an alternative, Vellum node will be used instead\\n"}
{"prompt":"## Context and Problem Statement\\nWe have quite a few components to build up in the backend to get DPE to a stable point. Although initially decided to do development in AWS, working on AWS had been quite challenging with a steep learning curve. In a single project cycle, we've reached a point where we had infrastructures of most microservices setup. Unfortunately resources were removed from the project and priorities shifted to UX research, while back-end development halted. Picking these development tasks from half baked microservices is difficult. We now have to relearn what we've developed and complete integration. In order to simplify back-end development, we are looking at Firebase (Google Cloud Platform). Since Firebase is a plaftorm that targets development for startups and R&D, it provides many features out of the box. This could speed up developing the backend, namely the realtime database, authentication, while keeping possibilities open for offline, mobile, and multi-user solutions. We must consider technical (security, development, transfer, upfront learning) and non-technical (legal) cost.\\nSince we've designed the React components to be reusable, there is a question of whether AWS vs GCP makes it easier for transfer. Previous projects had all been rebuilt from scratch, including the infrastructure. Realistically, does it matter which Cloud provider we go with?\\n![Firebase architectural diagram](https:\/\/github.com\/bbc\/digital-paper-edit-client\/blob\/firebase\/docs\/img\/firebase-arch.png)\\n### Description of Microservices\\n| Name                     | Technology                           | Description                                                                                                                                              |\\n| ------------------------ | ------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| Firebase                 | GCP Firebase hosting, main component | The hosted solution                                                                                                                                      |\\n| Firestore                | GCP Firestore, database              | The database with additional capabilities (realtime)                                                                                                     |\\n| Audio Converter Function | GCP Function, serverless             | The serverless service that strips audio from the original AV content                                                                                    |\\n| AWS Uplaoder Function    | GCP Function, serverless, KMS        | The serverless service that uploads a file to S3                                                                                                         |\\n| STT Client               | AWS EC2, PSTT                        | SQS Consumer that consumes messages to send to Platform STT (third party) and forwards the status updating messages from the SNS topics from PSTT        |\\n| STT Client Queue         | AWS SQS                              | A SQS service that takes new Job messages to send to PSTT Client                                                                                         |\\n| Notification Function    | Functions, serverless                | A serverless service that takes Notification messages off the Pub\/Subs serice to update Firestore                                                        |\\n| STT Topic                | AWS STS                              | A topic for anything to subscribe to for notifications. This is to help with collecting all the notification. The messages are created by the STT Client |\\n- Audio Converter Function is listening for Storage changes\\n- AWS Proxy Function is listening for Storage changes\\n- STT Client is listening for messages (job creation) from a queue\\n### Steps\\n1. User uploads AV file to Storage from Firebase\\n2. Audio Converter Function:\\n1. detects file upload\\n2. strips audio from file\\n3. upload file to Storage\\n3. AWS Proxy Function:\\n1. detects file upload\\n2. retrieves Key from GCP KMS\\n3. assumes role as AWS IAM\\n4. uploads file to S3\\n4. [S3 Event triggers to be sent to Queue](https:\/\/docs.aws.amazon.com\/AmazonS3\/latest\/user-guide\/enable-event-notifications.html)\\n5. STT Client:\\n1. creates new Plaform STT job:\\n4. upload file to Platform STT's S3 bucket\\n5. send message with file loaded to Platform STT\\n2. Forwards notifications from Platform STT's SNS to STT Notifications SNS\\n6. [AWS notification Subscription Function](https:\/\/cloud.google.com\/community\/tutorials\/cloud-functions-sns-pubsub)\\n1. updates Firestore\\n7. Firestore listener updates Firebase\\n![AWS architectural diagram](https:\/\/raw.githubusercontent.com\/bbc\/digital-paper-edit-infrastructure\/master\/docs\/adr\/newest_arch.png)\\n![Firebase - what's done?](https:\/\/github.com\/bbc\/digital-paper-edit-client\/blob\/firebase\/docs\/img\/dpe-aws-not-available.png)\\n## Decision Drivers\\n- Security (Authentication, PSTT integration)\\n- Legal\\n- Transfer\\n- Technical feasibility of DPE only\\n- Technical feasibility of CI, reproduceability\\n- Cost (time, money, engineering, opportunity)\\n","completion":"## Decision\n- Security (Authentication, PSTT integration)\\n- Legal\\n- Transfer\\n- Technical feasibility of DPE only\\n- Technical feasibility of CI, reproduceability\\n- Cost (time, money, engineering, opportunity)\\nFirebase and AWS combined as PSTT integration is feasible to do, even with cross acount complications. Features such as authentication, user specific data retrieval, integration with database without migrations is already completed. Firebase provides abstractions around security and integrations with the database, as well as Functions. To save time around the project with features such as realtime database and easy API use, let's progress with Firebase. There are still some things such as CI, Deployment management that can be worked on, but this is a stretch goal for now. All in all, I believe that it will save time, and simply the overall architecture and code.\\n### Positive Consequences\\n- Faster turnaround for development\\n- Authentication\\n- Realtime database beneficial for some of our DPE requirements\\n- Managed services with many abstractions\\n#### Secondary positive consequences\\n- Growing knowledge of GCP\\n- BBC Login (stretch goal)\\n- monitoring given out of the box (console.log maps to logging to GCP Logging Console)\\n### Negative consequences\\n- Upfront cost of learning for developers and there are many unknowns, until looked into\\n- Security concerns\\n- Redesigning architecture\\n- Redesigning database model\\n- Transfer not possible with infrastructure (?)\\n"}
{"prompt":"## Context\\nAs described in [ADR 0004, `Corvus.Tenancy` will not create storage containers automatically](.\/0003-no-automatic-storage-container-creation.md), `Corvus.Tenancy` v3 introduces a change: applications are now responsible for creating all necessary containers when onboarding a client. This creates a challenge for applications that have already been deployed on v2, because the following things may be true:\\n* a tenant may exist in which only a subset of its storage containers exist\\n* in a no-downtime migration, a compute farm may have a mixture of v2 and v3 components in use\\nTo enable applications currently using `Corvus.Tenancy` v2 to migrate to v3 without disruption, we need a clearly defined path of how a system will be upgraded.\\n","completion":"## Decision\nUpgrades from v2 to v3 use a multi-phase approach, in which any single compute node in the application goes through these steps:\\n1. using nothing but v2\\n1. using v3 libraries mostly (see below) in v2 mode\\n1. using v3 libraries, onboarding new clients in v3 style, using v3 config where available, falling back to v2 config and auto-creation of containers when v3 config not available\\n1. using v3 libraries in non-transitional mode\\nWhile in phase 3, we would run a tool to transition all v2 configuration to v3. Once this tool has completed its work, we are then free to move into phase 4. (There's no particular hurry to move into this final phase. Once all tenants that had v2 configuration have been migrated to v3, there's no behavioural difference between phases 3 and 4. The main motivation for moving to phase 4 is that it enables applications to remove transitional code once transition is complete. Phase 4 might not occur until years after the other phases. For example, libraries such as [Marain](https:\/\/github.com\/marain-dotnet) that enable developers to host their own instances of a service might choose to retain transitional code for a very long time to give customers of these libraries time to complete their migration.)\\nTo support zero-downtime upgrades, it's necessary to support a state where all compute nodes using a particular store are in a mixture of two adjacent phases. E.g., when we move from 1 to 2, there will be a period of time in which some nodes are still in phase 1, and some are in phase 2. However, we will avoid ever being in three phases simultaneously. For example, we will wait until all compute nodes have completed their move to state 2 before moving any into state 3.\\nThe following sections describe the behaviour required in each of the v3 states to support transition. (There's nothing to document here for phase 1, because that's how systems already using v2 today behave.)\\n### Phase 2: using v3 libraries, operating in v2 mode\\nA node in this phase has upgraded to v3 libraries, but is using the transition support and is essentially operating in v2 mode. It will never create new v3 configuration. New tenants continue to be onboarded in the same way as with v2 libraries\u2014the application does not pre-create containers, and expects the tenancy library to create them on demand as required. This gives applications a low-impact way in which to upgrade to v3 libraries without changing any behaviour, and also opens the path to migration towards the new style of operation.\\nThe one difference in behaviour (the reason we describe this as \"mostly\" v2 mode above) is that if v3 configuration is present for a particular configuration key, it has the following effects:\\n* the application will use the v3 configuration and will not even look to see if v2 configuration is present\\n* the application will presume that all relevant containers for this configuration have already been created, and will not attempt to create anything on demand\\nThis is necessary to support the case where all nodes have completed their transition to phase 2 (so none is in phase 1), and some have have moved to phase 3. Nodes that are still in phase 2 at this point need to be able to cope with the possibility that some clients have been onboarded by a phase 3 node, and so there will be only v3 configuration available. (We do not expect both v2 and v3 configuration to be present for any particular container at this point, because migration of tenants onboarded the v2 way into v3 configuration does not start until all nodes have reached phase 3.)\\nTo configure a node to run in this mode, use storage through a suitable transitional interface (e.g., `IBlobContainerSourceWithTenantLegacyTransition`). The application must provide two configuration keys: one for v2 configuration and one for v3 configuration. The transitional adapter will never create v3 configuration, but it will look for it, and only looks for v2 configuration when no v3 configuration is present.\\n### Phase 3: v3 libraries, operating in v3 mode, falling back to v2 as necessary\\nA node in this phase is using the v3 libraries. When onboarding new tenants, it pre-creates all necessary containers, and stores v3 config, but it still uses the transition support so that in cases where existing tenants have only v2 configuration available, it can fall back to the old behaviour.\\nThe only difference between phase 2 and phase 3 is how the application onboards new tenants. Both phases use the transitional adapter in exactly the same way.\\n### Configuration migration\\nOnce all nodes are in phase 3, a tool can be run to upgrade all v2 configuration to v3. Some aspects of this tooling are necessarily application-specific: only the application can know how to discover all of its tenants, and only the application can know what configuration it is storing, and under which keys.\\n"}
{"prompt":"## Context\\nThe editions lambda needs to be able to identify specific versions of an issue.\\n","completion":"## Decision\nTo have two deployments of the backend, one for previewing, and a second for published issues.\\nThe published issues deployment will replace the issueid path parameter with source\/issueid.\\n`source` will identify which file in the published bucket will be retreived to form the issue on.\\n"}
{"prompt":"## Context and Problem Statement\\nWe currently use a fairly basic mechanism for doing transaction type operations against Redis. We may expand our use of Redis to replace some or all of our MySQL database for OAuth. The expansion will include use cases that require new indexing, join-like, and transactional operations. To support future use cases and make current ones more concise we should evaluate our implementation in `fxa-shared\/redis`.\\n## Decision Drivers\\n- Need for robust redis transactions\\n- Increased use of redis for oauth\\n- Updating module dependencies\\n- move from \"redis\" (last updated 2 years ago) to \"ioredis\" (actively maintained)\\n","completion":"## Decision\n- Need for robust redis transactions\\n- Increased use of redis for oauth\\n- Updating module dependencies\\n- move from \"redis\" (last updated 2 years ago) to \"ioredis\" (actively maintained)\\nUse lua scripts. The proof-of-concept [PR](https:\/\/github.com\/mozilla\/fxa\/pull\/3278) showed lua as viable and an overall simpler option.\\n### Positive Consequences\\n- Future use cases should be simpler to implement\\n- Less custom Redis glue code and simpler stack traces\\n- Improved maintainability\\n### Negative Consequences\\n- Additional developer cost of understanding lua\\n- Changes to lua scripts require additional consideration with regard to performance. Performance implications should be a checklist item on any lua script change PR, similar to SQL stored procedures.\\n"}
{"prompt":"## Context and Problem Statement\\nHow do we want to organise work in branches and how should changes be released? How should different branches be continuously deployed for QA?\\n## Decision Drivers\\n* We need to have confidence in our releases.\\n* We want more structured releases while we're still getting our footing in a shared monorepo.\\n* We need simplicity and clear [takt time] so different teams can plan for what is going out the door from them.\\n* It should work well with our agile work environment.\\n","completion":"## Decision\n* We need to have confidence in our releases.\\n* We want more structured releases while we're still getting our footing in a shared monorepo.\\n* We need simplicity and clear [takt time] so different teams can plan for what is going out the door from them.\\n* It should work well with our agile work environment.\\nChosen option: \"OneFlow\" because it provides a single eternal branch with well structured releases.\\nWe'll implement OneFlow with these details:\\n* Release branches are set up the Monday after each sprint. This is sometimes called release trains, where features line up for different release trains.\\n* Release and quality managers from each team are responsible for reviewing and approving releases.\\n* Releases apply to all apps in the monorepo.\\n* Releases are versioned like this: `{cycle}.{sprint}.{hotfix}`. So version 3.1.2 is the release after cycle 3, sprint 1 with two hot fixes applied.\\n* Feature branches are merged using \"Squash and merge\", so they can be easily reverted.\\n* There are two ways to build larger features.\\n* If the feature is isolated and not likely to cause conflicts, they can stay on long-living feature branches until they are ready to be released.\\n* If the feature touches many parts of the codebase, it can be useful to merge changes more often but hide the feature in production with feature flags.\\n* If a project needs to deploy updates outside of the sprint rhythm, they should use hotfix branches.\\n### Future strategy\\nWith time, we expect to build up better testing capabilities which gives us more confidence in the health of our monorepo. Then we can move quicker, with a simpler GitHub Flow branching strategy and continuous delivery into production.\\n### Hosting environments\\nWe'll set up continuous delivery to different hosting environments:\\nEnvironment | Git source            | Databases\/services | Features\\n------------|-----------------------|--------------------|----------\\nsandbox     | feature branch        | Test               | All\\ndev         | master                | Test               | All\\nstaging     | master                | Prod               | All\\npre-prod    | release\/hotfix branch | Prod               | Finished\\nprod        | latest release tag    | Prod               | Finished\\nWe'll probably start with dev, staging, pre-prod and prod environments, since feature branch deployments are more dynamic and difficult to manage.\\n"}
{"prompt":"## Context\\nWhat is our testing strategy in terms of tooling, coverage and test types.\\nhow to test\\n- unit tests\\n- testing libs \/ frameworks\\n- testing approaches\\n- what to test\\n- how to e2e test before merging \/ releasing\\n- architecture check\\n","completion":"## Decision\n- For every public groovy Class method there is at least one unit test\\n- We aim for 100% branch coverage\\n- we use JUnit 4\\n- we use AssertJ\\n- we use ArchUnit\\n"}
{"prompt":"## Context\\nAsync Api 2.0 spec details that message headers property must be a Schema Object in which his properties will be the headers for that message.\\nAMF currently is parsing that object as the schema of a parameter object. This is confusing and semantically incorrect, because that property represents a set of parameters, not just one\\n","completion":"## Decision\nCreate a new field to the Message model, similar to the query string property of Raml 1.0 operations, that will contains the Schema Object. Any user looking at the model, will be able to read that schema as a list of headers based on the field in which is contained:\\n####ApiContract:HeaderSchema\\nThe header example property of the message will be validated against that schema.\\n"}
{"prompt":"## Context\\nWhen multiple demo instances exist in parallel, we need to avoid to step on each others toes. The scope is\\n- the manuela-gitops repository, where component deployment and configuration takes place as part of a demo run\\n- the manuela-dev repository, where coding changes take place as part of a demo run\\nOptions:\\n1. **BRANCHES** inside the existing sa-mw-dach repos, for each manuela demo env there would be a branch (e.g. name based on a convention).\\nPros:\\n- Easiest to set up since it's literally just cloning the central repos.\\n- only need to adjust branch names for new demo instance (however the number of required adjustments is likely the same)\\nCons:\\n- need to coordinate the creation\/assignment of branches\\n- all demoers need write access to the repos\\n- Danger of cluttering the repo with \"abandoned\" branches\\n2. **FORKS**: The \"owner\" of the new installation forks all repos in GitHub or creates new repositories from scratch. The changing remote URL needs to be adjusted when setting up the demo. Preferably, there would be somewhere one central configuration (GIT_BASE_URL) defining from which fork to pull from.\\nPros:\\n- demo instances can be set up without any coordination\\n- No conflicts when branches are used for e.g. production staging approvals\\nCons:\\n- need to adjust\\n- pipeline configs (pointing to forks of manuela-dev\/manuela-gitops)\\n- argoCD applications (poingint to forks of manuela-gitops)\\n3. **DEDICATED GIT**: We could deploy a dedicated GIT(LAB?) to a namespace, place the manuela-gitops +  manuela-dev repos there. As long as we can use only cluster-internal URLs these will not change across instances.\\nPros:\\n- Consistent URLs at least in a single-cluster scenario, no need to adapt anything\\nCons:\\n- Will require similar adaptations as **FORKS** in a multi-cluster scenario, since we will need to use external URLs for cross-cluster communication\\n","completion":"## Decision\nUse **FORKS**: this requires least coordination, access rights or additional components.\\n"}
{"prompt":"## Context\\nWe have a CookieOverflow error that is prompting this work [4]. It occurs when the service is storing more than 4KB of data within the cookie. This includes information on the user, auth tokens and the content of any flash messages.\\nA current work-around is to delete your cookie and sign in again.\\nRails acknowledges [5] that the default cookie storage is fast but prone to this error.\\n","completion":"## Decision\nUse Redis [1] as a server-side session store.\\n"}
{"prompt":"## Context\\nWe have a number of frameworks being delivered in a series of separate but closely related projects:\\nInitial marketing activities will be strongly focused on particular buyer segments - for instance with the education sector.\\nWe will use a number of partners to help build these frameworks, but CCS will be the final owners and operators. The frameworks will be built at pace, but will need to share many elements of data, user experience, and logic in order to meet CCS strategy and be deliverable in time at an acceptable level of quality.\\n","completion":"## Decision\nAs defined below, in line with technology strategy, common CMP components will be built and\/or used:\\n- store Agreement data including service offering and supplier interests in a common API\\n- use common repository for agreement definitions as part of the build process\\n- use CCS ID for buyer registration and authentication\\n- use SRS for supplier registration and authentication\\n- use common API for Party (Buyer and Supplier) details\\n- synchronise data to and from SalesForce as needed\\n- all services will be built and integrated together\\n- use sector specific (initially education) services for needs that are common across agreements\\n- share common services wherever practicable\\n- use a common store for questions and answer tasks for framework buyer needs\\n- use the same languages and frameworks wherever practicable\\n![Overall architecture](..\/images\/CMP%20applications%20high%20level%20architecture.jpg)\\n### Commercial agreement specific services\\n#### Apprenticeships Buyer Service\\nCode and services to support Apprenticeship Commercial Agreement. This is a Dynamic Purchasing system agreement, so that suppliers can qualify in and out of the agreement over time. The structure of the service includes items relating to different apprentice training needs.\\n#### Consulting Buyer Service\\nServices for the new framework for buying consulting services, with a further competition workflow. The further competition design will be built based on the DOS framework. Award and call-off creation will be created as soon as possible. We don\u2019t yet know whether common services can be built to facilitate the competition phase, but code will almost certainly be shared with Legal Services, which is very similar.\\n#### Legal Services Buyer Service\\nServices for the new framework for buying legal services, very like Consulting services.\\n#### Supply Teachers Buyer Service\\nServices for the new framework for finding supply teachers, with location specific and skills based features.\\n#### Facilities Management Buyer Service\\nServices for the new framework for buying facilities management, with two lots: one for large procurements, and one for smaller procurements. Features include locality, a large number of service items, and agreement specific rules for when to require further competition. Initially we expect the competition rules to be coded in this service.\\n### Common components\\n#### CCS ID\\nA service for authenticating buyers.\\n#### SRS - Supplier Registration Service\\nA service for registering suppliers, which will need to be extended to support some of these frameworks, if only to support SAML2 authentication to the services for suppliers.\\n#### DPS - Dynamic Purchasing Services\\nA service supporting qualification of suppliers to a number of commercial agreements, which will need to be supplemented for, at least, Apprenticeships.\\n#### Sector (e.g Education) specific Registries & Services\\nCode and services that support sector specific buyers. For example, several services may wish to consult Dfe data sources to understand various things about schools.\\n#### common services\\nCode and services that are shared between teams and commercial agreement services\\n#### User Needs Q&A Tasks\\nA store for task data in commercial agreement journeys that may be shared. This allows us to define journeys where buyers don\u2019t have to repeat themselves.\\n#### Search Index\\nA logical search capability to locate commercial agreement elements, to be used in CMp phases when buyer directed matching starts to become important.\\n#### Agreements Records\\nData store and API for commercial agreements, including Frameworks, DPS, call-offs\\n#### Party Records\\nData store and API for parties (buyer and supplier organisations)\\n#### Documents (Attachments)\\nData store for documents linked in agreements and elsewhere\\n#### Agreements Definitions\\nA repository defining agreement types, used at build time by commercial agreement related services.\\n#### SalesForce\\nCCS CRM and general workflow environment, and public website.\\n"}
{"prompt":"## Context\\nThere no agreed-upon way of handling application configuration. There are two issues this ADR aims to address.\\n### Issue 1: Nested keys\\nIn `config.exs`, configuration keys can be nested by using a Keyword as value. Unfortunately, there is no built-in support for this nesting. For example:\\n```\\nApplication.fetch_env!(:rig, :required_key)\\n```\\nwill show a nice error, while\\n```\\nApplication.fetch_env!(:rig, :required_key)[:required_subkey]\\n```\\nwill simply return nil in case `:required_key` is present but `:required_subkey` is not.\\n### Issue 2: Default values\\nSome default values are defined where they are needed in the code, which is a problem once a key is used more than once. Also, defaults are no easily inspectable.\\n","completion":"## Decision\n* We prefer not to nest configuration keys; instead, we prefix them where it makes sense. For example, instead of `config :rig, :kafka, client_id: :rig_brod_client` we write `config :rig, kafka_client_id: :rig_brod_client`. This allows us to leverage the built-in methods (e.g., `Application.fetch_env!`), which produce sensible error messages in case required values are not defined.\\n* We always set default values in config.exs (in turn this means that we prefer `Application.fetch_env!` over `Application.get_env`). This way, it is easy to reason about default values, and using a default value in more than one place is not an issue.\\n"}
{"prompt":"## Context\\nWe have used Travis CI for Continuous Integration and deployment since the start\\nof this project (see [005](https:\/\/github.com\/UKGovernmentBEIS\/beis-report-official-development-assistance\/blob\/develop\/doc\/architecture\/decisions\/0005-use-travis-for-ci.md)), but due to changes in the business model of Travis, we\\nnow have to pay for builds to be run, and the service we do pay for is slow, and\\nholding up the pipeline of work we can deliver.\\nGithub Actions has been in operation for a good while now, and is a mature and\\nstable product. It also supports all the tools we currently use on Travis, so\\nis easy to migrate across. We can also cache our Docker containers, speeding up\\nbuild time considerably.\\n","completion":"## Decision\nMove Continuous Integration from Travis CI to Github Actions, and remove our repo\\nfrom Travis CI.\\n"}
{"prompt":"## Context\\nWith today's technologies and frameworks, validation of input data looks like a solved problem. You just add few annotations on DTOs (Data Transfer Objects), hook up the validation service with the\\napp, and move on.\\nHowever, there are much more under the cover. When digging into the details, various questions pop up. Where exactly is the best place for input data validation? Are there multiple levels or phases\\nof validation? What if we need a system state for validation? What happens when numerous inbound channels are in play?\\nAfter trying to answer these, it turns out that validation is more involved than what is commonly shown in basic examples. Solutions are not too complex but require establishing some principles and\\nrules.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","completion":"## Decision\n**We will execute syntactic validation at the application facade layer. If necessary for UI requirements, it may also be executed on the adapter layer.**\\nCore validation at the application facade layer ensures that input data from any inbound channel will be validated in the same way. An example can be seen in `BookingOfferCommandApplicationService`\\nclass from `cargotracking-booking-app-commandside` module.\\nValidation at the inbound channel adapter layer is used when there is a need for UI or client-specific validations. A typical example is an email verification scenario where, with repetition, we want\\nto assure that the user submitted the correct email.\\n**We will organize validation into syntactic and semantic phases. Each of them will be divided further into subphases if necessary.**\\n**We will execute semantic validation inside the domain layer. Depending on nature, this will happen either in the aggregate or in the application facade.**\\n### Syntactic validations\\nWe consider validation being syntactic if it does not require any system state. It validates input data fields in isolation and possibly some interdependencies between them (cross-field validation).\\nSyntactic \"stateless\" validations are executed first. If they fail, semantic validations won't be triggered at all.\\n### Semantic validations\\nOn the other hand, semantic validation requires access to the system state. It may deal with checking if, for the current aggregate state, the request is valid or not. It may also check some other\\nnon-aggregate conditions, such as the existence of necessary data in registries. We are reporting semantic validation failures as domain exceptions.\\nRegarding the order, it is advisable to group and execute all non-aggregate-related validation first (checking against registry data, for example). Such validations do not require transactional\\naggregate locking. They may use the transaction but must not have the aggregate in that transaction. Only if they pass, we should proceed with validations that require opening a transaction for the\\naggregate (for example, if input data are valid for the current aggregate state).\\n### Syntactic validation subphases\\nWe can divide syntactic validation into several internal phases. Those phases are ordered from the simplest to the more complex - existence, size, lexical content, and syntax format.\\nExistence validation ensures that provided data exist and are not empty. Further processing makes no sense when data is empty. In this subphase, we check for null objects, empty strings (zero-length\\nor whitespace only), empty collections, etc.\\nSize validation verifies if data are reasonably big. Before further phases, we are checking the length\/size of input data no matter od the data type. Size checks will prevent additional processing of\\ntoo big data, which might cause performance issues. Also, reporting about size failures might be helpful from the user perspective as it is a widespread mistake.\\nLexical content validation checks if data contain the correct characters and encoding. This phase might be helpful if we are receiving the data in complex formats like JSON, XML, or HTML. For simpler\\ndata inputs like size-limited strings, this phase is commonly executed as a part of the following stage.\\nSyntax format validation verifies if the format is correct. For strings, this is often achieved with regular expressions. When regex is too complicated, we might get better results with specialized\\nvalidator implementations.\\nWe are reporting syntactic validation failures through means of an employed validation framework. As we are using JSR 380 (Jakarta Bean Validation 2.0) implementation, syntactic validation failures\\nare reported through `jakarta.validation.ConstraintViolationException`. With JSR 380, we are implementing syntactic validation ordering with `GroupSequence` annotation, as demonstrated by\\n`BookCargoCommandRequest` class from `cargotracking-booking-app-commandside` module.\\n"}
{"prompt":"## Context\\nEnd-to-end tests should be easy to run, and possible to run in a variety of use cases.\\nUntil [Playwright test runner](https:\/\/github.com\/microsoft\/playwright-test) is a little more stable, we want to set this up within Jest.\\n","completion":"## Decision\nPlaywright will be configured to support the following test modes:\\n- **`dev` (default)**: Playwright will run tests in a headful chromium browser with the devtools enabled. Good if you need to debug tests or see them being executed.\\n- **`headless`**: Playwright will run tests in a headless chromium browser only. Good if you're practicing some TDD and want to run the suite often and quickly.\\n- **`browsers`**: Playwright will run in headless mode, but it will run every end-to-end test in `chromium`, `firefox`, and `webkit`.\\n- **`devices`**: Playwright will run in headless mode with chromium only, but it will run every end-to-end test on all `iPhoneX` and `Pixel 2` screen sizes.\\n"}
{"prompt":"## Context\\nPreviously, it was decided in [ADR 0004](https:\/\/github.com\/ministryofjustice\/offender-management-architecture-decisions\/blob\/master\/decisions\/0004-separate-api-and-user-facing-applications) that we would separate data and presentation concerns.  This however was reversed by [ADR 0010](https:\/\/github.com\/ministryofjustice\/offender-management-architecture-decisions\/blob\/master\/decisions\/0010-allocation-api-has-less-responsibility) which meant there was an overlap of data concerns with some data access from the presentation layer, and some via the Allocation API.\\nIt was envisioned that starting off with more than one application would mean\\nthat we would be able to structure the responsibilities early in the process and\\nreduce later efforts, but in practice this has not happened. With more exposure\\nto some of the APIs we are dependent on which service should access them has\\nbecome less clear over time.  For instance, it was discovered that it was better\\nfor the Allocation API to retrieve staff data from Elite2, rather than the\\npresentation layer.\\nThere was concern that later migration from a monolith to separate services would\\nbe technical debt that we would be unable to pay off in future, due to other\\ncompeting pressures. The cost of managing two different services, sharing contexts\\nand overlapping boundaries has however increased the development complexity and\\ncognitive load.\\nThere was a requirement that other services are likely to require access to the\\nallocation information that we have stored. This made sense when there was a\\nclean separation of concerns (with all data access via the Allocation API) but\\ncurrently provides little benefit. Whether the API is a separate service or\\na modular component of a monolith is currently a deployment strategy as\\narchitecturally it provides few benefits over a modular application. It is\\nentirely possible to provide an API via a modular monolith.\\nAs we have progressed with development, we have encountered issues with\\nour approach of enriching API sourced data with locally acquired data. Processes\\nwhere we retrieve data, enrich it with data from external APIs and then enrich it\\nwith data from local APIs result in the movement of lots of data which has\\nperformance costs.  Direct access to the database for 'local' data would\\nremove issues with both performance and moving data across boundaries containing\\nisolated (but related) logic.\\n","completion":"## Decision\nWe will integrate the existing Allocation API into the Allocation Manager, and make\\nthe public api available at \/api.\\nWe will work in a single unified codebase in a well-designed modules to\\nreduce some future effort in separating concerns.\\n"}
{"prompt":"## Context\\nThe two main isolation models for Kubernetes are:\\n* Namespaces within a single cluster\\n* Running multiple clusters\\nAll Service Teams currently have separate AWS accounts.\\nSome Service Teams have separate AWS accounts for separate environment (ie. Staging, Production etc)\\nMany Service Teams have micro-service architectures\\nSome Service Teams have unique network isolation requirements that may be hard to implement in a shared environment.\\nTo ensure \"smooth transition\" during a migration it would be preferable to have clusters deployed to Service Team's VPCs.\\nTo ensure separation of billing it would be preferable to deploy clusters to Service Team's AWS accounts.\\nTo ensure strong network\/compute isolation between Service Teams it would be preferable to deploy separate clusters for separate environments.\\n","completion":"## Decision\nWe will manage multiple Kubernetes clusters, one per environment per tenant.\\nThis should provide:\\n* Strong network\/compute isolation between service teams\\n* Clear separation of billing\\n* Ability to deploy into target VPCs to make piecemeal microservice migration practical\\n"}
{"prompt":"## Context\\nThe Wikidata Bridge app uses VueJs version 2 and Vuex version 3 for state management.\\nBoth within the store and in vue components the calls to `dispatch()` and `commit()` are not type safe.\\nThat means that they accept any arguments and TypeScript will still compile without error.\\nSince `dispatch()` and `commit()` are typical seams that are usually mocked during unit testing,\\nit is up to integration, end-to-end and browser tests to detect these errors.\\nThis is particularly unfortunate as the store is one of the central locations where business logic happens.\\nWe considered two options:\\n1. writing our own set of wrappers for `dispatch()` and `commit()` to get type safety\\n1. using vuex-smart-modules\\nThe advantages of doing it ourselves included us not having another dependency.\\nThe risks include that this would be yet another homebrew layer of abstraction.\\nThe advantages of using vuex-smart-modules include:\\n- it gives us proper native type safety both in the store and in components\\n- we can call actions\/getters\/mutations with [method style access](https:\/\/github.com\/ktsn\/vuex-smart-module#method-style-access-for-actions-and-mutations)\\n- we can get rid of all the `BRIDGE_SET_TARGET_VALUE` constants without having to fall back to string literals\\n- we can actually use the IDE's `go to method definition` functionality\\n- we can rely on the action's return type instead of dispatch's `Promise<any>` being used everywhere\\n- we can drop the `vuex-class` dependency as we can use vuex-smart-modules for all store access in components\\n- it is developed by a VueJs core contributor\\nThe risks include:\\n- it is still a very new project with a 0.x.y version number\\n- it is another layer on top of vuex, which means the documentation may not be as good as it could be\\n- we are the first big project to use it\\n- mocking of dependencies and nested modules in testing still seems to be not handled as diligently as one would wish\\n","completion":"## Decision\nWe decided to rewrite our Vuex store using vuex-smart-modules version 0.3.4\\n"}
{"prompt":"## Context\\nWe define a course option as a specific combination of (course, study mode, site). A course has several sites, and each site may offer the course in full time or part time modes. Every course option we create for a course covers one of the combinations.\\nWe've recently completed a series of iterations that deal with:\\n- course options falling into a state that means they are no longer valid choices for an application\\n- how to deal with options that fall into this state after they've already been added to a candidate's application\\nThis ADR summarises the design choices made during this phase of work and provides an overview of how various course option states are handled.\\n","completion":"## Decision\nThe service currently stops an application being submitted (via error messages on submission) in the following cases:\\n- The course is full (ie - it has no vacancies at any of its course options)\\n- The course has been withdrawn by the provider from Find.\\n- The course has been closed by us on Apply.\\n- The specific course option chosen by the candidate is full (but other course options have vacancies).\\nThere's a fifth state we handle as well:\\n- Upon syncing, the Find API tells us that a specific site is no longer related to a course.\\nFor these cases, we:\\n- try and delete the course options for this site\/course combo, OR\\n- if the course option is part of a candidate's application we set the field `site_still_valid` to `false` and notify support instead.\\n"}
{"prompt":"## Context\\nWe need to have the mechanism for supporting and validating common architecture principles in all application.\\n","completion":"## Decision\nArchitecture tests with using Archunit should be implemented.\\n"}
{"prompt":"## Context\\n`menu-generation` application is packaged as a [Spring Boot](0005-use-spring-framework.md) executable JAR file.\\nRunning acceptance tests on development machine or during Continuous Integration must be quick, easy and the least\\ndependent of the underlying system.\\n[Docker](https:\/\/www.docker.com\/) is a widespread container based solution that can be used during development lifecycle\\non most operating systems as well as in well established Cloud solutions such as [Kubernetes](https:\/\/kubernetes.io\/).\\n","completion":"## Decision\n`menu-generation` application will be packaged as a docker image. A `docker-compose` definition will also be provided\\nto help running the application and its dependencies in a consistent and isolated environment.\\n"}
{"prompt":"## Context\\nIn order to dynamically provide secure Notebook containers it will be necessary to\\ndynamically generate and securely store secrets. We want to isolate this from the\\ndatabase and use a dedicated solution for this problem.\\n","completion":"## Decision\nWe have decided to use [Hashicorp Vault](https:\/\/www.vaultproject.io\/) to store secrets.\\nIt provides a dedicated system to securely store and manage access to secrets.\\n"}
{"prompt":"## Context and Problem Statement\\nWe are facing some issues that need to be addressed. We can list a couple of them as important:\\n- The hard limit of 100 people we currently have is not that bad for small events or some games and expositions. But we have it for the whole world. We need to make better use of the 100 limit, which is of network size and not of people. We also need to improve this limit for some situations, like big events.\\n- New users have a hard time understanding realms. Since we didn\u2019t make them an integral part of UI\/UX, the only way a user can know in which realm they are is in URL.\\n- We don\u2019t do enough to communicate the status of the realms to the users. A user can be alone in a realm and don\u2019t know why they aren\u2019t seeing anyone, given that there could be a lot of users in another realm.\\n","completion":"## Decision\nWe will start rolling out a series of Protocol optimizations to help Decentraland communications systems to scale in different ways.\\nPlanned optimizations topics are:\\n- Reduce complexity of layers for users, removing them completely, and by doing so, optimize \"finding people in the world\"\\n- Enable vertical & horizontal scallability\\n- Optimizing connections topologies in P2P networks\\n- Reduce latency between users\\n- Increase success rate in WebRTC P2P connections\\n### Rationale behind the removal of \"layers\"\\nLayers are logical \"connection groups\" inside the realms, as of today, realms are mapped 1to1 with a catalyst instance. Inside the catalyst, it would be sub-optimal to connect users all-to-all.\\nBy default every catalyst have 28 layers, each one with a different color name. We can infer then, that the maximum amount of concurrent users connected to a catalyst is 2800 users (28 * 100).\\nLayers are the color after the name of the realm:\\n```\\nartemis-amber\\n^^^^^ layer\\n^^^^^^^       realm\\n```\\nLayers served some purposes during their existence:\\n- To enable horizontal scallability in catalysts, without the need of DAO approval, each catalyst have layers, which are groups of up to 100 users inside a catalyst.\\n- By separating users logically, that gives the renderer some room to render fewer avatars and therefore allocate more resources to the rendering of other assets.\\nLayers served their purpose in early stages of the project, but the time has come to evolve the protocol. Therefore we introduce _connection islands_.\\n#### What are connection islands?\\nConnection islands are groups of connections based on their geographical location, instead of logic layers.\\nThe implementation performs real-time clustering, organizing connections in islands (clusters) of close peers, which will connect to each otheer\\nThat solves many problems:\\n- Reduces the complexity to find people in the world: Only the realm is required now to find a friend.\\n- Optimizes finding people: it was common to join a realm with lots of people, but the connection is assigned to a layer without people in the parcel you are standing. While in other layers you may find people in that layer.\\n- Removes the 2800 user limit: there can be virtually infinite islands in a catalyst, as long as the resources of the server allow it.\\nThe implementation of the algorithm can be found at https:\/\/github.com\/decentraland\/archipelago\\n"}
{"prompt":"## Context and Problem Statement\\nA rewrite would solve multiple issues we\u2019re currently experiencing.\\nCurrent issues:\\n* Integration with the post editor not working \\(probably since after Gutenberg transition\\)\\n* Doesn\u2019t work well with WP modals \\(is a modal itself, opened on top of a modal\\).\\n* A few larger and lots of small UX and performance bugs. See issues linked to this ticket [https:\/\/jira.greenpeace.org\/browse\/PLANET-5110](https:\/\/jira.greenpeace.org\/browse\/PLANET-5110).\\n* Lots of small fixes were applied, gradually making the code more complex.\\n* Uses jQuery.\\n* Some duplication of code in master theme.\\n* Credentials stored in settings.\\n* Coding standards are behind our other code.\\n* Dead code.\\n* Naming conflicts with WP Media Library \\(both in code and for users\\) can cause confusion.\\nMigration trajectory:\\n1. Find a good new name so we can use this consistently from the start to avoid additional renaming. \\(Suggestion from Kelli and Multimedia editors: \"Greenpeace Image Archive\" or \"GP Image Archive\" for short\\)\\n2. Add a setting to use the new version instead of the old plugin. Will be false for all NROs\\n3. Develop the new implementation in master-theme. Original plugin stays in use, but new code can already be merged as it won\u2019t be used.\\n4. When the new version is stable, switch one or a few NROs to it.\\n5. Fix any issues that came up for those NROs.\\n6. Switch all NROs to the new version.\\n7. Remove the old plugin.\\n","completion":"## Decision\nChosen option: **Rewrite the code as a part of master theme.**\\n"}
{"prompt":"# Context and Problem Statement\\nHow can plain text and markdown documents be converted into a PDF\\nfiles?\\nRendering images is not important here, since the files must be self\\ncontained when uploaded to Docspell.\\nThe test file is the current documentation page of Docspell, found in\\n`microsite\/docs\/doc.md`.\\n```\\n---\\nlayout: docs\\nposition: 4\\ntitle: Documentation\\n---\\n# {page .title}\\nDocspell assists in organizing large amounts of PDF files that are\\n...\\n","completion":"## Decision\nJava library \"flexmark\".\\nI think all results are great. It depends on the type of document and\\nwhat one expects to see. I guess that most people expect something\\nlike pandoc-html produces for the kind of files docspell is for (it is\\nnot for newspaper articles, where pandoc-latex would be best fit).\\nBut choosing pandoc means yet another external command to depend on.\\nAnd the results from flexmark are really good, too. One can fiddle\\nwith options and css to make it look better.\\nTo not introduce another external command, decision is to use flexmark\\nand then the already existing html->pdf conversion.\\n"}
{"prompt":"## Context and Problem Statement\\nWellcome Collection wishes to implement an Identity and Access\\nManagement platform, to provide authentication and authorisation\\nservices. Such a service is required to integrate with Wellcome\\nLibrary's existing pool of registered library members, and would expose\\nitself through recognised, standards based protocols.\\n## Decision Drivers\\n* Ease of implementation and ongoing maintenance.\\n* Low \/ nil upfront and ongoing subscription costs.\\n* Ability to integrate with, and maintain, an existing user pool.\\n","completion":"## Decision\n* Ease of implementation and ongoing maintenance.\\n* Low \/ nil upfront and ongoing subscription costs.\\n* Ability to integrate with, and maintain, an existing user pool.\\nChosen option: Auth0, because it offers superior integration with\\nexisting user pools.\\n"}
{"prompt":"## Context\\nEKS supports three different types of EKS worker node groups: [managed, self-managed, and Fargate](https:\/\/docs.aws.amazon.com\/eks\/latest\/userguide\/eks-compute.html). The three options are not mutually exclusive, and a single EKS cluster can schedule pods onto any combination of the three, with support for multiple managed and self-managed node groups within a single cluster.\\nManaged and self-managed node groups are both implemented as EC2 instances within autoscaling groups, with all of the configuration options that this implies (security groups, custom AMIs, instance types, etc). As Fargate is a managed service there are fewer configuration options available, but also less management overhead.\\nA major consideration is the implications for [cluster upgrades](https:\/\/docs.aws.amazon.com\/eks\/latest\/userguide\/update-cluster.html). As Kubernetes has a 4-month release cycle, upgrades will be relatively frequent, so a robust and mature node upgrade process is required, given the potentially destructive nature of worker updates.\\nAt this stage our requirements for worker nodes are not fully known, so maintaining flexibility at this stage is important.\\nThe functional capabilities of nodes in managed and self-managed node groups are equivalent, whereas Fargate places some limitations on pod and cluster capabilities (e.g. cannot run `DaemonSets`, specify an alternative Container Network Interface provider, or attach persistent EBS volumes to pods).\\nThe main distinction between managed and self-managed node groups is the node upgrade process; [managed node groups handle graceful upgrades largely automatically](https:\/\/docs.aws.amazon.com\/eks\/latest\/userguide\/update-managed-node-group.html), whereas self-managed groups would require us to [orchestrate the update process ourselves](https:\/\/docs.aws.amazon.com\/eks\/latest\/userguide\/migrate-stack.html).\\n","completion":"## Decision\nUse [managed node groups](https:\/\/docs.aws.amazon.com\/eks\/latest\/userguide\/managed-node-groups.html).\\n"}
{"prompt":"## Context\\nThe organizational model of a company has a great influence on the communication structure in the company. Within a department, people tend to use the same terminology and definitions. This relates closely to the definition of ubiquitous language. Therefore, the domain model used on the platform should be closely related to the organizational model of the company. This is also supported by Conway's law.\\n","completion":"## Decision\nWe will structure the domain model on the platform around the departments organizational model of the company.\\n"}
{"prompt":"## Context\\nThe current \"naive\" IBC Relayer strategy currently establishes a single predetermined IBC channel atop a single connection between two clients (each potentially of a different chain).  This strategy then detects packets to be relayed by watching for `send_packet` and `recv_packet` events matching that channel, and sends the necessary transactions to relay those packets.\\nWe wish to expand this \"naive\" strategy to a \"passive\" one which detects and relays both channel handshake messages and packets on a given connection, without the need to know each channel in advance of relaying it.\\nIn order to accomplish this, we propose adding more comprehensive events to expose channel metadata for each transaction sent from the `x\/ibc\/04-channel\/keeper\/handshake.go` and `x\/ibc\/04-channel\/keeper\/packet.go` modules.\\nHere is an example of what would be in `ChanOpenInit`:\\n```go\\nconst (\\nEventTypeChannelMeta = \"channel_meta\"\\nAttributeKeyAction = \"action\"\\nAttributeKeyHops = \"hops\"\\nAttributeKeyOrder = \"order\"\\nAttributeKeySrcPort = \"src_port\"\\nAttributeKeySrcChannel = \"src_channel\"\\nAttributeKeySrcVersion = \"src_version\"\\nAttributeKeyDstPort = \"dst_port\"\\nAttributeKeyDstChannel = \"dst_channel\"\\nAttributeKeyDstVersion = \"dst_version\"\\n)\\n\/\/ ...\\n\/\/ Emit Event with Channel metadata for the relayer to pick up and\\n\/\/ relay to the other chain\\n\/\/ This appears immediately before the successful return statement.\\nctx.EventManager().EmitEvents(sdk.Events{\\nsdk.NewEvent(\\ntypes.EventTypeChannelMeta,\\nsdk.NewAttribute(types.AttributeKeyAction, \"open_init\"),\\nsdk.NewAttribute(types.AttributeKeySrcConnection, connectionHops[0]),\\nsdk.NewAttribute(types.AttributeKeyHops, strings.Join(connectionHops, \",\")),\\nsdk.NewAttribute(types.AttributeKeyOrder, order.String()),\\nsdk.NewAttribute(types.AttributeKeySrcPort, portID),\\nsdk.NewAttribute(types.AttributeKeySrcChannel, chanenlID),\\nsdk.NewAttribute(types.AttributeKeySrcVersion, version),\\nsdk.NewAttribute(types.AttributeKeyDstPort, counterparty.GetPortID()),\\nsdk.NewAttribute(types.AttributeKeyDstChannel, counterparty.GetChannelID()),\\n\/\/ The destination version is not yet known, but a value is necessary to pad\\n\/\/ the event attribute offsets\\nsdk.NewAttribute(types.AttributeKeyDstVersion, \"\"),\\n),\\n})\\n```\\nThese metadata events capture all the \"header\" information needed to route IBC channel handshake transactions without requiring the client to query any data except that of the connection ID that it is willing to relay.  It is intended that `channel_meta.src_connection` is the only event key that needs to be indexed for a passive relayer to function.\\n### Handling Channel Open Attempts\\nIn the case of the passive relayer, when one chain sends a `ChanOpenInit`, the relayer should inform the other chain of this open attempt and allow that chain to decide how (and if) it continues the handshake.  Once both chains have actively approved the channel opening, then the rest of the handshake can happen as it does with the current \"naive\" relayer.\\nTo implement this behavior, we propose replacing the `cbs.OnChanOpenTry` callback with a new `cbs.OnAttemptChanOpenTry` callback which explicitly handles the `MsgChannelOpenTry`, usually by resulting in a call to `keeper.ChanOpenTry`.  The typical implementation, in `x\/ibc-transfer\/module.go` would be compatible with the current \"naive\" relayer, as follows:\\n```go\\nfunc (am AppModule) OnAttemptChanOpenTry(\\nctx sdk.Context,\\nchanKeeper channel.Keeper,\\nportCap *capability.Capability,\\nmsg channel.MsgChannelOpenTry,\\n) (*sdk.Result, error) {\\n\/\/ Require portID is the portID transfer module is bound to\\nboundPort := am.keeper.GetPort(ctx)\\nif boundPort != msg.PortID {\\nreturn nil, sdkerrors.Wrapf(porttypes.ErrInvalidPort, \"invalid port: %s, expected %s\", msg.PortID, boundPort)\\n}\\n\/\/ BEGIN NEW CODE\\n\/\/ Assert our protocol version, overriding the relayer's suggestion.\\nmsg.Version = types.Version\\n\/\/ Continue the ChanOpenTry.\\nres, chanCap, err := channel.HandleMsgChannelOpenTry(ctx, chanKeeper, portCap, msg)\\nif err != nil {\\nreturn nil, err\\n}\\n\/\/ END OF NEW CODE\\n\/\/ ... the rest of the callback is similar to the existing OnChanOpenTry\\n\/\/ but uses msg.* directly.\\n```\\nHere is how this callback would be used, in the implementation of `x\/ibc\/handler.go`:\\n```go\\n\/\/ ...\\ncase channel.MsgChannelOpenTry:\\n\/\/ Lookup module by port capability\\nmodule, portCap, err := k.PortKeeper.LookupModuleByPort(ctx, msg.PortID)\\nif err != nil {\\nreturn nil, sdkerrors.Wrap(err, \"could not retrieve module from port-id\")\\n}\\n\/\/ Retrieve callbacks from router\\ncbs, ok := k.Router.GetRoute(module)\\nif !ok {\\nreturn nil, sdkerrors.Wrapf(port.ErrInvalidRoute, \"route not found to module: %s\", module)\\n}\\n\/\/ Delegate to the module's OnAttemptChanOpenTry.\\nreturn cbs.OnAttemptChanOpenTry(ctx, k.ChannelKeeper, portCap, msg)\\n```\\nThe reason we do not have a more structured interaction between `x\/ibc\/handler.go` and the port's module (to explicitly negotiate versions, etc) is that we do not wish to constrain the app module to have to finish handling the `MsgChannelOpenTry` during this transaction or even this block.\\n","completion":"## Decision\n- Expose events to allow \"passive\" connection relayers.\\n- Enable application-initiated channels via such passive relayers.\\n- Allow port modules to control how to handle open-try messages.\\n"}
{"prompt":"## Context\\nCertain elements on the Talk To Frank site need to be loaded regardless of the user entry point,\\ne.g. the current user story involves adding a warning message bar that appears on all pages.\\nThe CMS needs to be updated to support the addition of site wide 'settings'.\\n","completion":"## Decision\nA new 'site settings' content model will be created.  A 'Sitewide' item will then be created to contain all sitewide settings.\\nThis would then allow for the following (future) setup,\\nContent items of type 'site settings'\\n- Sitewide\\n- Homepage - any settings that should override the sitewide values on the homepage.\\n- Drugs A-Z - any settings that should override the sitewide values on the Drugs A-Z page.\\nReferences:\\n- https:\/\/www.contentful.com\/r\/knowledgebase\/dynamic-microcopy\/\\n- https:\/\/www.contentfulcommunity.com\/t\/is-it-common-to-have-a-settings-content-model\/924\/2\\n"}
{"prompt":"## Context\\nWe are starting to develop a number of user-facing applications with web\\ninterfaces that need to be GOV.UK branded. In order to keep things consistent\\nwe want to pick a single programming language to write these in.\\nWe've previously chosen Ruby on Rails as our desired framework in the previous\\n[ADR022]. And whether or not it was a right choice, we decided to revisit and\\nreconsider that choice.\\nThe reason for that, is simply the direction GDS is heading at. It would appear\\nthat old components will be soon deprecated and the Frontend Community has no\\ndesire to support gems in the future. Saying that, we could have remain with\\nRails and delegate the component management to NPM. This would however increase\\nthe amount of possible maintenance work we would need to undertake, due to the\\nuse of [nunjucks] by the Design System team.\\nWe questioned the need of running Rails application for something that\\nessentially is a templating system for existing data and API.\\nAfter some more research, discussion with the head of the Frontend Community,\\nmembers of the team and some others in GDS, the better choice would be Node for\\nthe following reasons:\\n* It's the way Frontend Community is heading at\\n* It will be easier to rotate\/onboard Frontend Developers\\n* The initial applications are to be simple (thin layer between API calls and\\nHTML parsing)\\n* It's light and essentially is JavaScript\\n* It supports [nunjucks] which will help us in maintenance\\n","completion":"## Decision\nWe will use Node to create new user-facing applications that render a web\\ninterface for our service but will not be used to implement any significant\\n\u2018application logic\u2019.\\n"}
{"prompt":"## Context\\nCurrently requests for public manifests are routed through Donut in order to be re-cached in S3 if needed. We need to plan a strategy to handle IIIF manifest requests for manifests which were written by Meadow and should not be routed through Donut.\\n","completion":"## Decision\n- Manifests will be written for all works (public\/private\/restricted) on create\/update.\\n- IIIF Manifests will be written on to the `public` directory of the existing stack-\\*-pyramids bucket, and will live alongside existing Donut manifests.\\n- Requests for public manifests moving forward will route from the API gateway to a new lambda which will check Elasticsearch for the host application. Then it will either route directly to the S3 Pyramids `\/public` folder (Meadow) or to Donut. (This is temporary, until Donut content is migrated.)\\n- Meadow will use these manifests internally\\n"}
{"prompt":"# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which web framework to use for the data\\nAPI for the Performance Platform.\\nWe are writing the API in Python. We need to support accepting unstructured\\ndata, and returning structured data for reads.\\nThe API will not be RESTful initially. We want to optimise for easy writing,\\nand do the hard work to return structured data at read time. We do not\\nunderstand the model or resources that we will be using, so for now, we think\\nwe will allow data sets to be created and written to, and do some\\ntransformations at query time on those data sets.\\nThis means that we think we need to allow simple POST\/PUT to a data set, and\\nthen a more complex read query to produce structured data.\\nOver time, we expect to see common data sets \/ types and we can then look to\\nuse a more structured approach, defining resources and relationships. At that\\npoint, we might also want to consider using RESTful features, or switching\\nto a framework that has more support for that approach.\\nOptions considered include Django and Flask, since we are familar with those.\\n# Decision\\nWe will write the data API using Flask.\\nWe don't need the ORM for Django, and Flask seems simpler in terms of starting\\nwith a small API.\\n# Status\\nAccepted.\\n# Consequences\\nDevs and operations need to be comfortable with Flask apps.\\n","completion":"## Decision\nWe will write the data API using Flask.\\nWe don't need the ORM for Django, and Flask seems simpler in terms of starting\\nwith a small API.\\n# Status\\nAccepted.\\n# Consequences\\nDevs and operations need to be comfortable with Flask apps.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nConcurrent git operations can fail because git is locking the repo. The current implementation is waiting for jobs to finish, but multiple workers in different containers can still take jobs in the queue and proccess them in parallel.\\n","completion":"## Decision\nReduce worker concurrency to 1 by using a single container (for now).\\n"}
{"prompt":"## Context\\nSince the initial version of the pubsub, there's been a number of issues\\nraised: [#951], [#1879], [#1880]. Some of them are high-level issues questioning the\\ncore design choices made. Others are minor and mostly about the interface of\\n`Subscribe()` \/ `Publish()` functions.\\n### Sync vs Async\\nNow, when publishing a message to subscribers, we can do it in a goroutine:\\n_using channels for data transmission_\\n```go\\nfor each subscriber {\\nout := subscriber.outc\\ngo func() {\\nout <- msg\\n}\\n}\\n```\\n_by invoking callback functions_\\n```go\\nfor each subscriber {\\ngo subscriber.callbackFn()\\n}\\n```\\nThis gives us greater performance and allows us to avoid \"slow client problem\"\\n(when other subscribers have to wait for a slow subscriber). A pool of\\ngoroutines can be used to avoid uncontrolled memory growth.\\nIn certain cases, this is what you want. But in our case, because we need\\nstrict ordering of events (if event A was published before B, the guaranteed\\ndelivery order will be A -> B), we can't publish msg in a new goroutine every time.\\nWe can also have a goroutine per subscriber, although we'd need to be careful\\nwith the number of subscribers. It's more difficult to implement as well +\\nunclear if we'll benefit from it (cause we'd be forced to create N additional\\nchannels to distribute msg to these goroutines).\\n### Non-blocking send\\nThere is also a question whenever we should have a non-blocking send.\\nCurrently, sends are blocking, so publishing to one client can block on\\npublishing to another. This means a slow or unresponsive client can halt the\\nsystem. Instead, we can use a non-blocking send:\\n```go\\nfor each subscriber {\\nout := subscriber.outc\\nselect {\\ncase out <- msg:\\ndefault:\\nlog(\"subscriber %v buffer is full, skipping...\")\\n}\\n}\\n```\\nThis fixes the \"slow client problem\", but there is no way for a slow client to\\nknow if it had missed a message. We could return a second channel and close it\\nto indicate subscription termination. On the other hand, if we're going to\\nstick with blocking send, **devs must always ensure subscriber's handling code\\ndoes not block**, which is a hard task to put on their shoulders.\\nThe interim option is to run goroutines pool for a single message, wait for all\\ngoroutines to finish. This will solve \"slow client problem\", but we'd still\\nhave to wait `max(goroutine_X_time)` before we can publish the next message.\\n### Channels vs Callbacks\\nYet another question is whether we should use channels for message transmission or\\ncall subscriber-defined callback functions. Callback functions give subscribers\\nmore flexibility - you can use mutexes in there, channels, spawn goroutines,\\nanything you really want. But they also carry local scope, which can result in\\nmemory leaks and\/or memory usage increase.\\nGo channels are de-facto standard for carrying data between goroutines.\\n### Why `Subscribe()` accepts an `out` channel?\\nBecause in our tests, we create buffered channels (cap: 1). Alternatively, we\\ncan make capacity an argument and return a channel.\\n","completion":"## Decision\n### MsgAndTags\\nUse a `MsgAndTags` struct on the subscription channel to indicate what tags the\\nmsg matched.\\n```go\\ntype MsgAndTags struct {\\nMsg interface{}\\nTags TagMap\\n}\\n```\\n### Subscription Struct\\nChange `Subscribe()` function to return a `Subscription` struct:\\n```go\\ntype Subscription struct {\\n\/\/ private fields\\n}\\nfunc (s *Subscription) Out() <-chan MsgAndTags\\nfunc (s *Subscription) Canceled() <-chan struct{}\\nfunc (s *Subscription) Err() error\\n```\\n`Out()` returns a channel onto which messages and tags are published.\\n`Unsubscribe`\/`UnsubscribeAll` does not close the channel to avoid clients from\\nreceiving a nil message.\\n`Canceled()` returns a channel that's closed when the subscription is terminated\\nand supposed to be used in a select statement.\\nIf the channel returned by `Canceled()` is not closed yet, `Err()` returns nil.\\nIf the channel is closed, `Err()` returns a non-nil error explaining why:\\n`ErrUnsubscribed` if the subscriber choose to unsubscribe,\\n`ErrOutOfCapacity` if the subscriber is not pulling messages fast enough and the channel returned by `Out()` became full.\\nAfter `Err()` returns a non-nil error, successive calls to `Err() return the same error.\\n```go\\nsubscription, err := pubsub.Subscribe(...)\\nif err != nil {\\n\/\/ ...\\n}\\nfor {\\nselect {\\ncase msgAndTags <- subscription.Out():\\n\/\/ ...\\ncase <-subscription.Canceled():\\nreturn subscription.Err()\\n}\\n```\\n### Capacity and Subscriptions\\nMake the `Out()` channel buffered (with capacity 1) by default. In most cases, we want to\\nterminate the slow subscriber. Only in rare cases, we want to block the pubsub\\n(e.g. when debugging consensus). This should lower the chances of the pubsub\\nbeing frozen.\\n```go\\n\/\/ outCap can be used to set capacity of Out channel\\n\/\/ (1 by default, must be greater than 0).\\nSubscribe(ctx context.Context, clientID string, query Query, outCap... int) (Subscription, error) {\\n```\\nUse a different function for an unbuffered channel:\\n```go\\n\/\/ Subscription uses an unbuffered channel. Publishing will block.\\nSubscribeUnbuffered(ctx context.Context, clientID string, query Query) (Subscription, error) {\\n```\\nSubscribeUnbuffered should not be exposed to users.\\n### Blocking\/Nonblocking\\nThe publisher should treat these kinds of channels separately.\\nIt should block on unbuffered channels (for use with internal consensus events\\nin the consensus tests) and not block on the buffered ones. If a client is too\\nslow to keep up with it's messages, it's subscription is terminated:\\nfor each subscription {\\nout := subscription.outChan\\nif cap(out) == 0 {\\n\/\/ block on unbuffered channel\\nout <- msg\\n} else {\\n\/\/ don't block on buffered channels\\nselect {\\ncase out <- msg:\\ndefault:\\n\/\/ set the error, notify on the cancel chan\\nsubscription.err = fmt.Errorf(\"client is too slow for msg)\\nclose(subscription.cancelChan)\\n\/\/ ... unsubscribe and close out\\n}\\n}\\n}\\n### How this new design solves the current issues?\\n[#951] ([#1880]):\\nBecause of non-blocking send, situation where we'll deadlock is not possible\\nanymore. If the client stops reading messages, it will be removed.\\n[#1879]:\\nMsgAndTags is used now instead of a plain message.\\n### Future problems and their possible solutions\\n[#2826]\\nOne question I am still pondering about: how to prevent pubsub from slowing\\ndown consensus. We can increase the pubsub queue size (which is 0 now). Also,\\nit's probably a good idea to limit the total number of subscribers.\\nThis can be made automatically. Say we set queue size to 1000 and, when it's >=\\n80% full, refuse new subscriptions.\\n"}
{"prompt":"## Context\\nThe issue motivatina# [Do we need more GPUs to support a growth in ArcGIS Pro GTS usage?]\\n* Status: [proposed \/ exploratory] - optional\\n* Deciders: [Scott Sharp, Michelle Douville, Jeff Card, David Ell] - optional\\n* Date: [2020-03-22] - optional\\nTechnical Story: [description | ticket\/issue URL] - optional\\n* ArcGIS Pro is the main use case for GPU(s)\\n* IITD maintain servers with ArcGIS Pro on them currently as part of a proof on concept - there are currently two GPUs on 2 test servers.\\n* Only NVIDIA provides what ESRI recommends. <https:\/\/desktop.arcgis.com\/en\/arcmap\/latest\/extensions\/arcglobe\/faqs-for-selecting-graphics-cards-globe.htm>\\n* From ESRI Dev Summit - NVIDIA as concurrent licensing model is not cheap. and has Convoluted subscription levels..\\n* Scott Sharp is to Email on what the costs are.. And its not published - HP needs to provide the costs?\\nM60 in Calgary ($10000) 2 chips can only see one 16GB memory and in accessible already obsolete,\\nP4 ($3000 single newer cheaper\/ with concurrent use) in Kamloops with test license manager and unlimited key.. Used when load testing occurs.\\nGPU could be use as well for other things than graphical, such as mining, processes, that requires heaving processing\\nENV MIKE 00 - used for flood modelling.\\nFirewall has been opened to run on desktops as well, GPU is the faster for processing, more iteration. (license  manager on local PC, and run desktop, CPU based on GPU)..\\nArcGIS PRO use case\\n* 2D data, no GPU will use CPU, heavy CPU.. On GTS, load test with raw LIDAR, from Harold Steiner:\\n4 people without GPU spinning it around..\\n11 people with a GTS and still responsive.  Mark McGirr building geoprocessing tool (statusing tool - built in AGO, needs to be built in QGIS as a comparison)..\\nCurrent Status -\\n8 servers with ArcGIS Pro may require a GPU, metrics uptake for Pro not as high as anticipated.\\n* There are currently 28 concurrent max users in ArcGIS Pro pool .\\nMost users are still on ArcGIS Desktop, Scott is upgrading to ArcGIS 10.8 (EOL 2026 which may be the last veresion),\\n18 servers in production for Desktop (10.6, 10.3) - 10 years max we will still have Desktop.. P\\nPatches to update 10.8 - 10x more concurrent users on Desktop..\\nQGIS is on all servers 27 servers with 26 Distinct users.. (not concurrent)\\n2200 unique IDS, 1400 GTS, 700 BCTS, the rest are stand alone licenses.\\nGPU is around $4000 for concurrent licenses - need more info on this and perhaps David Ell has other options.\\nCitrix Licensing adds another trickiness level:\\nZENAPP and ZENDesktop, - (ZENAPP) 1st come and served on server..\\nGPU will be fully available for one server and only see on core, and affect all users on server.\\n(ZENDESKTOP - allows you to divy up resources BUT it interferes with HPAs agreement as it essentially creates a VM to handle requires. Good for power user, not great for others.\\nIdeal Statues..\\nGPU for Pro, cards are 8GB - 1 GB per user, ZENAPP\/ZENDesktop (ESRI doesn't recommend using it will not support it).\\nunknown costs of GPU licensing\\nOCIO has offered - SAG - little desktops for another Ministry using not citrix, but another VM ware.\\nBCTS 9 servers - ArcGIS - flipping switching and going PRO and their pool may want GPU.. Carol Gjetturd.. (admin of pool)..- Harry and Scott to LRM, oracle, desktop. BCTS specific tools were done by Carole.. 3 more years amortization, brand new GPU would be a waste. Server 2012. (single vs concurrent price comparison)..\\n$170 advanced desktop - vs GTS pool.. Deal with patching, licensing, versions, latency to file shares\\nUsing licensing manager.. 17-22 standalone..BCTS\/WF\/PTSG\/HAIDA GWAII...\\nCOP discussed PRO vs Desktop - training, access, GeoBC has a lot of stand alone PRO..\\nArcGIS ArcGIS Online -GEOBC -  licenses.. DataBC - disabled theirs.\\n2 GPU (in place) isolated prod and test.\\nConcurrent license manager vs manager\\nFrank Burkheart and David Ell\\n## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers - optional\\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... - numbers of drivers can vary\\n","completion":"## Decision\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... - numbers of drivers can vary\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | ... | comes out best (see below)].\\n### Positive Consequences - optional\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, ...]\\n* ...\\n### Negative Consequences - optional\\n* [e.g., compromising quality attribute, follow-up decisions required, ...]\\n* ...\\nThe change that we're proposing or have agreed to implement.\\n"}
{"prompt":"## Context and Problem Statement\\nAs we grow the functionalities of the explorer, the code complexity grows among with it.\\nCurrently the whole explorer codebase is in the decentraland\/explorer repository.\\nIt is roughly divided in two big chunks:\\n* Unity\\n* Kernel\\n<!--\\n```dot\\ndigraph G {\\nsubgraph cluster_0 {\\nstyle=filled;\\ncolor=lightgrey;\\nnode [style=filled,color=white];\\n\"C#\" -&gt; \"decentraland-renderer\";\\n\"decentraland-renderer\" [label=\"decentraland-renderer\\n(wasm)\",shape=box3d,color=black,fillcolor=white];\\nlabel = \"Unity\";\\n}\\nsubgraph cluster_1 {\\nnode [style=filled];\\n\"build-ecs\" [shape=box3d];\\namd [shape=box3d];\\n\"build-ecs\" -&gt; ecs;\\namd -&gt; ecs;\\necs [shape=box3d];\\nrpc [shape=box3d];\\nvoice [shape=folder];\\nui -&gt; entrypoints;\\n\"loader\" -&gt; \"unity\";\\n\"gif\" -&gt; entrypoints;\\n\"voice\" -&gt; unity;\\n\/\/ workers\\ngif [shape=note];\\n\"scene-system\" [shape=note];\\n\"scene-system\" -&gt; entrypoints;\\nloader [shape=note];\\n\"ui\" [shape=note,label=\"UI + Avatars\"];\\nsubgraph cluster_2 {\\nsagas;\\nlabel=shared;\\n}\\necs -&gt; \"scene-system\";\\nrpc -&gt; \"scene-system\";\\nrpc -&gt; entrypoints;\\n\"entrypoints\" -&gt; \"preview\" ;\\n\"entrypoints\" -&gt; \"builder\";\\n\"entrypoints\" -&gt; \"unity\";\\npreview [shape=note];\\nunity [shape=note];\\nbuilder [shape=note];\\nlabel = \"Kernel\";\\ncolor=blue\\n}\\n\"decentraland-renderer\" -&gt; \"entrypoints\";\\n}\\n```\\n-->\\n![resources\/fig-explorer-packages-organization.svg](resources\/fig-explorer-packages-organization.svg)\\n","completion":"## Decision\nStaged execution:\\n1. Build kernel artifacts and publish to NPM\\n2. Move react code to new \"website\" folder continuing consuming files from kernel\\n3. (future stages) slowly move sagas and browser specific code to the new website\/shell folder\\n"}
{"prompt":"## Context\\nGiven that the ADR flow tool is intended to be a drop-in tool, and integrate well with other tools and repositories, we should aim also for ADR files created outside the tool, not with the same naming convention.\\nThe tool generally works with the file system - looking for the ADR dir, enumerating files, etc.\\nThe existing naming convention is rather strict, so the functions looking for files rely on the same regular expression that's used to create it.\\nWe would like to extend the file enumeration to more options for file names.\\n","completion":"## Decision\nChange the regular expression that's used to enumerate files to include more options (hypens, spaces).\\nFile should still be with a `.md` extension, and reside in the ADR dir (the directory identified by `.adr` file).\\nThe file template itself still remains the same.\\n"}
{"prompt":"## Context\\nWhen joining a project everybody needs some point where he can start stepping\\ninto the system, building it, running it, understanding it. The main entrypoint\\nis always the README.\\n","completion":"## Decision\nI will consider the README as the most valuable place of documentation. The\\nREADME of any project shall give a new team-member everything he needs to\\njoin the team.\\n"}
{"prompt":"## Context\\nSince January, when we developed the strategy for IIIF Manifests in ADR 22, we have decided that Donut\/Glaze will be decomissioned at or before the time that Meadow\/Fen are live in production.\\nIn light of this, we decided to go with a simpler approach to routing requests for Meadow vs. Donut manifests in the staging environment.\\n","completion":"## Decision\nWe will not build the lambda that checks for the host application, instead we will use the API Gateway to route all requests for public IIIF manifests to the pyramids bucket's public directory on S3.\\n"}
{"prompt":"### Context\\nThe app needs a convenient way to deploy to multiple devices. However this is done, installing and updating the app should be effortless for users. Ideally, the developer would make any necessary changes to the app, publish those changes to a location, and someone on the other end would only need to double click a download\/update link.\\n### Decision\\nOlder WPF and WinForm applications used a separate setup application, which was responsible for guiding someone through the process of installing an app. A user first downloaded the setup application, which would be responsible for installing the separate application on a system. We abandoned this route early in the development process due to the frequent errors that occurred during the install process.\\nClickOnce is the standard publishing method for WPF applications. Visual studio even references ClickOnce under the \"Security\" tab of the properties window. Documentation on ClickOnce is also quite robust. Because of this, we will accept ClickOnce as our method of choice when publishing.\\n### Status\\nAccepted\\n### Consequences\\nThere aren't many options to choose from when it comes to deploying a WPF application. Unfortunately, if ClickOnce doesn't easily suit our needs, we'll likely have to spend a good bit of time troubleshooting.\\n_In Retrospect:_ Unfortunately, it is recommended that ClickOnce deploys to a CD-Rom, a website, or a file share\/UNC path. This seems particularly dated, especially with the CD-Rom reference. Also, the Adler Planetarium is deprecating their local file share system in lieu of using Google Drive. This is why we use Google Drive File Stream (see wiki) when publishing, which works _fine_. As WPF isn't necessarily cutting-edge technology, I don't see the publishing process evolving much in the future. While ClickOnce _usually_ works well, we'll have to deal with any errors in deployment as we have our hands tied with using ClickOnce.\\n","completion":"## Decision\nOlder WPF and WinForm applications used a separate setup application, which was responsible for guiding someone through the process of installing an app. A user first downloaded the setup application, which would be responsible for installing the separate application on a system. We abandoned this route early in the development process due to the frequent errors that occurred during the install process.\\nClickOnce is the standard publishing method for WPF applications. Visual studio even references ClickOnce under the \"Security\" tab of the properties window. Documentation on ClickOnce is also quite robust. Because of this, we will accept ClickOnce as our method of choice when publishing.\\n### Status\\nAccepted\\n### Consequences\\nThere aren't many options to choose from when it comes to deploying a WPF application. Unfortunately, if ClickOnce doesn't easily suit our needs, we'll likely have to spend a good bit of time troubleshooting.\\n_In Retrospect:_ Unfortunately, it is recommended that ClickOnce deploys to a CD-Rom, a website, or a file share\/UNC path. This seems particularly dated, especially with the CD-Rom reference. Also, the Adler Planetarium is deprecating their local file share system in lieu of using Google Drive. This is why we use Google Drive File Stream (see wiki) when publishing, which works _fine_. As WPF isn't necessarily cutting-edge technology, I don't see the publishing process evolving much in the future. While ClickOnce _usually_ works well, we'll have to deal with any errors in deployment as we have our hands tied with using ClickOnce.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in\\nthis article:\\n[http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n"}
{"prompt":"## Context\\nAs our API Endpoint will be exposed to the world, we require it to have some sort of protection and authentication against attackers.\\nWe will initially only be connecting internal facing services (AWS) and thrid party tooling, however in the future we would like to begin recording real user metrics (RUM).\\nWe would need to have a way of protecting against attackers spamming our API.\\n","completion":"## Decision\nWe have chosen to use AWS Api Gateway API Keys for our authorisation and usage limits. This will require an AWS Signature being generated from your credentials and API Key.\\nEach service or integration should have their own key.\\nEach service should also set their own usage limits which are contextual to their service. For example, CircleCI integration could be monitored for the average use during a day and have this set, as well as having a secure way to request the keys when they are rotated.\\nWe have also looked at [AWS WAFV2](https:\/\/docs.aws.amazon.com\/waf\/latest\/APIReference\/Welcome.html) and this is something we may look at in the future depending on our usage of the service.\\n"}
{"prompt":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n","completion":"## Decision\nAdd an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nWe define the initial use case for `ml-aoi` spec that exposes assumptions and reasoning for specific layout choices:\\nproviding training data source for Raster Vision model training process.\\n`ml-aoi` STAC Items represent a reified relation between feature rasters and ground-truth label in a machine learning training dataset.\\nEach `ml-aoi` Item roughly correspond to a \"scene\" or a training example.\\n### Justification for new extension\\nCurrent known STAC extensions are not suitable for this purpose. The closest match is the STAC `label` extension.\\n`label` extension provides a way to define either vector or raster labels over area.\\nHowever, it does not provide a mechanism to link those labels with feature images;\\nlinks with `rel` type `source` point to imagery from which labels were derived.\\nSometimes this imagery will be used as feature input for model training, but not always.\\nThe concept of source label imagery and input feature imagery are semantically distinct.\\nFor instance it is possible to apply a single source of ground-truth building labels to train a model on either Landsat or Sentinel-2 scenes.\\n### Catalog Lifetime\\n`ml-aoi` Item links to both raster STAC item and label STAC item.\\nIn this relationship the source raster and label items are static and long lived, being used by several `ml-aoi` catalogs.\\nBy contrast `ml-aoi` catalog is somewhat ephemeral, it captures the training set in order to provide model reproducibility and provenance.\\nThere can be any number of `ml-aoi` catalogs linking to the same raster and label items, while varying selection, training\/testing\/validation split and class configuration.\\n","completion":"## Decision\nWe will adopt the use and development of `ml-aoi` extension in future machine-learning projects.\\n"}
{"prompt":"## Context\\nWe want automated ETE tests on build to keep things consistent. We also want this to be free - because free is good. CircleCI provides a free tier and solid docker support.\\n","completion":"## Decision\nUse CircleCI for automated ETE tests\\n"}
{"prompt":"## Context and Problem Statement\\nThe Identity client app is served from a lightweight Node.js server, which handles OAuth authentication among other things. It achieves this with the Passport auth middleware, and the Auth0 strategy to interact with our auth service.\\nAuth0 [recently launched](https:\/\/auth0.com\/blog\/auth0-s-express-openid-connect-sdk\/) a new SDK for OpenID Connect, an alternative to Passport. In the launch announcement, they stated that development on their own Passport strategy was being reduced to security patches:\\n> ### What\u2019s Going to Happen to Passport-Auth0?\\n>\\n> The answer is\u2026 nothing. Unless someone finds a security bug, that is. Passport-Auth0 is used and loved by many Auth0 customers \u2014 and remains fully supported. There\u2019s no need to rush updates to your existing apps. We will keep fixing bugs and patch them. We plan to stop adding new features to Passport-Auth0, and the bar for the bugs we fix will be tuned accordingly. If you are about to start a new project, we recommend considering using express-openid-connect, as it will be our target for all innovation for web sign-in on the Node.js platform for the foreseeable future.\\n","completion":"## Decision\nFollowing discussion, it is decided that the project will continue to use Passport and no work to integrate the OpenID Connect SDK is currently required.\\n"}
{"prompt":"## Context and Problem Statement\\nNimbus UI uses GraphQL to fetch data from the backend and it highly depends on the three packages i.e. [graphene-django](https:\/\/github.com\/graphql-python\/graphene-django), [graphene](https:\/\/github.com\/graphql-python\/graphene) and [apollo-client](https:\/\/github.com\/apollographql\/apollo-client) and we have seen these packages not getting released new versions for a while. We have found out that using GraphQL significantly hinders the performance of the Experimenter as it adds extra overhead while making the call and the benefit of using GraphQL is not something we are making use of. We also have to significantly maintain the burden to implement most of the API in both DRF (Django Rest Framework) and GraphQL, and additional testing and tools are required to support that. As our application is growing so what steps we should take to improve the performance and better maintainability of the application?\\n## Decision Drivers\\n- Improve performance which leads to better UX Experience\\n- Reduce dependency on extra packages\\n- Better easier development tooling\\n- Better maintainability of the application\\n","completion":"## Decision\n- Improve performance which leads to better UX Experience\\n- Reduce dependency on extra packages\\n- Better easier development tooling\\n- Better maintainability of the application\\nChosen option: After careful consideration, we have decided to choose the option - Designing new pages with HTMX, and keep the existing UI (React and GraphQL), and eventually moving to HTMX. Moving to HTMX is simpler in terms of backend compatibility as we can use our existing code without any major modifications. Work includes creating new pages using HTMX, setting up test cases to test HTMX pages, and evaluating the success\/failure of HTMX on the Experimenter platform, so that if we are successful we can start the transition from React to HTMX completely. Whereas in option 1 we need to actively work on the backend and frontend to transit from GraphQL to REST Apis. This can only work once we have converted everything to Rest Api's i.e. both frontend and backend. Particular V5 API of the experimenter is not used anywhere else apart from the front end, so if we switch to HTMX, we will be not dependent on any kind of APIs and will be closer to the server. The main benefit of choosing option 2 is that it can co-exist with React\/GraphQL, meanwhile, we will update to the latest dependency packages to improve performance and can slowly progress towards 100% HTMX.\\n### Positive Consequences\\n- All web pages using HTMX!\\n- Better performance\\n- Learn new tool HTMX\\n### Negative Consequences\\n- We'd be looking for to determine whether the new pages aren't successful. The one possible solution would be to re-write those pages in Typescript\/React and instead of GraphQL and transition to Rest API.\\n"}
{"prompt":"## Context\\nWe started tracking performance and quality metrics by `content_id`, but after\\nthe first 3 months it was clear that the user needs to track metrics at the\\nbase_path level in Guides and Travel Advice.\\n","completion":"## Decision\nTrack metrics at the base_path level\\n"}
{"prompt":"## Context\\nThis idea was first conceived of in order to fulfill the use case of the\\ncreation of a decentralized Computer Emergency Response Team (dCERT), whose\\nmembers would be elected by a governing community and would fulfill the role of\\ncoordinating the community under emergency situations. This thinking\\ncan be further abstracted into the conception of \"blockchain specialization\\ngroups\".\\nThe creation of these groups are the beginning of specialization capabilities\\nwithin a wider blockchain community which could be used to enable a certain\\nlevel of delegated responsibilities. Examples of specialization which could be\\nbeneficial to a blockchain community include: code auditing, emergency response,\\ncode development etc. This type of community organization paves the way for\\nindividual stakeholders to delegate votes by issue type, if in the future\\ngovernance proposals include a field for issue type.\\n","completion":"## Decision\nA specialization group can be broadly broken down into the following functions\\n(herein containing examples):\\n- Membership Admittance\\n- Membership Acceptance\\n- Membership Revocation\\n- (probably) Without Penalty\\n- member steps down (self-Revocation)\\n- replaced by new member from governance\\n- (probably) With Penalty\\n- due to breach of soft-agreement (determined through governance)\\n- due to breach of hard-agreement (determined by code)\\n- Execution of Duties\\n- Special transactions which only execute for members of a specialization\\ngroup (for example, dCERT members voting to turn off transaction routes in\\nan emergency scenario)\\n- Compensation\\n- Group compensation (further distribution decided by the specialization group)\\n- Individual compensation for all constituents of a group from the\\ngreater community\\nMembership admittance to a specialization group could take place over a wide\\nvariety of mechanisms. The most obvious example is through a general vote among\\nthe entire community, however in certain systems a community may want to allow\\nthe members already in a specialization group to internally elect new members,\\nor maybe the community may assign a permission to a particular specialization\\ngroup to appoint members to other 3rd party groups. The sky is really the limit\\nas to how membership admittance can be structured. We attempt to capture\\nsome of these possiblities in a common interface dubbed the `Electionator`. For\\nits initial implementation as a part of this ADR we recommend that the general\\nelection abstraction (`Electionator`) is provided as well as a basic\\nimplementation of that abstraction which allows for a continuous election of\\nmembers of a specialization group.\\n``` golang\\n\/\/ The Electionator abstraction covers the concept space for\\n\/\/ a wide variety of election kinds.\\ntype Electionator interface {\\n\/\/ is the election object accepting votes.\\nActive() bool\\n\/\/ functionality to execute for when a vote is cast in this election, here\\n\/\/ the vote field is anticipated to be marshalled into a vote type used\\n\/\/ by an election.\\n\/\/\\n\/\/ NOTE There are no explicit ids here. Just votes which pertain specifically\\n\/\/ to one electionator. Anyone can create and send a vote to the electionator item\\n\/\/ which will presumably attempt to marshal those bytes into a particular struct\\n\/\/ and apply the vote information in some arbitrary way. There can be multiple\\n\/\/ Electionators within the Cosmos-Hub for multiple specialization groups, votes\\n\/\/ would need to be routed to the Electionator upstream of here.\\nVote(addr sdk.AccAddress, vote []byte)\\n\/\/ here lies all functionality to authenticate and execute changes for\\n\/\/ when a member accepts being elected\\nAcceptElection(sdk.AccAddress)\\n\/\/ Register a revoker object\\nRegisterRevoker(Revoker)\\n\/\/ No more revokers may be registered after this function is called\\nSealRevokers()\\n\/\/ register hooks to call when an election actions occur\\nRegisterHooks(ElectionatorHooks)\\n\/\/ query for the current winner(s) of this election based on arbitrary\\n\/\/ election ruleset\\nQueryElected() []sdk.AccAddress\\n\/\/ query metadata for an address in the election this\\n\/\/ could include for example position that an address\\n\/\/ is being elected for within a group\\n\/\/\\n\/\/ this metadata may be directly related to\\n\/\/ voting information and\/or privileges enabled\\n\/\/ to members within a group.\\nQueryMetadata(sdk.AccAddress) []byte\\n}\\n\/\/ ElectionatorHooks, once registered with an Electionator,\\n\/\/ trigger execution of relevant interface functions when\\n\/\/ Electionator events occur.\\ntype ElectionatorHooks interface {\\nAfterVoteCast(addr sdk.AccAddress, vote []byte)\\nAfterMemberAccepted(addr sdk.AccAddress)\\nAfterMemberRevoked(addr sdk.AccAddress, cause []byte)\\n}\\n\/\/ Revoker defines the function required for a membership revocation rule-set\\n\/\/ used by a specialization group. This could be used to create self revoking,\\n\/\/ and evidence based revoking, etc. Revokers types may be created and\\n\/\/ reused for different election types.\\n\/\/\\n\/\/ When revoking the \"cause\" bytes may be arbitrarily marshalled into evidence,\\n\/\/ memos, etc.\\ntype Revoker interface {\\nRevokeName() string      \/\/ identifier for this revoker type\\nRevokeMember(addr sdk.AccAddress, cause []byte) error\\n}\\n```\\nCertain level of commonality likely exists between the existing code within\\n`x\/governance` and required functionality of elections. This common\\nfunctionality should be abstracted during implementation. Similarly for each\\nvote implementation client CLI\/REST functionality should be abstracted\\nto be reused for multiple elections.\\nThe specialization group abstraction firstly extends the `Electionator`\\nbut also further defines traits of the group.\\n``` golang\\ntype SpecializationGroup interface {\\nElectionator\\nGetName() string\\nGetDescription() string\\n\/\/ general soft contract the group is expected\\n\/\/ to fulfill with the greater community\\nGetContract() string\\n\/\/ messages which can be executed by the members of the group\\nHandler(ctx sdk.Context, msg sdk.Msg) sdk.Result\\n\/\/ logic to be executed at endblock, this may for instance\\n\/\/ include payment of a stipend to the group members\\n\/\/ for participation in the security group.\\nEndBlocker(ctx sdk.Context)\\n}\\n```\\n"}
{"prompt":"## Context\\nPrevious APIs at Hackney have been written in a variety of languages and frameworks, including Ruby on Rails, C# and ASP.NET, and Node\/Typescript.\\n","completion":"## Decision\nUse C#, ASP.NET and Hackney's Clean Architecture framework, as specified in the [API Playbook](https:\/\/github.com\/LBHackney-IT\/API-Playbook-v2-beta). These applications are intended to be a robust part of HackIT's platform and iterated upon and the HackIT team has mostly C# skillset.\\n"}
{"prompt":"## Context\\n1. \u5f53\u524d\u4f7f\u7528\u5fae\u4fe1\u3001\u90ae\u4ef6\u4f5c\u4e3a\u5de5\u4f5c\u4e2d\u7684\u6c9f\u901a\u5de5\u5177\uff0c\u5de5\u4f5c\u4e0e\u751f\u6d3b\u6df7\u5408\uff1b\\n2. \u5404\u79cd\u544a\u8b66\u4fe1\u606f\u9700\u5206\u522b\u67e5\u770b\u90ae\u4ef6\u3001\u77ed\u4fe1\u6216\u662f\u76d1\u63a7\u5e73\u53f0\uff1b\\n3. \u968f\u7740\u4efb\u52a1\u7ba1\u7406\u5e73\u53f0\u3001Bug \u7ba1\u7406\u5e73\u53f0\u3001wiki\u3001\u6587\u6863\u5206\u4eab\u7f51\u7ad9\u3001\u804a\u5929\u5de5\u5177\u7684\u6d41\u884c\uff0c\u90ae\u4ef6\u7684\u4f7f\u7528\u573a\u666f\u8d8a\u6765\u8d8a\u5c0f\uff0c\u5e76\u4e14\u90ae\u4ef6\u4fe1\u606f\u5e76\u4e0d\u53ca\u65f6\uff1b\\n4. \u901a\u8fc7\u5fae\u4fe1\u8fdb\u884c\u6c9f\u901a\uff0c\u65b0\u4eba\u65e0\u6cd5\u4e86\u89e3\u5386\u53f2\u4fe1\u606f\uff0c\u7ecf\u5e38\u9700\u8981\u628a\u53d1\u8fc7\u7684\u5185\u5bb9\u91cd\u590d\u53d1\u9001\u3002\\n","completion":"## Decision\n### Slack \u652f\u6301\u7684\u529f\u80fd\\n1. **\u516c\u5f00\u4e0e\u79c1\u6709\u7684\u9891\u9053\uff0c\u9891\u9053\u53ef\u968f\u65f6\u52a0\u5165\u6216\u9000\u51fa**\uff1b\\n1. \u6309\u9879\u76ee\uff0cproject-crm, project-sms, \u7b49\uff1b\\n2. \u6309\u6280\u672f\u8bdd\u9898\uff0ctech-restful-api, tech-queue \u7b49\uff1b\\n3. \u53ef\u4ee5\u9080\u8bf7\u76f8\u5173\u4eba\uff0c\u4efb\u4f55\u4eba\u4e5f\u53ef\u968f\u610f\u52a0\u5165\u3002\\n2. **\u6d88\u606f\u652f\u6301\u8868\u60c5\u5feb\u901f\u56de\u590d**\uff1b\\n1. \u8868\u60c5\u662f\u9644\u52a0\u5728\u6d88\u606f\u4e0a\u7684\uff0c\u4e0a\u4e0b\u6587\u5185\u805a\u5f88\u9ad8\u3002\\n3. \u6d88\u606f\u652f\u6301\u6536\u85cf\uff1b\\n1. \u4e00\u4e9b\u4e0d\u9519\u7684\u91cd\u8981\u4fe1\u606f\uff0c\u53ef\u4ee5\u6536\u85cf\u8d77\u6765\u968f\u65f6\u67e5\u770b\u3002\\n4. \u652f\u6301\u5404\u79cd\u6587\u4ef6\u7684\u5206\u4eab\uff1b\\n1. pdf \u7b49\u6587\u4ef6\u5747\u53ef\u9884\u89c8\u3002\\n5. **\u5206\u4eab\u7684\u94fe\u63a5\u652f\u6301\u9884\u89c8**\uff1b\\n1. \u5206\u4eab\u7684\u94fe\u63a5\uff0c\u4e0d\u7528\u70b9\u5f00\u4e5f\u77e5\u9053\u5927\u4f53\u5185\u5bb9\u3002\\n6. \u641c\u7d22\u529f\u80fd\u5f3a\u5927\uff0c\u53ef\u901a\u8fc7\u5feb\u6377\u65b9\u5f0f\uff0c\u641c\u7d22\u540c\u4e8b\uff0c\u6d88\u606f\u8bb0\u5f55\uff0c\u6587\u4ef6\u7b49\uff1b\\n7. \u591a\u79cd\u7a0b\u5e8f\u4ee3\u7801\u652f\u6301\u9ad8\u4eae\uff1b\\n1. \u4ee3\u7801\u9ad8\u4eae\u9884\u89c8\uff0c\u81ea\u52a8\u6298\u53e0\uff0c\u4e0d\u5f71\u54cd\u6574\u4f53\u6548\u679c\u3002\\n8. **\u5f3a\u5927\u7684\u7b2c\u4e09\u65b9\u96c6\u6210\uff0c\u5c06\u6240\u6709\u6d88\u606f\u3001\u901a\u77e5\u6c47\u805a\u5728\u4e00\u5904**\uff0c\u4f8b\u5982\uff0cTrello\uff0cGithub\uff0cNewRelic, Sentry\uff0cJenkins \u7b49\uff1b\\n9. **\u65b0\u52a0\u5165\u8005\u53ef\u67e5\u770b\u7fa4\u7ec4\u5386\u53f2\u4fe1\u606f**\u3002\\n1. \u4fe1\u606f\u518d\u4e5f\u4e0d\u7528\u91cd\u590d\u53d1\u4e86\u3002\\n10. \u5f00\u653e\u6027\u975e\u5e38\u597d\\n1. \u4efb\u4f55\u4eba\u90fd\u53ef\u4ee5\u65b9\u4fbf\u7533\u8bf7\u5f00\u53d1\u8005 KEY\uff0c\u5efa\u7acb\u81ea\u5df1\u7684\u673a\u5668\u4eba\u3002\\n### \u7814\u53d1\u90e8\u9891\u9053\u8bbe\u8ba1\\n1. CI\/CD - \u7528\u4e8e\u63a5\u6536\u6d4b\u8bd5\u3001\u90e8\u7f72\u7ed3\u679c\uff0c\u6d4b\u8bd5\u8986\u76d6\u7387\uff0cPR \u7b49\u4fe1\u606f\uff0c\u4e5f\u7528\u4e8e\u53d1\u8d77\u6d4b\u8bd5\u3001\u90e8\u7f72\u7b49\uff1b\\n2. NewRelic - \u7528\u4e8e\u63a5\u6536\u5e94\u7528\u6027\u80fd\u62a5\u8b66\u7b49\u4fe1\u606f\uff1b\\n3. Sentry - \u7ebf\u4e0a\u5b9e\u65f6\u9519\u8bef\u4fe1\u606f\uff0c\u53ef\u6839\u636e\u9879\u76ee\u5355\u72ec\u62c6\u51fa\u6765\uff1b\\n4. Team-X - \u7528\u4e8e\u7ec4\u5185\u6c9f\u901a\uff0c\u4e00\u4e2a Scrum \u7ec4\uff0c\u5305\u62ec\u7814\u53d1\uff0c\u4ea7\u54c1\u53ca\u6d4b\u8bd5\uff1b\\n5. Knowledge - \u7528\u4e8e\u6240\u6709\u7814\u53d1\u4eba\u5458\u8fdb\u884c\u6c9f\u901a\u4e0e\u5206\u4eab\uff1b\\n6. Product - \u7528\u4e8e\u8de8 Team \u7684\u4ea7\u54c1\u8fdb\u884c\u6c9f\u901a\u4e0e\u5206\u4eab\uff1b\\n7. Backend - \u7528\u4e8e\u6240\u6709\u540e\u7aef\u4eba\u5458\u8fdb\u884c\u95ee\u9898\u54a8\u8be2\u53ca\u5206\u4eab\uff1b\\n8. Leads(\u79c1\u5bc6) - \u7528\u4e8e\u6240\u6709 leader \u8fdb\u884c\u6c9f\u901a\u3001\u5b89\u6392\u5468\u4f1a\u7b49\uff1b\\n9. Frontend, UI, Mobile, Devops, QA etc\\n### \u6211\u4eec\u7528\u4e86\u5982\u4e0b\u7b2c\u4e09\u65b9\u8f6f\u4ef6\\n* Sentry\\n* NewRelic\\n* RedMine\\n* Teambition\\n* Confluence\\n* Github\\n* Bugly\\n* FIR.im\\n* Jenkins\\n* etc\\n### Slack vs BearyChat\\n* Slack \u4f18\u7f3a\u70b9\uff1a\\n* \u9f3b\u7956\\n* \u51e0\u4e4e\u6240\u6709\u5e38\u7528\u8f6f\u4ef6(\u56fd\u5916)\u90fd\u6709\u96c6\u6210\\n* \u529f\u80fd\u5b8c\u5584\u5065\u58ee\uff0c\u516c\u53f8\u53ef\u4fe1\\n* \u7f51\u7edc\u4e0d\u7a33\u5b9a\\n* \u56fd\u5185\u5e94\u7528\u96c6\u6210\u5f88\u5c11\\n* BearChat \u4f18\u7f3a\u70b9\uff1a\\n* \u57fa\u672c\u6709 Slack \u7684\u6838\u5fc3\u529f\u80fd\\n* \u6570\u636e\u7684\u9690\u79c1\u65e0\u6cd5\u4fdd\u8bc1\\n* \u670d\u52a1\u7684\u53ef\u7528\u6027\u65e0\u6cd5\u4fdd\u8bc1\\n* \u7b2c\u4e09\u65b9\u96c6\u6210\u592a\u5c11\uff08must-read, Simple Poll, etc \u90fd\u6ca1\u6709\uff09\uff0c\u5012\u662f\u96c6\u6210\u4e86\u56fd\u5185\u7684 Teambiton, \u76d1\u63a7\u5b9d\u7b49\\n\u6700\u7ec8\uff0c\u6211\u4eec\u7ed9\u56fd\u4ea7\u8f6f\u4ef6(\u670d\u52a1)\u4e00\u4e2a\u673a\u4f1a\u3002\\n\u7ecf\u8fc7\u4e24\u5468\u7684 BearyChat \u8bd5\u7528\uff0c\u76ee\u524d\u5df2\u51c6\u5907\u8f6c\u7528 Slack - 2017-08-03\\n1. BearyChat \u7684 Teambition \u96c6\u6210\u5931\u8d25\uff0c\u6c9f\u901a\u540e\u8bf4\u662f TB \u7684\u63a5\u53e3\u8c03\u6574\uff1b\\n2. \u96c6\u6210\u6570\u7684\u9650\u5236\uff0c\u90fd\u662f 10 \u4e2a\uff0c\u4f46\u540c\u4e00\u4e2a\u7b2c\u4e09\u65b9\u591a\u6b21\u96c6\u6210\uff0cSlack \u7b97\u4e00\u4e2a\uff0cBearyChat \u7b97\u591a\u4e2a\uff0c\u6bd4\u5982 Github, Sentry\uff1b\\n3. \u7ecf\u8fc7\u5bf9\u5404\u4e2a\u56fd\u5185\u516c\u53f8\u7684\u4e86\u89e3\uff0c\u53d1\u73b0\u4f7f\u7528\u4e2d Slack \u7684\u56fd\u5185\u7a33\u5b9a\u6027\u8fd8\u597d\u3002\\n"}
{"prompt":"## Context\\nFrequency Capping allows Content Managers to limit the number of\\nimpressions or interactions users have with content. Is a widely\\navailable tool in Publishing Platforms.\\nIt's usually developed on the server side where the system can decide\\nhow many times to serve the content to the requesting users which we\\ncall \"Global Frequency Capping\". Additionally the system may be able\\nto limit the number of impressions per user which we call \"Local\" or\\n\"User Frequency Capping\".\\nFor example a Content Piece can be set to 1,000,000 Global Impressions\\nand 1 Impression per User, thus indirectly driving 1,000,000 different\\nusers to this Content.\\nThis functionality has been lacking from the Snippet Service due to\\ntechnical limitations imposed by the way metrics were collected and\\ncontent selection was handled on the client side. The latest\\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\\nPipeline unblock this capability. [0]\\n","completion":"## Decision\nWe decide to implement the Frequency Capping functionality into our\\nplatform to allow Content Managers to limit the number of Impressions,\\nClicks and Blocks per Job.\\nLocal or User Frequency Capping will be handled on the Browser level\\nby the Firefox Messaging Platform. The later supports only Impression\\nFrequency Capping.\\nThe Snippets Service will provide an interface (UI) for the Content\\nManagers to set upper limits on the number of Impressions a Job gets\\nper Hour, Day, Week, Fortnight, Month or for the complete Browser\\nProfile Lifetime. This information is included in the JSON generated\\nfor each Job.\\nFor Global Frequency Capping the Snippets Service will provide an\\ninterface (UI) for the Content Managers to set the limits on total\\nworldwide number of Impressions, Clicks and Blocks per Job.\\nSnippets Service will query Mozilla's Redash for Telemetry data every\\nten minutes and will fetch current impressions, clicks, blocks for\\neach Job with set limits.\\nWhen the reported numbers exceed the set limits then, the Job will be\\nmarked COMPLETE and will be pulled out of the Bundles on the next run\\nof `update_jobs` cron job.\\nThe Frequency Capping functionality is additional to the Date\\nPublishing controls, therefore a Job can end on a specific Date and\\nTime or when its Global Frequency Capping Limits are met.\\n### Monitoring and Handling of Errors\\nSince Global Frequency Capping depends on an external system for\\nMetrics (Redash \/ Telemetry) it is possible that the latest numbers are\\nnot always available to the Snippets Service to make a decision. Such\\ncases include scheduled or unplanned service interruptions or network\\nerrors.\\nIn co-ordination with Snippet Content Owner we decided that for cases\\nwhere the Snippets Service cannot get the latest numbers for more than\\n24 hours, Jobs with Global Frequency Capping will get canceled. The\\ncancellation reason will state that the Jobs where prematurely\\nterminated due to missing metrics.\\nThe cron job responsible for fetching the Data from Telemetry is\\nmonitored by a Dead Man's Snitch.\\n"}
{"prompt":"## Context:\\nIt is desirable that we have a way for characters to be randomly generated with different statistics and abilities in order to keep combat fresh and new with an element of surprise. Creating a way to generate random character systematically will keep us from having to create specific new characters.\\n","completion":"## Decision\nCreate a random character factory that will pump out characters.\\n"}
{"prompt":"## Context\\nThe projects structure of the platform can be used to protect components. By the seperation into projects a modular, loosely coupled design is created. A project belongs to a single [domain](0025-domains-correspond-to-business-departments.md), a domain can consist of multiple projects. The project implements a coherent set of functions within a single domain.\\n","completion":"## Decision\nThe set of functions implemented in one GCP project belongs to a single domain.\\n"}
{"prompt":"## Context\\nContext here...\\n","completion":"## Decision\nDecision here...\\n```\\nsd:               { \"guid\": \"ff58025f-6854-4195-9f75-3a3058dd8dcf\",\\n\"typeName\":\\n\"hive_storagedesc\"\\n}\\npartitionKeys:    null\\naliases:          null\\ncolumns:          [ { \"guid\": \"65e2204f-6a23-4130-934a-9679af6a211f\",\\n\"typeName\": \"hive_column\" },\\n{ \"guid\": \"d726de70-faca-46fb-9c99-cf04f6b579a6\",\\n\"typeName\": \"hive_column\" },\\n...\\n]\\n```\\n"}
{"prompt":"## Context\\nWithin SkyPy all functions used to create a \"simulation\" will in practice be taking in some values (either parameters or columns from a table) and creating new column(s) in an output table *or* selecting specific rows from an input table.\\nThe inputs and outputs of these functions are clearly defined so a directed acyclic graph (DAG) can be constructed to determine what order the functions should be run in.\\nTo aid in the creation of the tables and the DAG a helper class or decorator should be used so the person writing the function does not have to worry about the implementation details. This class or decorator is what we are currently referring to as the `Model`.\\nFor clarity in the options below we will assume the following example function:\\n```python\\ndef redshift_gamma(shape, scale, size):\\n\"\"\"Gamma-distributed redshifts (Smail et al. 1994).\\nSample `size` redshifts from a gamma distribution with the\\ngiven `shape` and `scale` parameters. See `numpy.random.gamma`.\\n\"\"\"\\n# redshift distribution\\nredshift = np.random.gamma(shape=shape, scale=scale, size=size)\\nreturn redshift\\n```\\n## Decision Drivers\\n- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\n","completion":"## Decision\n- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/pull\/38) option 3 has been picked.  This will be easiest for developers to write new functions and write clean unit tests.  Within the example given above `pyungo` was just used as an example, other DAG frameworks exist and picking one should be the topic of a different ADR.\\n"}
{"prompt":"## Context\\nURL's worden gestyled zodat deze clickable zijn in het Identify paneel. We willen graag dat ze ook bruikbaar zijn in de tabel.\\nUit de lagen.json heb ik afgeleid dat sommige van deze links veldtype URL hebben en andere veldtype String.\\nWe willen deze URL's graag op dynamische wijze tonen in de tabel (niet gewoon de URL zelf, maar ook niet telkens gewoon LINK).\\nEen groot deel kan getoond worden op basis van de waarde in een ander veld bv dossiernummer bij AV.\\nVoor andere lagen mag er een vaste string getoond worden.\\nEn voor sommige lagen een combinatie van 2 velden als label.\\n","completion":"## Decision\nMomenteel detecteren we links automatisch door naar de start van de string of naar veldtype te kijken. Als het met http of https begint of\\nveldtype 'url' heeft beelden we het af als een link.\\nWe houden voor de tabel vanaf nu rekening met 'html' veld. Indien ingevuld, dan gebruiken we de inhoud van dat html veld om de weergave af te beelden.\\nWe staan toe dat er tokens instaan om bepaalde velden dynamisch\\nin te vullen. Bvb:\\n{ \"isBasisVeld\": true, \"label\": \"Rapport\", \"naam\": \"rapport_url_1\", \"veldType\": \"url\", \"html\": \"{{periode_1_begin}} - {{periode_1_einde}}\" },\\nIndien 'html' leeg is, nemen we de waarde in 'label'.\\nMet het veld \"constante\" moet ook rekening gehouden worden, dat moet soms genomen worden ipv de waarde van het veld. Bvb:\\n{\\n\"isBasisVeld\": true,\\n\"label\": \"Open in Werf\",\\n\"naam\": \"externeurl\",\\n\"veldType\": \"string\",\\n\"html\": \"{werfid}\",\\n\"constante\": \"https:\/\/{domain.name}\/werf\/schermen\/werf\/{werfid};werf=werf%2Fapi%2Fwerf%2F{werfid}\"\\n},\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nOur API layer should communicate with business modules to fulfill client requests. To support the maximum level of autonomy, each module should expose a minimal set of operations (the module API\/contract\/interface).\\n","completion":"## Decision\nEach module will provide implementation for one interface with 3 methods:<\/br>\\n```csharp\\nTask<TResult> ExecuteCommandAsync<TResult>(ICommand<TResult> command);\\nTask ExecuteCommandAsync(ICommand command);\\nTask<TResult> ExecuteQueryAsync<TResult>(IQuery<TResult> query);\\n```\\nThis interface will act as a fa\u00e7ade (Fa\u00e7ade pattern) between API and module. Only Commands, Queries and returned objects (which are part of this interface) should be visible to the API. Everything else should be hidden behind the fa\u00e7ade (module encapsulation).\\n"}
{"prompt":"## Context\\nLearning Machine handles a Blockcerts verifier in multiple repository and with different ways of deploying. This is costly and hard to maintain.\\nDecision has been made to unify the verifier into one sole component and repository, with scalibility and maintainability at heart.\\n","completion":"## Decision\n#### JS\\nWe decided to use Polymer 3.0 as previous versions of the verifier were already written with Polymer. Also because Web Components seem like a promising technology that could open a interesting future for the usage of the component.\\nState of the application will be handled by Redux, as demonstrated in the [example project of Polymer](https:\/\/github.com\/Polymer\/pwa-starter-kit).\\n#### CSS\\nWe decided to use ITCSS for its interesting way to handle CSS scalability and maintainability.\\nMore information about this methodology can be found here:\\nhttps:\/\/github.com\/sky-uk\/css\\nhttps:\/\/www.xfive.co\/blog\/itcss-scalable-maintainable-css-architecture\/\\n#### Testing\\nWe are using the tools provided by the Polymer Project, hence [WCT](https:\/\/github.com\/Polymer\/tools\/tree\/master\/packages\/web-component-tester). We also test for accessibility.\\n#### Accessibility\\nThe Web Component needs to be WCAG2.0 AA compliant.\\n"}
{"prompt":"## Context\\nA deliverable for the SDBM NEH rebuild grant was integration of scanned notes from Seymour de Ricci's unfinished survey of British Manuscript. The value in the cards lies in the notes on the manuscripts of named individuals and organizations that could be linked to or added to the SDBM name authority.\\n","completion":"## Decision\nThe decision was to create a game that allows users to attach randomly selected cards to existing SDBM name entries or identify new names to be added. The data is stored in the SDBM database, but the PDFs of the notes are hosted by the Senate House Library at the University of London. For example,\\n- <https:\/\/dericci.senatehouselibrary.ac.uk\/web_pdf\/dericci_a_f_s.pdf>\\n"}
{"prompt":"## Context\\n*See [frontend architecture sessions](https:\/\/docs.google.com\/document\/d\/1VW3ah5VAvAz2KnqNZlNmVqCzFhBMlIcjPPUlsHMFRIY\/)\\nfor background information on micro-frontend\/SPA architecture.*\\n","completion":"## Decision\nWe are going to merge the following apps together in a *@molgenis-ui\/core* app:\\n* navigator\\n* data explorer 2\\n* data-row-edit\\n"}
{"prompt":"## Context and Problem Statement\\nWe want to base our user interface on map software and calculations in routing engine.\\nWhich map provider should we use?\\n## Decision Drivers\\n* Since this is research project of using free software, we require that the map usage MUST be free.\\n* Required is that the map has supporting library to generate and draw actual routes between two points.\\n* Required is that the map has supporting library or methods to calculate actual routes and time for driving, cycling and walking between two points.\\n","completion":"## Decision\n* Since this is research project of using free software, we require that the map usage MUST be free.\\n* Required is that the map has supporting library to generate and draw actual routes between two points.\\n* Required is that the map has supporting library or methods to calculate actual routes and time for driving, cycling and walking between two points.\\nChosen option: \"[`Openstreetmap`](https:\/\/openstreetmap.org) with service provided by [`OSRM routing engine`](http:\/\/project-osrm.org\/)\", because\\n* Only option that is free for implementatation and does not require registration of any kind.\\n* The product is research only, and not intended to installation to production.\\n* `ja.is` did not provide any documentation about using routing engine and tiles, so it cannot be considered.\\nWe accept that usage of `Google maps` would be lot easier to implement. `Google maps` does host and maintain all required api's and geographical data.\\n`Google directions api` seems to have better geographical data for Iceland than we see in accessable `OSRM routing profiles`\\nIf product was intended for production we must add in our decision conideration about the price of using `Google maps` compared with the cost of hosting and maintaining `OSRM routing engine`.\\n### Positive Consequences\\n* We do not need to pay for usage of map.\\n### Negative Consequences\\n* We need to install, host and maintain `OSRM routing engine`.\\n* We need to update and maintain routing info on local `OSRM routing engine` to avoid outdated map data and route calculations.\\n<hr>\\nContent of decision: Copyright \u00a9 2020 \u00deorleifur Bjarnason, All Rights Reserved.\\n"}
{"prompt":"## Context\\nThe prototype kit will need a way to consume the dependencies it requires to run. These will include think like getting the correct version of `Express`, the framework that prototype kit app is built on, and pulling in the correct version of the NHSUK UI kit assets.\\n","completion":"## Decision\nWe will use NPM (Node Package Manager). Yarn, an alternative Node based package manager, when first released, had some benefits over NPM. These differences have now been implemented in the newer version NPM, Yarn and NPM are now interchangeable. The main benefit of NPM is that it comes packaged with Node.js and does not require an additional install like Yarn, removing a step from the setup process.\\n"}
{"prompt":"## Context\\nThe visualization library contains a `Labels` component, which can be used in the same way as all other axial chart renderers:\\n```jsx\\n<Chart>\\n<Bars ... \/>\\n<Labels ... \/>\\n<\/Chart>\\n```\\n","completion":"## Decision\nHowever, the user can also display labels by passing a `showLabels` flag. This solves multiple problems:\\n- In the case of stacked bars or area charts, using the above API does not take stacking into account, so the labels will be in the wrong places.\\n- The `Labels` component only works for axial charts, not for pie charts, so `PieChart` would have needed a flag anyway - this ensures consistency across all renderers.\\n- For scatter plots, the labels are now automatically offset by the radius of the dots. Any other renderer-specific styling that may be required is now also easier.\\n"}
{"prompt":"## Context\\nIt is preferred to use a single message type for the business events. This makes it easier to handle messages on the pub\/sub system in a standerdized way.\\n","completion":"## Decision\nAll business events on the ODH platform topics are formatted as [JSON](https:\/\/tools.ietf.org\/html\/rfc7159)\\n"}
{"prompt":"## Context\\nWe have encountered probelms with the `xunit` task when run with `msbuild` (`xbuild works`) in that it is unable to locate required xunit assemblies.\\n","completion":"## Decision\nThis can be solved by copying the missing files to the same directory as `msbuild.exe` -- but this way may be easier to consume.\\nMerge the required assemblies into `Conduit.Adapters.Build.dll` or  `Conduit.Adapters.Targets.dll`\\n"}
{"prompt":"## Context\\nWe need a way of deploying our services to our various hosted environments (dev, staging, prod, etc.). We would\\nprefer these deployments to be automated with minimal to zero human interaction.\\n","completion":"## Decision\nThe DIT infrastructure already has a lot of tooling and infrastructure around deployments so we will utilise this. We\\nwill use Jenkins to automatically deploy from dedicated git branches - these will be:\\n- development\\n- staging\\n- uat\\nWe will have 5 Jenkins jobs in total - these will be:\\n- trade-access-program-backoffice\\n- trade-access-program-frontend\\n- trade-access-program-polling-dev\\n- trade-access-program-polling-staging\\n- trade-access-program-polling-uat\\nThe role of the \"polling\" jobs are to watch a related git branch for any commit changes. Once a change is detected then\\nit will trigger the `trade-access-program-backoffice` and `trade-access-program-frontend` jobs with a set of\\nenvironment parameters triggering the deployment to one of our three environments.\\nThis allows us to simply merge or push to one of the three dedicated git branches above and a full automated deployment\\nwill occur for that environment.\\n"}
{"prompt":"## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n","completion":"## Decision\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n"}
{"prompt":"## Context\\nAs with any modern web system there is a need to secure the application with\\nstrong authentication and Role Based Access Control (RBAC). RBAC allows us to\\ncreate permissions, apply them to roles and associates roles with users. From\\nthis we can restrict what users can do and see within the application based on\\ntheir role. In addition to this we also had to investigate where to store role\\ninformation and define teams to support our use of RBAC.\\n","completion":"## Decision\nAfter some investigation and discussion into various ways of implementing RBAC\\nwithin Ruby we have decided to implement the Pundit Ruby gem within the Verify\\nSelf Service application. This gem provides us with a way of creating policies\\nwhich apply to the app as a whole and to individual classes within the\\napplication.\\nWe have chosen pundit for a number of reasons. The first is that it seems\\nrelatively easy to implement within our application. A proof of concept was\\ncreated in a few hours which restricted creating new components within the\\napplication to those who hold the `gds` or `dev` roles. Policies and permissions\\nare defined in code as plain Ruby objects which are easy to understand and\\nsimple to implement. This ease of implementation and an appearance of providing\\nthe fuctionality we are seeking we believe makes pundit a good candidate for\\nimplementing RBAC within our application.\\nIn addition to this we are aware of prior-art use within GDS on the GOV.UK\\nproject. Finally we have found a lot of documentation and articles about how to\\nimplement and use pundit.\\nRole information we have decided to store in a custom field within our cognito\\ninstance. This will take the form of a comma seperated list which will be split\\ninto an array of roles for use within the application. This method allows us to\\nkeep all use information together within AWS Cognito and means that the only way\\nrole information can be changed is via SDK calls to AWS Cognito.\\nFinally we understand there is a need to define teams within our application. We\\nhave decided that a team is analogous to an AWS Cognito group. This allows a\\nuser to be part of one more groups and for a group to hold one or more users. We\\ncan easily query group membership using the AWS Cognito SDK and check for a\\nusers membership to a specific group.\\n"}
{"prompt":"## Context\\nUsing a CSS-in-JS library will allow us to scope styles to a component. Popular CSS-in-JS libraries include Styled Components, Emotion and Styletron.\\nStyletron generates atomic CSS, guaranteeing the smallest possible CSS output. However, it does not support language features that cannot be atomised, such as nesting and sibling selectors.\\nStyled Components and Emotion support more complex features and have a similar API (composing styled components using the `styled()` method). Emotion additionally supports adding styles to the `css` prop on React components. It is also smaller (5.66KB Gzipped vs 16.27KB for styled-components).\\n","completion":"## Decision\nWe will use Emotion as our CSS-in-JS library.\\n"}
{"prompt":"## Context\\nIn A2, we use protobufs to describe our APIs and config. Services talk to each other using clients generated from these protobuf files. The `deployment-service` uses the protobufs for config to allow users to configure A2.\\nWhile this works, it has a few flaws. First, there is no clean separation between a service's implementation and its API. This means that when certain services update a service, but not the API, dependent services must be rebuilt. A similar thing is true for config, where `deployment-service` ends up depending on all services, and chaining any of them causes `deployment-service` to be rebuilt.\\n","completion":"## Decision\nWe will define APIs and config in our top-level `api` directory. This top-level `api` directory will have 3 subdirectories:\\n* external - Our public facing API, which is currently exposed by `automate-gateway`\\n* internal - API definitions for each of our services\\n* config - The config definitions for the services and the global config definition\\nServices that want to communicate with each other should do so by using the clients that are in the `internal` API subdirectory. When taking such a dependency, the service must update the `.expeditor\/config.yml` file with this information. Components which are services should not have dependencies to other components which are services in the `.expeditor\/config.yml` file.\\n"}
{"prompt":"## Context\\nWe want to have API documentation for the code we publish. Rather than writing\\nseparate API docs that quickly get out of date, we would like to be able to\\ngenerate it from our code and comments in our code that live next to the thing\\nthey refer to. For JavaScript, the standard is [JSDoc](https:\/\/jsdoc.app\/), but\\nwith TypeScript, we're already defining the types in our code, and duplicating\\nthat in the associated comments is repeated effort and requires manual action to\\nkeep up-to-date.\\n[TypeDoc](https:\/\/typedoc.org\/) is a documentation generator based on JSDoc. It\\nuses a combination of comments and TypeScripts own types to generate API\\ndocumentation automatically.\\n","completion":"## Decision\nWe will use TypeDoc to generate documentation.\\nWe will document all exported code for the benefit of end users.\\nWe will commit the documentation we generate to the repository alongside changes\\nto behaviour.\\n"}
{"prompt":"## Context\\nDo we need a MarketPlace component?\\n","completion":"## Decision\nIt doesn't look like there is a requirement for a need of a MarketPlace component.  It's decided we will not have one for now.\\n"}
{"prompt":"# Context\\nWe have a single pipeline `create-cloudfoundry` which creates a Cloud Foundry\\ndeployment, and also deploys additional services to the platform.\\nThese services include:\\n- PaaS Accounts\\n- PaaS Admin\\n- PaaS Billing\\n- PaaS Metrics\\nWhich are core components to our platform, but not to Cloud Foundry.\\nCurrently these services are unnecessarily coupled in a couple of places:\\n- The `post-deploy` job\\n- The `custom-acceptance-tests` job\\nUnnecessarily coupling has resulted in flakey app tests blocking CVE\\nremediation from reaching production.\\n# Decision\\nMove, where possible, PaaS services into their own jobs (within the same\\n`create-cloudfoundry` pipeline) such that they do not impede progress of\\ndeployment to the core platform.\\n# Status\\nAccepted\\n# Consequences\\nThe pipeline will no longer be fully controlled by the `pipeline-lock` pool.\\nThe individual jobs in the pipeline will be less mysterious.\\n","completion":"## Decision\nMove, where possible, PaaS services into their own jobs (within the same\\n`create-cloudfoundry` pipeline) such that they do not impede progress of\\ndeployment to the core platform.\\n# Status\\nAccepted\\n# Consequences\\nThe pipeline will no longer be fully controlled by the `pipeline-lock` pool.\\nThe individual jobs in the pipeline will be less mysterious.\\n"}
{"prompt":"## Context\\nReactNative supports both JavaScript and TypeScript.\\n","completion":"## Decision\nJavascript was chosen at this time as it is a more widely used development language and, whilst Babel enables you to use TypeScript, it only enables transpilation of TypeScript and not type checking.\\n"}
{"prompt":"## Context\\nAzure Blob Storage has been proposed as the storage mechanism for CCP case & supporting files, because of the interoperability with other Azure & Microsoft (Dynamics) services.\\nHow the documents are uploaded & stored needs to be considered taking into account the following:\\n1. How will large file uploads be supported that can be restarted in the case of connection error, or resumed in the case of a user closing their browser?\\n2. How can the files be grouped & related to the Dynamics case (permit), in isolation within the Azure Blob Storage service?\\n","completion":"## Decision\nAzure Blob storage supports file uploads via block 'chunks' where a file is split into smaller parts that are uploaded and recommitted into the full file once all\\nblocks have uploaded.  This enables the following:\\n* browser upload limits are avoided as the file is usually split into 4-5 MB chunks\\n* more efficient uploads as multiple chunks can be uploaded in parallel\\n* in the case of the failed or interrupted uploads, the number of successfully uploaded blocks for a file can be requested from Azure.  Using this information the application can then resume an upload by sending only the remaining blocks, before committing the entire file.\\nAzure Blob Storage supports grouping files together into 'containers', essentially a folder structure.  SEPA should create a container for each permit application naming it with the CLAS CAR reference number.\\nAzure Blob Storage also supports file metadata and this should be used to store additional user information (user id) and the CAR reference.\\n"}
{"prompt":"## Context\\nAs the project is an example of a more advanced monolith architecture, it is necessary to save all architectural decisions in one place.\\n","completion":"## Decision\nFor all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\\nEach ADR will be recorded using [Michael Nygard template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\\n"}
{"prompt":"## Context and Problem Statement\\nWe want to use a state of the art database that can store services\/applications according to our data Model and is easy to integrate with [Spring Boot](0001-java-framework.md).\\n## Decision Drivers\\n* Integration with [Spring Boot](0001-java-framework.md)\\n* Different versions of a service will be stored as individual services\\n* Applications will be seen  as concrete instances of services\\n### Data Model Relational\\n```eval_rst\\n.. graphviz::\\ngraph G {\\n{\\nsubgraph cluster_0 {\\nlabel=\"not deployable\"\\nstyle=dotted\\nservice [label=Service, shape=rectangle]\\nserviceID [label=<<U>ID<\/U>>]\\nserviceVersion [label=Version]\\nserviceName [label=Name]\\nserviceExtra [label=\"...\"]\\npredecessor [shape=diamond]\\ndepends [label=\"depends on\" shape=diamond]\\nhas [shape=diamond]\\nversionRange [label=\"Version Range\"]\\nserviceDescription [label=\"Service Description\", shape=rectangle]\\nserviceDescriptionID [label=<<U>ID<\/U>>]\\nserviceDescriptionProtocol [label=Protocol]\\nserviceDescriptionType [label=\"Type (REST, gRPC)\"]\\n}\\nsubgraph cluster_1 {\\nlabel=\"deployable\"\\nstyle=dotted\\nisA [shape=diamond]\\nincludes [shape=diamond]\\napplication [label=Application, shape=rectangle]\\napplicationID [label=<<U>ID<\/U>>]\\ndeployInfo [label=\"Deploy Information\", peripheries=2]\\nvizData [label=\"Visualization Data\", peripheries=2]\\n}\\n}\\nservice -- {serviceID, serviceVersion, serviceName, serviceExtra}\\nservice -- has [label=1]\\nhas -- serviceDescription [label=\"1..n\"]\\nservice -- depends [label=1]\\ndepends -- versionRange\\ndepends -- service [label=\"1..n\"]\\nservice -- predecessor [label=1]\\npredecessor -- service [label=1]\\nserviceDescription -- {serviceDescriptionID, serviceDescriptionProtocol, serviceDescriptionType}\\napplication -- {applicationID, deployInfo, vizData}\\napplication -- includes [label=1]\\nincludes -- service [label=\"1..n\"]\\napplication -- isA [label=1]\\nisA -- service [label=1]\\n}\\n.. important::\\nThe :samp:`Service Description` entity in the model describes a :term:`Service Interface`.\\n```\\n### Data Model Graph\\n```eval_rst\\n.. graphviz::\\ndigraph G {\\n{\\nservice [label=<<U>Service<\/U>>]\\nserviceID [label=<<U>ID<\/U>>, shape=plaintext]\\nserviceVersion [label=Version, shape=plaintext]\\nserviceName [label=Name, shape=plaintext]\\nserviceExtra [label=\"...\", shape=plaintext]\\nx [shape=point]\\nversionRange [label=\"Version Range\", shape=plaintext]\\nserviceDescription [label=<<U>Service Description<\/U>>]\\nserviceDescriptionID [label=<<U>ID<\/U>>, shape=plaintext]\\nserviceDescriptionProtocol [label=Protocol, shape=plaintext]\\nserviceDescriptionType [label=\"Type (REST, gRPC)\", shape=plaintext]\\napplication [label=<<U>Application<\/U>, <U>Service<\/U>>]\\napplicationID [label=<<U>ID<\/U>>, shape=plaintext]\\ndeployInfo [label=\"Deploy Information\", shape=plaintext]\\nvizData [label=\"Visualization Data\", shape=plaintext]\\n}\\nservice -> {serviceID, serviceVersion, serviceName, serviceExtra} [dir=none]\\nservice -> x [dir=none]\\nx -> service [taillabel=\"depends on\"]\\nx -> versionRange [dir=none]\\nservice -> service [label=predecessor]\\nservice -> serviceDescription [label=has]\\nserviceDescription -> {serviceDescriptionID, serviceDescriptionProtocol, serviceDescriptionType} [dir=none]\\napplication -> {applicationID, deployInfo, vizData} [dir=none]\\napplication -> service [label=includes]\\n}\\n.. important::\\nThe :samp:`Service Description` entity in the model describes a :term:`Service Interface`.\\n```\\n","completion":"## Decision\n* Integration with [Spring Boot](0001-java-framework.md)\\n* Different versions of a service will be stored as individual services\\n* Applications will be seen  as concrete instances of services\\n### Data Model Relational\\n```eval_rst\\n.. graphviz::\\ngraph G {\\n{\\nsubgraph cluster_0 {\\nlabel=\"not deployable\"\\nstyle=dotted\\nservice [label=Service, shape=rectangle]\\nserviceID [label=<<U>ID<\/U>>]\\nserviceVersion [label=Version]\\nserviceName [label=Name]\\nserviceExtra [label=\"...\"]\\npredecessor [shape=diamond]\\ndepends [label=\"depends on\" shape=diamond]\\nhas [shape=diamond]\\nversionRange [label=\"Version Range\"]\\nserviceDescription [label=\"Service Description\", shape=rectangle]\\nserviceDescriptionID [label=<<U>ID<\/U>>]\\nserviceDescriptionProtocol [label=Protocol]\\nserviceDescriptionType [label=\"Type (REST, gRPC)\"]\\n}\\nsubgraph cluster_1 {\\nlabel=\"deployable\"\\nstyle=dotted\\nisA [shape=diamond]\\nincludes [shape=diamond]\\napplication [label=Application, shape=rectangle]\\napplicationID [label=<<U>ID<\/U>>]\\ndeployInfo [label=\"Deploy Information\", peripheries=2]\\nvizData [label=\"Visualization Data\", peripheries=2]\\n}\\n}\\nservice -- {serviceID, serviceVersion, serviceName, serviceExtra}\\nservice -- has [label=1]\\nhas -- serviceDescription [label=\"1..n\"]\\nservice -- depends [label=1]\\ndepends -- versionRange\\ndepends -- service [label=\"1..n\"]\\nservice -- predecessor [label=1]\\npredecessor -- service [label=1]\\nserviceDescription -- {serviceDescriptionID, serviceDescriptionProtocol, serviceDescriptionType}\\napplication -- {applicationID, deployInfo, vizData}\\napplication -- includes [label=1]\\nincludes -- service [label=\"1..n\"]\\napplication -- isA [label=1]\\nisA -- service [label=1]\\n}\\n.. important::\\nThe :samp:`Service Description` entity in the model describes a :term:`Service Interface`.\\n```\\n### Data Model Graph\\n```eval_rst\\n.. graphviz::\\ndigraph G {\\n{\\nservice [label=<<U>Service<\/U>>]\\nserviceID [label=<<U>ID<\/U>>, shape=plaintext]\\nserviceVersion [label=Version, shape=plaintext]\\nserviceName [label=Name, shape=plaintext]\\nserviceExtra [label=\"...\", shape=plaintext]\\nx [shape=point]\\nversionRange [label=\"Version Range\", shape=plaintext]\\nserviceDescription [label=<<U>Service Description<\/U>>]\\nserviceDescriptionID [label=<<U>ID<\/U>>, shape=plaintext]\\nserviceDescriptionProtocol [label=Protocol, shape=plaintext]\\nserviceDescriptionType [label=\"Type (REST, gRPC)\", shape=plaintext]\\napplication [label=<<U>Application<\/U>, <U>Service<\/U>>]\\napplicationID [label=<<U>ID<\/U>>, shape=plaintext]\\ndeployInfo [label=\"Deploy Information\", shape=plaintext]\\nvizData [label=\"Visualization Data\", shape=plaintext]\\n}\\nservice -> {serviceID, serviceVersion, serviceName, serviceExtra} [dir=none]\\nservice -> x [dir=none]\\nx -> service [taillabel=\"depends on\"]\\nx -> versionRange [dir=none]\\nservice -> service [label=predecessor]\\nservice -> serviceDescription [label=has]\\nserviceDescription -> {serviceDescriptionID, serviceDescriptionProtocol, serviceDescriptionType} [dir=none]\\napplication -> {applicationID, deployInfo, vizData} [dir=none]\\napplication -> service [label=includes]\\n}\\n.. important::\\nThe :samp:`Service Description` entity in the model describes a :term:`Service Interface`.\\n```\\nChosen option: The graph database Neo4J, because it fits our data model best.\\n"}
{"prompt":"## Context\\nThe project requires dealing with two relatively separate concerns:\\n1. Create topics when needed\\n2. Supply credentials for working with topics.\\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\\nso it makes sense to keep them in the same application.\\n","completion":"## Decision\nWe will ignore the SRP in this instance, and keep the two concerns in the same application.\\n"}
{"prompt":"## Context\\nThe ABCI was first introduced in late 2015. It's purpose is to be:\\n- a generic interface between state machines and their replication engines\\n- agnostic to the language the state machine is written in\\n- agnostic to the replication engine that drives it\\nThis means ABCI should provide an interface for both pluggable applications and\\npluggable consensus engines.\\nTo achieve this, it uses Protocol Buffers (proto3) for message types. The dominant\\nimplementation is in Go.\\nAfter some recent discussions with the community on github, the following were\\nidentified as pain points:\\n- Amino encoded types\\n- Managing validator sets\\n- Imports in the protobuf file\\nSee the [references](#references) for more.\\n### Imports\\nThe native proto library in Go generates inflexible and verbose code.\\nMany in the Go community have adopted a fork called\\n[gogoproto](https:\/\/github.com\/gogo\/protobuf) that provides a\\nvariety of features aimed to improve the developer experience.\\nWhile `gogoproto` is nice, it creates an additional dependency, and compiling\\nthe protobuf types for other languages has been reported to fail when `gogoproto` is used.\\n### Amino\\nAmino is an encoding protocol designed to improve over insufficiencies of protobuf.\\nIt's goal is to be proto4.\\nMany people are frustrated by incompatibility with protobuf,\\nand with the requirement for Amino to be used at all within ABCI.\\nWe intend to make Amino successful enough that we can eventually use it for ABCI\\nmessage types directly. By then it should be called proto4. In the meantime,\\nwe want it to be easy to use.\\n### PubKey\\nPubKeys are encoded using Amino (and before that, go-wire).\\nIdeally, PubKeys are an interface type where we don't know all the\\nimplementation types, so its unfitting to use `oneof` or `enum`.\\n### Addresses\\nThe address for ED25519 pubkey is the RIPEMD160 of the Amino\\nencoded pubkey. This introduces an Amino dependency in the address generation,\\na functionality that is widely required and should be easy to compute as\\npossible.\\n### Validators\\nTo change the validator set, applications can return a list of validator updates\\nwith ResponseEndBlock. In these updates, the public key _must_ be included,\\nbecause Tendermint requires the public key to verify validator signatures. This\\nmeans ABCI developers have to work with PubKeys. That said, it would also be\\nconvenient to work with address information, and for it to be simple to do so.\\n### AbsentValidators\\nTendermint also provides a list of validators in BeginBlock who did not sign the\\nlast block. This allows applications to reflect availability behaviour in the\\napplication, for instance by punishing validators for not having votes included\\nin commits.\\n### InitChain\\nTendermint passes in a list of validators here, and nothing else. It would\\nbenefit the application to be able to control the initial validator set. For\\ninstance the genesis file could include application-based information about the\\ninitial validator set that the application could process to determine the\\ninitial validator set. Additionally, InitChain would benefit from getting all\\nthe genesis information.\\n### Header\\nABCI provides the Header in RequestBeginBlock so the application can have\\nimportant information about the latest state of the blockchain.\\n","completion":"## Decision\n### Imports\\nMove away from gogoproto. In the short term, we will just maintain a second\\nprotobuf file without the gogoproto annotations. In the medium term, we will\\nmake copies of all the structs in Golang and shuttle back and forth. In the long\\nterm, we will use Amino.\\n### Amino\\nTo simplify ABCI application development in the short term,\\nAmino will be completely removed from the ABCI:\\n- It will not be required for PubKey encoding\\n- It will not be required for computing PubKey addresses\\nThat said, we are working to make Amino a huge success, and to become proto4.\\nTo facilitate adoption and cross-language compatibility in the near-term, Amino\\nv1 will:\\n- be fully compatible with the subset of proto3 that excludes `oneof`\\n- use the Amino prefix system to provide interface types, as opposed to `oneof`\\nstyle union types.\\nThat said, an Amino v2 will be worked on to improve the performance of the\\nformat and its useability in cryptographic applications.\\n### PubKey\\nEncoding schemes infect software. As a generic middleware, ABCI aims to have\\nsome cross scheme compatibility. For this it has no choice but to include opaque\\nbytes from time to time. While we will not enforce Amino encoding for these\\nbytes yet, we need to provide a type system. The simplest way to do this is to\\nuse a type string.\\nPubKey will now look like:\\n```\\nmessage PubKey {\\nstring type\\nbytes data\\n}\\n```\\nwhere `type` can be:\\n- \"ed225519\", with `data = <raw 32-byte pubkey>`\\n- \"secp256k1\", with `data = <33-byte OpenSSL compressed pubkey>`\\nAs we want to retain flexibility here, and since ideally, PubKey would be an\\ninterface type, we do not use `enum` or `oneof`.\\n### Addresses\\nTo simplify and improve computing addresses, we change it to the first 20-bytes of the SHA256\\nof the raw 32-byte public key.\\nWe continue to use the Bitcoin address scheme for secp256k1 keys.\\n### Validators\\nAdd a `bytes address` field:\\n```\\nmessage Validator {\\nbytes address\\nPubKey pub_key\\nint64 power\\n}\\n```\\n### RequestBeginBlock and AbsentValidators\\nTo simplify this, RequestBeginBlock will include the complete validator set,\\nincluding the address, and voting power of each validator, along\\nwith a boolean for whether or not they voted:\\n```\\nmessage RequestBeginBlock {\\nbytes hash\\nHeader header\\nLastCommitInfo last_commit_info\\nrepeated Evidence byzantine_validators\\n}\\nmessage LastCommitInfo {\\nint32 CommitRound\\nrepeated SigningValidator validators\\n}\\nmessage SigningValidator {\\nValidator validator\\nbool signed_last_block\\n}\\n```\\nNote that in Validators in RequestBeginBlock, we DO NOT include public keys. Public keys are\\nlarger than addresses and in the future, with quantum computers, will be much\\nlarger. The overhead of passing them, especially during fast-sync, is\\nsignificant.\\nAdditional, addresses are changing to be simpler to compute, further removing\\nthe need to include pubkeys here.\\nIn short, ABCI developers must be aware of both addresses and public keys.\\n### ResponseEndBlock\\nSince ResponseEndBlock includes Validator, it must now include their address.\\n### InitChain\\nChange RequestInitChain to give the app all the information from the genesis file:\\n```\\nmessage RequestInitChain {\\nint64 time\\nstring chain_id\\nConsensusParams consensus_params\\nrepeated Validator validators\\nbytes app_state_bytes\\n}\\n```\\nChange ResponseInitChain to allow the app to specify the initial validator set\\nand consensus parameters.\\n```\\nmessage ResponseInitChain {\\nConsensusParams consensus_params\\nrepeated Validator validators\\n}\\n```\\n### Header\\nNow that Tendermint Amino will be compatible with proto3, the Header in ABCI\\nshould exactly match the Tendermint header - they will then be encoded\\nidentically in ABCI and in Tendermint Core.\\n"}
{"prompt":"## Context\\nIt is possible to install any dependencies for child services in this base\\nimage. Doing so increases the image size, but speeds up child image builds.\\n","completion":"## Decision\nWe will install all common base dependencies in the base image. This includes:\\n* WSGI libraries\\n* Flask and related plugins\\n* Development requirements\\n"}
{"prompt":"## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding\/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n","completion":"## Decision\n### ProofOperator\\n`type ProofOperator` is an interface for Merkle proofs. The definition is:\\n```go\\ntype ProofOperator interface {\\nRun([][]byte) ([][]byte, error)\\nGetKey() []byte\\nProofOp() ProofOp\\n}\\n```\\nSince a proof can treat various data type, `Run()` takes `[][]byte` as the argument, not `[]byte`. For example, a range proof's `Run()` can take multiple key-values as its argument. It will then return the root of the tree for the further process, calculated with the input value.\\n`ProofOperator` does not have to be a Merkle proof - it can be a function that transforms the argument for intermediate process e.g. prepending the length to the `[]byte`.\\n### ProofOp\\n`type ProofOp` is a protobuf message which is a triple of `Type string`, `Key []byte`, and `Data []byte`. `ProofOperator` and `ProofOp`are interconvertible, using `ProofOperator.ProofOp()` and `OpDecoder()`, where `OpDecoder` is a function that each proof type can register for their own encoding scheme. For example, we can add an byte for encoding scheme before the serialized proof, supporting JSON decoding.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nPartner organizations work directly with the families in need. Sometimes they will need to collect PII such as address, names and ages of family members, and other contact information. This data is not necessary for Diaperbase to operate, and there are security concerns about that PII being disclosed should there be a breach. There is also a separation of concerns between the two applications; Diaperbase is inventory management and Partnerbase is effectively a CRM. At this time, we belive that they are different enough in their purposes that they should be separate.\\n","completion":"## Decision\nA new application, Partnerbase, will be created to handle the CRM aspects. It will communicate over a private API with Diaperbase to handle request fulfillment.\\n"}
{"prompt":"## Context\\nThe [PropertyInfoStore] interfaces with the [wb_property_info] DB table holding information about wikibase properties.\\nThe [CacheAwarePropertyInfoStore] stores the whole of the [wb_property_info]  table in a single cache key.\\nThe Wikibase wiring is setup to provide this CacheAware store using the default cache (memcached for WMF).\\nThe [CacheAwarePropertyInfoStore] has a high number of reads, and the method of storing the whole table in a single key\\nresults in lots of traffic to a single memcached instance as described in [T97368].\\nThe amount of traffic for the memcached key has steadily grown as the number of properties in the store have grown.\\nThis traffic also moves between memcached servers after each WMF deploy as the cache key changes.\\n","completion":"## Decision\nA layer of APC caching (per server) is added on top of the shared memcached caching.\\nThis is done in the service wiring by wrapping our [CacheAwarePropertyInfoStore] in another [CacheAwarePropertyInfoStore].\\nThis on APC cache has a short TTL to avoid the need to actively think about purging.\\nAdding this extra layer of caching was chosen rather than anything more drastic as it is a trivial code change vs re-working how the [CacheAwarePropertyInfoStore] works.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context and Problem Statement\\nThe GLAM data consists mainly of a histogram, percentiles, and counts for each\\nset of dimensions. This can be stored in a file whose path and name can\\nrepresent the dimensions and whose content can contain the actual data.\\n## Decision Drivers\\n- The data should be efficient to look up and read for the front-end\\n- The data should be efficient to write for the ETL\\n- The data should be cost efficient to write and maintain\\n","completion":"## Decision\n- The data should be efficient to look up and read for the front-end\\n- The data should be efficient to write for the ETL\\n- The data should be cost efficient to write and maintain\\nMoving the data into Postgresql was chosen, as outlined in the pros\/cons below.\\nTo summarize, the need to update the file as new aggregation data was produced\\nfor the same set of dimensions causes a file based approach to be more complex\\nand something databases were created to solve.\\nThe SQL approach of `INSERT ... ON CONFLICT DO UPDATE ...` accomplishes this\\nwell.\\n"}
{"prompt":"## Context\\nNginx is a webserver which can work on many ways, from proxy, to reverse proxy, load balancer and traditional web server.\\nWe need a configuration which suits the need for PHP fpm under sockets\\n","completion":"## Decision\nThe default configuration and custom variables will be shaped for PHP fpm needs, allowing an easy plug-and-play for these kind of projects.\\nThis includes using a shared PHP fpm socket under `\/var\/run`.\\n"}
{"prompt":"## Context\\nThere are a couple reasons to switch from multiple buckets to a single bucket. The first is that it simplifies the infrastructure provisioning that needs to be done when a new source is added. The second is that there is a hard limit on the number of buckets an AWS account can have, and our current approach to bucket creation is unsustainable.\\n","completion":"## Decision\nUse a single namespaced S3 bucket for all source data. The structure of the bucket should be:\\n```\\ns3:\/\/bucket\/<environment>\/<source>\/<files>\\n```\\nWhere `environment` would be either `prod` or `stage`, and `source` would be the source identifier. The source identifier used here should also be used as the prefix for the index name. No specific decisions are made here about how the files are structured within a source.\\nmario should ignore source identifiers that it does not know about.\\n"}
{"prompt":"## Context\\nOften a binding will need to provide notifications to users for binding specific administrative purposes. This is not linked to `Item` data, but is designed to provide users feedback on binding specific functions or alerts. Examples of this could be -:\\n* Alerting the user about failed joining, or a trust issue with devices joining\\nIn general, these notifications are designed for display on an administration UI rather than a user UI, but there is of course nothing to prevent any UI from subsribing for such events.\\nThe concept is to provide a generic binding notification that allows a binding to send a simple notification to users that can be interpretted by UIs in a standard way. This should provide enough information to allow the UI to present the notification in a standard way - showing the severity, and a notification text.\\nThe alerting system may be as simple as real-time notifications direct to the UI. In such case, if the administrative user is not watching the UI, notifications would be lost and not persisted. An enhancement may be to run these through a registry which would allow notifications to be registered and actively dismissed once read by a user with the appropriate access level. At this point, the notification may be removed, or archived to provide an audit trail.\\nFurther extensions to the notification system may be considered to provide images, action buttons etc are considered outside of the scope of this initial proposal.\\n","completion":"## Decision\nTBD\\n"}
{"prompt":"## Context\\nWe provide a CD Pipeline for SAP Cloud Platform applications, which adheres to the \"build once\" principle.\\nStill, the pipeline does each build step, like building the artifact, running tests or static code checks in separate stages.\\nWe use Maven for building the application, using Gradle or other build tools is not an option without much rework.\\nFor this document, the term \"release\" (noun) refers to a uniquely identifiable version of software.\\nThis includes the source code version (commit or tag) from which the artifacts are built, and the build artifacts themselves.\\nThe verb \"to release\" refers to the process of creating a new release.\\nPart of this process is to determine the version number of the release candidate.\\nThe release candidate becomes a release, when its build pipeline succeeded, and the build artifact is deployed to the Cloud Platform and the artifact repository.\\nWith Maven, this is usually facilitated with the [Maven Release Plugin](http:\/\/maven.apache.org\/maven-release\/maven-release-plugin\/).\\nUsing this plugin does not satisfy our requirements as described below.\\nThe pipeline automatically uploads build artifacts to an artifact repository and deploys the app to the Cloud Platform.\\nArtifact uploads and deployments happen only for commits on the so co called \"productive branch\" (`master` by default).\\nMaven's versioning schema appends `SNAPSHOT` suffix to versions which are not released.\\nA version like `1.0.2-SNAPSHOT` does not say from which commit this was built.\\nArtifact repositories might delete `SNAPSHOT` versions after some time, because those are not releases.\\n","completion":"## Decision\nWe implement an automated versioning schema, in which each commit to the productive branch is equivalent to a new release.\\nThis feature is enabled by default, but can be disabled.\\nThe version number shall contain a human readable _build_ time stamp (ISO 8601, without colons for file-name compatibility on Windows, always `UTC`) and the git commit id of the most recent commit to `master`, for example `2.7.3-2018-03-02T114757UTC_ff46bb0f00a663018f3efea697b2fb5e86fe6d41`.\\nAn auto-created release does not imply creating a tag in the repository.\\nCreating tags may be done manually to mark noteworthy versions by the developer.\\n### Reasoning\\n* Each commit on `master` is a new release: We assume the work happens in feature branches, which are merged once they implement a feature and meet the team's definition of done.\\nMerging to `master` is implicitly approval for release.\\n* Feature can be disabled: You might still have builds which don't follow this release approach.\\nFor those, it must be possible to disable automatic versioning.\\n* _Build_ instead of _commit_ time stamp: This implies that multiple builds of the same commit have a different version number.\\nThis avoids conflicts, when uploading a second build of a commit to a artifact repository.\\n* Always ISO 8601 date-time format: Can be sorted in lexical order which results in a chronological list.\\n* Always `UTC`: Most simple solution, avoids daylight saving time issues and is unambiguous for teams working distributed in multiple time zones.\\n* Don't create git tags: The version number contains the commit id, which is sufficient to check out this particular version.\\nIf we created tags automatically for each version, tags would be cluttered very quickly.\\nTags still can be used to mark a version on purpose, with semantic versioning if desired.\\n"}
{"prompt":"## Context\\nBrowsers often have subtle differences or bugs in their default styles.\\nTraditional CSS resets attempt to clear all styling, creating a baseline style\\nfrom which to build us custom styles. However, this approach often results in\\nreapplying styles that were already implemented by the browser.\\n[Normalize.css](http:\/\/necolas.github.io\/normalize.css\/), instead, normalizes\\nstyles to a common theme across browsers. It precisely targets only the styles\\nthat need normalizing.\\n","completion":"## Decision\nWe will use Normalize.css to normalize browser default styles.\\n"}
{"prompt":"## Context\\n`service-stac` is a new service with use cases that are different to the ones in all existing services, specially the fully CRUD (create, read, update, delete) REST interface. Therefore, the choice of the application structure cannot simply be borrowed from an existing service. The use cases for this service will involve heavy read\/write access to data via JSON REST API interface of a substantial amount of assets and metadata objects (tens to hundreds of Millions). Additionally, data must be editable manually, at least in a start\/migration phase.\\n","completion":"## Decision\nThe following decisions are taken regarding the application architecture\\n- Programming Language **Python**: Python is used in most of the backend services and is the programming language that's best known within devs, so no reason to change here.\\n- Application Framework **Django**: Django is used as application framework. Django is very mature and has a wide user community. It comes with excellent documentation and a powerfull ORM. Futhermore, there are well-supported and maintained extensions, e.g. for designing REST API's that can considerably reduce the amount of boilerplate code needed for serializing, authentication, ....\\n- Asset Storage **S3**: Since this service runs on AWS assets will be stored in S3 object store.\\n- Metadata Storage **PostGIS**: Since Metadata can contain arbitrary geoJSON-supported geometries, Postgres along with the PostGIS extension is used as storage for metadata.\\nThe application architecture does initially only involve syncronous operations. Async tasks will be reconsider once certain write operations don't meet performance requirements anymore.\\n"}
{"prompt":"## Context\\nContext here...\\n","completion":"## Decision\n\u4f7f\u7528 Github \u8fdb\u884c Code Review\\n"}
{"prompt":"## Context\\nThe PACE projects will evolve over time and breaking changes will be introduced. Users will need to be able to easily finding the correct documentation for their build.\\n[Semantic versioning](https:\/\/semver.org\/) defines a schema in which releases are given `major.minor.patch` version numbers where increments are made to the:\\n- `major` version with incompatible API changes,\\n- `minor` version when functionality is added in a backwards compatible manner, and\\n- `patch` version for backwards compatible bug fixes.\\nSphinx plugins are available that support multiple documentation versions ([sphinx-multiversion](https:\/\/pypi.org\/project\/sphinx-multiversion\/)) - this will not work correctly for Brille where documentation is extracted from the build artifacts.\\n","completion":"## Decision\nProjects will be semantically versioned and documentation will be retained and be available for users to access for each major or minor version released.\\nThere is no requirement to have patch-release specific documentation.\\n"}
{"prompt":"## Context\\nOur pipeline follows a continuous delivery method with the manual gate to release to production.\\n![alt text](0010-release-pipeline-context.png \"Diagram of how the editor is released through the pipeline.\")\\nWe have one Editor in test which causes issues when the rest of the team want to test a feature. Also, while the Editor is tied up in testing the pipeline is blocked for deploying to live-production.\\n","completion":"## Decision\n![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")\\nThe main problem to solve is having a feature that is being developed available to the wider team to test. Running locally was not considered as the MOJ Macbooks are locked down to those outside of the technical disciplines.\\nChanging the pipeline to create an Editor (all plugged in to the dev and test environments). This will involve, building and deploying the editor code into a pod and attaching to a unique URL. Not every branch will need to create a new test editor, the developer will need to add \"testable-\" to the beginning of the branch name (i.e. `testable-my-really-awesome-feature`).\\nThe Editor will use the current auth mechanism unless the effort outweighs the investment. At a mimimum, basic auth of a set username and password must be added.\\nTest Editor must be cleanly removed after it has fullfilled it's purpose. This should ideally be automated either triggered by merging the branch or via commandline.\\n"}
{"prompt":"## Context\\nIn `cluster.py` we used to install all the `kube-system` components using a `systemd` unit. This consisted basically in a bash script that deployed all the manifests from `\/srv\/kubernetes\/manifests\/*\/*.yaml` using `kubectl`.\\nWe obviously do not want to update versions manually via kubectl. Furthermore, this approach also meant that we had to launch a new master instance in order to apply the updated manifests.\\n","completion":"## Decision\nWe will do the following:\\n- remove entirely the \"install-kube-system\" unit from the master user data.\\n- create a folder with all the manifests for each of the kubernetes artifact\\n- apply all the manifests from the Cluster Lifecycle Manager code\\nSome of the possible alternatives for the folder structures are:\\n1. \/manifests\/APPLICATION_NAME\/deployment.yaml - which uses a folder structure that includes the APPLICATION_NAME\\n2. \/manifests\/APPLICATION_NAME\/KIND\/mate.yaml - which uses a folder structure that includes APPLICATION_NAME and KIND\\n3. \/manifests\/mate-deployment.yaml - where we have a flat structure and the filenames contain the name of the application and the kind\\n4. \/manifests\/mate.yaml - where mate.yaml contains all the artifacts of all kinds related to mate\\nWe choose number 1 as it seems the most compelling alternative.\\nNumber 2 will only introduce an additional folder level that does not provide any benefit. Number 3 will instead rely on a naming convention on the given kind.\\nNumber 4, instead, is a competitive alternative to number 1 and could be adopted, but we prefer to go with number 1 as this is very flexible and probably more readable for the maintainer.\\nFor the file naming convention, we recommend to split in files for kind when is possible and put the name (or just a prefix) in the file name. We will not make any assumption on the file naming scheme in the code.\\nAlso, no assumption will be made on the order of execution of such files.\\n"}
{"prompt":"## Context\\nWhen something goes wrong, the API should respond with as meaningful information as possible, to allow developers to pinpoint the source of the problem.\\n","completion":"## Decision\nIf a valid request comes in for data, we show data. If creating a resource, we show the created resource, along with HATEOAS links showing what a client may do with it.\\nIf something goes wrong, we use two simultaneous approaches\\n* Return an appropriate HTTP status code\\n* Return one or more custom error codes and messages, in order to offer more targeted error information For this we refer to [Problem Details for HTTP APIs](https:\/\/tools.ietf.org\/html\/rfc7807)\\n> HTTP [RFC7230] status codes are sometimes not sufficient to convey enough information about an error to be helpful.\\n### HTTP status codes\\nWithout overdoing it, here is a simple set of error codes that we will implement:\\n* 200 - Generic everything is OK\\n* 201 - Created something OK\\n* 202 - Accepted but is being processed async (eg video transcoding)\\n* 301 - Moved Permanently (although we would provide the new link in with HATEOAS also)\\n* 400 - Generic bad request (generally invalid syntax)\\n* 401 - Unauthorised (no current user and there should be)\\n* 403 - Forbidden - The current user is forbidden from accessing this data (in this way)\\n* 404 - That URL is not a valid route, or the item resource does not exist\\n* 405 - Method not allowed\\n* 410 - Data has been deleted, deactivated, suspended etc\\n* 413 - Payload too large\\n* 415 - Unsupported media type\\n* 500 - Something unexpected happened and it is the API's fault\\n* 503 - Service Unavailable - please try again later (although the client might choose to queue and retry any request which returns a 5xx)\\n### Multiple errors\\n[JSON-API states](https:\/\/jsonapi.org\/format\/#errors):\\n> A server MAY choose to stop processing as soon as a problem is encountered, or it MAY continue processing and encounter multiple problems. For instance, a server might process multiple attributes and then return multiple validation problems in a single response.\\nWith `opg-data` API **we will always endeavour to continue to encounter and provide feedback on an array of all errors** in a single response, rather than feeding errors out singly, causing the consumer to discover a new error with each subsequent request. Painful.\\n### <a name=\"errors-in-20x\"><\/a> 200 and 201 means success - with ONE exception\\nA 200 or 201 status code offers certainty to the API consumer that their operation was **A SUCCESS**. There should **never** be an error code\/message returned as a response to a 20x.\\n**With ONE exception**... (appropriated from Twitter):\\n> In some cases you may see the errors detailed above in a response that returned a 200 status code. In those cases, the endpoint is designed to return the data that it can, while providing detailed errors about what it could not return.\\nSo, for GET queries where a range of resources are attempted, we return those resources that could be successfully retrieved, and provide error messages about those that couldn't be.\\n### Error Formatting\\n#### an examination of the application\/problem+json format\\nIt is found that our proposed error format satisfies the requirements of the `application\/problem+json` type specified in [Problem Details for HTTP APIs](https:\/\/tools.ietf.org\/html\/rfc7807):\\n```json\\nHTTP\/1.1 404 Not Found\\nHost: api.example.com\\nContent-Type: application\/problem+json\\n{\\n\"type\": \"URI to problem type\",\\n\"title\": \"Human readable reason\",\\n\"detail\": \"Human readable occurance-specific details\",\\n\"instance\": \"path to specific occurance\",\\n\"balance\": \"problem-specificic extension\",\\n\"accounts\": [\"problem-specificic extension\",\\n\"problem-specificic extension\"]\\n}\\n```\\n> A problem details object can have the following members:\\n> * \"type\" (string) - A URI reference [RFC3986] that identifies the problem type.  This specification encourages that, when dereferenced, it provide human-readable documentation for the problem type (e.g., using HTML [W3C.REC-html5-20141028]).  When this member is not present, its value is assumed to be \"about:blank\".\\n> * \"title\" (string) - A short, human-readable summary of the problem type.  It SHOULD NOT change from occurrence to occurrence of the problem, except for purposes of localization (e.g., using proactive content negotiation; see [RFC7231], Section 3.4).\\n> * \"status\" (number) - The HTTP status code ([RFC7231], Section 6) generated by the origin server for this occurrence of the problem.\\n> * \"detail\" (string) - A human-readable explanation specific to this occurrence of the problem.\\n> * \"instance\" (string) - A URI reference that identifies the specific occurrence of the problem.  It may or may not yield further information if dereferenced.\\nWe can adopt the ethos of JSON-API (for consistency across our API) whilst also providing the information set out in the `application\/problem+json` type, above.\\n#### OPG-Data error format\\nAs per our regular content structure [0005-content-structure.md](0005-content-structure.md)\\n> At the root level is always a JSON object.\\n> This **MUST** contain at least one of the following root-level members:\\n> * data: the document's \"primary data\" resource object\\n>   * A single resource object is represented by a JSON object\\n>   * A collection or resource objects is represented by an array of objects\\n> * errors: an array of error objects\\n> [JSON-API](https:\/\/jsonapi.org\/format\/#errors) states:\\n> Error objects **MUST** be returned as an array keyed by errors in the top level of a JSON:API document.\\n"}
{"prompt":"## Context\\n16-25 April, Thabang is away, must work remotely. Major submission period over 29th April - 10th May, will impact productivity on software project.\\n","completion":"## Decision\n4 Sprints planned, consecutively. Will only start on 17th April, but have a \"Sprint 0\" from 9th to 16 April, with initial planning, research and folder structure creation, setup of Azure and Travis. Will not count towards actual number of sprints.\\n* Sprints will begin on Wednesdays, with a 1 hr review session in the morning.\\n* There will be a three hour sprint planning session in the afternooon each Wednesday.\\n* Release will be every Tuesday, by 8pm\\n* Product release window will be from 2pm - 8pm, with all pull requests done before 6pm to give time for any required last minute code review and testing\\n* Friday coding sessions together from 12pm - 4pm\\n* Standups via Whatsapp, or between lectures. Preferable to do in person but may not be possible.\\nRebecca: Product Manager\\nTyson: SCRUM Master\\nTheese roles will be alternated throughout the project each week.\\n"}
{"prompt":"## Context\\nWe need [dependency injection](https:\/\/stackoverflow.com\/questions\/130794\/what-is-dependency-injection) in order to build decoupled software units.\\nAlso we need access on a relational database.\\n","completion":"## Decision\nSpring offers a lot of convenience features for those purposes.\\nApache Wicket already provides a Spring integration.\\nWith the help of the [wicket-spring-boot](https:\/\/github.com\/MarcGiffing\/wicket-spring-boot) the convenience feature of Spring Boot can be applied on top of that.\\nThis makes the development faster and easier than using the alternative of dependency injection with [Google Guice](http:\/\/software.danielwatrous.com\/wicket-guice-including-unittests\/).\\nFurther usage of Spring enables potential useage of the various Spring sub-projects for advanced requirements like the integration of other software systems.\\n"}
{"prompt":"## Context\\nProject documentation must be published on the internet.\\n","completion":"## Decision\nUse [Jekyll](https:\/\/jekyllrb.com\/) on our servers to serve these documents just like GitHub Pages do.\\n"}
{"prompt":"## Context\\nCMS Export initiative was an attempt to increase VA.gov content capacity and decrease content deployment time.\\nAs of Feb 2021 CMS Export implementation was not expected to be completed in time for planned product (VAMC) rollout.\\nTesting and experimentation with GraphQL indicated it might be able to scale to support product release timeline.\\n","completion":"## Decision\nHalt implementation on CMS Export and focus on scaling GraphQL.\\n"}
{"prompt":"## Context\\ndxw have approved an RFC for following the pattern of Scripts To Rule Them\\nAll[1].\\nThis repository should include reference and document this decision.\\n[1]\\nhttps:\/\/github.com\/dxw\/tech-team-rfcs\/blob\/main\/rfc-023-use-scripts-to-rule-them-all.md\\n","completion":"## Decision\nWe will follow the Scripts to Rule Them All pattern for common tasks in this project.\\n"}
{"prompt":"## Context\\nDockerfiles make no provision for extracting duplication of steps unless they are rendered from a template. Similar `RUN` steps can multiply across different files.\\n`RUN` steps also allow no logic or encapsulation, and promote long chains of commands due to the necessity of producing a single layer.\\n`RUN` steps are not testable in isolation or re-runnable inside an image for debugging.\\n","completion":"## Decision\nExtract long `RUN` steps (or sequences of steps) into a bash script. If the script is only to be used by `root`, place it in `\/root\/scripts`.\\n"}
{"prompt":"## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n","completion":"## Decision\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript\\n\/\/ Good:\\n<Box padding\"small\" \/>\\n<Box padding={{xs: 'small', md: 'medium'}} \/>\\n<Input large \/>\\n<Text preset=\"heading3\" as=\"p\" \/>\\n```\\n```typescript\\n\/\/ Bad:\\n<Box className={customLayout} \/>\\n<Input style={{ height: 50, padding: 16 }} \/>\\n<Text className={styles.heading} \/>\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from Braid, which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n"}
{"prompt":"## Context\\nA consistent and well-defined document specification is required so that we may develop an API contract\\n","completion":"## Decision\nOur structure closely follows the [JSON-API](https:\/\/jsonapi.org\/format\/#document-structure) document structure\\n### Document Root Level\\nAt the root level is always a JSON object.\\nThis **MUST** contain at least one of the following root-level members:\\n* data: the document's \"primary data\" resource object\\n* A single resource object is represented by a JSON object\\n* A collection or resource objects is represented by an array of objects\\n* errors: an array of error objects\\n#### Single resource object\\n```json\\n{\\n\"data\": {},\\n\"meta\": {}\\n}\\n```\\n#### Collection of resource objects\\n```json\\n{\\n\"data\": [\\n{...},\\n{...}\\n],\\n\"errors\": [],\\n\"meta\": {},\\n\"links\": {}\\n}\\n```\\nJSON-API states\\n> The members data and errors MUST NOT coexist in the same document.\\n`opg-data` standard is that if a data resource OBJECT is returned, then there **MUST be no** error member.\\nHowever there ARE certain circumstances where an array of errors **MAY** be returned alongside a data resource COLLECTION. See [0007-error-handling-and-status-codes.md#errors-in-20x](0007-error-handling-and-status-codes.md#errors-in-20x)\\nThe root JSON object **MAY** also contain the following root-level members:\\n* meta: a meta object that contains non-standard meta-information\\n* links: a links object related to the primary data (typically used for pagination links if the data returned is a collection)\\n### The Resource Object\\nSee [JSON-AP](https:\/\/jsonapi.org\/format\/#document-resource-objects)\\nNamely:\\n* As a minimum, every resource object **MUST** contain:\\n* an id member\\n* a type member. The values of the id and type members **MUST** be strings. For consistency and the avoidance of confusion, types **MUST** use PLURAL. eg \"articles\", \"people\"\\n* an array of attributes (even if empty)\\n* IF the \"data\" top level member is present, then there **MUST** be a `links` array, containing as it's minimum, a `self` member with a URL which **MUST** be callable at the API\\n* A resource object's data attributes is presented in an array named \"attributes\"\\n* A resource object's links is presented in an object named \"links\"\\n* A resource object's relationships is presented in an array named \"relationships\"\\n* A resource object's attributes and its relationships are collectively called its \"fields\"\\n```json\\n{\\n\"data\": [\\n{\\n\"type\": \"articles\",\\n\"id\": \"1\",\\n\"attributes\": {\\n\"title\": \"My First Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/1\",\\n\"next\": \"https:\/\/api.example.com\/articles\/2\"\\n},\\n\"relationships\": {\\n...\\n}\\n},\\n{\\n\"type\": \"articles\",\\n\"id\": \"2\",\\n\"attributes\": {\\n\"title\": \"Second Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/2\",\\n\"prev\": \"https:\/\/api.example.com\/articles\/1\"\\n},\\n\"relationships\": {\\n...\\n}\\n}\\n],\\n\"links\": {},\\n\"meta\": {}\\n}\\n```\\n#### Relationships\\nRelationships will be defined in a separate ADR\\n### Meta\\nSee [JSON-API](https:\/\/jsonapi.org\/format\/#document-meta)\\n### Errors\\nErrors responses will be defined in a separate ADR\\n"}
{"prompt":"## Context\\n* Case management system requires secure cloud storage to store case related documents.\\n* GOV.UK  Ruby applications often use [Amazon S3 storage and Rails Active Storage](https:\/\/docs.publishing.service.gov.uk\/manual\/conventions-for-rails-applications.html#header) together and as it is preferred technical choice.\\n* A private aws-s3-bucket would be provisioned on Gov PaaS and only accessible via rails app  bind with the service.\\n","completion":"## Decision\n* Use Amazon S3 storage service on Gov PaaS and Rails Active Storage\\n"}
{"prompt":"## Context\\nIn the [original proposal](https:\/\/docs.google.com\/document\/d\/1EYRBVuQedU1r0zexgi8oMSOEFgaMNzM8JWBje3XuweU) for basic http\\ningress routing for CF on Kubernetes, we proposed writing a controller that wrote custom Route resources to the Kubernetes\\nAPI. Additionally we would develop a second controller that read the Route CRDs and would create k8s Services and Istio\\nVirtualServices.\\nWe discovered several issues with this design. First, we realized that we must have a single VirtualService per FQDN.\\nWhile multiple VirtualServices for the same FQDN are [technically permitted by Istio](https:\/\/istio.io\/docs\/ops\/traffic-management\/deploy-guidelines\/#multiple-virtual-services-and-destination-rules-for-the-same-host),\\nthe order in which the match rules for the paths are applied is non-deterministic. In CF we expect that the longest path\\nprefix is matched first, so this behavior did not suit our needs.\\nSince we had to aggregate multiple Route resources to construct a single VirtualService, this meant we could not use\\nMetacontroller for our second controller. Having multiple \"parent\" Routes for a single set of \"children\" VirtualServices would\\nviolate Metacontroller's assumptions.\\nWhile we could build a custom second controller using Kubebuilder, we decided that for simplicity and expediency we could\\njust omit the creation of the Route CRDs for the time being.\\n","completion":"## Decision\n* CF Route Syncer will directly create k8s Services and Istio VirtualServices instead of creating intermediate Route CRDs\\n"}
{"prompt":"## Context\\n[Command-query separation](https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n","completion":"## Decision\nCommand-query separation will be enforced in the [core hexagon](.\/0003-adopt-hexagonal-architecture.md), especially in\\napplication services.\\n"}
{"prompt":"# Search APIs as separate Bounded Context\\n### **Date:** 30th March 2021\\n### **Status:**  ACCEPTED\\n## **Context**\\nThere are use cases for advanced search functionalities (free text search, partial matching, wildcards, filters etc) against entities from different bounded contexts (e.g. search people, assets, tenures etc). For this kind of functionality querying a classic database (either SQL or NoSQL) does not fit the purpose, instead a search index is more appropriate. AWS ElasticSearch is the technology of choice as a search index for Hackney. AWS CloudSearch has been excluded as it\u2019s not actively supported by the AWS team, plus it\u2019s not available in the London Region.\\nTwo options have been considered:\\n1. Search endpoint inside each domain bounded context\\nWith this option the bounded context would have its database for the classic CRUD operations and its search index where to retrieve the list of entities via the advanced search functionalities provided by ElasticSearch. This option is more appropriate if it\u2019s required to search the same fields or a subset of fields from the domain entities.\\n2. Searches bounded context exposing a search endpoint per each domain\\nWith this option the domain bounded context does not have the search index but a search endpoint, with its index, is being added to the \u201csearch bounded context\u201d. In this way the searches bounded context has got a single AWS Elastic Search with multiple indexes, one per domain which requires search functionality.  This option is more appropriate if it\u2019s required to search also for fields that are related to the domain entity but those belong to different bounded contexts.\\n","completion":"## Decision\n**Searches bounded context exposing a search endpoint per each domain**\\nThis option has been chosen because the search for person and assets requires to search\/filter\/display data which belongs to other domains. For example, when searching for a person it\u2019s required to filter per tenure type, display some tenure information, display assets full address etc. So, some of the person related fields do not belong to the Person platform API which is supposed to be used across the entire Hackney and not just for housing. Instead, with a separate search service we can aggregate related fields from different bounded contexts.\\nThe search indexes will be kept in sync via events raised by the relevant platform APIs.\\nIn this way we can have a search for \u201cHousing domain\u201d which allows to search for a person by augmenting the entity with housing related information, things like asset address, tenures etc. With the same logic, in the future we might have another search service for \u201csocial care domain\u201d where the person can be augmented with social care related fields. Both searches will have the same Person platform API as one of master data.\\n"}
{"prompt":"## Context\\nCurrently we use [go-amino](https:\/\/github.com\/tendermint\/go-amino) throughout Tendermint. Amino is not being maintained anymore (April 15, 2020) by the Tendermint team and has been found to have issues:\\n- https:\/\/github.com\/tendermint\/go-amino\/issues\/286\\n- https:\/\/github.com\/tendermint\/go-amino\/issues\/230\\n- https:\/\/github.com\/tendermint\/go-amino\/issues\/121\\nThese are a few of the known issues that users could run into.\\nAmino enables quick prototyping and development of features. While this is nice, amino does not provide the performance and developer convenience that is expected. For Tendermint to see wider adoption as a BFT protocol engine a transition to an adopted encoding format is needed. Below are some possible options that can be explored.\\nThere are a few options to pick from:\\n- `Protobuf`: Protocol buffers are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data \u2013 think XML, but smaller, faster, and simpler. It is supported in countless languages and has been proven in production for many years.\\n- `FlatBuffers`: FlatBuffers is an efficient cross platform serialization library. Flatbuffers are more efficient than Protobuf due to the fast that there is no parsing\/unpacking to a second representation. FlatBuffers has been tested and used in production but is not widely adopted.\\n- `CapnProto`: Cap\u2019n Proto is an insanely fast data interchange format and capability-based RPC system. Cap'n Proto does not have a encoding\/decoding step. It has not seen wide adoption throughout the industry.\\n- @erikgrinaker - https:\/\/github.com\/tendermint\/tendermint\/pull\/4623#discussion_r401163501\\n```\\nCap'n'Proto is awesome. It was written by one of the original Protobuf developers to fix some of its issues, and supports e.g. random access to process huge messages without loading them into memory and an (opt-in) canonical form which would be very useful when determinism is needed (e.g. in the state machine). That said, I suspect Protobuf is the better choice due to wider adoption, although it makes me kind of sad since Cap'n'Proto is technically better.\\n```\\n","completion":"## Decision\nTransition Tendermint to Protobuf because of its performance and tooling. The Ecosystem behind Protobuf is vast and has outstanding [support for many languages](https:\/\/developers.google.com\/protocol-buffers\/docs\/tutorials).\\nWe will be making this possible by keeping the current types in there current form (handwritten) and creating a `\/proto` directory in which all the `.proto` files will live. Where encoding is needed, on disk and over the wire, we will call util functions that will transition the types from handwritten go types to protobuf generated types. This is inline with the recommended file structure from [buf](https:\/\/buf.build). You can find more information on this file structure [here](https:\/\/buf.build\/docs\/lint-checkers#file_layout).\\nBy going with this design we will enable future changes to types and allow for a more modular codebase.\\n"}
{"prompt":"## Context\\nDefining measurements for independent fluxes isn't always clear.\\nThis can occur when you measure more fluxes than there are degrees\\nof freedom in a network.\\nAn example would be this simplified network:\\nA -> B -> C\\nwhere reaction 1 and reaction 2 are dependent, implying that\\nno additional information is achieved by including both.\\nAnother issue is knowing when you do not have enough fluxes\\nmeasured, resulting in an underdetermined system. Due to the\\nBayesian implementation of Maud, these systems are still theoretically\\nresolvable. However, supplementing as much information\\nas possible will likely be beneficial.\\n","completion":"## Decision\nIdentifying underdetermined systems is acomplished by first calculating\\nthe null space of the matrix. This gives the number of degrees of freedom\\nof the system as well. Then we calculate the reduced row echelon form of\\nthe transpose of the null space. The resulting matrix represents the\\nindependent flux pathways through the network as rows. If you take the\\nmeasured subset of reactions and there is a row containing no non-zero\\nentries then the system is not fully described using the current measurements.\\nDetermining if the system is overspecified is achieved by comparing the\\nnumber of measurements to the degrees of freedom. If the number of measurements\\nis larger than the degrees of freedom then the system is overdetermined.\\nIt is possible to both have an underdetermined system which is overspecified\\nby having multiple measurements on dependent paths. It is also possible to\\nrecieve the warning that the system is overspecified by independent measurements.\\nFor instance, a linear pathway where the influx and efflux are both measured.\\nThis is still valid as they are independent measurements.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on Opinionated Digital Center.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\n[ADR036](ADR036-hsm-isolation-in-detail.md) described the network and\\ncredential isolation we use to ensure that unauthorised users cannot\\naccess the CloudHSM.\\nRecently in 3ea9de2ff, we introduced a GlobalNetworkPolicy object,\\nwhich is a Calico feature that allows a cluster-wide network policy to\\nbe imposed.  This allows us to control network access to and from\\nparticular namespaces in a way which cannot be overridden by tenants.\\nIn particular, currently access to the HSM is only allowed from pods\\nannotated with a `talksToHsm=true` label.\\nWhen working in their own namespace, a developer has full control over\\nwhat labels they put on their pods, so they can still choose to put\\nthe `talksToHsm=true` label on their pods.  But they do not have\\ncontrol over what labels the namespace itself has; to change this\\nwould require a change to the `gsp` or appropriate `cluster-config`\\nrepository, which would make such a change visible to many more\\npeople.\\nTherefore, if we extend the GlobalNetworkPolicy to require a\\n`talksToHsm=true` label on *both* the pod *and* the namespace, we will\\nprevent tenants from unilaterally opening up network access to the HSM\\nfrom their namespaces.\\n","completion":"## Decision\nWe will augment the GlobalNetworkPolicy (previously described in ADR036) by:\\n- setting a `GlobalNetworkPolicy` that denies access to the\\nCloudHSM's IP address unless the pod carries a label\\n(`talksToHsm=true`) and the namespace also carries a label\\n(`talksToHsm=true`) and allows all other egress traffic\\n"}
{"prompt":"## Context\\nReducing complexity in the `ConsoleUI` object and make it more reusable and testable.\\n","completion":"## Decision\n`ConsoleUI` now only knows when to output things, but not the contents of messages, which is now responsibility of a\\n`Formatter`. This object can be replaced by other formatters in the future.\\n"}
{"prompt":"## Context\\nWe are building a `\"Hello World\"` application with AWS Lambda. What\\ntools or frameworks can we leverage?\\n","completion":"## Decision\nWe will use the [Chalice] framework to buiild our serverless\\napplication.\\n"}
{"prompt":"## Context and Problem Statement\\nIt is the offical policy of the Digital Iceland and stated in the Techical Direction that it is to be implemented as free and open source. Open source software by definition is open to anyone to use, modify, distribute and study. These permissions are enforced an open source license. There are a number of well-known and widely used open source licenses available and we need to choose a license that best fits the goals of digital iceland.\\nThere are two main types of open source licences:  more permissive licences that confer broad freedoms and minimal obligations (e.g., the MIT, BSD and the Apache 2.0 licences); and sharealike licences that require licensing adaptations with the same licence if they distribute them (e.g., the GNU GPL).\\nDevelopment for Digital Iceland will be open and free with minimum complications for development for all involved. Reuse and transparency will be promoted.\\n## Decision Drivers\\n* The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.\\n* It is important to build on the experience of similar government led inititives in other countries.\\n* Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.\\n* It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.\\n","completion":"## Decision\n* The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.\\n* It is important to build on the experience of similar government led inititives in other countries.\\n* Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.\\n* It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.\\nThe MIT license was chosen, for the following reasons:\\n* It is the least restrictive of the licenses.\\n* It is very consise, simple and easy to understand and therefore should be clear to users and developers.\\n* Digital Iceland does not require protection of patents or existing intelletual property.\\n* Well known government lead initiatives like uk.gov and X-Road use the MIT license.\\n* The MIT license is the best known and most widely used free and open-source license in the world.\\n"}
{"prompt":"## Context\\nThe Web SRE Team manages a number of services that vary in users, technologies used, setup, our commitment, infrastructure, process, etc.\\nPre-existing documentation for these services, if it exists, also varies - in location, level of detail, scope, structure, presumed audience, etc.\\nTo help share knowledge between the Web SRE Team about these services' unique situations, a consistent baseline for our services documentation is a required first step.\\n","completion":"## Decision\nA Web SRE service here is defined as any codebase where we have some ownership over an aspect of this codebase being functional - e.g. the deployment process, automation, infrastructure running the service, observability of some aspect(s), or authoring the code ourselves. Web SRE services could be externally or internally-facing, including infrastructure services only Web SRE are aware of and leverage. Web SRE services also include non-prod environments for testing infrastructure code (like staging Kubernetes clusters to test building new clusters) and supporting services (log aggregation, alerting, etc) as well as non-prod application code.\\nFor each Web SRE Service, we will author a Service Documentation page in Mana as a child of https:\/\/mana.mozilla.org\/wiki\/display\/SRE\/Service+Documentation.\\nService Documentation pages:\\n* use the Web SRE Services Docs confluence page template for its structure;\\n* have a primary audience of the Web SRE Team itself, with a secondary audience of other SRE teams;\\n* are open to Mozilla internally, however not all teams of Mozilla are the intended audience of these pages (SRE-external teams will focus on the escalation path section primarily);\\n* have Runbook pages (what to do when a specific symptom of an issue is recognized) as child pages to the relevant Service Documentation page;\\n* have How-to pages (how to perform specific tasks for a service) as child pages to the relevant Service Documentation page;\\n* are maintained to reflect the current state - to the best of our knowledge - of a service's context, including a service being decommissioned (the Service Documentation page should note that but be left up as a tombstone marker of the decision);\\n* cover Web SRE Services as defined above.\\n* replace SRE_INFO.md files in Web SRE team-managed source code GitHub repositories. When an SRE_INFO.md file is encountered, it should be reviewed for any information that can be added to a Service Documentation page, removed, and an link to the Service Documentation page added to the codebase's REAMDE.md.\\nService Documentation pages do not:\\n* include all possible services Web SRE might work with - e.g. we don't document AWS Services generically or services owned and maintained by other teams;\\n* include all possible details for all audiences beyond SRE - when such documentation requests come up, they can be added as a How To as a child to a service page, or a generic How To if not limited to one service;\\n* live anywhere other than Mana. Web SRE Service documentation managed elsewhere, e.g. GitHub, Google Docs, should be migrated & then deprecated via links pointing readers to the Mana page;\\n* replace other forms of documentation living elsewhere - e.g. codebase-specific documentation within a git repository, decision records in this repository, collaboration \/ draft developer notes in Google documents, Mana pages in other places and formats walking through shared infrastructure or processes, etc..\\nService Documentation pages optionally have Runbook & How-To child pages. For these, there are the following expectatiosn:\\nRunbook pages should:\\n* include ways to validate what state a system is in (e.g. how to reproduce the problem);\\n* be as self-contained as is feasible;\\n* explain what problem a given state indicates;\\n* give commands to resolve the problem as clearly as possible;\\n* outline a fallback plan (who to call, what to do next).\\nHow-to pages should:\\n* repeat existing documentation as little as possible. Linking to external docs is encouraged, perhaps augmented with our specific contexts;\\n* assume a high level of competence from the audience (don't explain how to download a csv file, though giving an example command to establish context is great);\\n* explain the decision points in a process, and how to make them;\\n* outline what is needed to perform the work as early as possible in the document (what access, what tools);\\n* outline who needs to approve the work, or how to decide if it's safe to do the work as early as possible.\\n"}
{"prompt":"## Context and Problem Statement\\nTo run the applications I need to decide which platform to chose.\\n","completion":"## Decision\nChosen option: \"AWS Lambda\", because currently Istio as well as EKS is still in beta and for private projects AWS Lambda is the more cost efficient version. And for different workloads different solutions might be best e.g. [_BinaryAlert_](https:\/\/github.com\/airbnb\/binaryalert) \\(Serverless\\).\\nPositive Consequences: <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\nNegative consequences: <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n"}
{"prompt":"## Context\\neLife has numerous projects written completely and partly with the [Python programming language](https:\/\/www.python.org\/).\\nIn order to provide language version consistency across projects we need to get a consensus on which versions we are going to support.\\nWe have only gone up to Python 3.5 due to the default Python versions pre installed on the Ubuntu distributions we use.\\n","completion":"## Decision\nWe will use Python >=2.7.14 as our default version for any project that solely uses or supports Python 2.\\nWe will use Python 3.5 as our default supported version for any project that solely uses or supports Python 3.\\n"}
{"prompt":"## Context\\nWe need to distinguish what our application does versus how it does it.\\n","completion":"## Decision\nWe will use clean architecture to ensure a clear separation is maintained between what and how.\\nSee [README.md](..\/..\/README.md) for details.\\n"}
{"prompt":"## Context\\nNeed to find a way for developers to import this library into their own projects.\\n","completion":"## Decision\nWe will use npm to publish the library privately in [MyGet](https:\/\/www.myget.org) (a 3rd party npm compatible registry)\\n"}
{"prompt":"## Context\\nA method to authorize against the youtube API to get the access token needs to be added to the application.\\n","completion":"## Decision\nI decided to use the launchWebAuthFlow instead of the getAuthToken api to do the oauth2 authentication. This is\\nbecause getAuthToken can only authorize the google signed in in chrome. This prevents me from using a different account\\nfor youtube from my chrome account.\\n"}
{"prompt":"## Context\\nThe Data Platform currently has no capacity to ingest data from external APIs\\n","completion":"## Decision\n![API ingestion](..\/images\/api-ingestion.png)\\nDeploy a Lambda that will call the the required API and save the returned data\\nobject into S3, once all the records have been downloaded the lambda will\\ntrigger an AWS glue workflow that will convert JSON objects into parquet and\\ncrawl the data enabling users to access that form a data catalogue in AWS Athena.\\n"}
{"prompt":"## Context\\nBookit needs the concept of a user so it can identify who booked what bookable and allow certain users to overrule a booking.\\nThis involves both authentication (who you are) and authorization (what you're allowed to do).  For the MVP we are primarily focused on Authentication.\\nEmployees from different domains (Wipro, Designit, Cooper, etc) need to log into the system.\\nProtect all endpoints, optionally allow \/ping but, if authentication provided, return profile information as well.\\n","completion":"## Decision\n* We will use Azure AD v2 OpenID Connect to exchange an id_token for an opaque Bookit API opaque access_token\\n"}
{"prompt":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n","completion":"## Decision\nWe will use Dependabot to monitor dependency updates.\\n"}
{"prompt":"## Context\\nIn case of a structural change in a dataset you might need to migrate serialized data in a way to make in work with the latest code.\\nThe data that is affected of the structural change can still be within a message queue provider and is often out of access until message handling.\\nYou could unintentionally send duplicated messages to drain performance and increase I\/O operations overall.\\n","completion":"## Decision\n* Extract job actions from the messages into the storage.\\n* Separate job payloads from their actions.\\n* Prevent sending of duplicate messages.\\n* Normalize the structure of a job regarding the current message structure.\\n"}
{"prompt":"## Context\\nUsing 3rd party components, like libraries, modules or executables, introduces the risk of importing security vulnerabilities in those components. Hackers might target these components in a software supply chain attack. A basic precaution to this is to keep all components up-to-date, a practice referred to as life cycle management.\\n","completion":"## Decision\nWe will keep 3rd party components up-to-date to mitigate software supply chain attack risk.\\n"}
{"prompt":"## Context\\nIt should be possible for apps to define their entities. Furthermore, it should be possible, if desired, that these entities are available via Store API.\\nLater, it should also be possible for store operators to create such entities. The concept is to consider that Apps can not add PHP code into the system under current circumstances. Also, a store operator is, seen from our point of view, not able to write PHP code himself to guarantee logic for his custom entities.\\nTherefore, purely through the definition of a custom entity, certain business logic should be automatically guaranteed.\\n","completion":"## Decision\n### Schema\\n* Definition\\n* An app can include a `config\/custom_entity.xml` file.\\n* Multiple custom entities can be defined in the XML file.\\n* Each custom entity, is registered with the prefix `custom_entity_` or the `ce_` shorthand.\\n* App developers can then define that they would like to have `custom_entity_swag_blog` as an entity\\n* To prevent naming collisions, app developers should always add their developer prefix to the entity name\\n* We then create the `custom_entity_swag_blog` table\\n* Tables \/ Properties \/ Columns:\\n* A proper MySQL table is created for each custom entity.\\n* For each custom entity field we create a real MySQL table column.\\n* We support the following field data types:\\n* All scalar fields (int, string, text, float, date, boolean)\\n* All JSON fields (JSON, list, price, etc.)\\n* All \"linking\" associations (many-to-one and many-to-many)\\n* A bi-directional association will be left out for now.\\n* one-to-one and one-to-many will be not supported for now.\\n* Install & Update\\n* When installing and updating an app, the core automatically performs a schema update.\\n* Consider running a `dal:validate` on the schema when installing and updating an app.\\n* New fields on a custom entity must always be nullable or have a default\\n* Changing a field\/property data type is not allowed\\n* If a field is no longer defined in the .xml file, it will be deleted from the database.\\n* Identification and representation\\n* Each custom entity gets a `IdField(id)`, which serves as primary key\\n* Each custom entity gets a field `TranslatedField(label)`, which is required and serves as display name for the admin\\n### Bootstrapping\\n* At kernel boot we load all custom entities from the database and register them in the registry and di-container.\\n* For each custom entity, an entity definition is registered\\n* A generic entity definition is used, which gets the property\/column schema injected\\n* It must be checked how performant this is in case of bad performance we must put a cache in front of it (serialized properties\/columns e.g.)\\n* If no database connection exists, a kernel boot should still be possible\\n* The loading of the custom entities for the kernel boot should be outsourced to a CustomEntityKernelLoader\\n### Api availability\\nFor routing, we have to trick a bit, because currently for each entity in the system the routes defined exactly. This is not possible because the API route loader is triggered before the custom entities are registered. Therefore...\\n* We always register `\/api\/custom-entity-{entity}` as an API route and point to a custom controller that derives from ApiController.\\n* A request `\/api\/custom-entity-swag-blog`, then runs into our controller, and we get for the parameter `entity` the value `swag-blog`. We then pass this value to the parent method and prefetch it\\n* If the entity was defined with the `ce_` shorthand the API endpoints also use that shorthand, which means the route would be `\/api\/ce-{entity}`.\\n### Store api integration\\n* On the schema of the entity, the developer can define if this is `store_api_aware`.\\n* Entities which are not marked as `store_api_aware` will be removed from the response\\n* We will provide no automatic generated endpoint for the entities.\\n* Store api logics will be realized with the app-scripting epic\\n"}
{"prompt":"## Context\\nIn `Corvus.Tenancy` v2, the various storage-technology-specific libraries (e.g. `Corvus.Azure.Storage.Tenancy`) could dynamically create new containers for you the first time you asked for them. One of the problems this caused was that the definition types (e.g. `BlobStorageContainerDefinition`) needed to include all of the information required to be able to create a new container on demand. For example, with blob containers, that meant specifying the container's public access type. This was not a great idea, because it muddied the role of the definition types. These were primarily logical names, but they also ended up containing the default configuration settings to use in these auto-container-generation scenarios.\\nThe tenant onboarding process (the process of enabling a new tenant to use an application, or some particular piece of application functionality) necessarily includes these steps:\\n* determining the storage account (and relevant credentials) to use\\n* picking a suitable container name, ensuring proper tenant isolation\\n* creating the container\\nIn V2, applications were in control of the first of these. But in most cases, the second and third were handled by `Corvus.Tenancy`. These last two were unhelpfully tied together because of an unfortunate comingling of concerns. This happened due to good but misguided intentions. We were aiming to enable applications to have a single configuration serving multiple logical containers. For certain kinds of storage (e.g., Azure blob storage) it's common for an application to split data across multiple containers (e.g., putting all the user profile details in one container, and, e.g., to-do list entries in another container). In a non-tenanted application you'd expect to configure settings such as account name and credentials just once\u2014it wouldn't normally make sense to have per-container configuration settings because you'd expect to use the same account across all the logical containers. When it came to tenanted storage, the v2 libraries tried to support the same approach by offering convention-based mechanisms to enable multiple container 'definitions' (logical names) to refer to the same underlying configuration. However, this was inextricably linked to letting the storage libraries pick the container name.\\nThe problem arose because one of the things V2 tried to do for us was to map from the logical container names in the definition types (e.g. `BlobStorageContainerDefinition.ContainerName`) to an actual container name. To enable isolation of data across tenants even when they shared a storage account, this container name mapping would typically incorporate the tenant ID in the real container name. However, this naming scheme was initially an undocumented implementation detail, preventing applications from anticipating what the container would actually be called. If the application doesn't know the container name, it can't create the container itself prior to first use, and so these tenanted storage providers also automatically created the container too.\\nThis is how we ended up with the definition types (e.g. `BlobStorageContainerDefinition`), which were meant to be logical identifiers, needing to include all of the information required to be able to create a new container on demand.\\nIt was technically possible for application code to take control of all three of the steps listed above itself in V2, but it was problematic. You could disable the tenanted container name generation, giving you control over the container name, making it possible for the application to know the right container name, but an unfortunate side effect of this was that for each new tenant, you ended up needing to create one configuration for every logical container. (I.e., the decision to take control of container creation unavoidably meant using more complex configuration.) We did make some changes enabling applications to predict the names that would be using, so they could get in ahead, but with hindsight, we ended up regretting ever making the tenanted storage libraries create containers in the first place.\\nAnother problem with the automatic create-on-first-use behaviour was that any problems that would prevent creation became visible rather late in the day: you might think you'd successfully onboarded a new tenant, only to discover later that not everything is going to work.\\n","completion":"## Decision\nIn `Corvus.Tenancy` v3, applications are responsible for creating any containers they need. The new tenanted storage client libraries will never create a container for you.\\nAlso, applications determine the strategy for picking tenant-qualified names to ensure isolation in cases where multiple tenants are sharing a storage account. The tenancy libraries will provide mechanisms that do most of the work, so the main change is that the application has to opt in explicitly.\\n"}
{"prompt":"## Context\\n[context]: #context\\nCurrentrly existing users are identified by both a *user name* and an *e-mail address*.\\nOnly the user name is guaranteed to be unique, but not the e-mail address.\\nThe database already contains different users for the same e-mail address:\\n```sql\\nselect id, role, username, email, email_confirmed\\nfrom users\\nwhere email in (select email from users group by email having count(*) > 1)\\norder by email;\\n```\\nUsers are able to provide either the user name or the e-mail address and all\\ndatabase queries have to account for this duality.\\nFor new users the user name is already generated from the e-mail address\\nand cannot be chosen. The user name has become redundant and could be\\nreplaced by the e-mail address.\\n","completion":"## Decision\n[decision]: #decision\\nUse the e-mail address for identifying users:\\n- Merge users with the same e-mail address into a single user\\n- Prevent duplicate e-mail addresses in the future\\n- Replace all references of *username* by *email*\\n- Remove *username* from the database and the code base\\n"}
{"prompt":"## Context\\nThe Horace and Herbert projects are tightly coupled. Herbert build artifacts are required for integration and MATLAB testing of Horace source.\\nChanges made to Herbert may change or break dependent MATLAB or C++ code in Horace.\\n","completion":"## Decision\nTo make the depdencency explicit Herbert will be regarded as a library.\\nAs a consequence:\\n- Herbert builds will NOT trigger Horace builds\\n- Horace builds (both `PR` and `master`) will always use the latest `master` build of Herbert\\n- Build artifacts will will copied from the latest successful `master-<target-os>-<target-matlab>` build on the Herbert CI server.\\n"}
{"prompt":"## Context\\nThe census requires that the primary person within a household can be identified and routed differently than other household members. Currently all people within a household are represented as a list, with no distinction between them.\\n","completion":"## Decision\n### Add primary person attribute to lists\\nThe list store will include a `primary_person` key which will map to a `list_item_id`. For example, a primary householder \"Joe\" would be represented as follows:\\n```json\\n{\\n\"answers\": [\\n{\\n\"answer_id\": \"date-of-birth\",\\n\"value\": \"1982-08-01\",\\n\"list_item_id\": \"a9hd8j\"\\n},\\n{\\n\"answer_id\": \"first-name\",\\n\"value\": \"Joe\",\\n\"list_item_id\": \"a9hd8j\"\\n},\\n{\\n\"answer_id\": \"date-of-birth\",\\n\"value\": \"1990-05-23\",\\n\"list_item_id\": \"gyt5hy\"\\n},\\n{\\n\"answer_id\": \"first-name\",\\n\"value\": \"Jane\",\\n\"list_item_id\": \"gyt5hy\"\\n},\\n],\\n\"lists\": [\\n{\\n\"name\": \"people\",\\n\"primary_person\": \"a9hd8j\",\\n\"items\": [\\n\"a9hd8j\",\\n\"gyt5hy\"\\n]\\n}\\n]\\n}\\n```\\n### Create a PrimaryPersonCollector block type\\nA new block type \"PrimaryPersonCollector\" will be defined to support the necessary behaviour. It will extend the existing ListCollector and allow addition of a primary person through adding context to the list store. The following diagram identifies the flow to be implemented in runner.\\n![Primary Flow](0009-primary-flow.png)\\nBoth the PrimaryPersonListCollector and ListCollector operate upon the same list in the store. The PrimaryPersonCollector would create an item in the list store, before collecting any answers it refers to. This allows the identification of a primary person in one block and deferring their name entry to another.\\nAs per a ListCollector, the navigation path followed by the PrimaryPersonCollector exists separately from the main routing path. On routing backwards from the ListCollector, the primary person will be routed back to the driving question. This is represented in the reverse flow below, where the primary person would be unable to reach the primary name block when using the \"Previous\" link.\\n![Primary Reverse Flow](0009-primary-reverse-flow.png)\\n"}
{"prompt":"## Context\\nTeams using the GDS Supported platform require the ability to develop, test applications and prove conformance with the GDS Supported platform on local hardware. Teams need to learn how to use the GSP and to understand how applications are containerised, packaged and deployed to a cluster using the standard CICD tools provided by GSP.\\n","completion":"## Decision\nWe will [provide a way to run a full GSP compatible stack locally on a developer machine](\/docs\/gds-supported-platform\/getting-started-gsp-local.md) without the cloud provider specific configuration.\\n"}
{"prompt":"## Context\\nAccess to the solutions (applications) as a user, referred to as the _run_ environment, is based on the company identity provider. Access to the development\/operations environment, the GCP platform, is based on a separate, DevOps identity provider. This allows strict separation between Run and DevOps and makes automation of DevOps practices somewhat easier.\\n","completion":"## Decision\nWe will use a separate identity provider to access the platform for DevOps practices, disconnected from access to the _run_ environment.\\n"}
{"prompt":"## Context\\nAn erroneous report would be unsubmitted by the case manager, corrected and resubmitted by the deputy. In that case, we'd probably make the request again with the same source, report dates and year (though a different date submitted).\\n","completion":"## Decision\nThe original report submission needs to be retained, and the 'context' of that decision retained also. The context means all the supporting files around the original submission.\\nWhen the report is re-submitted, a new document is created in Supervision, and also the context surrounding THAT submission.\\nPractically, what this means is that all of the supporting documents need to be submitted again, because it's impractical for us to check for documents added, removed, or modifiedfor this new submission\\n"}
{"prompt":"## Context\\nThe application is about finding services closest to the search point. All\\nservices have a co-ordinate in lat\/lon format. Currently the application\\nrequests either an out-code or a postcode as the search point. The submitted\\npostcode needs to be resolved to a lat\/lon that can be used to query against.\\n","completion":"## Decision\n[Postcodes.io](https:\/\/postcodes.io\/) provides both an out-code and a postcode\\nlookup. The results of which return latitude and longitude. The service is\\nfree, [supported](https:\/\/postcodes.io\/about),\\n[monitored](https:\/\/status.ideal-postcodes.co.uk\/) and contains the full set of\\nactive postcodes for the UK. Supplied by the\\n[ONS](https:\/\/data.gov.uk\/dataset\/national-statistics-postcode-lookup-uk).\\n"}
{"prompt":"## Context\\nProgramming frameworks can promote the productivity of a project; producing smaller code bases, added reliability,\\nadditional features (than one would otherwise write themselves), etc.\\n","completion":"## Decision\nWe have used the [Spring framework](https:\/\/spring.io\/) within the team for a number of years, and its recent advances in the domain of REST applications makes it an ideal choice.\\n"}
{"prompt":"## Context and Problem Statement\\nWe need to measure uptime for at least two reasons:\\n1. To serve as feedback on what needs to be improved next.\\n2. To demonstrate compliance with our SLAs.\\nHow exactly should we measure uptime?\\n## Decision Drivers\\n* We want to reduce tools sprawl.\\n* We want to be mindful about capacity and infrastructure costs.\\n* We want to measure uptime as observed by a consumer -- i.e., application or user -- taking into account business continuity measures, such as redundancy, fail-over time, etc.\\n","completion":"## Decision\n* We want to reduce tools sprawl.\\n* We want to be mindful about capacity and infrastructure costs.\\n* We want to measure uptime as observed by a consumer -- i.e., application or user -- taking into account business continuity measures, such as redundancy, fail-over time, etc.\\nChosen option: \"use Probe for measuring uptime of internal Compliant Kubernetes services\", because it measures uptime as observed by a consumer. Although this requires a bit of extra capacity for running Blackbox, the costs are worth the benefits.\\nInstead of configuring Blackbox directly, `Probe` is a cleaner abstraction provided by the Prometheus Operator.\\nThe following is an example for a Probe:\\n```yaml\\napiVersion: monitoring.coreos.com\/v1\\nkind: Probe\\nmetadata:\\nname: google-is-up\\nlabels:\\nprobe: google\\nrelease: kube-prometheus-stack\\nspec:\\ninterval: 60s\\nmodule: http_2xx\\nprober:\\nurl: blackbox-prometheus-blackbox-exporter.monitoring.svc.cluster.local:9115\\ntargets:\\nstaticConfig:\\nstatic:\\n- https:\/\/www.google.com\\n```\\nThis will generate a metric as follows: `probe_success{cluster=\"ckdemo-wc\", instance=\"https:\/\/www.google.com\", job=\"probe\/demo1\/google-is-up\", namespace=\"demo1\"}`.\\n### Positive Consequences\\n* We measure uptime as observed by a consumer.\\n* Increasing redundancy, reducing failure time, etc. will contribute positively to our uptime, as desired.\\n### Negative Consequences\\n* We don't currently run Blackbox in the workload cluster, so we'll need a bit of extra capacity.\\n"}
{"prompt":"## Context\\n_The issue motivating this decision, and any context that influences or constrains the decision._\\n**Current**: Changes committed to the [V7\\nbranch](https:\/\/github.com\/cloudfoundry\/cli\/tree\/v7) need to be cherry-picked\\nonto the [main (V8) branch](https:\/\/github.com\/cloudfoundry\/cli), a manual\\nprocess.\\nThis depends on the pair to remember to cherry-pick. If forgotten, the commit\\nwill not be merged, and we run the risk of bugfixes (and features) not\\nmanifested in both branches.\\n","completion":"## Decision\n_The change that we're proposing or have agreed to implement._\\nCommit changes on the v7 branch then merge to main\/v8, e.g.:\\n```bash\\ngit co v7\\ngit pull -r\\n# make changes\\nmake\\ngit add -p\\ngit ci\\nbin\/push.sh\\n```\\n[`push.sh`](https:\/\/github.com\/cloudfoundry\/cli\/blob\/main\/bin\/push.sh) is a\\nscripts which pushes the v7 commit, rebases against main, checks out main,\\npulls, merges v7, pushes, and then checks out v7. It automates some of the more\\ntedious aspects of the commit cycle.\\nThis is the process that Ops Manager follows to maintain several branches.\\nIt is not without drawbacks, though. Here are the notes from the Ops Mgr anchor:\\n> We're still using merge forward as the primary method of sharing features \/ bug fixes across all of our releases. It generally still works well for us, but there are drawbacks.\\n> * If you care about a \"clean\", fairly linear git history for each branch. The merge commits can definitely make things difficult to follow sometimes. This isn't much of an issue for us, but I can imagine it being more important for ya'll since it's an open source project\\n> * It assumes that everything in the earlier branch belongs in the later branch. This makes a lot of sense for common bug fixes, but perhaps there are feature differences that could make it trickier in your case.\\n> * It somewhat discourages large refactors or cleanups. In 2.7 we did a major refactor of our React code, so anytime we need to do a UI bug fix in 2.6 or below, it becomes a pain to merge forward and we basically have to re-do the work. (Though this probably would apply to a cherry-pick strategy as well!)\\n> I think for Ops Manager, since we're maintaining so many branches at once, and the fact that most of what we would do on earlier branches is bug fixes for ALL branches, it's slightly less overhead for us to do merge forward. If the CF CLI is diverging in any significant manner between versions, and you're only going to be dealing with 2 versions, cherry-picking from one branch to the other might be cleaner. Merge forward should still work, but maybe you'd be getting less benefit out of it.\\n"}
{"prompt":"## Context\\nWe need to expose the API of our application to the outside world. For now, we expect one client of our application - FrontEnd SPA application.\\n","completion":"## Decision\nSolution 1.\\nCreating separate API projects for each module will add complexity and little value. Grouping endpoints for a particular business module in a special directory is enough. Another layer on top of the module is unnecessary.\\n"}
{"prompt":"## Problem\\nConfig files ideally do not copy any structure if they only want to\\nset a single key.\\n","completion":"## Decision\nSupport holes and values for non-leaves in a KeySet if the underlying format allows it.\\nIf the underlying format does not support it and there is also not an obvious\\nway how to circumvent it -- e.g., JSON which does not have comments -- holes and\\nvalues in non-leaves can be supported with key names starting with \u00aeelektra.\\n"}
{"prompt":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n","completion":"## Decision\nUse Simplecov with RSpec to monitor coverage changes on every test run\\n"}
{"prompt":"## Context and Problem Statement\\nDifferent services are going to push different kind of messages so there is a need in EDA architecture and some messaging server.\\n## Decision Drivers\\n* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\n","completion":"## Decision\n* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\nDecided to use RabbitMQ since\\n* It provides exact functionality that is neded for the product (publish-subscribe event messaging)\\n* It is easy to learn and use and I already had some knowledge of how to use it\\n* It has official container on Docker Hub\\n* It seems to be popular and trending these days\\n### Positive Consequences\\n* I will be able to setup EDA messaging in the product.\\n### Negative Consequences\\n* n\/a\\n"}
{"prompt":"## Context\\nJest is a general JavaScript testing framework. Enzyme is a testing utility that makes it easier to assert, manipulate, and traverse React components. [This](https:\/\/medium.com\/welldone-software\/an-overview-of-javascript-testing-in-2018-f68950900bc3) article goes into detail about various testing alternatives and [this](https:\/\/www.codementor.io\/vijayst\/unit-testing-react-components-jest-or-enzyme-du1087lh8) article details using Jest with Enzyme.\\n","completion":"## Decision\nBased on the familiarity of our engineers with Jest and Enzyme and the large community of support, use Jest and Enzyme to unit test our React application.\\n"}
{"prompt":"## Context\\nWe need to verify that the log event sent to the Adapter matches the URL used\\nto sent the request.\\nThe Broker must generate an Adapter URL that is unique to an application.\\nThe Broker must generate an Adapter URL in a way that is not easily tampered with (ie \/my-app-name).\\nThe Broker should be stateless.\\nThe Adapter must verify that the application GUID in the log event is valid for the URL used to send the event.\\nThe Adapter should be stateless.\\n","completion":"## Decision\nWe will have the Broker generate URLs containing the a\\n[hash-based message authentication code][hmac] of the application GUID an a\\nsecret shared between the Broker and the Adapter.\\n"}
{"prompt":"## Context\\n`adr-tools` seeks to communicate the history of architectural decisions of a\\nproject.  An important component of the history is the time at which a decision\\nwas made.\\nTo communicate effectively, `adr-tools` should present information as\\nunambiguously as possible.  That means that culture-neutral data formats should\\nbe preferred over culture-specific formats.\\nExisting `adr-tools` deployments format  dates as `dd\/mm\/yyyy` by default.  That\\nformatting is common formatting in the United Kingdom (where the `adr-tools`\\nproject was originally written), but is easily confused with the `mm\/dd\/yyyy`\\nformat preferred in the United States.\\nThe default date format may be overridden by setting `ADR_DATE` in `config.sh`.\\n","completion":"## Decision\n`adr-tools` will use the ISO 8601 format for dates:  `yyyy-mm-dd`\\n"}
{"prompt":"## Context\\nService teams' apps currently run on [one Kubernetes cluster](https:\/\/github.com\/ministryofjustice\/cloud-platform\/blob\/main\/architecture-decision-record\/012-One-cluster-for-dev-staging-prod.md). That includes their dev\/staging\/prod environments - they are not split off. The key reasoning was:\\n- Strong isolation is already required between apps from different teams (via namespaces, network policies), so there is no difference for isolating environments\\n- Maintaining clusters for each environment is a cost in effort\\n- You risk the clusters diverging. So you might miss problems when testing on the dev\/staging clusters, because they aren't the same as prod.\\n(We also have clusters for other purposes: a 'management' cluster for Cloud Platform team's CI\/CD and ephemeral 'test' clusters for the Cloud Platform team to test changes to the cluster.)\\nHowever we have seen some problems with using one cluster, and advantages to moving to multi-cluster:\\n- Scaling limits\\n- Single point of failure\\n- Derisk upgrading of k8s\\n- Reduce blast radius for security\\n- Reduce blast radius of accidental deletion\\n- Pre-prod cluster\\n- Cattle not pets\\n### Scaling limits\\nMulti-cluster helps us we encounter a scale limitation. For example, we've found ourselves unexpectantly hitting an AWS limit during k8s upgrade. In this situation we could off-load some apps to another cluster. It would be advantageous to put each cluster in its own AWS account, to avoid limits, which are imposed per-account.\\n### Single point of failure\\nRunning everything on a single cluster is a 'single point of failure', which is a growing concern as more services use CP. Multi-cluster would allow us to quickly move apps off a broken cluster to another cluster.\\nSeveral elements in the cluster are a single point of failure:\\n- ingress (incidents: [1](https:\/\/runbooks.cloud-platform.service.justice.gov.uk\/incident-log.html#incident-on-2020-10-06-09-07-intermittent-quot-micro-downtimes-quot-on-various-services-using-dedicated-ingress-controllers) [2](https:\/\/runbooks.cloud-platform.service.justice.gov.uk\/incident-log.html#incident-on-2020-04-15-10-58-nginx-tls))\\n- external-dns\\n- cert manager\\n- kiam\\n- OPA ([incident](https:\/\/runbooks.cloud-platform.service.justice.gov.uk\/incident-log.html#incident-on-2020-02-25-10-58))\\n### Derisk upgrading of k8s\\nOnce you start a Kubernetes version upgrade, rolling back becomes infeasible (incidents: [1](https:\/\/runbooks.cloud-platform.service.justice.gov.uk\/incident-log.html#q1-2020-january-march)).\\nWith multi-cluster we could do a \"blue-green\" upgrade - spin up an cluster at the newer k8s version and then carefully move the apps across to it.\\n### Reduce blast radius for security\\nTaking a layered approach to security, extra isolation is beneficial. It resists lateral movement and minimizes the impact of a breach.\\nIsolation is added when you split the workloads across multiple clusters, even if they are in the same VPC. And further isolation is gained with separate VPCs or separate AWS accounts.\\nMore sensitive apps may require this isolation.\\nPre-prod environments are likely to often be running new code that has not have been through all the reviews, quality and security checks yet, so there may be a case for keeping these more isolated from environments with access to production data.\\n### Reduce blast radius of accidental deletion\\nIn the case of accidental deletion being run by a Cloud Platform team member, the ability to run administrative commands on only one cluster at a time would reduce the impact of this event, such as [this incident](https:\/\/runbooks.cloud-platform.service.justice.gov.uk\/incident-log.html#incident-on-2020-09-21-18-27-some-cloud-platform-components-destroyed). Disaster Recovery procedures are in [good shape now](https:\/\/runbooks.cloud-platform.service.justice.gov.uk\/disaster-recovery-scenarios.html#cloud-platform-disaster-recovery-scenarios), but it's worth minimizing blast radius all the same.\\n### Pre-prod cluster\\nTest clusters are used by the Cloud Platform team to test changes. These are made as realistic as possible, inheriting IaC and config from the main cluster, and the availability of test apps. However test clusters have a different lifecycle, don't run realistic loads and don't have traffic \/ loads. Ideally we test CP changes in the most realistic way possible, before changes are deployed to the part of the platform where production workloads are running.\\nMulti-cluster will allow us to put pre-prod environments on a separate cluster to prod environments. Then changes to the platform, once tested on a test cluster, we can roll them out to the pre-prod cluster, as a more realistic test, ahead of rolling out to the prod clusters. This is particularly beneficial for more fundamental changes like upgrading k8s, and single points of failure such as ingress or OPA.\\n### Challenge of moving apps\\nIf we were to create a fresh cluster, and an app is moved onto it, then there are a lot of impacts:\\n- **Kubecfg** - a fresh cluster will have a fresh kubernetes key, which invalidates everyone's kubecfg. This means that service teams will need to obtain a fresh token and add it to their app's CI\/CD config and every dev will need to refresh their command-line kubecfg for running kubectl.\\n- **IP Addresses** - unless the load balancer instance and elastic IPs are reused, it'll have fresh IP addresses. This will particularly affect devices on mobile networks that accessing our CP-hosted apps, because they often cache the DNS longer than the TTL. And if CP-hosted apps access third party systems and have arranged for our egress IP to be allow-listed in their firewall, then they will not work until that's updated.\\n","completion":"## Decision\nTBD\\n"}
{"prompt":"## Context\\nCassandra is used within distributed James product to hold messages and mailboxes metadata.\\nCassandra holds the following tables:\\n- mailboxPathV2 + mailbox allowing to retrieve mailboxes informations\\n- acl + UserMailboxACL hold denormalized information\\n- messageIdTable & imapUidTable allow to retrieve mailbox context information\\n- messageV2 table holds message metadata\\n- attachmentV2 holds attachments for messages\\n- References to these attachments are contained within the attachmentOwner and attachmentMessageId tables\\nCurrently, the deletion only deletes the first level of metadata. Lower level metadata stay unreachable. The data looks\\ndeleted but references are actually still present.\\nConcretely:\\n- Upon mailbox deletion, only mailboxPathV2 & mailbox content is deleted. messageIdTable, imapUidTable, messageV2,\\nattachmentV2 & attachmentMessageId metadata are left undeleted.\\n- Upon mailbox deletion, acl + UserMailboxACL are not deleted.\\n- Upon message deletion, only messageIdTable & imapUidTable content are deleted. messageV2, attachmentV2 &\\nattachmentMessageId metadata are left undeleted.\\nThis jeopardize efforts to regain disk space and privacy, for example through blobStore garbage collection.\\n","completion":"## Decision\nWe need to cleanup Cassandra metadata. They can be retrieved from dandling metadata after the delete operation had been\\nconducted out. We need to delete the lower levels first so that upon failures undeleted metadata can still be reached.\\nThis cleanup is not needed for strict correctness from a MailboxManager point of view thus it could be carried out\\nasynchronously, via mailbox listeners so that it can be retried.\\n"}
{"prompt":"## Context\\nThe architectural technical material has no single definitive set of items identified as building blocks that can operate across all the departments and spaces at the same level of granularity in a re-usable way.\\nWhile this is the case if design and implementation are functions which generate outputs which cross between different spaces, friction and incompatibility are a problem.\\n","completion":"## Decision\nWe will identify the component parts of technical guidance material we have collected and used to date across the spaces within architecture and will work to de-duplicate, and arrive at a shared understanding of the definition of these pieces.  We will in addition work to gain a shared understanding on what the correct level of granularity is for these working parts.\\nNot exhaustive yet, but below is our initial suggestion for a list of component types:\\n* goals\\n* principles\\n* anti-patterns\\n* patterns\\n* constraints\\n* decision trees\\n"}
{"prompt":"## Context\\nThe existing print holdings system keeps all data in MySQL. While this allows\\nflexible querying, it is also computationally expensive and is difficult to\\nscale.\\nKnown use cases largely involve computing things specific to a clusters'\\nholdings and HathiTrust items, so one way to parallelize queries is in a\\nmap-reduce fashion - compute something about each cluster, then process the\\ndata from each cluster into a final result.\\n","completion":"## Decision\nWe will use MongoDB for a persistent data store. Each print holdings cluster\\nwill be a document, and will contain holdings, HathiTrust items, and shared\\nprint commitments for that cluster as sub-documents.\\n"}
{"prompt":"## Context\\nTendermint is required to monitor peer quality in order to inform its peer dialing and peer exchange strategies.\\nWhen a node first connects to the network, it is important that it can quickly find good peers.\\nThus, while a node has fewer connections, it should prioritize connecting to higher quality peers.\\nAs the node becomes well connected to the rest of the network, it can dial lesser known or lesser\\nquality peers and help assess their quality. Similarly, when queried for peers, a node should make\\nsure they dont return low quality peers.\\nPeer quality can be tracked using a trust metric that flags certain behaviours as good or bad. When enough\\nbad behaviour accumulates, we can mark the peer as bad and disconnect.\\nFor example, when the PEXReactor makes a request for peers network addresses from an already known peer, and the returned network addresses are unreachable, this undesirable behavior should be tracked. Returning a few bad network addresses probably shouldn\u2019t cause a peer to be dropped, while excessive amounts of this behavior does qualify the peer for removal. The originally proposed approach and design document for the trust metric can be found in the [ADR 006](adr-006-trust-metric.md) document.\\nThe trust metric implementation allows a developer to obtain a peer's trust metric from a trust metric store, and track good and bad events relevant to a peer's behavior, and at any time, the peer's metric can be queried for a current trust value. The current trust value is calculated with a formula that utilizes current behavior, previous behavior, and change between the two. Current behavior is calculated as the percentage of good behavior within a time interval. The time interval is short; probably set between 30 seconds and 5 minutes. On the other hand, the historic data can estimate a peer's behavior over days worth of tracking. At the end of a time interval, the current behavior becomes part of the historic data, and a new time interval begins with the good and bad counters reset to zero.\\nThese are some important things to keep in mind regarding how the trust metrics handle time intervals and scoring:\\n- Each new time interval begins with a perfect score\\n- Bad events quickly bring the score down and good events cause the score to slowly rise\\n- When the time interval is over, the percentage of good events becomes historic data.\\nSome useful information about the inner workings of the trust metric:\\n- When a trust metric is first instantiated, a timer (ticker) periodically fires in order to handle transitions between trust metric time intervals\\n- If a peer is disconnected from a node, the timer should be paused, since the node is no longer connected to that peer\\n- The ability to pause the metric is supported with the store **PeerDisconnected** method and the metric **Pause** method\\n- After a pause, if a good or bad event method is called on a metric, it automatically becomes unpaused and begins a new time interval.\\n","completion":"## Decision\nThe trust metric capability is now available, yet, it still leaves the question of how should it be applied throughout Tendermint in order to properly track the quality of peers?\\n### Proposed Process\\nPeers are managed using an address book and a trust metric:\\n- The address book keeps a record of peers and provides selection methods\\n- The trust metric tracks the quality of the peers\\n#### Presence in Address Book\\nOutbound peers are added to the address book before they are dialed,\\nand inbound peers are added once the peer connection is set up.\\nPeers are also added to the address book when they are received in response to\\na pexRequestMessage.\\nWhile a node has less than `needAddressThreshold`, it will periodically request more,\\nvia pexRequestMessage, from randomly selected peers and from newly dialed outbound peers.\\nWhen a new address is added to an address book that has more than `0.5*needAddressThreshold` addresses,\\nthen with some low probability, a randomly chosen low quality peer is removed.\\n#### Outbound Peers\\nPeers attempt to maintain a minimum number of outbound connections by\\nrepeatedly querying the address book for peers to connect to.\\nWhile a node has few to no outbound connections, the address book is biased to return\\nhigher quality peers. As the node increases the number of outbound connections,\\nthe address book is biased to return less-vetted or lower-quality peers.\\n#### Inbound Peers\\nPeers also maintain a maximum number of total connections, MaxNumPeers.\\nIf a peer has MaxNumPeers, new incoming connections will be accepted with low probability.\\nWhen such a new connection is accepted, the peer disconnects from a probabilistically chosen low ranking peer\\nso it does not exceed MaxNumPeers.\\n#### Peer Exchange\\nWhen a peer receives a pexRequestMessage, it returns a random sample of high quality peers from the address book. Peers with no score or low score should not be inclided in a response to pexRequestMessage.\\n#### Peer Quality\\nPeer quality is tracked in the connection and across the reactors by storing the TrustMetric in the peer's\\nthread safe Data store.\\nPeer behaviour is then defined as one of the following:\\n- Fatal - something outright malicious that causes us to disconnect the peer and ban it from the address book for some amount of time\\n- Bad - Any kind of timeout, messages that don't unmarshal, fail other validity checks, or messages we didn't ask for or aren't expecting (usually worth one bad event)\\n- Neutral - Unknown channels\/message types\/version upgrades (no good or bad events recorded)\\n- Correct - Normal correct behavior (worth one good event)\\n- Good - some random majority of peers per reactor sending us useful messages (worth more than one good event).\\nNote that Fatal behaviour causes us to remove the peer, and neutral behaviour does not affect the score.\\n"}
{"prompt":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n","completion":"## Decision\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n"}
{"prompt":"## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n","completion":"## Decision\nWe will use [TypeScript](https:\/\/github.com\/Microsoft\/TypeScript) for implementing all of Alfa. Being a superset of JavaScript, TypeScript has a low learning curve for people already familiar with JavaScript while providing a solid type system. We will enforce API contracts through generation of [declaration files](https:\/\/www.typescriptlang.org\/docs\/handbook\/declaration-files\/introduction.html) which will dictate the API surface that consumers can access. To the extent possible, we will keep a strict TypeScript configuration in order to catch as many issues as possible at compile time. In particular, this entails strict `undefined` and `null` checking in order to get rid of a previously common type of bug.\\n"}
{"prompt":"## Context\\nShould we depend on running in an unix environment with support for e.g. `sh` commands.\\n","completion":"## Decision\nWe depend on linux and therefore reduce the overhead of maintaining linux and windows support.\\n"}
{"prompt":"## Context and Problem Statement\\nThe JabRef repository contains Markdown (`.md`) files documenting the JabRef code.\\nThe documentation contains links to external resources.\\nFor high-quality documentation, external links should be working.\\n## Decision Drivers\\n* Checking external links should not cause issues in the normal workflow\\n","completion":"## Decision\n* Checking external links should not cause issues in the normal workflow\\nChosen option: \"\\[option 1\\]\", because \\[justification. e.g., only option, which meets k.o. criterion decision driver \\| which resolves force force \\| \u2026 \\| comes out best \\(see below\\)\\].\\n### Positive Consequences\\n* Automatic notification of broken external links\\n### Negative Consequences\\n* Some external sites need to [be disabled](https:\/\/github.com\/JabRef\/jabref\/pull\/6542\/files). For instance, GitHub.com always returns \"forbidden\". A [filter for status is future work of the used tool](https:\/\/github.com\/tcort\/markdown-link-check\/issues\/94#issuecomment-634947466).\\n"}
{"prompt":"## Context\\nWe need to choose tooling for various continuous integration and deployment\\ntasks:\\n- running automated tests on branches\/pull requests\\n- running security and code quality checks on branches\/pull requests\\n- building Docker images\\n- deploying applications to our environments\\nJenkins and CircleCI are commonly used at MOJ for these tasks, and the team\\nhave experience with both of them, as well as other options such as Travis.\\nThe Cloud Platform team operate a Jenkins server but are keen to move services\\naway from it, especially anything which doesn't use Template Deploy.\\nThe LAA fee calculator is built and deployed using CircleCI with the new cloud\\nplatform, so we'd have an example to follow if we also chose CircleCI. That\\nteam didn't evaluate a lot of options when they made that decision, but it's\\nworking well so far for them. Our applications are a little more complex than\\ntheirs since we have a database, but we don't expect to be doing anything\\nunusual in our build and deploy pipelines.\\nThere's also some [documentation](https:\/\/ministryofjustice.github.io\/cloud-platform-user-docs\/02-deploying-an-app\/004-use-circleci-to-upgrade-app\/)\\non using CircleCI with Helm for continuous deployment to the new cloud\\nplatform - we may not want to follow it exactly, but it covers some useful\\ntopics.\\nOur team haven't been practising continuous deployment (that is, deploying\\nevery change automatically to production) on Visit someone in prison - deploys\\nto production for that service need to be triggered by a human. We'd like to\\nkeep that option open for this service, though.\\nIt's important to us that our build and deployment configuration is managed in\\ncode. We've decided to start out with two applications (see [ADR 0004](0004-separate-api-and-user-facing-applications.md))\\nand want to be able to easily make and reproduce changes to builds and\\ndeployments for both.\\nWe're keen to get started quickly - we'd rather not spend time assessing lots\\nof tooling options at this stage. Our needs will evolve anyway, so it's fine\\nto pick something that works for us now and revisit that decision later on if\\nwe need to.\\n","completion":"## Decision\nWe will use CircleCI for continuous integration.\\nWe will use CircleCI for deploying.\\n"}
{"prompt":"## Context\\nThe decision to use Github, and git itself, as the versioning system for the forms was not recorded at the time so this is in place of that.\\nThe Form Builder platform needs a place to store the JSON metadata that the editor generates. This metadata is used by the Runner Node's in order to create the forms upon deployment.\\n","completion":"## Decision\nThe main reasons for choosing git and Github:\\n- Transparency. All the forms are published in the open for the public to see.\\n- Versioning. Git itself is a versioning tool and Github provides a user friendly view into that.\\n- Collaboration. Github provided a built in approach to collaboration that was felt to be necessary for the future users of the editor.\\n- It was low effort to integrate on the premise that the initial users of the editor would be either developers or those with some background of git.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context and Problem Statement\\nSource of data and information to be decided based on the requirements for **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.\\n**_Platform 1_**\\n**Name** : Budget - Government of Uttar Pradesh\\n**URL** : http:\/\/budget.up.nic.in\/\\n**Usage** : Analyse budget documents and understand accounting information available.\\n**_Platform 2_**\\n**Name** : Koshvani web -- A Gateway to Finance Activities in the State of Uttar Pradesh\\n**URL** : http:\/\/koshvani.up.nic.in\/\\n**Usage** : Scrape, analyse and publish open budget data for public consumption.\\n## Decision Drivers\\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\n","completion":"## Decision\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\nExplore both platforms to understand the open fiscal data available and techinical requirements.\\n"}
{"prompt":"## Context\\n`cf logs` for an app shows logs both emitted by that app, and access logs from\\nthe Gorouter. The access logs from Gorouter look like this in the logstream:\\n```\\n2020-06-25T23:42:19.00+0000 [<source_type>\/<instance_id>] OUT <log>\\n```\\nIn cf-for-k8s, an app developer should similarly be able to see access logs as\\nrequests for the app travel through the routing tier. One discussion we had was\\nwhether these access logs should come from the ingressgateway envoy and\/or from the\\napp sidecar envoys.\\nOne important piece of functionality in cf-for-BOSH is that when an app process has been killed, healthcheck requests still\\nshow up in the access log stream. This is because those requests still make it\\nto the Gorouter, even though they do not make it to the app itself.\\nThis is an example of an access log of a healthcheck request to a killed app.\\nThe 503 is being returned directly from the Gorouter.\\n```\\n2020-07-06T10:45:55.83-0700 [RTR\/0] OUT dora.maximumpurple.cf-app.com - [2020-07-06T17:45:55.828757970Z] \"GET \/health HTTP\/1.1\" 503 0 24 \"-\" \"curl\/7.54.0\" \"35.191.2.88:63168\" \"10.0.1.11:61002\" x_forwarded_for:\"76.126.189.35, 34.102.206.8, 35.191.2.88\" x_forwarded_proto:\"http\" vcap_request_id:\"0cd79f32-3cde-4eea-5853-9a2ca401be40\" response_time:0.004478 gorouter_time:0.000433 app_id:\"1e196708-3b2d-4edc-b5b8-bf6b1119d802\" app_index:\"0\" x_b3_traceid:\"7f470cc2fcf44cc6\" x_b3_spanid:\"7f470cc2fcf44cc6\" x_b3_parentspanid:\"-\" b3:\"7f470cc2fcf44cc6-7f470cc2fcf44cc6\"\\n```\\nWe've done some previous work exploring access logs on the Istio Envoy\\nIngressGateway (see related stories section below) and have [documented some of\\nthe fields\\nhere](https:\/\/github.com\/cloudfoundry\/cf-k8s-networking\/blob\/37dabf7907ffa7b284980cfcb6813ebcd449736c\/doc\/access-logs.md).\\n","completion":"## Decision\nWe decided to have the access logs come from the ingressgateway to begin with,\\nas we think those provide the most valuable information.\\nImagine a scenario where an app has crashed and the Pod is being rescheduled.\\nThe Envoy on the ingressgateway will still log this failed request. The sidecar,\\non the other hand, would be unreachable so it would not be able to log anything.\\nHaving the failed request in the access logs in this scenario could be valuable\\ninformation for a developer attempting to debug their app with `cf logs`.\\nWe also decided that the access log format would be JSON with the [following\\nfields](https:\/\/docs.google.com\/spreadsheets\/d\/1CuvoUEkiizVKvSZ2IaLya40sgMbm5at78CqxB8uUe80\/edit#gid=0)\\nThe work to enable this was completed in [#173568724](https:\/\/www.pivotaltracker.com\/story\/show\/173568724).\\n"}
{"prompt":"## Context\\nAfter adopting the last specifications of JMAP (see\\n[new JMAP specifications adoption ADR](https:\/\/github.com\/apache\/james-project\/blob\/master\/src\/adr\/0018-jmap-new-specs.md)),\\nit was agreed that we need to be able to serve both `jmap-draft` and the new `jmap` with a reactive server.\\nThe current outdated implementation of JMAP in James is currently using a non-reactive [Jetty server](https:\/\/www.eclipse.org\/jetty\/).\\nThere are many possible candidates as reactive servers. Among the most popular ones for Java:\\n* [Spring](https:\/\/spring.io)\\n* [Reactor-netty](https:\/\/github.com\/reactor\/reactor-netty)\\n* [Akka HTTP](https:\/\/doc.akka.io\/docs\/akka-http\/current\/introduction.html)\\n* ...\\n","completion":"## Decision\nWe decide to use `reactor-netty` for the following reasons:\\n* It's a reactive server\\n* It's using [Reactor](https:\/\/projectreactor.io\/), which is the same technology that we use in the rest of our codebase\\n* Implementing JMAP does not require high level HTTP server features\\n"}
{"prompt":"## Context\\nThe \"status\" of a step-by-step is determined from data about the step-by-step, e.g. if there is a `draft_updated_at` date but not a `published_at` date, then it has a status of `draft` etc.\\nThat worked until now, when there were only had 4 statuses:\\n* draft\\n* live\\n* scheduled\\n* unpublished changes\\nImplementing the 2i workflow will need the addition of least three more statuses:\\n* submitted for 2i\\n* in review\\n* 2i approved\\nThis will make the code to determine the status from the surrounding information very complicated.\\n### What status information is needed?\\nAs well as knowing what part of the 2i workflow a step-by-step is in, the step-by-step also needs to know if it has been previously published. This is so that the correct options are given to users to discard their changes. For example, if a step-by-step has previously been published, it can\u2019t be deleted.\\n### 2i workflows\\nThere are two \"happy path\" 2i workflows that a step-by-step can follow\\n### A new step-by-step is created\\n|Action                       |Publication State|Workflow State      |\\n|-----------------------------|:---------------:|-------------------:|\\n|Create new step-by-step      |Draft            |N\/A                 |\\n|Makes changes to step-by-step|Draft            |N\/A                 |\\n|Submit for 2i                |Draft            |Submitted for 2i    |\\n|Claim Review                 |Draft            |In Review           |\\n|2i Approved                  |Draft            |2i Approved         |\\n|2i Requests change           |Draft            |N\/A                 |\\n|Publish                      |Published        |N\/A                 |\\n|Schedule                     |Draft            |Scheduled           |\\n|Unschedule                   |Draft            |2i Approved         |\\n|Delete                       |Draft            |N\/A                 |\\n### A published step-by-step is updated\\n|Action                       |Publication State  |Workflow State      |\\n|-----------------------------|:-----------------:|-------------------:|\\n|Update published step-by-step|Published          |N\/A                 |\\n|Make changes to step-by-step |Unpublished changes|N\/A                 |\\n|Submit for 2i                |Unpublished changes|Submitted for 2i    |\\n|Claim Review                 |Unpublished changes|In Review           |\\n|2i Approved                  |Unpublished changes|2i Approved         |\\n|2i Requests changes          |Unpublished changes|N\/A                 |\\n|Publish                      |Published          |N\/A                 |\\n|Schedule                     |Unpublished changes|Scheduled           |\\n|Unschedule                   |Unpublished changes|2i Approved         |\\n|Discard Draft                |Published          |N\/A                 |\\n|Unpublish                    |Draft              |N\/A                 |\\n### Status assumptions\\n- Can only publish if workflow state is 2i Approved\\n- Can only schedule if workflow state is 2i Approved\\n- Can only Claim Review if workflow state is Submitted for 2i and the Claimer is not the same user as the one that submitted for 2i\\n- After 2i has been approved, the user can submit for 2i again\\n- If the 2i reviewer requests changes, the workflow state is reset and the 2i workflow starts again\\n- Can only delete a step-by-step if it\u2019s never been published\\n- \"live\" will be renamed to \"published\" to be consistent with the other publishing apps.\\n","completion":"## Decision\nCreate a new field in the database to store a value of `status` that allows values of:\\n- draft\\n- published\\n- scheduled\\n- submitted_for_2i\\n- in_review\\n- approved_2i\\n\"draft\" and \"unpublished changes\" mean the same thing in regards to their position in the publishing workflow, so if they are consolidated it won't be necessary to keep track of separate publication and workflow states. The presence of a `published_at` date can be used to determine if something has been previously published, as it is now.\\nThat means the 2i workflow can be simplified.\\n### 2i workflow\\n|Action                                  |Starting Status |End Status          |\\n|----------------------------------------|:--------------:|-------------------:|\\n|Create new\/update published step-by-step|N\/A or published|draft               |\\n|Makes changes to step-by-step           |draft           |draft               |\\n|Submit for 2i                           |draft           |submitted_for_2i    |\\n|Claim Review                            |submitted_for_2i|in_review           |\\n|2i Approved                             |in_review       |approved_2i         |\\n|2i Requests changes                     |in_review       |draft               |\\n|Publish                                 |approved_2i     |published           |\\n|Schedule                                |approved_2i     |scheduled           |\\n|Unschedule                              |scheduled       |approved_2i         |\\n### Other actions\\n- Delete: Only if step-by-step is not scheduled or been previously published\\n- Discard Draft: Only if step-by-step is not scheduled and has been previously published\\n- Unpublish: Only if step-by-step is not scheduled and has been previously published\\n### Pros\\n- It will be easy to expand the allowed statuses to include factcheck statuses.\\n- It will be easier to query and filter step-by-steps by status.\\n- The status can be used by internal change notes to give a better description of what has changed.\\n### Cons\\n- Extra validation will be needed to ensure that a status can be set. E.g Cannot set to `in_review` if there is no `reviewer` recorded.\\n- Existing step-by-steps will need to be updated to store a status.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context\\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\\nWe need to make a decision, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\\nFrom various discussions, we have a few findings that guide our decision:\\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\\n- We have two separate concerns that needs two separate configurations\\n","completion":"## Decision\n- We will define one new CRD object to configure topics and access to this\\n- App developers will create this in the cluster when deploying their application\\n- Kafkarator will watch this CRD and take needed actions\\n- App developers will add configuration to their Application resource listing kafka pools they need access to\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in\\n[this article.][1]\\n"}
{"prompt":"## Context\\nData storage and management is a key component of the tta smarthub platform. At a fundamental level, a relational as well as NoSQL systems were considered. Because the platform's data is mainly structured  and with a need for frequent queries and reports, a relational database management system was viewed as more suitable. With that in mind we looked at MySQL and PostreSQL (Postgres) both open source and popular choices.\\n","completion":"## Decision\nWhile both databases provide adequate storage and management, especially with updates provided by version 8 of MySQL, in the end Postgres was chosen. The main advantages of Postgres are implementations leading to better concurrency, specifically the MVCC (Multiversion Concurrency Control) without the need to use read locks, transactional ACID (Atomicity, Consistency, Isolation, Durability) support making the system less vulnerable to data corruption. Postgres also supports partial indexes, can create indexes in a non-blocking way, multiple cores, GIN\/GIST indexing accelerating full-text searches.\\n"}
{"prompt":"Context\\n=======\\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\\nThere are 3 main network sections between the user and the application:\\n* User to ELB\\n* ELB to router\\n* Router to cells\\nDecision\\n========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","completion":"## Decision\n========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n"}
{"prompt":"**Context**: operations that change state in the server may complete with an\\nindeterminate status code. For example: an error in the 500 range may be\\nproduced by a middle-tier server after the operation was started by the backend.\\nRe-issuing the request may not work as expected for the application developer.\\n**Decision**: by default the library will retry all requests, including\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n","completion":"## Decision\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n"}
{"prompt":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum.\\n","completion":"## Decision\nWe will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n"}
{"prompt":"## Context\\nThe quality of information included in our pull requests varies greatly which\\ncan lead to code reviews which take longer and are harder for the person to\\nunderstand the considerations, outcomes and consquences of a series of changes.\\nA couple of recent projects have found a GitHub pull request template to have\\nbeen a positive change. Prompting what pull request descriptions should include\\nhas lead to better documented changes that have been easier to review on the\\nwhole.\\n","completion":"## Decision\nInclude a basic pull request template for GitHub so that every pull request\\nprompts every author to fill it out.\\n"}
{"prompt":"## Context\\n`menu-generation` application needs to expose data and services in order to be used by multiple different applications,\\neither web or mobile front-ends or any other application that will reuse a sub-part of the `menu-generation`\\napplication or enhance the provided services.\\nWe want the application data and services to be easily consumed through well established communication protocols, such\\nas HTTP. It must be intuitive to manipulate the `menu-generation` application as a self-discoverable API, without\\nmanaging cumbersome protocols or data formats.\\n","completion":"## Decision\n`menu-generation` application data and services will be exposed through a REST API.\\n"}
{"prompt":"## Context\\nWe designed the API layer (known as the backend) to be a custom set of services. This allows the user to tailor it to\\ntheir own requirements, and introduce custom code as needed.\\nThe original plan was to make the presentation layer (known as the frontend) a monolith. That is, a single service that\\ngenerates all the HTML pages.\\nAs a monolith, it is hard, if not impossible, to change or replace parts without having to replace the whole. This would\\nmean a lot of configuration options to support differing requirements. It would also limit adoption of the platform\\nas it can't meet them.\\nWe investigated the use of micro frontends in [#371][Spike].\\n","completion":"## Decision\nWe will create separate services for individual, and groups of, pages. The user will use a proxy to make them appear as\\na single service. The frontends are independent of each other, and are always optional.\\n"}
{"prompt":"## Context\\nFairly early on in the rebuild of the Classifier, we started using newer technologies such as [CSS Grid](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/CSS\/CSS_Grid_Layout), which are not supported in older browsers like Internet Explorer 11.\\nAnd, while Edge is now several major versions in, we still have a percentage of users on IE11. As such, we're now at a point where we have to determine whether to drop support for legacy browsers and risk inconvenience for that segment, or invest significant time in coding and testing fallbacks.\\n","completion":"## Decision\nWe will only officially support the following browsers:\\n### Desktop\\n- Safari\\n- Chrome\\n- Firefox\\n- Edge\\n### Mobile\\n- Safari\\n- Chrome\\n- Opera\\nOf these, we will support the current and last two major versions.\\n"}
{"prompt":"## Context\\n> This section contains all the context one needs to understand the current state, and why there is a problem. It should be as succinct as possible and introduce the high level idea behind the solution.\\n","completion":"## Decision\n> This section explains all of the details of the proposed solution, including implementation details.\\n> It should also describe affects \/ corollary items that may need to be changed as a part of this.\\n> If the proposed change will be large, please also indicate a way to do the change to maximize ease of review.\\n> (e.g. the optimal split of things to do between separate PR's)\\n"}
{"prompt":"## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n","completion":"## Decision\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\nChosen option: \"Monocle API provides an interface for external task data crawlers\", because it will ease integration between Monocle and the\\nvarious task trackers available in the market.\\nA Monocle operator will need to write the crawler for its\\nown task tracker. However, over the time, operators might have written crawlers for most popular systems and released them under a open source license.\\nFrom the Monocle side, we provide a clear API for a task tracker\\ncrawler to push task related data to Monocle. Each task data sent to Monocle must at least set a predifined set of generic attributes like\\n\"severity\", \"change_url\", \"title\", ... The monocle API is then able to\\nfind corresponding Changes in the database that match the \"change_url\" field.\\nEach changes get a new attribute called \"task_data\" that is a list of\\nrelated task data records. Indeed multiple tasks might be related to a\\nsingle change.\\nAlso, Monocle keeps track of task data that do not match any Changes in the\\ndatabase. The Monocle change crawlers engine triggers a search for adoption of orphaned task data.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context\\nTo allow this project to be used by other parties (mostly non-Dutch speaking), it will be necessary to be able to translate all content in the application that is not coming from the APIs.\\n","completion":"## Decision\nThe application's content will be stored in translation files. The code will make use of [React-Intl](https:\/\/github.com\/formatjs\/react-intl). Also see the [i18n documentation](https:\/\/github.com\/react-boilerplate\/react-boilerplate\/blob\/master\/docs\/js\/i18n.md) in the [react-boilerplate repository](https:\/\/github.com\/react-boilerplate\/react-boilerplate).\\n"}
{"prompt":"## Context\\nKotlin has become the language of choice for back end development within HMPPS Digital.\\n","completion":"## Decision\nAdopt Kotlin as preferred language for new code within court-case-service. Where practical, migrate classes as we change them.\\n"}
{"prompt":"## **Context**\\nCurrently, access to Manage my Home (MMH) service is managed by Google groups - people are added to a Group related to their Hackney team (e.g. income-team) and the Google group is added to the list of allowed groups for accessing both the MMH frontend and backend APIs.\\nThe number of allowed Google groups is continuously increasing and is introducing additional complexity and maintenance overhead for the following areas:\\n1. Front end - each time a new Google group needs to be allowed to access the application, the front end needs to be redeployed as it maintains the list of allowed groups in an environment variable. This introduces delays and adds complexity to the process.\\n2. Back end APIs - similarly to the front end, each time a new Google group needs to be added to the list of groups allowed to access the service, a DynamoDB table needs to be updated to allow the front end to invoke our APIs. The API authentication mechanism is user-based and the logged in user must belong to at least one group allowed to access the API.\\n3. We are at a stage with an increasing need for permissions management within the service. Currently people who can access the service are allowed to perform all actions that it implements. Having a high number of groups will make future permissions management more complicated and will require continuous maintenance.\\nThe following solution is considered to remove the issues described above:\\nCreate several Google groups specifically for accessing MMH. The number of groups is dependent on the number of roles identified within the service. For example, there might be three groups:\\nmmh-officer-access\\nmmh-manager-access\\nmmh-read-only-access\\nBy introducing Google groups specific to the service, future access will be provided by adding individuals to the Google group relevant to the type of access they should have.\\nManagement of those Google groups will be given to the PO and the project team, thus reducing the dependency on other Hackney staff members, external to the project team, to add individuals to team-specific Google groups. Moreover, this will provide greater visibility over who is part of the given Google group and provide better control over managing access to the service.\\nThis approach is already successfully followed in other projects in Hackney like Repairs.\\n","completion":"## Decision\n**Introduce project specific Google groups for accessing the service**\\nThis option has been chosen because it will reduce the maintenance of access to the MMH service by introducing the following benefits:\\n1. Front end will not need to be redeployed each time a new individual needs to be given access to MMH as the individual will be added to a Google group already recognised by the front end configuration as allowed to access.\\n2. Back-end APIs access will not need to be updated frequently for the same reason described in point one.\\n3. Role-based Google groups reduce the complexity of any permissions management solution that will be based around Google groups as there will be less Google groups mapping to roles to maintain.\\n"}
{"prompt":"## Context\\nEvery remote environment in the cluster should be accessible only for clients authorized to this particular remote environment.\\nThis requirement raises a need for a mechanism that will differentiate between clients' permissions.\\n### Multiple server certificates\\nHaving separate server certificate per remote environment would make it hard to manage all those certificates.\\n### Single certificate\\nHaving only one certificate creates the problem that the client who obtained signed certificate for accessing one of the existing remote environments can access all of them using this particular certificate.\\n","completion":"## Decision\nThe decision is to check the Distinguished Name of the client's certificate in Ingress-Nginx configuration using `nginx.ingress.kubernetes.io\/configuration-snippet` in individual ingresses and grant permissions only in case the client certificate's Common Name matches the required one.\\n"}
{"prompt":"## Context\\nKorin would look much better with artist images. Unfortunately LastFm [removed artist images][lastfm-images-gone] from their API and have replaced it with a placeholder. There are some options to get artist images which are:\\n- [Music story][music-story-api]: The documentation is not easy to understand and the SDK is designed for web browsers. It also uses a custom ID to associate artists, not IMDB. One unique feature is that it has lyrics too.\\n- [Music brainz][music-brainz-api]: Creative commons but has everything except artist images.\\n- [Spotify][spotify-api]: Has artist information including images but uses a custom ID, not IMDB.\\n","completion":"## Decision\nSpotify is the only viable alternative.\\n"}
{"prompt":"## Context\\nThe data warehouse is no longer an application with an UI. It is an API that is\\nconsumed by other applications in order to get metrics through time for content\\nitems of GOV.UK.\\nThe name of the application is not compliant with GOV.UK guidelines for naming.\\n","completion":"## Decision\nRename Content Performance Manager to Content Data Admin\\n"}
{"prompt":"# Context and Problem Statement\\nDocspell currently only supports PDF files. This has simplified early\\ndevelopment and design a lot and so helped with starting the project.\\nHandling pdf files is usually easy (to view, to extract text, print\\netc).\\nThe pdf format has been chosen, because PDFs files are very common and\\ncan be viewed with many tools on many systems (i.e. non-proprietary\\ntools). Docspell also is a document archive and from this perspective,\\nit is important that documents can be viewed in 10 years and more. The\\nhope is, that the PDF format is best suited for this. Therefore all\\ndocuments in Docspell must be accessible as PDF. The trivial solution\\nto this requirement is to only allow PDF files.\\nSupport for more document types, must then take care of the following:\\n- extracting text\\n- converting into pdf\\n- access original file\\nText should be extracted from the source file, in case conversion is\\nnot lossless. Since Docspell can already extract text from PDF files\\nusing OCR, text can also be extracted from the converted file as a\\nfallback.\\nThe original file must always be accessible. The main reason is that\\nall uploaded data should be accessible without any modification. And\\nsince the conversion may not always create best results, the original\\nfile should be kept.\\n# Decision Drivers\\nPeople expect that software like Docspell support the most common\\ndocument types, like all the \u201coffice documents\u201d (`docx`, `rtf`, `odt`,\\n`xlsx`, \u2026) and images. For many people it is more common to create\\nthose files instead of PDF. Some (older) scanners may not be able to\\nscan into PDF files but only to image files.\\n# Considered Options\\nThis ADR does not evaluate different options. It rather documents why\\nthis feature is realized and the thoughts that lead to how it is\\nimplemented.\\n# Realization\\n","completion":"## Decision\nPeople expect that software like Docspell support the most common\\ndocument types, like all the \u201coffice documents\u201d (`docx`, `rtf`, `odt`,\\n`xlsx`, \u2026) and images. For many people it is more common to create\\nthose files instead of PDF. Some (older) scanners may not be able to\\nscan into PDF files but only to image files.\\n# Considered Options\\nThis ADR does not evaluate different options. It rather documents why\\nthis feature is realized and the thoughts that lead to how it is\\nimplemented.\\n# Realization\\n"}
{"prompt":"## Context and Problem Statement\\nDocument uploads will be stored in Azure Blob storage because of the requirement to store very large files & the fact that Azure Blob storage offers high availability and low costs.\\nThe Azure Blob storage system is a separate cloud service which is not directly linked to Dynamics CRM, therefore we need to consider how the permits, stored using CRM case\/incident entities, will be linked to the the files within Azure.\\n## Decision Drivers\\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n","completion":"## Decision\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n[Option 1] Use the existing 'annotation' entity in the CRM & set the entity attributes to match Azure blob data.\\n### Positive Consequences\\n* Minimises customisation of the CRM data, reuse of existing\\n### Negative Consequences\\n* Labelling within the CRM does not match the context of the upload.\\n"}
{"prompt":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","completion":"## Decision\nThe change that we're proposing or have agreed to implement.\\n"}
{"prompt":"## Context\\nThe web frontend of time-tracker is crucial, since it is the only way to access certain important features such as user management and invoicing. It will be built as a single-page application. Web frontend development is complex nowadays, and choosing the right stack can help mitigate this complexity and speed up the development process. We are evaluating Typescript and ClojureScript.\\nTypescript is widely used and has a lot of mindshare. As such, setting up a Typescript project and making use of the Typescript\/Javascript ecosystem is straightforward.\\nClojureScript offers a number of advantages over Typescript, such as programming in a functional-first style with immutable data structures, LISP syntax, seamless live code reloading, a fast REPL-driven workflow and generally less code and boilerplate. In addition to this, the [`re-frame`](http:\/\/day8.github.io\/re-frame\/) framework is a well thought-out approach to state management. It manages state and side effects very well and with much less boilerplate compared to popular JS\/TS equivalents such as Redux. All programmers at nilenso are familiar with Clojure (if not ClojureScript already), and learning ClojureScript and getting productive shouldn't be an issue for them.\\nOne potential concern with ClojureScript is easy access to the JavaScript\/NPM ecosystem of libraries and tooling features. This concern is addressed by `shadow-cljs`, a popular build tool for ClojureScript which (among other things) makes access to NPM packages as frictionless as possible.\\nSome features readily available in JS\/TS such as CSS modules are tricky to set up or unusable with ClojureScript.\\n","completion":"## Decision\n1. We will use [ClojureScript](https:\/\/clojurescript.org\/) as the programming language for the web frontend.\\n2. We will use [re-frame](http:\/\/day8.github.io\/re-frame\/) as the main application framework.\\n3. We will use [shadow-cljs](http:\/\/shadow-cljs.org\/) as the build tool for ClojureScript. For other assets such as CSS, we will use other build tooling as necessary.\\n"}
{"prompt":"## Context\\nIn clojure a normal work flow is use the repl. The problem is that when you reload the appliction the states die.\\n[mount](https:\/\/github.com\/tolitius\/mount) is here to preserve all the Clojure superpowers (powerful, simple and fun) while making the application state enjoyably reloadable.\\nDepending on how application state is managed during development, the above three superpowers can either stay, go somewhat, or go completely.\\n","completion":"## Decision\nUse mount libray and clojure tools space.\\nThe decision of mount over component is made afer review bouth solutions. My feeling is\\n* Mount is more clojure dialect oriented\\n* Mount use namespace and component\\nRecords, this made that the compliler control the dependencies\\n* Mount is [less contagious](https:\/\/engineering.riotgames.com\/news\/taxonomy-tech-debt)\\n"}
{"prompt":"## Context\\nMust be free! Python GUI libs to make a fancy clean lovely UI.\\n","completion":"## Decision\nQuestioning if it's actually worth setting up a GUI or if the python project should just look at spitting out data to be consumed by some kind of dashboard system like \"thingsBoard\". Arch thinking in next decision record. Postponing until the architectural overview is thought out!\\n"}
{"prompt":"## Context\\nThe MyMeetings domain contains 4 main subdomains: Meetings (core domain), Administration (supporting subdomain), Payments (supporting subdomain) and User Access (generic domain).\\nWe use Modular Monolith architecture so we need to implement one application which solves all requirements from all domains listed above.\\nWe need to modularize our system.\\n2. Create 4 modules based on Bounded Contexts which in this scenario maps 1:1 to domains. This solution is more difficult at the beginning. We need to set modules boundaries, communication strategy between modules and have more advanced infrastructure code. It is a more complex solution. On the other hand, it supports autonomy, maintainability, readability. We can develop our Domain Models in all of the Bounded Contexts independently.\\n","completion":"## Decision\nSolution 2.\\nWe created 4 modules: Meetings, Administration, Payments, User Access. The key factor here is module autonomy and maintainability. We want to develop each module independently. This is more cleaner solution. It involves more work at the beginning but we want to invest.\\n"}
{"prompt":"## Context\\n### Things we don't need\\n1. It's not going to be a very fancy site\\n2. It's not going to be very big\\n3. It's not going to be very customizable\\n4. It's not important to be able to embed this in another site\\n### Things we do need\\n1. really simple interface\\n2. really simple to deploy\\n3. I'd like to be able to re-use this in new contexts easily\\n- that means I'll have forgotten how to use it and will need to re-learn\\nquickly\\n","completion":"## Decision\nOutput a single HTML file with embedded JavaScript and CSS.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nWe want to be confident about the code we write, and for it to be\\nself-documenting as much as possible.\\n[TypeScript](https:\/\/www.typescriptlang.org\/) is a compiled language with\\noptional typing. It's a superset of JavaScript, so is familiar to developers who\\nknow JavaScript. It has wide editor support.\\nAs of writing, TypeScript is used by over\\n[1.4 million repositories](https:\/\/github.com\/microsoft\/TypeScript\/network\/dependents?package_id=UGFja2FnZS01MTE3ODUxNjg%3D)\\non GitHub.\\n","completion":"## Decision\nWe will use TypeScript.\\n"}
{"prompt":"## Context\\nWe need a communication mechanism between modules. We can let each module directly consume the API of another module, or\\nwe could let each module subscribe to events published by other modules\\n","completion":"## Decision\nSince in [ADR 2](0002-start-with-modular-monolith-backend.md) we decided to be ready for microservices, we will use the\\nsecond approach - events as the communication mechanism\\n"}
{"prompt":"## Context\\nOur postcode checker uses a database to store valid service areas and allowed post codes.\\nCurrently our requirements involve looking up a postcode using the [postcodes.io](https:\/\/postcodes.io).\\nIn future we may want to build features that involve more sophisticated geolocation capabilities. Most databases do not support geolocation natively.\\nPostgres is the most geolocation capable SQL database. In future we can enable the PostGIS extension.\\n","completion":"## Decision\nWe will use Postgres for our database.\\n"}
{"prompt":"## Context\\nThe scratch blocks need to be run on the ftduino.\\nTo run a scratch block a small runtime is needed.\\nThe ftduino supports both C and C++.\\nThe Arduino and ftduino library use C++.\\nInterfacing from C to C++ is possible but tricky.\\nC++ also offers constructs C doesn't e.g. type safe enums.\\n","completion":"## Decision\nWe will use C++ to interface with the ftduino.\\n"}
{"prompt":"## Context\\nThe components provided by this package SHOULD primarily be designed\\nwith the following aims:\\n### Stateless\\n@thi.ng\/hdom provides no guidance or opinion about how component state\\nshould be handled. However, stateless components are generally more\\nreusable (same component function can be used multiple times) and easier\\nto test and reason about.\\n### Composable\\nThe components provided by this package are often meant to be used as\\nbuilding blocks for larger, more complex components. This often requires\\nextensive configuration points.\\n### Configurable\\nComponents should be designed such that both their behavior and styling\\ncan be configured as much as possible\/feasible. At the same time, this\\nflexibility SHOULD NOT have too great an impact on user code.\\nFurthermore, the configuration process SHOULD be as uniform as possible\\nfor all provided components.\\n### Unstyled, but skinnable & themable\\nThe last point deals with the multi-step process and separation of:\\n1) configuring a raw component using a specific set of behavioral &\\naesthetic rules (usually via some form of CSS framework) and\\n2) application or instance specific theming as an additional\\ncustomization step\\nNeither of these steps SHOULD be in direct scope of the\\n@thi.ng\/hdom-components package, but the raw components themselves MUST\\nsupport these use cases for practical, real world usage.\\nIt also worth pointing out that skinning and theming MIGHT not always be\\nseparate steps and will be specific to the CSS framework used at\\nruntime.\\n","completion":"## Decision\nDefine all suitable components in a way which enables this uniform\\nworkflow:\\n### Raw component with configuration options\\nWhere required, components SHOULD be pre-configured via an higher order\\nfunction accepting a configuration object with component-specific\\noptions.\\nWhenever possible, the component SHOULD only require partial options and\\nmerge them with its own defaults. Each option MUST be documented using\\nJSDoc comments. Likewise the HOF component function MUST be documented,\\nspecifically to explain which runtime arguments are expected\/accepted by\\nthe returned function.\\n#### Runtime argument handling\\nIf there are more than 2 runtime args and \/ or the majority of them is\\noptional, the returned component function SHOULD accept those args as\\noptions object.\\nIf the component can take child elements as arguments, these SHOULD be\\naccepted as varargs and NOT as part of the options object.\\n#### Example component\\nThe following example button component demonstates these approaches.\\nBtw. It's the actual implementation of the [hdom-components button\\ncomponent](..\/src\/button.ts).\\n```ts\\n\/\/ button.ts\\nimport { IObjectOf } from \"@thi.ng\/api\";\\nexport interface ButtonOpts {\\n\/**\\n* Element name to use for enabled buttons.\\n* Default: \"a\"\\n*\/\\ntag: string;\\n\/**\\n* Element name to use for disabled buttons.\\n* Default: \"span\"\\n*\/\\ntagDisabled: string;\\n\/**\\n* Attribute object to use for enabled buttons.\\n* Default: none\\n*\/\\nattribs: any;\\n\/**\\n* Attribute object to use for disabled buttons.\\n* Default: none\\n*\/\\nattribsDisabled: any;\\n\/**\\n* Flag to indicate if user supplied `onclick` handler\\n* should be wrapped in a function which automatically\\n* calls `preventDefault()`.\\n* Default: true\\n*\/\\npreventDefault: boolean;\\n}\\nexport interface ButtonArgs {\\nattribs: IObjectOf<any>;\\nonclick: EventListener;\\ndisabled: boolean;\\n}\\n\/**\\n* Higher order function to create a new stateless button component,\\n* pre-configured via user supplied options. The returned component\\n* function accepts the following arguments:\\n*\\n* - hdom context object (unused)\\n* - partial `ButtonArgs` object (extra attribs, onclick, disabled)\\n* - body content (varargs)\\n*\\n* Any `attribs` provided as arg via `ButtonArgs` are merged with the\\n* default options provided to the HOF. The `disabled` arg decides which\\n* button version to create. The button can have any number of body\\n* elements (e.g. icon and label), given as varargs.\\n*\/\\nexport const button = (opts?: Partial<ButtonOpts>) => {\\n\/\/ init with defaults\\nopts = {\\ntag: \"a\",\\ntagDisabled: \"span\",\\npreventDefault: true,\\nattribs: {},\\n...opts\\n};\\n!opts.attribs.role && (opts.attribs.role = \"button\");\\nreturn (_: any, args: Partial<ButtonArgs>, ...body: any[]) =>\\nargs.disabled ?\\n[opts.tagDisabled, {\\n...opts.attribsDisabled,\\n...args.attribs,\\ndisabled: true,\\n}, ...body] :\\n[opts.tag, {\\n...opts.attribs,\\n...args.attribs,\\nonclick: opts.preventDefault ?\\n(e) => (e.preventDefault(), args.onclick(e)) :\\nargs.onclick\\n}, ...body];\\n};\\n```\\n### Create pre-configured components\\nTo use the raw component, instantiate it via supplied options. Since the\\ncomponent is stateless, the same instance can be used multiple times\\nfrom user code. Furthermore, this approach enables the publication of\\ndedicated packages, providing pre-defined, themed components, ready to\\nuse without further pre-configuration.\\nIn this example, we use [Tachyons](https:\/\/tachyons.io) CSS classes to\\nprovide themed versions of the above raw button component. However, a\\nmore \"traditional\" approach could inject CSS rules via the `style`\\nattribute. Also see\\n[@thi.ng\/hiccup-css](https:\/\/github.com\/thi-ng\/umbrella\/tree\/develop\/packages\/hiccup-css)\\nfor this purpose.\\n```ts\\n\/\/ themed-button.ts\\nimport { button as rawButton } from \".\/button\";\\n\/\/ predefine skinned buttons\\n\/\/ here using Tachyons CSS classes as example\\nexport const primaryButton = rawButton({\\nattribs: {\\nclass: \"dib ph3 pv2 mb2 mr2 br-pill link bg-blue hover-bg-black bg-animate white\",\\nhref: \"#\"\\n},\\nattribsDisabled: {\\nclass: \"dib ph3 pv2 mb2 mr2 br-pill bg-gray white\"\\n}\\n});\\nexport const button = rawButton({\\nattribs: {\\nclass: \"dib ph3 pv2 mb2 mr2 link dim br2 ba blue\",\\nhref: \"#\",\\n},\\nattribsDisabled: {\\nclass: \"dib ph3 pv2 mb2 mr2 br2 ba gray\"\\n}\\n});\\n```\\n### Usage & composition\\nUser code just needs to import pre-configured components and can further\\ncustomize them, e.g. to create an icon button (here using [Font\\nAwesome](https:\/\/fontawesome.com)).\\n```ts\\n\/\/ user.ts\\nimport { start } from \"@thi.ng\/hdom\";\\nimport { button, primaryButton } from \".\/themed-button\";\\n\/\/ derive icon buttons only accepting custom event handler arg\\nconst confirmButton = (_, onclick) =>\\n[primaryButton, { onclick }, [\"i.fas.fa-check.mr2\"], \"Confirm\"];\\nconst cancelButton = (_, onclick) =>\\n[button, { onclick }, [\"i.fas.fa-times.mr2\"], \"Cancel\"];\\nstart(\"app\",\\n[\"div\",\\n[primaryButton, { onclick: () => alert(\"bt1\") }, \"bt1\"],\\n[primaryButton, { onclick: () => alert(\"bt3\"), disabled: true }, \"bt2\"],\\n[button, { onclick: () => alert(\"bt3\") }, \"bt3\"],\\n[button, { onclick: () => alert(\"bt4\"), disabled: true }, \"bt4\"],\\n\/\/ icon buttons\\n[confirmButton, () => alert(\"confirm\")],\\n[cancelButton, () => alert(\"cancel\")],\\n]\\n);\\n```\\n"}
{"prompt":"## Context and Problem Statement\\nHow do we merge contributor identities across code review systems ?\\n## Decision Drivers\\n* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\n","completion":"## Decision\n* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\nChosen option: \"[option 2]\", Simpler implementation, better query performance (no complexification of queries) and no potential metrics errorneous results returned compared\\nto \"[option 1]\".\\nHere is a configuration example.\\n```\\nidents:\\n- ident: John Doe\\naliases:\\n- github.com\/john-doe\\n- review.opendev.org\/John Doe\/12345\\n```\\n### Negative Consequences\\n* A tool need to crawl the whole index to discover objects to update. However this\\noperation should not happen often.\\n"}
{"prompt":"## Context\\nWe need a consistent and battle tested process for releasing the app on both Android and iOS.\\n","completion":"## Decision\n### iOS\\n#### Internal Beta\\nOur internal Beta is managed through testflight on the Guardian developer account. The group which this beta is sent to is labelled `GNM`. This includes the team and internal stakeholders within the organisation. We build this automatically through Fastlane and Github Actions once a day. Occasionally we will set off builds to test things on a number of devices.\\nIn github actions we have a [scheduled build](https:\/\/github.com\/guardian\/editions\/actions?query=workflow%3Ascheduled-ios-beta) and an [ad-hoc one](https:\/\/github.com\/guardian\/editions\/actions?query=workflow%3A%22Upload+ios-beta%22) triggered by a [script](https:\/\/github.com\/guardian\/editions\/blob\/main\/script\/upload-ios-build.sh)\\nAll builds generate a ['release' in github](https:\/\/github.com\/guardian\/editions\/releases) to help us keep track of build numbers against certain commits. This is handled by the [make-release script](https:\/\/github.com\/guardian\/editions\/blob\/main\/script\/make-release.js).\\n#### External Beta\\nBefore every release, we aim to do at least one external beta to gather feedback. We have a number groups within testflight that are prefixed with the name `External Testers...`. These different groups represent the different authentication methods we support. When we decide a build is good enough from an internal test, we add the build to the groups.\\n#### Release\\nAfter a successful beta period, we release the same build (identified by its build number) through the app store submission process.\\n#### Post Release\\nWe update the version number in XCode and raise that as a PR. The version number will depend on the goals for the next release. We follow a major and minor number approach with no patch i.e. 5.6\\n### Android\\n#### Internal Beta\\nIn a similar vein as above, the Android internal beta is managed through Google Play. The APK for this is created using Fastlane through TeamCity. The name of this process is `android-beta-deploy`. The list for this is managed within the Google Play console. This process runs once per day. Users will need to update their app through their Google Play store.\\n#### External Beta\\nWe take a slightly different approach to iOS. Due to not being able to determine within the app whether or not the app is in Beta or release, we have a different build configuration for releasing to production - which hides the 'report bug' button.\\nWe build the APK using `android-beta-deploy`. This will release a build to google play to the 'internal beta' group for internal testing by the team. It is then manually promoted within the Google Play console to our external beta testers.\\nAs with iOS, releases to the play store can be tracked in [github releases](https:\/\/github.com\/guardian\/editions\/releases) - each time the teamcity build is run a new github release is created including the play store version code of that release.\\n#### Release\\nAfter a successful external beta test, we **DO NOT** promote the external beta. This is because we have code in the app that attempts to determine whether or not the user is in beta. This does not work on Android as there isn't a distinction.\\nAs a result, we then use the TeamCity process `android-release-deploy` to then build the APK. As you will only want to release a version that has been beta tested, you can use the [releases](https:\/\/github.com\/guardian\/editions\/releases) list to find a release for the build you want to release, and copy the tag for that build. You can then search for this tag in the branch list within teamcity (the `android-release-deploy` config treats github tags as if they were branchs) and run a build on that tag.\\nBe warned, this process will automatically release the new version of the app. You will then need to go into the Google Play console to update the release notes.\\n"}
{"prompt":"## Context\\nDuring the set up phase of the beta we investigated what the long term hosting should be. This was agreed to be Government's Platform as a Service (GPaaS) as it was the strategic platform all other digital services within BEIS were moving to.\\nWe have been unable to get access to the BEIS GPaaS account to set up the platform.\\nWe value having a real service hosted as soon as possible and would like to have this done in the first few sprints so the team have a live product to iterate.\\n","completion":"## Decision\nUse dxw's Heroku account to host a staging and production environment and migrate the service to GPaaS later.\\n"}
{"prompt":"## Context\\nWhen we define a secondary port, the purpose is to allow different kinds of adapters to hook to that port. However, in the simplest case, the implementation of an application will only allow a single adapter to be attached to a port at the same time. For example:\\n```java\\npublic interface Notifier\\n{\\npublic void notify(Message message);\\n}\\n```\\n```java\\npublic class Logger implements Notifier\\n{\\n@override\\npublic void notify(Message message)\\n{\\nlogger.log(message.getMessage());\\n}\\n}\\n```\\n```java\\npublic class Provider\\n{\\npublic void register()\\n{\\ncontainer.bind(Notifier, Logger);\\n}\\n}\\n```\\nso a single type of adapter can be attached to a specific port at the same time, and that's obvious because the application services that use that port, will each expect to receive a single object as instance of that port. But what if we need to attach multiple adapters to the same port at the same time? In the example above, what if we want multiple notifiers to send notifications when the abstract message is produced, for example sending an email in addition to logging the message?\\n","completion":"## Decision\nThe standard solution in this cases is using a message bus, so that multiple handlers can be registered to handle certain types of messages. What we can do, then, is creating a bus adapter that would work as a proxy that routes the messages to all registered handlers:\\n```java\\npublic class BusNotifier implements Notifier\\n{\\npublic void notify(Message message)\\n{\\nbus.send(message);\\n}\\n}\\n```\\nso we would still be attaching a single secondary adapter, but at the same time supporting multiple ones, like a logger, an email notifier, etc.\\nOf course the same application structure can be reused with no change whatsoever the moment we decide that we only need a single secondary adapter: we could then replace the bus adapter with the single adapter we need.\\n"}
{"prompt":"## Context\\nWe want to have API documentation for the code we publish. Rather than writing\\nseparate API docs that quickly get out of date, we would like to be able to\\ngenerate it from our code and comments in our code that live next to the thing\\nthey refer to. For JavaScript, the standard is [JSDoc](https:\/\/jsdoc.app\/), but\\nwith TypeScript, we're already defining the types in our code, and duplicating\\nthat in the associated comments is repeated effort and requires manual action to\\nkeep up-to-date.\\n[TypeDoc](https:\/\/typedoc.org\/) is a documentation generator based on JSDoc. It\\nuses a combination of comments and TypeScripts own types to generate API\\ndocumentation automatically.\\n","completion":"## Decision\nWe will use TypeDoc to generate documentation.\\nWe will document all exported code for the benefit of end users.\\nWe will commit the documentation we generate to the repository alongside changes\\nto behaviour.\\n"}
{"prompt":"## Problem\\nTo develop a [Web UI](https:\/\/github.com\/ElektraInitiative\/libelektra\/issues\/252),\\nwe need to be able to remotely configure Elektra via a network socket.\\nThe idea is to use a Pub\/Sub concept to synchronize actions which describe\\nchanges in the Elektra state.\\n","completion":"## Decision\nREST Api is used.\\n"}
{"prompt":"Context\\n-------\\nIn 0001 it was decided to use a callback to handle modified files. When used\\nwith real code:\\n```php\\n$this->watcher->watch($this->paths, function (ModifiedFile $file) {\\nasyncCall(function () use ($file) {\\n$job = $this->indexer->getJob($file->path());\\nforeach ($job->generator() as $file) {\\nyield new Delayed(1);\\n}\\n});\\n});\\n```\\nWhich is just odd. The callback is not part of the co-routine. Using a promise\\nimproves this:\\n```php\\nwhile (null !== $file = yield $watcher->wait())\\n$job = $this->indexer->getJob($file->path());\\nforeach ($job->generator() as $file) {\\nyield new Delayed(1);\\n}\\n});\\n```\\nDecision\\n--------\\nRefactor the code to yield promises for modified files.\\nWhilst initially I thought this would be quite difficult, it didn't take long.\\nEach watcher has an async co-routing which builds a queue of modified files\\nwhich are then subsequently yieled when `->wait()` is called on the `Watcher`\\n(if there are no files, then we pause the co-routine for some milliseconds\\nthen try again).\\nConsequences\\n------------\\nIt should be easier to integrate this library into Amp projects. On the\\ndownside it does mean coupling Amp to the public API - but seeing as this\\npackage is called AmpFsWatch, that's acceptable.\\n","completion":"## Decision\n--------\\nRefactor the code to yield promises for modified files.\\nWhilst initially I thought this would be quite difficult, it didn't take long.\\nEach watcher has an async co-routing which builds a queue of modified files\\nwhich are then subsequently yieled when `->wait()` is called on the `Watcher`\\n(if there are no files, then we pause the co-routine for some milliseconds\\nthen try again).\\nConsequences\\n------------\\nIt should be easier to integrate this library into Amp projects. On the\\ndownside it does mean coupling Amp to the public API - but seeing as this\\npackage is called AmpFsWatch, that's acceptable.\\n"}
{"prompt":"## Context\\nWe need a way to notifiy customers for doing a survey after their purchases, provide them with recommendations, letting them know how their orders via email and\/or SMS.\\n","completion":"## Decision\nA Notifier component is needed to send email and\/SMS to customers.\\n"}
{"prompt":"## Context\\n[context]: #context\\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n> MVP:\\n> - Nicht nur bei \u00c4nderungen, sondern auch bei Bewertungen\\nwerden Subscriber informiert\\n> - Am Ende jeder Notification Mail gibt es einen Link, wor\u00fcber\\ndas Abonnement beendet werden kann\\n> - Notification-Mails werden nur alle 30 min. verschickt, jeweils\\nmit der bis dahin letzten \u00e4nderung (Das verhindert eine\\nMailflut, wenn jemand 20 mal hintereinander den selber\\nBeitrag editiert)\\n> - Wird ein Eintrag gel\u00f6scht, wird den Piloten der L\u00f6schgrund\\nmitgeteilt.\\n> NTH:\\n> - Notifications von gel\u00f6schten Beitr\u00e4gen erhalten nur Piloten\\nund leute, die den einzelnen Beitrag abonniert haben.\\nNormale \"Fl\u00e4chenabonnementen\" werden nicht \u00fcber\\nl\u00f6schungen informiert.\\n> MVP: Wird derzeit in Wei\u00dfrussland entwickelt. Siehe\\nhttp:\/\/mapa.falanster.by\/\\n> - \u00dcber Sharing-Button rechts mittig kann der Nutzer folgende\\nOptionen w\u00e4hlen\\n#Kartenansicht abonnieren (Pop-up zum mailadresse\\neingeben.)\\n#URL-Teilen (Pop-up mit aktueller URL der Karte)\\n#Karte einbetten (Pop-up mit Iframe-Code)\\n#Download (Popup mit Feld f\u00fcr Mailadresse, an die der\\nDownloadlink geschickt werden soll. Eingeloggte Nutzer sehen\\nDownload-Link direkt. Evtl. Downloadbutton nur f\u00fcr eingeloggte\\nNutzer zeigen.\\n> - Beim Abonnieren wird nicht nur der Bereich ber\u00fccksichtigt,\\nsondern auch die gew\u00e4hlten Hashtags. Hat der Nutzer dagegen\\nnur einen einzelnen Eintrag ge\u00f6ffnet, wird nur dieser abonniert.\\n(also ganz logisch wird immer das abonniert bzw. geteilt, was\\ngerade in der URL steht.)\\n> - \u00dcber einen Kurztext unter der Mail-Eingabe-Zeile wird dann\\n> NTH:\\n> - Beim Abonnieren werden nicht nur Hashtags ber\u00fccksichtigt\\nsondern auch suchbegriffe\\n> - Umfassende Suchfunktion, die mehrere Suchbegriffe\\ngleichzeitig filtern kann als und-Suche bzw. mit Komma als\\nOder-Suche\\n","completion":"## Decision\n[decision]: #decision\\n- The service will be implemented as an isolated component within OpenFairDB\\n- A modification time stamp needs to be added to entries in the search index\\n- The search index allows filtering by modification time stamp\\n- Subscriptions consist of\\n- Name\\n- Selector of either\\n- either entry id\\n- or search query\\n- Bounding box (mandatory)\\n- List of tags (might be empty)\\n- Search text (optional)\\n- Max. modification time stamp of the most recent entry that has been processed (initial: creation time stamp of the subscription)\\n- Subscriptions are owned by users, one user per subscription\\n- Users may create multiple subscriptions\\n- All subscriptions are processed independently\\n- Subscriptions will be processed periodically starting at a configurable time, i.e. daily at 7 a.m. (to avoid disctractions by nightly emails)\\n- For each subscription a search request will be submitted (if selector is not an entry id)\\n- The number of search results is limited to 500, applying the default scoring\\n- The search results are collected into an e-mail, one per subscription\\n- The email contains:\\n- name of the subscription in the subject\\n- for each modified entry:\\n- modification time stamp\\n- title of entry\\n- link to entry\\n- The e-mail contents are submitted to external e-mail service provider that sends them.\\nSending e-mails via *sendmail* locally is not recommended.\\n"}
{"prompt":"## Context\\nDefine a supported installation method as per [ADR-0012](https:\/\/github.com\/home-assistant\/architecture\/blob\/master\/adr\/0012-define-supported-installation-method.md).\\n","completion":"## Decision\nThis is for running just the Home Assistant Core application on native OCI compatible containerization system. It does not provide the Supervisor experience, and thus does not provide the Supervisor panel and add-ons.\\nThis is a general installation method that is recommended as an alternative to the Home Assistant OS installation method. Due to the shared image with the Home Assistant OS installation method, almost all documentation applies to the Home Assistant Container as well.\\nThe only supported way to run the container is on the host network as root with full privileges.\\n### Supported Containerization system and version\\n- Any Open Container Initiative (OCI) compatible containerization system.\\n### Supported boards\/hardware\/machines\\n- Machines of the following architectures: amd64, i386, armhf, aarch64, armv7\\n### Supported Operating Systems and versions\\n- Running Home Assistant Container is only supported on Linux.\\n- Windows and BSD installations (e.g., macOS and FreeBSD) are not supported.\\n### Additional notes\\nThere is a wide variety of containerization software available. From that perspective, Home Assistant will only actively document the use of Docker.\\n### Required Expertise\\n- **Installation**\\nThis requires the user to have an existing system that can run Docker containers. Installation is either done by running a command from the Docker-cli or via a user interface (Synology, Portainer)\\n* **Start when the system is started:** The user is responsible for configuring the system to start the container when the system is started.\\n* **Run with full network access:** Default installation instructions prescribe net=host to be configured.\\n* **Access USB devices:** It is up to the user to ensure that all devices are correctly passed through to the container.\\n* **Maintaining the Home Assistant installation**\\nIf using the Docker-cli the user needs to manually update the run command. If using a UI the user might be notified of an upgrade or automatically update \u2013 automatically applying updates may result in the system not coming back online. There is no rollback in case the instance does not come online after an update.\\n- **Python upgrades:** Included in the Home Assistant container\\n- **Installing Python dependencies:** Included in the Home Assistant container\\n- **Updating Home Assistant:** Included in the Home Assistant container\\n- **Maintaining the Operating System**\\nSince this is just the core container, all OS responsibilities are with the user.\\n- **Security updates for OS:** Responsibility of the user.\\n- **Maintaining the components required for the Supervisor:** No supervisor, so N\/A\\n**Conclusion:** medium expertise required. Some Docker UIs make it easy to run and update containers. Mapping devices and manually updating Home Assistant will be challenging as they depend per platform.\\n"}
{"prompt":"## Context\\nNote: This has been superceeded. See [Status](#status) below.\\nIt is expected for the government websites to be secure and keep the user\\ninteractions private. Because that we want to enforce all communications to\\nany application and to the platform endpoints to use only and always HTTPS,\\nas [it is described in the Gov Service Manual](https:\/\/www.gov.uk\/service-manual\/technology\/using-https).\\nWhen a user inputs a website name without specifying the\\nprotocol in the URL, most browsers will try first the HTTP protocol by default.\\nEven if the server always redirect HTTP to HTTPS, an initial\\nunprotected request including user information will be transferred\\nin clear: full URL with domain, parameter, [cookies without secure flag](https:\/\/en.wikipedia.org\/wiki\/HTTP_cookie#Secure_and_HttpOnly)\\nor browser meta-information.\\n[HTTP Strict Transport Security](https:\/\/en.wikipedia.org\/wiki\/HTTP_Strict_Transport_Security)\\nmitigates this issue by instructing modern browsers that support it to\\nalways connect using HTTPS.\\nThis is also a [requirement in the service manual](https:\/\/www.gov.uk\/service-manual\/technology\/using-https).\\nThere is still a potential initial unprotected HTTP request that might happen\\nbefore retrieve the HSTS headers or after the specified HSTS `max-age`.\\nTo solve this issue, the root domain can be added to\\n[HSTS preload list](https:\/\/hstspreload.appspot.com\/) which will be used by most\\ncommon browsers.\\nCurrently the only way to avoid any clear text HTTP interaction is closing or\\ndropping any attempt to connect to the port 80 at TCP level.\\nAlthough not all application deployed on the PaaS will be \"services\"\\nas in the service manual meaning, we must not allow HTTP to make\\nit easier to service owners to comply with this requirements.\\n","completion":"## Decision\nWe will only open port 443 (HTTPS) and drop\/reject any TCP connection to TCP port 80 (HTTP).\\nWe will implement and maintain HSTS preload lists for our production domains.\\n"}
{"prompt":"## Context\\nWe would like to simplify code reviews and unify code style.\\n","completion":"## Decision\nUse Black code formatter: https:\/\/github.com\/ambv\/black\\n"}
{"prompt":"## Context\\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5467) and [4982](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4982) for additional context.\\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\\n","completion":"## Decision\nBalances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\\n### Account interface (x\/auth)\\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\\nnow be stored in & managed by the bank module.\\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\\naccount balance.\\nVesting accounts will continue to store original vesting, delegated free, and delegated\\nvesting coins (which is safe since these cannot contain arbitrary denominations).\\n### Bank keeper (x\/bank)\\nThe following APIs will be added to the `x\/bank` keeper:\\n- `GetAllBalances(ctx Context, addr AccAddress) Coins`\\n- `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\\n- `SetBalance(ctx Context, addr AccAddress, coin Coin)`\\n- `LockedCoins(ctx Context, addr AccAddress) Coins`\\n- `SpendableCoins(ctx Context, addr AccAddress) Coins`\\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\\ncore functionality or persistence.\\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\\nbut retrieval of all balances for a single account is presumed to be more frequent):\\n```golang\\nvar BalancesPrefix = []byte(\"balances\")\\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\\nif !balance.IsValid() {\\nreturn err\\n}\\nstore := ctx.KVStore(k.storeKey)\\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\\nbz := Marshal(balance)\\naccountStore.Set([]byte(balance.Denom), bz)\\nreturn nil\\n}\\n```\\nThis will result in the balances being indexed by the byte representation of\\n`balances\/{address}\/{denom}`.\\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\\naccount balance by denomination found in the (un)delegation amount. As a result,\\nany mutations to the account balance by will made by denomination.\\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\\ndirectly instead of calling `GetCoins()` \/ `SetCoins()` (which no longer exist).\\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\\naccount balances.\\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\\npossible as to not load the entire account balance.\\n### Supply module\\nThe supply module, in order to implement the total supply invariant, will now need\\nto scan all accounts & call `GetAllBalances` using the `x\/bank` Keeper, then sum\\nthe balances and check that they match the expected total supply.\\n"}
{"prompt":"## Context\\nIn various user research sessions we've observed users start the MSA or the verify service provider\\nand then want to check whether it's working correctly. There's also a need for users to be able to\\nmonitor the health of the system once it's deployed to their environment.\\nDropwizard allows you to configure an HTTP endpoint as a healthcheck. This can perform some arbitrary\\nactions that check the health of the system.\\n","completion":"## Decision\nWe will have a healthcheck endpoint that will check the verify-service-provider can read metadata from\\nthe hub and the MSA.\\n"}
{"prompt":"## Context\\nWe are creating a design guideline. All team members should follow defined rules.\\n","completion":"## Decision\n* Do not assign\\n- corner radius\\n- shadow\\n- border width\/color\\n- text color\\n- background color\\n- text size\\n- tint color\\non storyboard.\\n* Do not use any color that isn't in our color palette. If that is the case contact with designer team.\\n* Add icons as SVGs. Choose single scale while adding.\\n"}
{"prompt":"## Context\\nConsumers need to know how to use the Terraform Module. This module has a number\\nof configurable options.\\n","completion":"## Decision\nThe module will have several examples of how to use the module. Initially 2\\nexamples named `base` and `complete` will be created. These should show how to\\nprovide a minimal and a complete configuration.\\nThis module may be used inside another module or in isolation, so these two\\naspects should also be part of the example implementation.\\n"}
{"prompt":"## Context\\nThere are a lot of tools available for documenting APIs. These tools generate easy to read documentation that can be used by frontend engineers and even third party developers. Setting up a documentation workflow will allow those engineers to integrate with the TTAHUB API faster and with more confidence. In addition, these API tools open a path for further testing and automation in the future, since the definition of the API is machine readable.\\n","completion":"## Decision\nWe will document our API with [OpenAPI specification 3](https:\/\/swagger.io\/specification\/) (formerly known as swagger).\\n"}
{"prompt":"## Context\\nMany systems, teams, and designs require that the system will not utilize dynamic memory allocation. We should maintain a flexible design by allowing users to use dynamic memory allocation if they desire. However, we should be able to support the strictest operating model for maximum flexibility and potential use of the framework.\\n","completion":"## Decision\nNo dynamic memory allocation will be utilized by framework core components\\n"}
{"prompt":"## Context\\nWe need to choose a web framework for the app.\\n","completion":"## Decision\nWe use Spring Boot because it allows us starting fast, and concentrating on business logic of the app, rather than working on infrastructure tasks.\\n"}
{"prompt":"## Context\\nA complete Rust rewrite is planned (see ADR #0002)\\nGo code has code debt -> substantial effort to refactor\\nGo implementation would not be reusable in Holo front-end (whereas portions of rust compilable into WASM could be)\\nGo code has dependencies that make it hard to compile for mobile\\n","completion":"## Decision\n- Alpha1 go release is last major go release of holochain because energy will be focused on the new Rust version.  One team, one code base for now, may revisit this later\\n- We will call the rust release Alpha2, and will have at least the functionality of Alpha 1 plus World-model & new network-transport\\n"}
{"prompt":"## Context\\nData integration involves combining data residing in different sources and providing users with a unified view of them for both operational and analytical uses. Data integration encourages collaboration between internal as well as external users.\\nGiven an application landscape with a large variaty of application types (legacy, on-prem, SaaS, ...), business owners and parties involved, data integration can be challenging. Therefore it is important to make sure the need for data integration is recognized by all parties involved and technical knowledge is available for systems involved.\\n","completion":"## Decision\nWe integrate data through the ODH. Therefore we only make or buy solutions supporting this.\\n"}
{"prompt":"## Context\\nWhile signals\/front-end is a React web app, the client wants to provide it to mobile app users as well.\\nFurthermore, some changes are requested specifically for the 'app version':\\n- The application header should be hidden\\n- The button 'Doe een melding' should be replaced with a 'Sluit venster' button which closes the window\\n- The possibility to initialize the map center location\\nThe above requirements are described in more detail in [Jira](https:\/\/datapunt.atlassian.net\/browse\/SIG-3933).\\nAt this moment, rebuilding the frontend in a native app architecture is not possible due to time and financial constraints.\\n","completion":"## Decision\nAdd a feature flag `appMode` to the application configuration, and deploy the application with `appMode=true` to a separate domain.\\nUsing the feature flag, the application conditionally applies the app-specific requirements.\\nThe mobile app embeds the web app (e.g. using [React Native WebView](https:\/\/github.com\/react-native-webview\/react-native-webview)),\\nand provides additional information via query parameters (e.g. map coordinates).\\nConversely, communication from the web-app to the mobile app occurs via the `postMessage()` API, but its use should be limited.\\nBecause an embedded application cannot close itself, the `postMessage()` can be used to notify the app that it should be closed.\\n"}
{"prompt":"## Context\\nWe want the data to be in one place, be it internal (in the static assets) or external (served by an API or an S3 bucket...)\\nWe want the project to be data-driven, so that the document is re-rendered if any data changes.\\nWe want the data being spread across the Components using the latest technologies.\\n","completion":"## Decision\nThe source of data is kept internally. It will stay in the webapge, as a static asset from now on.\\nBut the project must be kept easily switchable to an external data source.\\nWe spread the data across Components using React Context API.\\n"}
{"prompt":"## **Context**\\nThe tenure data that currently exists in Universal Housing (UH) in the `tenagree` database table can be divided into two distinct categories:\\n- Master Tenure Record\\n- These records represent true tenures - i.e. the association between a person and a property\\n- Everything else\\n- These records are store in tenagree as this was a convenient location, however they are not a tenure as they do not have an association with a property - they are an \"account\" associated with a Master Tenure Record\\nFor the purpose of viewing a tenure in Manage My Home, the only category that is relevant is a Master Tenure Record.\\nWhen importing the data as part of the the development of Manage My Home, we have considered two options:\\n1. Import the data as it is currently stored in UH, and mark anything that isn't a Master Tenure Record as a \"child\" of a Master Tenure Record\\n2. Import only Master Tenure Records\\n- everything else stored in `tenagree` should be imported according to a separate data model\\n","completion":"## Decision\n**Import only Master Tenure Records**\\nThis option has been chosen for a number of reasons:\\n1. we are importing legacy data and have the ability to correct any inconsistency at this point\\n2. If we import the data as-is, we will be persisting the incorrectly modelled data\\n3. if we import the data as-is, we will need to either:\\n- display it when viewing the list of tenures associated with a person\/property\\n- hide it when viewing the list of tenures associated with a person\/property, therefore increasing complexity of the delivered solution\\n"}
{"prompt":"## Context (Discussion)\\nWhen writing custom scripts in the package.json files, one must take into account that those commands\\nshould be compatible with both sh and CMD.exe. So, it is common to use third-party npm packages that\\nact as an API to mimic these tasks in both environments.\\nOne of the most common tasks we do in these scripts is file manipulation (deleting, copying, moving).\\n### Pros and Cons of the Options:\\n#### Difference Between ShellJS and shx\\n**ShellJS:**\\nGood for writing long scripts, all in JS, running via NodeJS (e.g. node myScript.js).\\n**shx:**\\nGood for writing one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").\\n","completion":"## Decision\nIt is proposed to use [shx][shx-url], a wrapper for sh commands that works on Windows too, and behaves as the corresponding sh commands.\\nIt has more than 150K downloads per week, is well-maintaned by the community and it occupies less space than our current past alternatives: rimfaf and copyfiles.\\n"}
{"prompt":"## Context and Problem Statement\\nWorking with microservices and a issues \/ Pull Requests based workflow, labels\\nfor these issues and PRs are a must have and should be explicit about what the\\nissue\/PR is about.\\nHowever, there are numerous labels that represent different kind of\\ninformation (scope, lifecycle step, concerned component, etc). Colors help to\\nidentify these different kind of labels. In the end, managing all of this\\nmanually is painful, and implies duplicate work across multiple services\\nrepositories for common labels (closing reasons, PR lifecycle, etc.)\\nWe want to find a way to handle this cleanly.\\n## Decision Drivers <!-- optional -->\\n- How easy it will be to integrate new repositories\\n- How much work we have to do to for minor changes\\n","completion":"## Decision\n- How easy it will be to integrate new repositories\\n- How much work we have to do to for minor changes\\nChosen option: **\"Automate GitHub repositories management with\\ninfrastructure-as-code, namely Terraform\"**, because:\\n- We don't want to do things manually\\n- Having everything as-code is cool\\n- It will be easy to apply to many repositories\\n- We already use Terraform for a bunch of stuff so it's known by people\\nHowever, it will not be run automatically by our CI\/CD pipelines at the\\nbeginning, since Terraform needs an API access to GitHub and this would imply\\ncreating a \"machine user\" GitHub account, which will be more thing to manage\\nthan we want for now.\\n"}
{"prompt":"## Context\\nAs part of the template there is a need to provide a base configuration for Rspec. Ideally this would be performed using the `rails generate rspec:install` command. However, on testing the Thor command `generate(\"rspec:install)\"` it froze when it got to install rspec. This is likely due to [spring running as described in this error](https:\/\/stackoverflow.com\/questions\/33189016\/how-to-solve-rails-generate-commands-hanging).\\nFor now, `hmu-rails` just uses template files to configure rspec. An option to explore in the future would be to test whether calling `spring stop` as part of a `generate_rspec` method would resolve this issue.\\n","completion":"## Decision\nFor now `hmu-rails` will use template files for configuring rspec for speed but revisit the need to run the install command in the future.\\n"}
{"prompt":"## Context\\nServices that we are in the process of migrating or that have recently competed migration are likely to go through a period of needing more maintenance that a mature service.\\nOne answer to this is to handle all such problems on a case by case basis, fixing as we go in order to try to keep the velocity as high as possible. At the other end of the scale is to drop a significant cost on the project by automating all such processes.\\n","completion":"## Decision\nWe will automate every process possible. Where an automated process may be applicable to more than one server, then we will attempt to write the solution to be DRY.\\n"}
{"prompt":"## Context\\nIt will be necessary to access the servers that form the Datalabs network but we do not\\nwant to expose any services outside of the JASMIN tenancy that we do not have to.\\n","completion":"## Decision\nWe have decided that all access to the cluster will be via a Bastion server over an SSH\\nconnection on port 22. We will restrict access through the firewall to known IP address\\nranges including the development workstations, the STFC VPN and the Tessella public IP\\naddress.\\nThis excludes public facing services that should be available over HTTPS on port 443 via\\na different route.\\n"}
{"prompt":"## Context\\nWe want to handle nested order line items.\\nCurrently, the line items are available nested, but all cart processors only consider the first level of line items.\\nOn one hand, we could implement all cart processors, that they process all levels of line items, but on the other hand,\\nall nested line items are added via plugins, which would implement their own processing logic.\\n","completion":"## Decision\nThe core cart processors will continue to work with `getFlat()` in enrich. This way the required data for all items in the\\ncart will be fenced and each item could also be processed by its processor.\\nThe `process` method on other hand will still not work with `getFlat()`, but will only take care of line items that are on the first level.\\nThis way there will be no collisions in the processing of these line items. A plugin that reuses core line items can\\neasily call the other processors to handle the nested line items themselves.\\nExample:\\n```\\n<?php declare(strict_types=1);\\nnamespace Shopware\\Core\\Checkout\\Cart;\\nuse Shopware\\Core\\Checkout\\Cart\\Error\\IncompleteLineItemError;\\nuse Shopware\\Core\\Checkout\\Cart\\LineItem\\CartDataCollection;\\nuse Shopware\\Core\\Checkout\\Cart\\LineItem\\LineItem;\\nuse Shopware\\Core\\Content\\Product\\Cart\\ProductCartProcessor;\\nuse Shopware\\Core\\System\\SalesChannel\\SalesChannelContext;\\nclass PluginCartProcessor implements CartProcessorInterface\\n{\\n\/**\\n* @var CreditCartProcessor\\n*\/\\nprivate $creditCartProcessor;\\n\/**\\n* @var ProductCartProcessor\\n*\/\\nprivate $productCartProcessor;\\npublic function __construct(CreditCartProcessor $creditCartProcessor, ProductCartProcessor $productCartProcessor)\\n{\\n$this->creditCartProcessor = $creditCartProcessor;\\n$this->productCartProcessor = $productCartProcessor;\\n}\\npublic function process(\\nCartDataCollection $data,\\nCart $original,\\nCart $toCalculate,\\nSalesChannelContext $context,\\nCartBehavior $behavior\\n): void {\\n$lineItems = $original->getLineItems()->filterType('plugin-line-item-type');\\n\/*\\n* Structure of the plugin line item:\\n* - plugin line item\\n*      - product line item(s)\\n*      - credit line item(s)\\n*\/\\nforeach ($lineItems as $lineItem) {\\n$this->calculate($lineItem, $original, $context, $behavior, $data);\\n$toCalculate->add($lineItem);\\n}\\n}\\nprivate function calculate(LineItem $lineItem, Cart $original, SalesChannelContext $context, CartBehavior $behavior, CartDataCollection $data): void\\n{\\nif (!$lineItem->hasChildren()) {\\n$original->remove($lineItem->getId());\\n$original->addErrors(new IncompleteLineItemError($lineItem->getId(), 'children'));\\nreturn;\\n}\\n$tempOriginalCart = new Cart('temp-original', $original->getToken());\\n$tempCalculateCart = new Cart('temp-calculate', $original->getToken());\\n\/\/ only provide the nested products and credit items\\n$tempOriginalCart->setLineItems(\\n$lineItem->getChildren()\\n);\\n\/\/ first start product calculation - all required data for the product processor is already loaded and stored in the CartDataCollection\\n$this->productCartProcessor->process($data, $tempOriginalCart, $tempCalculateCart, $context, $behavior);\\n\/\/ now calculate the credit, the credit is scoped to the already calculated products - all required data for the credit processor is already loaded and stored in the CartDataCollection\\n$this->creditCartProcessor->process($data, $tempOriginalCart, $tempCalculateCart, $context, $behavior);\\n\/\/ after all line items calculated - use them as new children\\n$lineItem->setChildren(\\n$tempCalculateCart->getLineItems()\\n);\\n}\\n}\\n```\\n"}
{"prompt":"## Context\\n_The issue motivating this decision, and any context that influences or constrains the decision._\\nThe Loggregator team decided to move to a [\"Shared\\nNothing\" architecture](https:\/\/docs.google.com\/document\/d\/1_Ve4wAkeCC0fIJ1TiAWSfxzNp5zB_Ndoq5UmfUNJtgs\/edit?usp=sharing)\\nthat would improve scalability of log egress from the platform. This architecture deprecated the V1 Firehose Traffic Controller,\\nthe component from which the cf CLI retrieved application logs. As a result, the Loggregator team proposed modifying the cf CLI to retrieve\\nlogs from Log Cache instead.\\n","completion":"## Decision\n_The change that we're proposing or have agreed to implement._\\nAfter December 2019, the CLI's minimum supported version of cf-deployment contains Log Cache. Therefore, in 2020:\\n* cf CLI v7 will be modified to retrieve application logs from Log Cache\\n* cf CLI v6 will be modified to retrieve application logs from Log Cache _with the exception of experimental v3-prefixed commands_\\n* cf CLI v6\/v7 legacy command implementations accessible through the Plugin API\\nwill be modified to retrieve application logs from Log Cache\\nThere will be no user-facing or breaking changes.\\n"}
{"prompt":"## Context\\nWe wish to build a RESTful API, and to be technically RESTful, we must implement HATEOAS. This gets us to stage 3 of the [Richardson Maturity Model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n","completion":"## Decision\nOur API is to be as RESTful as it can be.\\nWe recognise that we must implement Hypermedia as the Engine of Application State.\\nThe content structure already discussed in [0005-content-structure.md](0005-content-structure.md) implements links within our resource object... this ADR is to explicitly state our aim is to implement HATEOAS.\\nFurthermore, for every endpoint on the API we will implement an OPTIONS verb, returning every action possible at that endpoint.\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n"}
{"prompt":"## Context\\nBefore GOV.UK Frontend, our projects used jQuery for DOM interactions, events and data manipulation.\\nWe\u2019re taking a step back from jQuery due to its lack of support for the browsers we support, its large file size, lack of security updates and from conversations with the community.\\n","completion":"## Decision\nWe\u2019re now writing standard ES5 JavaScript instead, that we polyfill where necessary.\\nThis means that in places where we would have previously used [`$.each`](http:\/\/api.jquery.com\/jquery.each\/) we\u2019re using [`.forEach`](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Array\/forEach) instead, and polyfilling the missing gaps.\\nWe use polyfills provided by the Financial Times\u2019 [Polyfill service](https:\/\/polyfill.io).\\nThis approach ensures that multiple polyfills can be sourced from this service with greater confidence that they\u2019ll work without conflicting with each other.\\nThe Polyfill service does not do runtime detection in browsers and instead opts to do this on the server via user-agent sniffing. It only ships the code needed for that browser, which means newer browsers don\u2019t have to run anything. We may investigate lazy-loading in the future, but for now we\u2019re using a bundled approach based on the lowest common denominator.\\nWe are vendoring these polyfills to avoid any [single point of failure](https:\/\/en.wikipedia.org\/wiki\/Single_point_of_failure) issues that could arise from relying on a CDN. By doing this, we can detect if polyfills are needed at runtime, which results in all browsers getting the same polyfill bundle.\\nWe hope that our approach can be automated or moved into a reusable npm package, based on the Financial Times [npm package](https:\/\/github.com\/Financial-Times\/polyfill-service#library).\\nHere is an [example of polyfilling `addEventListener`](https:\/\/github.com\/alphagov\/govuk-frontend\/blob\/master\/docs\/polyfilling.md).\\nAny polyfills included in GOV.UK Frontend will be tested to work in supported browsers and devices, including assistive technology. Any community discussion and documentation around potential bugs or downsides will also be considered before deciding to include a polyfill in GOV.UK Frontend.\\n"}
{"prompt":"## Context\\nAs discussed in [ADR-0023][adr-0023] and [ADR-0024][adr-0024], Report MI will be moving from generating a\\nregular file extract to using the Workday API for creating management fee\\ninvoices.\\nAs we move to using the API, it becomes less important to batch up the creation\\nof invoices, and we can move towards a more real-time operation.\\nThe generation of invoices is not part of the critical-path of a management\\ninformation submission, so the creation of it shouldn't block or slow down the\\nexperience of suppliers making their return.\\nAs a result, the process for generating the invoice and submitting it to Workday\\nshould happen asynchronously from the rest of the submission.\\nThe best way of achieving this is to add completed tasks to a queue, and have\\nthe API interaction undertaken by a worker.\\n","completion":"## Decision\nThe creation and submission of invoices to Workday will happen asynchronously\\nusing a queue\/worker pattern.\\nWhen a supplier task is completed, Report MI should add it to a queue. A worker\\nshould read from the queue, generate the invoice and submit to the Workday API.\\nIf the Workday API is unavailable, or an error occurs, the submission should be\\nretried. After a (to be defined) number of retries, this should be reported for\\ninvestigation.\\n"}
{"prompt":"## Context and Problem Statement\\nWe have to transfer json data and verify the integrity of the data.\\nThe transfer involves an Authorization server which provides the json, a client which gets the data form that server and pass it to the WebSocket API.\\nThe WebSocket API must able to verify the integrity of the json data.\\n## Decision Drivers <!-- optional -->\\n* Use standard encodings\\n","completion":"## Decision\n* Use standard encodings\\nChosen option: \"Send SHA256 hash of Base64Url encoded json\", because this method is platform independent and not much session state is required.\\n### Positive Consequences <!-- optional -->\\n* The JWT really function as a verification token for the other requests.\\n* Can be applied to all json data that must be verified.\\n### Negative Consequences <!-- optional -->\\n* The json must be transferred in Base64Url encoding\\n"}
{"prompt":"## Context\\nWe have background jobs that are processed by Sidekiq workers on several dedicated background processing\\nmachines.  The background jobs include a variety of different tasks from different sources:\\n* processing user requests (e.g., ingesting files, (re)generating derivatives)\\n* cleaning up after user actions (e.g., propagating state and visibility, removing files before deleting\\ntheir parent resource)\\n* refreshing metadata from Voyager\/PULFA\\n* bulk-ingesting content\\n* routine cleanup (cleaning out guest users)\\n* fixity checking\\n","completion":"## Decision\nWe will have three queues for processing background jobs:\\n1. `default` for processing user-initiated requests needed for ingesting and displaying objects\\n2. `low` for bulk processing, validation, cleanup, etc.\\n3. `super_low` for fixity checking and other long-term preservation actions\\n"}
{"prompt":"## Context\\nWe want to be able to distribute this library to me ingested by TypeScript or\\nplain JavaScript (both commonJS and module) applications.\\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\\nsupport for TypeScript and simple configuration.\\n","completion":"## Decision\nWe will build distributables using Rollup.js.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is to buy hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n","completion":"## Decision\nKafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n"}
{"prompt":"## Context and Problem Statement\\nAs recorded in [INTERLOK-2872](https:\/\/adaptris.atlassian.net\/browse\/INTERLOK-2872) lookup-name as a field has little or no meaning within the UI; in fact, it may not be entirely supported fully within the UI.\\nThe background of the `lookup-name` is so that you can override the name+prefix for the shared component that is stored within the internal JNDI server.\\nIf you have\\n```\\n<ftp-connection>\\n<unique-id>MyFtpConnection<\/unique-id>\\n<\/ftp-connection>\\n```\\nThis would be stored in JNDI under the name of `comp\/env\/MyFtpConnection`; if you need to reference this in configuration then you would just use `lookup-name=MyFtpConnection` in your _SharedConnection_\\nIf you have\\n```\\n<ftp-connection>\\n<unique-id>MyFtpConnection<\/unique-id>\\n<lookup-name>comp\/env\/ftp\/MyFtpConnection<\/lookup-name>\\n<\/ftp-connection>\\n```\\nThis would be stored in JNDI under the name of `comp\/env\/ftp\/MyFtpConnection`; if you need to reference this in configuration then you would just use `lookup-name=comp\/env\/ftp\/MyFtpConnection` in your _SharedConnection_; this is possibly the thing that might never have worked in the UI.\\nThere is a special case for `DatabaseConnection` subclasses; the _javax.sql.DataSource_ is bound against `comp\/env\/jdbc\/unique_id`. Which means that you can address the datasource within _persistence.xml_ as `adapter:comp\/env\/jdbc\/unique_id`. If lookup-name is specified then this is used to bind the _javax.sql.DataSource_ (__which might actually mean you can't ever get to the DatabaseConnection again via a SharedConnection__)\\n","completion":"## Decision\nChosen option: Deprecate and remove in 3.11.0\\n### Do Nothing\\nIf the user already has the extended documentation available; it already mentions.\\nWe would further hide the field using the new annotation that becomes available as part of [INTERLOK-2663](https:\/\/adaptris.atlassian.net\/browse\/INTERLOK-2663).\\n* Good, because we don't have to do much work.\\n* Bad, because we might have to make the UI support this style of configuration.\\n* Bad, because DatabaseConnection will still have odd behaviour if you specify the lookup-name\\n* Bad, because it's ambiguous in the context of configuration; regardless of how much documentation we write no-one ever reads it.\\n### Deprecate\\nWe mark the field as deprecated in 3.9.1; with an intention to remove in 3.11.0.\\n* Good, because it fixes a logical flaw with DatabaseConnection when someone actually specifies lookup-name\\n* Good, because the UI doesn't have to change.\\n* Neutral, because there hasn't been a production instance where someone has used lookup-name!\\n* Bad, because we're going to remove methods from an interface...\\n"}
{"prompt":"## Context\\nCurrently in ibc-go light clients are defined as part of the codebase and are implemented as modules under\\n`modules\/light-clients`. Adding support for new light clients or updating an existing light client in the event\\nof a security issue or consensus update is a multi-step process which is both time-consuming and error-prone.\\nIn order to enable new IBC light client implementations it is necessary to modify the codebase of ibc-go (if the light\\nclient is part of its codebase), re-build chains' binaries, pass a governance proposal and validators upgrade their nodes.\\nAnother problem stemming from the above process is that if a chain wants to upgrade its own consensus, it will\\nneed to convince every chain or hub connected to it to upgrade its light client in order to stay connected. Due\\nto the time consuming process required to upgrade a light client, a chain with lots of connections needs to be\\ndisconnected for quite some time after upgrading its consensus, which can be very expensive in terms of time and effort.\\nWe are proposing simplifying this workflow by integrating a Wasm light client module that makes adding support for\\nnew light clients a simple governance-gated transaction. The light client bytecode, written in Wasm-compilable Rust,\\nruns inside a Wasm VM. The Wasm light client submodule exposes a proxy light client interface that routes incoming\\nmessages to the appropriate handler function, inside the Wasm VM for execution.\\nWith the Wasm light client module, anybody can add new IBC light client in the form of Wasm bytecode (provided they are\\nable to submit the governance proposal transaction and that it passes) as well as instantiate clients using any created\\nclient type. This allows any chain to update its own light client in other chains without going through the steps outlined above.\\n","completion":"## Decision\nWe decided to implement the Wasm light client module as a light client proxy that will interface with the actual light client\\nuploaded as Wasm bytecode. To enable usage of the Wasm light client module, users need to add it to the list of allowed clients\\nby updating the `AllowedClients` parameter in the 02-client submodule of core IBC.\\n```go\\nparams := clientKeeper.GetParams(ctx)\\nparams.AllowedClients = append(params.AllowedClients, exported.Wasm)\\nclientKeeper.SetParams(ctx, params)\\n```\\nAdding a new light client contract is governance-gated. To upload a new light client users need to submit\\na [governance v1 proposal](https:\/\/docs.cosmos.network\/main\/modules\/gov#proposals) that contains the `sdk.Msg` for storing\\nthe Wasm contract's bytecode. The required message is `MsgStoreCode` and the bytecode is provided in the field `wasm_byte_code`:\\n```proto\\n\/\/ MsgStoreCode defines the request type for the StoreCode rpc.\\nmessage MsgStoreCode {\\n\/\/ signer address\\nstring signer = 1;\\n\/\/ wasm byte code of light client contract. It can be raw or gzip compressed\\nbytes wasm_byte_code = 2;\\n}\\n```\\nThe RPC handler processing `MsgStoreCode` will make sure that the signer of the message matches the address of authority allowed to\\nsubmit this message (which is normally the address of the governance module).\\n```go\\n\/\/ StoreCode defines a rpc handler method for MsgStoreCode\\nfunc (k Keeper) StoreCode(goCtx context.Context, msg *types.MsgStoreCode) (*types.MsgStoreCodeResponse, error) {\\nif k.GetAuthority() != msg.Signer {\\nreturn nil, errorsmod.Wrapf(ibcerrors.ErrUnauthorized, \"expected %s, got %s\", k.GetAuthority(), msg.Signer)\\n}\\nctx := sdk.UnwrapSDKContext(goCtx)\\nchecksum, err := k.storeWasmCode(ctx, msg.WasmByteCode, ibcwasm.GetVM().StoreCode)\\nif err != nil {\\nreturn nil, errorsmod.Wrap(err, \"failed to store wasm bytecode\")\\n}\\nemitStoreWasmCodeEvent(ctx, checksum)\\nreturn &types.MsgStoreCodeResponse{\\nChecksum: checksum,\\n}, nil\\n}\\n```\\nThe contract's bytecode is not stored in state (it is actually unnecessary and wasteful to store it, since\\nthe Wasm VM already stores it and can be queried back, if needed). The checksum is simply the hash of the bytecode\\nof the contract and it is stored in state in an entry with key `checksums` that contains the checksums for the bytecodes that have been stored.\\n### How light client proxy works?\\nThe light client proxy behind the scenes will call a CosmWasm smart contract instance with incoming arguments serialized\\nin JSON format with appropriate environment information. Data returned by the smart contract is deserialized and\\nreturned to the caller.\\nConsider the example of the `VerifyClientMessage` function of `ClientState` interface. Incoming arguments are\\npackaged inside a payload object that is then JSON serialized and passed to `queryContract`, which executes `WasmVm.Query`\\nand returns the slice of bytes returned by the smart contract. This data is deserialized and passed as return argument.\\n```go\\ntype QueryMsg struct {\\nStatus               *StatusMsg               `json:\"status,omitempty\"`\\nExportMetadata       *ExportMetadataMsg       `json:\"export_metadata,omitempty\"`\\nTimestampAtHeight    *TimestampAtHeightMsg    `json:\"timestamp_at_height,omitempty\"`\\nVerifyClientMessage  *VerifyClientMessageMsg  `json:\"verify_client_message,omitempty\"`\\nCheckForMisbehaviour *CheckForMisbehaviourMsg `json:\"check_for_misbehaviour,omitempty\"`\\n}\\ntype verifyClientMessageMsg struct {\\nClientMessage *ClientMessage `json:\"client_message\"`\\n}\\n\/\/ VerifyClientMessage must verify a ClientMessage.\\n\/\/ A ClientMessage could be a Header, Misbehaviour, or batch update.\\n\/\/ It must handle each type of ClientMessage appropriately.\\n\/\/ Calls to CheckForMisbehaviour, UpdateSta\u00e5te, and UpdateStateOnMisbehaviour\\n\/\/ will assume that the content of the ClientMessage has been verified\\n\/\/ and can be trusted. An error should be returned\\n\/\/ if the ClientMessage fails to verify.\\nfunc (cs ClientState) VerifyClientMessage(\\nctx sdk.Context,\\n_ codec.BinaryCodec,\\nclientStore storetypes.KVStore,\\nclientMsg exported.ClientMessage\\n) error {\\nclientMessage, ok := clientMsg.(*ClientMessage)\\nif !ok {\\nreturn errorsmod.Wrapf(ibcerrors.ErrInvalidType, \"expected type: %T, got: %T\", &ClientMessage{}, clientMsg)\\n}\\npayload := QueryMsg{\\nVerifyClientMessage: &VerifyClientMessageMsg{ClientMessage: clientMessage.Data},\\n}\\n_, err := wasmQuery[EmptyResult](ctx, clientStore, &cs, payload)\\nreturn err\\n}\\n```\\n### Global Wasm VM variable\\nThe 08-wasm keeper structure keeps a reference to the Wasm VM instantiated in the keeper constructor function. The keeper uses\\nthe Wasm VM to store the bytecode of light client contracts. However, the Wasm VM is also needed in the 08-wasm implementations of\\nsome of the `ClientState` interface functions to initialise a contract, execute calls on the contract and query the contract. Since\\nthe `ClientState` functions do not have access to the 08-wasm keeper, then it has been decided to keep a global pointer variable that\\npoints to the same instance as the one in the 08-wasm keeper. This global pointer variable is then used in the implementations of\\nthe `ClientState` functions.\\n"}
{"prompt":"## Context and Problem Statement\\nSince we use the OpenAPI for frontend client code generations, the HATEOAS links are no longer needed or used.\\n## Decision Drivers\\n* readable API documentation\\n* development effort to test\/gather the HATEOAS links\\n","completion":"## Decision\n* readable API documentation\\n* development effort to test\/gather the HATEOAS links\\nChosen option: remove the HATEOAS links from all entities, because this further simplifies the DTO entities\\n"}
{"prompt":"## Context\\nCreate an alarm for budgets for free\\n","completion":"## Decision\nUse AWS budgets\\n"}
{"prompt":"## Context\\nRead [Distributed Mail Queue](0031-distributed-mail-queue.md) for full context.\\n**enqueuedMailsV3** and **deletedMailsV2** is never cleaned up and the corresponding blobs are always referenced. This is not\\nideal both from a privacy and space storage costs point of view.\\nNote that **enqueuedMailsV3** and **deletedMailsV2** rely on timeWindowCompactionStrategy.\\n","completion":"## Decision\nAdd a new `contentStart` table referencing the point in time from which a given mailQueue holds data, for each mail queue.\\nThe values contained between `contentStart` and `browseStart` can safely be deleted.\\nWe can perform this cleanup upon `browseStartUpdate`: once finished we can browse then delete content of **enqueuedMailsV3**\\nand **deletedMailsV2** contained between `contentStart` and the new `browseStart` then we can safely set `contentStart`\\nto the new `browseStart`.\\nContent before `browseStart` can safely be considered deletable, and is applicatively no longer exposed. We don't need an\\nadditional grace period mechanism for `contentStart`.\\nFailing cleanup will lead to the content being eventually updated upon next `browseStart` update.\\nWe will furthermore delete blobStore content upon dequeue, also when the mail had been deleted or purged via MailQueue\\nmanagement APIs.\\n"}
{"prompt":"## Context\\nOne of the final stack canditate for persistence is [cockroachdb](https:\/\/www.cockroachlabs.com\/), but the windows version\\n","completion":"## Decision\nWe remove all complexity at the beginnign using sqlite. As cockorachdb is SQL compatible, we not expect difficults to migrate\\n"}
{"prompt":"## Context\\nTraditionally we built an \"asset-master-1\" instance with a large disk, and exported\\nthat disk as an NFS share.\\nThe \"backend\", \"whitehall-backend\" and \"asset-slave\" instances all mounted this NFS\\nshare for the tasks they needed to do (uploading files for the backend machines, and\\nmaking backups on the asset-slave machines).\\nIn AWS, we have the option to use [Elastic File System (EFS)](https:\/\/aws.amazon.com\/efs).\\nThis is an autoscaling filesystem so we do not have to manage disk space, and is independent\\nto any instance. Each instance would have to mount this filesystem in the same way as NFS,\\nbut it is not managed by us.\\n","completion":"## Decision\nWe will create an EFS resource and expose the mount using our internal Route 53 DNS. We will allow\\nthe required machines to mount the resource using Security Groups.\\nThe backend and whitehall-backend instances will mount as usual, and the asset-master will also\\nmount the disk like an external share.\\nThe asset-master is required to mount because it moves files about the filesystem after running\\nvirus scans.\\nA decision has yet to be made on the role of the asset-slave, as we could potentially move\\nthese tasks onto the asset-master (pushing backups to an S3 bucket, for example).\\n"}
{"prompt":"## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\n","completion":"## Decision\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\nAuth0 is selected as the solution, due to its managed nature requiring\\nzero deployment and effort. After an evaluation of features compared\\nagainst the project requirements, it has been determined that Auth0's\\nfree tier will suffice and as a result there are no ongoing costs\\neither.\\n### Positive Consequences\\n* As a managed solution, there are no initial deployment or ongoing\\nhosting \/ infrastructure costs.\\n* Patching and maintenance of the solution is provided by Auth0 at no\\nadditional cost \/ interruption to the customer.\\n* It is expected that (at least initially) the free tier will suffice to\\nmeet the requirements of the project.\\n### Negative Consequences\\n* The features and customisations available are limited without\\nupgrading to a paid tier:\\n* Email templates cannot be customised on the free plan.\\n* A custom domain cannot be used to serve the Auth0 tenant.\\n* Enterprise integrations (AD, LDAP, etc) are not available.\\n* Multifactor authentication is not available.\\n* Auth0 hosted UI's can be customised but only to a limited extent.\\n* Limit of 7000 monthly active users.\\n"}
{"prompt":"## Context\\nWe're building a game, which means we need a way to render things, do audio, create windows\\nand other game-like activities.\\nWe could use an engine like `Helm`, but since this project is about learning and exploring\\npossible architectures I don't want to take a pre-existing architecture.\\nWe could use [gloss](https:\/\/hackage.haskell.org\/package\/gloss). However I used it in\\na previous project and I don't like how it takes over the main loop, particularly as it\\nmade it tricky to integrate into a MTL stack.\\nInstead I'm going to use the haskell bindings for [sdl2](https:\/\/hackage.haskell.org\/package\/sdl2). It's fairly low level but it's a cross-platform way to get an OpenGL context, and the high-level bindings seem reasonable enough.\\n","completion":"## Decision\nWe're using [sdl2](https:\/\/hackage.haskell.org\/package\/sdl2)\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context\\nTenancy has always been a first class citizen of all Marain services, however this by itself is not enough to make the system truly multitenanted. In order to do this, we need to determine how tenants should be created, managed and used within the Marain \"world\".\\nWe would like the option of deploying Marain as either a managed service, hosted by us and licenced to users as a PaaS offering, or for clients to deploy private instances into their own cloud subscriptions. We also want to give clients of the managed services the option for data to be stored in their own storage accounts or databases, but still have us run the compute aspects of the platform on their behalf. This also extends to those clients who are using Marain to implement their own multi-tenanted services: these clients should also be able to isolate their own clients' storage.\\nIn addition, we need to be able to differentiate between a Marain service being available for a client to use directly and one being used as a dependency of a service they are using. For example, the Workflow service makes use of the Operations service. As a result, clients that are licenced to use the Workflow service will be using the Operations service indirectly, despite the fact that they may not be licenced to use it directly.\\nWe need to define a tenancy model that will support these scenarios and can be implemented using the `Marain.Tenancy` service.\\n","completion":"## Decision\nTo support this, we have made the following decisions\\n1. Every client using a Marain instance will have a Marain tenant created for them. For the remainder of this document, these will be referred to as \"Client Tenants\".\\n1. Every Marain service will also have a Marain tenant created for it. For the remainder of this document, these will be referred to as \"Service Tenants\".\\n1. We will make use of the tenant hierarchy to group Client Tenants and Service Tenants under their own top-level parent. This means that we will have a top-level tenant called \"Client Tenants\" which parents all of the Client Tenants, and an equivalent one called \"Service Tenants\" that parents the Service Tenants (this is shown in the diagram below).\\n1. Clients will access the Marain services they are licenced for using their own tenant Id. Whilst the Marain services themselves expect this to be supplied as part of endpoint paths, there is nothing to prevent an API Gateway (e.g. Azure API Management) being put in front of this so that custom URLs can be mapped to tenants, or so that tenant IDs can be passed in headers.\\n1. When a Marain service depends on another one as part of an operation, it will pass the Id of a tenant that is a subtenant of it's own Service Tenant. This subtenant will be specific to the client that is making the original call. For example, the Workflow service has a dependency on the Operations Control service. If there are two Client Tenants for the Workflow Service, each will have a corresponding sub-tenant of the Workflow Service Tenant and these will be used to make the call to the Operation service. This approach allows the depended-upon service to be used on behalf of the client without making it available for direct usage.\\nEach of these tenants - Client, Service, and the client-specific sub-tenants of the Service Tenants - will need to hold configuration appropriate for their expected use cases. This will normally be any required storage configuration for the services they use, plus the Ids of any subtenants that have been created for them in those services, but could also include other things.\\nAs an example, suppose we have two customers; Contoso and Litware. For these customers to be able to use Marain, we must create Contoso and Litware tenants. We also have two Marain services available, Workflow and Operations. These also have tenants created for them (in the following diagrams, Service Tenants are shown in ALL CAPS and Client Tenants in normal sentence case. Service-specific client subtenants use a mix to indicate what they relate to):\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n```\\nContoso is licenced to use Workflow, and Litware is licenced to use both Workflow and Operations. This means that:\\n- The Contoso tenant will contain storage configuration for the Workflow service (as with all this configuration, the onboarding process will default this to standard Marain storage, where data is siloed by tenant in shared storage accounts - e.g. a single Cosmos database containing a collection per tenant. However, clients can supply their own storage configuration where required).\\n- The Litware tenant will contain storage configuration for both Workflow and Operations services, because it uses both directly.\\nIn addition, because both clients are licenced for workflow, they will each have a sub-tenant of the Workflow Service Tenant, containing the storage configuration that should be used with the Operations service. The Operations service does not have any sub-tenants because it does not have dependencies on any other Marain services:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|\\n+-> OPERATIONS\\n```\\nAs can be seen from the above, each tenant holds appropriate configuration for the services they use directly. In the case of the Client Tenants, they also hold the Id of the sub-tenant that the Workflow service will use when calling out to the Operations service on their behalf; this is necessary to avoid a costly search for the correct sub-tenant to use.\\nYou will notice from the above that Litware ends up with two sets of configuration for Operations storage; that which is employed when using the Operations service directly, and that used when calling the Workflow service and thus using the Operations service indirectly. This gives clients the maximum flexibility in controlling where their data is stored.\\nNow let's look at a slightly more complex example. Imagine in the scenario above, there is a third service, which we'll just call the FooBar service, and that both the Workflow and Operations service are dependent on it. In addition, Contoso are licenced to use it directly. This is what the dependency graph now looks like:\\n```\\n+------------+\\n|            |\\n+-------> WORKFLOW   +------+-----------------+\\n+---------+       |       |            |      |                 |\\n|         +-------+       +-^----------+      |                 |\\n| Contoso |                 |                 |                 |\\n|         |                 |                 |                 |\\n+----+----+                 |           +-----v------+          |\\n|                      |           |            |          |\\n|                      |     +-----> OPERATIONS +----+     |\\n|      +---------+     |     |     |            |    |     |\\n|      |         +-----+     |     +------------+    |     |\\n|      | Litware |           |                       |     |\\n|      |         +-----------+                       |     |\\n|      +---------+                               +---v-----v--+\\n|                                                |            |\\n+------------------------------------------------> FOOBAR     |\\n|            |\\n+------------+\\n```\\nIn order to support this, we start with an additional Service Tenant for the FooBar tenant.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso to use the Workflow service. This causes a chain of enrollments whereby a sub-tenant is created for WORKFLOW+Contoso, which is then enrolled to use the Operations service, creating a sub-tenant of OPERATIONS, OPERATIONS+WORKFLOW+Contoso, which is then enrolled to use the FooBar service (since FooBar does not have dependencies, this does not create any further sub tenants). The Workflow service is also directly dependent on FooBar, so WORKFLOW+Contoso is also enrolled to use FooBar resulting in storage configuration for FooBar being added to it.\\nThis leaves the tenant hierarchy looking like this:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso for the FooBar service. Since there are no additional dependencies, this does not result in any further sub-tenants being created, but does add storage configuration for FooBar to the Contoso tenant. As in the first example, Contoso now has two sets of storage configuration for the FooBar service, one for direct use and one for indirect use.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe now repeat the process of enrolling Litware for the Workflow service:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nSince Litware is not licenced to use FooBar, the Litware Client Tenant does not hold any configuration for that service itself.\\nFinally, we enroll Litware to use the Operations service. In this example, because Operations depends on FooBar, we need to create another sub-tenant of Operations to call FooBar with when Litware uses Operations directly, and enroll this new subtenant with FooBar. This leaves us with the following:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project. This context section should include the history and driving reason(s) on why a decision needed to be made.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context\\nMy first instinct for the tool was to use the\\nNPM [Serverless](https:\/\/serverless.com\/) framework.\\nThe main advantage of serverless is that it is cloud\\nagnostic. However JavaScript is not widely used at GDS.\\nAWS [Chalice](https:\/\/chalice.readthedocs.io) is a similar\\nframework which has a multi-stage deploy process to\\nautomate provisioning Lambda and Api Gateway but written\\nin Python.\\nBoth languages have frameworks for accessing the AWS API:\\n* The [SDK](https:\/\/docs.aws.amazon.com\/sdk-for-javascript)\\nfor JavasSript and\\n* [Boto3](https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/index.html)\\nfor Python.\\n","completion":"## Decision\nGiven that we had already made the decision that cloud\\nagnostic was not important for this service and that\\nPython was more in keeping with GDS common practice we\\ndecided to use Chalice.\\n"}
{"prompt":"## Context\\nThere are two purposed ways to setup the frontend and backend of the TTADP system.\\n1. The backend hosts the static frontend directly; there is one app. The single app will require one Authority to Operate (ATO). Some tooling can easily be shared between the frontend and backend. Also synchronized deploys will be easier to accomplish with a single app.\\n2. The backend and frontend are two separate apps, the backend a node express app and the frontend static HTML, javascript and CSS. Two apps allow for greater separation between the two components. The ATO will be more complicated to document. Tooling will be harder to share when the apps are split into two.\\n","completion":"## Decision\nWe will have a single unified app that combines the frontend and backend.\\n"}
{"prompt":"## Context\\nWe need to decide between [Kube Prometheus](https:\/\/github.com\/coreos\/kube-prometheus)\\nand [Prometheus Operator](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/prometheus-operator)\\nfor use in deploying the prometheus monitoring solution for DataLabs.\\n","completion":"## Decision\nWe have decided to use the [Prometheus Operator Helm Chart](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/prometheus-operator)\\nas it gives us the option to use an Helm chart over kubectl used in the kube-prometheus\\noption. The Prometheus Operator Helm chart provides a similar feature set to the\\nkube-prometheus option.\\n"}
{"prompt":"## Context\\nThe data backing some of our Pact states is created in the `before-test.sql`. This SQL was originally intended for application local integration tests. Unfortunately by using this same data for Pact testing we've coupled this data with the prepare-a-case Pact tests which means any changes to our integration test data can potentially break the prepare-a-case contract; and by extension both the court-case-service and prepare-a-case builds. This makes changes to `before-test.sql` incredibly risky as changes may need to be made not only to our integration tests, but also our Pact tests and those of prepare-a-case. The prepare-a-case codebase itself has similar coupling issues between its integration\/pact data and so an inconsequential change to `before-test.sql` can create huge amounts of otherwise unnecessary work.\\nTo a lesser extent, data defined in `before-test.sql` which is used in multiple integration tests can lead to more localised problems with unrelated integration tests within court-case-service breaking.\\n# Decision\\n1. Pact tests should use mocks at the service level to avoid coupling with integration test data.\\n2. (Under review) To avoid similar situations arising in local integration tests we should be open to creating new test data per test rather than re-using `before-test.sql`.\\n","completion":"## Decision\n1. Pact tests should use mocks at the service level to avoid coupling with integration test data.\\n2. (Under review) To avoid similar situations arising in local integration tests we should be open to creating new test data per test rather than re-using `before-test.sql`.\\n"}
{"prompt":"## Context\\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\\nThis project defines a library of components build on top of the AWS CDK and aiming to improve the user experience for managing infrastructure at the Guardian. As the library continues to grow, it is important that the library is structured sensibly, both for developers maintaining the library and those using it.\\n","completion":"## Decision\n<!-- What is the change that we're proposing and\/or doing? -->\\nThe top level directories with the `constructs` directory should mirror the AWS CDK library names.\\nEach directory should contain an `index.ts` file which exports all of the classes within it.\\nFiles within these directories can either be at the top level or nested within directories. Where nested directories exist, they should only be used for grouping multiple implementations of the same underlying construct. For example, `GuLogShippingPolicy`, `GuSSMRunCommandPolicy`, `GuGetS3ObjectPolicy` could all be in seperate files within the `constrcuts\/iam\/policies` directory. These directories should also export all memebers in an `index.ts` file.\\nPatterns can all be defined at the top level within the `patterns` directory. They should all be exported in the `index.ts` file so that they can all be imported from `@guardian\/cdk`\\n"}
{"prompt":"Context\\n=======\\nWe need to provide tenants with the ability to provision databases for use in\\ntheir applications. Our first iteration of this will be using RDS.\\nWe investigated some implementations of a service broker which supported RDS\\n- [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\n- [18F](https:\/\/github.com\/18F\/rds-service-broker)\\nDecision\\n========\\nWe will use the [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\nrds broker. As this is not a supported product, we will fork this and maintain\\nthis and implement new features ourselves.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe will be maintaining a new service broker, but have a head start on creating\\nit by basing it on an existing service broker.\\n","completion":"## Decision\n========\\nWe will use the [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\nrds broker. As this is not a supported product, we will fork this and maintain\\nthis and implement new features ourselves.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe will be maintaining a new service broker, but have a head start on creating\\nit by basing it on an existing service broker.\\n"}
{"prompt":"## Context\\nTo visualize the metrics of the etcd cluster, I would like to implement a\\ndashboard using the USE and RED methods.\\n- USE to measure the performance of the system hosting the etcd cluster\\n- RED to measure the performance of the gRPC side of the etcd cluster\\nBut I haven't found anything like this and I have no time to waste.  So\\nI have found a dashboard on the grafana site that has some metrics.\\n","completion":"## Decision\nAs I'm out of time, I'm just going to implement it at the configuration\\nmanagement system, to automatically provision the dashboard and the\\ndatasource.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nThe current URL structure used in runner is too long and overloaded. The URL should be as clean and readable as possible.\\nThe current URL structure of runner is:\\n```\\n\/questionnaire\/<eq_id>\/<survey_id>\/<collection_exercise_id>\/<group_id>\/<group_instance>\/<block_id>\\n```\\nWhich generates URLs that look like:\\n```\\n\/questionnaire\/1\/0005\/e9521994-e475-46cc-ae90-8ce4727e908f\/pay-pattern\/0\/pay-pattern-frequency\\n```\\n- `eq_id`, `survey_id` and `collection_exercise_id` are unnecessary as we can get that information from the session\\n- `group_id` isn't needed as all `block_id`s are unique within a given schema and we can look up the group id\\n- `group_instance` shouldn't be required for blocks that aren't repeating (currently it defaults to 0)\\nRemoving these from the URL effectively reduces the example above to:\\n```\\n\/questionnaire\/pay-pattern-frequency\\n```\\n","completion":"## Decision\n### Block ids are the root urls\\n```\\n\/<block_id>\\n\/do-you-live-here\\n\/what-is-your-name\\n\/i-dont-live-here\\n```\\n- No group id and unnecessary zero based group instance\\n- Easy to identify simple non repeating blocks\\n### Blocks that drive repeats (list creators)\\n```\\n\/<block_id>\\n\/does-anyone-else-live-here\\n\/<list_identifier>\/<add_block_id>\\n\/<list_identifier>\/<list_item_id>\/<edit_block_id>\\n\/<list_identifier>\/<list_item_id>\/<remove_block_id>\\n```\\n- Listing blocks have a list property declaring what list they populate\\n- The repeat identifier is generated when adding something to the list\\n- The repeat identifier is a short (6 characters?) randomly generated string\\n- Repeat identifiers are persisted to the questionnaire state to keep track of items added to the list\\n- The schema defines the relationship between the listing and add\/change\/remove blocks\\n### Blocks that repeat from a listing\\n```\\n\/<list_identifier>\/<list_item_id>\/<block_id>\\n\/householders\/<list_item_id>\/proxy\\n\/householders\/<list_item_id>\/sex\\n\/householders\/<list_item_id>\/date-of-birth\\n```\\n- `list_identifier` is a schema defined name for a list that can be created\\n- `list_item_id` identifies an item in that list\\n- List ids cannot clash with block ids\\n- A list can be populated by one or more list creator blocks e.g. \"does anyone else live here?\" and \"is there anyone temporarily away?\"\\n"}
{"prompt":"## Context and Problem Statement\\nOften, integrators use a `RegexpMetadataService` and extract the whole payload and store it as metadata. The payload can span multiple lines and be very long; sometimes they forget to remove it after they're done with it. If the message is funnelled into another workflow (via JMS, or something that supports MIME encoding), then the metadata is transported with the message to the new workflow.\\nWhen the message enters the second workflow, it is logged optionally including the payload, but always with the metadata. In the event that we have extra long metadata values, then this can make logging unusable when attempting to debug various runtime operations.\\nThis is recorded as [INTERLOK-2647](https:\/\/adaptris.atlassian.net\/browse\/INTERLOK-2647)\\n","completion":"## Decision\nAdd the MessageLogger interface\\n"}
{"prompt":"## Context and Problem Statement\\nWe want to **deploy** pods, services, and other resources on **Kubernetes**,\\nwhich is a GKE cluster for now.\\nWhat tool should we use to do this?\\n## Decision Drivers\\n- Ease of use\\n- How \"far\" is the tool from the Kubernetes lifecycle? - _\"Is it trailing\\nafter upstream and if so, how far away from it\"_\\n- Ability to express deployments in a descriptive \"as-code\" way\\n- Ease of use in a CI\/CD pipeline - _Does it require a local state? Weird\\nrequirements? Auth to other systems?_\\n- Reliability - _\"Will the tool usage often lead to messed up\\nservices\/cluster?\"_\\n","completion":"## Decision\n- Ease of use\\n- How \"far\" is the tool from the Kubernetes lifecycle? - _\"Is it trailing\\nafter upstream and if so, how far away from it\"_\\n- Ability to express deployments in a descriptive \"as-code\" way\\n- Ease of use in a CI\/CD pipeline - _Does it require a local state? Weird\\nrequirements? Auth to other systems?_\\n- Reliability - _\"Will the tool usage often lead to messed up\\nservices\/cluster?\"_\\nChosen option: \"A mix of **kubectl** and **Helm**\", because:\\n- `kubectl` is the native way of deploying to Kubernetes and it thus the most\\ndocumented, widely-used, appropriate and up-to-date.\\n- Helm fits best for complex stacks deployment and we're eager to try things\\nsuch as Helm charts repositories!\\nHelm will be used as an alternative to `kubectl` in a second part, when we'll\\nramp up things and when we will need to deploy more complex stacks. The\\nbeginning will be `kubectl` for our own services, and Helm for\\ncomponents\/middleware\/databases with existing charts such as RabbitMQ!\\nThis imply that the provided Kubernetes cluster(s) for this project will need\\nto be **provisionned with Helm requirements such as Tiller**.\\n"}
{"prompt":"## Context\\nDocumenting changes for a release can be challenging. It often involves reading\\nback through commit messages and PRs, looking for and classifying changes, which\\nis a time consuming and error prone process.\\n","completion":"## Decision\nWe will use a changelog (`CHANGELOG.md`) in the\\n[Keep a Changelog 1.0.0](https:\/\/keepachangelog.com\/en\/1.0.0\/) format to be\\nupdated when code changes happen, rather than at release time.\\n"}
{"prompt":"## Context\\nThe Federated Properties MVP introduced a [federatedPropertiesEnabled] setting which enables a Wikibase instance (local wiki) to read all Properties from a remote Wikibase (source wiki). A wiki with this setting enabled can only use federated Properties and disallows the creation or use of any local Properties.\\nFederated Properties v2 aims to make it possible for users to enable Federated Properties even if their Wikibase already contains data, so that they can choose to use both remote & local Properties to make statements.\\nTo achieve that we need to be able to differentiate between local and federated Properties.\\n### Considered Actions\\nWe considered two options:\\n1. Build source information into the Property IDs via repository prefixes.\\n2. Build source information into the Property IDs via Concept URIs.\\n#### Prefixes\\nThe administrator of the local Wikibase needs to configure a prefix for the source wiki, which will be used only for federated properties. Local properties will not have a prefix.\\n**Pros**:\\n- Easier to parse\\n- Easier to type\\n**Cons**:\\n- Ambiguous without knowing the config\\n#### Concept URIs\\nConcept URIs can be set for any Wikibase and also have a default value. An example of referencing a federated property from Wikidata is `http:\/\/www.wikidata.org\/entity\/P31`.\\nThe administrator of the local Wikibase needs to configure the concept URI for the source wiki in case it's not Wikidata.\\nSince concept URIs guarantee to end in the entity's ID we can think of them as prefixes. In the above example `http:\/\/www.wikidata.org\/entity\/` is the prefix. This way we don't need to worry about parsing them to extract the entity ID.\\n**Pros**:\\n- Unambiguous\\n- Standard practice in the linked data world\\n**Cons**:\\n- Typing urls in the browser when containing e.g. '&' is not nice. People might not remember to url encode.\\nWe think having symbols like '&' in the URL in unlikely. Also, API Sandbox can be used to test the API requests and figure out the right format if needed.\\n- Hard to change \/ Out of \u201cour\u201d control if they change\\nIf we use the URI only as a prefix to the entity, and not as an actual URL that will be used as such, then this shouldn't be a problem. We acknowledge it's possible for people who use the API to treat it like an URL.\\nWe feel responsible only if Wikidata's URI changes, as we anticipate shipping an example configuration for Wikidata which would then need updating. URIs configured for a source wiki different than Wikidata will have to be updated by the administrator of the local wiki in case they change.\\n","completion":"## Decision\nBuild source information into the Property IDs via Concept URIs, treating them like a prefix.\\n"}
{"prompt":"## Context\\nWe reviewed storage API design for completeness and consistency.\\n","completion":"## Decision\n* All requests\/responses use a single parameter that represents the request\/response object. This allows us to extend\/update request\/response object without changing the API.\\n* Add Delete() method\\n* Support bulk operations: BulkDelete() and BulkSet(). All operations in the bulk are expected to be completed within a single transaction scope.\\n* Support a generic BulkOperation() method, which is carried out as a single transaction.\\n* Transaction across multiple API requests is postponed to future versions.\\n* Actor state operations are moved to a new Actor interface. Please see [API-002-actor-api-design](.\/API-002-actor-api-design.md).\\n"}
{"prompt":"## Context\\nJekyll supports the markdown syntax by default. Files must be prefixed\\nwith a YAML Front Matter section for the engine to categorize, format\\nand display the blog post\/page. Project documentation files (e.g.: `README.md`)\\ntaken from a project release bundle must not have YAML Front Matter section\\nor any Jekyll or third-party specific formatting but pure markdown syntax.\\n","completion":"## Decision\nCreate YAML Front Matter index files in the `\/_posts` directory,\\nlike `2017-01-01-readme.md` and use the `include_relative`\\n[Jekyll command](https:\/\/jekyllrb.com\/docs\/includes\/) to include\\nthe unmodified `README.md`. Create a new layout format in Jekyll\\nnamed `markdown.html` and use the `markdown` value specifying\\nthe `layout:` in the YAML Front Matter index file, see example:\\n```\\n2017-01-01-readme.md\\n---\\nlayout: markdown\\ntitle: Sample title here\\nweight: 3\\ncategory: a-category-here\\ncategoryItemType: documentation\\ncategoryItemIsShown: 0\\n---\\n{% include_relative README.md  %}\\n```\\nThe `markdown.html` should have the same contents as the `post.html`\\nwith an extra `article`, `section` tags surrounding the `{content}` tag\\nto keep formatting consistent.\\n"}
{"prompt":"## Context\\nWikibase Manifest needs to provide essential metadata and configuration options about a Wikibase instance in an automated way.\\nWe can achieve this by using either the [Wikibase Action API](https:\/\/www.mediawiki.org\/wiki\/Wikibase\/API) or the [MediaWiki REST API](https:\/\/www.mediawiki.org\/wiki\/API:REST_API).\\n### Wikibase Action API\\n`+` The developers on the team have experience working with Wikibase's API\\n### MediaWiki REST API\\n`+` Built more recently\\n`+` Has good test coverage thanks to [mediawiki-tools-api-testing](https:\/\/github.com\/wikimedia\/mediawiki-tools-api-testing)\\n`+` Developers want to try out this new REST API\\n`+` Broadly accepted as a standard and probably easier to use for tool builders than the Action API\\nWe don't see any significant downsides of using one or the other.\\n","completion":"## Decision\nUse MediaWiki REST API.\\n"}
{"prompt":"## Context and Problem Statement\\nCurrently, both the client-side code (JavaScript and SCSS) and the\\nserver-side code are in the same repository. This leads to long build\\ntimes and unneeccessary CI runs if either of the code parts change.\\nAlso, having both \"parts\" in the same repository makes it harder to talk\\nabout each part, because we used the terminology of the Fundraising\\nDepartment, which called the Fundraising Application \"Frontend\" and the\\nFundraising Operation Center \"Backend\". For us as developers, \"Frontend\"\\nand \"Backend\" have different meanings.\\n## Decision Drivers\\n* Improve separation of concerns\\n* Allow parallel development of two distinct and independent domains\\n* Have two separate CI steps for each part\\n* Unblock the potential creation of a server-side JSON-based API\\n* Reduce deployment time for the backend repository\\n","completion":"## Decision\n* Improve separation of concerns\\n* Allow parallel development of two distinct and independent domains\\n* Have two separate CI steps for each part\\n* Unblock the potential creation of a server-side JSON-based API\\n* Reduce deployment time for the backend repository\\nSince the tweaking of the build system would introduce addionional\\ncomplexities in the build and CI system and would not solve the naming\\nconfusion, we decided to split the repository in two, named\\n`fundraising-app` for the server-side code and `fundraising-app-frontend`\\nfor the client-side code. We'll also take the opportunity to move the code\\nform GitHub to GitLab.\\nWe track the renaming of the `fundraising-backend` repository to\\n`fundraising-operation-center`\\nin a [separate ticket](https:\/\/phabricator.wikimedia.org\/T246796)\\n"}
{"prompt":"## Context and Problem Statement\\nHeroes are now permanent, which means they are always restarted.\\nShould they be?\\n## Decision Drivers\\n* A Hero must not exist on the system once the player leaves the game (normal exit)\\n* Restoring them from failures is not a requirement but nice to have\\n* Project is not finished, lacks most important features\\n","completion":"## Decision\n* A Hero must not exist on the system once the player leaves the game (normal exit)\\n* Restoring them from failures is not a requirement but nice to have\\n* Project is not finished, lacks most important features\\nChosen option: \"Restart `:temporary`\", because it has the minimum impact.\\n"}
{"prompt":"## Context and Problem Statement\\nThe data extraction w\/ transaction timestamp from Koshvani platform is a challenging task.\\n- The temporal data directly available on the platform is not easy to extract\\n- The validation of temporal data to ensure accuracy depends on factors\\n- Districts\/Treasuries which have updated their data\\n- Data\/Time of data upload vs extraction from departments\\n## Decision Drivers\\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n","completion":"## Decision\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n- Do a daily level data extraction from the platform\\n- Conduct analysis to identify daily, weekly and monthly expenditure numbers\\n- Select the accurate time period and mention methodology for the same\\n- Ignore previous time period missed in this analysis.\\nConduct the analysis on accurate data collection at in November, 2020 to take decision.\\n"}
{"prompt":"## Context and Problem Statement\\nCurrently, Jetstream is responsible for daily processing experiment data for analysis after enrollment has completed. We're also interested in processing experiment data for monitoring purposes. Monitoring data is a little different from analysis data in that it is live (updated every few minutes) and needs to be available immediately when an experiment starts collecting data. Also, unlike analysis data, monitoring data sacrifices absolute accuracy for timeliness in order to detect big problems early. Jetstream is only responsible for generating analysis data and we need an alternative approach to access monitoring data from Experimenter.\\nCurrently, [Grafana](https:\/\/grafana.telemetry.mozilla.org) is used to display monitoring data. It works by querying live tables from BigQuery on-demand and does not have any caching. We would like to eventually replace Grafana capabilities with custom visualizations inside of Experimenter. The goal is to have more flexibility than Grafana gives for how to display the data as well as less overhead for users when context switching from one visualization platform to another.\\n## Decision Drivers\\n* Data needs to be displayed \"live\", updating within minutes\\n* Cost needs to stay low\\n* Have flexibility with how the data is visualized\\n* Data loading speed\\n","completion":"## Decision\n* Data needs to be displayed \"live\", updating within minutes\\n* Cost needs to stay low\\n* Have flexibility with how the data is visualized\\n* Data loading speed\\n##### Chosen option: \"(2) Query Data On-Demand from Experimenter short term and (4) Optimize the ETL long term\"\\nOption (3) has too many cons compared to (1), (2) and (4) so it can be dismissed.\\nOption (1) is fast but will be quite expensive compared to Option (2) and (4) so it can also be dismissed.\\nOption (2) would be slower than (1) and (4) but it is cheaper than (1) and (3). Although Option (2) is slow, it would not be any slower than Grafana currently is for loading data. In some cases when the cache is hit, it would be faster than Grafana.\\nOption (4) has the potential to be the cheapest but it cannot be implemented immediately as it needs some further breakdown.\\nIt would be ideal to implement option (2) immediately to get some basic user-friendly monitoring in experimenter as soon as possible while continuing work on option (4). Once (4) ETL optimizations are complete, the Experimenter backend can switch to pulling data from this new source.\\nFor this first pass at adding monitoring data, only basic user counts per branch and experiment health metrics will be added.\\n"}
{"prompt":"## **Context**\\nProviding rich logging information will make it easier to investigate issues without making use of intrusive approaches (i.e: debug, memory dump), also making visible the behaviour of services by using monitoring tools to extract and\/or query these logs.\\nThe idea is to utilize services offered by AWS as they are comprehensive and can operate at scale with minimal administrative overhead.\\n","completion":"## Decision\n**AWS x-ray**\\nAWS X-Ray is an AWS managed service that provides the functionality to debug and analyze distributed applications.\\n"}
{"prompt":"## Context\\n\u4e8b\u6545\u9891\u53d1\uff0c\u5904\u7406\u901f\u5ea6\u6162\uff0c\u6d41\u7a0b\u4e0d\u6e05\u6670\uff0c\u5e76\u4e14\u4e00\u76f4\u672a\u505a\u8bb0\u5f55\uff0c\u5bfc\u81f4\u540c\u6837\u7684\u95ee\u9898\u4f1a\u518d\u6b21\u590d\u73b0\u3002\\n","completion":"## Decision\n\u5b9a\u4e49\u6545\u969c\u62a5\u544a\u6a21\u677f\uff1a\\n1. \u5bf9\u6545\u969c\u8fdb\u884c\u63cf\u8ff0\uff0c\u4ee5\u4fbf\u67e5\u8be2\u5e76\u5bf9\u6211\u4eec\u7684\u7cfb\u7edf\u7a33\u5b9a\u6027\u505a\u8bc4\u4f30\uff1b\\n2. \u5bf9\u6545\u969c\u505a\u5206\u6790\uff0c\u4fbf\u4e8e\u672a\u6765\u53ef\u5feb\u901f\u5904\u7406\u540c\u6837\u95ee\u9898\uff1b\\n3. \u52a0\u76d1\u63a7\uff0c\u4ee5\u4fbf\u95ee\u9898\u51fa\u73b0\u524d\u5c31\u505a\u5904\u7406\uff1b\\n4. \u89e3\u51b3\u5e76\u5347\u7ea7\u65b9\u6848\uff0c\u5b8c\u5168\u907f\u514d\u6b64\u7c7b\u95ee\u9898\u3002\\n"}
{"prompt":"# Context and Problem Statement\\nWith support for more file types there must be a way to extract text\\nfrom all of them. It is better to extract text from the source files,\\nin contrast to extracting the text from the converted pdf file.\\nThere are multiple options and multiple file types. Again, most\\npriority is to use a java\/scala library to reduce external\\ndependencies.\\n# Considered Options\\n","completion":"## Decision\n- MS Office files: POI library\\n- Open Document files: Tika, but integrating the few source files that\\nmake up the open document parser. Due to its huge dependency tree,\\nthe library is not added.\\n- PDF: Apache PDFBox. I know this library better than itext.\\n"}
{"prompt":"## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n","completion":"## Decision\nUse Static Code Analysis to find violations of standards.\\n"}
{"prompt":"## Context\\nDomain-driven design (DDD) is the concept that the structure and language of software code (class names, class methods, class variables) should match the business domain. For example, if a software processes loan applications, it might have classes such as LoanApplication and Customer, and methods such as AcceptOffer and Withdraw.\\nDDD connects the implementation to an evolving model.\\nDomain-driven design is predicated on the following goals:\\n- placing the project's primary focus on the core domain and domain logic;\\n- basing complex designs on a model of the domain;\\n- initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems.\\nConcepts of the model include:\\n### Context\\nThe setting in which a word or statement appears that determines its meaning;\\n### Domain\\nA sphere of knowledge (ontology), influence, or activity. The subject area to which the user applies a program is the domain of the software;\\n### Model\\nA system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain;\\n### Ubiquitous Language\\nA language structured around the domain model and used by all team members to connect all the activities of the team with the software.\\n","completion":"## Decision\nWe In the context of the ODH we will use the pricniples of Domain Driven Design and use the language which is related to the business where it is operated.\\n"}
{"prompt":"## Context and Problem Statement\\nWe have to transfer json data and verify the integrity of the data.\\nThe transfer involves an Authorization server which provides the json, a client which gets the data form that server and pass it to the WebSocket API.\\nThe WebSocket API must able to verify the integrity of the json data.\\n## Decision Drivers <!-- optional -->\\n* Use standard encodings\\n","completion":"## Decision\n* Use standard encodings\\nChosen option: \"Send SHA256 hash of Base64Url encoded json\", because this method is platform independent and not much session state is required.\\n### Positive Consequences <!-- optional -->\\n* The JWT really function as a verification token for the other requests.\\n* Can be applied to all json data that must be verified.\\n### Negative Consequences <!-- optional -->\\n* The json must be transferred in Base64Url encoding\\n"}
{"prompt":"- [Context](#context)\\n- [Alternative Approaches](#alternative-approaches)\\n- [Decision](#decision)\\n- [Detailed Design](#detailed-design)\\n- [EventSink](#eventsink)\\n- [Supported Sinks](#supported-sinks)\\n- [`KVEventSink`](#kveventsink)\\n- [`PSQLEventSink`](#psqleventsink)\\n- [Configuration](#configuration)\\n- [Future Improvements](#future-improvements)\\n- [Consequences](#consequences)\\n- [Positive](#positive)\\n- [Negative](#negative)\\n- [Neutral](#neutral)\\n- [References](#references)\\n","completion":"## Decision\n- [Detailed Design](#detailed-design)\\n- [EventSink](#eventsink)\\n- [Supported Sinks](#supported-sinks)\\n- [`KVEventSink`](#kveventsink)\\n- [`PSQLEventSink`](#psqleventsink)\\n- [Configuration](#configuration)\\n- [Future Improvements](#future-improvements)\\n- [Consequences](#consequences)\\n- [Positive](#positive)\\n- [Negative](#negative)\\n- [Neutral](#neutral)\\n- [References](#references)\\nWe will adopt a similar approach to that of the Cosmos SDK's `KVStore` state\\nlistening described in [ADR-038](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/master\/docs\/architecture\/adr-038-state-listening.md).\\nWe will implement the following changes:\\n- Introduce a new interface, `EventSink`, that all data sinks must implement.\\n- Augment the existing `tx_index.indexer` configuration to now accept a series\\nof one or more indexer types, i.e., sinks.\\n- Combine the current `TxIndexer` and `BlockIndexer` into a single `KVEventSink`\\nthat implements the `EventSink` interface.\\n- Introduce an additional `EventSink` implementation that is backed by\\n[PostgreSQL](https:\/\/www.postgresql.org\/).\\n- Implement the necessary schemas to support both block and transaction event indexing.\\n- Update `IndexerService` to use a series of `EventSinks`.\\nIn addition:\\n- The Postgres indexer implementation will _not_ implement the proprietary `kv`\\nquery language. Users wishing to write queries against the Postgres indexer\\nwill connect to the underlying DBMS directly and use SQL queries based on the\\nindexing schema.\\nFuture custom indexer implementations will not be required to support the\\nproprietary query language either.\\n- For now, the existing `kv` indexer will be left in place with its current\\nquery support, but will be marked as deprecated in a subsequent release, and\\nthe documentation will be updated to encourage users who need to query the\\nevent index to migrate to the Postgres indexer.\\n- In the future we may remove the `kv` indexer entirely, or replace it with a\\ndifferent implementation; that decision is deferred as future work.\\n- In the future, we may remove the index query endpoints from the RPC service\\nentirely; that decision is deferred as future work, but recommended.\\n"}
{"prompt":"## Context\\nServices help us by wrapping and presenting functionality. Typically services\\npresent one piece of functionality, and use a single method to invoke this\\nfunctionality. For the sake of consistency, we want to align this \"entry point\"\\nso that services can be called the same way.\\n","completion":"## Decision\nUse option 2, `call`. Additionally, provide both the instance method `#call` and a class\\nmethod `.call`, of which the later instantiates the service and calls `#call`.\\n"}
{"prompt":"## Context\\nThis Maven Plugin needed a build tool to compile, test and package it.  Maven and Gradle were the only viable options.\\n","completion":"## Decision\nI decided to use Maven as it felt a better fit to use Maven to build a Maven plugin, rather than using Gradle.\\n"}
{"prompt":"## Context and Problem Statement\\nWe previously decided to keep inputs and outputs of play-frontend-govuk components a 1-to-1 match with govuk-frontend\\nnunjucks example implementations.\\nGenerally components from govuk-frontend accept their copy as input, however the skip link in govukTemplate is\\nhard-coded which is causing accessibility failures for services.\\nThis is an accessibility failure because a page translated into welsh may cause assistive technology to mispronounce the\\ncontent of the english skip link because it has no explicit language attribute.\\n## Decision Drivers\\n* Because it's an accessibility failure we want to fix this as a priority quickly\\n* And we would like for this fix to be available as widely as possible without requiring template changes from services\\n* At the same time we don't want to reduce test coverage \/ diverge from our current testing strategy\\n","completion":"## Decision\n* Because it's an accessibility failure we want to fix this as a priority quickly\\n* And we would like for this fix to be available as widely as possible without requiring template changes from services\\n* At the same time we don't want to reduce test coverage \/ diverge from our current testing strategy\\nWe've chosen option 1 because unlike other components, we don't have any automatically generated fixtures or integration\\ntests for govukTemplate. We maintain test fixtures for the templates manually in the additional fixtures folder. This\\nmakes it possible for us to patch this change into govukTemplate with minimal maintenance cost without reducing our test\\ncoverage \/ diverging far from our current testing strategy.\\n"}
{"prompt":"## Context\\nCurrently we store `capi-kpack-watcher` directly in `capi-k8s-release`. This has\\ncaused some complexity when bumping image versions in our concourse pipeline.\\nWe're also planning on reworking the watcher to use [kubebuilder](https:\/\/github.com\/kubernetes-sigs\/kubebuilder), and so now is a\\ngood time to change any source code organization decisions.\\nProposed locations for `capi-kpack-watcher` source code:\\n1. `cf-api-kpack-watcher` a repo just for this particular component\\n1. Pros:\\n1. Very clear what goes where\\n1. It'd be a standard `kubebuilder` go repo\\n1. Fits our current pattern of go service repositories (1 repo for each service)\\n1. Cons:\\n1. Many more repos to manage to keep this pattern consistent in the\\nnear-future as we will also create controllers for routes, droplets, etc\\n1. `cf-api-controllers`\/`cf-controllers` which would contain other future controllers we create for CF API resources\\n1. Pros:\\n1. It'd be a standard `kubebuilder` go repo with multiple controllers\\n1. Only one more repo to manage to keep this pattern consistent\\n1. Cons:\\n1. Possibly unclear in the future whether controllers without CF API dependency should go here. (This is specifically\\nif we use a more generic name like `cf-controllers` which doesn't make clear this repository is only managed by our team)\\n1. Marrying the versions of all these controllers together makes it\\ndifficult if you ever to want use different effective versions of\\nsource for the controllers\\n- e.g. if we introduce a bug in `controller-a` source, but want to\\nkeep the same version of `controller-b` source\\n1. `cloud_controller_ng` mono-repo\\n1. Pros:\\n1. 0 more repos to manage to keep this pattern consistent\\n1. No way for API to get out-of-sync with controllers - they all come from\\nthe same source code.\\n- Versioning, for instance, becomes simplified a bit with this\\n1. Cons:\\n1. 1 repo with Ruby and Go code inside can be _very_ annoying to grep.\\n1. Nonstandard `kubebuilder` layout\\n1. Marrying the versions of CCNG to these controllers makes it difficult\\nif you ever want to split apart the versioning\\n- e.g. if we introduce a bug in the controller code, but want to keep the\\nsame version of CCNG code\\n1. leave it in `capi-k8s-release\/src\/capi-kpack-watcher`\\n1. Pros:\\n1. 0 more repos to manage to keep this pattern consistent\\n1. Cons:\\n1. Bumping `capi-k8s-release`'s built image reference with a SHA that points\\nat itself is confusing and difficult to manage within concourse.\\n1. Nonstandard `kubebuilder` layout\\n","completion":"## Decision\nWe are going to proceed with the option of leaving it in `capi-k8s-release\/src\/capi-kpack-watcher`; however, we are also going\\nto rename the folder\/pseudo-repository containing the source to be called `cf-api-controllers` for now with the intention to\\nmove the folder out into a separate repository later if we still so desire.\\n"}
{"prompt":"## Context\\nWe need this to make the 'Referrals Notification' work.\\nNeeded a way to store the 'last_seen' value during the whole session. Because it is updated every hour by a middleware, we can't use it directly from the user model \/ database, as that would cause us to never show a notification ('last seen' is updated before we even get a chance to show it on the page.)\\n### Options\\n1. Laravel Sessions - Didn't work\\n2. Regular PHP Sessions\\n","completion":"## Decision\nOption 2 worked, so it's implemented.\\nIn the LastSeen middleware we check for a 'prev_last_seen' session variable.\\n- If missing, we update it with the 'last_seen' value.\\n- If it exists, we don't change it.\\nLater, the User->hasNewReferrals() checks it to determine the right cutoff time, either 10 days ago or the more distant 'last_seen' value.\\n"}
{"prompt":"## Context\\nSome modules have components which require many acl privileges. Examples\\nare the rule builder or the media manager. Therefore, you need all privileges\\nin each module which have these components. Also you do not want to add the\\nmodule to the dependency section because then the user has full access to module\\nin the administration.\\n","completion":"## Decision\nTo avoid duplication of these privileges we use a helper function. These\\nfunction returns all privileges from the other module dynamically. You can\\nuse it directly in the privileges:\\n```js\\nShopware.Service('privileges')\\n.addPrivilegeMappingEntry({\\ncategory: 'permissions',\\nparent: null,\\nkey: 'promotion',\\nroles: {\\nviewer: {\\nprivileges: ['promotion:read',],\\ndependencies: []\\n},\\neditor: {\\nprivileges: [\\n'promotion:update',\\nShopware.Service('privileges').getPrivileges('rule.creator')\\n],\\ndependencies: [\\n'promotion.viewer'\\n]\\n}\\n}\\n});\\n```\\n"}
{"prompt":"## Context\\nWe need to record the architectural and desing decisions made during this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by\\nMichael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\nAlso https:\/\/medium.com\/better-programming\/here-is-a-simple-yet-powerful-tool-to-record-your-architectural-decisions-5fb31367a7da\\n"}
{"prompt":"## Context\\nI need to decided if I use a SPA or a Server Side rendered app. This include performance and maintenance considerations.\\n","completion":"## Decision\nI use Client Side Rendering (Create React App).\\n"}
{"prompt":"## Context\\nWe need a consistent way to access the Application Bar to update the Caption or change the Hamburger menu to homeAsUp\\n","completion":"## Decision\nWe create an interface which will be implemented by the Activity. Due to the single activity architecture, the base\\nfragment will cast the context to which the fragment is attached to the interface and set a member variable. Also some\\nmethods will be exposed by the fragment so that extended fragments can make use of these methods.\\n"}
{"prompt":"## Context\\n_The issue motivating this decision, and any context that influences or constrains the decision._\\nAs the V3 Acceleration Team was developing the V7 version of `cf push`, the team saw opportunities to structure the code to make it easier to understand and maintain. In general, the `push` command logic is some of the most complex in the codebase, and this command was still considered experimental, so it seemed like the right time and place to invest our energy in refactoring.\\nIn the V6 CLI, the code for `push` was split roughly into two methods: `Conceptualize` and `Actualize`. `Conceptualize` was responsible for taking user input (flags, manifest properties, etc) and generating a \"push plan\" for each app that is being pushed (a struct containing various pieces of information needed to create\/push\/update the app). `Actualize` was responsible for taking these push plans and, based on the plans, taking the necessary actions to complete the push process. The refactor preserves the spirit of this division, but prevents these two methods from growing too large and unmaintainable.\\nSome of the goals of the refactor were:\\n- Make it easier to add features to `push` (e.g. new flags, new options for flags, new manifest properties)\\n- Make it easier to unit test the components of the `push` workflow. This means meant splitting the command into several smaller functions that can be tested individually but composed into sequences based on the given user input.\\n","completion":"## Decision\n_The change that we're proposing or have agreed to implement._\\nBoth the `Conceptualize` and `Actualize` parts of the push process have been split up and refactored in a manner inspired by [hexagonal architecture](https:\/\/medium.com\/@nicolopigna\/demystifying-the-hexagon-5e58cb57bbda).\\n### Hexagonal architecture\\nFor our purposes, \"hexagonal architecture\" means the following:\\n- There is one place in the code responsible for calling a given sequence of functions in order. The output of one function is passed in as input to the next function.\\n- All of these functions have the same signature.\\n- The central place where the functions are called is agnostic to what the functions are doing.\\n- Any new features\/branches\/logic should be added to one or more of these functions (or, a new function should be added to encapsulate it). We generally shouldn't need to touch the place where the functions are called.\\n### Splitting up `Conceptualize`\\nWhat was previously called `Conceptualize` now looks roughly like this:\\n_Note: We are still iterating on this code, so it may evolve over time and no longer look exactly like this. Still, the code below illustrates the idea we were going for._\\n```go\\nvar pushPlans []PushPlan\\nfor _, manifestApplication := range getEligibleApplications(parser, appNameArg) {\\nplan := PushPlan{\\nOrgGUID:   orgGUID,\\nSpaceGUID: spaceGUID,\\n}\\nfor _, updatePlan := range actor.PreparePushPlanSequence {\\nvar err error\\nplan, err = updatePlan(plan, overrides, manifestApplication)\\nif err != nil {\\nreturn nil, err\\n}\\n}\\npushPlans = append(pushPlans, plan)\\n}\\nreturn pushPlans, nil\\n```\\nThis is the central place where all of the \"prepare push plan\" functions are called. We loop through `actor.PreparePushPlanSequence`, an array of functions, and call each one with a push plan. They each have the opportunity to **return a modified push plan**, which then gets **passed into the next function**.\\nIn this case, each function is also called with `overrides` and `manifestApplication`, which represent user input (in the form of flags and manifest properties, respectively). This lets each function inspect the user input and modify the push plan accordingly.\\nWhen this loop completes, the original push plan has flowed through each function in the sequence and has been modified to include information based on the given flags\/manifest.\\nLet's now look at how `actor.PreparePushPlanSequence` is defined:\\n```go\\nactor.PreparePushPlanSequence = []UpdatePushPlanFunc{\\nSetupApplicationForPushPlan,\\nSetupDockerImageCredentialsForPushPlan,\\nSetupBitsPathForPushPlan,\\nSetupDropletPathForPushPlan,\\nactor.SetupAllResourcesForPushPlan,\\nSetupDeploymentStrategyForPushPlan,\\nSetupNoStartForPushPlan,\\nSetupNoWaitForPushPlan,\\nSetupSkipRouteCreationForPushPlan,\\nSetupScaleWebProcessForPushPlan,\\nSetupUpdateWebProcessForPushPlan,\\n}\\n```\\nThis is just a simple array with a bunch of functions that all conform to the correct interface (they all have type `UpdatePushPlanFunc`).\\nHere is an example of one of them:\\n```go\\nfunc SetupScaleWebProcessForPushPlan(pushPlan PushPlan, overrides FlagOverrides, manifestApp manifestparser.Application) (PushPlan, error) {\\nif overrides.Memory.IsSet || overrides.Disk.IsSet || overrides.Instances.IsSet {\\npushPlan.ScaleWebProcessNeedsUpdate = true\\npushPlan.ScaleWebProcess = v7action.Process{\\nType:       constant.ProcessTypeWeb,\\nDiskInMB:   overrides.Disk,\\nInstances:  overrides.Instances,\\nMemoryInMB: overrides.Memory,\\n}\\n}\\nreturn pushPlan, nil\\n}\\n```\\nThis simple function populates fields on the push plan based on flag overrides and returns the enhanced push plan, making it easy to test on its own.\\nWhen all of the functions have run and updated the push plan, the \"actualize\" step doesn't need to know about flags and manifest properties anymore; it only needs to receive a push plan because all user input has been resolved and combined into the push plan object.\\n### Splitting up `Actualize`\\nThere is still (at the time of this writing) a function called `Actualize`, but it now looks like this:\\n```go\\nfor _, changeAppFunc := range actor.ChangeApplicationSequence(plan) {\\nplan, warnings, err = changeAppFunc(plan, eventStream, progressBar)\\nwarningsStream <- warnings\\nif err != nil {\\nerrorStream <- err\\nreturn\\n}\\nplanStream <- plan\\n}\\n```\\nThis is quite similar to the loop through `actor.PreparePushPlanSequence` above. We loop through `actor.ChangeApplicationSequence(plan)`, which returns an array of functions, and we call each one with the push plan. Each one returns a push plan that then gets passed into the next function.\\n_Note: The rest of that code uses streams to report progress, errors, and warnings, but this is not the focus of this ADR (and we may end up changing this as well)._\\nThe **biggest difference** from `Conceptualize` is that instead of being a static list of functions (like `actor.PreparePushPlanSequence`), `actor.ChangeApplicationSequence` is a function that takes in a push plan and returns an array of `ChangeApplicationFunc`s. This allows us to dynamically build up the sequence of actions we run based on the push plan, rather than run the same sequence every time.\\nLet's look at how that works:\\n```go\\nactor.ChangeApplicationSequence = func(plan PushPlan) []ChangeApplicationFunc {\\nvar sequence []ChangeApplicationFunc\\nsequence = append(sequence, actor.GetUpdateSequence(plan)...)\\nsequence = append(sequence, actor.GetPrepareApplicationSourceSequence(plan)...)\\nsequence = append(sequence, actor.GetRuntimeSequence(plan)...)\\nreturn sequence\\n}\\n```\\nThis function is responsible for building up the sequence based on the given plan. It delegates to three helpers, each of which builds up a subsequence of actions. Here's one of them:\\n```go\\nfunc ShouldCreateBitsPackage(plan PushPlan) bool {\\nreturn plan.DropletPath == \"\" && !plan.DockerImageCredentialsNeedsUpdate\\n}\\n\/\/ ...\\nfunc (actor Actor) GetPrepareApplicationSourceSequence(plan PushPlan) []ChangeApplicationFunc {\\nvar prepareSourceSequence []ChangeApplicationFunc\\nswitch {\\ncase ShouldCreateBitsPackage(plan):\\nprepareSourceSequence = append(prepareSourceSequence, actor.CreateBitsPackageForApplication)\\ncase ShouldCreateDockerPackage(plan):\\nprepareSourceSequence = append(prepareSourceSequence, actor.CreateDockerPackageForApplication)\\ncase ShouldCreateDroplet(plan):\\nprepareSourceSequence = append(prepareSourceSequence, actor.CreateDropletForApplication)\\n}\\nreturn prepareSourceSequence\\n}\\n```\\nIn this case, we only want to include one of these three functions in the final sequence, which is determined based on properties of the push plan.\\nSince all of these functions are small and straightforward on their own, they are easy to unit test. They can be **composed together in different sequences to build up different push workflows** (based on different flags\/manifests), and this is where the refactor really starts to pay off.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nThe tool provides a number of related commands to create and manipulate architecture decision records.\\nHow can the user find out about the commands that are available?\\n","completion":"## Decision\nThe tool defines a single command, called adr.\\nThe first argument to adr (the subcommand) specifies the action to perform. Further arguments are interpreted by the subcommand.\\nRunning adr without any arguments lists the available subcommands.\\nSubcommands are implemented as Java classes with a defined interface in the package `adr.commmand`. A reflection mechanism is used so that new commands can be added without changing the rest of the code e.g. the subcommand new is implemented as class`CommandNew`, the subcommand help as the class `CommandHelp` and so on. Each command class is annotated with the name of the command (so the class name is not important) and also with its own help instructions so that it is completely self-contained.\\n"}
{"prompt":"Context\\n-------\\nThe purpose of this library is to collect together data from one or more sensor devices, save that data to a database and periodically create and send batches of data for processing.\\nShould the library be setup to handle multiple devices, checking and logging data from each device in turn or should that be left to the application using the library?\\nPros for Multi device\\n-------\\n1. easier for application developer - just provide the access details of each device and away you go.\\nPros for Single device\\n-------\\n1. Each device may well have different timings and manage data in different ways. At two extremes:\\n- data is stored on the device and collected daily\\n- device requires sampling at 10 second intervals\\nThis level of complexity is better handled by the application.\\n2. If there are delays in collecting data from one device, this could have a knock on effect in the collection interval of another.\\n3. If one goes down they all go down.\\nDecision\\n----------\\nKeep it Simple - Library written for one device.  In future may add settings to accomodate threading if this proves useful.\\nConsequences\\n--------------\\n","completion":"## Decision\n----------\\nKeep it Simple - Library written for one device.  In future may add settings to accomodate threading if this proves useful.\\nConsequences\\n--------------\\n"}
{"prompt":"## Context\\nOrganize your cloud assets to support operational management and accounting requirements. Well-defined naming and metadata tagging conventions help to quickly locate and manage resources. These conventions also help associate cloud usage costs with business teams via chargeback and showback accounting mechanisms.\\n","completion":"## Decision\nThe naming cloud object naming convention is based on the Ironstone Naming conventions and can be found on our interal Wiki\\n"}
{"prompt":"## Context\\nWe need to choose how we will build our app.\\n","completion":"## Decision\nWe use Gradle because it is very flexible, has some plugins for Docker integration and @mkuzmin know this technology very well.\\n"}
{"prompt":"## Context\\nCurrently used: 2.1.0\\nThe latest stable version of Spring Boot is 2.2.1:\\nhttps:\/\/github.com\/spring-projects\/spring-boot\/wiki\/Spring-Boot-2.2-Release-Notes\\nMore frequent but smaller upgrades are recommended.\\n","completion":"## Decision\nSpring Boot will be upgraded to 2.2.1.\\nAccording to release notes no migration needed on Mokka side.\\n"}
{"prompt":"## Context\\nTo navigate between tabs, users can swipe left or right or click a tab.\\n","completion":"## Decision\nWe will use NativeBase's component \"Tabs\".\\nPreviously used [react-native-tab-view](https:\/\/github.com\/react-native-community\/react-native-tab-view) since it's a cross-platform component that works perfectly on iOS and Android.\\n"}
{"prompt":"## Context\\nSome Ops (esp. convolution) have many parameters. Many of them can have reasonable defaults, but even then creating\\nsignatures for evey reasonable configuration may be impossible, as those signatures would require different naming in\\norder to be actually distinguishable from each other.\\nIn other cases, an op may have a lot of same typed parameters that are required (e.g. GRU, LSTM, SRU) but it is very\\neasy to mix them up.\\nFor both of those cases (many optional parameters, easily mixed up required parameters) it is reasonable to use a\\nconfig holder with builder pattern in languages that do not support named or default parameters.\\nIn our current codebase those configurations are often used across several related ops.\\n","completion":"## Decision\nWe add a `Config(\"name\"){ ... }` section to the namespace context. It supports `Input` and `Arg` definitions in the same\\nway that `Op` does.\\nOps that want to use that config can use `useConfig(conf)`. As configs are often reused across related objects, this\\nwill have the effect of a mixin: All inputs and args defined in that config will also be automatically defined on that\\nOp. If there is a naming conflict, an exception will be thrown at construction time.\\nFor default signatures, configs will be passed at the end, in the order that they were added to the Op.\\nIf other signatures are desired, configs, like regular inputs and args, can be passed to `Signature`.\\nIn languages that do not support default or named parameters, a config holder will be created, that will take the\\nparameters of the config using a builder pattern. For languages with default and named parameters, no additional config\\nholder will be created, and the parameters of the config will be treated as if they were directly configured on the Op.\\n### Example\\nThis example shows a very simple case in order to highlight how this feature would be used.\\n```kotlin\\nfun RNN() = Namespace(\"RNN\"){\\nval sruWeights = Config(\"SRUWeights\"){\\nInput(FLOATING_POINT, \"weights\"){ description = \"Weights, with shape [inSize, 3*inSize]\" }\\nInput(FLOATING_POINT, \"bias\"){ description = \"Biases, with shape [2*inSize]\" }\\n}\\nOp(\"SRU\"){\\nInput(FLOATING_POINT, \"x\"){ description = \"...\" }\\nInput(FLOATING_POINT, \"initialC\"){ description = \"...\" }\\nInput(FLOATING_POINT, \"mask\"){ description = \"...\" }\\nuseConfig(sruWeights)\\nOutput(FLOATING_POINT, \"out\"){ description = \"...\" }\\n}\\nOp(\"SRUCell\"){\\nval x = Input(FLOATING_POINT, \"x\"){ description = \"...\" }\\nval cLast = Input(FLOATING_POINT, \"cLast\"){ description = \"...\" }\\nval conf = useConfig(sruWeights)\\nOutput(FLOATING_POINT, \"out\"){ description = \"...\" }\\n\/\/ Just for demonstration purposes\\nSignature(x, cLast, conf)\\n}\\n}\\n```\\n"}
{"prompt":"### Context and Problem Statement\\nMost of the Form types (as [identified](https:\/\/miro.com\/app\/board\/uXjVO\\_vBIYc=\/)) are already integrating into external systems. But not all of them do (eg. Quiz). For the ones that don\u2019t we need to identify what\u2019s a reasonable policy for NRO admins to have the time to decide the extent of data they want to keep but also have the time to export them. But even for the forms that send data to other systems (eg. Petitions) we need a failsafe to make sure data is actually sent over before deletion.\\nAt the same time we should aim for respecting GDPR requirements for the period of time these data remain on websites\u2019 databases.\\n### Decision Outcome\\n#### **Data retention**\\nEnforce a 90 days retention policy. All submission entries will be automatically deleted after point. It would be up to NRO admins to export any submission data that are not synced to another external system if they wish to.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-retention.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n#### **Integrate into Wordpress export\/delete tools**\\nThis is useful in cases where an NRO gets a request by a supporter to delete or export personal data.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-exporter.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n#### **Minimize personal data footprint**\\nDon\u2019t store IP addresses on form submissions.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-ip-address.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n### Links\\n* [Data Flows Miro board](https:\/\/miro.com\/app\/board\/uXjVO\\_vBIYc=\/)\\n","completion":"## Decision\n#### **Data retention**\\nEnforce a 90 days retention policy. All submission entries will be automatically deleted after point. It would be up to NRO admins to export any submission data that are not synced to another external system if they wish to.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-retention.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n#### **Integrate into Wordpress export\/delete tools**\\nThis is useful in cases where an NRO gets a request by a supporter to delete or export personal data.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-exporter.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n#### **Minimize personal data footprint**\\nDon\u2019t store IP addresses on form submissions.\\n<figure><img src=\"..\/..\/.gitbook\/assets\/form-builder-ip-address.png\" alt=\"\"><figcaption><\/figcaption><\/figure>\\n### Links\\n* [Data Flows Miro board](https:\/\/miro.com\/app\/board\/uXjVO\\_vBIYc=\/)\\n"}
{"prompt":"## Context\\nThis application will require a fairly standard application stack - web server, database store and an asynchronous task queue. In the past, the team has used either Vagrant + Ansible or Docker + Docker Compose to build these application stacks.\\nIn general, Docker containers improve the CI build and deployment workflows, at the expense of a slightly more complicated development environment, especially for users not using Linux. In the past, the team has wrapped the Docker environment within a barebones Ubuntu VM using Vagrant to streamline the development workflow across different OSes. Recently however, Docker released a new tool, Docker for Mac, which attempts to streamline native use of containers on macOS. This tool may eliminate the need for the wrapper VM, but has some potential pitfalls.\\nPrevious projects defaulted to an Ubuntu VM, with the containers using the simplest debian-based OS. Azavea maintains a series of Docker containers that provide the building blocks for the application stack we will be building.\\nThis project contains a 'Bicycle Network Analysis' task which is runs on a self-contained PostgreSQL instance. Running this via Vagrant+Ansible or a Docker container should be relatively straightforward either way, since the task has known software dependencies and does not have any external dependencies. When this project begain, this task was configured via Vagrant+Ansible. Some additional work would be necessary to convert this task to a Docker container.\\n","completion":"## Decision\nIn order to take advantage of the better deployment and provisioning workflows provided by AWS ECS when using containers, we decided to construct the development environment using a Docker Compose environment wrapped within an Ubuntu VM. While Docker for Mac looks compelling, it has a few downsides:\\n- We cannot control the version of Docker installed, which could be problematic as the project ages\\n- There are potential incompatibilies for users with the older Docker Toolbox installed\\n- It may be difficult to cull outdated container images across projects\\n- It may be difficult to isolate various project instances and their dependencies\\nUsing the wrapper VM avoids these issues and provides us with a relatively 'known good' experience for a project with somewhat limited budget constraints.\\n"}
{"prompt":"## Context\\nGUUI intends to be, at some level, a library of components.\\nDotcom rendering will require a mapping of CAPI element to React Component.\\nStorybook is a widely used library which allows a series of demos and examples to be easily constructed. It also has support for typescript.\\n","completion":"## Decision\nIt is possible to envision a split in components:\\n- those which form our design system\\n- those which render individual elements of content from CAPI\\nEach of these should have an independant storybook, allowing the design system ones to express the variety of ways each component can and should be used. And allowing, as they are developed, each CAPI element rendering component to demonstrate the variety of content they can encapsulate.\\n"}
{"prompt":"## Context\\nJMAP Push notifications allow clients to efficiently update (almost) instantly to stay in sync with data changes on the server.\\nIn order to support this, we need to have the server emit newest state changes to the client over a PUSH channel.\\n","completion":"## Decision\nWe will implement [RFC-8887 - A JSON Meta Application Protocol (JMAP) Subprotocol for WebSocket](https:\/\/tools.ietf.org\/html\/rfc8887)\\nas a transport mechanism for PUSH notifications.\\nWe will [generalize EventBus](0046-generalize-event-bus.md) in order to achieve an out-of-the box Publish-Subscribe\\nsystem for JMAP related events, that does not fit in the mailbox API.\\nWe will implement listeners registered on the JMAP event bus for WebSocket clients in order to carry over state changes\\nto the client.\\n"}
{"prompt":"## Context\\nThe data synchronisation between front end and back end is broken and is taking\\ntime to fix. In the interim the data can be stored locally within the\\napplication.\\n","completion":"## Decision\nThe data for West Hampshire will be stored locally within the application until\\nsuch a time as the data synchronisation has been fixed. Once fixed, the local\\ndata store will be removed so the centrally held data can be requested and used\\nwithin the application.\\n"}
{"prompt":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nDeveloping a React component to be published as a standalone npm module can pose a few challenges, if it's left as an after thought while using `create-react-app`. Eg identifying the boundaries between the component and the rest of the application.\\n## Decision Drivers <!-- optional -->\\n<!-- * [driver 1, e.g., a force, facing concern, \u2026] -->\\n* A simple and straight forward way to develop a React component\\n* Easy to work and update\\n* Easy to build and package the component\\n* Close to none learning curve for newcomers to the project.\\n* Able to see a preview of the component, in a \"demo environment\"\\n","completion":"## Decision\n<!-- * [driver 1, e.g., a force, facing concern, \u2026] -->\\n* A simple and straight forward way to develop a React component\\n* Easy to work and update\\n* Easy to build and package the component\\n* Close to none learning curve for newcomers to the project.\\n* Able to see a preview of the component, in a \"demo environment\"\\nStorybook and bit, seemed great if you are defining a style guide, and want a tool that gives you a fair bit of functionality to help with that, but they might have a bit of a learning curve, and might put new people off in getting involved if it requires familiarizing with those systems to get up to speed.\\n`create-component-lib` seemed straight forward to use, simple and transparent in its implementation with the accompanying blog post. It's built on top of create-react-app, and that can used to create demo application to showcase the component in action.\\nSo decided to use `create-component-lib` module.\\nAnd updated it to [Create React App 2.0](https:\/\/reactjs.org\/blog\/2018\/10\/01\/create-react-app-v2.html) to be able to use CSS modules.\\nAs well as updated `babel-cli` to transpile the code of the component, see [notes](..\/notes\/2018-10-05-babel-setup.md) for more details on this setup.\\n<!--\\nSetup\\n```\\nnpm i react react-dom react-scripts  @babel\/core @babel\/cli @babel\/preset-react rimraf --save-dev\\n```\\nPackage.json to build component\\n\"build:component\": \"rimraf dist && NODE_ENV=production babel src\/lib --out-dir dist --copy-files --ignore __tests__,spec.js,test.js,__snapshots__\",\\n-->\\n<!-- because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n<!--\\n### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n"}
{"prompt":"## Context\\nWe want to keep our test coverage as high as possible without having to run manual checks as these take time.\\n","completion":"## Decision\nUse the free tier of Coveralls to give us statistics and to give our pull requests feedback.\\n"}
{"prompt":"## Context\\nAs part of the [Ashlar 2018 summer fellowship](https:\/\/github.com\/azavea\/ashlar-2018-fellowship),\\nwe want to build a demo app on top of the Ashlar suite in order to:\\n1. Provide a compelling proof-of-concept for the Ashlar suite\\n2. Create more reference code (in addition to the [reference\\nimplementation](https:\/\/github.com\/azavea\/ashlar-blueprint)) to help\\nfuture users understand how to build on top of Ashlar\\n3. Help guide the development of the Ashlar suite to ensure that we prioritize\\nthe most impactful features\\nAn important component of this development, however, involves selecting\\na frontend framework to use to build the demo app. This particular decision is\\nimportant because I've never built an app from scratch using a frontend framework\\nbefore, so the overhead for development will be high and the work invested\\nwill not be useful unless we continue development with the same framework. In\\nother words, the costs of picking the wrong framework are potentially quite\\nhigh.\\nIn evaluating frameworks, we're looking for one that will be:\\n1. Easy for me to pick up and build a simple app in 2-4 weeks\\n2. Familiar enough to Derek and Kathryn that they can provide code review and\\nguidance on architecting the app\\n3. Exciting for me to learn to use\\nBased on these criteria, I evaluated four frameworks:\\n- [React + Redux](#react--redux)\\n- [Angular](#angular)\\n- [Ember](#ember)\\n- [Vue.js + Vuex](#vuejs--vuex)\\nBelow, I review the pros and cons of these four frameworks.\\n### React + Redux\\n[React](https:\/\/reactjs.org\/) is a JavaScript library for building stateful\\nuser interfaces. React focuses on providing a core library for building\\ncomponents in pure JavaScript (with a markup extension language called JSX),\\nbut it also comes with a large ecosystem of extensions for building full-featured\\nweb apps, including [Redux](https:\/\/redux.js.org\/), a state-management library\\nbuilt by core contributors to the React project.\\n#### Pros\\n- The civic apps team uses React + Redux, and already has [well-established\\ntooling](https:\/\/github.com\/azavea\/civic-apps-app-template) for it.\\n- I've been interested in learning React for some time, and I've read the\\nquickstart guide already, so I'm already familiar with the basics of JSX and\\nthe component model.\\n- React is mostly pure JavaScript, with some markup added in the form of JSX.\\nThis is appealing to me from a developer perspective, since I don't like dealing\\nwith templates and I want more experience writing complex JavaScript.\\n- Although it has the same problems as many hybrid mobile frameworks, React Native\\nis a comparatively well-supported library for porting React apps to mobile,\\nwhich would seem to open up the possibility of potentially creating a native\\nversion of the demo app sometime in the future.\\n#### Cons\\n- React is more of a library than a framework, and is not very opinionated about\\nhow to handle meat and potatoes things like AJAX calls\\n([source](https:\/\/medium.freecodecamp.org\/angular-2-versus-react-there-will-be-blood-66595faafd51#c87b)).\\nThe React + Redux pairing has more established patterns, particularly around\\nmanaging global state in the application, but still leaves many other\\npackaging decisions to the user. This means that it may be harder for me to\\nget a handle on best practices quickly.\\n- Related to the point above, React + Redux will likely have a less clear \"path\" to\\nbuilding the kind of app I want to build, which will incur more time\\nresearching appropriate packages.\\n- Derek and Kathryn have limited experience with React + Redux, and so won't be\\nable to provide as detailed guidance as they might with a framework like\\nAngular.\\n- Although many posts cite React + Redux as being simpler to learn than Angular, the\\nlearning curve seems like it will still be quite steep, since I have to get\\nacquainted with the JSX syntax alongside the conceptual framework for both\\nReact and Redux.\\n### Angular\\n[Angular](https:\/\/angular.io\/) is a web app framework written in TypeScript,\\nwith both JavaScript and TypeScript supported as first-class languages. Angular\\nfocuses on providing a full suite of opinionated tools for building complex,\\nstateful web apps.\\n#### Pros\\n- DRIVER is written in AngularJS (Angular 1.x), which has two important benefits:\\n1. Many of the existing Ashlar components, like the schema editor and\\nthe data collector, are Angular 1.x apps.\\n2. Should DRIVER want to upgrade to Ashlar 2+ during the next round of\\ndevelopment, it would be easy to integrate with any work I do to\\nextend the Ashlar suite using Angular (e.g. [bringing the schema editor\\nup to date](https:\/\/github.com\/azavea\/ashlar-2018-fellowship\/issues\/21)).\\n- Derek and Kathryn are already very familiar with Angular, which would make\\ncode review faster and more effective.\\n#### Cons\\n- As a full-featured framework, Angular has a lot of API-specific syntax\\nand a complicated conceptual architecture that I'll have to learn.\\n- Most of the \"Angular vs. React\" blog posts I was able to find come out in\\nfavor of React, for a number of reasons (see [further\\nreading](#further-reading) for links). This doesn't necessarily sway me, since\\nI don't put much stock in hype, but the opinion was consistent enough that it\\nmade an impression on me.\\n- Although TypeScript is not required to use Angular, all of the Angular docs are\\nwritten in TypeScript, and I would want to learn it\\nfor the sake of learning it anyway. While I'm interested in learning\\nTypeScript, this would further increase the amount\\nof time it would take me to get up to speed writing code for Angular.\\n- Angular purports to support native mobile applications, but I couldn't find\\nany documentation on their website actually proving this. It seems like you\\nhave to use a third-party hybrid app framework like [Ionic](https:\/\/ionicframework.com\/),\\nor even [React Native](https:\/\/github.com\/angular\/react-native-renderer).\\n### Ember\\n[Ember](https:\/\/emberjs.com\/) is a frontend framework for building web apps\\nusing a standard model-view-controller (MVC) architecture.\\n#### Pros\\n- Ember seems to rely on traditional MVC concepts like [routers, models, and\\nviews](https:\/\/guides.emberjs.com\/release\/getting-started\/core-concepts\/).\\nComing from a Django background, I'll be comfortable with these concepts\\nout of the gate.\\n#### Cons\\n- As an MVC framework, Ember represents the smallest departure from concepts\\nthat I'm currently familiar with. This could make development easier, but\\nit also means I'm not as excited to learn it.\\n- Derek and Kathryn don't have any experience with Ember, and so couldn't\\ngive as detailed guidance.\\n- Ember is [highly opinionated](https:\/\/vuejs.org\/v2\/guide\/comparison.html#Ember),\\nwhich could contribute to project overhead and startup costs.\\n### Vue.js + Vuex\\n[Vue.js](https:\/\/vuejs.org\/) is a library and a framework for building\\nweb apps in JavaScript. Its focus is on composable views, providing a core\\nlibrary for this functionality that can optionally scale up to a full framework\\nwith batteries included.\\n[Vuex](https:\/\/vuex.vuejs.org\/) is a state-management\\nlibrary for building Vue.js apps, written by core contributors to Vue.js. Vuex\\nprovides Redux-like patterns for managing global state in stateful Vue.js\\napplications.\\n#### Pros\\n- Vue.js is small, both in size (~30kb zipped) and in setup (can simply\\ninclude it in a `script` tag, or set up more complicated build procedures).\\n- The Vue team considers it a priority to design the library such that it\\n[is incrementally adoptable](https:\/\/vuejs.org\/v2\/guide\/index.html#What-is-Vue-js).\\nCompared to other frameworks, Vue aims to make it easy for the developer to\\nuse only parts of Vue in an app. This seems appealing for potential libraries\\nlike the schema editor and data collector, which we hope to be pluggable into\\na variety of different applications.\\n- Fast learning time: the docs suggest that it takes a developer typically less\\nthan a day to understand the key concepts and start building ([source](https:\/\/vuejs.org\/v2\/guide\/comparison.html#Scale)).\\n- Vue is built on a standard HTML templating system, but also [supports\\nJSX](https:\/\/vuejs.org\/v2\/guide\/comparison.html#HTML-amp-CSS). This pattern\\nwould allow me to get started quickly, but potentially move away from using\\nHTML templating if I have time.\\n- Thorough, well-paced, and fun to read [documentation](https:\/\/vuejs.org\/v2\/guide\/).\\nVue.js was the only framework I considered that offered [detailed\\ncomparisons with other similar frameworks](https:\/\/vuejs.org\/v2\/guide\/comparison.html),\\nincluding comparisons with React, Angular, and Ember.\\n#### Cons\\n- Native rendering is [still\\nexperimental](https:\/\/vuejs.org\/v2\/guide\/comparison.html#Native-Rendering),\\nso a hybrid mobile app in Vue is unlikely.\\n- Although Derek has looked into Vue.js and is working on [one app that uses\\nit](https:\/\/github.com\/azavea\/cartwheel), no one on the team has extensive Vue.js\\nexperience, so couldn't provide as detailed guidance.\\n- Similar to React + Redux, a less-opinionated framework means that it'll be more\\ndifficult for me to get a handle on best practices.\\n- Of the four frameworks I considered, Vue.js is the newest, and although it\\nappears to be growing quickly its community is comparatively small.\\n- For context, the StackOverflow tags for the four frameworks show:\\n- Angular: ~120k questions\\n- React: ~90k questions\\n- Ember: ~22k questions\\n- Vue: ~20k questions (~9k for Vue 2.0, released September 2016)\\n","completion":"## Decision\nBased on my review above, I recommend moving forward with **Vue.js + Vuex**. To me,\\nVue seems to strike the right balance between being A) a new and exciting\\nparadigm to learn and B) not requiring huge startup\/investment costs. Even\\nthough Derek and Kathryn don't have extensive experience with Vue, my\\nunderstanding is that Vue's focus on a simple set of core functions and low\\ninitial investment in learning means that Kathryn and Derek could still give\\nfeedback without having much experience, or could even familiarize themselves\\nwith the core concepts quickly enough to understand what I'm doing in detail.\\nIn addition, the fact that the Vue team prioritizes incremental adoption seems\\nideal in this case, given that there's high uncertainty around what the Ashlar suite\\nwill look like in the medium- to long-term. Of the four frameworks, Vue seems\\nto provide the lowest initial investment overhead.\\n"}
{"prompt":"## Context\\nRight now, we can query the present validator set, but there is no history.\\nIf you were offline for a long time, there is no way to reconstruct past validators. This is needed for the light client and we agreed needs enhancement of the API.\\n","completion":"## Decision\nFor every block, store a new structure that contains either the latest validator set,\\nor the height of the last block for which the validator set changed. Note this is not\\nthe height of the block which returned the validator set change itself, but the next block,\\nie. the first block it comes into effect for.\\nStoring the validators will be handled by the `state` package.\\nAt some point in the future, we may consider more efficient storage in the case where the validators\\nare updated frequently - for instance by only saving the diffs, rather than the whole set.\\nAn alternative approach suggested keeping the validator set, or diffs of it, in a merkle IAVL tree.\\nWhile it might afford cheaper proofs that a validator set has not changed, it would be more complex,\\nand likely less efficient.\\n"}
{"prompt":"## Context\\nBy live application I mean that the APIs are required for the lifetime of the application and by static, the APIs are only required once - at build time or for storage.\\nBuilding a data-driven application for a portfolio has the risk that the data will become stale or inaccessible - for example if the API keys expire, the API changes or the service is no longer available. This can break the consuming application. In maintaining the Korin app, I have experienced Last.fm and IBM Watson APIs breaking due to API changes or expired API keys.\\n","completion":"## Decision\nFacing the concern that data-driven applications will break due to maintenance issues, I have decided to build them as static apps i.e. process or store the data once and present the results.\\n"}
{"prompt":"## Problem\\nInconsistent use of booleans in various parts of Elektra.\\n","completion":"## Decision\nOnly the strings `0` and `1` are allowed in the `KeySet` for `type = boolean`, for both values and defaults.\\nEverything else should lead to errors in checkers (in `kdbSet`).\\nA spec with `type = boolean` without a specified default should be interpreted as `default = 0`.\\nExample for an implementation in C in an application:\\n```c\\nif (k != NULL && strcmp(keyString(k), \"1\") == 0) {\/*true*\/} else {\/*false*\/}\\n```\\nStorage plugins are allowed any representation as suitable, e.g., a JSON plugin might render `1` as `true`.\\nThe type checker plugin should allow\\n- non-presence\\n- the string \"0\"\\n- the string \"1\"\\n"}
{"prompt":"## Context\\nOpenBounty is a system which has value flowing through it.\\nNaturally security is a concern that should be taken into consideration.\\nCurrently an attacker might get access to an account of a team member\\nand pose as that developer, merging PRs and pushing changes.\\nStatus.im as a company is also encouraging the use of GPG signing and\\nhas a Pull Request check in place on Github. This check will mark PRs\\nas failing if the commits come from an organization member and have not\\nbeen GPG-signed.\\n","completion":"## Decision\nIn order to verify that commits in the repository are actually authored by the specified\\nauthor we adopt [GPG signing of Git commits](https:\/\/git-scm.com\/book\/id\/v2\/Git-Tools-Signing-Your-Work).\\nThis will allow us to verify authenticity of the author information saved in\\na Git commit and make workflows like deploying on push safer.\\nIt also introduces some complexity because contributors who want to sign\\ntheir commits need to set up the appropriate tooling. Due to that we will\\nnot require outside contributors to sign their commits for now.\\nAdopting GPG signing for contributors will also make our PR checks pass\\nallowing us to more easily discern actually broken and working PRs.\\n"}
{"prompt":"## Context\\nThe visualization features of the project started as a spike, and the code is not very clear. We decided that a templates engine will solve some of the code problems and will make easier to create new visualizations in the future.\\n","completion":"## Decision\nThe recommended options from the team were, React, Lit-HTML and Nunjucks. React looks like an excellent option, but had some problems with the current templates, so Nunjucks replaced it as the best option at this moment.\\n"}
{"prompt":"## Context\\nI'm unsure about this one.\\nOur lockfile situation is a little messy.\\nRecently our TerriaMap CI builds have been failing, probably due to a recent change via travis using `yarn --frozen-lockfile` & failing when yarn's equivalent package-lock.json (yarn.lock) is out of sync (https:\/\/github.com\/travis-ci\/travis-build\/pull\/1858).\\nRemoving yarn.lock fixes CI as travis currently thinks it's a yarn-developed project.\\nI don't fully understand why we have yarn.lock commited in the first place, however we DO recommend & use yarn when developing using the workspace feature.\\nOther things to note for this proposal:\\n- greenkeeper is set up to update package-lock.json and not yarn.lock\\n- we always cite npm in docs\\n- i've always used npm when doing release builds\\n- our other projects using things like github actions (NSW DT) use `npm ci` and not yarn\\nSome snippets from slack a little later:\\n(wing)\\n>maybe a simple way forward to unstuck us for now is force travis to use npm, continue to commit yarn lock (because lock files should be committed at least on the terriamap level), and revise again if needed.\\n(emma)\\n>my vote is for yarn and not npm because we use stuff like yarn workspaces and also yarn lets you do stuff like specific version relsoutions\\n(kevin)\\n>so I used to always update both yarn.lock and package-lock.json when updating TerriaMap to use a new terriajs\\nthat way our users can use the tool they prefer\\n>if you just .gitignore yarn.lock, it sorta just hides problems.. cause people who use yarn end up generating one, and then their system uses from that then on without them realizing it\\n(wing)\\n>yeah it's seeming more that we continue to use both, but we should update our release process to reflect the context & decisions made - the primary reason we are using yarn in dev is for workspaces right? i know for me that's why i use yarn in dev. but npm is the default tool that ships with node. so given that, we continue to ship with npm, and we dev however we like internally with yarn?\\n(crispy stephen)\\n>We used to use npm link, and then npm-git-dev when link failed us. Yarn is just another tool on top of npm we\u2019re using to make dev easier. Unfortunately it comes with it\u2019s own lockfiles and opinions, but it\u2019s still way better than any other way we can dev with npm at the moment\\n","completion":"## Decision\nKeep both, but force travis to use npm, npm ci, package-lock.json etc(?), at the cost of having to keep yarn.lock up to date as well.\\nContinue building production\/\"gulp release\" builds of TerriaMap using package-lock.json, keeping yarn.lock in source as well.\\n"}
{"prompt":"## Context\\nWe need the ability to identify which keys are used to sign and encrypt messages so that we can support multiple keys\\n","completion":"## Decision\nWe will use the kid value in the header of the JWE and JWT tokens to identify the key that was used to sign or encrypt the message payload.\\nThe kid value will be a SHA1 hash of the digest of the Public Key\\n"}
{"prompt":"## Context\\nHistorically, our automated deployment scripts have tended to include a large amount of common boilerplate. This is partly because they are self-contained\u2014they can typically be run on their own to perform a deployment. And it is partly just because they originate from what the Visual Studio tooling creates.\\nThe problem with this is that it makes the project-specific details hard to spot. When looking at a sea of code that's almost identical to every other project, it's hard to see what it's doing that is in any way different from everything else.\\n","completion":"## Decision\nWith `Marain.Instance`, deployment scripts in individual services do not communicate directly with either ARM or Azure AD. (They should not even be aware of what mechanisms are being used to perform this work\u2014they should not need to know whether we are using the PowerShell Az module, the az CLI or even custom library code to talk to Azure, for example.)\\nAnything that needs to be done either in Azure or AAD must be done through operations provided by the shared `Marain.Instance` code. It passes in an object that provides various methods that provide the necessary services.\\n"}
{"prompt":"## Context\\nOne of the main goals I'm interested on is in to provide a 12factor app\\narchitecture to the solution.\\netcd is a daemon that runs using:\\na. environment variables or arguments in command line\\nb. config-file\\nAll are valid for a 12factor app, but which is faster\/better to implement?\\nI have read most of etcd invocations use command line arguments, but that's not\\ngoing to work fine with Docker.\\nI can create a config file in a volume and map it from the docker container in\\nruntime.  That is how it should work in kubernetes using config maps.\\nBut for something more 12factor alike, like in heroku, I would rather prefer to\\nuse ENV VARS.  Etcd has a good support for them.\\n","completion":"## Decision\nI'm going to try first with a config file, and then I will try to inject env\\nvars through a docker env-file option.\\nAs I want to create a three node cluster, I would need three env_files... Umm.\\n"}
{"prompt":"## Context\\nOriginally, we supported 3rd party status extensions in form of javascript jailed execution environment (eq Jail) + small js library exposing chat commands API to js code.\\nWhile neat in theory, it has some serious downsides:\\n- Design of the API was quite poor, preferring mutable semantics over stateless API leveraging strength of the host application (cljs\/re-frame).\\nAs a result of that, more dynamic\/state requiring things (like the live tx detail in `\/send` command messages) were very hard to do,\\nso instead of \"eating our own dogfood\", we decided to side-step the API and implement such things as hard-coded logic in the app, while partly\\nretaining the js code for \"easier\" things (like parameter declaration).\\nNeedles to say, such efforts produced code of very poor quality, riddling our app with hard-coded \"magic\" everywhere in the codebase, completly\\ndefeating the point of \"dogfooding\" while still requiring more effort and being much more error prone (no way to unit test jail logic) because\\nof the need to asynchronously communicate with jail for leftover logic in command messages (the parts not hardcoded in app).\\n- We were in a state where there was not even one command defined completely in jail, with all of them requiring custom hardcoded logic in app to\\nactually work.\\nDue to numerous changes and rush to get things working, half of the API was obsolete and was not working\/working differently then described in\\nthe API documentation.\\n- We are quite tight stretched regarding performance and the Jail didn't particularly help with that - whenever some command message was defined in\\nJail, it required constant RN Bridge \"ping-pong\" whenever sending the message or receiving the message, eating more resources + breaking otherwise\\nquite simple logic into harder to reason asynchronous call chains (execute method defined in Jail, wait for the result, execute next method in Jail...).\\n- Till now, there was no real interest in 3rd party bots anyway - the only \"real\" DAPPs submitted to status were \"normal\" DAPPs utilising just the `web3.js`\\nAPI and built on top of regular web technologies, so they could be ran in any Ethereum client like Metamask or Mist, besides Status.\\n- There is a very promising new concept of extensions (project pluto), which will enable to extend status with much nicer and declarative extension\\ndefinitions.\\nSuch extensions will not be able to only hook-in into the command messages logic, but also other application (host) hooks, like wallet assets\/collectibles,\\nchat contacts, etc.\\n","completion":"## Decision\nIn the light of points above, we decided to remove Jail and `Status.js` API from the application and re-work command messages in set of nicely\\nencapsulated app level protocols, so command related logic will be defined in one place, commands will be able to leverage strength of our platform\\nand development of any new functionality related to them will be much faster and more pleasant.\\nWe will address the 3rd party code issue separately by extensions.\\n"}
{"prompt":"## Context\\nThe ADR flow tool is a command line tool for managing ADR files. It is aimed to be used in command line, in MS-Windows primarily, but also other platforms.\\nAs a guiding principle, and in order to preserve simplicity, we don't want to introduce a separate data store. Data is stored and managed in files. This includes configuration, which should be explicit and easily identifiable and editable.\\nGiven this, in [ADR 1](1-ADR_directory_identified_by_marker_file.md), we decided that the ADR directory is identified by a file called `.adr`. This file will also contain configuration of the tool. This is to reduce the number of moving parts; so the marker file (`.adr`) is used also as a configuration file.\\nPart of the value statement is for these files to be available in source control, and easily shared and collaborated on by a team. This means the marker file needs to be version-controlled as well.\\nAt the same time, the configuration may change on a per-developer basis. For example, the location of the editor used to edit ADRs.\\n### Alternative 1: Introduce a Separate Configuration File\\nThis means that we, by definition, and always will have a separate configuraton file, on top of an empty marker file.\\nThis makes sharing configuration slightly harder, and makes it hard to share some of the configuration and customize others.\\n### Alternative 2: Use Environment Variables\\nWith this approach we configure the system using environment variables.\\nThis is OS-dependenet, and makes the configuration somewhat less explicit. It's also harder to share across a team where necessary.\\n","completion":"## Decision\nWe will keep the `.adr` file, with its current dual role - a marker file, and a *shared* configuration file.\\nIn addition, we'll add the option to supply a `local.adr` file, with the same format as the `.adr` file, and allow a developer to specify properties to override there.\\nProperties that are specified only in `.adr` will have their values taken from there.\\n"}
{"prompt":"## Context\\n### Initial Snapshot Download\\nWhen spawning a new node it downloads the blockchain and verifies it, before it starts\\nproperly participating in the network. This is necessary to identify double spends and\\nwhat unspent transaction outputs there are which can be spent.\\nThis takes a considerable amount of bandwidth, time to sync, and computing power to\\nverify. In order to have nodes join the network more quickly a fast sync mechanism would\\nbe beneficial.\\nBitcoin already supports pruning mode in which it downloads only part of the blockchain\\nand iterates through the blockchain in a sliding window manner, such that at any given\\ntime it does not require more then a certain amount if disk space. It still downloads the\\nfull blockchain and scans through it, thus it only saves disk space, not bandwidth or\\ncomputing power.\\nIn order to determine what balance a wallet has only the _unspent transaction outputs_\\n(UTXOs) are needed (that is the spendable money in the system). The idea behind the\\n_initial snapshot download_ (ISD) is to simply download the set of UTXOs only instead\\nof the whole blockchain. The UTXO set can be used to determine the spendable funds\\nand a node can start staking (proposing or validating right away).\\n### Requirements\\n- Calculating and providing a snapshot should be an optional feature, that is it should\\nbe possible to spawn a node with ISD support activated or deactivated. P2P nodes use\\nversion bits for feature negotiation.\\n- Ideally no additional consesus rules should be neeed to be added to the system.\\n- The liveness of the system should not be affected by the fast sync \/ ISD mechanism,\\nthat is: Even if a majority of the nodes do not activate ISD the network should still\\nfunction.\\n### Security concerns\\nA node downloading a snapshot can not know whether this snapshot is legitimate or not.\\nThus the network needs to provide a simple way of verifying a downloaded snapshot (for\\nexample a hash of a snapshot at given block height). The downloaded snapshot could then\\nbe verified by computing its hash and comparing it to the known hash value from the\\nnetwork. This value unfortunately is another critical value just like the block hash\\nof the tip of the active chain, thus posing another consensus aspect.\\nThe problem is complicated by the fact that the feature is optional. Thus not every node\\ncan deliver this information. Malicious nodes could take over, the less nodes having ISD\\nactivated, the easier; providing false snapshots to newly spawned ISD nodes.\\n","completion":"## Decision\nThe incremental utxo set's hash is included in every block's coinbase transaction\\nand can be used for verifying downloaded snapshots. The hash can be updated incrementally\\nby adding the created and deleting the spent transactions, using ECMH. This computation\\nis cheap enough (and has further benefits) that it can be done by all nodes. Also it\\ncan be done by looking at the previous block without having the full UTXO set.\\n"}
{"prompt":"## Context and Problem Statement\\nDecision required on the framework to be used to scrape and store data from the Koshvani platform in machine readable format.\\n","completion":"## Decision\n- Structure of the Koshvani platform\\n- Platfrom links do not reflect selection criteria\\n- Automation job requirements for data scraping\\n- Periodical jobs and access to new data\\nUse [Selenium](https:\/\/www.selenium.dev\/) to create robust, browser-based automation.\\n"}
{"prompt":"# Context\\nWe need consistency in terms of format and structure for our patterns across the customer facing, integration and other architectures.\\nWe are also keen to link back to business strategy directly so we can isolate points for consideration in design, implementsation and assessment.\\n# Decision\\nWe propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n","completion":"## Decision\nWe propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n"}
{"prompt":"## Context\\nThe OpenFIDO project will mediate access to a workflow service that runs\\nGridLabD jobs. The frontend React application will need access to these\\nresources, but only if they have permission to access via an organization.\\n","completion":"## Decision\nCreate a Flask Rest service - since the other microservices in this suite of\\nservices are also Flask based, keep this using the same kind of infrastructure\\nso that common utilities can be shared (openfido-utils) and the maintenance will\\nbe simplified.\\nOrganize the database logic into a simplified CQRS-inspired style code\\nstructure. Since we anticipate many conceptual resources, each resource will\\nhave its own module:\\n* app\/RESOURCE\/models.py - contains all models for RESOURCE.\\n* app\/RESOURCE\/routes.py - contains all Rest routes specific to RESOURCE.\\n* app\/RESOURCE\/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\\n* app\/RESOURCE\/services.py - all db commands that modify database state.\\n* app\/RESOURCE\/queries.py - all db queries to the database.\\nAdditional libraries we anticipate using:\\n* [marshmallow](https:\/\/marshmallow.readthedocs.io\/en\/stable\/) will be used since there are many rest api endpoints that will take nested bodies.\\n* [alembic](https:\/\/alembic.sqlalchemy.org\/en\/latest\/) to manage database schema migrations.\\nOther thoughts on design:\\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\\nUse the structure of the [openfido-workflow-service](https:\/\/github.com\/slacgismo\/openfido-workflow-service) project as a reference.\\n"}
{"prompt":"## Context and Problem Statement\\nThe Operate First Cloud environment spans multiple data centers and multiple regions.\\nA central authentication system helps identifying users and system operators without maintaining multiple isolated user registries.\\nThis ADR builds upon and extends [0010-common-auth-for-applications](0010-common-auth-for-applications.md) and [0017-authentication-for-platform](0017-authentication-for-platform.md)\\nand decides on technical aspects and implementation.\\n## Decision Drivers\\nPrevious ADRs decided input conditions such as:\\n* GitHub is the ultimate identity provider.\\n* Applications running within the platform should be able to leverage common authentication as a client.\\nThis ADR answers on how to achieve following:\\n1. Fully declarative - new clusters can be integrated with SSO via a PR.\\n2. Provide seamless migration and transition for users.\\n3. Centralized and single SSO instance.\\n","completion":"## Decision\nPrevious ADRs decided input conditions such as:\\n* GitHub is the ultimate identity provider.\\n* Applications running within the platform should be able to leverage common authentication as a client.\\nThis ADR answers on how to achieve following:\\n1. Fully declarative - new clusters can be integrated with SSO via a PR.\\n2. Provide seamless migration and transition for users.\\n3. Centralized and single SSO instance.\\nChosen option: _\"5. Authenticate OpenShift against Keycloak and provide app authentication via Dex against OpenShift\"_, because it's a fully declarative solution that is fully supported and available today.\\nThis ADR should be revisited after Keycloak becomes capable of importing\/providing OpenShift Groups data ([Keycloak RFE 18532][1]). Then option _3. Authenticate OpenShift and apps against Keycloak_ should be preferred.\\n### Authenticating Openshift clusters\\nEach cluster will connect to a centralized Keycloak instance. This centralized Keycloak is coupled with a GitHub application which facilitates authentication:\\n![](https:\/\/chart.googleapis.com\/chart?cht=gv&chl=digraph{\"Openshift+cluster+0\"->Keycloak[type=s];\"Openshift+cluster+1\"->Keycloak[type=s];\"Openshift+cluster+N\"->Keycloak[type=s];Keycloak->Github[type=s]})\\n### Authenticating application within Operate First\\nThis doesn't change and remains the same as described in [ADR 0010](0010-common-auth-for-applications.md).\\n### Positive Consequences\\n* When adding a new clusters, all SSO configuration happens via Git\\n* When adding a new application, all SSO configuration happens via Git\\n* Single, centralized Keycloak instance available to all clusters\\n### Negative Consequences\\n* Per cluster Dex deployment to allow application authentication due to the fact that Keycloack can't import OpenShift groups.\\n"}
{"prompt":"## Context\\nSince this app doesn't rely heavily on React libraries or things like suspense, switching to [`preact`](https:\/\/preactjs.com\/) offers a significantly smaller bundle size (~70kb => ~40kb).\\nDue to preact's compatability package very little should change in day-to-day development.\\n","completion":"## Decision\nUse `preact` instead of `react`.\\n"}
{"prompt":"## Context\\nOriginally, PrestaShop was made out mainly of static classes, with no dependency injection. To address that problem, it [was decided](https:\/\/build.prestashop.com\/news\/new-architecture-1-6-1-0\/) that non namespaced code would be progressively refactored into a `Core` namespace, which would only contain code using with dependency injection. Furthermore, Core code wouldn't be allowed to depend directly on non namespaced classes, but it could to it indirectly by the means of `Adapter` classes that would act as a bridge between new and old code.\\nThe \"no direct dependency between Core and Legacy\" rule led to an ever-growing collection of adapters, which resulted in greatly increased code complexity and duplication. In some cases, the same service can have a legacy, adapter and core implementations, with subtle differences between each one. Furthermore, the constraints of backward compatibility further increase the difficulties to refactor code into Core, because the surface of the \"public API\" is larger.\\n","completion":"## Decision\nThe following decision applies to both `Core` and `PrestaShopBundle` classes (referred as to \"Core\" for shortness):\\n1. **All new Core classes SHOULD be placed either in the `Core` or the `PrestaShopBundle` namespace**, following on the rules established previously.\\n- New classes MUST NOT be added to the `Adapter` namespace, and SHOULD NOT be added to the legacy (root) namespace.\\n2. **Core classes MAY depend on instances of legacy classes**, provided the following rules are respected:\\n- Legacy classes MAY be used either as injected parameters or constructed within, but caution must be exerted when using legacy classes that produce side effects, have global state or don't guarantee internal consistency. In those cases, these classes SHOULD be accessed through dedicated services which enforce consistency.\\n- Core classes MUST NOT call static methods on other classes, except for factory methods, stateless tool methods, or within services dedicated to encapsulate a static class.\\n- Core classes MAY access to data provided by static classes or methods static classes by relying on dedicated services (Application services, Repositories, Data Providers...).\\n3. **Core classes MUST NOT reimplement code found in legacy classes**, without deprecating the original method\/class (and optionally, making it rely on the new implementation).\\n4. **The Adapter namespace MUST be phased out** eventually:\\n- Classes in the Adapter namespace MUST be copied to the Core namespace.\\n- The original Adapter classes MUST be emptied out, made to extend the Core classes, and deprecated so that they can be fully removed in a following major.\\n- Adapter services MUST be deprecated and copied into the core namespace as well.\\n- Code MUST NOT depend on Adapter classes or services.\\n"}
{"prompt":"## Context\\nThis application is largely about supporting authentication protocols. To achieve a quick time to market, use of a third party library is desirable. Initially the application will support the OpenID Connect and SAML 2.0 authentication protocols, but it is quite possible that other protocols may be desired in the future.\\n","completion":"## Decision\nA (JavaScript) library that offers a plethora of authentication protocol implementations is named passport.js, licensed under a permissive license. While this library does not implement the particular protocols itself, it facilitates and integrates with Node.js in a way that makes it very easy for developers to use. There are currently over 400 Passport modules available, including OIDC and SAML.\\nAs such, this application will leverage the Passport library and the OIDC and SAML modules.\\n"}
{"prompt":"## Context\\nCCS is developing many new services over the coming months, each of which will be producing new application source code.\\nThese services will be developed by several independent teams including people from different supplier organisations.\\nThere is a risk that each workstream operates in a different way using different standards which could lead to confusion and complexity in the future.\\n#### Version control\\nIt is standard industry practice that all source code should be stored somewhere that allows it to be properly versioned and controlled.\\nThere are several options for software version control, including for example SVN, Git, Mercurial and Team Foundation Server (TFS) etc.\\nCCS already uses [Git](https:\/\/en.wikipedia.org\/wiki\/Git#Adoption) for the Data Submission Service and Salesforce. Git is also widely used across government, and the industry as a whole (for example Microsoft is migrating Windows development to use Git).\\nGit is a distributed version control system, which means that a full copy of the repository is stored locally on the developer\u2019s device. It is standard practice for a copy of a Git repository to be stored in a central place so that code can be shared between developers.\\nCCS uses GitHub as this central place for Salesforce and Data Submission Service, while DigiTS uses BitBucket. GitHub is widely used across government including for [all GDS services](https:\/\/github.com\/alphagov). Other services also exist including BitBucket, GitLab, AWS CodeCommit and Google CloudSource.\\n### Trunk-based development\\n[Trunk-based development](https:\/\/trunkbaseddevelopment.com\/) is a source code management methodology which is aimed at making it easier for multiple people to make changes to code at the same time without breaking each other\u2019s work.\\nWith many teams working on common interfaces, there is a risk that development activities will work to different versions, which can cause integration issues later on. Trunk based development reduces the risk of this occurring.\\n### Continuous integration (CI)\\n[Continuous integration](https:\/\/en.wikipedia.org\/wiki\/Continuous_integration) (CI) is a software development practice that involves regularly and automatically running unit and integration tests on code as it is being developed. The aim of this practice is to reduce potential integration issues later in development.\\nThe adoption of CI can also reduce the time-to-deployment by introducing automated testing and standardised builds. This is important part of being able to [deploy software regularly](https:\/\/www.gov.uk\/service-manual\/technology\/deploying-software-regularly) as recommended in the Service Manual.\\n","completion":"## Decision\nAll new services will use Git for version control and GitHub as a central code repository. We will use public repositories by default, unless there is a convincing reason not to (which must approved by TDDA and then be documented in the README of the private repository).\\nAll new services will adopt Trunk-based development and Continuous Integration practices. Any branches should be short-lived feature branches which last for no more than a few days before being merged into the trunk. Feature switches should be deployed where necessary.\\nServices should use a common continuous integration tool and build monitor, with a common set of build phases. A decision on this common tool and common build phases isn\u2019t made in this ADR, but will be subject to a future ADR.\\nThe decision to use GitHub as the central code repository should be reviewed in 6 months to determine if a better alternative is available.\\nCCS-owned source code should be migrated to use Git and GitHub.\\nWe should develop an approach to evaluate the impacts of migrating existing services to use Trunk-based development and Continuous Integration practices.\\n"}
{"prompt":"Context\\n-------\\nWe can develop event transformers either using the \u201cBackend\u201d architecture\\nor \u201cProcessor\u201d architecture. Making the transformers \u201cbackends\u201d will result\\nin relatively more nesting in the configurations as this \u201cbackend\u201d will have\\nits own configurations.\\nIf we decide to develop the event transformers as \u201cprocessors\u201d, it will result\\nin less complexity in the code since the transformer can be easily appended in\\nany backend\u2019s (router or logger) processors\u2019 pipeline.\\nDecision\\n--------\\nTransformers will be developed as event processors that can be added in\\nany backend\u2019s pipeline. Then the transformed events can be used for any purpose,\\neither for simply logging using the LoggerBackend or to route events using\\nEventRoutingBackend.\\nConsequences\\n------------\\nDeveloping transformers as processors will result in relatively less complex\\nconfigurations and it would provide us wider range of use cases for the transformers.\\n","completion":"## Decision\n--------\\nTransformers will be developed as event processors that can be added in\\nany backend\u2019s pipeline. Then the transformed events can be used for any purpose,\\neither for simply logging using the LoggerBackend or to route events using\\nEventRoutingBackend.\\nConsequences\\n------------\\nDeveloping transformers as processors will result in relatively less complex\\nconfigurations and it would provide us wider range of use cases for the transformers.\\n"}
{"prompt":"## Context\\n0. \u5ba2\u6237\u7f51\u7edc\u73af\u5883\u4e0d\u4f73\uff0c\u5b58\u5728\u6d41\u91cf\u52ab\u6301\uff1b\u4f7f\u7528\u7684\u7b2c\u4e09\u65b9\u670d\u52a1\u8981\u6c42 https\uff1b\\n1. 10+ \u4e3b\u57df\u540d\uff0c50+ \u4e8c\u7ea7\u57df\u540d\uff1b\\n2. 190+ \u9879\u76ee\uff1b\\n3. 10+ \u6570\u636e\u5e93\u8868\uff1b\\n4. \u5927\u91cf\u81ea\u884c\u6258\u7ba1\u53ca\u7b2c\u4e09\u65b9\u8d44\u6e90\u6587\u4ef6\u3002\\n","completion":"## Decision\n### why\\n1. \u9632\u6b62\u6d41\u91cf\u52ab\u6301\uff0c\u63d2\u5165\u5e7f\u544a\uff1b\\n2. \u9632\u6b62\u8d26\u53f7\u5bc6\u7801\u7b49\u9690\u79c1\u6570\u636e\u88ab\u76d7\u53d6\uff1b\\n3. \u652f\u6301 HTML5 API\uff0c\u5982\u7528\u6237\u5730\u7406\u4f4d\u7f6e\uff0c\u97f3\u89c6\u9891\u7b49\u9690\u79c1\u6570\u636e\u83b7\u53d6\uff1b\\n4. \u652f\u6301 HTTP\/2\uff1b\\n5. Apple\uff0c\u5fae\u4fe1\u7b49\u6709\u8981\u6c42\u3002\\n### How\\n1. \u4e86\u89e3\u6574\u4e2a\u7cfb\u7edf\uff0c\u7edf\u8ba1\u4f7f\u7528\u5230\u7684\u57df\u540d\u53ca\u8d2d\u4e70\u4ec0\u4e48\u7c7b\u578b\u7684\u57df\u540d\u8bc1\u4e66\uff1b\\n2. \u76f8\u5173\u8d44\u6e90\u652f\u6301 https\uff0c\u89e3\u51b3 Mixed Content \u95ee\u9898\\n1. \u9488\u5bf9\u6b64\u95ee\u9898\uff0c\u6d4f\u89c8\u5668\u4f1a\u63d0\u793a\u8b66\u544a\uff0cAndroid \u7684 webview \u76f4\u63a5\u65e0\u6cd5\u6253\u5f00\uff1b\\n2. \u524d\u7aef\u9875\u9762\u7684\u5916\u94fe\u8d44\u6e90\uff08CSS\u3001JS\u3001\u56fe\u7247\u3001\u97f3\u9891\u3001\u5b57\u4f53\u6587\u4ef6\u3001\u5f02\u6b65\u63a5\u53e3\u3001\u8868\u5355 action \u5730\u5740\u7b49\u7b49\uff09\u56fa\u5b9a\u4e86\u534f\u8bae\u5f15\u7528(http:\/\/, https:\/\/)\uff0c\u9700\u66f4\u65b0\u4e3a(\/\/)\uff1b\\n3. \u540e\u7aef\u4ee3\u7801\u4e2d\u534f\u8bae\u662f\u5426\u4e3a\u52a8\u6001\u7684\uff1b\\n1. \u8fd4\u56de\u63a5\u53e3\u534f\u8bae\u5e94\u548c\u8bf7\u6c42\u534f\u8bae\u4fdd\u6301\u4e00\u81f4\uff1b\\n4. \u6570\u636e\u5e93\uff1b\\n1. \u9488\u5bf9\u81ea\u6709\u8d44\u6e90\uff0c\u5e94\u53ea\u4fdd\u5b58\u8def\u5f84\u4fe1\u606f\uff0c\u534f\u8bae\u548c\u57df\u540d\u5efa\u8bae\u52a8\u6001\u8865\u5145\uff1b\\n2. \u9488\u5bf9\u7b2c\u4e09\u65b9\u8d44\u6e90\uff0c\u9ed8\u8ba4\u4fdd\u7559\u539f\u5730\u5740\uff0c\u8ddf\u8fdb\u9700\u8981\u518d\u505a\u8f6c\u6362\uff0c\u5b9e\u5728\u4e0d\u884c\u9700\u8981\u505a\u4ee3\u7406\u3002\\n5. \u8d44\u6e90\u6587\u4ef6\uff1a\u786e\u4fdd\u652f\u6301 https\u3002\\n3. \u652f\u6301 https\uff1b\\n1. \u79fb\u52a8\u7aef\u9002\u914d https\\n1. \u9488\u5bf9\u8fd0\u8425\u5546 DNS \u52ab\u6301(\u964d\u4f4e https \u8bf7\u6c42\u6210\u529f\u7387)\uff0c\u9700\u652f\u6301\u4e24\u79cd\u534f\u8bae\uff0c\u5e76\u6709\u52a8\u6001\u964d\u7ea7\u65b9\u6848\uff1b\\n2. nginx proxy + backend server\\n1. \u9700\u5173\u6ce8 scheme \u83b7\u53d6\u662f\u5426\u6b63\u786e\uff0c\u6ce8\u610f log \u8bb0\u5f55\u3002\\n3. \u8df3\u8f6c\\n1. \u5148 302 \u6d4b\u8bd5 https \u5168\u7ad9\u6b63\u5e38\uff0c\u518d 301 \u8df3\u8f6c\uff1b\\n2. POST \u8bf7\u6c42\u4f1a\u4e22\u5931 body \u4fe1\u606f\u3002\\n4. \u6d4b\u8bd5\\n1. [https:\/\/www.ssllabs.com\/ssltest\/][1]\\n4. \u5f3a\u5236 https\uff1b\\n5. \u6240\u6709\u73af\u5883\u5747\u8981\u5347\u7ea7 https\uff1b\\n1. \u9664\u751f\u4ea7\u5916\uff0c\u9700\u8981\u5c06\u5f00\u53d1\u3001\u6d4b\u8bd5\u3001\u9884\u53d1\u5e03\u5747\u8fdb\u884c\u5347\u7ea7\uff0c\u4fdd\u6301\u73af\u5883\u7684\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u4e0d\u53ef\u9884\u4f30\u7684\u95ee\u9898\u53d1\u751f\\n6. \u4f18\u5316\u3002\\n"}
{"prompt":"## Context and Problem Statement <!-- required -->\\nTo advance SDR evolution towards decoupling from Fedora, we should store workflow state outside of Fedora (in the workflow service's database).\\n","completion":"## Decision\nRemove the datastream.\\nThis was done in dor-services v9.0.0 ([commit](https:\/\/github.com\/sul-dlss\/dor-services\/commit\/8745e7c2e86edbbaa7577af85779c4ea06258dd3)).\\n"}
{"prompt":"## Context and Problem Statement\\nSome tool windows like the status bar and the side bar menu need special placement in the UI and therefore special treatment by the window manager. The window manager must be able to identify them to treat them accordingly.\\n","completion":"## Decision\nChosen option: \"Client Properties \/ Atoms\", because it is the most reliable and side-effect free way to identify the tool windows.\\n"}
{"prompt":"## Context\\nCoding conventions are a set of guidelines for a specific programming language that recommend programming style, practices, and methods for each aspect of a program written in that language. These conventions usually cover file organization, indentation, comments, declarations, statements, white space, naming conventions, programming practices, programming principles, programming rules of thumb, architectural best practices, etc. These are guidelines for software structural quality. Software programmers are highly recommended to follow these guidelines to help improve the readability of their source code and make software maintenance easier.\\nCoding guidelines result in enhanced efficiency, reduced risk, mininized complexity, maintainability, bug rectification, comprehensive look and cost efficiency.\\n","completion":"## Decision\nWe will use coding guidelines for all languages used on the platform.\\n"}
{"prompt":"## Context\\nThere are no formal standards for describing or managing most types of cloud resources. This is because the cloud hosting market moves so rapidly. Organisations are very quick to adopt new cloud products that offer significant opportunities, but this reduces the opportunities to work with standards.\\nFor this reason, when choosing to develop a product on top of a non-standard cloud component there's a risk of lock-in.\\n","completion":"## Decision\nWe should take steps to avoid lock-in with cloud hosting providers.\\n"}
{"prompt":"## Context and Problem Statement\\nA repository for each microservice or only one for all of them?\\n## Decision Drivers\\n* Development agility\\n* Microservices decoupling\\n* Big-picture scenario visibility\\n","completion":"## Decision\n* Development agility\\n* Microservices decoupling\\n* Big-picture scenario visibility\\nChosen option: A single repository for all microservices and service definitions this time, because this way its easy to get a big picture of the approach, for the purposes of this example.\\nIn production, I'll consider separating microservices and service definitions on repos, in order to have separate triggers on CI for them.\\nI think [this approach](https:\/\/medium.com\/namely-labs\/how-we-build-grpc-services-at-namely-52a3ae9e7c35) will be a good fit for address this concern in production.\\n"}
{"prompt":"## Context\\nThe recommended way of managing application config needed at runtime on the\\ncloud platform is to use Kubernetes secrets. These can be created manually\\nusing kubectl on the command line to apply config files, or as part of a\\ndeployment process.\\nWe want to manage as much of our application config as possible in\\nversion-controlled code, so that we can see when config was changed and so\\nthat we can easily reproduce changes and rebuild environments.\\nThe team are used to using [git-crypt](https:\/\/www.agwa.name\/projects\/git-crypt\/)\\nto encrypt files containing secrets from their work on Visit someone in prison,\\nbut that service is hosted on a different platform which means that the secrets\\nare in a private repo, separate from the application. It's easy to forget to\\nupdate config at the right time when it's managed separately from the\\napplication, and that has resulted in production outages in the past on that\\nservice.\\nIf we use git-crypt to encrypt secrets files in public repos rather than\\nprivate ones, the risk associated with accidentally pushing secrets in\\nunencrypted files is increased. If we take this approach, we would want to make\\nit easier to avoid doing that rather than relying only on people being careful.\\nWe are using kubectl for deploying. Some other teams are using helm, which has\\nsupport for templating so that values for config files can be kept separately\\nfrom the structure of the configuration. Unless we start using templating of\\nsome kind in our config files, we will need to keep the whole files containing\\nKubernetes secrets definitions private instead of just the secret values.\\n### Other options\\nAlternatives to using git-crypt in public repos include:\\n- separating out secrets into a private repo, using git-crypt there instead.\\nThis would increase the likelihood of us not updating the config when it's\\nneeded, leading to production outages.\\n- creating Kubernetes secrets manually from the command line, using config\\nfiles committed in the application repo without secret values or encryption -\\nso the secret values would be generated as needed and only stored in the\\nrelevant namespace. This would mean that our Kubernetes secrets could not be\\ndeployed as part of our automated pipeline but would need to be manually\\nupdated, and the manual workflow for updating a single value in a secret\\nwith multiple data values would be more complex. We would need to be careful\\nto not accidentally commit the values in the secrets files after deploying\\nthem.\\n- storing secret values in CircleCI envvars, to be used when creating\\nKubernetes secrets during deployment, using templating in our secrets files.\\nCircleCI has no concept of environments (all envvars for a project are\\navailable to all jobs) so all of our test and build steps would have access\\nto production secrets, and we would need to define an approach to namespacing\\nCircleCI envvars for different environments. Even so, envvars needed for\\nrunning tests, building and pushing images, running deployments and running\\napplications in different environments would all live together in one list,\\nwhich would make that list more confusing than it already is.\\n- using another secrets store to manage our secrets, probably instead of using\\nKubernetes secrets. This would add another external dependency for our\\napplications and would have the same challenges around either managing\\nsecrets values manually (thereby losing the ability to automate processes) or\\nfinding somewhere to manage version-controlled secrets from, and having\\nanother place to coordinate changes to required config with application\\ndeploys. It could also introduce another system for the team to authenticate\\nwith in order to manage secrets.\\n","completion":"## Decision\nWe will keep the Kubernetes secrets definitions which are related to our\\napplications in the public application repos, encrypted with git-crypt.\\nWe will only add team members who need access to production secrets to the\\ngit-crypt setup. The only exception to this is our CircleCI deployment jobs\\nwhich need to decrypt the secrets to deploy them.\\nWe will define a standard for creating and managing GPG keys and only use keys\\nwith the git-crypt setup which meet that standard.\\nWe will set up pre-commit hooks on our repos which contain encrypted secrets\\nto help us use git-crypt well.\\nWe will agree processes for revoking credentials with other relevant teams in\\ncase they are accidentally exposed.\\nIf another standard way of managing secrets for applications running on the\\ncloud platform is developed, we will consider using that instead.\\n"}
{"prompt":"## Context\\nWe are currently building a digital service to enable buyers on five CCS frameworks to specify requirements and\\nshortlist suppliers online. This MVP will form the basis of a broader service to be delivered iteratively which\\nwill ultimately enable buyers to run competitions and make awards online as a single service.\\nThe MVP will also establish a model and foundation for extending the digital service to other frameworks managed by CCS.\\nThe five frameworks for MVP are spread across different pillars and business areas, and therefore have different user bases, and users with different needs.\\nIt is likely that users in organisations like schools will make use of more than one of the five initial frameworks - they may use Supply Teachers for their short term staffing needs, and Facilities Management less regularly for their maintenance and facilities needs.\\nHowever, it is also likely that a large proportion of users of one of the frameworks will rarely if ever use one of the other 4. We expect that in the future when more and more frameworks make use of the digital platform, the likelihood of a single user regularly using more than one framework will increase.\\nUsers from different organisations and with different needs for goods or services are likely to arrive at the service through different routes - perhaps through GOV.UK, the CCS website, or other government services.\\nThe CMp infrastructure supports any number of web facing components deployed as individual subdomains under a single automatically managed domain record.\\n","completion":"## Decision\n### We will use a single domain for all commercial agreement journeys for buyers\\nWe will use a single domain for all framework journeys for buyers rather than a different domain for each commercial agreement.\\nThis will minimise the maintenance overhead of managing one domain per framework which will become particularly cumbersome as the platform grows over time.\\nThis will also ensure that the experience for users remains simple over time, maintaining the flexibility to distinguish or combine routes in to the service to ensure that they continue to match buyer behaviour, searching patterns, and purchasing needs even as the number of frameworks grows.\\nA single domain ensures that this kind of simplicity can be maintained at lower cost (changes inside the service being cheaper to make and lower impact on users than changes to the underlying infrastructure) whilst the service scales in size and complexity.\\nThe domain will be:\\n```\\nmarketplace.service.crowncommercial.gov.uk\\n```\\n"}
{"prompt":"## Context\\nA given mail is often written to the blob store by different components. And mail traffic is heavily duplicated (several recipients receiving similar email, same attachments). This causes a given blob to often be persisted several times.\\nCassandra was the first implementation of the blobStore. Cassandra is a heavily write optimized NoSQL database. One can assume writes to be fast on top of Cassandra. Thus we assumed we could always overwrite blobs.\\nThis usage pattern was also adopted for BlobStore on top of ObjectStorage.\\nHowever writing in Object storage:\\n- Takes time\\n- Is billed by most cloud providers\\nThus choosing a right strategy to avoid writing blob twice is desirable.\\nHowever, ObjectStorage (OpenStack Swift) `exist` method was not efficient enough to be a real cost and performance saver.\\n","completion":"## Decision\nRely on a StoredBlobIdsList API to know which blob is persisted or not in object storage. Provide a Cassandra implementation of it.\\nLocated in blob-api for convenience, this it not a top level API. It is intended to be used by some blobStore implementations\\n(here only ObjectStorage). We will provide a CassandraStoredBlobIdsList in blob-cassandra project so that guice products combining\\nobject storage and Cassandra can define a binding to it.\\n- When saving a blob with precomputed blobId, we can check the existence of the blob in storage, avoiding possibly the expensive \"save\".\\n- When saving a blob too big to precompute its blobId, once the blob had been streamed using a temporary random blobId, copy operation can be avoided and the temporary blob could be directly removed.\\nCassandra is probably faster doing \"write every time\" rather than \"read before write\" so we should not use the stored blob projection for it\\nSome performance tests will be run in order to evaluate the improvements.\\n"}
{"prompt":"## Context\\nThe decision to go with a Domain Driven Design approach to architecture is well documented in this [shared document](https:\/\/docs.google.com\/document\/d\/1fN4_0tAMCbPptn1cjauJdDwLMonOeaM7Ha4FK-Vao-k\/) and summarised in this [presentation](https:\/\/docs.google.com\/presentation\/d\/13YlQLMZPCsvCVdKVh6hsOUkS3PCAyuMATwikXVRMjQA) given during the March 2019 PubSweet meet. To summarise the objectives of the re-architecture, we want to make the application:\\n- Easy to extend without making existing code harder to maintain\\n- Easy to re-use with differing workflows without the need to edit large amounts of code\\nDDD allows us to separate our application into bound contexts (ie: submission\/review steps) to make them easier to maintain and more modular. This separation should mean that adding new contexts should not make maintaining existing contexts more challenging and that contexts that do not have overlapping boundaries (ie: **some** submission\/review steps) could be altered or removed entirely without requiring changes to other, non-related contexts.\\nWhile at the March 2019 PubSweet meet, Hindawi demonstrated their open-source project [xpub-reviewer](https:\/\/gitlab.com\/hindawi\/xpub\/xpub-review) which showed their approach to DDD put into practice. As we at eLife were still early on in the planning stage of how to implement a DDD approach to our own application, we decided it would be beneficial to take a similar approach to Hindawi so as to be able to share experience between organisations and possibly feed back into our shared framework [PubSweet](https:\/\/gitlab.coko.foundation\/pubsweet\/pubsweet) making it easier for others to follow a similar design pattern.\\n","completion":"## Decision\nWe initially plan to convert our current code base structure to match that of Hindawi's repository. Once this is complete we will then attempt to fix some of the concerns we have identified with this implementation, making sure to consult with Hindawi to ensure we maintain as similar an approach as possible. Once these details have been addressed we can then try to identify sections of the two applications which are the same\/similar to then recommend inclusion in future releases of PubSweet where they can then be easily adopted and maintained by the wider community.\\n### Overview of suggested architecture pattern\\nThe suggested pattern of architecture would be to place each bound context, both client and server side code it its own self-contained `component` directory, with the naming convention `component-xxxx` for bound contexts and `component-model-xxxx` for any shared data-model components. These would then leverage PubSweet's ability to pull in resolvers to form a full server side system while the client would make use of an `app` directory which contains a React router which pulls together the different client-side components. An example file structure can be found below\\n```\\n\/packages\\n\/app\\n\/component-dashboard\\n\/component-login\\n\/component-submission\\n\/component-model-manuscript\\n\/component-model-file\\n\/component-elife-ui\\n...\\n```\\nEach component behaves like a stand alone module with its own `package.json`, `\/config`, `\/src` and `\/test` directory (some don't have `\/config`).\\nContext components are then structured:\\n```\\n\/component-xxxx\\n\/client\\n\/components\\n\/graphql\\n\/pages\\n\/tests\\n\/config\\n\/server\\n\/src\\n\/use-cases\\nresolvers.js\\ntypeDefs.graphqls\\n\/tests\\nindex.js\\npackage.json\\n```\\nModel components are structured:\\n```\\n\/component-model-xxxx\\n\/src\\n\/entities\\n\/use-cases\\nresolvers.js\\ntypeDefs.graphqls\\n\/congig\\n\/test\\nindex.js\\npackage.json\\n```\\n### Pros\\n- Easy to extend without making existing code harder to maintain\\n- Easy to re-use with differing workflows without the need to edit large amounts of code\\n- Flexible directory structure within components means we can pick and choose whether we include tests, client-side code, etc.\\n- Components should have a single responsibility\\n- Models are injected making testing easier\\n- Better seperation of code makes testing easier\\n### Cons\\n- Some components don't need the same resources across the stack, so some components won't have UI or resolvers\\n- Components can have the need to rely on shared code, which is a sign of domains not having been decoupled from each other. This may lead to code duplication or components depending on each other\\n"}
{"prompt":"## Context and Problem Statement\\nWe'd like to change the spacing system to something more generic.\\n## Decision Drivers <!-- optional -->\\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\n","completion":"## Decision\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\nGeneric tshirt system was chosen over the other options.\\n- Coding a direct \"arbitrary value\" into a CSS class (color, number representing a unit) is generally considered an anti-pattern for reasons that are too long to cover in this ADR.\\n- We like the numbering system but we use t-shirt sizes everywhere else in our system so it makes sense to skew towards alignment with the larger system.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Decision Drivers\\n- We want to resolve the race conditions that appear when the `-race` flag is turned on in Go\\n","completion":"## Decision\n- We want to resolve the race conditions that appear when the `-race` flag is turned on in Go\\n- Remove all the spinner code\\n- Use [YAC Spin](https:\/\/github.com\/theckman\/yacspin)\\n- Replace [pterm](https:\/\/github.com\/pterm\/pterm) completely\\n- Continue to use [pterm](https:\/\/github.com\/pterm\/pterm) spinner\\nRemove all spinner code.\\n- This allows us to resolve races conditions that are flagged by Go's `-race` flag. Resolving these race conditions helps resolve [STIG V-70185](https:\/\/www.stigviewer.com\/stig\/application_security_and_development\/2018-12-24\/finding\/V-70185).\\n- Having a spinner display is not critical to the operation of the application. The text of the message should be retained to maintain context as to what is being run.\\n### Remove all spinner code\\n- `+` No race conditions created from spinners.\\n- `+` Less code to maintain.\\n- `-` Work to be done to remove the spinners.\\n- `-` No visibility on if the task that would be using the spinner is active.\\n### Use YAC Spin\\n- `+` No race conditions from spinners because YAC Spin is thread safe.\\n- `-` The features that we use for the terminal user interface are split across two libraries.\\n- `-` Some work will need to be done to replace pterm's spinner.\\n### Replace pterm\\n- `+` Wouldn't have to deal with race conditions from pterm.\\n- `+` All the features for a terminal user interface are in one library.\\n- `-` Have to find a sutable replacement that is thread safe and does everything that MilMove uses pterm for.\\n- `-` Work needs to be done to refactor and replace pterm.\\n### Continue to use pterm's spinner\\n- `+` All the terminal user interface related stuff are in one library.\\n- `+` No code changes needed.\\n- `-` MilMove is stuck with race conditions since the race conditions are occurring in a third party library.\\n"}
{"prompt":"## Context\\n![][image-1]\\n\u89c6\u9891\u94fe\u63a5\uff1a[http:\/\/v.youku.com\/v\\_show\/id\\_XMjk5ODQ3NzA3Mg==.html][1]\\n\u5de5\u4f5c\u4e2d\u6211\u4eec\u4f1a\u4f7f\u7528\u5927\u91cf\u7684\u5bc6\u7801\uff0c\u5176\u4e2d\u5305\u62ec\u670d\u52a1\u5668\uff0c\u6570\u636e\u5e93\uff0c\u6d4b\u8bd5\u8d26\u53f7\uff0c\u7b2c\u4e09\u65b9\u670d\u52a1\uff08\u963f\u91cc\u4e91\uff0cDnspod\uff0c\u5e94\u7528\u5e02\u573a\u7b49\uff09\u3002\\n\u6240\u6709\u7684\u5bc6\u7801\u90fd\u662f\u5206\u6563\u7684\u7ba1\u7406\u7740\uff0c\u6bcf\u4e2a\u4eba\u90fd\u81ea\u884c\u5bf9\u8fd9\u51e0\u5341\u4e2a\u5bc6\u7801\u8fdb\u884c\u7ba1\u7406\u4e0e\u5b58\u50a8\uff0c\u65b9\u5f0f\u5404\u6837\uff0cword, excel, \u5728\u7ebf\u6587\u6863\u7b49\u3002\\n\u5f53\u524d\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1a\\n1. \u4f7f\u7528\u5bb9\u6613\u8bb0\u4f46\u5f88\u4e0d\u5b89\u5168\u7684\u5bc6\u7801\uff0c\u4f8b\u5982\uff0c`123456`\uff1b\\n2. \u5728\u6bcf\u4e00\u4e2a\u670d\u52a1\u4e0a\u4f7f\u7528\u76f8\u540c\u7684\u975e\u5b89\u5168\u5bc6\u7801\uff1b\\n3. \u5f53\u56e2\u961f\u6210\u5458\u6709\u53d8\u52a8\u65f6\uff0c\u5bc6\u7801\u6ca1\u6709\u6539\u53d8\uff1b\\n4. \u65e0\u52a0\u5bc6\u7684\u5c06\u5bc6\u7801\u5b58\u5728\u672c\u5730\u6587\u4ef6\u6216\u662f\u4e91\u7b14\u8bb0\u4e2d\uff1b\\n5. \u901a\u8fc7\u90ae\u7bb1\u6216\u662f\u5fae\u4fe1\u8fdb\u884c\u5bc6\u7801\u5206\u4eab\u3002\\n\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u5de5\u5177\u7ba1\u7406\u6211\u4eec\u7684\u5bc6\u7801\uff0c\u5305\u62ec\u5982\u4e0b\u529f\u80fd\u53ca\u8003\u8651\uff1a\\n1. \u751f\u6210\u590d\u6742\u5bc6\u7801\uff1b\\n2. \u5bc6\u7801\u88ab\u52a0\u5bc6\u540e\u4fdd\u5b58\uff1b\\n3. \u5206\u53d1\u65b9\u5f0f\u5b89\u5168\uff1b\\n4. \u4f7f\u7528\u8d77\u6765\u7b80\u5355\u65b9\u4fbf\uff1b\\n5. \u652f\u6301\u56e2\u961f\u5206\u7ec4\u7ba1\u7406\uff1b\\n6. \u9700\u8981\u53ef\u5b58\u50a8\u9644\u4ef6\uff08\u6bd4\u5982\uff0cssh \u516c\u79c1\u94a5\uff09\uff1b\\n7. \u517c\u5bb9 Win\/Mac\/Linux;\\n8. \u8d39\u7528\u5408\u9002\u3002\\n\u8d26\u53f7\u5bc6\u7801\u4f5c\u4e3a\u516c\u53f8\u7684\u91cd\u8981\u865a\u62df\u8d44\u4ea7\uff0c\u4f7f\u7528\u4f20\u7edf\u7684\u65b9\u5f0f\u4f1a\u5e26\u6765\u6781\u5927\u7684\u98ce\u9669\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u6709\u6548\u7684\u7ba1\u7406\u65b9\u6cd5\u3002\\n","completion":"## Decision\n* LastPass(\u529f\u80fd\u7e41\u6742\uff0c\u4f7f\u7528\u4eba\u6570\u6700\u591a\uff0c\u8d39\u7528 1450 USD \/ 50 users)\\n* PassPack(**Chosen**)(\u529f\u80fd\u7b80\u5355\uff0c\u5c0f\u4f17\uff0c\u652f\u6301\u6587\u672c\u6bb5\uff0c\u8d39\u7528 144 USD \/ 80 users)\\n* 1Password(\u529f\u80fd\u7b80\u5355\uff0c\u4f7f\u7528\u65b9\u5f0f\u53cb\u597d\uff0c\u8d39\u7528 7200 USD \/ 50 users)\\n"}
{"prompt":"### Context\\nPHPUnit assertions are static methods, yet in our code base we call them with `$this` instead of\\n`self`.\\nWhilst \"incorrect\", this usage does not break anything. Besides:\\n- [PHUnit documentation][phpunit-doc] itself uses this by default\\n- `$this` is much more widely used than `self` in this context in the community\\n- all Infection code uses `$this`\\nThere is not much shortcomings from using this other than the \"incorrectness\" of using a static\\nmethod as a non-static one.\\n### Decision\\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","completion":"## Decision\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n"}
{"prompt":"## Context\\n- Proxy support is required for some enterprises and organizations to start using their own self-hosted runners\\n- While there is not a standard convention, many applications support setting proxies via the environment variables `http_proxy`, `https_proxy`, `no_proxy`, such as curl, wget, perl, python, docker, git, and R\\n- Some of these applications use `HTTPS_PROXY` versus `https_proxy`, but most understand or primarily support the lowercase variant\\n","completion":"## Decision\nWe will update the Runner to use the conventional environment variables for proxies: `http_proxy`, `https_proxy`, and `no_proxy` if they are set.\\nThese are described in detail below:\\n- `https_proxy` a proxy URL for all https traffic. It may contain basic authentication credentials. For example:\\n- http:\/\/proxy.com\\n- http:\/\/127.0.0.1:8080\\n- http:\/\/user:password@proxy.com\\n- `http_proxy` a proxy URL for all http traffic. It may contain basic authentication credentials. For example:\\n- http:\/\/proxy.com\\n- http:\/\/127.0.0.1:8080\\n- http:\/\/user:password@proxy.com\\n- `no_proxy` a comma-separated list of hosts that should not use the proxy. An optional port may be specified. For example:\\n- `google.com`\\n- `yahoo.com:443`\\n- `google.com,bing.com`\\nWe won't use `http_proxy` for https traffic when `https_proxy` is not set, this behavior lines up with any libcurl based tools (curl, git) and wget.\\nOtherwise, action authors and workflow users need to adjust to differences between the runner proxy convention, and tools used by their actions and scripts.\\nExample:\\nCustomer sets `http_proxy=http:\/\/127.0.0.1:8888` and configures the runner against `https:\/\/github.com\/owner\/repo`, with the `https_proxy` -> `http_proxy` fallback, the runner will connect to the server without any problem. However, if a user runs `git push` to `https:\/\/github.com\/owner\/repo`, `git` won't use the proxy since it requires `https_proxy` to be set for any https traffic.\\n> `golang`, `node.js`, and other dev tools from the Linux community use `http_proxy` for both http and https traffic based on my research.\\nA majority of our users are using Linux where these variables are commonly required to be set by various programs. By reading these values, we simplify the process for self-hosted runners to set up a proxy and expose it in a way users are already familiar with.\\nA password provided for a proxy will be masked in the logs.\\nWe will support the lowercase and uppercase variants, with lowercase taking priority if both are set.\\n### No Proxy Format\\nWhile exact implementations are different per application on handling `no_proxy` env, most applications accept a comma-separated list of hosts. Some accept wildcard characters (`*`). We are going to do exact case-insensitive matches, and not support wildcards at this time.\\nFor example:\\n- `example.com` will match `example.com`, `foo.example.com`, and `foo.bar.example.com`\\n- `foo.example.com` will match `bar.foo.example.com` and `foo.example.com`\\nWe will not support IP addresses for `no_proxy`, only hostnames.\\n"}
{"prompt":"## Context and Problem Statement\\nIn the course of implementing a class-based role metadata model we have also\\nreviewed options on how to design serialization infrastructure between wire\\nformats and the class model. In an initial attempt we have implemented\\nserialization on the metadata class (see option 1), but issues with inheritance\\nand calls for more flexibility have caused us to rethink this approach.\\n## Decision Drivers\\n* A class-based role metadata model (see ADR4) requires serialization routines\\nfrom and to wire formats\\n* TUF integrators may require custom serialization implementations for custom\\nwire formats\\n* Readability and simplicity of implementation for users and maintainers\\n* Recognizability of specification\\n","completion":"## Decision\n* A class-based role metadata model (see ADR4) requires serialization routines\\nfrom and to wire formats\\n* TUF integrators may require custom serialization implementations for custom\\nwire formats\\n* Readability and simplicity of implementation for users and maintainers\\n* Recognizability of specification\\nChosen option: \"Compromise 2\", because implementing dict conversion as methods\\non a corresponding class is idiomatic and allows for well-structured code.\\nTogether with a separated serialization interface, it provides both ease of use\\nand maintenance, and full flexibility with regards to custom serialization\\nimplementations and wire formats.\\n"}
{"prompt":"## Context and Problem Statement\\nAzure Blob Storage will be used to store all files associated with permit application.  Both front stage and back stage users need to be able to read and upload documents to a 'case file'.\\n## Decision Drivers\\n* Back stage users should be able to see all files relating to a permit\\n* Front stage users should only be able to view and download files that are relevant to them.\\n* Sensitive data privacy \/ security requirements mean access to certain documents should be restricted for both front stage and back stage users.\\n","completion":"## Decision\n* Back stage users should be able to see all files relating to a permit\\n* Front stage users should only be able to view and download files that are relevant to them.\\n* Sensitive data privacy \/ security requirements mean access to certain documents should be restricted for both front stage and back stage users.\\n[Option 2] Using a combination of Active Directory groups and SAS tokens provides the greatest flexibility to manage document access.\\n### Positive Consequences\\n* Front stage users are only able to access documents that are relevant to them, this would be managed by the frontend application & the document type attribute stored in the CRM \/ document metadata.\\n* Using SAS tokens for sensitive documents also simplifies the private access.\\n### Negative Consequences\\n* SAS tokens would potentially need be generated for each file download.\\n"}
{"prompt":"## Context\\nWe want to implement user authentication on the monitoring stack's web applications (Grafana, Prometheus, Alertmanager).\\nWe also want to use Github as the identity provider (IdP), as that provides a means for us to control access based on GDS Github organisation and team membership; the alternative option, GSuite, does not currently have the granular role\/group structure that we would require to effectively control access, and introducing that group structure would be out of scope for this project, and would likely replicate the Github group\/team structure that currently exists.\\nGrafana supports many authentication methods, including [Github OAuth2](https:\/\/grafana.com\/docs\/grafana\/latest\/auth\/github\/) and [OIDC (via generic OAuth2)](https:\/\/grafana.com\/docs\/grafana\/latest\/auth\/generic-oauth\/).\\nPrometheus and Alertmanager do not support any form of web-based user authentication, so this must be handled via a proxy or load balancer in front of the application.\\nWe are currently using ALBs in front of Prometheus and Alertmanager, via the [AWS Load Balancer Controller](0004-use-aws-load-balancer-controller-for-edge-traffic-services.md), which supports [authentication via AWS Cognito or OIDC-compliant identity providers](https:\/\/kubernetes-sigs.github.io\/aws-load-balancer-controller\/v2.3\/guide\/ingress\/annotations\/#authentication).\\nGithub as an IdP does not support OIDC, only OAuth2, so cannot be used with ALB auth. AWS Cognito [does not support OAuth2, only OIDC](https:\/\/docs.aws.amazon.com\/cognito\/latest\/developerguide\/cognito-identity.html), so cannot be used with Github as an IdP.\\nArgoCD, our current CD tool, also [implements authentication via OIDC](https:\/\/argo-cd.readthedocs.io\/en\/stable\/operator-manual\/user-management\/#existing-oidc-provider), and [includes an OIDC-compliant identity broker](https:\/\/argoproj.github.io\/argo-workflows\/argo-server-sso-argocd\/), [Dex](https:\/\/dexidp.io).\\nIn summary, OIDC is the only auth protocol supported universally by both our current web-based UIs that support authentication internally, and by ALBs for apps that do not. Github however does not provide an OIDC-compliant IdP.\\n","completion":"## Decision\nUse [Dex](https:\/\/dexidp.io) as an SSO service and identity provider for all web-based user authentication. This is strictly for use with user-facing web-based cluster services such as monitoring and CI\/CD tools, and should not be used for any other purposes such as AWS or Kubernetes authentication.\\n"}
{"prompt":"Problem Statement\\nThe MilMove project has multiple teams pushing code to shared repositories. All teams are currently using the same branch in each repo: main. This is very efficient pre-production, because all teams see each others' changes quickly. When we go to production (and maybe even a little before), this breaks down. A couple of example scenarios will illustrate this:\\nScenario 1: The Bug vs. The Incomplete Feature\\nWhile one of our teams is part-way through an epic that adds a new feature, a critical bug is reported. A fix for the bug is quickly found and committed. But if we deploy code from the main branch at that point, half of the new feature will also be included, since it was already committed. Releasing half of a feature to production is generally frowned upon.\\nScenario 2: Changing Authentication\\nAfter we have been live for several months, the customer requires us to move from using login.gov to authenticate users to Okta. Writing and testing the code is no problem. But when we deploy it, every user now has to set up a new authentication account. Many of those users will be in the middle of moving to their next duty station when we spring this on them. Users are constantly moving, there will not be a break in the schedule where we can do the changeover.\\nScenario 3: Prime Freeze\\nFor 30 days before we go live, the Prime (HSA) requires a code freeze for testing. We do not want to stop deploying for 30 days, and we really don't want to stop development for 30 days. Feature flags would allow us to build and deploy code during the freeze and remain productive.\\n### Feature Flag Workflow\\nThe [launch darkly overview of feature flags](https:\/\/launchdarkly.com\/blog\/what-are-feature-flags\/) is a good summary of what feature flags can do.\\nImagine MilMove is being used daily in production. Further, imagine we are adding new functionality to MilMove. We need to make schema changes, backend changes, and frontend changes. We want to be able test all aspects of the feature before releasing it for general availability.\\nWe want a way where we can hide this new functionality until is is ready, but we don't want to have to have a separate branch in git and wait to merge all this code into main. We want to separate the release of the feature from the deployment of the code.\\nSuppose we want to have a limited rollout in production and only want certain service members to have access to MilMove for setting up their move. We could add logic to the auth handlers in the backend that might look like\\n```go\\n\/\/ how we might allow only certain users to log into the\\n\/\/ production environment when we are testing moves\\nconst loginAllowedFlag = \"loginAllowed\"\\nflag, err := h.FeatureFlagFetcher().GetFlagForUser(r.Context(), appCtx, loginAllowedFlag, map[string]string{})\\nif err != nil {\\n\/\/ in this case, we will fail open in case of feature flag\\n\/\/ failure. The user has already been authenticated by login.gov\\nappCtx.Logger().Warn(\"Feature flag failure in auth handler\", zap.Error(err))\\n} else {\\nif !flag.IsEnabledVariant() {\\nappCtx.Logger().Info(\"User does not have login flag enabled\", zap.String(\"flag, loginEnabledFlag))\\ninvalidPermissionsResponse(appCtx, h.HandlerConfig, h.Context, w, r)\\nreturn\\n}\\n}\\n```\\nThe feature flag service can use the information about the user to decide if they can log in.\\nOr imagine when we are implementing OCONUS moves, but we don't want to roll it out to all service members at once. Maybe we have something in the UI that presents different options.\\n```javascript\\nimport { FeatureFlag, featureIsEnabled } from 'components\/FeatureFlag\/FeatureFlag';\\n\/\/ ...\\nexport const ConusOrNot = () => {\\nconst enabledOconus = () => {\\n\/\/ something\\n}\\nconst disabledOconus = () => {\\n\/\/ something\\n}\\nconst featureFlagRender = (flagValue) => {\\nif (featureIsEnabled(flagValue)) {\\nreturn enabledOconus;\\n}\\nreturn disabledOconus;\\n};\\nreturn <FeatureFlag flagKey=\"service-member-oconus\" render={featureFlagRender} \/>;\\n}\\n```\\n","completion":"## Decision\nChosen Alternative: Option #5: Flipt\\nThis section is an overview of why choose Flipt over the alternatives.\\nLaunchDarkly is the industry standard, but there is no room for it in the budget. Unleash has similar features to AWS AppConfig, but requires more setup and maintenance effort when self-hosted (e.g. it requires a PostgreSQL database). AWS AppConfig is really a thin shim over config files and doesn't provide much in the way of helping us manage feature groupings.\\nRolling our own is a possibility, but one key feature we'd want to think about is how to promote feature flags from one environment to another (e.g. from staging to production). That suggests that tying the feature flag to the environment by e.g. storing the flags in the database isn't ideal as we need to recreate the flag settings in each environment. Combining that with the desire to have feature flags be enabled on a per user basis, it makes the complexity of rolling our own outweigh the relatively simple flipt deployment.\\nFlipt provides a [filesystem backend](https:\/\/www.flipt.io\/docs\/experimental\/filesystem-backends) which would allow a way for us to manage our feature flags using a [gitops](https:\/\/about.gitlab.com\/topics\/gitops\/) style process. We can test out our flag configuration in separate environments (e.g. experimental, demo, staging) before rolling out to production. It also allows us to deploy the Flipt service without requiring another stateful system (e.g. no database).\\nWe will have a couple of different options for how we deploy flipt. Examining the options for how flipt is deployed will be its own ADR.\\n"}
{"prompt":"## Context\\nThe layer selector plugin has grown in complexity over the years and has\\naccumulated a fair amount of technical debt. Fixing bugs and adding new\\nfeatures has become increasingly difficult and time consuming as a result.\\nSome issues with the current implementation:\\n* The widget we use ([Ext JS Tree Panel](https:\/\/docs.sencha.com\/extjs\/6.0\/components\/trees.html))\\ndoes not provide enough control over how nodes are rendered.\\n* It is difficult to add buttons and other interactions to each leaf node\\n(zoom to extent, download button, etc.) using Ext JS Tree Panel.\\n* Separation between *saved state* and *actual state*. The in-memory representation\\nof the tree often differs from the serialized tree state, which has led to\\nsynchronization issues.\\n* Lack of high-level data abstractions make it difficult to understand\\nhow the plugin transforms data for rendering.\\n* Mutable state occasionally leads to data corruption caused by performing\\ncertain sequences.\\n","completion":"## Decision\nWe have decided to rewrite the layer selector from scratch before adding\\nnew features.\\nTo resolve the problems previously described, the new layer selector should\\nhave the following qualities:\\n* The UI should be defined using simple `underscore` templates so we retain\\nfull control over the look & feel.\\n* All user interactions should aspire to update a singular state object,\\nwhich will then cause the UI to be redrawn. In this way, the UI will always\\nbe a reflection of what actually exists in the saved state.\\n* There should be a separation between the layer config and the layer data\\nthat is loaded at runtime. This is to prevent bugs that can result from\\nmutating a single state repeatedly.\\n* Services and layer data should be lazily loaded when possible.\\n"}
{"prompt":"## Context\\nIt was decided to configure the TDR user administrators in the top-level realm in Keycloak: [0008 Keycloak User Administrator Configuration](0008-keycloak-user-administrators-configuration.md)\\nWhilst implementing the configuration in this way it was found the only way to import the top-level realm json configuration wiped any existing users, at docker container start up.\\nThe decision to configure TDR user administrators in the Keycloak top-level realm was re-considered as a consequence of this issue.\\n","completion":"## Decision\nFollowing further investigation and discussion it was decided to take the following approach to avoid any configuration in the Keycloak top-level realm:\\n1. TDR user administrators are to be configured in the TDR realm instead of the top-level Keycloak realm.\\n2. A Keycloak group will be set up in the TDR realm with the necessary role mappings to provide the required permissions to administer transferring body users.\\n3. Logging and alerting will be implemented to notify of any users added to the TDR user administrator group, to mitigate against the risk of elevated privileges abuses.\\nThis approach balances the needs of ensuring Keycloak remains secure, whilst still allowing for ease of maintenance, and extension in the future.\\nOther options that were considered are outlined below.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context and Problem Statement\\nSetting up the Teaching Vacancies app for local development is a frustrating process involving many\\nsteps and multiple dependencies driven by manually updated setup documentation. A workflow based on\\n[devcontainers](https:\/\/code.visualstudio.com\/docs\/remote\/create-dev-container) would alleviate\\nmuch of this setup pain, and provide a trivially reproducible environment for local development,\\nbenefitting both developers and non-developers on the team.\\n## Decision Drivers\\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\n","completion":"## Decision\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\nAdd devcontainers as an option for now, with a view to iterate on it and improve it to the point\\nwhere we can consider it the \"official\" default way of running Teaching Vacancies (while still\\nallowing other development workflows for developers who prefer different ways of working).\\n### Positive Consequences\\n- Drastically easier onboarding and \"re-boarding\" (e.g. on a new device or after an OS upgrade\\ncausing developer tooling issues)\\n- Dependencies reduced to just Git, Docker, and VS Code\\n- A fully functioning development environment is ready in 10 minutes from scratch, with no user\\ninteraction beyond opening the repository in VS Code and selecting \"Reopen in container\"\\n- Moving entirety of development experience into a container fixes past Docker development workflow\\nissues experienced on the team (where tasks and services where executed from the host instead of\\ninteracting with a shell and an editor from inside the container itself)\\n- Developers and other team members can develop on any host OS (macOS\/Linux\/Windows) but we only\\nneed to support one single consistent environment\\n- Does away with all the Mac vs Linux vs WSL setup steps in our current documentation\\n- Reduces likelihood of \"works on my machine\" development environment issues\\n- \"Leave no trace\" on the host machine and complete isolation from other projects\\n- Removes possibility of \"dependency hell\" when working on multiple projects\\n- Removes need to clutter local environment with applications and dependencies that need to be\\nkept up to date and in sync (e.g. Google Chrome and `chromedriver`)\\n- Removes need for language version managers (`rbenv`, `nvm`)\\n- Provides _executable documentation_ of project setup and dependencies\\n- Removes need for manually updated setup documentation that can become stale\\n- Experienced developers who have a different preferred workflow can get a clear, in-code view\\nof setup steps and dependencies\\n- Good workflow for everyone, but excellent additional integration with Visual Studio Code\\n- Automatic passthrough of SSH and GPG keys\\n- Language extensions run within the container itself, and can be specified in the devcontainer\\nconfiguration file for instant setup of useful extensions for new users\\n- Automatic bootstrapping of personal dotfiles\\n- Ability to easily move to cloud-based workflows in the future\\n- Ability to easily propagate new tools and improved configuration to all developers on the team\\n- Trivial rebuilds to a known good state when performing \"dangerous\" operations in the container\\n### Negative Consequences\\n- Slightly reduced performance on some host OSs (non-Linux) due to Docker being Linux-native\\ntechnology (overhead of containers running in an intermediate VM)\\n- Somewhat mitigated by use of volumes for IO-intensive cache directories\\n- Can be worked around entirely by moving workspace root into the container, and we will continue\\nto investigate before we fully agree on devcontainers as our default workflow\\n- Container layers need occasional pruning on the host as Docker can fill up disk space quickly\\n- Some duplication of Docker configuration between production and development configuration (but\\nthat is to be expected given that use cases are very different)\\n"}
{"prompt":"### Context\\nFor signature verification in Cosmos SDK, the signer and verifier need to agree on\\nthe same serialization of a `SignDoc` as defined in\\n[ADR-020](.\/adr-020-protobuf-transaction-encoding.md) without transmitting the\\nserialization.\\nCurrently, for block signatures we are using a workaround: we create a new [TxRaw](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/9e85e81e0e8140067dd893421290c191529c148c\/proto\/cosmos\/tx\/v1beta1\/tx.proto#L30)\\ninstance (as defined in [adr-020-protobuf-transaction-encoding](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-020-protobuf-transaction-encoding.md#transactions))\\nby converting all [Tx](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/9e85e81e0e8140067dd893421290c191529c148c\/proto\/cosmos\/tx\/v1beta1\/tx.proto#L13)\\nfields to bytes on the client side. This adds an additional manual\\nstep when sending and signing transactions.\\n### Decision\\nThe following encoding scheme is to be used by other ADRs,\\nand in particular for `SignDoc` serialization.\\n","completion":"## Decision\nThe following encoding scheme is to be used by other ADRs,\\nand in particular for `SignDoc` serialization.\\n"}
{"prompt":"## Context\\nWe rely on dynamic mappings to expose our mail headers as a JSON map. Dynamic mapping is enabled for adding not yet encountered headers in the mapping.\\nThis causes a serie of functional issues:\\n- Maximum field count can easily be exceeded\\n- Field type 'guess' can be wrong, leading to subsequent headers omissions [1]\\n- Document indexation needs to be paused at the index level during mapping changes to avoid concurrent changes, impacting negatively performance.\\n","completion":"## Decision\nRely on nested objects to represent mail headers within a mapping\\n"}
{"prompt":"## Context\\nNow we have applications that get deployed into our microservices platform, we\\nneed to manage the configuration in an easy way.  YAML files are nice, but are\\ndifficult to reuse or to publish in order to manage vast amounts of\\ndeployments.  We need more tools.\\nHelm is a solution that eases distribution of kubernetes applications by using\\nthe concepts of Charts and Releases.\\nAs the manual states, a Chart is a Helm package. It defines the resources to\\nrun an application or tool inside a Kubernetes cluster.  A Release is an\\ninstance of a Chart running in a cluster.  Each time a Chart is installed, a\\nnew release is created.  So it's reusable code.\\nTo reuse this code and make it more accesible to others, there are repositories\\nlike in other package managers.  A Helm Chart can be published in a Repository\\nto use it widely.\\nThis provides huge advantages to the manageability of the applications deployed\\nin Kubernetes clusters.\\n","completion":"## Decision\nI'm going to implement helm packages to replace the templates in the kube\\nsubfolder of each application.\\nBy now, I'm not going to use a remote repository, just the local definition of\\nthe Helm Chart.\\n"}
{"prompt":"## Context and Problem Statement\\nWe frequently write wrapper scripts. They bring the following value:\\n1. They bind together several tools and make them work together as a whole, e.g., `sops` and `kubectl`.\\n2. They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files.\\n3. They enforce best practices, e.g., encrypt secrets consumed or produced by tools.\\nUnfortunately, wrapper scripts can also bring disadvantages:\\n1. They make usages that are deviating from the \"good way\" difficult.\\n2. They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked.\\n3. They add overhead when adding new features or supporting new use-cases.\\n4. They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the [Law of Leaky Abstractions](https:\/\/www.joelonsoftware.com\/2002\/11\/11\/the-law-of-leaky-abstractions\/).\\n## Decision Drivers\\n* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\n","completion":"## Decision\n* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\nWe have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value.\\nThis decision applies for new wrapper scripts. We will not rework old wrapper scripts.\\n### Positive Consequences\\n* The operations team can encode standard operating procedures and scale ways of working.\\n* Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort.\\n* Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability.\\n### Negative Consequences\\n* There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.\\n"}
{"prompt":"## Context\\nNeed to decide on a high level structure for this app to take. It's goals are:\\n- Me to learn\\n- Fun times \/ be aspirational (as a selling point if it ever gets published seriously)\\n- be a potential thing I'll publish\\n- make some elements of life easier \/ be an enabler to life automation.\\n","completion":"## Decision\n\"Federated syncing\". It's aspirational, one hell of a challenge, awesome for the CV. The main con for this is complexity... but this _is_ a learning project so: bring it on! Next up is to sketch out the layout of the pieces.\\n"}
{"prompt":"## Context\\nIncoming requests to the backend server must be validated to avoid\\nincomplete, and invalid data coming in.\\nSince most of the APIs will be asynchronous for the front-end, there\\nwill be no user intervention when validation errors occur. Hence, such\\nvalidation must be coherent with the validation performed on the\\nmobile, front-end.\\n","completion":"## Decision\nUse swagger to formally record API specifications, and use the\\ninternal JSON schema to validate incoming requests, using JSON schema\\nvalidators.\\nThis makes validation errors predictable, and fixable during dev\\ntime. Also, if we use swagger-codgen to generate clients for the\\nfrontend, we would get compile time exceptions for type errors, etc.\\nAny additional validations that the backend performs, like conditional\\nrequirement, must be made explicit in the API documentation, or raise\\n500s.\\n"}
{"prompt":"## Context\\nA change in catchment in a device can be caused by two reasons.\\n1. The user-catchment assignment has changed in the server\\n2. A new user has logged in.\\nIn both these conditions,\\n- It is possible that there is unsynced data in the device\\n- The new user might not have control over old data\\nA new catchment means that the last update date times are wrong, and the data is also wrong on the device, so this means a full sync from the server. We should do this, but not allow existing unsynced data to be wiped out.\\nAlso note that the catchment an individual belongs to is set when he\/she is created. Therefore, a sync of data will not affect any unsynced data.\\n","completion":"## Decision\nWhen a change in catchment is detected by openchs-client, existing unsynced data will be synced to the server, all data deleted, and a fresh sync started.\\n"}
{"prompt":"## Context\\nHigh privilege access (HPA) limits production access for developers to only the components and period this access is required to investigate issues of check system health. This implements the [principle of least privilege](0039-least-privilege-access.md) for support on production systems.\\n","completion":"## Decision\nWe will use a high privilege access procedure to secure access to production systems for support.\\n"}
{"prompt":"## Context\\nData lineage includes the data origin, what happens to it and where it moves over time. Data lineage gives visibility while greatly simplifying the ability to trace errors back to the root cause in a data analytics process.\\nBy adding tracing information to every single message it is possible to trace back a single business event to its source (and all the systems inbetween).\\n","completion":"## Decision\nEvery event published on a pub\/sub topic has a gobits record added. Every applications handling\/modifing or relaying the event should add a gobits record to that single business event.\\n"}
{"prompt":"## Context\\nThe web application we want to develop and offer to our users is a picture service that allows :\\n* Uploading images and gifs with a title and tags\\n* Searching images by title or tag\\n* Downloading an image in various file formats and sizes\\nAs we want to quicly build a first version to be able to quickly go online and answer competitors, we choose\\nto deploy our application to Google Cloud (as we do not have any bare metal servers).\\nThe application should meet the following criteria:\\n* it should be quick enough so the search answers in less than 500ms\\n* it should be scalable so we can meet future usage\\n","completion":"## Decision\nIn order to meet required criteria we choose to develop our application using a microservices approach :\\neach business need will be addressed by a specific microservice, as described in the following schema:\\n![](microservices-architecture.png)\\nEach microservice should take a simple input and produce a simple output, each microservice should act as a black box\\nregarding other microservices, meaning state should not be shared across different instances of same or differents microservices.\\n"}
{"prompt":"## Context\\n1. The framework rationale requires a separation of concern between \"code\\nis code, data is data\" and because of that some libraries were not\\nable to proceed to be used in the application.\\n2. The framework also wants to be a complete solution which means that\\nour users should not be required to learn too many libraries in order\\nto be productive. Instead, the framework should provide layers of\\nindirection to wrap such functionalities and provide centralized\\ncontrol over the features.\\n","completion":"## Decision\n- Usage of `yogthos\/config` to handle config files\\n- Usage of `stuartsierra\/components` to handle dependency management\\n- Usage of `honeysql` to handle SQL interactions\\n- Build a framework layer to handle migrations in `honeysql` style\\n- Build framework layer to wrap honeysql, honeysql-plugins, next-jdbc,\\nhikari-cp and possible others lirbaries.\\n"}
{"prompt":"## Context\\nBuilding a landing page has different requirements then building a normal applications. I have to decide I you the normal UI Framework (AntDesign), build most of the landing page without a framework or use the landing page provided by ant design.\\n","completion":"## Decision\nUse [Ant Design Landing](https:\/\/landing.ant.design) to create the landing page.\\n"}
{"prompt":"## Context\\n#### Original year range behavior\\nStarting in [v3.2.0](https:\/\/github.com\/holidays\/holidays\/releases\/tag\/v3.2.0) of the ruby gem [\\[1\\]](#footnote-1) we have had the ability to specify year ranges for specific holidays. This allows for a holiday\\nto only be considered 'valid' or 'active' based on specific criteria surrounding years. The available criteria were:\\n* `before` - holiday only valid if it takes place on the target year or before\\n* `after` - holiday only valid if it takes place on the target year or after\\n* `limited` - holiday only valid if it takes place on at least one of an array of target years (e.g. [2002, 2004])\\n* `between` - holiday only valid if it takes place between an inclusive year range (e.g. 2002..2004)\\nThis change added useful functionality and its use has since spread to multiple regions.\\n#### Confusion about criteria behavior\\nOn January 24th, 2019 [an issue was opened](https:\/\/github.com\/holidays\/definitions\/issues\/117) expressing that the `before` and `after` criteria were named in a confusing manner since it was not clear whether they operated inclusively or exclusively on the target year.\\nAs an example, the value `after: 2018` could be construed by some to mean the holiday is valid starting in 2019 and onward. In reality the current implementation is that the holiday is valid starting in 2018 itself and onward.\\nWhile this is ultimately up to individual perception it is true that the current names do not provide strong guidance on how the definition will behave.\\n","completion":"## Decision\nThe [above issue](https:\/\/github.com\/holidays\/definitions\/issues\/117) also contained a proposal to make the following changes:\\n* Rename `before` to `until`\\n* Rename `after` to `from`\\nThese names give a clearer understanding of the desired behavior as `until` and `from` are more generally understood to indicate inclusivity rather than exclusivity.\\nIf we take the example from above and make the change then the value `from: 2018` would intend for the holiday to be valid\/active starting in 2018 and onward.\\n#### Additional changes\\nWhile looking into the above issue I noticed two important things that will also be addressed alongside the above changes:\\n* The definition validator does not currently prevent users from specifying multiple selectors at a time for a `year_ranges` entry and we perform no validation that these selectors do not conflict with one another.\\n* The `between` key currently accepts a string representation of a Ruby range (e.g. '2008..2012'). While this was not causing any issues today we would like to remove all Ruby-specific values from our definitions so that other languages could more easily parse them.\\nTo that end the following changes will also be made:\\n* Update the definition validation to only allow a single selector per `year_ranges` entry and update all definitions to match. This will result no behavior changes but will make clear the expected behavior.\\n* `between` will no longer accept a ruby-like range string but instead require explicit `start` and `end` keys with integer values representing a year.\\n"}
{"prompt":"## Context\\nDocker has a state directory that contains images and containers. Docker has a separate update mechanism for the images to that of the root filesystem and expects its state to be mutable. The dApps and their updates should logically be disconnected to the updates to the system itself.\\n","completion":"## Decision\nWe will put the Docker's state directory on the read-write data volume.\\n"}
{"prompt":"## Context\\nIn order for the Cosmos SDK to implement the [IBC specification](https:\/\/github.com\/cosmos\/ics), modules within the Cosmos SDK must have the ability to introspect recent consensus states (validator sets & commitment roots) as proofs of these values on other chains must be checked during the handshakes.\\n","completion":"## Decision\nThe application MUST store the most recent `n` headers in a persistent store. At first, this store MAY be the current Merklised store. A non-Merklised store MAY be used later as no proofs are necessary.\\nThe application MUST store this information by storing new headers immediately when handling `abci.RequestBeginBlock`:\\n```go\\nfunc BeginBlock(ctx sdk.Context, keeper HistoricalHeaderKeeper) error {\\ninfo := HistoricalInfo{\\napphash: ctx.HeaderInfo().AppHash,\\nTime: ctx.HeaderInfo().Time,\\nNextValidatorsHash: ctx.CometInfo().NextValidatorsHash,\\n}\\nkeeper.SetHistoricalInfo(ctx, ctx.BlockHeight(), info)\\nn := keeper.GetParamRecentHeadersToStore()\\nkeeper.PruneHistoricalInfo(ctx, ctx.BlockHeight() - n)\\n\/\/ continue handling request\\n}\\n```\\nAlternatively, the application MAY store only the hash of the validator set.\\nThe application MUST make these past `n` committed headers available for querying by Cosmos SDK modules through the `Keeper`'s `GetHistoricalInfo` function. This MAY be implemented in a new module, or it MAY also be integrated into an existing one (likely `x\/staking` or `x\/ibc`).\\n`n` MAY be configured as a parameter store parameter, in which case it could be changed by `ParameterChangeProposal`s, although it will take some blocks for the stored information to catch up if `n` is increased.\\n"}
{"prompt":"## Context\\nDomain models were used as-is for persistence and over-the-wire communication, making it difficult\\nto define a proper and strong domain model.\\n","completion":"## Decision\nSeparate DTOs should be defined for persistence and over-the-wire communication, which can have a\\nmore adequate representation for their use-case, without affecting our domain model.\\n"}
{"prompt":"## Context\\nWe need to decide between using skylight versus sentry for performance monitoring.\\n","completion":"## Decision\n[There was an attempt](https:\/\/github.com\/DFE-Digital\/teacher-training-api\/pull\/1860) to use sentry, but we managed to fill up our quota in less than a day, and while Sentry does offer tracing a subset of requests, this would only allow us to trace less than 1% of our requests.\\nAs such, we have decided to continue using skylight for performance monitoring.\\n"}
{"prompt":"## Context\\nTesting is an important component of a fast changing software. Without testing\\nit is not possible to guarantee that new components or changes to the current\\ncodebase do not break pre-existing functionality. The purpose of unit testing in\\nparticular is to ensure that isolated parts of the codebase work as expected by\\nthe developer. Tests should provide enough evidence to the developer that his\\ncontributions are working. It also aids other developers to understand the\\npurpose of each and every component.\\nIn order to reduce the burden of writing tests, we need good test management\\ntools that play well with `python` which is the main scripting language for this\\nproject.\\nThere are many testing framework for Python, including:\\n[`nosetests`](https:\/\/nose.readthedocs.io\/),\\n[`unittest`](https:\/\/docs.python.org\/3\/library\/unittest.html) and\\n[`pytest`](https:\/\/docs.pytest.org\/).\\nWe were previously using `nosetests`, which makes use of classic xunit-style\\nsetup whose main strength is easily declaring setup and teardown methods for\\neach testing function. While these methods are simple and familiar to those\\ncoming from a unittest or nose background,  pytest\u2019s more powerful fixture\\nmechanism leverages the concept of dependency injection, allowing for a more\\nmodular and more scalable approach for managing test state, especially for\\nlarger projects and for functional testing.\\nIn our particular case, we can leverage the power of fixtures to manage docker\\nimages and to define connection procedures. Ideally, all of the testing\\nrequirements are defined within the testing framework. Fixtures are really\\npowerful in providing powerful abstractions for resource allocation.\\n","completion":"## Decision\nWe will use pytest for testing Flowkit components.\\n"}
{"prompt":"## Context\\nResources have semantically a certain lifetime. That means, after a resources is created it will experience state-transition until it reaches the end of its lifetime (e.g. archival). The lifetimes and state-transitions need to be defined in an abstract way, in order to allow the platform to excel for different use-cases.\\n","completion":"## Decision\n### Lifetime of a single resource\\n```mermaid\\ngraph TD\\nA[Create resource] -->|Auto-assign to creator| B\\nB[State: Open] --> |Do work|B\\nB --> |Assign to somebody who can close| C\\nC[State: Open & Ready to close] --> |Close| D\\nD[Resource closed]\\n```\\nRe-Opening is not supported; it could be discussed as long as the parent is still open (hierarchy must hold). Different usage-scenarios are acommodated via _manual_ steps.\\n#### Example: 4-Eyes Principle\\nScenario: A subproject must only be closed when 2 people A & B (e.g. different departments) checked it.\\nPrereqs: Closing right is given to an administrative person and not to the people signing off\\nApproach:\\n1. Subproject is executed as usual, then for sign-off is assigned to Person A\\n2. Sign-Off Person A enters a Workflow-Item validating the state and assigns Person B\\n3. Sign-Off Person B enters a Workflow-Item validating the state and assigns to administrative personell that has the close-rights\\n4. Administrative person checks whether all required sign-offs are there and then finally closes; otherwise assigns back\\n### Impact to child resources\\nA resource hierarchically describes a node in a tree-like structure. Therefore we need to look at impact of a closed resource to other nodes in the tree.\\nIf a resource has no children, there is no impact on other resources.\\nIf a resource has children (e.g. subprojects \/ workflowitems), **the resource cannot be closed**. All children need to be in state closed before a parent can be closed.\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n"}
{"prompt":"## Context\\nThis is a new side project. It would be useful to capture the context behind past decisions.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nThe frontend side of the Digital Citizenship project consists of a mobile\\napplication named [italia-app](https:\/\/github.com\/teamdigitale\/italia-app).\\nThe `italia-app` application relies on a backend application ([italia-backend](https:\/\/github.com\/teamdigitale\/italia-backend))\\nfor intermediating the interaction with external services (e.g. the Digital\\nCitizenship APIs) and for coordinating the SPID authentication process.\\n","completion":"## Decision\nThe backend application (`app_backend`) is deployed as a Docker container\\ninside a Kubernetes cluster, provisioned by Terraform.\\n![img](0013-1.svg)\\n[source of diagram](https:\/\/www.planttext.com\/?text=XP7FIiGm4CRlynJp0cK5zo2oo887tSkoO0-2I4ZJsJPicyH9ilW7tzsaQbjwKCWXpFpovPlvjZv83h7l08oj2NMGdPP2EWTmPDaJolV899WQRGr-_60kLfrMGe_KALR4XW9veRhe0_78QjCmUIyyyLah6dMT4vLK9pArSBOUyLaTuFFtm6GCDqLHt4mMy1glBbRtPNdu6rglBmVg0M0gRpArSFWjMDuU_kUoPWPXsHhusIWpasdCGCYwQGFQ8pujdZu3xkzs-qTaEXFxj6kshs-0QQMzDZ9j68SfK5bZI8KK2o3Rc1jCBv5ym4fwgZ7brLeVLaw65hA7_370DbrJu5y0)\\nRequests coming from the app to the backend get routed through a few components:\\n1. An Azure public IP with a firewall configured to listen on ports 80 and 443\\n1. A [K8S Service](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/) that routes the ports 80 and 443 to the Ingress\\n1. A [K8S Ingress](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/ingress\/) that terminates the HTTPS connection and routes the request based on the HTTP `Host` and path\\n1. The `italia-backend` app.\\nFor details about the Service and the Ingress configuration, see\\n[ingress.yml](https:\/\/github.com\/teamdigitale\/digital-citizenship\/blob\/master\/infrastructure\/kubernetes\/ingress.yml).\\n"}
{"prompt":"## Context\\nABCI-server could expose its own RPC-server and act as a proxy to Tendermint.\\nThe idea was for the Tendermint RPC to just be a transparent proxy to the app.\\nClients need to talk to Tendermint for proofs, unless we burden all app devs\\nwith exposing Tendermint proof stuff. Also seems less complex to lock down one\\nserver than two, but granted it makes querying a bit more kludgy since it needs\\nto be passed as a `Query`. Also, **having a very standard rpc interface means\\nthe light-client can work with all apps and handle proofs**. The only\\napp-specific logic is decoding the binary data to a more readable form (eg.\\njson). This is a huge advantage for code-reuse and standardization.\\n","completion":"## Decision\nWe dont expose an RPC server on any of our ABCI-apps.\\n"}
{"prompt":"Context\\n=======\\nIt is expected for the government websites to be secure and keep the user\\ninteractions private. Because that we want to enforce all communications to\\nany application and to the platform endpoints to use only and always HTTPS,\\nas [it is described in the Gov Service Manual](https:\/\/www.gov.uk\/service-manual\/technology\/using-https).\\nWhen a user inputs a website name without specifying the\\nprotocol in the URL, most browsers will try first the HTTP protocol by default.\\nEven if the server always redirect HTTP to HTTPS, an initial\\nunprotected request including user information will be transferred\\nin clear: full URL with domain, parameter, [cookies without secure flag](https:\/\/en.wikipedia.org\/wiki\/HTTP_cookie#Secure_and_HttpOnly)\\nor browser meta-information.\\n[HTTP Strict Transport Security](https:\/\/en.wikipedia.org\/wiki\/HTTP_Strict_Transport_Security)\\nmitigates this issue by instructing modern browsers that support it to\\nalways connect using HTTPS.\\nThis is also a [requirement in the service manual](https:\/\/www.gov.uk\/service-manual\/technology\/using-https).\\nThere is still a potential initial unprotected HTTP request that might happen\\nbefore retrieve the HSTS headers or after the specified HSTS `max-age`.\\nTo solve this issue, the root domain can be added to\\n[HSTS preload list](https:\/\/hstspreload.appspot.com\/) which will be used by most\\ncommon browsers.\\nCurrently the only way to avoid any clear text HTTP interaction is closing or\\ndropping any attempt to connect to the port 80 at TCP level.\\nAlthough not all application deployed on the PaaS will be \"services\"\\nas in the service manual meaning, we must not allow HTTP to make\\nit easier to service owners to comply with this requirements.\\nDecision\\n========\\nWe will only open port 443 (HTTPS) and drop\/reject any TCP connection to TCP port 80 (HTTP).\\nWe will implement and maintain HSTS preload lists for our production domains.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe must configure and maintain our domain in the HSTS preload lists.\\nUsers of browsers which do not support HSTS, or HSTS preload lists, will not\\nbe able to connect to the sites without specify the protocol `https:\/\/` in\\nthe URL. This only happens when the user manually inputs the URL in the\\nbrowser.\\n","completion":"## Decision\n========\\nWe will only open port 443 (HTTPS) and drop\/reject any TCP connection to TCP port 80 (HTTP).\\nWe will implement and maintain HSTS preload lists for our production domains.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe must configure and maintain our domain in the HSTS preload lists.\\nUsers of browsers which do not support HSTS, or HSTS preload lists, will not\\nbe able to connect to the sites without specify the protocol `https:\/\/` in\\nthe URL. This only happens when the user manually inputs the URL in the\\nbrowser.\\n"}
{"prompt":"## Context and Problem Statement\\nSince there will be multiple services deployed in the Operate First environment\\n(ex. Jupyterhub, Argo, Superset, Observatorium, etc) distributed over various\\nnamespaces, we need to be able to monitor them.\\nTo do so we will use [Prometheus](https:\/\/prometheus.io\/).\\nThis will be deployed using the [ODH Operator](https:\/\/opendatahub.io\/) (ODH\\noperator includes the Prometheus Operator to do this).\\nNormally, Prometheus Operator would have been able to monitor services across\\nnamespaces using cluster-wide access, but the recent versions of Openshift have\\nan instance of Prometheus Operator pre-installed for cluster monitoring. As such we\\ncannot do a cluster-wide Prometheus Operator setup since this would conflict\\nwith the cluster monitoring instance.\\nWe also cannot use Operator Groups to do a multiple namespace installation,\\nsince selection of multiple target namespaces using operator groups might be\\n[deprecated in a future release](https:\/\/docs.openshift.com\/container-platform\/4.6\/operators\/understanding\/olm\/olm-understanding-operatorgroups.html#olm-operatorgroups-target-namespace_olm-understanding-operatorgroups).\\nOne possible solution is User Workload Monitoring, but it is still a\\n[feature preview in Openshift v4.5](https:\/\/docs.openshift.com\/container-platform\/4.5\/monitoring\/monitoring-your-own-services.html#enabling-monitoring-of-your-own-services_monitoring-your-own-services)\\nand no documentation is available for UWM in Openshift v4.6 yet.\\nSo the main question here is how do we structure the\\nPrometheus-Operator\/Prometheus Deployments in Operate First?\\n## Decision Drivers <!-- optional -->\\n* Access to cluster-wide permissions\\n* The Prometheus instance might not have cluster wide resource access\\n* Prometheus resource locations\\n* Where do we keep the monitoring resources like servicemonitors\/podmonitors?\\n* Whose responsibility is it to create these monitoring resources?\\n* Complexity of Prometheus deployments\\n* How many instances of Prometheus do we deploy?\\n* Which namespaces should these Prometheus instances be deployed in?\\n","completion":"## Decision\n* Access to cluster-wide permissions\\n* The Prometheus instance might not have cluster wide resource access\\n* Prometheus resource locations\\n* Where do we keep the monitoring resources like servicemonitors\/podmonitors?\\n* Whose responsibility is it to create these monitoring resources?\\n* Complexity of Prometheus deployments\\n* How many instances of Prometheus do we deploy?\\n* Which namespaces should these Prometheus instances be deployed in?\\nChosen option: Option 1, because:\\n* Set up is less complex than Option 2, only a single Prometheus instance is\\nneeded\\n* No clusterroles are required like Option 3, only roles to access specific\\nnamespaces are required\\n"}
{"prompt":"## **Context**:\\nPrimary Health checks were not checking if dependencies were primary or not before checking their health.\\nThis was not the intended behavior of the primary health check endpoint, as it should only check the primary dependencies of any given project.\\n","completion":"## Decision\nIn order to do this passively, we changed the dependency class to default to primary unless otherwise specified.\\nWe did this because we don't know if our consumers rely on the previous behavior of meta\/health checking dependencies if they were of unspecified importance.\\n"}
{"prompt":"## Context\\nAs part of the ACS with AWS Services initiative, we are looking to use more Amazon services around our Kubernetes deployment. To do this we have to find a way to provision Amazon services like S3 buckets, EKS Cluster, Amazon MQ, Aurora DB which are outside of our current helm deployment. We have investigated 3 options for doing this provisioning.\\nThe first option would be to use CloudFormation templates to do the provisioning.\\nCloudFormation would be in alignment with our AWS First company direction and it can allow us to provision all types of Amazon resources needed.\\nAn additional plus is that we have experience working with this tool within our team.\\nHowever, CloudFormation locks us to Amazon only services and makes us have separated tools for provisioning Alfresco Content Services and the adjacent resources.\\nThe second option is having Terraform as an outside of AWS provisioner.\\nTerraform allows us to provision and make use of services from different cloud providers in our solutions as well as totally unrelated services like Github, Consul, PagerDuty and more importantly Bare Metal on-prem provisioning. Terraform is also abstracting away a good part of the required metadata needed for the provisioning of resources.\\nHowever, we have limited experience in using terraform.\\nThe final option is using kubernetes controllers to deploy Amazon resources as part of the helm deployment for acs.\\nImplementing kubernetes controllers for dynamically provisioning resources along with the usual kubernetes deployment for Alfresco Content Services would make us more consistent in how we deploy our applications and would ease up maintenance in the future.\\nHowever, we would still need another way for provisioning the actual kubernetes cluster and our experience in developing custom resource definitions used in kubernetes controllers is inexistent.\\n","completion":"## Decision\nWe will use Amazon CloudFormation templates as it is in alignment with Alfresco's AWS First direction. Also, we have experience in developing and using this tool within the company. It also brings us closer to potentially having a quickstart template for deploying the Alfresco Digital Business Platform.\\n"}
{"prompt":"## Context\\nWe have seen a number of OutOfMemoryErrors with Solr, mostly related to garbage collection.  Our memory\\nsettings include many overrides of the defaults, and it's not clear what they do individually or as a group.\\nThese settings have also been in use for several years, and we're not sure if they are still a good match for\\nour hardware and usage.\\n","completion":"## Decision\nRemove the following memory tuning options from the Java options:\\n* `-Xss=256k`\\n* `-XX:NewRatio=3`\\n* `-XX:SurvivorRatio=4`\\n* `-XX:TargetSurvivorRatio=90`\\n* `-XX:MaxTenuringThreshold=8`\\n* `-XX:ConcGCThreads=4`\\n* `-XX:ParallelGCThreads=4`\\n* `-XX:PretenureSizeThreshold=64m`\\n* `-XX:CMSInitiatingOccupancyFraction=50`\\n* `-XX:CMSMaxAbortablePrecleanTime=6000`\\nAdd the following option to disable throwing an OutOfMemoryError if garbage collection takes too long:\\n* `-XX:-UseGCOverheadLimit`\\nDecrease total memory allocation from 72 GB to 40 GB.\\n"}
{"prompt":"## Context\\nWe need a way to communicate changes to the registers service to our users.\\nFor example: breaking changes to the API, or new registers that have become available.\\nWe have previously collected contact information, but not actually sent any emails.\\n","completion":"## Decision\nWe reviewed the [service manual guidance on sending email](https:\/\/www.gov.uk\/service-manual\/technology\/how-to-email-your-users).\\nWe reviewed the use of GOV.UK Notify for managing email subscribers, but found that it only handles the email sending part, and does not currently provide a way to manage lists of subscribers. The GOV.UK team at GDS did use Notify for this purpose, but they built extra software around it to manage subscriptions. This is too complex for our use case, and email is not central to our product, so we've decided not to use Notify right now.\\nWe decided to use mailchimp as it's a technology already in use by other teams at GDS, and the free tier can handle the number of subscribers we'll have in the forseeable future.\\nWe will continue to use Registers Frontend for the signup form, and use the API to integrate with mailchimp.\\nInstead of migrating existing subscribers, we'll send them an email with instructions on how to sign up again, so it's clear what they are signing up for.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context\\nWe need to record the product decisions made on this project, including the context, rationale and date. This will help maintain common understanding across the wider project team, and enable us to review past decisions when the context or the basis for a decision change.\\nSee Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's [adr-tools](https:\/\/github.com\/npryce\/adr-tools).\\n","completion":"## Decision\nWe will capture key Product Decisions and save them in an [accessible GitHub repository](https:\/\/github.com\/Crown-Commercial-Service\/CMp-Architecture-Decision-Records).\\nWe will not capture day-to-day decisions, but those material to the product direction, scope, or approach.\\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\nto store these decisions.\\nIn particular, we recognise that decisions can be proposed and accepted, as well as deprecated or superseded, as we continue to iterate and learn.\\nThe Product Manager(s) own the status and content of PDRs.\\n"}
{"prompt":"## Context\\neLife follows a publication process that is a well defined workflow for a submission to follow to completion.\\nThe editorial team are familiar with this process and commonly use the language in their day to day jobs.\\n[This workflow has been modelled](https:\/\/drive.google.com\/drive\/u\/0\/folders\/1gRuWuoI9KcEwfgNrFYVNDUiQ5YzK88de) as part of the development work.\\nA simple library (npm package) [javascript-state-machine](https:\/\/github.com\/jakesgordon\/javascript-state-machine) was used to codify this workflow. The code generates an image of the workflow: (manuscript-workflow)[https:\/\/github.com\/diversemix\/manuscript-workflow]\\n[Libero](https:\/\/github.com\/libero\/) also have had a requirement for workflow and have [decided on Airflow](https:\/\/github.com\/libero\/walking-skeleton\/blob\/master\/adr\/0003-workflow-system.md).\\nThis solution was also considered for the submission workflow.\\nHowever, out of the box there is no solution for user interactions.\\nIt is assumed that all tasks in the flow are processes performed on the server and not by a user.\\nThere are plugins to overcome this but it does seem like its not the right tool for the job.\\nTwo Other tools were also considered:\\n* [bpmn-engine](https:\/\/github.com\/paed01\/bpmn-engine)\\n* [node-workflow](https:\/\/github.com\/joyent\/node-workflow)\\nBoth of these also did not support the idea of user interaction fully.\\nThe `bpmn-engine` did have the concept of a userTask however this was treated as a distict entity separate from the `task` that forms the workflows.\\n","completion":"## Decision\nWhile it would be good to have a separate service that would have the responsibility to manage the submission's workflow in the system,\\nthere does not seem to be anything that would readily support this.\\nThe decision is to use the npm package `javascript-state-machine` as demo'ed above in the repo.\\nIt is envisaged that this will be used to extend the `Manuscript` object within `xpub-elife`\\n"}
{"prompt":"## Context\\nIt's useful to parse snippets of the rendered html to assert on them easier\\n(e.g. in transform.js unit tests).\\nThe xml parser we bundle with the app can technically parse html, but it doesn't\\nseem to support basic operations like `innerHTML`. That's a major nuisance.\\nJSDom is a mature and popular html parsing library.\\n","completion":"## Decision\nAdd [JSDom](https:\/\/www.npmjs.com\/package\/jsdom) as a dev dependency to parse html in test.\\n"}
{"prompt":"# Context and Problem Statement\\nCurrently there is a `Scheduler` that consumes tasks off a queue in\\nthe database. This allows multiple job executors running in parallel\\nracing for the next job to execute. This is for executing tasks\\nimmediately \u2013 as long as there are enough resource.\\nWhat is missing, is a component that maintains periodic tasks. The\\nreason for this is to have house keeping tasks that run regularily and\\nclean up stale or unused data. Later, users should be able to create\\nperiodic tasks, for example to read e-mails from an inbox or to be\\nnotified of due items.\\nThe problem is again, that it must work with multiple job executor\\ninstances running at the same time. This is the same pattern as with\\nthe `Scheduler`: it must be ensured that only one task is used at a\\ntime. Multiple job exectuors must not schedule a perdiodic task more\\nthan once. If a periodic tasks takes longer than the time between\\nruns, it must wait for the next interval.\\n# Considered Options\\n1. Adding a `timer` and `nextrun` field to the current `job` table\\n2. Creating a separate table for periodic tasks\\n","completion":"## Decision\nThe 2. option.\\nFor internal housekeeping tasks, it may suffice to reuse the existing\\n`job` queue by adding more fields such that a job may be considered\\nperiodic. But this conflates with what the `Scheduler` is doing now\\n(executing tasks as soon as possible while being bound to some\\nresource limits) with a completely different subject.\\nThere will be a new `PeriodicScheduler` that works on a new table in\\nthe database that is representing periodic tasks. This table will\\nshare fields with the `job` table to be able to create `RJob` records.\\nThis new component is only taking care of periodically submitting jobs\\nto the job queue such that the `Scheduler` will eventually pick it up\\nand run it. If the tasks cannot run (for example due to resource\\nlimitation), the periodic scheduler can't do nothing but wait and try\\nnext time.\\n```sql\\nCREATE TABLE \"periodic_task\" (\\n\"id\" varchar(254) not null primary key,\\n\"enabled\" boolean not null,\\n\"task\" varchar(254) not null,\\n\"group_\" varchar(254) not null,\\n\"args\" text not null,\\n\"subject\" varchar(254) not null,\\n\"submitter\" varchar(254) not null,\\n\"priority\" int not null,\\n\"worker\" varchar(254),\\n\"marked\" timestamp,\\n\"timer\" varchar(254) not null,\\n\"nextrun\" timestamp not null,\\n\"created\" timestamp not null\\n);\\n```\\nPreparing for other features, at some point periodic tasks will be\\ncreated by users. It should be possible to disable\/enable them. The\\nnext 6 properties are needed to insert jobs into the `job` table. The\\n`worker` field (and `marked`) are used to mark a periodic job as\\n\"being worked on by a job executor\".\\nThe `timer` is the schedule, which is a\\n[systemd-like](https:\/\/man7.org\/linux\/man-pages\/man7\/systemd.time.7.html#CALENDAR_EVENTS)\\ncalendar event string. This is parsed by [this\\nlibrary](https:\/\/github.com\/eikek\/calev). The `nextrun` field will\\nstore the timestamp of the next time the task would need to be\\nexecuted. This is needed to query this table for the newest task.\\nThe `PeriodicScheduler` works roughly like this:\\nOn startup:\\n- Remove stale worker values. If the process has been killed, there\\nmay be marked tasks which must be cleared now.\\nMain-Loop:\\n0. Cancel current scheduled notify (see 4. below)\\n1. get next (= earliest & enabled) periodic job\\n2. if none: stop\\n3. if triggered (= `nextrun <= 'now'`):\\n- Mark periodic task. On fail: goto 1.\\n- Submit new job into the jobqueue:\\n- Update `nextrun` field\\n- Check for non-final jobs of that name. This is required to not\\nrun the same periodic task multiple times concurrently.\\n- if exist: goto 4.\\n- if not exist: submit job\\n- Unmark periodic task\\n4. if future\\n- schedule notify: notify self to run again next time the task\\nschedule triggers\\n"}
{"prompt":"## Context\\nMarkdown is generally safer to use than HTML for user submitted content on the web because it limits what a user can submit to predefined allowable strings that can be easily sanitized and then get converted to HTML. Panoptes-Front-End uses markdown through out the entire application. Currently we support an in-house markdown renderer, [markdownz](https:\/\/github.com\/zooniverse\/markdownz), that uses  [markdown-it](https:\/\/github.com\/markdown-it\/markdown-it). The library markdown-it is mature and has several plug-ins available for it that we've added to markdownz as well as some of our own customizations.\\nMarkdown, however, isn't totally free from being exploitable, nor is React. Markdownz relies on a React method, `dangerouslySetInnerHTML` that potentially open us to vulnerabilities (see this line: https:\/\/github.com\/zooniverse\/markdownz\/blob\/master\/src\/components\/markdown.jsx#L99).\\nNow that we've adopted Grommet as general React component library, Grommet also provides a React [markdown](https:\/\/v2.grommet.io\/markdown) renderer ([code](https:\/\/github.com\/grommet\/grommet\/blob\/master\/src\/js\/components\/Markdown\/Markdown.js)). Grommet's markdown component uses [markdown-to-jsx](https:\/\/github.com\/probablyup\/markdown-to-jsx) which instead converts markdown to React components to use instead of relying on `dangerouslySetInnerHTML`. However, after extensive evaluation, `markdown-to-jsx` does not have the plugin eco-system that we need and so we would need to rewrite a lot of customizations to get to basic parity with what we already support. This defeats the purpose of reducing the maintenance of our own code for common markdown support.\\n","completion":"## Decision\nWe will make a new `Markdownz` React component that will be a part of the Zooniverse React component library. This new component will be built using [`remark`](https:\/\/github.com\/remarkjs\/remark). Remark is a popular markdown rendering library with a good plugin eco-system. It is supported by Zeit, which also supports Next.js, the server-side rendering library we have decided upon.\\nHere is how markdown-it's plugins will map to remark's plugins:\\n|markdown-it plugin\/custom plugin|remark plugin\/custom plugin|notes|\\n|--------------------------------|---------------------------|-----|\\n|markdown-it-emoji|remark-emoji|remark-emoji does not support emoticons like `:-)` but does gemojis like `:smile:`|\\n|markdown-it-sub|remark-sub-super||\\n|markdown-it-sup|remark-sub-super||\\n|markdown-it-footnote|built in|Remark supports this and can be enabled by passing `footnote: true` into its settings object|\\n|markdown-it-imsize|N\/A|This has been replaced by leveraging the component customization that remark-react supports. For `img`, we have defined a custom function that will set the `width` and `height` props on the Image component if the sizing syntax is defined in the alt tag of the markup. This is in contrast to the sizing syntax originally being defined in the src markup. We do not want to modify the sanitization remark-react does on source urls, so instead we have moved support of syntax to the alt tag area of the markup|\\n|markdown-it-video|deprecating|We are deprecating this because we don't want project owners embedding youtube videos with ads|\\n|markdown-it-table-of-contents|remark-toc|This works instead by looking for a heading that has case insensitive `table of contents`, `toc`, or `table-of-contents`|\\n|markdown-it-anchor|N\/A|Remark has basic anchor support.|\\n|twemoji|N\/A|Do we really need to use images of Twitter's emojis? Unicode support for emojis is fairly ubitiquous now.|\\n|markdown-it-html5-embed|N\/A|This has been replaced by leveraging the component customization that remark-react supports. For `img`, we define a custom function that returns HTML 5 video instead of an image of the src is a video mime-type or returns HTML 5 audio if the src is an audio mime-type|\\n|replaceSymbols|our own fork of remark-ping|remark-ping supports our needs for doing at-mentions of users, but it is forked to also support talk hashtags and the subject mentions using `^S`|\\n|relNofollow|remark-external-links|This plugin adds nofollow to absolute urls|\\n|markdownNewTab|remark-external-links|remark-external-links plugin adds `target='_blank'` and nofollow, noopener, noreferrer to all absolute urls. `+tab+` in front of the url will no longer work because of the sanitization that remark-react does. It may not be a good idea to modify how the sanitization works to allow this and instead just update our users on how this works instead.|\\n`remark-react` is added to parse the markdown to jsx which is inherantly safer than using `dangerouslySetInnerHTML` and allows customizing which react components get used for html elements like `markdown-to-jsx`.\\n"}
{"prompt":"Context\\n-------\\nThere are many cloud kubernetes offerings (Amazon, Microsoft, and Google\\nall offer it), and part of the appeal in general is to not have to think\\nabout where the servers are and what kind of maintenance they need.\\nHowever, we already operate servers in three datacenters, and we fully\\nmanage one of those. We may prefer this not to be the case, but as of\\nwriting this, we deal with bare-metal servers regardless of what we'd\\nprefer. Adding ec2 instances costs extra money, where running additional\\nsoftware on servers (with an electric bill we're already paying) costs\\nthe labor of setting it up.\\nThe attitude of the field so far appears to be \"definitely just run it\\nin AKS,\" but the field is largely made up of people whose primary\\ninterests are reliability and profits. As a digital academic library, we\\nalso have those interests, but we have an additional responsibility to\\nalways have our own copy of everything.\\nDecision\\n--------\\nWe will provide kubernetes clusters at our datacenters with the\\npossibility of expanding outward into cloud providers as needed.\\nConsequences\\n------------\\nBy providing a service that mirrors a cloud service, we can use the same\\ncode and configuration to deploy locally and remotely. We'll need to\\nfigure out how we want to manage applications and how we'll want them to\\nscale depending on where they live. Since developers will essentially be\\ndeploying to three different clusters, we'll need to decide how they\\nshould do that.\\n","completion":"## Decision\n--------\\nWe will provide kubernetes clusters at our datacenters with the\\npossibility of expanding outward into cloud providers as needed.\\nConsequences\\n------------\\nBy providing a service that mirrors a cloud service, we can use the same\\ncode and configuration to deploy locally and remotely. We'll need to\\nfigure out how we want to manage applications and how we'll want them to\\nscale depending on where they live. Since developers will essentially be\\ndeploying to three different clusters, we'll need to decide how they\\nshould do that.\\n"}
{"prompt":"## Context\\nThe Modernisation Platform needs to be able to communicate privately with various other areas of the MoJ. The main use cases are as follows:\\n- Access from various MoJ sites to the platform\\n- Access from VPNs for remote devices to the platform\\n- Connectivity from a business units existing hosting to the platform during migration\\n- Connectivity from a business units existing hosting to the platform for applications not yet migrated\\n- Connectivity to other MoJ hosting platforms\\nThis connectivity between different networks is provided by the MoJ Transit Gateway. In order to enable to required routing we need to ensure that our IP range allocation is unique to avoid overlaps or clashes.\\nOur current IP ranges are not being managed centrally and we have identified some clashes with existing networks.\\n","completion":"## Decision\nThe Modernisation Platform will be allocated IP ranges from the Network Operations Team and Vodafone who maintain the MoJ record of IP ranges. This will avoid clashes with any existing or future infrastructure registered in the same way.\\n"}
{"prompt":"## Context and Problem Statement\\n`play-frontend-govuk` and `play-frontend-hmrc` provide Scala \/ Play \/ Twirl implementations of the components provided\\nas Nunjucks in `govuk-frontend` and `hmrc-frontend`, using the assets provided by those libraries. How much should the\\nplay-frontend implementations diverge from their \u201cbase\u201d repositories?\\n## Decision Drivers\\n* Need to create frontend components that are easy to use by Scala developers on the platform\\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\\n`hmrc-frontend` are released\\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\\n","completion":"## Decision\n* Need to create frontend components that are easy to use by Scala developers on the platform\\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\\n`hmrc-frontend` are released\\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\\nChosen option: Option 3, because it allows for quick and continuous upgrading to follow GDS and HMRC design system\\nchanges, allows for a robust testing strategy of multiple implementations of the templates thanks to YAML provided by\\nGDS.\\n### Positive Consequences\\n* Design of case classes to follow GDS \/ HMRC design system means PlatUI as library maintainers do not have to create\\nviewmodel structure from scratch every time\\n* Adding new components can follow a clear and straightforward path\\n* Robust test strategy can be developed using Nunjucks components and Twirl templates using a parser\\n### Negative Consequences\\n* Feedback suggests that some developers do not find the API to be intuitive\\n* Separate decisions need to be made on handling multilingual support\\n* Enrichment of library needs to be done via separate Twirl helpers,\\nsee [related ADR](..\/adr\/0001-play-frontend-hmrc-mirrors-hmrc-frontend.md)\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nTerraform allows to pin the spcific versions of providers required for this module.\\n","completion":"## Decision\nThis module will not enforce maximum versions contraints for all required providers\\n"}
{"prompt":"## Context\\nIf you look at our documentation, it\u2019s all over the place. Install it in Docker, in a VM, on a NAS or on one of the many Linux distributions.\\nThe reason we have this many guides is that since the start of the Home Assistant website, we have always gladly accepted every contribution to get Home Assistant running on any platform. The more the merrier!\\nHowever, in software, nothing ever stays the same. All software gets updates to fix bugs, fix security vulnerabilities, improve performance or to add new features. As a software application you need to grow along or else you get stuck with an insecure system.\\nSo as Home Assistant grows and evolves, some of these installation guides became outdated and stopped working. We wouldn\u2019t even know it was broken until a user raised an issue. But when they do, we wouldn\u2019t know how to fix it unless we could get a hold of the original contributor.\\nThis can be frustrating. Any guide on our official website should lead to a working system. A system that not only works today, but also tomorrow.\\n","completion":"## Decision\nA supported installation method in the Home Assistant context means:\\n> A way of installing and running Home Assistant in a way that is supported by the Home Assistant developers. Supported means the installation method is tested and documented in the official documentation. Running Home Assistant using such a supported method, leads to the optimal user experience.\\nThe Home Assistant team will not prevent you from running Home Assistant using an unofficial method. However, we cannot help with issues that you encounter. We are open for contributions that improve compatibility with a community-supported method as long as they do not impact officially supported methods, add a significant amount of code exceptions or future maintenance burden on the Home Assistant development team.\\n"}
{"prompt":"## Context\\nWe want to think Marsha as accessible from the beginning. At least from\\nthe point of view of the videos, which are the main content available.\\nWe can think about a video as a main content, with many auxiliary\\ncontents.\\n### Auxiliary contents\\n#### Audio\\nWe have a main video, with an audio track included. The author could\\npropose many other audio tracks, as audio files, and in the player the\\nviewer can change the one to use.\\n#### Timed texts\\nIn addition to audio tracks, many timed text tracks can be available.\\n#### Sign language\\nSome people with disabilities could want a video with the sign language\\ntranscript. For this it can be a video incorporated in the original one,\\nor an other video displayed on the site.\\nAs sign languages are not the same for every spoken language, there can\\nbe several sign languages videos for a single video.\\n","completion":"## Decision\nWe decided to take all these elements into account right from the\\nbeginning.\\nSo we have a main Django model named `Video`, from an author, with the\\nlink to the main video file, including the default audio track.\\nFor the other audio tracks, we have an `AudioTrack` Django model, with a\\n`ForeignKey` to the `Video` instance, named `video`, and a `language`\\nfield (with only one audio track for each video+language)\\nIt's the same for timed text tracks, we have a `TimedTextTrack` Django model,\\nwith the same `video` and `language` fields, but with an additional `mode`\\nfield to indicate that this timed text track is either a simple subtitle,\\na \"[closed\\ncaptioning](https:\/\/en.wikipedia.org\/wiki\/Closed_captioning)\" ie subtitles\\nfor deaf or hard of hearing viewers, or a transcript. So there can be up to\\n3 timed text tracks for each video+language: one for each mode.\\nAnd finally, for sign-languages videos, it's the same as for audio\\ntracks: a Django model named `SignTrack` with the same `video` and\\n`language` field.\\n"}
{"prompt":"## Context\\nPresently all account management functionality is accessed through profile.theguardian.com, backed by the identity project within [frontend](https:\/\/github.com\/frontend) and the [identity-frontend](https:\/\/github.com\/guardian\/identity-frontend) projects.\\nThe code relating to account management lives in the front end, and is occasionally referred to as _manage my account_ or _mma_. It is an odd hybrid of hand made SPA and server-side template. Adding functionality is difficult, and testing new functionality is time consuming.\\nThis cannot handle the complexity of subscription management, which is instead handled by [subscriptions-frontend](https:\/\/github.com\/guardian\/subscriptions-frontend).\\nEach of Membership, Contributions and Subscriptions have a page within this, and all code around this makes the assumption that no more than one of each category can be held by a user. For subscriptions, there is no such restriction on purchase.\\nBasic functionality is missing from the user management pages.\\nA great deal of account management functionality also lives within Salesforce, and there is significant duplication of effort here.\\n","completion":"## Decision\nWe will develop a new front end to provide access to user management functionality. Relevant parts will be accesible using Salesforce authentication to minimise duplication of systems.\\n"}
{"prompt":"## Context\\nThe apps need to be hosted on a custom domain, behind CloudFront\\n","completion":"## Decision\nWe will create a cdn-route on GPaas, which will create a CloudFront endpoint with the custom domain\\nThis will also generate the TLS certificates\\n"}
{"prompt":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n","completion":"## Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\nWe are going to use three main patterns for our development process:\\n* [Forking](https:\/\/docs.github.com\/en\/free-pro-team@latest\/github\/getting-started-with-github\/fork-a-repo)\\n* [Gitflow](https:\/\/www.atlassian.com\/git\/tutorials\/comparing-workflows\/gitflow-workflow)\\n* [Commit](https:\/\/www.conventionalcommits.org\/en\/v1.0.0\/)\\nThe first one relates to **how we host our development code**. Based on this ADR, you should not create branches in the `origin` remote. Instead, create your fork and make your branches there. This decision avoids many unwanted side effects and accidents.\\nYou can create branches at `origin` if those branches are long-lived and relate to a newer version or a massive work from multiple developers. There are some default branches all our repositories might have:\\n* `main` (default branch)\\n* `develop` (next stable release branch)\\n* `test` (working environment for testing purposes)\\nThe second one relates to naming and Git workflow. Our branches naming should follow Gitflow standards, like: `feature\/<sub-path>\/<name>`, and others.\\nAlso, the `develop` branch must remain stable, receiving only new finished features in it.\\nThe third one relates to linting our commits. We should learn how to write useful commit messages, using that convention, to improve our developer experience.\\n"}
{"prompt":"#### Context and Problem Statement\\nA technology stack is required for the projects data access application programming interface (API).\\n#### Considered Options\\n- ASP.NET Web API (C#)\\n#### Decision Outcome\\nChosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n","completion":"## Decision\nChosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n"}
{"prompt":"## Context\\nWe need to provide a secure way for users to authenticate to interact with cluster resources.\\nThere are four different roles identified based on need:\\n| Cluster Role | Need |\\n|---|---|\\n| deployer | ability to make changes to cluster and full access to AWS resources (for CI) |\\n| admin | ability to make changes to cluster resources, and restricted access to AWS resources |\\n| sre | read only access to all cluster resources |\\n| dev | read only access to resources potentially scoped to a namespace |\\n","completion":"## Decision\nWe will authenticate all users to IAM roles via the [aws-iam-authenticator](https:\/\/github.com\/kubernetes-sigs\/aws-iam-authenticator) and map those IAM roles to [ClusterRoles](https:\/\/kubernetes.io\/docs\/reference\/access-authn-authz\/rbac\/) within the GSP cluster.\\nWe will store the mapping of IAM user ARN to Cluster Role in Github so that it can be verified. [gds-trusted-developers](https:\/\/github.com\/alphagov\/gds-trusted-developers)\\n"}
{"prompt":"# Context #\\nThe library currently assumes that the ID tokens are signed by a\\npublic key (and the code unconditionally tries to build the actual\\npublic key from the data we get from the ID Token issuer).\\nAt the same time, the OpenID Connect Core 1.0 specification states\\nthat:\\n> \"Symmetric signatures MUST NOT be used by public (non-confidential)\\n> Clients because of their inability to keep secrets\"\\n(see https:\/\/openid.net\/specs\/openid-connect-core-1_0.html#Signing).\\nAs we don't expect to support confidential clients, it seems safe to\\nignore Symmetric Keys when validating tokens.\\n# Decision #\\nTokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n","completion":"## Decision\nTokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n"}
{"prompt":"## Context\\nThere are three main things the package must do:\\n1. give useful feedback if the input is in an invalid format\\n2. transform the input into an HTML file (including embedded CSS & JavaScript)\\n3. that HTML file must show the links and respond correctly to user input.\\nItems #1 and #2 are very easy to test, item #3 is not.\\n","completion":"## Decision\nMinimize the amount of code that must be tested in the browser. Do as much as\\npossible in the build step and as little as possible in the browser.\\n"}
{"prompt":"## Context and Problem Statement\\nWe need to have an easy way to deploy this server for it to be runnable on the most various environments.\\n## Decision Drivers sorted by priority\\n- Easily installable and runnable on various environments\\n- Easily deployed\\n- Maintainable\\n","completion":"## Decision\n- Easily installable and runnable on various environments\\n- Easily deployed\\n- Maintainable\\nChosen option: **\"NextJS server side rendering solution for React\"**.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use a modified version of Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). ADR documents that are specific to this project will be stored in the `docs\/adr` directory within this repository. Instead of using the \"deprecated\" and \"superseded\" status value, we will move ADRs that are no longer applicable into the `docs\/adr\/archive` directory in this repository.\\n"}
{"prompt":"## Context\\nIn the Modernisation Platform, we want to reduce the blast radius and increase the scalability of how we create, maintain, and support applications in the context of what AWS account(s) they sit within.\\n","completion":"## Decision\nWe've decided to use a multi-account strategy, split by application. We have a complete write-up as part of our [environments concept](https:\/\/user-guide.modernisation-platform.service.justice.gov.uk\/concepts\/environments\/).\\n"}
{"prompt":"## Context\\nWriting code that adheres to style guides and other best practices can be\\nannoying. We want to standardize on some best-in-class tools.\\n","completion":"## Decision\nWe will use isort, black, and flake8.\\n"}
{"prompt":"## Context\\nAn application is needed for real time production error discovery and reporting. Sentry is currently being\\nused by DLS for various applications.\\n","completion":"## Decision\nWe will use Sentry for application exception monitoring.\\n"}
{"prompt":"## Context\\nWe need to protect the allocation API with authentication, but we'd rather not\\nhave to come up with an approach to do that ourselves from scratch.\\nThe new [NOMIS OAuth2 server](https:\/\/github.com\/ministryofjustice\/nomis-oauth2-server)\\nis already being used in production for authentication on almost all of the\\nNOMIS APIs and some other APIs built in Sheffield. We will need to use it to\\nauthenticate with the Custody API, and the other services which may need to use\\nthe allocation API are very likely to already be using this authentication\\nmethod for the other APIs they use.\\nClients can use one token (of a particular grant type) to authenticate with all\\nAPIs which use the NOMIS OAuth2 server, which makes things simpler for all\\nthose services - they don't have to work with multiple different authentication\\napproaches.\\nThe NOMIS OAuth2 server uses JWTs signed with a private key, so relying\\nservices can verify the integrity and authenticity of tokens presented by\\nclients using the corresponding public key.\\nWe've decided that the allocation manager will be entirely responsible for user\\naccess control and will call other APIs directly, and the allocation API will\\nbe a smaller interface onto its data (see [ADR 0010](0010-allocation-api-has-less-responsibility.md)).\\nThat means that the allocation API doesn't need to know which user it's\\nreturning data for, and we can use a system-to-system approach to\\nauthentication.\\nWe don't know of any other shared approaches to API authentication which are\\nused in the prison space.\\n","completion":"## Decision\nWe will use the NOMIS OAuth2 server for authentication on the allocation API.\\nWe will use the client credentials OAuth2 grant type for authentication on the\\nallocation API.\\nWe will verify signatures on presented tokens in the allocation API.\\nWe will respect expiration times on presented tokens in the allocation API.\\n"}
{"prompt":"## Context\\nWhilst tools like [Dependabot](https:\/\/github.blog\/2020-06-01-keep-all-your-packages-up-to-date-with-dependabot\/) simplify the process of testing updates to a repository's package dependencies, there can still be signficant coordination work to apply such changes and have the resulting updates cascade through the rest of your software estate.\\nBefore committing a dependency update there are several considerations:\\n* What type of [semantic version](https:\/\/semver.org) increment is this change?\\n* How confident are we in the repository's tests for catching breaking changes in dependencies?\\n* How reliable is the publisher's SemVer attribution?\\n* Should this dependency update trigger a new release?  If so, how should such updates be batched?\\n* To what extent could this update trigger a cascading update across other repositories?\\nThis ADR describes an automated process that can reduce the burden associated with such updates and their cascading effects, whilst still providing control mechanisms to prevent unrestricted updates.\\n**NOTE**: *The process, as described, assumes the use GitHub Dependabot (as opposed to the dependabot.io service that has slightly different functionality)*\\n","completion":"## Decision\nThe process for approving, merging and releasing these types of updates can be automated so long as suitable control measures are in-place to manage the different package promotion requirements.\\n### Concepts\\nA number of terms are used to describe the process, they are defined as follows:\\n* `semver-increment`: The scale of a given update as indicated by the change in the semantic version (i.e. patch, minor, major)\\n* `auto-approve`: The process of a CI\/CD bot approving a pull request\\n* `auto-merge`: The process of a pull request being merged by a bot, once in a mergeable state (e.g. passing checks, approved etc.)\\n* `auto-merge-candidate`: A pull request that is approved for `auto-merge`\\n* `auto-release`: The process of triggering the release pipeline of the consuming project (e.g. creating a git tag)\\n* `auto-release-candidate`: A pull request that is approved for `auto-release`\\n* `no-release`: An override mechanism for suppressing the `auto-release` behaviour\\n* `release-pending`: The state a pull request is in when it is approved for `auto-release`\\n### Principles\\nThese set out the core requirements for the process we wish to automate.\\n1. Dependabot updates can utilise `auto-approve` and `auto-merge` based on an allow list of package names\\n1. Dependabot updates otherwise approved for `auto-merge` can be opted-out based on their `semver-increment` - by default, 'major' changes should be ignored by this process\\n1. Dependabot updates approved for `auto-merge` can additionally utilise `auto-release`, based on an allow list of package names\\n1. Regular pull requests must never utilise `auto-approve`\\n1. Regular pull requests will, by default, utilise `auto-release`\\n1. Regular pull requests may opt-out of `auto-release`\\n1. All `auto-release-candidate` pull requests must be batched together, to avoid unnecesary release churn\\nA process that implements these mechanisms is illustrated in the following flowchart diagram:\\n![pr-autoflow: High-Level Process Flow][flowchart]\\n### Implementation\\nThe current implementation consists of the following components:\\n* a PowerShell module [`Endjin.PRAutoflow`](\/module) - implements the pull request title parsing and version comparison logic\\n* 2 x GitHub Actions (docker-based)\\n* [`dependabot-pr-parser`](\/actions\/dependabot-pr-parser) - given a PR title, it identifies Dependabot pull requests that meet the specified naming & `semver-increment` criteria and extracts the required metadata\\n* [`dependabot-pr-watcher`](\/actions\/dependabot-pr-parser) - given a list of PR titles, it returns those that meet the specified naming criteria\\n* 3 x GitHub Actions workflows:\\n* [`dependabot_approve_and_label`](https:\/\/github.com\/endjin\/.github\/workflow-templates\/dependabot_approve_and_label.yml)\\n* [`auto_merge`](https:\/\/github.com\/endjin\/.github\/workflow-templates\/auto_merge.yml)\\n* [`auto_release`](https:\/\/github.com\/endjin\/.github\/workflow-templates\/auto_release.yml)\\nThese are used to identify `auto-merge-candidate` and `auto-release-candidate` pull requests and perform the required processing on them.\\nThe Pull Request title parsing expects the following convention:\\n```\\nBump <PackageName> from <FromVersion> to <ToVersion> in <SolutionName>\\n```\\n### Limitations\\nThe current implementation is tailored to the NuGet eco-system which results in the following limitations:\\n* The pull request title parsing is unlikely to work for Dependabot pull requests created for other eco-systems\\n* The [`auto_release`](https:\/\/github.com\/endjin\/.github\/workflow-templates\/auto_release.yml) workflow utilises [GitVersion](https:\/\/gitversion.net\/docs\/) (a cross-platform .Net tool) to generate the version number used to create the release tag, which implies certain expectations in terms of git usage\\n"}
{"prompt":"## Context\\nThe infrastructure monitoring and alerting platform consists of a number of services deployed as docker containers. So far these containers have been running on ECS via Fargate, chosen because of the relative ease with which it allows us to get instances provisioned.\\nAs the solution has grown, and the interactions between new services have become more complex, we have found that we are running up against Fargate's limitations and require finer grained control over our deployments.\\nKubernetes is the industry standard platform for orchestrating and running container based deployments of servies, and provides considerably more flexibility in comparison to ECS and Fargate.\\n","completion":"## Decision\nStarting with Prometheus and Thanos, we are migrating our services over to AWS's managed kubernetes offering - EKS.\\n"}
{"prompt":"## Context\\nA workflow determines which subject viewer to use with the `workflow.configuration.subject_viewer` property. Subject viewers include `singleImage`, `lightCurve`, `multiFrame`, and `subjectGroup` at creation of this ADR. Some subject viewers can utilize, or require, additional configuration information.\\n### [multiFrame](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/components\/Classifier\/components\/SubjectViewer\/components\/MultiFrameViewer)\\nA workflow using the multi-frame subject viewer might have a preference regarding:\\n- marks per frame: some might prefer marks filtered per frame, like a transcription workflow where each frame represents a unique page to transcribe with marks only relevant to each page, while other workflows might prefer marks persist between frames like [Space Warps](https:\/\/www.zooniverse.org\/projects\/aprajita\/space-warps-hsc\/classify) or [Power to the People](https:\/\/www.zooniverse.org\/projects\/alycialeonard\/power-to-the-people). However, after reviewing projects that enabled marks to persist between frames (`multi_image_clone_markers` [in PFE](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/blob\/master\/app\/classifier\/tasks\/drawing\/markings-renderer.cjsx#L55)) it appears the PFE setting is unclear, as a few of the related workflows do not include a drawing task or the subjects do not have multiple frames.\\n- positioning: some might prefer pan, zoom, and rotation reset per frame, like a transcription workflow where each frame represents a unique page to transcribe, while other workflows might prefer pan, zoom, and rotation maintained between frames, like [Wildcam Gorongosa](https:\/\/www.zooniverse.org\/projects\/zooniverse\/wildcam-gorongosa\/classify) or [Backyard Worlds](https:\/\/www.zooniverse.org\/projects\/marckuchner\/backyard-worlds-planet-9\/classify) (in flipbook mode, not separate frames)\\n### [subjectGroup](https:\/\/github.com\/zooniverse\/front-end-monorepo\/tree\/master\/packages\/lib-classifier\/src\/components\/Classifier\/components\/SubjectViewer\/components\/SubjectGroupViewer)\\n- A workflow using the subject-group subject viewer might want to define the subject cell width, height, or style, or the subject viewer grid columns or grid rows.\\n","completion":"## Decision\nSubject viewer configuration is an object stored in `workflow.configuration.subject_viewer_config` and is structured as follows for the subject viewers noted:\\n`multiFrame`:\\n```javascript\\n{\\nfilter_marks_per_frame: <boolean>  \/\/ replaces multi_image_clone_markers in PFE\\npositioning: <enumerable> \/\/ includes pan, zoom, and rotation, enumerable i.e. \"maintain\", \"reset\"\\n}\\n```\\n`subjectGroup`:\\n```javascript\\n{\\ncell_width: <number of pixels>\\ncell_height: <number of pixels>\\ncell_style: { [CSS property]: <CSS property value> }\\ngrid_columns: <number>\\ngrid_rows: <number>\\n}\\n```\\nSubject viewers to define the configuration object in the related subject viewer README, if applicable.\\nThe ScatterPlotViewer and BarChartViewer accept a configuration object directly in the JSON structure of the subject data to support variability in the display settings including the plot labels since it's possible this might vary per subject. The `subject_viewer_config` object should only be used for  configuration options that apply to all subjects linked to the workflow.\\n"}
{"prompt":"## Context\\nAs described in [2. Use Node.js](0002-use-node-js.md), the goal is to make\\nreasonable choices without taking too much time.\\n","completion":"## Decision\nUse familiar tools whenever possible.\\n"}
{"prompt":"## Context\\nWe want to be able to style our components in a way that reduces chances of\\nclashes, or unwanted styles.\\nWe could use a convention like [BEM](http:\/\/getbem.com\/), but they are generally\\nhard to enforce.\\n[`styled-jsx`](https:\/\/github.com\/zeit\/styled-jsx) is an alternative that scopes\\nstyles to individual components, and is integrated into Next.js by default. We\\ncan still fall back to global stylesheets if required (for instance if we are\\nusing GOV.UK styles).\\n","completion":"## Decision\nWe will use `styled-jsx` for internal component styling.\\n"}
{"prompt":"## Context\\n### Background on Compliance Profile Storage and API\\nCompliance profiles are stored and accessed using a base name that serves as a namespace and a profile name, and, optionally, a version. Here's an example of how a profile is specified using the [audit coobkook](https:\/\/github.com\/chef-cookbooks\/audit):\\n```ruby\\ndefault['audit']['profiles'].push(\\n# Profile from Chef Compliance\\n{\\n'name': 'linux',\\n'compliance': 'base\/linux'\\n},\\n# Profile from Chef Compliance at a particular version\\n{\\n'name': 'linux-baseline',\\n'compliance': 'user\/linux-baseline',\\n'version': '2.1.0'\\n})\\n```\\nIn the example above, two profiles are specified. The first is named `linux` and is stored in the `base` namespace. The second is named `linux-baseline` and stored in a namespace called `user`.\\nIn A2, the API supports creating, reading, and deleting profiles (profiles are immutable, so there is no update).\\nHere's the read API from the gateway:\\n```proto\\nrpc Read (ProfileDetails) returns (Profile) {\\noption (google.api.http) = {\\nget: \"\/compliance\/profiles\/read\/{owner}\/{name}\/version\/{version}\"\\n};\\noption (chef.automate.api.policy) = {\\nresource: \"compliance:profiles:storage:{owner}\"\\naction: \"read\"\\n};\\noption (chef.automate.api.iam.policy) = {\\nresource: \"compliance:profiles:{owner}\"\\naction: \"compliance:profiles:get\"\\n};\\n};\\n```\\nThe delete REST API follows the same URL path.\\n### Current Usage and Authorization for Compliance Profiles\\nIn A2 today, each user has an implicit compliance profile namespace mapped to their A2 username. Users can create and delete profiles within their own username-based namespace. Default authorization policy allows users to read any profile. The username being used in profile URLs does not include the authentication connector (e.g. local, LDAP, SAML). As a result, the username-based namespace is not unique across auth connectors and the system is unable to distinguish between a local user \"olivia\" and an LDAP user \"olivia\".\\nWhen we tested an improvement to the username-based namespace approach with SAML users, we identified a bug in the system in which the auth system and UI have a disagreement about which field to use as \"username\". For SAML users, this prevented the username-based namespace for compliance profiles to function properly.\\n","completion":"## Decision\nWe will treat the base name of compliance profiles as a generic namespace.\\nA2 admins will create compliance profile namespaces and corresponding IAM policies that will determine which users are allowed to create, read, and delete profiles in a given namespace. In future, we can extend the system to support user creation and implicit ownership of profile namespaces, but this will not be supported initially.\\nWe will represent compliance profile namespaces in the system (not just implicitly via IAM policies). This will allow listing profile namespaces and copying a profile from one namespace to another (note that this is a reference copy as internally profiles are stored by content SHA256).\\nWe will address migration of current deployments through default policy behavior, documentation, and suggested workflow for A2 admins. The outline for migration is:\\n* On upgrade, all users can READ profiles in any namespace.\\n* On upgrade, only admin users can CREATE and DELETE profiles (in any namespace). All non-admin users have read-only access -- even for profiles that they previously created.\\n* Documentation will walk admin users through the process of creating compliance profile namespaces along with IAM policies to control access to those namespaces.\\n* Documentation will walk admin users through the process of consolidating existing username-based namespaces via copy operation.\\n* Documentation will walk admin users through process of using existing username-based namespaces and restoring access by adding IAM policies (not recommended approach). This is only relevant for customers who maintain a strong desire to maintain the existing behavior and layout for profile management.\\n"}
{"prompt":"## Context\\nCurrent on-chain approach to handle demands and certificates matching is not scalable enough, prone to front-running and hard to extend with new features.\\n","completion":"## Decision\nThe change provides a new way of trading\/matching certificates using off-chain order book matching engine.\\n"}
{"prompt":"## Context\\n- Application logs needs to be aggregated and presented back to the dev team to assist in monitoring and debugging of the live service.\\n- [DfE Digital have technical guidance expressing a preference for Logit](https:\/\/github.com\/DFE-Digital\/technical-guidance\/blob\/8380ad9dbfeefaeece081cace9f13e4c36200cd0\/source\/documentation\/guides\/default-technology-stack.html.md.erb#L93)\\n- We were prompted for a Logit account by DfE when setting up our GPaaS account\\n- GPaaS does provide access to logs but it is clumsy to access each environment over by using the CLI\\n- dxw have used other logging aggregators in the past such as Papertrail but as DfE have expressed a preference it makes sense to align our technical tooling\\n","completion":"## Decision\nUse Logit\\n"}
{"prompt":"## Context\\nWhen the library was originally built, an extension method on `IServiceCollection` was provided to make it simple to add library components to the dependency injection container.\\nOver time, additional components have been added to this single extension method. As a result, it is misnamed; it's currently called `AddJsonSerializerSettings` but it adds:\\n- Endjin's standard `IJsonSerializerSettingsProvider` implementation\\n- Property bag support via `JsonNetPropertyBagFactory`\\n- Several `JsonConverter` implementations\\nThe problem described in https:\/\/github.com\/corvus-dotnet\/Corvus.Extensions.Newtonsoft.Json\/issues\/145 has highlighted the fact that not all clients require all of those different things to be registered. For example, the referenced issue would not happen if it weren't for the combination of our standard `JsonSerializerSettingsProvider` (which sets `DateParseHandling` to `DateParseHandling.DateTimeOffset`) and the inclusion of the `DateTimeOffsetConverter`, which was originally implemented as a way of ensuring `DateTimeOffset` values serialized into CosmosDb remained sortable and filterable.\\nAs a result, it would be useful for clients to be able to register the particular components they need from those provided by the library.\\n","completion":"## Decision\nWe will modify the `JsonSerializerSettingsProviderServiceCollectionExtensions` class to provide separate methods to register the various components.\\nThe existing `AddJsonSerializerSettings` method will be deprecated via the `ObsoleteAttribute` and replaced with multiple separate methods.\\n"}
{"prompt":"## Context\\nGitHub Pages is a perfectly good static site host, however, not having the\\nability for preview builds is problematic when wishing to test changes prior to\\nthem being merged into the main branch. Netlify has the ability to create\\npreview builds for each PR or even each branch along with a number of other\\nbenefits over GitHub Pages as can be seen in this (totally unbiased!)\\n[comparison](https:\/\/www.netlify.com\/github-pages-vs-netlify\/).\\n","completion":"## Decision\nThe decision is to move hosting and deployment onto Netlify\\n"}
{"prompt":"## Context\\nCurrently we are using a traditional RDBMS, PostgreSQL as the storage of all the CI\/CD state.\\nThe DB is the one and only state of the whole CI\/CD cluster, storing all of the pipeline, resource, artifact definitions, runs, logs etc. For all of these, tracking historical changes is of utmost importance.\\nUsing a traditional CRUD workflow with PostgeSQL raises the following issues:\\n- Analytics are really hard as the Storage and Querying is coupled and a heavy query slows the whole cluster down\\n- Central locking is a severe impediment to scale\\n- When a change occurs in the pipeline, resource provider or artifact store definition its quite difficult to track these changes across time for audit and rollback needs\\n- To debug failures we need to have extended historical logging and is difficult to trace back in an update-in place system\\n- The tabular structure comes in our way with rigidity and is not simple to introduce schema changes\\n- Need of elaborate migration strategies which is further complicated with distributed state\\n","completion":"## Decision\nBased on the above facts the following is decided:\\n- Use [XTDB](https:\/\/xtdb.com) as the temporal, document store for the following reasons:\\n- Though being newer compared to [Datomic](https:\/\/www.datomic.com\/) its free and open source and ready for production use\\n- Has a quite unbundled design and uses a variety of storage backends and is transactor free\\n- Is [bi-temporal](https:\/\/xtdb.com\/articles\/bitemporality.html) and thereby offering more systematic analytical queries across time\\n- Has an HTTP interface for non JVM clients\\n- Use it with JDBC\/PostgreSQL backend which is quite readily available and managed in all popular cloud providers\\n- Remove the CRUD way of doing things, expose the DB too via API for more powerful, direct analytical querying\\n"}
{"prompt":"## Context\\nThe current data model assumes all inputs to arrive via a single\\ncollection of `(e a v)` tuples. Although this is very flexible, we are\\nfacing three major problems:\\n(1) Reading data from sources other than Datomic (e.g. Kafka or files\\non disk) is awkward, as those sources usually keep data in separate\\nrelations (e.g. Kafka topics). Reading across sources also invalidates\\nthe assumption of a single time axis on which all inputs reside.\\n(2) Inflexible arrangements. The hard assumption on a single input\\ncollection is reflected in similarly hardcoded indexing structures\\n(the `DB` and `ImplContext` structs). External sources would therefore\\nnot benefit from any shared arrangements currently. Ultimately, we\\nwant all decisions on what computations to store in arrangements to be\\nmade by some kind of optimiser. The current indexing scheme is merely\\na special case of that problem and should not require special\\nconstructs.\\n(3) Layout inefficiencies. In principle we could do away with tuples\\nentirely and simply rely on `Vec<Value>` everywhere. As most queries\\nusually interact with many attributes directly, this would mean\\nleaving a lot of optimisation potential on the table.\\n","completion":"## Decision\nWe will move to a column-oriented data model, by maintaining a\\nseparate collection for each attribute.\\nWe will separate specialized operator implementations from the process\\nof chosing the right operators in every situation. E.g. we will not\\nonly provide a general `Join`, but also something like\\n`JoinColumns`. We will also encode information on existing\\narrangements to use in the plan type, e.g. `MatchE -> MatchArrangedE`.\\n"}
{"prompt":"## Context\\nWe use git and GitHub in different ways, it\u2019s emergency to teach team members with the git basics and style guide we will use.\\n","completion":"## Decision\n### git basics\\n1. setup ssh keys ([https:\/\/github.com\/settings\/keys][1]) or GUI;\\n2. clone repo into local system: `git clone git@github.com:huifenqi\/django-project-skeleton.git`;\\n3. files store in three stages:\\n![][image-1]\\n1. `Working Directory`: holds the actual files;\\n2. `Index`: staging area which store your changed files;\\n3. `HEAD`: points to the last commit you've made.\\n4. `Working Directory` -\\> `Index`: `git add <filename>` or `git add *`;\\n5. `Index` -\\> `HEAD`: `git commit -m \"Commit message\"`;\\n6. push updates to Github: `git push origin master`;\\n7. create branch: `git checkout -b feature\/x`\\n![][image-2]\\n8. push branch to Github and others can see: `git push origin <branch_name>`;\\n9. sync local with Github: `git pull`;\\n10. make a new tag: `git tag <tag_name>`;\\n11. show history: `git log`;\\n12. show changes: `git diff <previous_commit_hash>`;\\n13. others: `git status`, `git branch`, `git tag`, etc.\\n### git style guide\\n#### Branches\\n* Choose short and descriptive names: [https:\/\/github.com\/agis-\/git-style-guide#branches][2];\\n* Use dashes to separate words.\\n#### Commits\\n* Each commit should be a single logical change. Don't make several logical changes in one commit;\\n#### Messages\\n* when writing a commit message, think about what you would need to know if you run across the commit in a year from now.\\n### Refs\\n* [http:\/\/rogerdudler.github.io\/git-guide\/index.html][3]\\n* [https:\/\/confluence.atlassian.com\/bitbucketserver\/basic-git-commands-776639767.html][4]\\n* [https:\/\/github.com\/agis-\/git-style-guide][5]\\n"}
{"prompt":"## Context\\nWe want to ensure that a patient\u2019s medical records within the state\\nare always available to the nurse for treatment if they are connected\\nto the internet. See related\\n[PRD](https:\/\/docs.google.com\/document\/d\/1q6cppByQULfh3_mMXC4BJpiNN9Uc_awA6rreeEtUBaM\/edit#)\\nfor more details on the feature and related specifications.\\nThe lookup will be a new API on the server that the mobile app will\\ncall. The following aspects of the problem are addressed in this ADR:\\n- API contract\\n- Data retention\\n- Access restrictions\\n- Audit logging\\n- Rate limiting\\n","completion":"## Decision\n### Data retention\\nThere are two broad kinds of data retention on the app at a facility:\\n1. Permanent: patient is within sync criteria: the same block, has an\\nappointment, or is assigned to the facility\\n2. Temporary: patient is not within the sync criteria\\nIn order to have a better control of the retention, we will send this\\ninformation as `retention type` from the server. We will also send a\\n`retention duration` in seconds, which will be a static number to\\nbegin with, but can later vary depending on the state, country or\\nother factors. See the API contract below for more details.\\nWe will implement temporary retention in the app with this\\nfeature. With every patient retrieved via the lookup API, we will\\nstore the time until which the record needs to be retained if the\\nretention type is temporary.\\n```\\nretain_until = sync_time (now) + retention duration\\n```\\nAfter a sync (that happens every 15 minutes), we will delete the\\nrecords that:\\n- should be retained temporarily\\n- and have passed their retention time period: `now > retain_until`\\nIf a `temporary` record is synced via the sync API, then the retention\\ntype should be set to `permanent`.\\nWe will treat manual and automatic syncs in the same way, and\\nconfigure the retention period to suit the needs of showing patients\\nin the recent list, etc.\\n~~_Alternatively_: we can choose to hard-code the retention period on\\nthe app.~~\\n### API contract\\nFor the request, we will use the endpoint `GET`:\\nhttps:\/\/api.simple.org\/api\/v4\/patients\/identifier\/, where `identifier`\\nis any valid patient business identifier.\\nThe type of the identifier will not be specified in the request because:\\n- the client might not be able to discern the type\\n- the same identifier might exist across different types (BP passport,\\nNHID, or a future type)\\nIn the response, will return a _list of patients_ that have that\\nidentifer. Note that it is possible for more than a single patient to\\nhave the same identifier. The response contract will be similar to\\n[the API used in the BP Passport\\nApp](https:\/\/api.simple.org\/api-docs#tag\/Patient\/paths\/~1patient\/get):\\n````\\n{ \"patients\": [{\\n\"id\": \"497f6eca-6276-4993-bfeb-53cbbbba6f08\",\\n\"full_name\": \"string\",\\n\"age\": 0,\\n\"gender\": \"male\",\\n\"status\": \"active\",\\n\"recorded_at\": \"2019-08-24T14:15:22Z\",\\n\"reminder_consent\": \"granted\",\\n\"phone_numbers\": [],\\n\"address\": {...},\\n\"registration_facility\": {...},\\n\"medical_history\": {...},\\n\"blood_pressures\": [],\\n\"blood_sugars\": [],\\n\"appointments\": [],\\n\"medications\": [],\\n\"business_identifiers\": [],\\n\"retention\": {\\n\"type\": \"temporary\", \/\/ or \"permanent\"\\n\"duration_seconds\": 3600\\n}\\n}]\\n}\\n````\\nThis API will have a 5s timeout from the android app to prevent delay\\nin patient care.\\n~~_Alternatively_: We could use a v5 prefix to disambiguate with the\\nPassport app lookup API more clearly.~~\\n### Access restrictions\\nAccess for this API will be restricted to the state that the user is\\nregistered in. Trying to lookup a patient that resides outside the\\nstate will return 404.\\nIf a patient has travelled across states, and has records in both\\nstates corresponding to the same identifier, then only the patients\\nbelonging to the requesting user's state will be returned in the API.\\n### Audit logging\\nSimilar to the sync API, we will create a _lookup audit log_ for\\nsuccessful lookups performed using this API. This is separate from the\\nsync audit log since looking up a specific patient is conceptually\\ndifferent from fetching the block's patients. This will have the\\nfollowing fields in it:\\n````\\n{\\nuser_id: user.id,\\nfacility_id: current_facility.id,\\nidentifier: identifier,\\npatient_ids: [patient_id],\\ntime: Time.current\\n}\\n````\\n~~_Alternatively_: we can try to repurpose the existing audit logs for\\nthis, while capturing all the information.~~\\n### Rate limiting\\nWe currently rate limit our authentication endpoints using\\n[rack-attack](https:\/\/github.com\/rack\/rack-attack). We will do the\\nsame for this API. See the [relevant section in the\\nPRD](https:\/\/docs.google.com\/document\/d\/1q6cppByQULfh3_mMXC4BJpiNN9Uc_awA6rreeEtUBaM\/edit#)\\nfor the rationale and the chosen rate limiting configuration.\\n"}
{"prompt":"## Context\\nThis application was created to allow services to manage gathered evidence documents in a consistent, safe and centralised way. An early decision point was reached with regards to designing the architecture around this capability.\\nThe basic functionalities known to be required are:\\n- secure storage and management of documents provided by residents and other third parties\\n- tracking of rights and retention over those documents by the council\\n- requests and approval of evidence from third parties\\n- management of evidence requests by officers\\nInfluencing factors:\\n- HackIT's [API Playbook](https:\/\/github.com\/LBHackney-IT\/API-Playbook-v2-beta)\\n- [Clean Architecture](https:\/\/github.com\/madetech\/clean-architecture) principles\\n","completion":"## Decision\nWe decided to create two Platform APIs (as defined in the API Playbook):\\n- A [Documents API](https:\/\/github.com\/LBHackney-IT\/documents-api) for the storage and management of documents and claims over them\\n- An [Evidence API](https:\/\/github.com\/LBHackney-IT\/evidence-api) for the management of evidence requests and reviews\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\n\u6211\u4eec\u6709\u591a\u4e2a\u9879\u76ee\u4f7f\u7528 Python \u5f00\u53d1\uff0c\u968f\u7740\u9879\u76ee\u7684\u53d1\u5c55\uff0c\u5927\u5bb6\u4e5f\u5199\u4e86\u8bb8\u591a\u7684\u5e93\uff0c\u6bd4\u5982\uff0cprice\u3001sms\u3001mail \u7b49\u3002\u800c\u5176\u4ed6\u9879\u76ee\u4e5f\u6709\u8fd9\u6837\u7684\u9700\u6c42\uff0c\u5f53\u524d\u9879\u76ee\u4e4b\u95f4\u662f\u901a\u8fc7\u62f7\u8d1d\u7684\u65b9\u5f0f\u8fdb\u884c\u590d\u7528\uff0c\u4e0d\u662f\u5e93\u8fd8\u5b58\u5728\u9879\u76ee\u5185\u72ec\u81ea\u81ea\u884c\u66f4\u65b0\u3002\u8fd9\u5c31\u5bfc\u81f4\u9879\u76ee\u4e4b\u95f4\u6240\u4f7f\u7528\u7684\u5e93\u4ea7\u751f\u4e0d\u4e00\u81f4\uff0c\u5e76\u91cd\u590d\u9020\u4e86\u5f88\u591a\u7684\u8f6e\u5b50\u3002\\n","completion":"## Decision\n1. \u6784\u5efa\u81ea\u5df1\u7684 pypi \u670d\u52a1\u5668\uff1b\\n* \u4e0d\u53ea\u53ef\u4ee5\u89e3\u51b3\u81ea\u5efa\u5e93\u7684\u590d\u7528\u95ee\u9898\uff1b\\n* \u4e5f\u53ef\u4ee5\u5c06\u6211\u4eec\u7684\u5e38\u7528\u5e93\u7f13\u5b58\uff0c\u52a0\u901f pip \u7684\u5b89\u88c5\uff1b\\n* \u6709\u7ef4\u62a4\u6210\u672c\u3002\\n2. \u4f7f\u7528 git \u4f5c\u4e3a pip \u5b89\u88c5\u5305\\n* \u5355 repo \u5355 package\uff1a\u6240\u6709 libs \u805a\u5408\u5728\u4e00\u4e2a package \u91cc\uff0c\u5bf9\u4e8e\u5927\u91cf\u7b80\u5355 lib \u53ef\u884c\uff1b\\n* \u5355 repo \u591a package\uff1a\u5206\u76ee\u5f55\u6784\u5efa package\uff0c\u9002\u5408\u7a0d\u5fae\u5927\u4e9b\u7684 libs \u4e4b\u95f4\u505a\u9694\u79bb\uff1b\\n* \u591a repo \u591a package\uff1a\u6bcf\u4e2a repo \u662f\u4e00\u4e2a package\uff0c\u9002\u5408\u5927\u7684 lib \u5e93\uff0clib \u7c92\u5ea6\u8fc7\u7ec6\u4f1a\u6709\u5e93\u7ba1\u7406\u95ee\u9898\u3002\\n3. \u4f7f\u7528 Artifactory\uff0cPro+ \u7248\u672c\u624d\u63d0\u4f9b pypi \u670d\u52a1\u3002\\n\u9274\u4e8e\u5f53\u524d lib \u5e76\u4e0d\u591a\uff0c\u800c\u4e14\u6bcf\u4e2a lib \u5f88\u7b80\u5355\uff0c\u6545\u9009\u62e9\u5355 repo \u5355 package \u65b9\u6848\u3002\\n"}
{"prompt":"## Context\\nThe SDBM staff desired a method for receiving and responding to user feedback and questions in a public forum.\\n","completion":"## Decision\nThe Thredded gem was chosen to enable threaded forum discussions.\\n"}
{"prompt":"Context\\n-------\\nWith three controller nodes all listening on 6443, they need a single IP\\naddress they can use to represent \"the kubernetes master.\" Additionally,\\neach worker node listens on 30000-32767 in order to expose NodePort\\nservices to the outside world. Rather than choose a particular worker\\nnode to talk to, it makes sense to load-balance these requests as well\\nto whichever worker nodes are currently online.\\nWhat's written above is straightforward enough to add to our current\\nHAProxy config.\\nHowever, kubernetes doesn't provide any support for claiming IP\\naddresses outside its internal network. Rather, kubernetes merely\\nindicates that it wants an IP address for a service.\\nFor example, let's say we have a web service that is listening on ports\\n80 and 443. If we set its exposure to LoadBalancer, then kubernetes will\\nindicate that and set some NodePort mappings. So, for example, it might\\nmap 80:32689 and 443:31268.\\nIt's then our job to do the following, outside kubernetes:\\n1.  Notice a new service requesting to be type LoadBalancer.\\n2.  Choose an IP address that isn't already in use.\\n3.  Configure our load balancers to claim that floating IP address.\\n4.  Configure our load balancers to round robin 80 and 443 requests for\\nthat IP address to the kubernetes worker nodes over 32689 and 31268,\\nrespectively.\\nIt would be good if we didn't have to do that by hand. If we do automate\\nit, that means our HAProxy config could be changing routinely, which\\ncould have an affect on other production services.\\nDecision\\n--------\\nKubernetes gets its own HAProxy servers, separate from the ones\\ncurrently configured by puppet. For the time being, puppet will\\nconfigure them, but it also provides a simple list of peer IP addresses\\nso that they will one day be capable of configuring themselves.\\nConsequences\\n------------\\nWe can make changes to the load balancers for kubernetes without\\nworrying about breaking access to our first-class services already in\\nproduction. Even though the details of these servers are very similar,\\nthis separation lets us view these HAProxy machines not as load\\nbalancers but rather as highly available gateways to kubernetes.\\n","completion":"## Decision\n--------\\nKubernetes gets its own HAProxy servers, separate from the ones\\ncurrently configured by puppet. For the time being, puppet will\\nconfigure them, but it also provides a simple list of peer IP addresses\\nso that they will one day be capable of configuring themselves.\\nConsequences\\n------------\\nWe can make changes to the load balancers for kubernetes without\\nworrying about breaking access to our first-class services already in\\nproduction. Even though the details of these servers are very similar,\\nthis separation lets us view these HAProxy machines not as load\\nbalancers but rather as highly available gateways to kubernetes.\\n"}
{"prompt":"## Context\\nMOJ Digital's approach to infrastructure management and ownership has evolved over time, and has led to the following outcomes:\\n- Unclear boundaries on ownership and responsibilities between service teams and the cloud platforms team\\n- Significant variation in deployment, monitoring and lifecycle management across products\\n- Inefficient use of AWS resources due to the use of virtual machine-centric architecture, despite our standardisation on Docker containers\\nThe last few years has seen the advent of several products specifically focused on the problem of running and managing containers in production:\\n- Kubernetes\\n- Mesos \/ Mesosphere \/ DC\/OS\\n- Docker Swarm\\n- AWS ECS\\n- CloudFoundry\\nGiven the technology landscape within MOJ, we require a container management platform that can support a wide range of applications, from \"modern\" cloud-native 12-factor applications through to \"legacy\" stateful monolithic applications, potentially encompassing both Linux- and Windows-based applications; this removes CloudFoundry from consideration, given its focus on modern 12-factor applications and reliance on buildpacks to support particular runtimes.\\nFrom the remaining list of major container platforms, Kubernetes is the clear market leader:\\n- Rapid industry adoption during 2017 establishing it as the emerging defacto industry standard\\n- Managed Kubernetes services from all major cloud vendors\\n- Broad ecosystem of supporting tools and technologies\\n- Increasing support for Kubernetes as a deployment target for commercial and open-source software projects\\nThere is also precedent for Kubernetes use within MOJ, as the Analytical Platform team has been building on top of Kubernetes for around 18 months.\\n","completion":"## Decision\nUse Kubernetes as the container management component and core technology for our new hosting platform.\\n"}
{"prompt":"## Context\\nTerraform writes plaintext of the state of our backend. The ability to collaborate in the workspaces is severely handicapped by this. Many groups use AWS and\/or GC storage with dynamodb locking on the state of the file to avoid clobbering on each other. Using Terraform Cloud for small teams will allow us a little more leeway and one less thing to manage.\\n","completion":"## Decision\nUse Terraform Cloud for teams\\n"}
{"prompt":"## Context\\nFrequent small releases are preferred strategy to prevent accumulating risk and deliver benefits to users more quickly\\n","completion":"## Decision\nAim for continuous integration and continuous delivery. [Proposed Flow](..\/diagrams\/CI%20CD%20Pipelines.png)\\n"}
{"prompt":"## Context\\nWe need a clearly defined way for handling data feed files for processing, what are the expected inputs and outputs from the parsers and how will this feed into the whole parsing process for ETL ?\\n","completion":"## Decision\n1. A EAM Parser Factory: this produces a content reader which will be used by all written parsers for reading the contents of a giving data source.\\n1. A Processing Adapter per EAM data feed type which has registered different parsers which handle the retrieval of different types of data out of giving data feed source (e.g CreditSuisse XML).\\n1. Custom Data Extractors (e.g IncomeCashFlowParsers, OrderBroker) which are responsible for extracting different data types from the ContentReader, these are then accumulated by the data feed ProcessingAdapter into a unified format which can be transformed into portions of the expected PriveXML format.\\n1. The custom data extractors will have rights to define specific errors for their data extraction process and how that will affect that specific extraction or for a giving set of files. We will have errors which may be critical and cause immediate failure or which can be considered non-critical and only stop giving feed extraction or ensure it is logged and continued from. The key is that such details should not be the responsibility of the core and as far as only specific errors which the core is concerned with towards stopping immediately for that giving source or a set of sources.\\n![Target Parser Flow](..\/assets\/images\/workflows\/image1.png)\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nSecrets must be protected according to the least-privilege principle. To reduce the trusted computing base, preferrably a 3rd party secret management tool is used to manage and use secrets. Google Secret Mananager is a managed service on GCP, integrated into the platform. This makes it a suitable tool to manage our secrets.\\n","completion":"## Decision\nWe will use GCP Secret Manager to manage, store and use secrets.\\n"}
{"prompt":"## Context\\nNot adding content-type to state store, pubsub and bindings.\\n","completion":"## Decision\n* We will not add content-type since it is a persisted metadata and it can cause problems such as:\\n* Long term support since metadata persisted previously would need to be supported indefinitely.\\n* Added requirement for components to implement, leading to potentially hacky implementations to persist metadata side-by-side with data.\\nOriginal issue and discussion: https:\/\/github.com\/dapr\/dapr\/issues\/2026\\n"}
{"prompt":"## Context\\nFor cloud-edge hybrid scenarios and multie-region deployment scenarios, we need the ability to facilitate communications cross clusters. Specifically, it's desirable to have services scoped by cluster names so that a service in one cluster can address and invoke services on another trusted cluster through fully qualified names in a universal namespace, such as cluster1.serviceb.\\n","completion":"## Decision\nWe should consider adding universal namespace capabilities to Dapr.\\n"}
{"prompt":"## Context\\nWe needed a straight-forward, customizable UI style framework that was somewhat opinionated (we don't have a design team), customizable, intuitive in naming convention, and one which would not interfere with our JavaScript React component implementations.\\nWe reviewed a few UI frameworks including Ant Design, Bootstrap, Bulma, and TailwindCSS.\\n","completion":"## Decision\nWe've chosen [Bulma](https:\/\/bulma.io\/) for it's blend of simplicity, ease of use, and CSS-only approach, and current market share. It has a modern look & feel, seems to be gaining in popularity, and doesn't look like a lot of Bootstrap applications.\\n"}
{"prompt":"## Context\\nDefine a strategy for deprecations.\\n","completion":"## Decision\n### Dogma\\n* Don't do changes without feature-flags (only exception are bugfixes)\\n* Don't break things without an alternative\\n* Don't break things in a minor release\\n* Annotate upcoming breaks as soon as possible\\n* Test all new implementations and changes\\n* Be expressive and very verbose on instructions in your inline feature flag comments\\n* There is a world outside with developers that use our public code\\n### Synopsys\\nAs we decided to work in the trunk based development from now on, there are different kinds of cases we need to consider while implementing changes to not cause any breaks while developing for future features.\\nThe main difference we have to take in account, is if we break currently behaviour with our changes or not.\\nFor this difference we have 4 different cases:\\n* Minor Changes which don't cause any breaks or deprecations\\n* Minor Changes which cause deprecations\\n* Minor Changes as part of a major feature which don't cause any breaks\\n* Major changes which cause breaks\\nFor a quick overview this is how you have to deal with the different cases.\\nConcrete Examples and further explanation follow below.\\n#### Only Minor Changes (no breaks)\\nFeature and changes tend to be released in a minor release. Don't cause breaks. Simple additions, refactorings, etc\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Only Minor Changes (with deprecating code)\\nFeature and Changes tend to be released in a minor release and are developed in a backward compatible manner, but deprecate old code. For example a class is replaced by a new one.\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Major Changes (Breaks)\\nParts of a major feature or refactoring which breaks current behaviour. Removal of classes, methods or properties, change of signatures, business logic changes...\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* only difference between the case above is, that you have to take care about the fact, that the whole old behaviour needs to be fully functional until the next major.\\n* Write specific tests for the major flag which tests the new behaviour.\\n* Detailed description here [Detailed Rules](DetailedRules)\\n"}
{"prompt":"## Context and Problem Statement\\nA framework makes the creation of a web app significantly easier. Which framework should be used?\\n","completion":"## Decision\nChosen option: \"Vue.js\", because the team has more experience with Vue.js.\\n"}
{"prompt":"## Context\\nThe node package manager (npm) creates a so called [\"lock file\"](https:\/\/docs.npmjs.com\/files\/package-locks) when running `npm install`.\\nThe recommended action is to commit this file to version control, as stated by the command itself:\\n```\\n$ npm install\\nnpm notice created a lockfile as package-lock.json. You should commit this file.\\n```\\nSome npm commands, which are part of the pipeline, such as `ci` and `audit` require that a package lock file exists.\\nThus, if a project lacks the package lock file, there are tree options:\\n* Refuse to build the project, which enforces the recommended practice of committing the lock file\\n* Silently create a package lock file as part of the pipeline\\n* Avoid npm commands which require a package lock\\n* Would disable the npm audit stage\\n","completion":"## Decision\nIf a project has a `package.json` file, but no package lock, we create a package lock file as part of the pipeline and warn the user about this.\\nThe created package lock file is not archived as part of the pipeline build artifacts.\\n"}
{"prompt":"## Context\\nCurrently we are using [Docker](https:\/\/www.docker.com\/) as the runtime and orchestration platform for implementing the pipeline steps.\\nGiven its maturity, ubiquitous deployment and tooling around it, it gave us great ease to use it specially via its REST API.\\nBob is what it is mainly due to the enablement Docker had.\\nHowever, it raises the following issues:\\n- Docker mainly runs as a daemon and moreover **needs root access** for the daemon function\\n- Bob is generally intended as a [Cloud Native](https:\/\/en.wikipedia.org\/wiki\/Cloud_native_computing) tool which means all components should be containerized.\\n- Given that docker needs root permissions to run, the runner needs to be privileged to function, causing a security risk to the cluster\\n- The alternative being to mount the host's docker socket into the runner container which is an even bigger security risk\\n- Docker running as a daemon in a container is not a very reliable setup and its own monitoring is still a concern\\n","completion":"## Decision\nBased on the above facts the following is decided:\\n- Use [Podman](https:\/\/podman.io\/) as the container runtime and orchestration engine for the following reasons:\\n- It is rootless and daemonless\\n- Developed by [RedHat](https:\/\/www.redhat.com\/) and the [OCI](https:\/\/opencontainers.org\/) community\\n- Fully FOSS\\n- Exposes a REST API which is docker complaint too\\n- Brings in the possibilities of more things like pods and more\\n- Swap out [clj-docker-client](https:\/\/github.com\/into-docker\/clj-docker-client) in favor of [contajners](https:\/\/github.com\/lispyclouds\/contajners) as the Clojure interface to the engine\\n- Have a self contained image having the runner, the JVM and Podman and run it **unprivileged**.\\n"}
{"prompt":"## Context\\nThe current design is based around a single Bob node written in Clojure responsible for:\\n- Exposing the REST API\\n- Implementing the step execution logic via a local Docker daemon\\n- Implementing the registration, creation and update of all the resources\\n- Controlling the lifecycle of a pipeline\\nThis node is expected to be replicated based on the scaling needs and they all would be simply load balanced behind a simple Application Load Balancer like NGINX.\\nThis brings forth the following issues:\\n- There is no back-pressure support: When the nodes are overwhelmed, there is no way to queue up builds and results in dropping of jobs and errors\\n- There is a shared state of where exactly the pipeline is really running and requests like stopping, pausing which need to be exactly delivered to the concerned node. There is no clean way of doing this\\n- The node being implemented in Clojure\/JVM and using docker to implement the steps has a opinionated view:\\n- Platforms which are either unsupported\/resource constrained by the JVM cannot be addressed in a simple manner\\n- Builds with special privileged needs aren't simple to implement\\n- There is no central place for errors and no ability to program\/orchestrate on errors. Use case: CD for machine learning\\n- The scale bottle neck is the runner but the scale unit is the whole of Bob which is quite suboptimal\\n- It's not simple to implement a declarative style of CI\/CD without queueing and back-pressure\\n","completion":"## Decision\nBased on the above facts the following is decided:\\n- Break the core up into 3 services:\\n- API Server, implementing the spec-first API\\n- Entities, implementing the creation, registration of entities like Pipeline, Resource Provider and Artifact Store\\n- Runner, implementing the step execution logic based on Docker, streaming of logs to DB and pushing out the errors to the queue\\n- Use [RabbitMQ](https:\/\/www.rabbitmq.com\/) as the central queue and rendezvous point for all the services for the following reasons:\\n- It's quite ubiquitous and well battle tested\\n- The protocol and client ecosystem is quite diverse and mature\\n- It's quite resilient and independently scalable\\n- Use the fanout capabilities of the queue to broadcast the stop, pause requests to all connected runners\\n"}
{"prompt":"## Context\\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\\nThe AWS CDK provides components with two levels of abstraction. CfnComponents map directly to cloudformation resources whilst constructs are a higher abstraction which may create multiple related resources. For example, the `AutoScalingGroup` construct will also create `LaunchConfig` and `SecurityGroup` resources. This is a useful abstraction as it allows you to define all the components required for a particular concept in one place.\\nFurther to this, various patterns are available - both AWS supported and open source. These patterns define an entire stack, based around common template. For example, a `EC2App` pattern might provide an `AutoScalingGroup`, `LoadBalancer` and the required `Roles` and `SecurityGroups`. These patterns are likely composed of multiple constrcuts (rather than CfnComponents) under-the-hood.\\nThis library aims to standardise and simplify the process of setting up Guardian stacks by providing reusable components but what level(s) of abstraction should be provided?\\n","completion":"## Decision\n<!-- What is the change that we're proposing and\/or doing? -->\\nThis library should define a number of Guardian flavoured constructs which extend those provided by the AWS CDK library with Guardian defaults baked in. For example, a `GuAutoScalingGroup` and `GuApplicationLoadBalancer`.\\nWhere those constructs are used in multiple ways, it should provide utlity classes for any common usages. For example, for the `Policy` constructs: `GuSSMPolicy`, `GuLogShippingPolicy` and `GuGetS3ObjectPolicy`\\nBuilt on top of those, it should also provide a number of patterns to cover common Guardian stack architectures. For example, `GuEC2App` and `GuLambdaApp` patterns. These patterns should be the encouraged entry point to the library, with the constructs only used outside of standard cases.\\n"}
{"prompt":"## Context\\nBecause we're writing a game we have lots of nested state that we need to update, traverse and\\notherwise manipulate. Querying data is relatively straightforward in vanilla haskell but\\nupdating nested records is a pain.\\nThis is the problem [lens](https:\/\/github.com\/ekmett\/lens) is supposed to help us with.\\nAdditionally by using lenses we have the option to use the \"classy\" lenses style to let\\nindividual subsystems of our game target a subset of our State monad instead of requiring the\\nfull state.\\n","completion":"## Decision\nIt's Lens time!\\n"}
{"prompt":"## Context\\nWe needed to decide where to terminate TLS connections for public and tenant\\nfacing endpoints and how to manage the corresponding private keys.\\nWe had previously decided to only support HTTPS to both deployed applications\\nand Cloud Foundry endpoints.\\nAt the time of writing there were 4 endpoints to consider:\\n- Deployed applications (gorouter). Accessed by the public.\\n- CF API. Accessed by tenants.\\n- UAA. Accessed by tenants.\\n- Loggregator. Accessed by tenants.\\n- SSH proxy. In theory accessed by tenants, but not working in our environment.\\nWe had an existing credentials store suitable for storing the private keys at\\nrest. Only a small number of engineers within the team can access; the same\\nones that can make IAM changes using our account-wide terraform config.\\nPlacing ELBs in front of public-facing services is an architectural pattern\\nadvised by Amazon [in order to reduce attack\\nsurface](https:\/\/d0.awsstatic.com\/whitepapers\/DDoS_White_Paper_June2015.pdf).\\nSpecifically they advise that it helps withstand volumetric Denial of Service\\nattacks; the ELB handles TCP connections and therefore the responsibility for\\nhandling DDOS at Layer 4 and below resides with the ELB team.\\nWe did a spike, where we attempted to place everything public-facing or\\ntenant-facing behind ELBs. We found that:\\n- In HTTP mode the ELBs do not support web sockets. This is known to break\\nloggregator, which relies on them for log streaming. It would also prevent\\ntenants from using web sockets within their applications.\\n- When the ELB is in TCP mode, we have no way of communicating the client IP\\naddress to the downstream service. Practical consequences of this would be\\nthat tenants would be unable to see in their logs who is using their service or\\ndo any access control based on client IP address.\\nIn attempting to solve the second problem, we explored some options:\\n- ELB has support for the [Proxy\\nProtocol](http:\/\/www.haproxy.org\/download\/1.5\/doc\/proxy-protocol.txt), but\\nunfortunately none of the downstream services, such as gorouter, support it. It\\nseemed simple to add support to gorouter.\\n- We could introduce another intermediary proxy such as HAProxy, which\\nunderstands the proxy protocol and adds or appends to an `X-Forwarded-For`\\nheader with the client IP address as provided via the proxy protocol.\\n","completion":"## Decision\nWe decided to:\\n- use the ELB to terminate TLS\\n- use the ELB in TCP mode\\n- submit proxy protocol support to gorouter\\n- use S3 logging to ensure we have the IP addresses of clients using the CF\\nendpoint\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n"}
{"prompt":"## Context\\nObjects in Pomegranate need to get their metadata from Figgy, where they are\\nadministered. Spotlight ships with a IIIF-based indexer. Figgy already produces\\nIIIF manifests to support viewing the objects. However the metadata bucket\\ndoesn't contain rich enough metadata for pomegranate use cases.\\nFiggy (plum, at the time) didn't have an API at the time this decision was made. Manifests were the\\nonly way to get data out. Today Figgy has a graphql API.\\n","completion":"## Decision\nWe will use the IIIF Manifests to pull data from Figgy into Pomegranate. The\\nManifest gives us the manifest url (used for presenting a viewer), the thumbnail\\niiif image url, and the jsonld metadata location (via seeAlso).\\n"}
{"prompt":"### Context\\nWith the coming of quantum computers (QC), the community have started discussing\\naround the use of quantum resistant algorithms in privacy enhancing\\ntechnologies.\\n### Decision\\nOTRv4 does not take advantage of quantum resistant algorithms for the following\\nreasons:\\nFirstly, OTRv4 aims to be possible and easy to implement in today's environments\\nin a reasonable time frame. OTRv4 only aims to lay the foundation for future\\nchanges by adding version rollback protection, DAKEs, upgrade of primitives, and\\nnon-interactive conversations.\\nSecondly, current quantum resistant algorithms and their respective libraries\\nare not ready for incorporation. Production level libraries may take up to 6-18\\nmonths to be ready. Future versions of the protocol may incorporate these\\nlibraries and algorithms when they come into place.\\n### Consequences\\nOTRv4 does not use any algorithms which aim to provide effective resistance to\\nattacks done in quantum computers. If elliptic curve cryptography and 3072-bit\\nDiffie-Hellman can be attacked by quantum computers in the next upcoming years,\\nOTRv4's primitives will become unsafe and unusable. However, the use of a\\n3072-bit Diffie-Hellman \"brace key\" is used partly due to the potential of\\nquantum computers arriving earlier than predicted. When fault-tolerant quantum\\ncomputers break Ed448-Goldilocks keys, it will take some years beyond that point\\nto break 3072-bit Diffie-Hellman keys. Notice, though, that the 3072-bit Diffie\\nHellman keys does not provide any kind of post-quantum confidentiality. We use\\nthem because, in theory, the Elliptic Curve Discrete Logarithm Problem (ECDLP)\\nwill be broken faster than the Discrete Logarithm Problem (DLP). According to\\n[\\[1\\]](#references)), it will be needed 4060 qubits for breaking Ed448 and 6146\\nfor breaking 3072-bit Diffie Hellman.\\n### References\\n1. Roetteler, M., Naehri, M., Krysta M., and Lauter K. (2017).\\n*Quantum Resource Estimates for Computing Elliptic Curve Discrete Logarithms*.\\nAvailable at: https:\/\/eprint.iacr.org\/2017\/598.pdf\\n","completion":"## Decision\nOTRv4 does not take advantage of quantum resistant algorithms for the following\\nreasons:\\nFirstly, OTRv4 aims to be possible and easy to implement in today's environments\\nin a reasonable time frame. OTRv4 only aims to lay the foundation for future\\nchanges by adding version rollback protection, DAKEs, upgrade of primitives, and\\nnon-interactive conversations.\\nSecondly, current quantum resistant algorithms and their respective libraries\\nare not ready for incorporation. Production level libraries may take up to 6-18\\nmonths to be ready. Future versions of the protocol may incorporate these\\nlibraries and algorithms when they come into place.\\n### Consequences\\nOTRv4 does not use any algorithms which aim to provide effective resistance to\\nattacks done in quantum computers. If elliptic curve cryptography and 3072-bit\\nDiffie-Hellman can be attacked by quantum computers in the next upcoming years,\\nOTRv4's primitives will become unsafe and unusable. However, the use of a\\n3072-bit Diffie-Hellman \"brace key\" is used partly due to the potential of\\nquantum computers arriving earlier than predicted. When fault-tolerant quantum\\ncomputers break Ed448-Goldilocks keys, it will take some years beyond that point\\nto break 3072-bit Diffie-Hellman keys. Notice, though, that the 3072-bit Diffie\\nHellman keys does not provide any kind of post-quantum confidentiality. We use\\nthem because, in theory, the Elliptic Curve Discrete Logarithm Problem (ECDLP)\\nwill be broken faster than the Discrete Logarithm Problem (DLP). According to\\n[\\[1\\]](#references)), it will be needed 4060 qubits for breaking Ed448 and 6146\\nfor breaking 3072-bit Diffie Hellman.\\n### References\\n1. Roetteler, M., Naehri, M., Krysta M., and Lauter K. (2017).\\n*Quantum Resource Estimates for Computing Elliptic Curve Discrete Logarithms*.\\nAvailable at: https:\/\/eprint.iacr.org\/2017\/598.pdf\\n"}
{"prompt":"## Context\\n[context]: #context\\nIn addition to entries (= places) every user should be able to create and edit events.\\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n> MVP: Termine k\u00f6nnen genauso wie Initiativen und\\nUnternehmen eingetragen und verschlagwortet werden\\n> NTH:\\n> - Termine k\u00f6nnen wie in Trello als Kalenderansicht dargestellt\\nwerden.\\n> - Termine k\u00f6nnen \u00fcber Webdav ausgelesen und bei\\nentsprechender Berechtigung auch eingelesen werden.\\n(eingeloggte Nutzer, admin entsprechender Schlagworte)\\n","completion":"## Decision\n[decision]: #decision\\n> This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n"}
{"prompt":"## Context\\nThe ETL process takes a long time to run, approximately 6 hours.\\nRunning the ETL process makes close to 30,000 requests to the Syndication API.\\nSeveral applications need access to the JSON file created by the ETL.\\n","completion":"## Decision\nRather than each application running its own copy of the ETL to obtain the Syndication data in JSON format,\\na single instance of the ETL will run and provide access to the resultant file via an nginx web server\\nrunning in the ETL container.\\nThe output JSON is hosted in the container to remove reliance on external solutions, such as Azure.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nGitHub Pages support two [publishing sources](https:\/\/help.github.com\/en\/github\/working-with-github-pages\/configuring-a-publishing-source-for-your-github-pages-site) for project documentation:\\n- the `docs` folder on the `master` branch\\n- the root folder of the `gh-pages` branch\\na third option is to store the built project documentation in\\n- documentation GitHub project.\\nIf the built documentation is stored on the `master` branch:\\n- the codebase includes built artifacts\\n- any release tag needs to be created on the commit after that from which the build was executed\\n- requires the CI server to make commits to the `master` branch\\n- it's straightforward to manage document builds from branches other than `master`\\nWhere the build artifacts on a separate branch:\\n- harder to compare source with the built documentation and manage the build artifacts from branches,\\n- cleanly separates build artefacts from source code\\nStoring built documentations in a separate GitHub repository:\\n- offers clear separation between source and built documentation\\n- carries an overhead or managing another repository\\n- documentation will served at a URL that doesn't match the source: e.g. `pace-neutrons.github.io\/horace-docs`\\n","completion":"## Decision\nBuilt documentation will be stored on the `gh-pages` branch.\\n"}
{"prompt":"## Context\\nJMAP messages have a handy preview property displaying the firsts 256 characters of meaningful test of a message.\\nThis property is often displayed for message listing in JMAP clients, thus it is queried a lot.\\nCurrently, to get the preview, James retrieves the full message body, parse it using MIME parsers, removes HTML and keep meaningful text.\\n","completion":"## Decision\nWe should pre-compute message preview.\\nA MailboxListener will compute the preview and store it in a MessagePreviewStore.\\nWe should have a Cassandra and memory implementation.\\nWhen the preview is precomputed then for these messages we can consider the \"preview\" property as a metadata.\\nWhen the preview is not precomputed then we should compute the preview for these messages, and save the result for later.\\nWe should provide a webAdmin task allowing to rebuild the projection. The computing and storing in MessagePreviewStore\\nis idempotent and the task can be run in live without any concurrency problem.\\nSome performance tests will be run in order to evaluate the improvements.\\n"}
{"prompt":"## Context\\nOpenHAB uses OSGi as a runtime.\\nThis runtime promotes clear separation between implementation and contract (API\/implementation\/SPI) packages.\\nOnce program gets launched OSGi framework such Apache Felix or Eclipse Equinox makes sure that \"implementation\" packages stay hidden.\\nOn the build tool side we do not have such strong separation because many parts of project are co-developed.\\nInternal packages and API are in the same source root, and often functionally different elements of code are included in the same bundle.\\nFor example, this means that the `org.openhab.core.items` package is in the same module as `org.openhab.core.items.internal`.\\nAs a result, during compile time we have all of the dependencies together - ones which are required by `core.items` and ones used by `core.items.internal` package.\\nWhile it might not cause major issues for this module, it might have devastating influence over callers who depend on public parts of the API.\\nDuring compilation phase they will get polluted by internal package dependencies and quite often use them.\\nSuch approach promotes tight coupling between contract and implementation.\\nMore over, it also promotes exposure of specific implementation classes via public API.\\nThe natural way to deal with such things is to address them with a build tool that includes an appropriate includes\/excludes mechanism for dependencies.\\nIt would work properly, but openHAB core is a single jar which makes things even harder.\\nThis means that quite many dependencies get unnecessarily propagated to all callers of public APIs.\\nopenHAB utilizes Apache Karaf for provisioning of the application.\\nKaraf provisioning itself is capable of verifying its \"features\" based on declared modules, bundles, JAR files, etc.\\nCurrently, most of the project features depend on one of two root features, `openhab-core-base` or `openhab-runtime-base`, making no distinction on how particular parts of the framework interact with each other.\\nA tiny extension (SPI) bundle that is targeted at a specific framework feature would need to resolve one of the above features, which would then lead to the activation of the entire openhab framework.\\nPresent structure of modules \/ bundles is as follows:\\n```\\n[openhab thing core] <--- [openhab rest core]\\n```\\n","completion":"## Decision\nSince openHAB core is a framework, we need to keep strong and clear separation of API and implementation packages.\\nThis will clarify module responsibilities and streamline development of extensions through smaller and easier to maintain modules.\\nThere are also significant opportunities to speed up the build processes and make better use of resolution and graph analysis between framework parts.\\nWe decided that a clear separation between API and implementation packages should be made.\\nThis means that each service intended to be interacted with through an OSGi services API should form its own contract bundle.\\nNew framework modules will separate `internal` package under a `core.<area>.core` bundle and have a distinct interface in a separate module.\\nAdditionally, functionally separate components should be placed in separate bundles to ensure that unnecessary dependencies are not pulled in to a build.\\nThis is a big change which involves a lot of code relocations and also cleans up the project dependencies which were built up over time.\\nBy preserving public package names we will be able to keep backward compatibility at an affordable level.\\n```\\n[openhab thing api] <--- [openhab rest thing]\\n^\\n|\\n[openhab thing core]\\n```\\nIdeally, such a pattern should evolve further to include a common structure used for keeping contracts and implementations separate.\\n"}
{"prompt":"## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application.\\nActve Directory B2C (business to consumer) provides existing 'user flows' and allows for creation of custom policies that configure & customise the authentication process.  Which option is most suitable needs to be considered taking into account security & support for the custom UX.\\n## Decision Drivers\\n* Identity management must allow for permission based access to MS Dynamics 365 & file storage solutions.\\n* The level of customisation available for the sign up & sign in process should support the user journey being prototyped as much as possible.\\n","completion":"## Decision\n* Identity management must allow for permission based access to MS Dynamics 365 & file storage solutions.\\n* The level of customisation available for the sign up & sign in process should support the user journey being prototyped as much as possible.\\n[Option 2] Using Azure B2C with an existing 'Sign up and sign in' policy leverages the best out of the box authentication.  Users register and login using an Azure hosted service, users are redirected back to the application where a valid OAuth JWToken identifies who is currently logged in.\\nA custom domains would be best option, but this seems to be unavailable at the time of writing.\\n### Positive Consequences\\n* Leverages best practice for auth now and in the future.\\n### Negative Consequences\\n* Additional policy customisations would be required to fully customise the flow of the registration and login process (see links).\\n* Standalone HTML\/CSS files will be need to be managed and uploaded to \/ updated in the Azure Directory\\n"}
{"prompt":"## Context\\nWe started unit testing with doctest. However, when trying to compile with exceptions disabled, it seems to be a problem.\\nCatch provides support for compiling with -fno-exceptions, and even lets us configure the termination handler.\\n","completion":"## Decision\nWe're proposing to use Catch for unit testing. But we need to actually try it out firs.t\\n"}
{"prompt":"## Context\\nHorace and Herbert will require access to large `sqw` and sets of `nxspe` data files as source data and \"expected\" results for unit and system testing.\\nThese data files are too large to store in GitHub along side the test code, but will not change frequently.\\nSimilar data files are also required for Euphonic testing.\\n","completion":"## Decision\nThe data will be stored in STFC hosted SAN (storage area network).\\nTests will read the data from this network storage location, either by copying the files locally or reading the remote file.\\n"}
{"prompt":"## Context\\nWe want a test framework that has good support for TypeScript and Node. Jest is\\na fast testing framework with good resources for mocking.\\n","completion":"## Decision\nWe will use Jest as our testing framework.\\n"}
{"prompt":"## Context\\nSome routes for the `sales-channel-api` and the `store-api\/storefront` depend on `SalesChannelContext` to identify whether the Customer is logged or not.\\nFor keeping clean code, consistency, and more easy to readable API. We create a new annotation for routing `\\Core\\Framework\\Routing\\Annotation\\LoginRequired`.\\n","completion":"## Decision\nWith the `store-api\/storefront` routing needs requiring logged in for access, developers need to define annotation `@LoginRequired `for API.\\nThis annotation to the following:\\n* `@LoginRequired`\\n* This annotation is validating the `SalesChannelContext` has Customer return success, otherwise throw `CustomerNotLoggedInException`\\n* `@LoginRequired(allowGuest=true)`\\n* This annotation is validating the `SalesChannelContext` has Customer and allow Guest admits, otherwise throw `CustomerNotLoggedInException`\\nAn example looks like the following:\\n```php\\n\/**\\n* @Since(\"6.0.0.0\")\\n* @LoginRequired()\\n* @Route(path=\"\/store-api\/v{version}\/account\/logout\", name=\"store-api.account.logout\", methods={\"POST\"})\\n*\/\\n\/**\\n* @Since(\"6.2.0.0\")\\n* @LoginRequired(allowGuest=true)\\n* @Route(\"\/account\/order\/edit\/{orderId}\", name=\"frontend.account.edit-order.page\", methods={\"GET\"})\\n*\/\\n```\\n"}
{"prompt":"## Context\\nDeploying artifacts to nexus was done with this plugin before, but a bug was reported.\\nWhen reading a child pom without `version`, for example, it does not retrieve the information inherited from the parent pom.\\nWe have to investigate alternatives.\\n### Alternatives\\n* [Apache Maven Deploy Plugin](http:\/\/maven.apache.org\/plugins\/maven-deploy-plugin\/)\\n* Maven lifecycle phase : deploy\\n* [Nexus Artifact Uploader](https:\/\/wiki.jenkins.io\/display\/JENKINS\/Nexus+Artifact+Uploader)\\n### Pros and Cons\\n#### Apache Maven Deploy Plugin (deploy:deploy-file)\\nFor this option, we only consider the goal `deploy:deploy-file`.\\n##### :+1:\\n- Official maven plugin for deployment, which is perfect for any maven projects if you only care whether your artifacts are deployed correctly.\\n##### :-1:\\n- A list of parameters has to be generated before using the plugin, including `artifactId` and `version`, which is the same case as the `Nexus Artifact Uploader`.\\n- Credential info has to be stored in the `settings.xml`, which introduces additional implementation.\\nLet's assume users have saved all the credentials in the Jenkins server.\\nWe may inject a list of `server` tags under the `servers` tag with credentials info into the global `settings.xml`.\\nTo make it as secrets, `mvn --encrypt-master-password <password>` has to be executed afterwards.\\n#### Maven lifecycle phase: deploy\\nBy default, the maven lifecycle phase `deploy` binds to the goal `deploy:deploy` of the `Apache Maven Deploy Plugin`.\\n##### :+1:\\n- Same as the `Apache Maven Deploy Plugin`\\n- You don't have to pass the parameters as `Apache Maven Deploy Plugin` and `Nexus Artifact Uploader`,\\nbecause `package` phase is executed implicitly and makes the parameters ready before `deploy` phase.\\n##### :-1:\\n- Same case as the `Apache Maven Deploy Plugin` for handling credentials.\\n- As a maven phase, a list of phases is triggered implicitly before this phase, including `compile`, `test` and `package`.\\nTo follow the build-once principle, all these phases have to be skipped.\\nHowever, it's not possible to skip some of the maven goals binding to certain phases.\\nFor example, if the `<packaging>` tag of the `pom.xml` is set to `jar`, then the `jar:jar` goal of the [`Apache Maven JAR Plugin`](https:\/\/maven.apache.org\/plugins\/maven-jar-plugin\/) is bound to `package` phase.\\nUnfortunately, however, `Apache Maven JAR Plugin` does not provide an option to skip the the `jar:jar` goal.\\n**This is the main reason why we cannot use this option.**\\n#### Nexus Artifact Uploader\\n##### :+1:\\n- Without the pain of handling the credentials, which was mentioned above in `Apache Maven Deploy Plugin` section.\\n- It's promising, when the plugin is used properly\\n##### :-1:\\n- Same as the `Apache Maven Deploy Plugin`. A list of parameters has to be prepared.\\n### Decision\\n`Nexus Artifact Uploader` is chosen, because:\\n- `Maven lifecycle phase: deploy` does not meet our build-once principle.\\n- `Nexus Artifact Uploader` has the same situation regarding parameters as `Apache Maven Deploy Plugin`, but can handle credentials as a Jenkins plugin.\\n","completion":"## Decision\n`Nexus Artifact Uploader` is chosen, because:\\n- `Maven lifecycle phase: deploy` does not meet our build-once principle.\\n- `Nexus Artifact Uploader` has the same situation regarding parameters as `Apache Maven Deploy Plugin`, but can handle credentials as a Jenkins plugin.\\n"}
{"prompt":"## Context\\n- I don't want to make the CoreDNS container in my cluster the local DNS source of truth, because my view of it is entirely for cluster DNS\\n- I don't want to expose my services to the internet _yet_ (but expect to later)\\n- I have a dedicated domain for my services\\n- I can set up TLS for \"internal\" domains via Let's Encrypt + CloudFlare DNS\\n- I need something to route internal requests to my cluster without querying external DNS resolvers, which won't know about my internal cluster\\n- I can probably do this on my UniFi setup, but it'd likely require some configuration outside of the controller software\\n- Pi-Hole introduces some other neat features\\n- I have multiple spare Raspberry Pis\\n","completion":"## Decision\n- Flash Raspberry Pi OS (minimal install) on a microSD card\\n- Use that card to boot a Raspberry Pi 3B+ (`Raspberry Pi 3 Model B Rev 1.2`)\\n- Configure for SSH, use wired LAN\\n- Consider adding WLAN later, but Pi-Hole will expect a single static IP?\\n- Install and configure unattended upgrades\\n- Install Pi-Hole\\n"}
{"prompt":"## Problem\\nOnly libelektra-core is supposed to access private data but this contradicts the goal to keep the library minimal.\\n`kdbprivate.h` was too generic, it contained many other parts next to the struct definitions of Key\/KeySet.\\n","completion":"## Decision\nAlso allow `libelektra-operations` library to access private Key\/KeySet.\\nPut struct definitions of Key\/KeySet in a separate header file, which gets\\nincluded by parts that need it\\n- none\\n"}
{"prompt":"## Context\\nABCI errors should provide an abstraction between application details\\nand the client interface responsible for formatting & displaying errors to the user.\\nCurrently, this abstraction consists of a single integer (the `code`), where any\\n`code > 0` is considered an error (ie. invalid transaction) and all type\\ninformation about the error is contained in the code. This integer is\\nexpected to be decoded by the client into a known error string, where any\\nmore specific data is contained in the `data`.\\nIn a [previous conversation](https:\/\/github.com\/tendermint\/abci\/issues\/165#issuecomment-353704015),\\nit was suggested that not all non-zero codes need to be errors, hence why it's called `code` and not `error code`.\\nIt is unclear exactly how the semantics of the `code` field will evolve, though\\nbetter lite-client proofs (like discussed for tags\\n[here](https:\/\/github.com\/tendermint\/tendermint\/issues\/1007#issuecomment-413917763))\\nmay play a role.\\nNote that having all type information in a single integer\\nprecludes an easy coordination method between \"module implementers\" and \"client\\nimplementers\", especially for apps with many \"modules\". With an unbounded error domain (such as a string), module\\nimplementers can pick a globally unique prefix & error code set, so client\\nimplementers could easily implement support for \"module A\" regardless of which\\nparticular blockchain network it was running in and which other modules were running with it. With\\nonly error codes, globally unique codes are difficult\/impossible, as the space\\nis finite and collisions are likely without an easy way to coordinate.\\nFor instance, while trying to build an ecosystem of modules that can be composed into a single\\nABCI application, the Cosmos-SDK had to hack a higher level \"codespace\" into the\\nsingle integer so that each module could have its own space to express its\\nerrors.\\n","completion":"## Decision\nInclude a `string code_space` in all ABCI messages that have a `code`.\\nThis allows applications to namespace the codes so they can experiment with\\ntheir own code schemes.\\nIt is the responsibility of applications to limit the size of the `code_space`\\nstring.\\nHow the codespace is hashed into block headers (ie. so it can be queried\\nefficiently by lite clients) is left for a separate ADR.\\n"}
{"prompt":"## Context\\nI have to choose a framework to implement moneycount-api project. It could be a familiar framework, such as Spring Boot, Spark, Servlets + Jersey or even Python + Flask, or I could try another different framework to learn something new.\\n","completion":"## Decision\nI decided to use Spring Boot 1.5.10, the last stable 1.X release at the time, because I want to have a first version of the software in a well known framework that allows me to implement it fast, with easy integration with other features that I can choose in future improvements.\\n"}
{"prompt":"## Context\\nIdentify the components the VRS will integrate with that includes the parameters that needed to be passed.\\n","completion":"## Decision\nComponents that will call the VRS are the following:\\n### 1. Voice Assistant Platform (VAS)\\n- Search for registered names (GET)\\n### 2. VRS User Interface for registration, searching, and updating status\\n- Register new names (post)\\n- Update information for current registered names (PUT)\\n- Cancel registration for current registered names (PUT)\\n- Search for registered names (GET)\\n### 3. Local VRS resolver (Internet Of Things)\\n- Search for registered names (GET)\\n> ![](docs\/..\/..\/..\/components\/component_assets\/vrs_003_01.png?raw=true \"Fig. 1 - VRS Integrations\")\\nLocal implementation of VRS is a requirement to highlight the importance of privacy and enterprise-level implementation.\\nIn summary, VRS will always have a system-to-system integration and not directly to the user. These integrations are in alignment with our architecture guiding principle about VRS.\\n"}
{"prompt":"## Context\\nWe should determine how to develop and document our accptance tests. This decision describes the choice around the langauge (DSL) that we use to describe them.\\n","completion":"## Decision\nWe have determined to describe our acceptance tests using Gherkin.\\n"}
{"prompt":"## Context\\nWe need to set some APIs keys without publishing them on GitHub.\\n","completion":"## Decision\nWe will add `config.js.example` and then explain in the README under a setup section that you need to copy that and call it `config.js` and add the secrets to it. `config.js` added to `.gitignore`.\\n"}
{"prompt":"## Context\\nAWS infrastructure such as API gateway endpoints and lambdas are deployed via terraform, and so are represented at a filesystem-level by terraform .tf files\\n","completion":"## Decision\nRepo naming convention:\\nopg-data-[microservice-domain]\\n* .tf files pertaining to API Gateway infra to be stored in the [opg-sirius-api-gateway repo](https:\/\/github.com\/ministryofjustice\/opg-sirius-api-gateway)\\n* .tf files pertaining to a specific integration (eg lambdas) are managed in the integration-specific opg-data-[microservice-domain] repo, which is then 'deployed over' the API Gateway\\nfor example, files for deputy-reporting \/reports endpoint are saved in [opg-data-deputy-reporting](https:\/\/github.com\/ministryofjustice\/opg-data-deputy-reporting)\\n"}
{"prompt":"## Context\\nOriginally, we utilized a Spring's JdbcTemplate to quickly CRUD our entities against the data store.  While this was quick and easy, we did most of our filtering in the application as opposed to SQL WHERE clauses.  As we continued, each addition to our entities required a lot of boilerplate code.\\nSpring has great JPA support and Boot uses Hibernate out of the box.  Our entity models are still relatively simple, but using JPA reduces a lot of the boilerplate code, and opens up a lot of additional features \"for free.\"  Specifically, we can utilize JPA to manage our schema updates (naively, later if we need something more robust we can look to Liquibase or Flyway).  It also simplifies joins, where clauses, and gives us more database independence.\\n","completion":"## Decision\n* Use JPA to map objects into database tables\\n* Use Hibernate as the JPA implementation - Spring Boot's default\\n* Leverage Spring Data's JPA support to implement queries via Repository interface patterns\\n"}
{"prompt":"## Context\\nActually it does not make sense to store ports as strings, as those Backends are not in the cluster so they don't have names and can only be referenced by a port number. Nevertheless the Kuberenetes API makes it possible to store ports as string which will not be used at the endpoints generated by the External Service Operator.\\n","completion":"## Decision\nThe change that we're proposing or have agreed to implement.\\n"}
{"prompt":"## Context\\nhttp:\/\/www.pac4j.org\\nWhile exporing alternatives to our custom filter and token validator, we came across pac4j which can deal with Azure AD out of the box.\\nThe driver for this alternative is to have a library where we can point it to the discovery url (https:\/\/login.microsoftonline.com\/organizations\/v2.0\/.well-known\/openid-configuration) and have it \"just work.\"  Microsoft is a bit off standard where they add {tenantId} to the Issuer URL because this is a multi-tenant endpoint.  While it's not hard, openid connect libraries need to be adjusted to handle this.  Pac4j does this for free.\\n","completion":"## Decision\n* Use Pac4J to validate MS id_tokens instead of our current custom filter code\\n"}
{"prompt":"## Context\\nWe need to select a base operating system to install on all virtual machines that form\\nthe Datalabs environment. There are three choices available through the JASIMN portal\\nUbuntu 14.04, Ubuntu 16.04 and CentOS 6.9.\\n","completion":"## Decision\nWe have selected Ubuntu 16.04 as the base operating system for our servers for several\\nreasons:\\n* The team are more familiar with Ubuntu over CentOS.\\n* Packages are likely to be more easily available on Ubuntu.\\n* CentOS 6.9 is no longer being updated (last update 10\/5\/2017).\\n* Ubuntu 16.04 will be supported for far longer. 14.04 end of life is early 2019.\\n"}
{"prompt":"## Context\\nOnce we have the data, we need a way to analyze it, and derive meaningful data from it.\\nNurses, medical officers, officials from Resolve and IHMI, and admins on Simple\\nwill want to view vital information to track control of hypertension and adoption of the app.\\n### Kinds of reports\\n1. Admin\/Web reports\\n- Admins, CVHOs, STSes, and officials on Simple that have access to the admin dashboard see these reports.\\n- They provide analyis of adoption, and usage of the app.\\n- They also provide the registration and control data that previously came from the IHMI registries at facilities.\\n2. Nurse\/In-app reports\\n- These are available inside the app for the nurses, MOs and officials at the facilities.\\n- They generally include BP\/registration\/control data for that facility, over various periods of time.\\n- They might also include a log of recent patients who visited the clinics.\\n3. Top-level\/PDF reports\\n- These are sent out every month, or quarter to officials, and have analysis at all levels.\\n\u2013 These might need subjective interpretations added to them, so they might be manually compiled.\\n### Data and the scale\\nWe should engineer for a realistic scale, with a potential to expand to the max scale.\\n|Entity     | Realistic scale   | Max scale|\\n|-------    |-----------------  |----------|\\n| BP        |       1.2 billion | 6 billion |\\n| Patients |    100 million | 300 million |\\n| Users     |       20 k        | 100 k |\\n| Facilities |      10 k | 50 k |\\n| Drugs     |       1.2 billion | 6 billion |\\n- BPs are the most populous entity, running into billions, within a few years.\\n- Patients are next, and they run into millions. They are also capped at the realistic 300m, based on the indian hypertensive population.\\n### Granularities\\nAll these reports might be pulled at all these granularities:\\n#### By time\\n- Daily *\\n- Weekly\\n- Monthly\\n- Quarterly\\n- Cohorts (9 month intervals)\\nNote that daily is the smallest time interval that matters for reports.\\n#### By territory\\n- User\\n- Facility\\n- District\\n- State\\n- Protocol\\n- Country\\n#### Other characteristics of reports\\n- Data for a date might change up to a few months from the date depending on when a user's device syncs. The reports should account for this.\\n- BP, User, and Patient data can be aggregated daily.\\n- Control data can't be aggregated, and it should be regenerated from scratch every time.\\n- Computing reports needs to be idempotent.\\n","completion":"## Decision\nWe'll go with a simple architecture, as described in the figure below:\\n![reporting-arch](resources\/reporting-arch.png \"Reporting architecture\")\\n### Service architecture\\nWe'll keep all the reporting code in the `simple-server` repository,\\nin a separate namespace. If we need to run reports as a separate\\nservice, we'll deploy the same repo onto another instance and use that\\nexclusively for reports.\\nThis enables a small team of 2\/3 developers to move quickly. Splitting\\nrepositories can be considered when the team is larger.\\nWe will run batch jobs over a cron, or cron-like interface to compute\\ncontrol rates or other reports.\\n### Database design for reporting\\nFor the time being, we will not change anything. i.e, we will continue\\nto use the transactional database schema for running reports as\\nwell. As and when our reporting queries become more stable, and our\\nperformance needs improvement, we will consider other designs:\\n1. We can use a read replica database to run large queries, so that we\\ndon't impact the transactional database.\\n2. We can use native postgres partitioning to slice the incoming data\\ninto multiple tables.  Over time, each child table should have about\\n100m rows, not more.\\n### API design\\nWe need a reporting API for the Nurse\/In-app reports seen on mobile.\\n- The API could send JSON data representing daily reports\\n- If we are experimenting with reports, the API could deliver an inlined HTML report that can be cached on the device.\\n### Performance\\n- Avoid doing anything without profiling.\\n- Avoid caching until we _need_ it.\\n- We'll cache reports at the application level, and into the database\\nfor fine control\\n- Once we hit 10s of millions of BPs, we'll need to aggregate daily\\ndata for speed\\n### Metabase\\nMetabase is a BI tool for business owners to create dashboards, and\\nexplore data visually. This saves us weeks of engineering effort,\\nsince metabase supports graphing, caching and embedding.\\nWe could use metabase as our reporting software for the most part,\\nembedding dashboards into an admin interface, until we decide we need\\ncustom reports.\\n"}
{"prompt":"## Context\\nA drawing mark's sub-task is designed to support volunteers answering additional questions for each drawing mark annotation. It allows single choice or multiple choice question task, text task, dropdown task, and slider task.\\n### Annotation JSON structure\\nThe current sub-task annotation JSON structure is:\\n```json\\n\/\/ a point with sub-task consisting of a question task and a dropdown task\\n{\\n\"annotations\": [\\n{\\n\"task\": \"T0\",\\n\"value\": [\\n{\\n\"frame\": 0,\\n\"tool\": 0,\\n\"x\": 452.18341064453125,\\n\"y\": 202.87478637695312,\\n\"details\": [\\n{\"value\": 0},\\n{\"value\": [\\n{\"value\": \"option-1\"},\\n{\"value\": \"option-2\"},\\n{\"value\": null}\\n]}\\n]\\n},\\n{\\n\"frame\": 0,\\n\"tool\": 0,\\n\"x\": 374.23454574576868,\\n\"y\": 455.23453656547428,\\n\"details\": [\\n{\"value\": 1},\\n{\"value\": [\\n{\"value\": \"option-3\"},\\n{\"value\": \"option-4\"},\\n{\"value\": \"option-5\"}\\n]}\\n]\\n},\\n{\\n\"frame\": 0,\\n\"tool\": 1,\\n\"x\": 404.61279296875,\\n\"y\": 583.4398803710938,\\n\"details\": [\\n{\"value\": 1},\\n{\"value\": [\\n{\"value\": \"option-3\"},\\n{\"value\": \"option-4\"},\\n{\"value\": \"option-5\"}\\n]}\\n]\\n}\\n]\\n}\\n]\\n}\\n```\\nThe annotation structure for the sub-task, under `details`, has a few issues because it solely relies on an array index to relate back to the original sub-task. This makes it difficult to make downstream analysis and aggregation scripts. The aggregation code now has to parse the details array and make a \"mock annotation\" of the correct structure to be passed along to the next reducer.\\n### Sub-task UI\\nThe sub-task UI positioned itself fixed below relative to the position of the mark. Notably transcription projects have commented that this interferes with being able to transcribe successfully since the dialog may cover up part of the subject and cannot be moved without moving the drawing mark.\\n","completion":"## Decision\nFor initial support, we will support the single and multiple choice question tasks and the text task in the sub-task. Slider task may be deprecated and dropdown task may be changing in ways we do not have a plan for yet, so they can be supported later if it makes sense to add them.\\n### Annotation JSON structure\\nThe annotations in the details array will be updated to be an object that just contains a reference to the sub-task's unique identifier. The task annotation itself will be stored in the classification's annotations array flattened.\\nThe main benefit of this reorganization will be with downstream analysis and aggregation. When aggregating drawn shapes the first step is clustering. Once the clusters are found the subtasks need to be aggregated within each cluster. This will be easier to do if the structure of each subtask annotation is the same as if that task was asked on its own. The code can just take all subtask annotations within a cluster and just pass it to the reducer as if it is a list of main task annotations without having to reshape them.\\nAn addition of `markIndex` is being added for the sub-task annotations for the purpose of having an identifier relating it back to the parent drawing task annotation value array which represents marks.\\nAn example of the new sub-task annotation JSON structure at classification submission:\\n```json\\n{\\n\"annotations\": [\\n{\\n\"task\": \"T0\",\\n\"taskType\": \"drawing\",\\n\"value\": [\\n{\\n\"frame\": 0,\\n\"toolIndex\": 0,\\n\"toolType\": \"point\",\\n\"x\": 452.18341064453125,\\n\"y\": 202.87478637695312,\\n\"details\": [\\n{\"task\": \"T0.0.0\"},\\n{\"task\": \"T0.0.1\"}\\n]\\n},\\n{\\n\"frame\": 0,\\n\"toolIndex\": 0,\\n\"toolType\": \"point\",\\n\"x\": 374.23454574576868,\\n\"y\": 455.23453656547428,\\n\"details\": [\\n{\"task\": \"T0.0.0\"},\\n{\"task\": \"T0.0.1\"}\\n]\\n},\\n{\\n\"frame\": 0,\\n\"toolIndex\": 1,\\n\"toolType\": \"point\",\\n\"x\": 404.61279296875,\\n\"y\": 583.4398803710938,\\n\"details\": [\\n{\"task\": \"T0.1.0\"},\\n{\"task\": \"T0.1.1\"}\\n]\\n}\\n]\\n},\\n{\\n\"task\": \"T0.0.0\",\\n\"taskType\": \"single\",\\n\"markIndex\": 0,\\n\"value\": 0\\n},\\n{\\n\"task\": \"T0.0.1\",\\n\"taskType\": \"dropdown\",\\n\"markIndex\": 0,\\n\"value\": [\\n{\"value\": \"option-1\"},\\n{\"value\": \"option-2\"},\\n{\"value\": null}\\n]\\n},\\n{\\n\"task\": \"T0.0.0\",\\n\"taskType\": \"single\",\\n\"markIndex\": 1,\\n\"value\": 1\\n},\\n{\\n\"task\": \"T0.0.1\",\\n\"taskType\": \"dropdown\",\\n\"markIndex\": 1,\\n\"value\": [\\n{\"value\": \"option-3\"},\\n{\"value\": \"option-4\"},\\n{\"value\": \"option-5\"}\\n]\\n},\\n{\\n\"task\": \"T0.1.0\",\\n\"markIndex\": 2,\\n\"taskType\": \"single\",\\n\"value\": 1\\n},\\n{\\n\"task\": \"T0.1.1\",\\n\"markIndex\": 2,\\n\"taskType\": \"dropdown\",\\n\"value\": [\\n{\"value\": \"option-3\"},\\n{\"value\": \"option-4\"},\\n{\"value\": \"option-5\"}\\n]\\n}\\n],\\n\"metadata\": {\\n\"classifier_version\": \"2.0\"\\n}\\n}\\n```\\nThe sub-task identifiers follow a convention of `TASK_KEY.TOOL_INDEX.DETAILS_INDEX`.\\nNote that this is the structure at classification submission. The classifier's internal store models may have differences for the purposes of keeping track of in-progress annotations and marks being made.\\n### Drawing sub-task UI\\nThe UI will change to adopt the design of Anti-Slavery Manuscripts (ASM) with this [generalized design](https:\/\/projects.invisionapp.com\/d\/main#\/console\/12923997\/396381420\/preview). It will be a pseudo-modal, but with a few notable differences from a true modal:\\n- The initial position will be near the associated mark made\\n- Interactions will be allowed with the image toolbar to allow zoom, rotate, as well as opening of the tutorial, field guide, and task help. Submission of the classification should not be allowed.\\n- If the sub-tasks are required, the modal should not be closeable until the required annotations are made or the mark is deleted if cancelled\\n- The dialog can be moved and resized\\nTo support movability and resizing, we will leverage [react-rnd](https:\/\/github.com\/bokuweb\/react-rnd) which is the same library ASM used. Grommet's `Layer` cannot be used since it is intended for actual modal or side panel use and cannot be arbitrarily positioned or moved.\\n"}
{"prompt":"## Context\\nDesigning modern scalable cloud based applications requires intentionally\\ndesigning the architecture to take advantage of the cloud.\\nOne leading way to do that is\\n[The Twelve Factor](https:\/\/12factor.net) methodology.\\n","completion":"## Decision\nWe will follow Twelve Factor methodology.\\n"}
{"prompt":"## Context\\nWe need a simple way to manage our package version.\\n","completion":"## Decision\nWe use versioneer to do this for us.\\n"}
{"prompt":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n","completion":"## Decision\nDecision here...\\n"}
{"prompt":"## Context\\nContext here...\\n","completion":"## Decision\nDecision here...\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nOpen Source Software should be honored and apart from our obligation to show the used open source libraries, we think\\nthe great people have to be named to honor their contribution.\\n","completion":"## Decision\nWe need a new view showing all relevant open source libraries we are using including a link to the project-page.\\nFor simplicity we should use a simple webview showing a pre created HTML page.\\n"}
{"prompt":"## Context and Problem Statement\\nPreviously LaTiS V2 combined Data and Metadata in the Variable trait.  What appeared to be a good object-oriented design decision turns out to create more problems than are solved by combining these two concepts.\\n## Decision Drivers\\n* performance, computing  with metadata is simply inefficient\\n* separation of concerns\\n* inconsistency between data and metadata\\n","completion":"## Decision\n* performance, computing  with metadata is simply inefficient\\n* separation of concerns\\n* inconsistency between data and metadata\\nChosen option: Data and metadata will be separated by removing metadata from the Sample class.\\n### Positive Consequences\\n* Loading data into Spark will be greatly simplified by removing the metadata\\n* Applying transformations to the data and the model separately will reduce code complexity\\n### Negative Consequences\\n* data and metadata may become out of sync\\n"}
{"prompt":"## Context\\nOur componentisation solution includes a framework for building components, a library of shared components and a service that generates HTML for the dotcom frontend (and maybe for other applications too). We can potentially pull out sharable code into their own repos, or keep all our componentisation code together in the same repo.\\nHaving multiple repos expresses the intention that parts of our code can be used in different applications (e.g. the GUUI component library). However, it slows down development and adds complexity. It would become necessary to clone and symlink multiple codebases, for instance in order to work on a shared component, add some new colours to pasteup and then view it in the context of dotcom. There is also the problem of discoverability, and knowing how the various codebases link together.\\n","completion":"## Decision\nWe will keep all the code for componentisation solution in one repo makes the dotcom frontend easy to setup. The code shares code style rules, keeping it consistent.\\nWe will organise the code as a monorepo, expressing the intention that some of our code (GUUI and Pasteup) should remain decoupled from the dotcom frontend website. Changes to these libraries will be published separately.\\n"}
{"prompt":"## Context\\n> Any one who considers arithmetical methods of producing random digits\\n> is, of course, in a state of sin.\\n>\\n> --Dr. John von  Neumann\\nThe existing random beacon used by Oasis Core, is largely a placeholder\\nimplementation that naively uses the previous block's commit hash as the\\nentropy input.  As such it is clearly insecure as it is subject to\\nmanipulation.\\nA better random beacon which is harder for an adversary to manipulate\\nis required to provide entropy for secure committee elections.\\n","completion":"## Decision\nAt a high level, this ADR proposes implementing an on-chain random beacon\\nbased on \"SCRAPE: Scalabe Randomness Attested by Public Entities\" by\\nCascudo and David.  The new random beacon will use a commit-reveal scheme\\nbacked by a PVSS scheme so that as long as the threshold of participants\\nis met, and one participant is honest, secure entropy will be generated.\\nNote: This document assumes the reader understands SCRAPE. Details\\nregarding the underlying SCRAPE implementation are omitted for brevity.\\n### Node Descriptor\\nThe node descriptor of each node will be extended to include the following\\ndatastructure.\\n```golang\\ntype Node struct {\\n\/\/ ... existing fields omitted ...\\n\/\/ Beacon contains information for this node's participation\\n\/\/ in the random beacon protocol.\\n\/\/\\n\/\/ TODO: This is optional for now, make mandatory once enough\\n\/\/ nodes provide this field.\\nBeacon *BeaconInfo `json:\"beacon,omitempty\"`\\n}\\n\/\/ BeaconInfo contains information for this node's participation in\\n\/\/ the random beacon protocol.\\ntype BeaconInfo struct {\\n\/\/ Point is the elliptic curve point used for the PVSS algorithm.\\nPoint scrape.Point `json:\"point\"`\\n}\\n```\\nEach node will generate and maintain a long term elliptic curve point\\nand scalar pair (public\/private key pair), the point (public key) of\\nwhich will be included in the node descriptor.\\nFor the purposes of the initial implementation, the curve will be P-256.\\n### Consensus Parameters\\nThe beacon module will have the following consensus parameters that\\ncontrol behavior.\\n```golang\\ntype SCRAPEParameters struct {\\nParticipants  uint64 `json:\"participants\"`\\nThreshold     uint64 `json:\"threshold\"`\\nPVSSThreshold uint64 `json:\"pvss_threshold\"`\\nCommitInterval  int64 `json:\"commit_interval\"`\\nRevealInterval  int64 `json:\"reveal_interval\"`\\nTransitionDelay int64 `json:\"transition_delay\"`\\n}\\n```\\nFields:\\n- `Participants` - The number of participants to be selected for each\\nbeacon generation protocol round.\\n- `Threshold` - The minimum number of participants which must\\nsuccessfully contribute entropy for the final output to be\\nconsidered valid.\\n- `PVSSThreshold` - The minimum number of participants that are\\nrequired to reconstruct a PVSS secret from the corresponding\\ndecrypted shares (Note: This usually should just be set to\\n`Threshold`).\\n- `CommitInterval` - The duration of the Commit phase, in blocks.\\n- `RevealInterval` - The duration of the Reveal phase, in blocks.\\n- `TransitionDelay` - The duration of the post Reveal phase delay, in blocks.\\n### Consensus State and Events\\nThe on-chain beacon will maintain and make available the following consensus\\nstate.\\n```golang\\n\/\/ RoundState is a SCRAPE round state.\\ntype RoundState uint8\\nconst (\\nStateInvalid  RoundState = 0\\nStateCommit   RoundState = 1\\nStateReveal   RoundState = 2\\nStateComplete RoundState = 3\\n)\\n\/\/ SCRAPEState is the SCRAPE backend state.\\ntype SCRAPEState struct {\\nHeight int64 `json:\"height,omitempty\"`\\nEpoch EpochTime  `json:\"epoch,omitempty\"`\\nRound uint64     `json:\"round,omitempty\"`\\nState RoundState `json:\"state,omitempty\"`\\nInstance     *scrape.Instance      `json:\"instance,omitempty\"`\\nParticipants []signature.PublicKey `json:\"participants,omitempty\"`\\nEntropy      []byte                `json:\"entropy,omitempty\"`\\nBadParticipants map[signature.PublicKey]bool `json:\"bad_participants,omitempty\"`\\nCommitDeadline   int64 `json:\"commit_deadline,omitempty\"`\\nRevealDeadline   int64 `json:\"reveal_deadline,omitempty\"`\\nTransitionHeight int64 `json:\"transition_height,omitempty\"`\\nRuntimeDisableHeight int64 `json:\"runtime_disable_height,omitempty\"`\\n}\\n```\\nFields:\\n- `Height` - The block height at which the last event was emitted.\\n- `Epoch` - The epoch in which this beacon is being generated.\\n- `Round` - The epoch beacon generation round.\\n- `State` - The beacon generation step (commit\/reveal\/complete).\\n- `Instance` - The SCRAPE protocol state (encrypted\/decrypted shares of\\nall participants).\\n- `Participants` - The node IDs of the nodes selected to participate\\nin this beacon generation round.\\n- `Entropy` - The final raw entropy, if any.\\n- `BadParticipants` - A map of nodes that were selected, but have failed\\nto execute the protocol correctly.\\n- `CommitDeadline` - The height in blocks by which participants must\\nsubmit their encrypted shares.\\n- `RevealDeadline` - The height in blocks by which participants must\\nsubmit their decrypted shares.\\n- `TransitionHeight` - The height at which the epoch will transition\\nassuming this round completes successfully.\\n- `RuntimeDisableHeight` - The height at which, upon protocol failure,\\nruntime transactions will be disabled.  This height will be set to\\nthe transition height of the 0th round.\\nUpon transition to a next step of the protocol, the on-chain beacon will\\nemit the following event.\\n```golang\\n\/\/ SCRAPEEvent is a SCRAPE backend event.\\ntype SCRAPEEvent struct {\\nHeight int64 `json:\"height,omitempty\"`\\nEpoch EpochTime  `json:\"epoch,omitempty\"`\\nRound uint64     `json:\"round,omitempty\"`\\nState RoundState `json:\"state,omitempty\"`\\nParticipants []signature.PublicKey `json:\"participants,omitempty\"`\\n}\\n```\\nField definitions are identical to that of those in the `SCRAPEState`\\ndatastructure.\\n"}
{"prompt":"## Context and Problem Statement\\nTo achieve 100% API coverage, we need to handle emails by API.\\n","completion":"## Decision\nChosen option: \"Using events\", because it allows us to send email using events, commands and handlers. Thanks to this we can queue few messages in async transport.\\n"}
{"prompt":"## Context\\nDuring the modernization of the client-side code outlined in \"[ADR 006 - Using Vue.js](006_Vue.js.md)\" and the first prototype for that project - the Address Change Form - we also tried out using [TypeScript](https:\/\/www.typescriptlang.org\/). After the project we evaluated if we want to continue to use TypeScript for client side code. We collected following arguments.\\n### Learning Curve\\nNobody of us has experience with TypeScript, there will be a learning curve and possible slowdowns while setting up features.\\n### Migration Pain\\nMigrating the existing JavaScript code to TypeScript looks daunting. However, we're already planning a rewrite of the the Frontend with Vue and Vuex and want to spare us the pain of another \"rewrite\" with Typescript. Instead, all code will be written in TypeScript.\\n### How \"Future-Proof\" is TypeScript?\\nWith the decline of CoffeeScript in our minds, we don't know if TypeScript is \"just a fad\" and we'll have to port the client-side code to another language in 3-5 years. We're optimistic that TypeScript will still be in active development in the Future, since it's both open source and has Microsoft as a corporate \"sponsor\" behind it.\\n### How well-established is TypeScript?\\nCompared to JavaScript there are less tutorials and reference articles. However, since TypeScript is \"JavaScript with types\", we can apply all the existing material. We are confident that the TypeScript-specific documentation will improve. We are hopeful that the `tslint` linter will get feature-parity with `eslint`.\\n### Onboarding\\nHaving to learn TypeScript might slow down the onboarding process of new developers.\\n### \"Sunk Cost\"\\nWe now have working code that uses TypeScript. Removing it would incur additional effort. In terms of effort spent, we encountered biggest time sink already - the initial setup of the test and build environment. We can build on that from now on.\\n### Maintainability and Readability\\nTypescript will make our code more maintainable, because it helps us to avoid low-level type errors during code changes. The TypeScript interfaces document our intentions and our domain language. Our experiences with PHP, moving from code without type hints to code with type hints support that argument. This is also the main argument and driver of the final decision.\\n### Tooling support\\nTyped code helps IDEs to show errors while writing the code and allows for easy refactoring and code navigation.\\n### Vue.js 3.0\\nVue.js 3.0 is rewritten in TypeScript. We assume there will be benefits from using TypeScript as well and that their decision is based on criteria that we are not even aware of yet (see \"Learning Curve\").\\n","completion":"## Decision\n* We will use TypeScript as our client-side language, because the benefit of maintainability is the most important argument for us.\\n* We will document the personal \"best practices\" in our team, documenting the structure of our client side code and how we write TypeScript.\\n"}
{"prompt":"## Context\\nWe need to make sure that the module will be reusable in a friendly way, functions and parameters names should be easy to guess\\nand should not conflict with other Non-easyvista related module.\\n","completion":"## Decision\nIn order to fullfill our goal :\\n- We will only use verb from the powershell's approved list  (get-verb)\\n- We will prefix all our noun with EZV in uppercase\\n- We will use easyvista's terminology in our nouns:\\n- i.e: a service request or a change request will be both referenced as a request (new-EZVRequest) since\\nthis is the same from the API point of view (\/requests)\\n- We will add an *s* to any function returning a list of something\\n"}
{"prompt":"# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop \/ HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n","completion":"## Decision\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in\\n[this article](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context\\nInitial conversation: https:\/\/github.com\/tendermint\/tendermint\/issues\/2901\\nSome applications can handle transactions in parallel, or at least some\\npart of tx processing can be parallelized. Now it is not possible for developer\\nto execute txs in parallel because Tendermint delivers them consequentially.\\n","completion":"## Decision\nNow Tendermint have `BeginBlock`, `EndBlock`, `Commit`, `DeliverTx` steps\\nwhile executing block. This doc proposes merging this steps into one `DeliverBlock`\\nstep. It will allow developers of applications to decide how they want to\\nexecute transactions (in parallel or consequentially). Also it will simplify and\\nspeed up communications between application and Tendermint.\\nAs @jaekwon [mentioned](https:\/\/github.com\/tendermint\/tendermint\/issues\/2901#issuecomment-477746128)\\nin discussion not all application will benefit from this solution. In some cases,\\nwhen application handles transaction consequentially, it way slow down the blockchain,\\nbecause it need to wait until full block is transmitted to application to start\\nprocessing it. Also, in the case of complete change of ABCI, we need to force all the apps\\nto change their implementation completely. That's why I propose to introduce one more ABCI\\ntype.\\n# Implementation Changes\\nIn addition to default application interface which now have this structure\\n```go\\ntype Application interface {\\n\/\/ Info and Mempool methods...\\n\/\/ Consensus Connection\\nInitChain(RequestInitChain) ResponseInitChain    \/\/ Initialize blockchain with validators and other info from TendermintCore\\nBeginBlock(RequestBeginBlock) ResponseBeginBlock \/\/ Signals the beginning of a block\\nDeliverTx(tx []byte) ResponseDeliverTx           \/\/ Deliver a tx for full processing\\nEndBlock(RequestEndBlock) ResponseEndBlock       \/\/ Signals the end of a block, returns changes to the validator set\\nCommit() ResponseCommit                          \/\/ Commit the state and return the application Merkle root hash\\n}\\n```\\nthis doc proposes to add one more:\\n```go\\ntype Application interface {\\n\/\/ Info and Mempool methods...\\n\/\/ Consensus Connection\\nInitChain(RequestInitChain) ResponseInitChain           \/\/ Initialize blockchain with validators and other info from TendermintCore\\nDeliverBlock(RequestDeliverBlock) ResponseDeliverBlock  \/\/ Deliver full block\\nCommit() ResponseCommit                                 \/\/ Commit the state and return the application Merkle root hash\\n}\\ntype RequestDeliverBlock struct {\\nHash                 []byte\\nHeader               Header\\nTxs                  Txs\\nLastCommitInfo       LastCommitInfo\\nByzantineValidators  []Evidence\\n}\\ntype ResponseDeliverBlock struct {\\nValidatorUpdates      []ValidatorUpdate\\nConsensusParamUpdates *ConsensusParams\\nTags                  []kv.Pair\\nTxResults             []ResponseDeliverTx\\n}\\n```\\nAlso, we will need to add new config param, which will specify what kind of ABCI application uses.\\nFor example, it can be `abci_type`. Then we will have 2 types:\\n- `advanced` - current ABCI\\n- `simple` - proposed implementation\\n"}
{"prompt":"## Context\\nIn order to create topics and test the provisioned MSK Cluster a Client Instance\\nis required. This Client Instance requires Kafka to be installed on it and be\\nable to access the MSK Cluster.\\n","completion":"## Decision\nThe MSK Client Instance will be built as a separate internal module, that can be\\nused to add security rules to the MSK Cluster security group to enable access\\nfrom it to the brokers. This separates the MSK Client Instance from the Cluster\\nas you would expect, as consumers and producers are not going to be run in the\\nMSK Cluster.\\nThe Client Instance will have limited SSH access and then the instance will be\\npart of the security rule to access the cluster. So only the Client Instance\\nwill be able to access the cluster.\\n"}
{"prompt":"## Context\\n### What is a cookie?\\nA cookie is a small amount of data sent along with every HTTP request to a\\nwebsite. We can set it to any value we like. In this project we are using the\\ncookie as a way to store data that a person who is registering has given us.\\nCookies constitute a security risk as they can be intercepted in flight or\\non machine of the user. That's why we want them hanging around for as little\\ntime as possible.\\n### What is a Session Cookie?\\nA session cookie is a cookie that disappears when a browser window is closed.\\n### What is a Persistent cookie?\\nA persistent cookie is one that disappears after a set time, called its\\n`max-age`.\\n### Assumptions\\n* More people are using their own devices than public ones\\n* We will set a sensible (short) `max-age` on a persistent cookie\\n* People rarely close their browsers\\n","completion":"## Decision\n### Use session cookies\\nGiven that most people will be using their own devices, and most likely their\\nphones it makes more sense to use persistent cookies with a short timeout rather\\nthan a session cookie which could lie around on their device for months.\\n"}
{"prompt":"## Context\\nOff-chain data is accessible via REST API. Currently all system components uses direct REST calls in various places making unit test hard.\\n","completion":"## Decision\nCreate client library and use it as dependency in components that want to read the off-chain data. Include the mocked version of the service so unit-tests does not have to rely on the implementation.\\n"}
{"prompt":"## Context\\nAs part of the migration to AWS we need to define our IP addressing strategy and assign ranges to different environments. We will attempt to keep the new IP layout as close as possible to the current one in order to ease migration. This is not completely possible due to Amazon AZs necessarily being on different subnets within the same VPC range.\\nThis record does not currently cover the disaster recovery environments and treats CI as an embedded part of integration.\\n","completion":"## Decision\n* 1 VPC per environment (currently integration, staging and production)\\n* 3 public subnets, spread across availability zones\\n* 3 private subnets, spread across availability zones\\n**These IP ranges have been superseded by [ADR #0033](0033-ip-ranges.md).**\\nThe VPCs will be assigned the following IP ranges:\\n|Environment|IP Range|\\n|-----------|--------|\\n|Integration|10.1.0.0\/16|\\n|Staging|10.2.0.0\/16|\\n|Production|10.3.0.0\/16|\\n|Test|10.200.0.0\/16|\\nEach AZ shall be a `\/24` within the above ranges.\\neg. Integration - AZ1: 10.1.1.0\/24, AZ2: 10.1.2.0\/24, AZ3: 10.1.3.0\/24\\nWe will be deliberately flattening the current VDC separation and placing all the hosts in a private subnet per availability zone and using security groups to maintain our current network isolation.\\n"}
{"prompt":"## Context\\n[context]: #context\\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n> MVP: Eingeloggte Nutzer*innen k\u00f6nnen auf Bewertungen\\nreagieren und \u00e4nderungen vorschlagen\\n> - Unter jeder Bewertung steht ein \"kommentieren\" Button, durch\\nklicken \u00f6ffnet sich ein Kommentarfeld und ein Quellen-Feld,\\nallerdings etwas einger\u00fcckt, wo der Nutzer seine Anmerkung\\nhinterlassen kann. (Der Quelle link muss nach links b\u00fcndig sein,\\nsodass rechts b\u00fcndig \"kommentieren\" steht.)\\n> - So wie jetzt bei Bewertungen auch, steht unter jedem\\nKommentarfeld die 4 Rating optionen, die aber vorausgef\u00fcllt\\nsind, je nachdem, welche Zukunftsf\u00e4higkiet die vorigen\\nBewerter*innen im durchschnitt zu dem Aspekt gesagt haben.\\n(Zu jedem Kommentar wird die Bewertung mit abgespeichert\\nund ist sichtbar. Die letztlich g\u00fcltige Bewertung ist der Mittelwert\\naus allen Bewertungen. )\\n> - Statt Titel wird das oberste Feld \"Stichwort\" genannt und ist\\ndaf\u00fcr gedacht ein (oder mit komma getrennt auch mehrere)\\nStichwort(e) ein zu geben. Stattdessen also keine Titel. (Das\\nwas Titel sind, kann auch die erste Zeile der Bewertung sein)\\n> - Wenn bereits Kommentare zu einer Bewertung abgegeben\\nwurden, steht dort statt \"Kommentieren\" einfach \"Kommentare\\n(X)\" mit einer Nummer, wie viele Kommentare bereits existieren.\\n> NTH:\\n> - Alle Stichworte, die was mit nachhaltigkeit zu tun haben (bio,\\nfair, c2c, refill, demeter...) werden bei den Positivfaktoren\\nbereits angezeigt, zu der entsprechenden Kategorie und\\nm\u00fcssen nur noch per Kommentar, Quellenangabe und Wertung\\n\"aktiviert\" werden.\\n> - Sp\u00e4ter werden \u00fcber externe Quellen (Biogro\u00dfh\u00e4ndler,\\nDemeterverband, GW\u00d6...) auch automatisiert solche\\nBewertungen erscheinen. Alle Manuell eingebeenen\\nBewertungen zum gleichen Stichwort werden dann als\\nKommentare der maschinellen Wertung untergeordent.\\n","completion":"## Decision\n[decision]: #decision\\n> This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nAuthnRequests contain an ID attribute the value of which will be sent back in\\nthe Response as an \"InResponseTo\" attribute.\\nSomething needs to decide what the value of the ID is, and something needs to validate that the InResponseTo is the same as we expected.\\n","completion":"## Decision\nThe service provider will generate a random GUID to use as the AuthnRequest ID.\\n"}
{"prompt":"## Context\\nIn [#178030273](https:\/\/www.pivotaltracker.com\/story\/show\/178030273) we added a liveness probe to the `registry-buddy`\\ncontainers so that they will be restarted when the container is unhealthy. The `registry-buddy` is a simple http server\\nimplemented in Go that is configured with permissions to push and delete images from a container registry.\\nIt does not perform any authentication or authorization checks of its callers and is only protected by virtue of it being\\na sidecar container on trusted workloads (`cf-api-server` and `cf-api-worker`). Given that it makes no auth(n\/z) checks\\nwe have it binding only to the loopback address (e.g. localhost). We cannot use the regular [`http` liveness probe](https:\/\/kubernetes.io\/docs\/tasks\/configure-pod-container\/configure-liveness-readiness-startup-probes\/#define-a-liveness-http-request)\\nto health check it since the `kubelet` does not have access to `localhost` within the Pod's network namespace.\\nTo enable the `kubelet` to reach the `registry-buddy` we would need to listen on a reachable address instead of `localhost`.\\nThis may expose it to the rest of the Pod network, though, and open up the registry to access by non-trusted `Pods`.\\n","completion":"## Decision\nInstead of adding additional auth(n\/z) to `registry-buddy` and having it listen on an externally reachable address, we chose\\nto instead use an `exec` liveness probe to execute a `curl` within the container itself.\\n"}
{"prompt":"## Context\\nSplits are accessible to any app in the same ecosystem, but not always\\ndefined by within that app.\\nIncreasingly, migrations may not be validated centrally in local\\ndevelopment ecosystems as we build out a standalone migration runner,\\nand there's no guarantee that all apps in an ecosystem will be up to\\ndate, so forcing central validation could introduce significant\\ndeveloper friction.\\nTherefor it is unlikely that we could strongly valiate the variants\\nof split that was defined by another app at the time that an\\napp_remote_kill is created.\\nAlso, such validations, if they're only possible in production, could\\nlead to broken migrations in production that are already applied in\\nother ecosystems.\\n","completion":"## Decision\nWe will not validate the override_to value of an AppRemoteKill against\\nthe split. The registry knock-out will simply no-op at runtime if the\\nchosen variant is not available, allowing the team to quickly ship another\\nmigration to disable the feature.\\nAt this time, we will still strongly couple app_remote_kills to split\\nexistence, however. This may change in the future if it proves to cause\\noperational issues\\n"}
{"prompt":"## Context\\n### Audit events\\nThe GOV.UK PaaS has components which expose events which can be used for\\nauditing. For example:\\n- BOSH director kernel audit logs\\n- BOSH director Credhub security events\\n- BOSH director UAA events\\n- Cloud Foundry UAA events\\n- Cloud Foundry Cloud Controller security events\\nThe BOSH director and BOSH managed instances store these logs in\\n`\/var\/vcap\/sys\/log\/`.\\n### Logging service\\nThe Cyber Security team run a centralised log ingestion system called the\\nCentral Security Logging Service (CSLS).\\nThis service runs in AWS and uses [CloudWatch log group subscription\\nfilters](https:\/\/docs.aws.amazon.com\/AmazonCloudWatch\/latest\/logs\/SubscriptionFilters.html)\\nto ingest logs, which are then sent to Splunk for indexing.\\n","completion":"## Decision\nWe will use Terraform to create log groups in our existing pipelines:\\n- `create-bosh-concourse`\\n- `create-cloudfoundry`\\nWe will store audit logs in CloudWatch for 18 months (545 days).\\nWe will use Terraform to create log group subscription filters which will send\\nlogs to CSLS.\\nTerraform failing to create log group subscription filters should not block the\\nexecution of the pipeline, so our pipelines are not coupled to CSLS.\\nWe will run a fork of the\\n[awslogs-boshrelease](https:\/\/github.com\/alphagov\/paas-awslogs-boshrelease),\\non all instances that have relevant audit and security event logs, to send logs\\nfrom the instances to CloudWatch.\\nThe CloudWatch log groups will have subscription filters that will send logs\\nto CSLS so that logs are indexed in Splunk.\\n"}
{"prompt":"## Context\\nWhen handling a message in an `AggregateMessageHandler` or\\n`ProcessMessageHandler`, there is a practical need to check if the instance\\nalready exists before performing an operation on the scope, such as producing a\\nnew message or destroying\/ending the instance.\\nFor example, it may be necessary to ignore a command that has been sent to an\\naggregate that has been destroyed. There is currently no idiomatic way to do\\nthis, short of calling `Create()` only to immediately call `Destroy()` if\\ncreation succeeded.\\n","completion":"## Decision\nWe have chosen to add an `Exists()` method to aggregate scopes, and an analogous\\n`HasBegun()` method to process scopes.\\n"}
{"prompt":"## Context\\nDynamoDB can utilise the [AWS Application Auto\\nScaling](https:\/\/docs.aws.amazon.com\/autoscaling\/application\/APIReference\/Welcome.html)\\nto dynamically adjust provisioned throughput capacity on your behalf. Not all\\nuses of DynamoDB will use this autoscaling service.\\nThere is a plan to create a Terraform `Data Storage Module` which will include\\nthe DynamoDB module.\\n","completion":"## Decision\nTo support autoscaling a Terraform sub module will be utilised.\\nThis sub module will be included in the DynamoDB module using Terraform `module`\\nsyntax. An `enabled` toggle will be used to determine whether or not the\\nautoscaling resources will be created.\\n"}
{"prompt":"## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers\\n* Consistent environment\\n* Automation capabilities\\n* Ease of deployment\\n* Ease of development\\n","completion":"## Decision\n* Consistent environment\\n* Automation capabilities\\n* Ease of deployment\\n* Ease of development\\nChosen option: \"Docker\", because it allows to have same environment locally as well as in production. Every dev and environment will have same configuration.\\ndotnet CLI is quite good at it as well but it requires a little bit more specifics, while Docker can be hosted virtually everywhere.\\n### Positive Consequences\\n* Docker has large support among hosting providers\\n### Negative Consequences\\nLocal development for docker might be harder as you need to rebuild container. While VS team puts a lot of effort into it it is still slower than run it locally on .NET Core.\\nWe can do major development using local setup with .NET Core, but all CI steps automated using docker.\\n"}
{"prompt":"## Context\\nOctagon strives to be a view layer for consumers.  Applications manage their state via their own independent mechanisms.  State embedded within Octagon components may reduce the declaritive ability of said components & violate the top-down, _\"config via props\"_ React mantra.  Consequently, stateful components generally deviate from the pure view layer goals we wish to provide.  Embedded state reduces the ablitity for apps to \"declare\" Octagon components into view state.\\nExample, consider if a `<OctoNavItem \/>` maintains state on whether or not that navigation page is active or not.  If a user reloaded the page, an active item may suddenly show as not active, because that component's state is lost on reload.\\nInstead, the API should be stateless, and be used like: `<OctoNavItem active \/>`.\\nNow, the user of the view layer is in control.\\n","completion":"## Decision\nMake things stateless.  In rare cases, some emphemeral state is OK to maintain internally to components.  These should be highlighted, drawn out, and discussed before merge.  Ideally, these issues should be raised during design, pre-merge if a feature branch is longer running.\\n"}
{"prompt":"## Context\\nI built the site as a front-end learning experience, using only browser local storage for persisting data. What the app is most missing is permanent data storage. For that it will need a hosted web service beyond GitHub pages.\\nSo, now it's time to figure out cloud service architecture. I've inherited an AWS project elsewhere that isn't in the greatest shape, and my team has ideas about how to do better, and I'd like to try some of those concepts out here on my personal project.\\n","completion":"## Decision\nI will build a backend system on AWS.\\n"}
{"prompt":"## Context and Problem Statement\\nCurrent tech stack is very complicated and requires users to download a tool to use it. It takes away from the main idea of this project.\\n## Decision Drivers\\n* sql.js is a library that allows opening up an SQLite database on the website\\n* desktop applciation stack with Python backend is complciated\\n","completion":"## Decision\n* sql.js is a library that allows opening up an SQLite database on the website\\n* desktop applciation stack with Python backend is complciated\\nDropped the backend\/frontend solution. Moved to a pure website.\\n### Positive Consequences\\n* Easier to use - just open a website, upload a file and use the results.\\n* Less complex tech stack.\\n### Negative Consequences\\n* Launching games, and other advanded features are out of scope.\\n"}
{"prompt":"## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nWhen it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.  It could be more sensible to only verify the state\/model.\\nBut, we had a small problem described in ADR-0002.\\n","completion":"## Decision\nWe will assert both against the session in the provider **and** against the DOM elements to make sure the application view is updating properly.\\n"}
{"prompt":"## Context\\nError handling is a very important process that is needed by every crate of the original Ockam rust library, as well as any third-party crate that was designed to supplement Ockam rust library (such as transport and vault implementations).\\nThere are multiple requirements to error handling:\\n- agile enough to be used in different cases\\n- portable to work in different environments with different constraints\\n- convenient for both library developer and library user\\n","completion":"## Decision\nIn search of balance between above-mentioned requirements it has been decided that errors are handled in native to Rust way of using Result type, Error type for such Result should be either of ockam_core::Error of implement Into<ockam_core::Error>\\n"}
{"prompt":"## Context\\nThe goal of the authentication integration project is to enable our products to leverage existing authentication standards as implemented by third party providers. This consists of OpenID Connect and SAML 2.0, both of which are open standards with numerous providers available to choose from. For the purpose of integrating with Helix Core server, this integration must be facilitated via something akin to triggers.\\nAround the time this project first started, support for extensions was being introduced to Helix Core. This offered a standard, easy to learn language, a built-in runtime, with logging, and library of functions to use that would be available on all platforms supported by Helix Core. Compare this with the option of writing triggers in something like Python or Ruby. While that is certainly feasible, it is not always a simple matter to install and configure triggers based on a dynamic scripting language. The previous generation of SAML integration employed a set of Python triggers -- it was okay on Linux-based systems where Python is easy to install, but unworkable for Windows systems (e.g. installing the `libxml` prerequisite was basically impossible).\\n","completion":"## Decision\nIt was decided early on by the architect(s) to use Helix Core **extensions** to integrate with the authentication providers. While installation and configuration of extensions is easy, it does require having a current release of Helix Core (at least 2019.1).\\nOther benefits of using extensions include the ease-of-use, primarily around installation and configuration. If triggers were used, they would need to be configured, probably with a file, and that file location would have to be known at run time by the triggers, and be readable by the user running the trigger. Additionally, the trigger would need a long-lived p4 ticket in order to be able to invoke commands against the server. With extensions, we do not have any of those problems. Lastly, since the extension stays resident in memory during the login process, the request identifier retrieved in `AuthPreSSO` is accessible to\u00a0`AuthCheckSSO` without the need to persist the value on disk.\\n"}
{"prompt":"## Context\\nWe want to have an application wide logging infrastructure in place so that we can trace user requests through the\\napplication. It should be possible to trace each request through the application layers so that we can see what outgoing\\ncalls were made for incoming requests.\\n","completion":"## Decision\nThe use of `monolog\/monolog` to provide configurable logging levels throughout the application. Unique tracing\\ninformation is already provided by the amazon loadbalancers so this should be made available to the logging library and\\nattached to the logged information.\\n* Logging of service code to be main source of log information. If needed handlers can also be logged.\\n* Most logging to be done at an _info_ level.\\n* User actions that need monitoring (e.g. authentication failures) at _notice_ level\\n* Personally Identifiable Information (PII) to **not** be logged anywhere but _debug_\\n"}
{"prompt":"## Context\\nWe need to provide i18n string with some requirements:\\n- dynamic string\\n- support plurals\\n","completion":"## Decision\nNeed to implement it with `intl`\\n"}
{"prompt":"## Context and Problem Statement\\nWhich language version should we prefer? Should we restrict the version at all?\\n## Decision Drivers\\n* Target-typed new expressions and improved pattern matching\\n* Binary compatible with Unity\u00a02021.3\\n* Source compatible with Unity\u00a02021.3\\n* Source compatible with online coding platforms: CodinGame[^CG], LeetCode[^LC], HackerRank[^HR]\\n","completion":"## Decision\n* Target-typed new expressions and improved pattern matching\\n* Binary compatible with Unity\u00a02021.3\\n* Source compatible with Unity\u00a02021.3\\n* Source compatible with online coding platforms: CodinGame[^CG], LeetCode[^LC], HackerRank[^HR]\\nChosen option: \u201cC# 9\u201d.\\nThis is specified in one of the _Directory.Build.props_ files:\\n```xml\\n<Project>\\n<PropertyGroup>\\n<LangVersion>9<\/LangVersion>\\n<Nullable>enable<\/Nullable>\\n<\/PropertyGroup>\\n<\/Project>\\n```\\n### Consequences\\n* Good, because C# 9 is supported by the most popular online coding platforms.\\n* Good, because C# 9 is both binary (via .NET Standard 2.1) and source code compatible with Unity 2021.3.\\n"}
{"prompt":"## Context\\nSome libs from Apollo stack require react 16.8.0.\\n","completion":"## Decision\nTo be consistent and maintain one react version accross application I will use react 16.8.0.\\n"}
{"prompt":"## Context\\n[context]: #context\\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n","completion":"## Decision\n[decision]: #decision\\n> This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\nTags will still remain plain tags without any metadata. Additionally we will introduce directed relationships from\\na *source* to a *target* tag with the following roles:\\n- *generalization*: The source tag is a specialization of the target tag. Examples: target = `#social-economy`, sources = `#socent`\/`#gw\u00f6`\/`#b-corp`\/`#b-corp`\/`sustainabel-finance`\\n- *synonym*: The terms of both tags name the *same* concept. The *source* tag is the *alias* term, the *target* tag\\nis the preferred *canonical* term. Example: target = `#urban-gardening`, source = `#urbangardening` (alternative name), source = `#urban-guardening` (misspelling), source = `#stadtgarten` (german synonym).\\n- *i18n*: A special kind of synonym relationship between canonical terms of different languages? See [Consequences](consequences).\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nadr new Implement as Unix shell scripts\\nThis will create a new, numbered ADR file and open it in your\\neditor of choice (as specified by the VISUAL or EDITOR environment\\nvariable).\\nTo create a new ADR that supercedes a previous one (ADR 9, for example),\\nuse the -s option.\\nadr new -s 9 Use Rust for performance-critical functionality\\nThis will create a new ADR file that is flagged as superceding\\nADR 9, and changes the status of ADR 9 to indicate that it is\\nsuperceded by the new ADR.  It then opens the new ADR in your\\neditor of choice.\\n3. For further information, use the built in help:\\nadr help\\n"}
{"prompt":"## Context\\nOur existing messaging interface names lack of clarity. This review was to make sure messaging interfaces were named appropriately to avoid possible confusions.\\n","completion":"## Decision\n### Dapr\\n* All messaging APIs are grouped under a **messaging** namespace\/package.\\n* We define three distinct messaging interfaces:\\n- **direct**\\nOne-to-one messaging between two parties: a sender sending message to a recipient.\\n- **broadcast**\\nOne-to-many messaging: a sender sending message to a list of recipients.\\n- **pub-sub**\\nMessaging through pub-sub: a publisher publishing to a topic, to which subscribers subscribe.\\n* We distinguish message and direct invocation. For messaging, we guarantee at-least-once delivery. For direct invocation, we provide best-attempt delivery.\\n"}
{"prompt":"## Context\\nOur main source of data on prisoners and prison staff is NOMIS. The\\nallocation tool currently uses the Custody API, as decided in [ADR 0006](https:\/\/github.com\/ministryofjustice\/offender-management-architecture-decisions\/blob\/master\/decisions\/0006-use-the-custody-api-to-access-nomis-data.md), to retrieve\\ndata on both offenders and staff.\\nThere are still four APIs into NOMIS providing general data access, with varying\\napproaches to presenting the data and authentication. We still do not want to add\\nto this duplication.\\nAlthough it has been agreed by the HMPPS technical community that we would\\nlike to move all clients to use the Custody API in preference to the other APIs,\\ninitial use of the Custody API has raised some issues. Problems exist\\nwith the locality of data, N+1 API requests, and data that is unavailable\\nthrough the currently published endpoints. These issues are intrinsic to\\nthe current design of the Custody API, and it is unlikely that they will\\nbe resolved or mitigated in the short to medium term. The scale of the work\\nrequired makes it unrealistic that we would be able to deliver this in a\\nrealistic timescale.\\nThe Elite2 API, is under more active development, is entirely owned by the\\nSheffield team and provides endpoints more focused on the need of clients.\\nMost of the endpoints available work on the caseload of the connecting\\nclient token, but also support the provision of more specific parameters\\nto handle alternate needs. Although we will not be authenticating with\\nthe token of the eventual end user, enough flexibility exists for us to\\nobtain the data we require from Elite2.\\nElite2 provides functionality closer to what we currently require\\nand its design encompasses the need for extra API endpoints for specific\\nservices. Moving to Elite2 is a pragmatic, and tactical approach to\\nresolving the issues around the Custody API to allow us to deliver the\\nallocation component of MOiC. This decision doesn't invalidate the\\noverall agreed strategy of moving HMPPS services to the Custody API\\nover time, but it highlights that more coordinated work is needed to\\nachieve that than we are able to take on ourselves.\\n","completion":"## Decision\nWe will use the Elite2 API for access to the data we require from NOMIS.\\nWe will work with the team in Sheffield on development of the Elite2 API to\\nadd support for accessing the data we need, in the structure that we need it.\\n"}
{"prompt":"## Context\\nThe OpenAPI generator has to generate names for APIs, their operations and schemas.\\nWe initially used the file name to generate an API name, but found that for bigger APIs this is would result in very complex APIs, therefore we need a more sensible default.\\nIn addition API owners whish to be able to customize names of APIs and operations.\\nTo follow naming conventions in JavaScript\/TypeScript we planned on formatting API names as pascal case, operation names as camel case and property and parameter names as camel case.\\nFor properties and parameters we would have to resolve the original names to use them in the requests.\\nWe found that for most cases this is straight forward, but there are corner cases that require a different\/more complex architecture.\\n### Issues with properties and parameters\\nGiven the following two types and their original properties, if those were used in combination, e. g. `allOf`, there is no possibility to set both values.\\n```ts\\ntype A {\\nsomeProperty: string; \/\/ original name: 'some_property`\\n}\\ntype A {\\nsomeProperty: string; \/\/ original name: 'some-property`\\n}\\n```\\nThis could be solved by making names unique throughout the whole service.\\nHowever this again would lead to less convenient names, e.g. `name`, `name1`, `name2`.\\n","completion":"## Decision\n### API naming\\nAPIs are named based on the first tag, if there are any.\\nIf there is no tag, a \"default\" tag is used.\\nTo allow customizing of APIs, we introduced the `x-sap-cloud-sdk-api-name` extension, which takes precedence over the default name generation.\\nThis extension can be set on on multiple levels:\\n- operation, causing this operation to be treated as part of the given API.\\n- path, causing all operations under this path to be treated as part of the given API.\\n- document root, causing all operations in the document to be treated as part of the given API.\\nAll names are transformed to pascal case (`SomeApi`).\\nAPI names are formed by appending an \"Api\" suffix.\\nIf the original name ends with an \"api\" suffix (case independent), the original suffix is removed prior to appending \"Api\".\\nExamples:\\n- `my-api` => `MyApi`\\n- `my` => `MyApi`\\n- `my-api-api` => `MyApiApi`\\n### Operation naming\\nOperations are named based on their given `operationId`.\\nIf no `operationId` is given the name is parsed from the method and the path under which the operation occurred.\\nExample:\\n- path: `entity\/{id}`\\n- method: `get`\\n- resulting name: `getEntityById`\\nUsers can set specific operation names using the `x-sap-cloud-sdk-operation-name` extension.\\nThis extension can be set on the operation only.\\nNames will be transformed to camel case, duplicate names will cause generation to fail.\\n### Property and query parameter naming\\nKeep the names as provided per the specification.\\nNo camel case renaming is done to avoid the original name corner cases discussed above.\\n"}
{"prompt":"## Context\\nThe configuration of validators are mounted into Connaisseur as a configmap, as it is common practice in the Kubernetes ecosystem. When this configmap is upgraded, say with a `helm upgrade`, the resource itself in Kubernetes is updated accordingly, but that doesn't mean it's automatically updated inside the pods which mounted it. That only occurs once the pods are restarted and until they are the pods still have an old version of the configuration lingering around. This is a fairly unintuitive behavior and the reason why Connaisseur doesn't mount the image policy into its pods. Instead, the pods have access to the kube API and get the image policy dynamically from there. The same could be done for the validator configuration, but there is also another solution.\\n## Problem 1 - Access to configuration\\nHow should Connaisseur get access to its configuration files?\\n### Solution 1.1 - Dynamic access\\nThis is the same solution as currently employed for the image policy configuration. The validators will get their own CustomResourceDefinition and Connaisseur gets access to this resource via RBAC so it can use the kube API to read the configuration.\\n**Pros:** Pods don't need to be restarted and the configuration can be changed \"on the fly\", without using Helm.\\n**Cons:** Not a very Kubernetes native approach and Connaisseur must always do some network requests to access its config.\\n### Solution 1.2 - Restart pods\\nThe other solution would be to use ConfigMaps for validators and image policy and then restart the pods, once there were changes in the configurations. This can be achieved by setting the hash of the config files as annotations into the deployment. If there are changes in the configuration, the hash will change and thus a new deployment will be rolled out as it has a new annotation. This corresponds to the [suggestion](https:\/\/helm.sh\/docs\/howto\/charts_tips_and_tricks\/#automatically-roll-deployments) made by Helm.\\n**Pros:** Kubernetes native and no more CustomResourceDefinitions!\\n**Cons:** No more \"on the fly\" changes.\\n### Decision Outcome (1)\\nSolution 1.2 was chosen, going with the more Kubernetes native way.\\n","completion":"## Decision\nSolution 1.2 was chosen, going with the more Kubernetes native way.\\nSolution 2.2 was chosen as it is the more simpler of the two.\\n"}
{"prompt":"## Context\\nOne of the biggest challenges this project faces is to proof that the\\nimplementations of the specifications are correct, much like we strive to\\nformaly verify our alogrithms and protocols we should work towards high\\nconfidence about the correctness of our program code. One of those is the core\\nof Tendermint - Consensus - which currently resides in the `consensus` package.\\nOver time there has been high friction making changes to the package due to the\\nalgorithm being scattered in a side-effectful container (the current\\n`ConsensusState`). In order to test the algorithm a large object-graph needs to\\nbe set up and even than the non-deterministic parts of the container makes will\\nprevent high certainty. Where ideally we have a 1-to-1 representation of the\\n[spec](https:\/\/github.com\/tendermint\/spec), ready and easy to test for domain\\nexperts.\\nAddresses:\\n- [#1495](https:\/\/github.com\/tendermint\/tendermint\/issues\/1495)\\n- [#1692](https:\/\/github.com\/tendermint\/tendermint\/issues\/1692)\\n","completion":"## Decision\nTo remedy these issues we plan a gradual, non-invasive refactoring of the\\n`consensus` package. Starting of by isolating the consensus alogrithm into\\na pure function and a finite state machine to address the most pressuring issue\\nof lack of confidence. Doing so while leaving the rest of the package in tact\\nand have follow-up optional changes to improve the sepration of concerns.\\n### Implementation changes\\nThe core of Consensus can be modelled as a function with clear defined inputs:\\n* `State` - data container for current round, height, etc.\\n* `Event`- significant events in the network\\nproducing clear outputs;\\n* `State` - updated input\\n* `Message` - signal what actions to perform\\n```go\\ntype Event int\\nconst (\\nEventUnknown Event = iota\\nEventProposal\\nMajority23PrevotesBlock\\nMajority23PrecommitBlock\\nMajority23PrevotesAny\\nMajority23PrecommitAny\\nTimeoutNewRound\\nTimeoutPropose\\nTimeoutPrevotes\\nTimeoutPrecommit\\n)\\ntype Message int\\nconst (\\nMeesageUnknown Message = iota\\nMessageProposal\\nMessageVotes\\nMessageDecision\\n)\\ntype State struct {\\nheight      uint64\\nround       uint64\\nstep        uint64\\nlockedValue interface{} \/\/ TODO: Define proper type.\\nlockedRound interface{} \/\/ TODO: Define proper type.\\nvalidValue  interface{} \/\/ TODO: Define proper type.\\nvalidRound  interface{} \/\/ TODO: Define proper type.\\n\/\/ From the original notes: valid(v)\\nvalid       interface{} \/\/ TODO: Define proper type.\\n\/\/ From the original notes: proposer(h, r)\\nproposer    interface{} \/\/ TODO: Define proper type.\\n}\\nfunc Consensus(Event, State) (State, Message) {\\n\/\/ Consolidate implementation.\\n}\\n```\\nTracking of relevant information to feed `Event` into the function and act on\\nthe output is left to the `ConsensusExecutor` (formerly `ConsensusState`).\\nBenefits for testing surfacing nicely as testing for a sequence of events\\nagainst algorithm could be as simple as the following example:\\n``` go\\nfunc TestConsensusXXX(t *testing.T) {\\ntype expected struct {\\nmessage Message\\nstate   State\\n}\\n\/\/ Setup order of events, initial state and expectation.\\nvar (\\nevents = []struct {\\nevent Event\\nwant  expected\\n}{\\n\/\/ ...\\n}\\nstate = State{\\n\/\/ ...\\n}\\n)\\nfor _, e := range events {\\nsate, msg = Consensus(e.event, state)\\n\/\/ Test message expectation.\\nif msg != e.want.message {\\nt.Fatalf(\"have %v, want %v\", msg, e.want.message)\\n}\\n\/\/ Test state expectation.\\nif !reflect.DeepEqual(state, e.want.state) {\\nt.Fatalf(\"have %v, want %v\", state, e.want.state)\\n}\\n}\\n}\\n```\\nHeight           int64\\nRound            int\\nBlockID          BlockID\\n}\\ntype TriggerTimeout struct {\\nHeight           int64\\nRound            int\\nDuration         Duration\\n}\\ntype RoundStep int\\nconst (\\nRoundStepUnknown RoundStep = iota\\nRoundStepPropose\\nRoundStepPrevote\\nRoundStepPrecommit\\nRoundStepCommit\\n)\\ntype State struct {\\nHeight           int64\\nRound            int\\nStep             RoundStep\\nLockedValue      BlockID\\nLockedRound      int\\nValidValue       BlockID\\nValidRound       int\\nValidatorId      int\\nValidatorSetSize int\\n}\\nfunc proposer(height int64, round int) int {}\\nfunc getValue() BlockID {}\\nfunc Consensus(event Event, state State) (State, Message, TriggerTimeout) {\\nmsg = nil\\ntimeout = nil\\nswitch event := event.(type) {\\ncase EventNewHeight:\\nif event.Height > state.Height {\\nstate.Height = event.Height\\nstate.Round = -1\\nstate.Step = RoundStepPropose\\nstate.LockedValue = nil\\nstate.LockedRound = -1\\nstate.ValidValue = nil\\nstate.ValidRound = -1\\nstate.ValidatorId = event.ValidatorId\\n}\\nreturn state, msg, timeout\\ncase EventNewRound:\\nif event.Height == state.Height and event.Round > state.Round {\\nstate.Round = eventRound\\nstate.Step = RoundStepPropose\\nif proposer(state.Height, state.Round) == state.ValidatorId {\\nproposal = state.ValidValue\\nif proposal == nil {\\nproposal = getValue()\\n}\\nmsg =  MessageProposal { state.Height, state.Round, proposal, state.ValidRound }\\n}\\ntimeout = TriggerTimeout { state.Height, state.Round, timeoutPropose(state.Round) }\\n}\\nreturn state, msg, timeout\\ncase EventProposal:\\nif event.Height == state.Height and event.Round == state.Round and\\nevent.Sender == proposal(state.Height, state.Round) and state.Step == RoundStepPropose {\\nif event.POLRound >= state.LockedRound or event.BlockID == state.BlockID or state.LockedRound == -1 {\\nmsg = MessageVote { state.Height, state.Round, event.BlockID, Prevote }\\n}\\nstate.Step = RoundStepPrevote\\n}\\nreturn state, msg, timeout\\ncase TimeoutPropose:\\nif event.Height == state.Height and event.Round == state.Round and state.Step == RoundStepPropose {\\nmsg = MessageVote { state.Height, state.Round, nil, Prevote }\\nstate.Step = RoundStepPrevote\\n}\\nreturn state, msg, timeout\\ncase Majority23PrevotesBlock:\\nif event.Height == state.Height and event.Round == state.Round and state.Step >= RoundStepPrevote and event.Round > state.ValidRound {\\nstate.ValidRound = event.Round\\nstate.ValidValue = event.BlockID\\nif state.Step == RoundStepPrevote {\\nstate.LockedRound = event.Round\\nstate.LockedValue = event.BlockID\\nmsg = MessageVote { state.Height, state.Round, event.BlockID, Precommit }\\nstate.Step = RoundStepPrecommit\\n}\\n}\\nreturn state, msg, timeout\\ncase Majority23PrevotesAny:\\nif event.Height == state.Height and event.Round == state.Round and state.Step == RoundStepPrevote {\\ntimeout = TriggerTimeout { state.Height, state.Round, timeoutPrevote(state.Round) }\\n}\\nreturn state, msg, timeout\\ncase TimeoutPrevote:\\nif event.Height == state.Height and event.Round == state.Round and state.Step == RoundStepPrevote {\\nmsg = MessageVote { state.Height, state.Round, nil, Precommit }\\nstate.Step = RoundStepPrecommit\\n}\\nreturn state, msg, timeout\\ncase Majority23PrecommitBlock:\\nif event.Height == state.Height {\\nstate.Step = RoundStepCommit\\nstate.LockedValue = event.BlockID\\n}\\nreturn state, msg, timeout\\ncase Majority23PrecommitAny:\\nif event.Height == state.Height and event.Round == state.Round {\\ntimeout = TriggerTimeout { state.Height, state.Round, timeoutPrecommit(state.Round) }\\n}\\nreturn state, msg, timeout\\ncase TimeoutPrecommit:\\nif event.Height == state.Height and event.Round == state.Round {\\nstate.Round = state.Round + 1\\n}\\nreturn state, msg, timeout\\n}\\n}\\nfunc ConsensusExecutor() {\\nproposal = nil\\nvotes = HeightVoteSet { Height: 1 }\\nstate = State {\\nHeight:       1\\nRound:        0\\nStep:         RoundStepPropose\\nLockedValue:  nil\\nLockedRound:  -1\\nValidValue:   nil\\nValidRound:   -1\\n}\\nevent = EventNewHeight {1, id}\\nstate, msg, timeout = Consensus(event, state)\\nevent = EventNewRound {state.Height, 0}\\nstate, msg, timeout = Consensus(event, state)\\nif msg != nil {\\nsend msg\\n}\\nif timeout != nil {\\ntrigger timeout\\n}\\nfor {\\nselect {\\ncase message := <- msgCh:\\nswitch msg := message.(type) {\\ncase MessageProposal:\\ncase MessageVote:\\nif msg.Height == state.Height {\\nnewVote = votes.AddVote(msg)\\nif newVote {\\nswitch msg.Type {\\ncase Prevote:\\nprevotes = votes.Prevotes(msg.Round)\\nif prevotes.WeakCertificate() and msg.Round > state.Round {\\nevent = EventNewRound { msg.Height, msg.Round }\\nstate, msg, timeout = Consensus(event, state)\\nstate = handleStateChange(state, msg, timeout)\\n}\\nif blockID, ok = prevotes.TwoThirdsMajority(); ok and blockID != nil {\\nif msg.Round == state.Round and hasBlock(blockID) {\\nevent = Majority23PrevotesBlock { msg.Height, msg.Round, blockID }\\nstate, msg, timeout = Consensus(event, state)\\nstate = handleStateChange(state, msg, timeout)\\n}\\nif proposal != nil and proposal.POLRound == msg.Round and hasBlock(blockID) {\\nevent = EventProposal {\\nHeight: state.Height\\nRound:  state.Round\\nBlockID: blockID\\nPOLRound: proposal.POLRound\\nSender: message.Sender\\n}\\nstate, msg, timeout = Consensus(event, state)\\nstate = handleStateChange(state, msg, timeout)\\n}\\n}\\nif prevotes.HasTwoThirdsAny() and msg.Round == state.Round {\\nevent = Majority23PrevotesAny { msg.Height, msg.Round, blockID }\\nstate, msg, timeout = Consensus(event, state)\\nstate = handleStateChange(state, msg, timeout)\\n}\\ncase Precommit:\\n}\\n}\\n}\\ncase timeout := <- timeoutCh:\\ncase block := <- blockCh:\\n}\\n}\\n}\\nfunc handleStateChange(state, msg, timeout) State {\\nif state.Step == Commit {\\nstate = ExecuteBlock(state.LockedValue)\\n}\\nif msg != nil {\\nsend msg\\n}\\nif timeout != nil {\\ntrigger timeout\\n}\\n}\\n```\\n### Implementation roadmap\\n* implement proposed implementation\\n* replace currently scattered calls in `ConsensusState` with calls to the new\\n`Consensus` function\\n* rename `ConsensusState` to `ConsensusExecutor` to avoid confusion\\n* propose design for improved separation and clear information flow between\\n`ConsensusExecutor` and `ConsensusReactor`\\n"}
{"prompt":"## Context\\nGiven our application allows making changes to the federation configuration,\\nit is neccessary to secure the service appropriately. The same instance\\nwill be accessed by different users with varying level of permissions.\\nTherefore we need to have a strong authentication and user management in place.\\nOur key authentication requirements are:\\n- Secure\\n- Flexible and extendable\\n- Cost effective\\n- Multi-factor authentication (MFA) support\\n- GDPR compliant\\n","completion":"## Decision\nBased on our requirements, we decided to use Amazon Cognito - a service provided by AWS.\\nAmazon Cognito satisfies all the requirements and offers an easy way to implement\\nauthentication in our application.\\nInitially we integrated with Cognito using OAuth. However, we soon realized the\\ncustomisation options are very limited. In the interest of more flexibility and\\na more consistent user journey we have decided to integrate with Cognito using AWS SDK and APIs.\\nIn terms of cost, Cognito is free under 50,000 monthly active user. We never envision hitting the threshold.\\nFurthermore, to ease our implementation using the SDK we have imported Devise to provide\\nthe app with an authentication framework.\\n"}
{"prompt":"## Context\\nThe system has various data types that users may want aggregated together so that they are easily accessible and sortable.\\ne.g. Types `Whale` and `Observation`. The system should have a consistent interface so that the user may access various\\ntypes of records.\\n","completion":"## Decision\nWe decided to implement a `Repository <T>` interface that can be realised by `Whale` or `Observation` objects. Users\\nwho need to access a large list\/repository of Whale's or Observation's will do so through the `Repository <T>` interface.\\n"}
{"prompt":"## Context\\nFlask turned out to be poorly designed piece of software which relays on too\\nmuch magic like manipulations of global objects like `g`.\\nSeems like we will also decide to use relational database.\\n","completion":"## Decision\nWe will switch to Django. It's not only well written server but it has also\\n\"batteries included\" like a good ORM layer. And some other features like\\nmiddlewares will simplify things.\\n"}
{"prompt":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n","completion":"## Decision\nWe will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nWe raised [spike-575](https:\/\/contino.tpondemand.com\/entity\/575) because we wanted to explore the alternatives to expose outputs from the product infrastructure pipelines to the application pipelines. This is necessary because we know that application pipelines will need to consume artificats that will be dinamically generated by other external pipelines (i.e. published urls which are outputs by Product Infrastructure Pipelines).\\n","completion":"## Decision\nFor this spike we looked at both, Azure\u2019s KeyVault and Hashicorp\u2019s Vault, as key\/value stores for making Terraform outputs available for other applications to consume in a secure manner.\\nBoth offerings are very similar however there are a couple of differences worth noting here. The first one is that KeyVault is a hosted cloud native service and key part of the Azure offering, whereas Vault will require additional (although not significant) infrastructure to be provisioned in order to run this service.\\nThe second main difference is around the Terraform support currently available. Although we have Terraform providers covering both services the resource capabilities are more extensive for the Hashicorp Vault provider. Only the Hashicorp Vault provider allow us to populate the store with secrets using terraform code (`azurerm_key_vault` only allow us to create a keyvault); this pretty much reduces the scope of this spike to Hashicorp Vault. Despite the fact that it is possible to populate KeyVault with outputs, any approach will require us to perform actions outside of the current Terraform workflow adding nothing but complexity (i.e. runnint the terraform output command and parsing stdout).\\nStoring output in Vault as part of any terraform workflow can be simple. The code below shows a very simple and basic example illustrating how this can be accomplished:\\n```code\\n.\\n\u251c\u2500\u2500 modulea\\n\u2502   \u2514\u2500\u2500 output.tf\\n\u251c\u2500\u2500 terraform.tfstate\\n\u251c\u2500\u2500 terraform.tfstate.backup\\n\u2514\u2500\u2500 vault.tf\\n1 directory, 4 files\\nportable:vaultpoc dan$ cat vault.tf\\nmodule \"testmodule\" {\\nsource = \".\/modulea\"\\n}\\nresource \"vault_generic_secret\" \"example\" {\\npath = \"secret\/foo1\"\\ndata_json = <<EOT\\n{\\n\"foo\":   \"${module.testmodule.test}\",\\n}\\nEOT\\n}\\nportable:vaultpoc dan$ cat modulea\/output.tf\\noutput  \"test\" {\\nvalue = \"testing\"\\n}\\n```\\nLikewise, it is also very easy to consume the outputs since Vault (and KeyVault as the matter of fact) is a RESTful service after all, providing a good level of abstraction which means any http client can retrieve values as shown below:\\n```code\\nportable:vaultpoc dan$ curl -H \"X-Vault-Token: 86aa6a1f-cf36-59ff-abd1-ab2624100dec\" -X GET http:\/\/127.0.0.1:8200\/v1\/secret\/foo1 | jq\\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\\nDload  Upload   Total   Spent    Left  Speed\\n100   198  100   198    0     0  34226      0 --:--:-- --:--:-- --:--:-- 39600\\n{\\n\"request_id\": \"8518b07f-bce0-dd61-f2a9-20ca8da1f212\",\\n\"lease_id\": \"\",\\n\"renewable\": false,\\n\"lease_duration\": 2764800,\\n\"data\": {\\n\"foo\": \"testing\",\\n},\\n\"wrap_info\": null,\\n\"warnings\": null,\\n\"auth\": null\\n}\\nportable:vaultpoc dan$ curl -H \"X-Vault-Token: 86aa6a1f-cf36-59ff-abd1-ab2624100dec\" -X GET http:\/\/127.0.0.1:8200\/v1\/secret\/foo1 | jq -r .data.foo\\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\\nDload  Upload   Total   Spent    Left  Speed\\n100   198  100   198    0     0  40268      0 --:--:-- --:--:-- --:--:-- 49500\\ntesting\\n```\\nRegarding naming conventions for secret storage there are many options available but we recommend to start by mirroring the agreed conventions for TF state detailed in the [general terraform guidelines](https:\/\/github.com\/contino\/moj-infrastructure-guidelines#storing-state) i.e.\\n```code\\n<statestorebackend>\/<project>\/<env>\/<output key>\\n```\\nSimilarly there are many approaches when it comes to access policies for Vault, it is a very extensive topic and well documented by [hashicorp](https:\/\/www.vaultproject.io\/docs\/concepts\/policies.html). A good starting point, however, would be - again - to mirror the policies the Terraform\/Jenkins user have against the Azure storage containers used to store the state, which at the time of this writing are pretty much matching the sudo level in Vault. With time and when we start considering more complex use cases, Vault access policies are flexible enough and provide and very good level of granularity for us to start writing more comprehensive policies.\\nFinally, it is important to understand the general guidelines when writing Terraform code but in particular, when writing Vault related resources, please do not forget the following:\\n> **Important** All data provided in the resource configuration will be written in cleartext to state and plan files generated by Terraform, and will appear in the console output when Terraform runs. Protect these artifacts accordingly. See the main provider documentation for more details.\\nOne might think that the above can be mitigated by setting the sensitive attribute in the output to true, but during our testing (specifically when running the terraform plan command) we were hitting the following limitation of sensitive outputs:\\n> **Sensitivity is not tracked internally, so if the output is interpolated in another module into a resource, the value will be displayed.**\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context\\nThis record descibes how to safely release new dapr binaries and the corresponding configurations without any blockers to users.\\n","completion":"## Decision\n### Integration build release\\nIntegration build refers to the build from `master` branch once we merge PullRequest to master branch. This build will be used for development purposes and must not be released to users and impact their environments.\\n### Official build release\\n#### Pre-release build\\nPre-release build will be built from `release-<major>.<minor>` branch and versioned by git version tag suffix e.g. `-alpha.0`, `-alpha.1`, etc. This build is not released to users who use the latest stable version.\\n**Pre-release process**\\n1. Create branch `release-<major>.<minor>` from master and push the branch. e.g. `release-0.1`\\n2. Add pre-release version tag(with suffix -alpha.0 e.g. v0.1.0-alpha.0) and push the tag\\n```\\n$ git tag \"v0.1.0-alpha.0\" -m \"v0.1.0-alpha.0\"\\n$ git push --tags\\n```\\n3. CI creates new build and push the images with only version tag\\n4. Test and validate the functionalities with the specific version\\n5. If there are regressions and bugs, fix them in release-* branch and merge back to master\\n6. Create new pre-release version tag(with suffix -alpha.1, -alpha.2, etc)\\n7. Repeat from 4 to 6 until all bugs are fixed\\n#### Release the stable version to users\\nOnce all bugs are fixed, we will create the release note under [.\/docs\/release_notes](https:\/\/github.com\/dapr\/dapr\/tree\/master\/docs\/release_notes) and run CI release manually in order to deliver the stable version to users.\\n### Release Patch version\\nWe will work on the existing `release-<major>.<minor>` branch to release patch version. Once all bugs are fixed, we will add new patch version tag, such as `v0.1.1-alpha.0`, and then release the build manually.\\n"}
{"prompt":"## Context\\nAttempts were made to upgrade all usage of Python from an older EOL'd version to Python 3. This presented a number of issues as we also use Ansible and Amazon Linux. The combination of these showed difficulty around how we use Ansible to install packages (`yum` module currently). The Ansible `yum` module is reserved specifically for Python 2. Python 3 requires usage of the `dnf` module. However, according to statements and current evidence, AWS has no intention to support a `dnf` extra for the life of Amazon Linux 2. It was also stated [in this forum post](https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=317863) \"We are committed to keeping Python 2.7 secure for the life of Amazon Linux 2\".\\nAnother attempt was then made to replace our underlying OS with CentOS 8 perhaps to see how heavy of a lift this could be. With a bit of tweaks, we were able to at least run the automation that creates encrypted base AMIs. However, this is only the surface of everything that would have to be tested\/changed for this move to happen. The referenced tweaks were as follows (for tracking purposes):\\n- Change the default value for the `bake_ami` variable found [here](https:\/\/github.com\/department-of-veterans-affairs\/devops\/blob\/master\/ansible\/applications\/config\/default.yml#L3) to a CentOS AMI ID (ami-0f5f21f0f86d11d6e was used for testing which is a \"hardened\" CentOS 8 image maintained \/ distributed by Frontline).\\n- Change all usage of the `yum` module to the `dnf` module in this [role \/ configuration file](https:\/\/github.com\/department-of-veterans-affairs\/devops\/blob\/master\/ansible\/build\/roles\/common\/base-os\/tasks\/amazon.yml).\\n- Create a task to set symbolic links after installing Python 3 for `\/usr\/bin\/python -> \/usr\/bin\/python3` and `\/usr\/bin\/pip -> \/usr\/bin\/pip3` since Python 3 installations follow the `<package>3` naming convention by default.\\n- Comment out \/ remove the task for [installing, starting, and enabling the NTP daemon](https:\/\/github.com\/department-of-veterans-affairs\/devops\/blob\/master\/ansible\/build\/roles\/common\/base-os\/tasks\/amazon.yml#L27-L37) as that package was not found with default repositories on the CentOS image and it also already comes with `chrony` installed and enabled.\\n- Comment out \/ remove Amazon Linux distro based conditional for the install of the `epel-release` package as well as an additional task that currently exists specifically for usage of Amazon Linux 2 image ([both outlined here](https:\/\/github.com\/department-of-veterans-affairs\/devops\/blob\/master\/ansible\/build\/roles\/common\/base-os\/tasks\/amazon.yml#L44-L48)).\\n- Comment out \/ remove task that [removes yum lock](https:\/\/github.com\/department-of-veterans-affairs\/devops\/blob\/master\/ansible\/build\/roles\/common\/base-os\/tasks\/amazon.yml#L53-L56) since `dnf` was the package manager \/ module used in this scenario.\\n- Addition of a task to install the ssm-agent as it would no longer come by default if we stopped leveraging Amazon Linux distributions.\\n- Drop the usage of `unicode` type in some of our custom Ansible related Python files within the `filter_plugins` directory [here](https:\/\/github.com\/department-of-veterans-affairs\/devops\/blob\/master\/ansible\/filter_plugins\/aws_finders.py#L25) and [here](https:\/\/github.com\/department-of-veterans-affairs\/devops\/blob\/master\/ansible\/filter_plugins\/resource_tags.py#L404). More on the \"why\" of that change\/dynamic can be found in [this article](https:\/\/medium.com\/better-programming\/strings-unicode-and-bytes-in-python-3-everything-you-always-wanted-to-know-27dc02ff2686).\\n","completion":"## Decision\nAfter multiple days of testing, analysis, troubleshooting, and presenting these numerous issues to the Operations team as well as a chat with Joe to gather thoughts from a security perspective, it has been decided at this time to \"accept risk\" on the continued usage of Python 2 at this time. At this time, our Python code is not end-user-facing (frontend is Ruby) and the bulk of it exists in internal scripts, lambdas, and our CI\/CD pipeline.\\nIntentions to revisit this issue would largely hinge on one of the following scenarios:\\n- Amazon Linux 3 is released with full Python 3 + dnf support\\n- Amazon Linux 2 changes direction to fully support Python 3 + dnf\\n- Through the work of our infrastructure recommit, if we can get to a fully containerized environment, we will ensure the images used do not leverage the end of life'd version of Python if at all possible.\\n"}
{"prompt":"## Context\\nConsumers of the Vendor API can filter applications by updated_at value, to get a list of applications that have changed since a certain time.\\nThey typically use this to avoid processing applications that have already been ingested into their services.\\nIf the updated_at values are inaccurate, there is a potential for vendors to process applications twice or miss application changes entirely.\\nFor that reason, we set up touch relationships from the application form to the application choice to ensure that changes to the application form touch the application choices.\\nThis approach resulted in false positives (where the updated_at value had changed but the data in the application API response had not). This is because there are fields on the application form which do not appear in the application API response.\\nFor example, when geocoding data was added to applications. Every application was touched, but the data was not included in the response so API consumers could not see any changes.\\n","completion":"## Decision\nFor changes to the application form we will specify a whitelist of attributes that, when changed, affect the application API response. There are pros and cons to this method:\\nPros:\\n- We keep an explicit list of significant attributes\\n- This is a simple approach using standard Rails techniques, so is easy to follow\\nCons:\\n- Forgetting to mark a field as significant will result in the application updated_at not being changed (false negative)\\n- Forgetting to remove a field from the list of significant ones when it no longer affects an application response will result in the application updated_at being changed (false positive)\\nIn order to mitigate the cons, we have added specs to check that the list of significant attributes contains all attributes which affect an API response, and that there are no extra attributes included.\\nSee [single_application_presenter_spec.rb](..\/spec\/presenters\/vendor_api\/single_application_presenter_spec.rb)\\nThis solution will require that spec to be kept up to date with a variety of applications in different states so as to cover all the code paths the application presenter has.\\nAs described so far, this solution only addresses changes from the ApplicationForm model that affect API responses for ApplicationChoices.\\nThere are other models (e.g. ApplicationQualification or ApplicationWorkExperience) for which changes to their attributes affect their associated applications.\\nFor these smaller models, we have added a concern TouchApplicationChoices which touches the application_choices whenever the model is created, updated or deleted.\\n"}
{"prompt":"## Context\\nFor the app to be able to login a user to the dashboard it needs to send a\\nblind proof credential for the correct authorizable attribute\/community to\\nthe dashboard.\\nThe login flow starts by the user being given a URL with the following two\\nparameters: `sessionId` and `callback`. The `sessionId` value must be passed\\nback to the dashboard so the correct user can be identified and a session\\ncreated, and the `callback` url is where the request must be sent.\\nThe page rendered by the webapp must show a content like saying:\\n> BCNNow requires a credential to log you in.\\n> You can login by sharing a secure credential with the dashboard.\\n> [Login Button]\\nOn pressing the Login button the front end needs to start the process to log\\nthe user in described below.\\nOn successfully logging in the app should send a login request to the\\ndashboard somehow (structure described below), and display a successful login\\nmessage.\\n","completion":"## Decision\nThe Vue front end will send the required information to the backend via the\\nexisting websocket\/channels; the backend will receive the message from the\\nfront end, and then wrap it up and send it on to the dashboard api over HTTP.\\n### Front end message\\nThe message will be emitted as a `dashboard-login` event with the following body:\\n```json\\n{\\n\"callback\": \"http:\/\/bcnnow.decodeproject.eu\/oauth\/iot_login_callback\",\\n\"request\": {\\n\"sessionId\": \"abc123\",\\n\"credential\": {\\n\"authorizable_attribute_id\": \"foobar\",\\n\"value\": {\\n\"proof\": blind_proof_credential.proof\\n},\\n\"credential_issuer_endpoint_address\": \"https:\/\/credentials.decodeproject.eu\"\\n},\\n\"optionalAttributes: []\\n}\\n}\\n```\\nThe backend will receive the above message, and attempt to send the request\\nto the given callback URL. This could have just been done directly by the\\nfront end here, but we will treat like all other requests, i.e. the front end\\nonly sends socket messages to the backend, and the backend is responsible for\\nsending out all remote requests.\\nSo the backend will send the request object to the specified callback URL\\nwhich will either return 200 OK or an error. This value must be returned to\\nthe frontend via a reply to the channel (either `ok` or `error`).\\nOn receiving the response in the front end we will either render a success or\\nfailure message.\\n"}
{"prompt":"## Context\\nHad a go with `esbuild`, and it *is* very fast, but we can't use yet due to runtime failures in the browser.\\n> 'Dynamic require of \"http\" is not supported'\\nWhich stems from our usage of `rss-parser` in `src\/adapters\/rss\/feed.ts`.\\n### Tips\\nOur current `esbuild` fails at runtime, but we alse needed to resolve a couple of problems to get the build itself to work.\\nWe ended up with:\\n```\\n.\/node_modules\/.bin\/esbuild .\/src\/adapters\/application\/real-application.js\\\\n--external:http --external:https --external:timers\\\\n--bundle --global-name=real --outfile=.\/src\/adapters\/web\/gui\/assets\/dist\/real.bundle.js\\n```\\nWhich resolves the following issues.\\n#### How to allow node libraries like `http`\\nSolution is to use `--external`.\\nWithout `--external:http --external:https --external:timers` you get these errors:\\n```\\n> node_modules\/rss-parser\/lib\/parser.js:2:21: error: Could not resolve \"http\" (use \"--platform=node\" when building for node)\\n2 \u2502 const http = require('http');\\n\u2575                      ~~~~~~\\n> node_modules\/rss-parser\/lib\/parser.js:3:22: error: Could not resolve \"https\" (use \"--platform=node\" when building for node)\\n3 \u2502 const https = require('https');\\n\u2575                       ~~~~~~~\\n> node_modules\/xml2js\/lib\/parser.js:17:25: error: Could not resolve \"timers\" (use \"--platform=node\" when building for node)\\n17 \u2502   setImmediate = require('timers').setImmediate;\\n\u2575                          ~~~~~~~~\\n```\\n#### How to make `real.*` to work\\nFor example, in the html page we do this:\\n```js\\nvar toggles = new real.QueryStringToggles(document.location.search);\\n```\\nTo make `real.QueryStringToggles` available, use [`--global-name`](https:\/\/esbuild.github.io\/api\/#global-name).\\n```\\n--global-name=real\\n```\\nThis is like webpack's `library` field.\\n","completion":"## Decision\nwait for later version.\\n"}
{"prompt":"## Context\\nThe Voice Assistance Platform's job is to identify if the call is implicit or an explicit utterance. If explicit, then it will engage and call VRS.\\nOur goal is to identify who is responsible for determining the VRS in the utterance.\\n","completion":"## Decision\nBased on the meeting's (03.04.2021) discussion, the quorum for the best part forward is option 1. The biggest reason is keeping the role of the VRS as straightforward as possible. The team acknowledged that the lower-level details solutions need to be done in partnership with Voice Assistant Platform.\\n<br>\\n"}
{"prompt":"## Context\\nCurrently, the messages exchanged between tendermint and a (potentially remote) signer\/validator,\\nnamely votes, proposals, and heartbeats, are encoded as a JSON string\\n(e.g., via `Vote.SignBytes(...)`) and then\\nsigned . JSON encoding is sub-optimal for both, hardware wallets\\nand for usage in ethereum smart contracts. Both is laid down in detail in [issue#1622].\\nAlso, there are currently no differences between sign-request and -replies. Also, there is no possibility\\nfor a remote signer to include an error code or message in case something went wrong.\\nThe messages exchanged between tendermint and a remote signer currently live in\\n[privval\/socket.go] and encapsulate the corresponding types in [types].\\n[privval\/socket.go]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/privval\/socket.go#L496-L502\\n[issue#1622]: https:\/\/github.com\/tendermint\/tendermint\/issues\/1622\\n[types]: https:\/\/github.com\/tendermint\/tendermint\/tree\/master\/types\\n","completion":"## Decision\n- restructure vote, proposal, and heartbeat such that their encoding is easily parseable by\\nhardware devices and smart contracts using a  binary encoding format ([amino] in this case)\\n- split up the messages exchanged between tendermint and remote signers into requests and\\nresponses (see details below)\\n- include an error type in responses\\n### Overview\\n```\\n+--------------+                      +----------------+\\n|              |     SignXRequest     |                |\\n|Remote signer |<---------------------+  tendermint    |\\n| (e.g. KMS)   |                      |                |\\n|              +--------------------->|                |\\n+--------------+    SignedXReply      +----------------+\\nSignXRequest {\\nx: X\\n}\\nSignedXReply {\\nx: X\\nsig: Signature \/\/ []byte\\nerr: Error{\\ncode: int\\ndesc: string\\n}\\n}\\n```\\nTODO: Alternatively, the type `X` might directly include the signature. A lot of places expect a vote with a\\nsignature and do not necessarily deal with \"Replies\".\\nStill exploring what would work best here.\\nThis would look like (exemplified using X = Vote):\\n```\\nVote {\\n\/\/ all fields besides signature\\n}\\nSignedVote {\\nVote Vote\\nSignature []byte\\n}\\nSignVoteRequest {\\nVote Vote\\n}\\nSignedVoteReply {\\nVote SignedVote\\nErr  Error\\n}\\n```\\n**Note:** There was a related discussion around including a fingerprint of, or, the whole public-key\\ninto each sign-request to tell the signer which corresponding private-key to\\nuse to sign the message. This is particularly relevant in the context of the KMS\\nbut is currently not considered in this ADR.\\n[amino]: https:\/\/github.com\/tendermint\/go-amino\/\\n### Vote\\nAs explained in [issue#1622] `Vote` will be changed to contain the following fields\\n(notation in protobuf-like syntax for easy readability):\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Vote {\\nVersion       fixed32\\nHeight        sfixed64\\nRound         sfixed32\\nVoteType      fixed32\\nTimestamp     Timestamp         \/\/ << using protobuf definition\\nBlockID       BlockID           \/\/ << as already defined\\nChainID       string            \/\/ at the end because length could vary a lot\\n}\\n\/\/ this is an amino registered type; like currently privval.SignVoteMsg:\\n\/\/ registered with \"tendermint\/socketpv\/SignVoteRequest\"\\nmessage SignVoteRequest {\\nVote vote\\n}\\n\/\/  amino registered type\\n\/\/ registered with \"tendermint\/socketpv\/SignedVoteReply\"\\nmessage SignedVoteReply {\\nVote      Vote\\nSignature Signature\\nErr       Error\\n}\\n\/\/ we will use this type everywhere below\\nmessage Error {\\nType        uint  \/\/ error code\\nDescription string  \/\/ optional description\\n}\\n```\\nThe `ChainID` gets moved into the vote message directly. Previously, it was injected\\nusing the [Signable] interface method `SignBytes(chainID string) []byte`. Also, the\\nsignature won't be included directly, only in the corresponding `SignedVoteReply` message.\\n[Signable]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/types\/signable.go#L9-L11\\n### Proposal\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Proposal {\\nHeight            sfixed64\\nRound             sfixed32\\nTimestamp         Timestamp         \/\/ << using protobuf definition\\nBlockPartsHeader  PartSetHeader     \/\/ as already defined\\nPOLRound          sfixed32\\nPOLBlockID        BlockID           \/\/ << as already defined\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalRequest\"\\nmessage SignProposalRequest {\\nProposal proposal\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalReply\"\\nmessage SignProposalReply {\\nProp   Proposal\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n### Heartbeat\\n**TODO**: clarify if heartbeat also needs a fixed offset and update the fields accordingly:\\n```proto\\nmessage Heartbeat {\\nValidatorAddress Address\\nValidatorIndex   int\\nHeight           int64\\nRound            int\\nSequence         int\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatRequest\"\\nmessage SignHeartbeatRequest {\\nHb Heartbeat\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatReply\"\\nmessage SignHeartbeatReply {\\nHb     Heartbeat\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n"}
{"prompt":"## Context\\nArchitectural Decision 003 Use a common records layer decides to use a common records layer. Common Sourcing design defines how to interact various components in a coordinated sourcing strategy.\\nAll Agreement services need party and agreement records to coordinate supplier registration and DPS qualification with the buyer journey.\\n","completion":"## Decision\nAll Agreement services need party and agreement records to coordinate supplier registration and DPS qualification with the buyer journey.\\n- Store all new suppliers and updates using Party API\\n- Store all new buyers and updates using Party API\\n- Retrieve details of services and supplier offers from Agreement API\\n- Retrieve attachments from Document store (S3, with metadata in agreements)\\n- Store purchase orders in Agreements API where appropriate\\n- Synchronise with SalesForce as needed\\nAs Suppliers are added to, removed from or change their offer on agreements, the service will call the common record API to update all required consumers.\\nStore relevant supporting documents and attachments directly in AWS S3, with a supporting document API to synchronise with SalesForce content linked to document URI. Salesforce API links may be fragile and have access limits. S3 documents have an easier API for volume based consumption. The S3 store can be used to insulate the service for SalesForce web releases.\\nThe agreement and Party APIs have not been defined yet but will use json over HTTPS with RESTful access patterns, with OAuth identity tokens. This is in line with GDS guidelines. OAuth is used instead of simple shared secrets because the number of API clients is likely to be large and fluid. Data definitions will be controlled by CCS but will map to OCDS standards wherever practical.\\nAgreement records will contain:\\n- Agreements, lot structure, items (services) and related content, and call-offs\\n- Interests linking suppliers (by ID) to Agreements and their offers\\n- Offers from Suppliers linked to the relevant part of agreements services\\n- Purchase orders where appropriate\\nParty records will contain details about buyer and supplier organizations (not individuals):\\n- Supplier identifiers and details\\n- supplier qualifications questions and answers\\n- other details not specific to agreements\\n- Buyer identifiers and details.\\n- articulation of various user needs not specific to agreements, such as details of physical estate, people, plans, budgets.\\nIndividual data, where needed, will be stored as user ID references to CCS ID only.\\n"}
{"prompt":"## Context\\nSome parts of the code were quite different and some \"bad practices\" started to appear. Because no person in the team is an expert in TS, we believe a stricter static analysis process could help.\\n","completion":"## Decision\nBecause the community mainly accepts Google's TSLint rules as a good practice, we will use them.\\n"}
{"prompt":"## Context\\nWe need a process to bootstrap a new Amazon environment. This should be both heavily automated\\nand include set places where the operator can evaluate the status of the provisioning.\\nThis bootstrapping process assumes the following:\\n* The required repositories are available\\n* An Amazon Web Services admin account is available\\n* Backups of our data can be retrieved (for the restore process)\\n","completion":"## Decision\nThe chosen process has a few, early bootstrap steps, that differ from normal operations. These\\naim to quickly provide the usual level of self-service to allow teams to independently\\nrestore services without a dependency on a central operations team. An early draft of\\nthis process, which will be updated as changes are required, will be:\\n1. Clone all the relevant repositories\\n1. Build the S3 bucket for Terraform state\\n1. Provision the new DNS zones for this environment\\n1. Build the Puppet master\\n1. Deploy the puppet code and secrets\\n1. Build the deploy Jenkins\\n* Rebuild everything else in the usual deployment ways\\nThis document will be written so that one of the non-infrastructure members of the team\\nwill be able to provision a complete environment within our allowed recovery time.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context\\nArticles need tags to ease classification and navigation across similar topics.\\nTags must have a name, slug, language, optional description and optional image.\\nGiven this complexity, we cannot use embedded documents.\\nThe relation between tags' translations is not useful.\\nWe want to be able to retrieve articles for a given tag.\\nWe want to suggest tags for a given language when an article is\\ncreated\/modified.\\nWe want to be able to modify the description\/image for a given tag.\\n","completion":"## Decision\nCustom implementation of tags as a separate collection.\\n"}
{"prompt":"## Context\\nThe current design of task scheduler creates one queue per repo. We can't see how this can work with an out-of-process worker, since the worker needs to know in advance which queues to listen on, and the list of repos could change.\\nThis design was primarily influenced by needing a way to synchonise the acceptance tests at the point where the jobs for a repo had all finished.\\nAs we work to implement feedback when repo connections fail, this design was causing friction.\\n","completion":"## Decision\nIts simpler for the time being to remove the task scheduler, so repo connections and fetches will be immediately consistent, for the time being.\\n"}
{"prompt":"## Context\\nIn `Wegwijs` we used a memory cache across all event handlers.\\nThese memory caches, however, were built up from certain projections. An example of this was the `organisation names` cache, which was built up from the `OrganisationDetail` projection.\\nUsing the memory cache outside the in-process event handlers, where the `OrganisationDetail` projection was built, causes a dependency between projections. This means that if we now want to rebuild projection X while the OrganisationDetail is not fully built, the memory cache is in an incorrect state. This defeats the purpose of the memory cache.\\n","completion":"## Decision\nWe will re-evaluate the use of memory caches. If we do use them, we will use them fully isolated within the containing handler.\\n"}
{"prompt":"# Context\\n* Is the [decentraland\/standards](https:\/\/github.com\/decentraland\/standards) repository still useful?\\n* Does it make sense to have ADR and standards?\\n* How does it play with [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals)?\\n# Options\\n* Archive both repositories, leaving only [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n* \u2705 Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards), leaving [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\n* Only use [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n# Decision\\n* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n","completion":"## Decision\n* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n"}
{"prompt":"## Context\\nSome of the target collections are already present in Dome, and it is simpler\\nto use Dome as the ingest source than upstream systems. Further, as Dome supports\\nOAI-PMH and OAI-ORE, these interfaces could be used to retrieve\\nrelevant information, such as the bitstreams of digital assets, directly from Dome.\\nTo accomplish this, a custom ingest script or a third-party tool could be used for this purpose.\\nSuch a tool is available for TIMDEX related ingests [link]() and could be modified for ingesting\\ndigital assets to DOS.\\n","completion":"## Decision\nIngest will be handled through the existing OAI-PMH\/ORE tool in conjunction with scripts written to prepare that output for the DOS API.\\n"}
{"prompt":"## Context and Problem Statement\\nKnow it before they do!\\nWe need a tool to discover, triage, and prioritize errors in real-time.\\n","completion":"## Decision\nChosen option: `Sentry`, because it ranks higher in a community survey regarding our stack (Javascript). It's also much cheaper and offers the choice to be completely free if we self-host it.\\n"}
{"prompt":"## Context\\nConsidering only inferences, we have two endpoints that are responsible to take inputs for prediction.\\n- application\/json\\n- \/:predictionType\/:inputDataFormat # For JSON inputs\\n- multipart\/form-data\\n- \/:predictionType\/:inputDataFormat # For multipart inputs (FILES)\\nGiven those two endpoints we have done a ton of work on creating a python client that (as of right now) needs more proper documentation, examples and maintenance planning for making the APIs more adaptable.\\nSince, we're planning to have APIs in multiple languages (python, java, C# and others), it might get difficult to maintain and document them separately in the future.\\n","completion":"## Decision\n### Using swagger annotations to generate and document client APIs.\\n[Swagger annotations](https:\/\/github.com\/swagger-api\/swagger-core\/wiki\/Swagger-2.X---Annotations) are a quick and easy way to generate openapi specifications from the source code. Annotations apply to classes, methods and arguments. Using this way, it's easier to get rid of client APIs maintenance (generation, documentation and packaging) in different languages.\\n#### How it works?\\nBy refactoring konduit-serving source code into classes that contain APIs for different types of verticles. For example:\\n```java\\n@Path(\"\/\")\\n@Produces(MediaType.APPLICATION_JSON)\\npublic class InferenceApi {\\n@GET\\n@Path(\"\/config\")\\npublic InferenceConfiguration getConfig() {\\nreturn new InferenceConfiguration();\\n}\\n@POST\\n@Path(\"\/{predictionType}\/{inputDataFormat}\")\\n@Consumes(MediaType.MULTIPART_FORM_DATA)\\n@Operation(summary = \"Get inference result with multipart data.\",\\ntags = {\"inference\"},\\ndescription = \"You can send multipart data for inference where the input names will be the names of the inputs of a transformation process. \" +\\n\"or a model input and the corresponding files containing the data for each input.\",\\nresponses = {\\n@ApiResponse(description = \"Batch output data\",\\nresponseCode = \"200\",\\ncontent = @Content(schema = @Schema(oneOf = {\\nClassifierOutput.class,\\nRegressionOutput.class,\\nDetectedObjectsBatch.class,\\nManyDetectedObjects.class\\n}))\\n),\\n}\\n)\\npublic BatchOutput predict(@PathParam(\"predictionType\") Output.PredictionType predictionType,\\n@PathParam(\"inputDataFormat\") Input.DataFormat inputDataFormat,\\n@Parameter(description = \"An array of files to upload.\") File[] multipartInput) {\\nreturn new ClassifierOutput();\\n}\\n}\\n```\\nThis will require similar refactoring for the other verticles and their respective routers. Currently, the following classes will have to be refactored based on the above details:\\n- PipelineRouteDefiner\\n- MemMapRouteDefiner\\n- ConverterInferenceVerticle\\n- ClusteredVerticle\\n#### How it would look at the end?\\nHaving an API that looks like the class above will generate an API specification that will look like:\\n```yaml\\nopenapi: 3.0.1\\ninfo:\\ntitle: Konduit Serving REST API\\ndescription: RESTful API for various operations inside konduit-serving\\ncontact:\\nname: Konduit AI\\nurl: https:\/\/konduit.ai\/contact\\nemail: hello@konduit.ai\\nlicense:\\nname: Apache 2.0\\nurl: https:\/\/github.com\/KonduitAI\/konduit-serving\/blob\/master\/LICENSE\\nversion: 0.1.0-SNAPSHOT\\nexternalDocs:\\ndescription: Online documentation\\nurl: https:\/\/serving.oss.konduit.ai\\ntags:\\n- name: inference\\ndescription: Tag for grouping inference server operations\\n- name: convert\\ndescription: Tag for grouping converter operations\\n- name: memmap\\ndescription: Tag for grouping memory mapping operations\\npaths:\\n\/config:\\nget:\\noperationId: getConfig\\nresponses:\\ndefault:\\ndescription: default response\\ncontent:\\napplication\/json:\\nschema:\\n$ref: '#\/components\/schemas\/InferenceConfiguration'\\n\/{predictionType}\/{inputDataFormat}:\\npost:\\ntags:\\n- inference\\nsummary: Get inference result with multipart data.\\ndescription: You can send multipart data for inference where the input names\\nwill be the names of the inputs of a transformation process. or a model input\\nand the corresponding files containing the data for each input.\\noperationId: predict\\nparameters:\\n- name: predictionType\\nin: path\\nrequired: true\\nschema:\\ntype: string\\nenum:\\n- CLASSIFICATION\\n- YOLO\\n- SSD\\n- RCNN\\n- RAW\\n- REGRESSION\\n- name: inputDataFormat\\nin: path\\nrequired: true\\nschema:\\ntype: string\\nenum:\\n- NUMPY\\n- JSON\\n- ND4J\\n- IMAGE\\n- ARROW\\nrequestBody:\\ndescription: An array of files to upload.\\ncontent:\\nmultipart\/form-data:\\nschema:\\ntype: array\\nitems:\\ntype: string\\nformat: binary\\nresponses:\\n\"200\":\\ndescription: Batch output data\\ncontent:\\napplication\/json:\\nschema:\\noneOf:\\n- $ref: '#\/components\/schemas\/ClassifierOutput'\\n- $ref: '#\/components\/schemas\/RegressionOutput'\\n- $ref: '#\/components\/schemas\/DetectedObjectsBatch'\\n- $ref: '#\/components\/schemas\/ManyDetectedObjects'\\ncomponents:\\nschemas:\\nInferenceConfiguration:\\ntype: object\\nproperties:\\nsteps:\\ntype: array\\nitems:\\n$ref: '#\/components\/schemas\/PipelineStep'\\nservingConfig:\\n$ref: '#\/components\/schemas\/ServingConfig'\\nmemMapConfig:\\n$ref: '#\/components\/schemas\/MemMapConfig'\\nMemMapConfig:\\ntype: object\\nproperties:\\narrayPath:\\ntype: string\\nunkVectorPath:\\ntype: string\\ninitialMemmapSize:\\ntype: integer\\nformat: int64\\nworkSpaceName:\\ntype: string\\nPipelineStep:\\ntype: object\\nproperties:\\ninput:\\n$ref: '#\/components\/schemas\/PipelineStep'\\noutput:\\n$ref: '#\/components\/schemas\/PipelineStep'\\noutputColumnNames:\\ntype: object\\nadditionalProperties:\\ntype: array\\nitems:\\ntype: string\\ninputColumnNames:\\ntype: object\\nadditionalProperties:\\ntype: array\\nitems:\\ntype: string\\ninputSchemas:\\ntype: object\\nadditionalProperties:\\ntype: array\\nitems:\\ntype: string\\nenum:\\n- String\\n- Integer\\n- Long\\n- Double\\n- Float\\n- Categorical\\n- Time\\n- Bytes\\n- Boolean\\n- NDArray\\n- Image\\noutputSchemas:\\ntype: object\\nadditionalProperties:\\ntype: array\\nitems:\\ntype: string\\nenum:\\n- String\\n- Integer\\n- Long\\n- Double\\n- Float\\n- Categorical\\n- Time\\n- Bytes\\n- Boolean\\n- NDArray\\n- Image\\noutputNames:\\ntype: array\\nitems:\\ntype: string\\ninputNames:\\ntype: array\\nitems:\\ntype: string\\nServingConfig:\\ntype: object\\nproperties:\\nhttpPort:\\ntype: integer\\nformat: int32\\nlistenHost:\\ntype: string\\noutputDataFormat:\\ntype: string\\nenum:\\n- NUMPY\\n- JSON\\n- ND4J\\n- ARROW\\nuploadsDirectory:\\ntype: string\\nlogTimings:\\ntype: boolean\\nincludeMetrics:\\ntype: boolean\\nmetricTypes:\\ntype: array\\nitems:\\ntype: string\\nenum:\\n- CLASS_LOADER\\n- JVM_MEMORY\\n- JVM_GC\\n- PROCESSOR\\n- JVM_THREAD\\n- LOGGING_METRICS\\n- NATIVE\\n- GPU\\nClassifierOutput:\\ntype: object\\nproperties:\\ndecisions:\\ntype: array\\nitems:\\ntype: integer\\nformat: int32\\nprobabilities:\\ntype: array\\nitems:\\ntype: array\\nitems:\\ntype: number\\nformat: double\\nlabels:\\ntype: array\\nitems:\\ntype: string\\nbatchId:\\ntype: string\\nRegressionOutput:\\ntype: object\\nproperties:\\nvalues:\\ntype: array\\nitems:\\ntype: array\\nitems:\\ntype: number\\nformat: double\\nbatchId:\\ntype: string\\nDetectedObjectsBatch:\\ntype: object\\nproperties:\\ncenterX:\\ntype: number\\nformat: float\\ncenterY:\\ntype: number\\nformat: float\\nwidth:\\ntype: number\\nformat: float\\nheight:\\ntype: number\\nformat: float\\npredictedClassNumbers:\\ntype: array\\nitems:\\ntype: integer\\nformat: int32\\npredictedClasses:\\ntype: array\\nitems:\\ntype: string\\nconfidences:\\ntype: array\\nitems:\\ntype: number\\nformat: float\\nbatchId:\\ntype: string\\nManyDetectedObjects:\\ntype: object\\nproperties:\\ndetectedObjectsBatches:\\ntype: array\\nitems:\\n$ref: '#\/components\/schemas\/DetectedObjectsBatch'\\nbatchId:\\ntype: string\\nwriteOnly: true\\nBatchOutput:\\ntype: object\\nproperties:\\nbatchId:\\ntype: string\\nwriteOnly: true\\n```\\nAnd from this yaml the clients will be generated using [openapi-generator](https:\/\/github.com\/OpenAPITools\/openapi-generator). For example:\\n```bash\\njava -jar openapi-generator-cli.jar generate -i openapi.yaml -g python -o python_api_client\\n```\\nThe above command will generate the python clients for us and the related docs for using the API in python.\\nThis will be a similar process for other languages as well.\\n"}
{"prompt":"## Context\\nWe need a workflow to build our appliation and components.\\n","completion":"## Decision\nWe use Storybook only for building new self contained components.\\n"}
{"prompt":"## Context\\nElasticSearch (ES) is used as the search engine for:\\n- UI search for vacancies\\n- Querying vacancies to construct email job alerts based on a users criteria.\\nThey both currently use a ruby ES client library to submit queries. Both use a base client class(es) that search and job alerts functionality extend from.\\nCurrent search has been identified as not giving optimal search results and job alerts are based on very simplistic search rules currently. Is there a better tool to use that is more easily configurable?\\nCurrently indexing and maintaining ranking of search terms is an engineering task.\\n","completion":"## Decision\nAlgolia was identified an alternative as it:\\n- has easier\/improved 'out of the box' functionality yielding better search results\\n- is up to 200x faster than ES\\n- has a comprehensive dashboard UI that means rankings\/weightings could potentially be managed by wider range of people\\n- has a comprehensive toolset including UI components and libraries\\n"}
{"prompt":"# Context and Problem Statement\\nSome PDFs contain only images (when coming from a scanner) and\\ntherefore one is not able to click into the pdf and select text for\\ncopy&paste. Also it is not searchable in a PDF viewer. These are\\nreally shortcomings that can be fixed, especially when there is\\nalready OCR build in.\\nFor images, this works already as tesseract is used to create the PDF\\nfiles. Tesseract creates the files with an additional text layer\\ncontaining the OCRed text.\\n# Considered Options\\n* [ocrmypdf](https:\/\/github.com\/jbarlow83\/OCRmyPDF) OCRmyPDF adds an\\nOCR text layer to scanned PDF files, allowing them to be searched\\n","completion":"## Decision\nAdd ocrmypdf as an optional conversion from PDF to PDF. Ocrmypdf is\\ndistributed under the GPL-3 license.\\n"}
{"prompt":"## Context\\nNd4j relies upon the c++ library [libnd4j](..\/libnd4j) for native math execution.\\nIt uses [javacpp](https:\/\/github.com\/bytedeco\/javacpp) to link against\\nlibnd4j. Libnd4j is capable of being compiled a myriad of different ways allowing different trade offs to be made\\nin terms of performance and dependencies. This presents complexity in exposing this flexibility to the end user.\\n","completion":"## Decision\nIn order to allow users to pick which configuration they would like to use, while avoiding adding a lot of different artifact\\nids to the project, the following javacpp platform extensions are used:\\ncompiled type (avx etc or blank if normal) - software linked against (cudnn, onednn, armcompute) - version\\nAn example for the one dnn platform extension could be:\\ndnnl-2.2\\navx256-dnnl-2.2\\nThis presents 2 examples where a special compilation is enabled and one where it's not\\nboth linking against dnnl\/onednn 2.2.\\n"}
{"prompt":"## Context\\nWe need to decide whether to run PostgreSQL on the Puppetmaster itself or use\\nRDS from the off.\\nCurrently our Puppetmaster uses a local PostgreSQL instance as its database. In moving to AWS we have the opportunity to replace this with an RDS instance.\\nThis may remove a reasonable amount of puppet code used to configure the database and associated tasks (e.g. backups). It would also require an amount of Terraform work to provision the RDS instance and some updates to the Puppetmaster code to use that instance.\\n","completion":"## Decision\nThe Puppetmaster will continue to use a local instance of PostgreSQL.\\n"}
{"prompt":"## Context\\nWe need an overview for the execution architecture\\n","completion":"## Decision\nTruBudget can be deployed either locally or in the cloud using Docker. Interfaces are exposed via web interface and via blockchain connection.\\n![Overview of architecture](.\/img\/0007-execution-architecture-overview.png)\\n"}
{"prompt":"## Context\\nA well constructed macondo command should be easy to run directly (without\\nmacondo) by just executing it. This is hard to achieve unless the user in the\\ncontainer resembles as much as possible the host running it.\\nOne case where this is important is modifying files in the host. This is\\nachieved by mounting a volume into the running container. However, the files\\nwritten from the docker container are owned by the docker user, i.e. the user id\\nand group id of the file is that of whatever docker user happened to write it.\\nSo if the container runs with the root user, which is unfortunately common, then\\nthe files written to the host also are owned by root, making them innacessible\\nto the host user.\\nPotential ideas:\\n- A flag that enables user alignment and mounts HOME into docker's HOME.\\n- Customize home when running in OSX to be \/Users\/bla instead of \/home\/ble\\n- Even if mounting the whole home, it should be easy to mount current PWD into\\nsomething else \/mnt\/blablabla and use tha as working directory\\n","completion":"## Decision\nAdd a mechanism to align the user\/group of the host system with that of the docker container.\\n"}
{"prompt":"## Context and Problem Statement\\nThe FxA back-end service stack contains an application architecture that is ad-hoc, not documented, and missing modern features (such as Dependency Injection) which results in the following problems:\\n- New developers struggle to get up to speed as they must learn the architecture by reading the code as we have no documentation on the application structure, why they're structured the way they are, or how new components should be added to fit in. Each back-end service may vary in its ad-hoc architecture as well.\\n- Adding new objects needed in a route handler can be time-consuming as the object must be plumbed through the entire initialization chain vs. more elegant methods like Dependency Injection (DI).\\n- Not clear where\/how to add new components and takes time to study\/understand how things are currently setup in an attempt to mimic the structure for the new component.\\n- Time consuming to setup boiler-plate for components, as we have no tooling to work with the current ad-hoc application architectures.\\n- Our ad-hoc architecture frequently mixes concerns such as having business logic mixed in with request handling logic, and has other warts from its evolution over time vs. being planned up front.\\n- New back-end services evolve differently resulting in more ad-hoc application architectures to learn.\\n- Shared components in `fxa-shared` can't rely on basic object lifecycles or setup approaches as they may be used in multiple different ad-hoc application architectures.\\nNot choosing an application framework means that we have choosen to make ad-hoc application architectures which will continue to exhibit the problems above.\\nIt is assumed that the four newest FxA back-end services (admin-server, support-panel, event-broker, gql-api) will be switched to the chosen approach for consistency.\\n## Decision Drivers\\n- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\n","completion":"## Decision\n- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\nChosen Option: Use an existing framework: NestJS\\nNestJS and LoopBack are the two most compelling options, however NestJS has substantially better GraphQL support and a much larger user-base. LoopBack has its user-base divided between two versions with substantial changes between them. We will use NestJS for new projects and update the newest services (admin-server, support-panel, event-broker, gql-api) to NestJS.\\nTo reduce documentation needs and address that auth-server will not be reasonable to migrate, we will backport code organization schemes and directory\/filename conventions from NestJS and use `typedi` as a DI system to ease configuration\/testing in auth-server and possibly other legacy packages. This will allow developers familiar with NestJS projects to still locate and easily work on FxA legacy packages.\\n### Positive Consequences\\n- Less documentation to create, as we only document differences for legacy services.\\n- NestJS GraphQL support matches existing typegraphql paradigm closely for minimal effort switch.\\n### Negative Consequences\\n- There will be some effort involved to migrate the 4 packages to NestJS.\\n- We will need to document the differences between NestJS and other legacy services.\\n"}
{"prompt":"## Context\\nWe want to provide users with the pvalue of each individual family under the lambda provided (either\\ncalculated or provided by the user)\\n","completion":"## Decision\nThe procedure is as follows:\\n1. For every possible root family size (starting with 1, not 0), randomly generate 1000 families based on the lambda\\n2. Compute the likelihood of each family generated, sort the results smallest to largest. The result is the\\nconditional distribution.\\n3. Compute the likelihood of each of the user's families for every root family size\\n4. For every root family size less than 125% of the largest species size in the family, compute a pvalue for each\\nof the user's families, based on the conditional distribution at that family size\\n5. Take the maximum pvalue calculated\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context and Problem Statement\\nWe need to maintain the quality of the codebase, minimize the time between introducing quality degradation and discovering it and make sure we have deployable artefacts at all times. In the context of a [monorepo](..\/monorepo.md) we need to do this efficiently in order to make this process scale for an ever-growing number of projects in the repository.\\n### Terms\\n- `code integration` - this is a process that checks the integrity\/quality of the code - static code analysis, code formatting, compilation, running automated tests, etc. The process is usually in the form of one or more scripts and uses tools local to the repository with minimum external dependencies.\\n- `artefact building` - this is a process that packages artefacts, labels them and usually publishes them to a central artefact repository so that they can be used by the deployment process. This process makes sense to be executed only after `code integration` process finishes successfully.\\n- `continuous integration` - the practice of running the `code integration process` triggered by events such as\\n- pushing new revisions of the code to the code repository\\n- opening a Pull Request\\n- fixed schedule\\n- etc.\\nWe also run the `artefact building` process after a successful `code integration` process to have artefacts ready for deployment at all times.\\n- `continuous integration platform` (CI platform from here on) - it is a platform (self-hosted or SaaS) that provides integrations to make it easy to run your `continuous integration` and publish the results\\n## Decision Drivers (Policy)\\n- The code integration process needs to be independent from CI platform integration\\n- Benefits\\n- Easier troubleshooting and development of the process\\n- Easier migration to a different CI platform\\n- Easier experimentation\\n- Easier to run as part of the development process\\n- Drawbacks\\n- Needs more knowledge and experience\\n- We use Docker as much as possible to implement the steps in the integration process\\n- Benefits\\n- Platform independence\\n- Repeatability\\n- Security\\n- Drawbacks:\\n- Needs expertise in Dockerfile and Docker in general\\n- We only build the code affected by the change but re-tag all unchanged code artefacts\\n- Benefits\\n- Be able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hash\\n- Supports the monorepo benefits and ideology\\n- Drawbacks\\n- Can be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)\\n- We support only Linux as a target operating system when we cannot use Docker\\n- Benefits\\n- Same as the OS we use in our production environment, which minimizes the chances of failure because of OS differences\\n- We minimize the effort and complexity of supporting different operating systems\\n- Linux tooling works on all major other OSs today - macOS and Windows\\n- Drawbacks:\\n- Devs that use non-Linux OS might need to install additional software and customizations\\n","completion":"## Decision\n- The code integration process needs to be independent from CI platform integration\\n- Benefits\\n- Easier troubleshooting and development of the process\\n- Easier migration to a different CI platform\\n- Easier experimentation\\n- Easier to run as part of the development process\\n- Drawbacks\\n- Needs more knowledge and experience\\n- We use Docker as much as possible to implement the steps in the integration process\\n- Benefits\\n- Platform independence\\n- Repeatability\\n- Security\\n- Drawbacks:\\n- Needs expertise in Dockerfile and Docker in general\\n- We only build the code affected by the change but re-tag all unchanged code artefacts\\n- Benefits\\n- Be able to release a consistent version of all necessary services. The Docker images for all services in the monorepo should have the same Docker image tag available for a commit hash\\n- Supports the monorepo benefits and ideology\\n- Drawbacks\\n- Can be tricky, especially for artefacts that are not Docker images(currently we do not plan to have those)\\n- We support only Linux as a target operating system when we cannot use Docker\\n- Benefits\\n- Same as the OS we use in our production environment, which minimizes the chances of failure because of OS differences\\n- We minimize the effort and complexity of supporting different operating systems\\n- Linux tooling works on all major other OSs today - macOS and Windows\\n- Drawbacks:\\n- Devs that use non-Linux OS might need to install additional software and customizations\\nGitHub Actions\\n- Number 1 CI platform on GitHub at the time of this writing\\n- Easy customization of which parts of the CI process to run depending on branching patterns and pull requests\\n- Good integration of code health with the pull request process\\n- As a GitHub open-source project, we have an unlimited number of \"compute\"-minutes that come as a part of the package\\n- Supports parallelisation of the process which can be pretty important in the context of monorepo\\n- Support using own runners which can be helpful to maximize speed, minimize costs and increase security.\\n"}
{"prompt":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThe original author of go-bindata delete their account and then the repo was\\nrecreated under a different owner. The dependency has shifted around enough that\\nwe have lost faith in the intention of the maintainers\\n[more details here](https:\/\/twitter.com\/francesc\/status\/961249107020001280?lang=en)\\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\\nduring a test run or final build) made it hard to reason about.\\n","completion":"## Decision\nUse [Packr2](https:\/\/github.com\/gobuffalo\/packr\/tree\/master\/v2) instead.\\n"}
{"prompt":"## Context\\nCurrently the RPC layer of Tendermint is using a variant of the JSON-RPC protocol. This ADR is meant to serve as a pro\/con list for possible alternatives and JSON-RPC.\\nThere are currently two options being discussed: gRPC & JSON-RPC.\\n### JSON-RPC\\nJSON-RPC is a JSON-based RPC protocol. Tendermint has implemented its own variant of JSON-RPC which is not compatible with the [JSON-RPC 2.0 specification](https:\/\/www.jsonrpc.org\/specification).\\n**Pros:**\\n- Easy to use & implement (by default)\\n- Well-known and well-understood by users and integrators\\n- Integrates reasonably well with web infrastructure (proxies, API gateways, service meshes, caches, etc)\\n- human readable encoding (by default)\\n**Cons:**\\n- No schema support\\n- RPC clients must be hand-written\\n- Streaming not built into protocol\\n- Underspecified types (e.g. numbers and timestamps)\\n- Tendermint has its own implementation (not standards compliant, maintenance overhead)\\n- High maintenance cost associated to this\\n- Stdlib `jsonrpc` package only supports JSON-RPC 1.0, no dominant package for JSON-RPC 2.0\\n- Tooling around documentation\/specification (e.g. Swagger) could be better\\n- JSON data is larger (offset by HTTP compression)\\n- Serializing is slow ([~100% marshal, ~400% unmarshal](https:\/\/github.com\/alecthomas\/go_serialization_benchmarks)); insignificant in absolute terms\\n- Specification was last updated in 2013 and is way behind Swagger\/OpenAPI\\n### gRPC + gRPC-gateway (REST + Swagger)\\ngRPC is a high performant RPC framework. It has been battle tested by a large number of users and is heavily relied on and maintained by countless large corporations.\\n**Pros:**\\n- Efficient data retrieval for users, lite clients and other protocols\\n- Easily implemented in supported languages (Go, Dart, JS, TS, rust, Elixir, Haskell, ...)\\n- Defined schema with richer type system (Protocol Buffers)\\n- Can use common schemas and types across all protocols and data stores (RPC, ABCI, blocks, etc)\\n- Established conventions for forwards- and backwards-compatibility\\n- Bi-directional streaming\\n- Servers and clients are be autogenerated in many languages (e.g. Tendermint-rs)\\n- Auto-generated swagger documentation for REST API\\n- Backwards and forwards compatibility guarantees enforced at the protocol level.\\n- Can be used with different codecs (JSON, CBOR, ...)\\n**Cons:**\\n- Complex system involving cross-language schemas, code generation, and custom protocols\\n- Type system does not always map cleanly to native language type system; integration woes\\n- Many common types require Protobuf plugins (e.g. timestamps and duration)\\n- Generated code may be non-idiomatic and hard to use\\n- Migration will be disruptive and laborious\\n","completion":"## Decision\n> This section explains all of the details of the proposed solution, including implementation details.\\n> It should also describe affects \/ corollary items that may need to be changed as a part of this.\\n> If the proposed change will be large, please also indicate a way to do the change to maximize ease of review.\\n> (e.g. the optimal split of things to do between separate PR's)\\n"}
{"prompt":"## Context\\nWe don't store any user data on back-end yet, so we can do it in AsyncStorage.\\n","completion":"## Decision\nWe will use [redux-persist](https:\/\/github.com\/rt2zz\/redux-persist)\\n"}
{"prompt":"## Context\\nWhen viewing different file types over the web, large video files or other non-standard files, may not display well in\\nthe browser, so the common solution is to create derivatives for a better user experience. These also include thumbnails\\nfor a sample picture of what the resource looks like.\\nManaging and creating derivatives can be hard and requires careful architectural considerations.\\nHowever, the modern web has adapted to larger files, and generally handles a wider array of file types than it did in\\nthe past. Also, thumbnails only work for image and video formats. Generic icon-based images for any file type can be\\nmore informative.\\n","completion":"## Decision\nScholarsphere will not create any derivative files. Icons will be used to represent file types, and the original\\nuploaded file can be rendered when applicable.\\n"}
{"prompt":"## Context\\nWe need to be able to monitor network services separately.\\n","completion":"## Decision\nWe will use _akka_ for managing the task to monitor a particular network service.\\n"}
{"prompt":"## Context\\nWe need to decide how the shared library should be loaded for usage.\\n- A library version can be a tag, a branch or a commit SHA.\\n- A library can be loaded explicitly or implicitly\\n- A library default version can be configured globally or not\\n### Loading implicitly\\n````groovy\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- a default version of the library needs to be defined globally\\n- there is no need to import the library, as it will be loaded automatically\\n- you can override the version by importing it manually: `@Library('my-shared-library@my-custom-version') _`\\n### Loading explicitly\\n````groovy\\n@Library('my-shared-library@1.0.0') _\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- an import statement is always needed\\n- a default version could be defined globally, then the version can be left out when importing\\n### Using a global predefined version\\n- can be overriden in the pipeline\\n- does not force users to select a version\\n### No globel predefined version\\n- import will not work without version\\n- consumers are aware of the version that they use\\n- can be dynamic (auto updating) when using release channels with branches\\n","completion":"## Decision\nWe will use explicit loading without a predefined version and release channels for releasing based on branches.\\n- reduces the magic of the used version\\n- can still be auto updating\\n- consumers know that a library is used and what the version is\\n"}
{"prompt":"## Context\\nI'd like to clarify my plans and intentions for this project. I'd like to record\\nthem as architectural records.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nWe want to build a serverless application with a function that\\nreturns a `\"Hello World!\"` message. We need to pick a programming\\nlanguage.\\n","completion":"## Decision\nWe will use [Python3.8]\\n"}
{"prompt":"## Context\\nOld or specific versions of a documentation can not be accessed.\\nOnly the actual version exists online. Every project's documentation\\nis in the same directory, no structure.\\n","completion":"## Decision\nCreate project specific directories under the `\/_posts` directory and\\ncreate version numbered directories e.g.: `\/_posts\/android\/1.0`,\\n`\/_posts\/ios\/2.1`. Put each documentation file (`README.md`, `CHANGELOG.md`, ...)\\ninside the version numbered directory.\\nCreate one YAML Front Matter index file for each product which points\\nto the latest version, see example below where latest version is `0.5`.\\nUse the `permalink: \/player-sdk\/android\/latest` key to specify where\\nthe latest documentation is accessible online, like:\\nhttp:\/\/developers.ustream.tv\/player-sdk\/android\/latest\\n```\\n\/_posts\/android\/2017-01-01-readme.md\\n---\\nlayout: markdown\\ntitle: Player SDK for Android (v0.5)\\nweight: 30\\ncategory: player-sdk\\ncategoryItemType: documentation\\ncategoryItemIsShown: 1\\ncategoryItemWidth: 6\\ncategoryItemDescription:\\ncategoryItemLabel: Read the documentation\\npermalink: \/player-sdk\/android\/latest\/\\n---\\n{% include_relative 0.5\/README.md  %}\\n```\\nChange the `include_relative` setting to include the latest version.\\nUse the `categoryItemIsShown: 1` setting to direct Jekyll to show\\nthis document when listing category contents.\\nCreate a YAML Front Matter index file in each version numbered directory\\nin which the version is specified, example:\\n```\\n\/_posts\/android\/0.5\/2017-02-02-readme.md\\n---\\nlayout: markdown\\ntitle: Player SDK for Android (v0.5.x)\\nweight: 3\\ncategory: player-sdk\\ncategoryItemType: documentation\\ncategoryItemIsShown: 0\\ncategoryItemWidth: 6\\ncategoryItemDescription:\\ncategoryItemLabel: Read the documentation\\npermalink: \/player-sdk\/android\/0.5\/\\n---\\n{% include_relative README.md  %}\\n```\\nUse the `categoryItemIsShown: 0` to hide this version from category listing,\\nas only the latest should be listed.\\nPrevious version can be accessed online using urls like:\\n* http:\/\/developers.ustream.tv\/player-sdk\/android\/0.4\/\\n* http:\/\/developers.ustream.tv\/player-sdk\/android\/0.5\/\\n* http:\/\/developers.ustream.tv\/player-sdk\/android\/latest\/ - which only points to `0.5`\\n"}
{"prompt":"## Context\\n### Current Operations\\nCurrently the geoprocessing service provides this set of operations:\\n* [`rasterGroupedCount`][github 1]\\n* [`rasterGroupedAverage`][github 2]\\n* [`rasterAverage`][github 3]\\n* [`rasterLinesJoin`][github 4]\\n* [`rasterSummary`][github 5]\\nEach operation takes a multipolygon and a list of rasters, queries those\\nrasters cropped to the multipolygon, and performs the respective calculation,\\nand returns the result grouped by the given rasters.\\nThese operations are then [configured][github 6] and [called][github 7] in\\nModel My Watershed (MMW) for different sets of rasters. The results are then\\naggregated in MMW using Python. In some cases, such as MapShed, **each project\\ninvokes 10 separate calls to the geoprocessing service**, the results of which\\nare then aggregated in Python before being sent back to the front-end.\\nThe performance of each of these operations for a shape around the size of a\\nHUC-12 (which is slightly smaller than a 512 tile) is about 1s. For a larger\\nshape like a HUC-8 which can be 15-50 times larger, the average time is about\\n6s. Since the front-end invokes a number of these jobs, and spends time polling\\nwith Celery, the average wait time for a HUC-8 MapShed job is around 30s. This\\nhas so far been acceptable.\\nWhen a geoprocessing service returns successfully, we cache the results. So if\\nanother user selects the same shape and the same rasters are queried for it,\\nthe cached results are used and the overall wait is much shorter.\\n### Subbasin Modeling\\nSubbasin Modeling divides a larger HUC-8 into its component HUC-12s to improve\\naccuracy of results. Under this new scheme, instead of running a single HUC-8,\\nwe run MapShed for all component HUC-12s and then aggregate those results. This\\nseverely affects the runtime, to the point of not being usable. For more\\ndiscussion on this methodology and affect on runtime, please see\\n[#2537][github 8].\\nMuch of this additional runtime comes from processing the same tiles for\\nmultiple HUC-12s. For example, consider [the following case][ocks]:\\n![image][githubusercontent]\\nThe red shape intersects five tiles, and the purple shape intersects only one\\ntile, one of red's five. When querying the red shape we fetch all five tiles,\\nand when querying the purple shape we fetch that tile again. This kind of\\nduplication happens all across a HUC-8, since it is querying a number of\\nadjacent HUC-12s.\\nFurthermore, it is our understanding that fetching tiles from S3 is the largest\\nportion of a single request's execution time. Thus, removing its duplication is\\nthe greatest speedup we could achieve in a single step.\\n","completion":"## Decision\nWe will add a new operation to the geoprocessing service that takes a list of\\nshapes and a list of operations. The first shape in the list is assumed to be\\nthe parent shape that contains all the other child shapes. Using the parent\\nshape we pull the tiles corresponding to every raster in the list of\\noperations, and collect them in a sequence.\\nThen, for each child shape, we go through the list of operations and calculate\\nthe results using the respective tile sequence from above. All the results are\\nthen collected in order and returned as a list.\\nOnce this ADR is approved, the next steps will be:\\n1. Define input and outputs for new geoprocessing operation\\n2. Implement new geoprocessing operation\\n3. Benchmark the new operation against current ones\\n4. Make a new release of the geoprocessing service and publish it\\n5. Update MMW to use the new geoprocessing service\\n6. Update MapShed to use the new operation\\n7. Update Subbasin code to use the new operation\\n"}
{"prompt":"## Context\\nThere are features that exist in the codebase we are not yet ready to release in production. We would like to use feature flags to toggle the availability of certain features, which will help prevent development and production branches from drifting.\\n","completion":"## Decision\nWe've decided to implement very simple feature flags that can be toggled with environment variables.\\n"}
{"prompt":"Context\\n-------\\nThe router will use some sort of configuration to decide what events are\\nto be sent to what consumers and how. These configurations can be\\ndefined for each partner\/enterprise so we could end up having 20-50\\nconfigurations.\\nWe have these two options for storing these configurations:\\n-  Store configurations in Django settings (YAML configuration files).\\n-  Store configurations in some Django model (database).\\nDecision\\n--------\\nKeeping in mind a large number of enterprise clients, we will store\\nrouter configurations in `Django configuration models\\n<https:\/\/github.com\/openedx\/django-config-models>`__.\\nConsequences\\n------------\\nStoring configurations in `Django configuration\\nmodels <https:\/\/github.com\/openedx\/django-config-models>`__ will keep the\\nsettings files cleaner.\\nDjango configuration models have built-in caching support which would\\nhelp us address performance-related challenges.\\nSince enterprise customer management might be done by teams other than\\noperators, putting these configurations in database will allow us to let\\ncustomer support teams manage the settings or could also make make it self\\nservice in future.\\nRejected Alternatives\\n---------------------\\n**Store configurations in Django settings**\\nHaving too many settings can clutter app settings and would be difficult\\nto manage.\\n","completion":"## Decision\n--------\\nKeeping in mind a large number of enterprise clients, we will store\\nrouter configurations in `Django configuration models\\n<https:\/\/github.com\/openedx\/django-config-models>`__.\\nConsequences\\n------------\\nStoring configurations in `Django configuration\\nmodels <https:\/\/github.com\/openedx\/django-config-models>`__ will keep the\\nsettings files cleaner.\\nDjango configuration models have built-in caching support which would\\nhelp us address performance-related challenges.\\nSince enterprise customer management might be done by teams other than\\noperators, putting these configurations in database will allow us to let\\ncustomer support teams manage the settings or could also make make it self\\nservice in future.\\nRejected Alternatives\\n---------------------\\n**Store configurations in Django settings**\\nHaving too many settings can clutter app settings and would be difficult\\nto manage.\\n"}
{"prompt":"## Context\\nWe need to add new functionality to the project - the ability to create custom NRPE checks.\\nThis means having templated configuration for adding service-specific plugins for the purposes of monitoring.\\nThe `base` module already contains NRPE specific code.\\nIt installs NRPE and ensures it is running. It also configures a set of default monitoring plugins.\\n","completion":"## Decision\nWe considered three possible approaches to adding the new NRPE functionality:\\n- Add it to the `base::nrpe`\\n- Create a separate `nrpe_custom_checks` module and keep `base` the way it is\\n- Create a separate `nrpe_custom_checks` module and refactor `base` to use it for check creation\\nRather than extending the base NRPE class, we chose to create a standalone NRPE custom checks module and keep `base` independent.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n"}
{"prompt":"## Context\\n* Our application works by indexing an XML file given by WikiMedia\\n* WikiMedia manually create a new XML file twice a month on average\\n* We have not been given an indication on how fresh the data our service provides needs to be\\n* We want the software to be deployable without relying on a shared file system\\n","completion":"## Decision\n* We store the XML dump in this git repo and update it when we need to\\n* The file will be accessed over http to allow us to move to a different storage solution in the future\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nWe need an efficient, automated build process that creates an Elixir\\nrelease within a compact Docker container.\\n","completion":"## Decision\nWe use a [multi-stage Dockerfile](https:\/\/docs.docker.com\/develop\/develop-images\/multistage-build\/) to install Elixir dependencies,\\nbuild JavaScript assets, and create the Elixir release in separate\\ncontainers, then copy all of the artifacts into a bare-bones Alpine\\nruntime image.\\n"}
{"prompt":"## Context\\nThe scratch app has to invoke the arduino-cli program to compile the converted programs.\\nThe scratch app has to invoke the scratch-to-c program to convert the scratch blocks to a runnable arduino program.\\nIn both cases the scratch app would have to invoke a native program.\\nThis is not possible in current web browsers.\\nThis would be possible using e.g. Electron, but the authors don't know Electron and Electron would add around 100 MB size overhead and also mean that without Electron the app can't be used.\\nUsing a local web-server all that is needed is any browser, the scratch ftduino app and the local web-server.\\n","completion":"## Decision\nWe will use a local web-server that will be used by the scratch app to invoke certain native programs.\\nThe local web-server may also be used to host the scratch app itself for example when there is no Internet connectivity available.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n"}
{"prompt":"## Context\\nWe now support distributed tracing across Dapr sidecars, and we inject correlation id to HTTP headers and gRPC metadata before we hand the requests to user code. However, it's up to the user code to configure and implement proper tracing themselves.\\n","completion":"## Decision\nWe should consider adding a tracing endpoint that user code can call in to log traces and telemetries.\\n"}
{"prompt":"## Context\\nBy choosing an API documentation standard we make it easier to auto generate developer documentation. There are two existing standards used to document REST APIs--RAML and OpenAPI Specification (OAS). Both seem capable of doing the job. The main difference seems to be that RAML is focused on defining data models while OpenAPI is focused on the nuts and bolts of the API. If we were supporting several APIs, RAML might be more useful for defining reusable types across systems. In this case OAS seems more suited to our task.\\n","completion":"## Decision\nUse OpenAPI specification to document the API.\\n"}
{"prompt":"## Context\\nWhile Docker containers now provide good root isolation from the host compared to earlier\\nversions of Docker there are still security risks. We intend to provide sudo access\\nfor users of the Notebook containers and this significantly magnifies the risks.\\n","completion":"## Decision\nWe have decided to use [AppArmor](https:\/\/wiki.ubuntu.com\/AppArmor) to improve our\\ncontainer security with the intention to make all policies as restrictive as possible.\\nTo make it easier to build AppArmor profiles we have also decided to use\\n[Bane](https:\/\/github.com\/jessfraz\/bane).\\nTo further secure containers we have also decided to run all of our custom build\\ncontainers as a non root user.\\n"}
{"prompt":"## Context\\nSince Security Hub is a native AWS service for central security alert management, checks and mitigation, superwerker enables it for all AWS accounts.\\n","completion":"## Decision\n- Enable SH for existing Control Tower core accounts (master, Audit, Log Archive) and all future member accounts\\n- Use Control Tower `Setup\/UpdateLandingZone` Lifecycle events to start the invite setup for SH\\n- The delegated administrator feature is currently not supported by Lambda and\/or SSM Automation runtimes - since upgrading the current mechanism to this feature as soon as it's available is officially supported we're postponing this (#70); this subsequently requires us to implement integrity protection\\n- SH out-of-the-box complains about a lot of security check issues - this has been scoped out from 1.0 (#99)\\n"}
{"prompt":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has defaults that differ between schematic types. In order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files and directories.\\n## Decision Drivers\\n- Provide consistency when navigating the codebase\\n- The earlier we decide on this, the better\\n","completion":"## Decision\n- Provide consistency when navigating the codebase\\n- The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the file name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid using kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines: Only use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`: `import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n```text\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n```\\nor:\\n```text\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n```\\nrather than\\n```text\\ncomponents\/cta-button\/CtaButton.tsx\\n```\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n- Easier to navigate the codebase\\n- File names are more readable, and developers know what to expect\\n- This approach is the most common practice, and something most JS and TS developers are familiar with.\\n"}
{"prompt":"## Context\\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\\n","completion":"## Decision\nWe decided to store off-chain data in a way which doesn't reference the ID of any of the entities, but purely stores the precise proof data under the hash of the off-chain data.\\n"}
{"prompt":"## Context\\nWe use ADRs to keep track of decisions as described in\\n[ADR-1](https:\/\/github.com\/dtr-org\/unit-e-docs\/blob\/master\/adrs\/ADR-0001.md).\\nThis is supposed to create a history of immutable records of decisions for\\nfuture reference.\\nWe also need a place for discussing and agreeing on design questions, especially\\non the protocol level. This also needs to record the context of which decisions\\nwere taken, when, and why.\\nThere needs to be a process for:\\n* discussing decisions\\n* taking decisions\\n* recording decisions\\n* updating decisions\\nIt should be easy to reference decisions. If there are multiple versions of a\\ndecision it needs to be clear which version is referenced.\\nDesign decision documents should also be able to serve as specification.\\nWe have used ADRs for both, recording decisions, including code and process\\ndecisions and for having the protocol design discussions and decisions. This has\\ncreated some problems as it wasn't clear if documents should be updated or\\nreplaced, the ADR template is not covering specific aspects of design decisions,\\nand it is mixing decisions of different domains which possibly also have a\\ndifferent audience.\\nADRs are also a good way to decide on a certain way of doing things such as \"we\\nbelieve in open-source and gonna strive to open our repos as much as possible\"\\nand demonstrate the team signing off on this. The protocol design process with\\nmore complex status transitions and the need for review by experts on the\\nprotocol level is not so suitable for this.\\n### Current ADRs\\n| ADR | Title | Status | Created | Accepted |\\n|---|---|:---:|:---:|:---:|\\n|[ADR-0001](https:\/\/github.com\/dtr-org\/unit-e-docs\/blob\/master\/adrs\/ADR-0001.md)|Adopt ADR|Accepted|2018-08-16|2018-08-17|\\n|[ADR-0002](https:\/\/github.com\/dtr-org\/unit-e-docs\/blob\/master\/adrs\/ADR-0002.md)|Define naming convention for actors in Esperanza|Accepted|2018-08-23|2018-08-24|\\n|[ADR-0003](https:\/\/github.com\/dtr-org\/unit-e-docs\/blob\/master\/adrs\/ADR-0003.md)|Unit-e Block Header to include SegWit Merkle Root|Accepted|2018-08-23|2018-09-04|\\n|[ADR-0004](https:\/\/github.com\/dtr-org\/unit-e-docs\/blob\/master\/adrs\/ADR-0004.md)|Unit-e Block Header NOT to include Kernel Hash|Accepted|2018-08-23|2018-09-04|\\n|[ADR-0006](https:\/\/github.com\/dtr-org\/unit-e-docs\/blob\/master\/adrs\/ADR-0006.md)|Adopt Google C++ Style Guide|Accepted|2018-08-23|2018-08-27|\\n|[ADR-0007](https:\/\/github.com\/dtr-org\/unit-e-docs\/blob\/master\/adrs\/ADR-0007.md)|Use genesis block for initial supply|Accepted|2018-08-30|2018-09-04|\\n|[ADR-0008](https:\/\/github.com\/dtr-org\/unit-e-docs\/blob\/master\/adrs\/ADR-0008.md)|Proper `headers` message|Accepted|2018-09-25|2018-09-25|\\n|[ADR-0009](https:\/\/github.com\/dtr-org\/unit-e-docs\/blob\/master\/adrs\/ADR-0009.md)|Validator permissioning specification|Accepted|2018-10-02|2018-10-02|\\n|[ADR-0011](https:\/\/github.com\/dtr-org\/unit-e-docs\/blob\/master\/adrs\/ADR-0011.md)|Snapshot|Accepted|2018-10-09|2018-11-06|\\n|[ADR-0012](https:\/\/github.com\/dtr-org\/unit-e-docs\/blob\/master\/adrs\/ADR-0012.md)|Fork choice rule|Accepted|2018-10-17|2018-10-25|\\n|[ADR-0016](https:\/\/github.com\/dtr-org\/unit-e-docs\/blob\/master\/adrs\/ADR-0016.md)|Adopt DCO|Accepted|2018-10-22|2018-11-06|\\n|[ADR-0017](https:\/\/github.com\/dtr-org\/unit-e-docs\/blob\/master\/adrs\/ADR-0017.md)|No Nonce or Extra Nonce|Accepted|2018-11-05|2018-11-03|\\n|[ADR-0018](https:\/\/github.com\/dtr-org\/unit-e-docs\/blob\/master\/adrs\/ADR-0018.md)|unit-e Transaction Types|Accepted|2018-11-02|\\n|ADR-10|Snapshot Verification|Proposed|\\n|ADR-15|Remote Staking|Proposed|\\n|ADR-21|Transfer Esperanza Transactions|Proposed|\\n|ADR-22|Coinstake Maturity|Proposed|\\n|ADR-19|Coin serialization per TX type|Retracted, converted to issue, not implemented|\\n|ADR-20|Propagate stake modifier using fast sync|Retracted, converted to issue, implemented|\\nThe ADRs cover different domains:\\n* Protocol: 3, 8, 9, 11, 12, 17, 18, 10, 15, 21, 22, 20\\n* Process: 1, 16\\n* Code: 2, 4, 6, 7, 19\\n","completion":"## Decision\n* We split up the process for discussing design from recording decisions. For\\nthe first we create a new UIP (Unit-e Improvement Proposal) process, for the\\nsecond we continue to use the ADR process. Protocol related design decisions\\nfall under discussing design (in UIPs), process and code falls under recording\\ndecisions (as ADRs).\\n* ADRs are used for recording decisions. They result in a (generally) immutable\\nrecord of decisions which are relevant on the development process or\\nimplementation and code level. This is the record of a decision at a given\\npoint in time. It can be revised by taking another decision which is recorded\\nin a new ADR. ADR 1, 2, 4, 6, 7, 16, 19 represent this kind of decision\\nrecords.\\n* UIPs are used for discussing and deciding design, especially related to\\nprotocol and API. They result in a design document which also serves as\\nspecification for the current protocol. These documents are mutable and\\nreflect the current state of the design. ADR 3, 8, 9, 10, 11, 12, 17,\\n18, 19, 20, 21, 22 represent this kind of design document.\\n* The UIP process is moved to a dedicated repository `dtr-org\/uips` (which will\\nbe made public along with the other repositories).\\n* The protocol related ADRs which are in fact more design documents than\\ndecision records, as listed before, are moved as UIP to the\\n`dtr-org\/uips` repo. Their numbers will be kept.\\n* The UIP process will be documented in UIP-1. It will be based on the [BIP\\nprocess](https:\/\/github.com\/bitcoin\/bips\/blob\/master\/bip-0002.mediawiki). The\\nexact details of how this process will work are out of scope of this ADR. This\\nwill be sorted out in UIP-1.\\n* For both processes:\\n* The pull request review process is used for discussions of proposals, there\\nmight be a pre-discussion through other channels.\\n* A summary of the discussion in the pull request should be added to the\\ndocument before approving it.\\n* Approving and merging the pull request means acceptance of the document as\\ndecision. The UIP process will define a more fine-grained set of decision\\nlevels to reflect states such as `Draft`, `Proposed`, `Final`. Exact\\ndefinition of these states is out of scope of this ADR and will be defined\\nas part of UIP-1.\\n* When an accepted document is updated through another pull request which\\ncontains non-trivial changes, a changelog section is added.\\n* Rejected proposals are not merged but kept in the pull request history, they\\nare added to the table of content for reference.\\n* ADR-1 is updated to reflect the decisions of this ADR-23.\\n"}
{"prompt":"## Context and Problem Statement\\nCustom classes for the TUF signed metadata wrapper (Metadata) and metadata\\npayload containers (Root, Timestamp, Snapshot, Targets) were added recently.\\nComplex attributes on these classes are still represented as dictionaries.\\nShould we add classes for these attributes too?\\n## Decision Drivers\\n* Transition to class-based role metadata containers in progress (see *\"class\\nmodel\"* links below)\\n* Harden in-memory representation of metadata model\\n* Replace `securesystemslib` schema validation (see *\"schema checker\"* link\\nbelow)\\n","completion":"## Decision\n* Transition to class-based role metadata containers in progress (see *\"class\\nmodel\"* links below)\\n* Harden in-memory representation of metadata model\\n* Replace `securesystemslib` schema validation (see *\"schema checker\"* link\\nbelow)\\nChosen option: \"Use custom classes for complex attributes\", to provide a\\nconsistently object-oriented, well-defined, single source of truth about the\\nTUF metadata model (not only its containers). In addition to convenience update\\nmethods, the model may be extended with self-validation behavior (see\\n*\"validation guidelines\"* link below) to replace `securesystemslib` schema\\nchecks.\\n### Negative Consequences\\n* Implementation overhead\\n* Less flexibility in usage and development (this is actually desired)\\n* Maybe less idiomatic than dictionaries\\n"}
{"prompt":"## Context\\nCurrently, the Cosmos SDK utilizes [go-amino](https:\/\/github.com\/tendermint\/go-amino\/) for binary\\nand JSON object encoding over the wire bringing parity between logical objects and persistence objects.\\nFrom the Amino docs:\\n> Amino is an object encoding specification. It is a subset of Proto3 with an extension for interface\\n> support. See the [Proto3 spec](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3) for more\\n> information on Proto3, which Amino is largely compatible with (but not with Proto2).\\n>\\n> The goal of the Amino encoding protocol is to bring parity into logic objects and persistence objects.\\nAmino also aims to have the following goals (not a complete list):\\n- Binary bytes must be decode-able with a schema.\\n- Schema must be upgradeable.\\n- The encoder and decoder logic must be reasonably simple.\\nHowever, we believe that Amino does not fulfill these goals completely and does not fully meet the\\nneeds of a truly flexible cross-language and multi-client compatible encoding protocol in the Cosmos SDK.\\nNamely, Amino has proven to be a big pain-point in regards to supporting object serialization across\\nclients written in various languages while providing virtually little in the way of true backwards\\ncompatibility and upgradeability. Furthermore, through profiling and various benchmarks, Amino has\\nbeen shown to be an extremely large performance bottleneck in the Cosmos SDK <sup>1<\/sup>. This is\\nlargely reflected in the performance of simulations and application transaction throughput.\\nThus, we need to adopt an encoding protocol that meets the following criteria for state serialization:\\n- Language agnostic\\n- Platform agnostic\\n- Rich client support and thriving ecosystem\\n- High performance\\n- Minimal encoded message size\\n- Codegen-based over reflection-based\\n- Supports backward and forward compatibility\\nNote, migrating away from Amino should be viewed as a two-pronged approach, state and client encoding.\\nThis ADR focuses on state serialization in the Cosmos SDK state machine. A corresponding ADR will be\\nmade to address client-side encoding.\\n","completion":"## Decision\nWe will adopt [Protocol Buffers](https:\/\/developers.google.com\/protocol-buffers) for serializing\\npersisted structured data in the Cosmos SDK while providing a clean mechanism and developer UX for\\napplications wishing to continue to use Amino. We will provide this mechanism by updating modules to\\naccept a codec interface, `Marshaler`, instead of a concrete Amino codec. Furthermore, the Cosmos SDK\\nwill provide three concrete implementations of the `Marshaler` interface: `AminoCodec`, `ProtoCodec`,\\nand `HybridCodec`.\\n- `AminoCodec`: Uses Amino for both binary and JSON encoding.\\n- `ProtoCodec`: Uses Protobuf for or both binary and JSON encoding.\\n- `HybridCodec`: Uses Amino for JSON encoding and Protobuf for binary encoding.\\nUntil the client migration landscape is fully understood and designed, modules will use a `HybridCodec`\\nas the concrete codec it accepts and\/or extends. This means that all client JSON encoding, including\\ngenesis state, will still use Amino. The ultimate goal will be to replace Amino JSON encoding with\\nProtbuf encoding and thus have modules accept and\/or extend `ProtoCodec`.\\n### Module Codecs\\nModules that do not require the ability to work with and serialize interfaces, the path to Protobuf\\nmigration is pretty straightforward. These modules are to simply migrate any existing types that\\nare encoded and persisted via their concrete Amino codec to Protobuf and have their keeper accept a\\n`Marshaler` that will be a `HybridCodec`. This migration is simple as things will just work as-is.\\nNote, any business logic that needs to encode primitive types like `bool` or `int64` should use\\n[gogoprotobuf](https:\/\/github.com\/gogo\/protobuf) Value types.\\nExample:\\n```go\\nts, err := gogotypes.TimestampProto(completionTime)\\nif err != nil {\\n\/\/ ...\\n}\\nbz := cdc.MustMarshalBinaryBare(ts)\\n```\\nHowever, modules can vary greatly in purpose and design and so we must support the ability for modules\\nto be able to encode and work with interfaces (e.g. `Account` or `Content`). For these modules, they\\nmust define their own codec interface that extends `Marshaler`. These specific interfaces are unique\\nto the module and will contain method contracts that know how to serialize the needed interfaces.\\nExample:\\n```go\\n\/\/ x\/auth\/types\/codec.go\\ntype Codec interface {\\ncodec.Marshaler\\nMarshalAccount(acc exported.Account) ([]byte, error)\\nUnmarshalAccount(bz []byte) (exported.Account, error)\\nMarshalAccountJSON(acc exported.Account) ([]byte, error)\\nUnmarshalAccountJSON(bz []byte) (exported.Account, error)\\n}\\n```\\n### Usage of `Any` to encode interfaces\\nIn general, module-level .proto files should define messages which encode interfaces\\nusing [`google.protobuf.Any`](https:\/\/github.com\/protocolbuffers\/protobuf\/blob\/master\/src\/google\/protobuf\/any.proto).\\nAfter [extension discussion](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030),\\nthis was chosen as the preferred alternative to application-level `oneof`s\\nas in our original protobuf design. The arguments in favor of `Any` can be\\nsummarized as follows:\\n* `Any` provides a simpler, more consistent client UX for dealing with\\ninterfaces than app-level `oneof`s that will need to be coordinated more\\ncarefully across applications. Creating a generic transaction\\nsigning library using `oneof`s may be cumbersome and critical logic may need\\nto be reimplemented for each chain\\n* `Any` provides more resistance against human error than `oneof`\\n* `Any` is generally simpler to implement for both modules and apps\\nThe main counter-argument to using `Any` centers around its additional space\\nand possibly performance overhead. The space overhead could be dealt with using\\ncompression at the persistence layer in the future and the performance impact\\nis likely to be small. Thus, not using `Any` is seem as a pre-mature optimization,\\nwith user experience as the higher order concern.\\nNote, that given the SDK's decision to adopt the `Codec` interfaces described\\nabove, apps can still choose to use `oneof` to encode state and transactions\\nbut it is not the recommended approach. If apps do choose to use `oneof`s\\ninstead of `Any` they will likely lose compatibility with client apps that\\nsupport multiple chains. Thus developers should think carefully about whether\\nthey care more about what is possibly a pre-mature optimization or end-user\\nand client developer UX.\\n### Safe usage of `Any`\\nBy default, the [gogo protobuf implementation of `Any`](https:\/\/godoc.org\/github.com\/gogo\/protobuf\/types)\\nuses [global type registration]( https:\/\/github.com\/gogo\/protobuf\/blob\/master\/proto\/properties.go#L540)\\nto decode values packed in `Any` into concrete\\ngo types. This introduces a vulnerability where any malicious module\\nin the dependency tree could registry a type with the global protobuf registry\\nand cause it to be loaded and unmarshaled by a transaction that referenced\\nit in the `type_url` field.\\nTo prevent this, we introduce a type registration mechanism for decoding `Any`\\nvalues into concrete types through the `InterfaceRegistry` interface which\\nbears some similarity to type registration with Amino:\\n```go\\ntype InterfaceRegistry interface {\\n\/\/ RegisterInterface associates protoName as the public name for the\\n\/\/ interface passed in as iface\\n\/\/ Ex:\\n\/\/   registry.RegisterInterface(\"cosmos_sdk.Msg\", (*sdk.Msg)(nil))\\nRegisterInterface(protoName string, iface interface{})\\n\/\/ RegisterImplementations registers impls as a concrete implementations of\\n\/\/ the interface iface\\n\/\/ Ex:\\n\/\/  registry.RegisterImplementations((*sdk.Msg)(nil), &MsgSend{}, &MsgMultiSend{})\\nRegisterImplementations(iface interface{}, impls ...proto.Message)\\n}\\n```\\nIn addition to serving as a whitelist, `InterfaceRegistry` can also serve\\nto communicate the list of concrete types that satisfy an interface to clients.\\nIn .proto files:\\n* fields which accept interfaces should be annotated with `cosmos_proto.accepts_interface`\\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\\n* interface implementations should be annotated with `cosmos_proto.implements_interface`\\nusing the same full-qualified name passed as `protoName` to `InterfaceRegistry.RegisterInterface`\\nIn the future, `protoName`, `cosmos_proto.accepts_interface`, `cosmos_proto.implements_interface`\\nmay be used via code generation, reflection &\/or static linting.\\nThe same struct that implements `InterfaceRegistry` will also implement an\\ninterface `InterfaceUnpacker` to be used for unpacking `Any`s:\\n```go\\ntype InterfaceUnpacker interface {\\n\/\/ UnpackAny unpacks the value in any to the interface pointer passed in as\\n\/\/ iface. Note that the type in any must have been registered with\\n\/\/ RegisterImplementations as a concrete type for that interface\\n\/\/ Ex:\\n\/\/    var msg sdk.Msg\\n\/\/    err := ctx.UnpackAny(any, &msg)\\n\/\/    ...\\nUnpackAny(any *Any, iface interface{}) error\\n}\\n```\\nNote that `InterfaceRegistry` usage does not deviate from standard protobuf\\nusage of `Any`, it just introduces a security and introspection layer for\\ngolang usage.\\n`InterfaceRegistry` will be a member of `ProtoCodec` and `HybridCodec` as\\ndescribed above. In order for modules to register interface types, app modules\\ncan optionally implement the following interface:\\n```go\\ntype InterfaceModule interface {\\nRegisterInterfaceTypes(InterfaceRegistry)\\n}\\n```\\nThe module manager will include a method to call `RegisterInterfaceTypes` on\\nevery module that implements it in order to populate the `InterfaceRegistry`.\\n### Using `Any` to encode state\\nThe SDK will provide support methods `MarshalAny` and `UnmarshalAny` to allow\\neasy encoding of state to `Any` in `Codec` implementations. Ex:\\n```go\\nimport \"github.com\/cosmos\/cosmos-sdk\/codec\"\\nfunc (c *Codec) MarshalEvidence(evidenceI eviexported.Evidence) ([]byte, error) {\\nreturn codec.MarshalAny(evidenceI)\\n}\\nfunc (c *Codec) UnmarshalEvidence(bz []byte) (eviexported.Evidence, error) {\\nvar evi eviexported.Evidence\\nerr := codec.UnmarshalAny(c.interfaceContext, &evi, bz)\\nif err != nil {\\nreturn nil, err\\n}\\nreturn evi, nil\\n}\\n```\\n### Using `Any` in `sdk.Msg`s\\nA similar concept is to be applied for messages that contain interfaces fields.\\nFor example, we can define `MsgSubmitEvidence` as follows where `Evidence` is\\nan interface:\\n```protobuf\\n\/\/ x\/evidence\/types\/types.proto\\nmessage MsgSubmitEvidence {\\nbytes submitter = 1\\n[\\n(gogoproto.casttype) = \"github.com\/cosmos\/cosmos-sdk\/types.AccAddress\"\\n];\\ngoogle.protobuf.Any evidence = 2;\\n}\\n```\\nNote that in order to unpack the evidence from `Any` we do need a reference to\\n`InterfaceRegistry`. In order to reference evidence in methods like\\n`ValidateBasic` which shouldn't have to know about the `InterfaceRegistry`, we\\nintroduce an `UnpackInterfaces` phase to deserialization which unpacks\\ninterfaces before they're needed.\\n### Unpacking Interfaces\\nTo implement the `UnpackInterfaces` phase of deserialization which unpacks\\ninterfaces wrapped in `Any` before they're needed, we create an interface\\nthat `sdk.Msg`s and other types can implement:\\n```go\\ntype UnpackInterfacesMessage interface {\\nUnpackInterfaces(InterfaceUnpacker) error\\n}\\n```\\nWe also introduce a private `cachedValue interface{}` field onto the `Any`\\nstruct itself with a public getter `GetCachedValue() interface{}`.\\nThe `UnpackInterfaces` method is to be invoked during message deserialization right\\nafter `Unmarshal` and any interface values packed in `Any`s will be decoded\\nand stored in `cachedValue` for reference later.\\nThen unpacked interface values can safely be used in any code afterwards\\nwithout knowledge of the `InterfaceRegistry`\\nand messages can introduce a simple getter to cast the cached value to the\\ncorrect interface type.\\nThis has the added benefit that unmarshaling of `Any` values only happens once\\nduring initial deserialization rather than every time the value is read. Also,\\nwhen `Any` values are first packed (for instance in a call to\\n`NewMsgSubmitEvidence`), the original interface value is cached so that\\nunmarshaling isn't needed to read it again.\\n`MsgSubmitEvidence` could implement `UnpackInterfaces`, plus a convenience getter\\n`GetEvidence` as follows:\\n```go\\nfunc (msg MsgSubmitEvidence) UnpackInterfaces(ctx sdk.InterfaceRegistry) error {\\nvar evi eviexported.Evidence\\nreturn ctx.UnpackAny(msg.Evidence, *evi)\\n}\\nfunc (msg MsgSubmitEvidence) GetEvidence() eviexported.Evidence {\\nreturn msg.Evidence.GetCachedValue().(eviexported.Evidence)\\n}\\n```\\n### Amino Compatibility\\nOur custom implementation of `Any` can be used transparently with Amino if used\\nwith the proper codec instance. What this means is that interfaces packed within\\n`Any`s will be amino marshaled like regular Amino interfaces (assuming they\\nhave been registered properly with Amino).\\nIn order for this functionality to work:\\n- **all legacy code must use `*codec.LegacyAmino` instead of `*amino.Codec` which is\\nnow a wrapper which properly handles `Any`**\\n- **all new code should use `Marshaler` which is compatible with both amino and\\nprotobuf**\\n- Also, before v0.39, `codec.LegacyAmino` will be renamed to `codec.LegacyAmino`.\\n### Why Wasn't X Chosen Instead\\nFor a more complete comparison to alternative protocols, see [here](https:\/\/codeburst.io\/json-vs-protocol-buffers-vs-flatbuffers-a4247f8bda6f).\\n### Cap'n Proto\\nWhile [Cap\u2019n Proto](https:\/\/capnproto.org\/) does seem like an advantageous alternative to Protobuf\\ndue to it's native support for interfaces\/generics and built in canonicalization, it does lack the\\nrich client ecosystem compared to Protobuf and is a bit less mature.\\n### FlatBuffers\\n[FlatBuffers](https:\/\/google.github.io\/flatbuffers\/) is also a potentially viable alternative, with the\\nprimary difference being that FlatBuffers does not need a parsing\/unpacking step to a secondary\\nrepresentation before you can access data, often coupled with per-object memory allocation.\\nHowever, it would require great efforts into research and full understanding the scope of the migration\\nand path forward -- which isn't immediately clear. In addition, FlatBuffers aren't designed for\\nuntrusted inputs.\\n"}
{"prompt":"## Context\\nMultiple projects need to publish or subscribe to messages from the `bus`.\\nProjects are written in a fixed set of supported programming languages.\\nThere is a set of non-functional requirements to cater for in integrations, especially when listening to queues (retries, graceful shutdown, timeouts, etc).\\n","completion":"## Decision\nAccess the bus exclusively through the eLife SDKs, either for publishing or subscribing to messages:\\n- [bus-sdk-php](https:\/\/github.com\/elifesciences\/bus-sdk-php)\\n- [bus-sdk-python](https:\/\/github.com\/elifesciences\/bus-sdk-python)\\n"}
{"prompt":"## Context\\nWe need to provide a way to authenticate users who will interact with our Kubernetes clusters.\\nWe do not have a organisation-wide identity provider. Virtually everyone will have a Google account. Many people will have a GitHub account.\\nPeople working on GitHub repositories are likely the same people who are deploying to a cluster. Access to repositories likely indicates which users should have access to a cluster. We can reuse this user:team mapping in order to control access to clusters.\\n","completion":"## Decision\nWe will use GitHub as our identify provider.\\n"}
{"prompt":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n","completion":"## Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nAmong most frameworks, [ThemeUI](https:\/\/theme-ui.com\/) stands out for the following reasons:\\n* It is incredibly light (only 19kb unpacked)\\n* It is highly customizable (we can use the `sx` props to use theme tokens without hooks)\\n* It is slightly opinionated (more than [styled-system](https:\/\/styled-system.com\/) but less than [rebass](https:\/\/rebassjs.org\/))\\n* It contains some common structures\\nWe've not decided to use Rebass, since most of the community is already changing to ThemeUI, and also it allows us to do a more robust structure in our themes.\\n"}
{"prompt":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\\n[ADR 020](.\/adr-020-protobuf-transaction-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nThis ADR continues from [ADD 020](.\/adr-020-protobuf-transaction-encoding.md)\\nto specify the encoding of queries.\\n","completion":"## Decision\n### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```protobuf\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/cosmos\/gogoproto)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the Cosmos SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryService(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryService(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work directly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe Cosmos SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context`will receive a new method`QueryConn`that returns a`ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n"}
{"prompt":"## Context\\nCurrently the `Commit` structure contains a lot of potentially redundant or unnecessary data.\\nIt contains a list of precommits from every validator, where the precommit\\nincludes the whole `Vote` structure. Thus each of the commit height, round,\\ntype, and blockID are repeated for every validator, and could be deduplicated,\\nleading to very significant savings in block size.\\n```\\ntype Commit struct {\\nBlockID    BlockID `json:\"block_id\"`\\nPrecommits []*Vote `json:\"precommits\"`\\n}\\ntype Vote struct {\\nValidatorAddress Address   `json:\"validator_address\"`\\nValidatorIndex   int       `json:\"validator_index\"`\\nHeight           int64     `json:\"height\"`\\nRound            int       `json:\"round\"`\\nTimestamp        time.Time `json:\"timestamp\"`\\nType             byte      `json:\"type\"`\\nBlockID          BlockID   `json:\"block_id\"`\\nSignature        []byte    `json:\"signature\"`\\n}\\n```\\nThe original tracking issue for this is [#1648](https:\/\/github.com\/tendermint\/tendermint\/issues\/1648).\\nWe have discussed replacing the `Vote` type in `Commit` with a new `CommitSig`\\ntype, which includes at minimum the vote signature. The `Vote` type will\\ncontinue to be used in the consensus reactor and elsewhere.\\nA primary question is what should be included in the `CommitSig` beyond the\\nsignature. One current constraint is that we must include a timestamp, since\\nthis is how we calculuate BFT time, though we may be able to change this [in the\\nfuture](https:\/\/github.com\/tendermint\/tendermint\/issues\/2840).\\nOther concerns here include:\\n- Validator Address [#3596](https:\/\/github.com\/tendermint\/tendermint\/issues\/3596) -\\nShould the CommitSig include the validator address? It is very convenient to\\ndo so, but likely not necessary. This was also discussed in [#2226](https:\/\/github.com\/tendermint\/tendermint\/issues\/2226).\\n- Absent Votes [#3591](https:\/\/github.com\/tendermint\/tendermint\/issues\/3591) -\\nHow to represent absent votes? Currently they are just present as `nil` in the\\nPrecommits list, which is actually problematic for serialization\\n- Other BlockIDs [#3485](https:\/\/github.com\/tendermint\/tendermint\/issues\/3485) -\\nHow to represent votes for nil and for other block IDs? We currently allow\\nvotes for nil and votes for alternative block ids, but just ignore them\\n","completion":"## Decision\nDeduplicate the fields and introduce `CommitSig`:\\n```\\ntype Commit struct {\\nHeight  int64\\nRound   int\\nBlockID    BlockID      `json:\"block_id\"`\\nPrecommits []CommitSig `json:\"precommits\"`\\n}\\ntype CommitSig struct {\\nBlockID  BlockIDFlag\\nValidatorAddress Address\\nTimestamp time.Time\\nSignature []byte\\n}\\n\/\/ indicate which BlockID the signature is for\\ntype BlockIDFlag int\\nconst (\\nBlockIDFlagAbsent BlockIDFlag = iota \/\/ vote is not included in the Commit.Precommits\\nBlockIDFlagCommit                    \/\/ voted for the Commit.BlockID\\nBlockIDFlagNil                       \/\/ voted for nil\\n)\\n```\\nRe the concerns outlined in the context:\\n**Timestamp**: Leave the timestamp for now. Removing it and switching to\\nproposer based time will take more analysis and work, and will be left for a\\nfuture breaking change. In the meantime, the concerns with the current approach to\\nBFT time [can be\\nmitigated](https:\/\/github.com\/tendermint\/tendermint\/issues\/2840#issuecomment-529122431).\\n**ValidatorAddress**: we include it in the `CommitSig` for now. While this\\ndoes increase the block size unecessarily (20-bytes per validator), it has some ergonomic and debugging advantages:\\n- `Commit` contains everything necessary to reconstruct `[]Vote`, and doesn't depend on additional access to a `ValidatorSet`\\n- Lite clients can check if they know the validators in a commit without\\nre-downloading the validator set\\n- Easy to see directly in a commit which validators signed what without having\\nto fetch the validator set\\nIf and when we change the `CommitSig` again, for instance to remove the timestamp,\\nwe can reconsider whether the ValidatorAddress should be removed.\\n**Absent Votes**: we include absent votes explicitly with no Signature or\\nTimestamp but with the ValidatorAddress. This should resolve the serialization\\nissues and make it easy to see which validator's votes failed to be included.\\n**Other BlockIDs**: We use a single byte to indicate which blockID a `CommitSig`\\nis for. The only options are:\\n- `Absent` - no vote received from the this validator, so no signature\\n- `Nil` - validator voted Nil - meaning they did not see a polka in time\\n- `Commit` - validator voted for this block\\nNote this means we don't allow votes for any other blockIDs. If a signature is\\nincluded in a commit, it is either for nil or the correct blockID. According to\\nthe Tendermint protocol and assumptions, there is no way for a correct validator to\\nprecommit for a conflicting blockID in the same round an actual commit was\\ncreated. This was the consensus from\\n[#3485](https:\/\/github.com\/tendermint\/tendermint\/issues\/3485)\\nWe may want to consider supporting other blockIDs later, as a way to capture\\nevidence that might be helpful. We should clarify if\/when\/how doing so would\\nactually help first. To implement it, we could change the `Commit.BlockID`\\nfield to a slice, where the first entry is the correct block ID and the other\\nentries are other BlockIDs that validators precommited before. The BlockIDFlag\\nenum can be extended to represent these additional block IDs on a per block\\nbasis.\\n"}
{"prompt":"# 15. Phoenix Context Organization\\nDate: 2019-09-11\\n## Context\\nOur Phoenix Contexts are becoming bloated. We need some organizational pattern to keep things modular.\\n","completion":"## Decision\nWe've decided to use the rules described here: [A Proposal for Some New Rules for Phoenix Contexts](http:\/\/devonestes.herokuapp.com\/a-proposal-for-context-rules)\\n1. Resources have Schema files, and those contain only schema definitions, type definitions, validations and changeset functions\\n2. Every Schema has its own Secondary Context\\n3. The only place you use your Repos is in a Secondary Context, and only for the associated resource\\n4. Primary Contexts define higher level ideas in your application, and most interactions between resources will take place there\\n"}
{"prompt":"## Context\\nThe same agreement definitions for CMP services will be needed for suppliers to define their offers, and buyer journeys to select offers from. Also there will be many common elements shared between agreements, such as the tasks for buyers (and suppliers) to complete in order to complete a buying journey.\\nFor each version of a CMp agreement, both buyer and supplier services, will have to release their code in sync to make sure that supplier responses to their service offer questions line up with buyer selection activities.\\nFor example, a framework will have a number of lots, each with a number of services. The services may have various parameters - like cost models, date ranges, locations. Suppliers to that framework will need to answer questions that map to these services. These things should be defined in a consistent way so build pipelines can be built consistently.\\nBuild pipelines may find it more convenient to pull Agreement definition files from a backing repository rather than an API.\\n","completion":"## Decision\nThe structure of all agreements will be defined in a common CCS GitHub repository. For example, the lot and item structure of frameworks should be defined in the repository so that web forms can be built in a repeatable way without having to code the agreement.\\nThe structure of the data is yet to be defined but will map to OCDS standards wherever practical. It will include _elements like_:\\n- Agreement schema\\n- Lot structure\\n- Service items\\n- descriptions for buyers\\n- value parameters (price, description, location, timelines etc)\\n- question and answer formats for suppliers\\n- Qualification criteria\\n- links to supporting Documents (in S3)\\n- supporting content\\n- relevant tasks and Q&A text\\n- Interests  (links supplier offer to agreement)\\n-Supplier Offers\\n-(service, offer details)\\nThe \u2018agreement schema\u2019 should be built in a technology agnostic way, using a data schema. Example domain models for different languages can be generated from the schema.\\nWe will probably use a data schema like format, such as yaml.\\n![flow of defeinitions changes](..\/images\/cmp-shared-agreement-definition.jpg)\\n1. edit a new version of Agreement -  e.g. new services\\n2. add \/ link new related attachments\\n3. update Agreement API\\n4. update SalesForce content \/ pages\\n5. new content and questions\/answer forms for suppliers\\n6. update purchase services\\n7. define text and structure for user tasks with relation to agreement\\n"}
{"prompt":"## Context\\nElixir provides a very high level of fault tolerance and quick restarts of failed processes, but our code has\\nto be written properly to take advantage of this restart capability. Work Queues are one solution, but not\\nalways feasible (because iterating over and queueing up thousands of tasks can be a long-running operation in\\nitself).\\n","completion":"## Decision\nAvoid writing long-running, iterative operations \u2013\u00a0especially those that change data \u2013\u00a0as loops. Rather,\\nuse a single, atomic initialization task to record tasks to be done (and a way to track completion), and use a\\n[`GenServer`](https:\/\/hexdocs.pm\/elixir\/GenServer.html) or [`GenStage`](https:\/\/hexdocs.pm\/gen_stage\/GenStage.html)\\nto handle the task (and mark it done) in atomic, transactional pieces.\\n### Pseudocode Pattern\\nInit (one-time):\\n```\\nstart transaction\\ninitialize all task tickets\\ncommit transaction\\n```\\nProcess (periodic):\\n```\\nbatch = load n pending task tickets\\nmark batch tickets as processing\\niterate over batch tickets\\nstart transaction\\nprocess one ticket\\nmark ticket as complete\\ncommit transaction\\n```\\nSweep (periodic):\\n```\\nexpired = find processing task tickets older than a reasonable timeout\\nmark expired as pending in a single update\\n```\\n"}
{"prompt":"## Context 1\\nWhen generating only from one spec (either generator), which is probably the most common use case, when doing this manually, the resulting folder structure is `<outputDir>\/serviceDir`, which in turn is unexpected.\\nThe name of the generator command is: `generate-X-client` indicating that only one client will be generated.\\n","completion":"## Decision\n1. Different behavior if only one specification is found vs. multiple.\\nFor one specification, put client directly into `<outputDir>`, for multiple make subdirectories.\\n2. `generate-openapi-client batch -i <inputFile> -o <outputDir>`\\nGenerate only the first matched input file found by default.\\nShow a log message in case there are multiple inputs found, that batch argument is required to nest the APIs.\\n3. `generate-openapi-client -i <inputFile> -o <outputDir> --flat`\\nFlatten the directory to be `<outputDir>`.\\nThrow an error if input is a directory (with multiple files).\\n4. Keep it as is. (decided)\\n5. Rename the client:\\n1. `openapi-generator` (decided)\\n2. `sap-cloud-sdk-openapi-generator`\\n| Current Name, Aliases                                    | Future Name, Aliases             | Current Behavior                                                                                            | Future Behavior                                                                                                                                                                                                |\\n| :------------------------------------------------------- | :------------------------------- | :---------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `--inputDir`, `-i` _(required)_                          | `--input`, `-i` _(required)_     | Input directory\/file                                                                                        | _same_                                                                                                                                                                                                         |\\n| `--outputDir`, `-o` _(required)_                         | `--outputDir`, `-o` _(required)_ | Output directory                                                                                            | _same_                                                                                                                                                                                                         |\\n| `--clearOutputDir`                                       | `--clearOutputDir`               | Delete all files in output directory                                                                        | _same_                                                                                                                                                                                                         |\\n| `--generateJs`                                           | `--transpile`, `-t`              | Transpiles, default true                                                                                    | Transpiles, default false. If set if tsconfig is enabled with default (unless configured). This should be explicitly stated in the documentation.                                                              |\\n| `--generatePackageJson`                                  | `--[no]-packageJson`             | Writes a default package.json, default true.                                                                | Writes a default package.json, default true (needs core dependency). Optionally in the future: Writes a custom package.json if passed. Keep boolean for now.                                                   |\\n| `--serviceMapping`                                       | `--optionsPerService`            | Considers a per service configuration file. Shows a warning if none is given (differs from OData behavior). | Only generates a per service configuration, if a file path is provided. No warning if it is not provided. If provided, `packageName` and `directoryName` will be generated, but are not required to be set. \\* |\\n| `--tsConfig`                                             | `--tsConfig`                     | tsconfig.json file to overwrite the default \"tsconfig.json\".                                                | Writes a custom tsconfig.json if passed. Document that this should be used in combination with `transpile`.                                                                                                    |\\n| `--versionInPackageJson`                                 | `--packageVersion`               | Version in package.json, default is generator version.                                                      | Version in package.json, default is `1.0.0`. Hide it.                                                                                                                                                          |\\n| **Options in OData, but not (yet in OpenAPI) generator** |\\n| `--forceOverwrite`                                       | `--overwrite`                    | Overwrite files even it they exist, default false.                                                          | _same_                                                                                                                                                                                                         |\\n| `--generateTypedocJson`                                  | `--typedocJson`                  | Writes a default typedoc.json, default true.                                                                | Remove\/deprecate, use `files` instead.                                                                                                                                                                         |\\n| **Currently hidden options**                             |\\n| `--additionalFiles`                                      | `--include`                      | Copy additional files, identified by glob. Hidden.                                                          | _same_ Expose it.                                                                                                                                                                                              |\\n| `--writeReadme`                                          | `--readme`                       | Writes a default README.md, default false.                                                                  | _same_                                                                                                                                                                                                         |\\n| **New options**                                          |\\n| -                                                        | `--skipValidation`               | Duplicate names are renamed by default.                                                                     | Duplicate names throw an error by default. Disable validation to rename duplicates.                                                                                                                            |\\n| -                                                        | `--verbose`                      | Logs everything that happens during generation by default.                                                  | Log only success \/ error per service by default. Enable verbosity through this flag.                                                                                                                           |\\n| -                                                        | `--version`, `-v`                | -                                                                                                           | Prints the version of the generator.                                                                                                                                                                           |\\n"}
{"prompt":"## Context\\n1. \u6211\u4eec\u7684\u5f88\u591a\u4e1a\u52a1\u90fd\u6709\u8fd9\u4e2a\u9700\u8981\uff0c\u5f53\u524d\u7684\u4e00\u4e9b\u5b9a\u65f6\u4efb\u52a1\u5df2\u5bfc\u81f4\u673a\u5668\u7684\u4ea7\u751f\u74f6\u9888\uff0c\u5f88\u591a\u4e1a\u52a1\u4e4b\u95f4\u7684\u8026\u5408\u6027\u4e5f\u5f88\u9ad8\uff1b\\n2. \u5f53\u524d\u53ea\u6709\u5c11\u91cf\u7684\u4e1a\u52a1\u5728\u4f7f\u7528\u6d88\u606f\u961f\u5217\u670d\u52a1\uff08activeMQ\uff09\uff1b\\n3. activeMQ \u5bf9\u975e java \u8bed\u8a00\u7684\u652f\u6301\u5e76\u4e0d\u53cb\u597d\uff1b\\n4. \u81ea\u5df1\u9700\u8981\u7ef4\u62a4\u961f\u5217\u670d\u52a1\uff0c\u505a\u76d1\u63a7\uff0c\u9ad8\u53ef\u7528\u7b49\u3002\\n","completion":"## Decision\n\u6211\u4eec\u6025\u9700\u4e00\u4e2a\u591a\u8bed\u8a00\u652f\u6301\u4e14\u53ef\u9760\u7684\u670d\u52a1\u53ef\u4ee5\u89e3\u51b3\u5982\u4e0a\u95ee\u9898\uff0c\u5373\\n* \u5f02\u6b65\u89e3\u8026\\n* \u524a\u5cf0\u586b\u8c37\\nAliyun \u7684 MNS \u548c ONS \u529f\u80fd\u7684\u91cd\u5408\u5bfc\u81f4\u9009\u62e9\u8d77\u6765\u5f88\u56f0\u96be\uff0c\u6700\u7ec8\u9009\u62e9 MNS \u6e90\u4e8e\u4ee5\u4e0b\u51e0\u70b9\u3002\\n1. \u652f\u6301\u6d88\u606f\u4f18\u5148\u7ea7\uff1b\\n2. \u53ef\u9760\u6027\u66f4\u9ad8\uff1b\\n3. \u6587\u6863\u66f4\u5b8c\u5584\uff1b\\n4. \u670d\u52a1\u66f4\u6210\u719f\uff1b\\n5. \u4f7f\u7528\u8d77\u6765\u7b80\u5355\u65b9\u4fbf\uff0c\u5b9a\u65f6\u6d88\u606f\u4e0d\u652f\u6301\u8fd9\u70b9\u6bd4\u8f83\u9057\u61be\uff1b\\n6. ONS \u4f7f\u7528\u8d77\u6765\u6709\u4e9b\u590d\u6742\uff0c\u76d1\u63a7\u65b9\u9762\u7684\u529f\u80fd\u786e\u5b9e\u5f3a\u5927\u8bb8\u591a\uff0c\u4f46\u591a\u6570\u5728\u5f00\u53d1\u4e0e\u516c\u6d4b\u4e2d\uff1b\\n7. \u4e0d\u597d\u7684\u70b9\u662f\u5404\u81ea\u90fd\u52a0\u4e86\u4e00\u4e9b\u79c1\u6d3b\uff0cMNS \u52a0\u4e86\u81ea\u5bb6\u7684\u77ed\u4fe1\u670d\u52a1\uff0cONS \u52a0\u5165\u4e86 MQTT \u7269\u8054\u7f51\u5957\u4ef6\uff0c\u8ba9 Q \u770b\u8d77\u6765\u4e0d\u5355\u7eaf\u3002\\n"}
{"prompt":"## Context and Problem Statement\\nWhich language version should we prefer? Should we restrict the version at all?\\n## Decision Drivers\\n* Nullable reference types\\n* Source compatible with Unity\u00a02018.4\\n* Binary compatible with Unity\u00a02018.4\\n* Source compatible with Unity\u00a02020.3\\n* Source compatible with online coding platforms like CodinGame or LeetCode\\n","completion":"## Decision\n* Nullable reference types\\n* Source compatible with Unity\u00a02018.4\\n* Binary compatible with Unity\u00a02018.4\\n* Source compatible with Unity\u00a02020.3\\n* Source compatible with online coding platforms like CodinGame or LeetCode\\nChosen option: \u201cC# 8\u201d.\\nThis is specified in one of the Directory.Build.props files:\\n```xml\\n<Project>\\n<PropertyGroup>\\n<LangVersion>8<\/LangVersion>\\n<Nullable>enable<\/Nullable>\\n<\/PropertyGroup>\\n<\/Project>\\n```\\nThe main reason to ditch 7.3 is that it complicates nullable annotations.\\nThe latest available compiler version C#\u00a09 is not supported by some popular online coding platforms as of June 2021.\\n### Negative Consequences\\n- C# 8 is not source-compatible with Unity\u00a02018.4.\\nHowever it is binary-compatible via .NET Standard 2.0.\\nUnity 2020.3 partially supports C# 8.\\n"}
{"prompt":"## Context\\nVerify have a number of pre-existing (closed source) libraries at various\\nlevels of abstraction for handling SAML behaviour. By depending on these\\nwe can benefit from all the work that's already been done making these easy\\nto use and secure by default.\\nThere's a plan in the medium term to open source a lot of these libraries.\\nBy depending on as few of them as possible we should be able to call the\\nverify-service-provider \"fully open\" sooner.\\nSome of the libraries at higher levels of abstraction are of questionable\\nvalue - although they make it easy to be consistent with the rest of verify\\nthey abstract away the exact nature of the SAML which makes the code hard to\\nread.\\n","completion":"## Decision\nGood libraries that we should use for now:\\n* saml-serialisers\\n* ida-saml-extensions\\n* saml-security\\n* saml-metadata-bindings\\nLibraries we think we should ignore\\n* hub-saml\\n"}
{"prompt":"## Context\\nThe services need to look and feel the same. They need to comply with [adr 0002 - Use a consistent user experience](0002-use-a-consistent-user-experience.md).\\nNQC\u2019s SRS and DPS services are already built. They comply with GOV.UK styles to a certain extent. They should be partially exempted from this decision for the moment\\nGDS have released their new [Design System](https:\/\/design-system.service.gov.uk). It does not yet support non-GOV.UK designs but that is on the [roadmap](https:\/\/design-system.service.gov.uk\/roadmap\/).\\n[CCS Website](https:\/\/www.crowncommercial.gov.uk\/s\/) has complied with, and contributed to, GDS patterns, but has a divergent colour palette.\\n","completion":"## Decision\n- All new services should share a common front end toolkit\\n- There may be different modules per language, sharing the same HTML, CSS, JavaScript assets\\n- SRS stylesheets should comply with this kit as closely as practicable. We should revisit this decision as new phases of work are considered for transparency and supplier registration including DPS\\n- Comply with GDS Design System except where CCS styling requires differences; document the differences\\n- Use a checkout of the Frontend toolkit as the base for front end resources\\n- Add a new repository for style and other resource overrides to meet CCS needs,\\n- Periodically pull new versions of Frontend toolkit and test\\n- Work with GDS to help them support non-GOV.UK styles\\n"}
{"prompt":"## Context\\nSQLite does not feature a date format data type. As such writing a date format\\ndata object from R, results in conversion to an integer with no relevance to the\\noriginal date.\\n","completion":"## Decision\nIncoming raw data will not be converted to date format in R, and instead\\nmaintained as a string for full dates (Y-m-d) or part dates (Y-m) and as an\\ninteger for years.\\n"}
{"prompt":"## Context\\nCurrently, the SDK allows for custom account types; the `auth` keeper stores any type fulfilling its `Account` interface. However `auth` does not handle exporting or loading accounts to\/from a genesis file, this is done by `genaccounts`, which only handles one of 4 concrete account types (`BaseAccount`, `ContinuousVestingAccount`, `DelayedVestingAccount` and `ModuleAccount`).\\nProjects desiring to use custom accounts (say custom vesting accounts) need to fork and modify `genaccounts`.\\n","completion":"## Decision\nIn summary, we will (un)marshal all accounts (interface types) directly using amino, rather than converting to `genaccounts`\u2019s `GenesisAccount` type. Since doing this removes the majority of `genaccounts`'s code, we will merge `genaccounts` into `auth`. Marshalled accounts will be stored in `auth`'s genesis state.\\nDetailed changes:\\n### 1) (Un)Marshal accounts directly using amino\\nThe `auth` module's `GenesisState` gains a new field `Accounts`. Note these aren't of type `exported.Account` for reasons outlined in section 3.\\n```go\\n\/\/ GenesisState - all auth state that must be provided at genesis\\ntype GenesisState struct {\\nParams   Params           `json:\"params\" yaml:\"params\"`\\nAccounts []GenesisAccount `json:\"accounts\" yaml:\"accounts\"`\\n}\\n```\\nNow `auth`'s `InitGenesis` and `ExportGenesis` (un)marshal accounts as well as the defined params.\\n```go\\n\/\/ InitGenesis - Init store state from genesis data\\nfunc InitGenesis(ctx sdk.Context, ak AccountKeeper, data GenesisState) {\\nak.SetParams(ctx, data.Params)\\n\/\/ load the accounts\\nfor _, a := range data.Accounts {\\nacc := ak.NewAccount(ctx, a) \/\/ set account number\\nak.SetAccount(ctx, acc)\\n}\\n}\\n\/\/ ExportGenesis returns a GenesisState for a given context and keeper\\nfunc ExportGenesis(ctx sdk.Context, ak AccountKeeper) GenesisState {\\nparams := ak.GetParams(ctx)\\nvar genAccounts []exported.GenesisAccount\\nak.IterateAccounts(ctx, func(account exported.Account) bool {\\ngenAccount := account.(exported.GenesisAccount)\\ngenAccounts = append(genAccounts, genAccount)\\nreturn false\\n})\\nreturn NewGenesisState(params, genAccounts)\\n}\\n```\\n### 2) Register custom account types on the `auth` codec\\nThe `auth` codec must have all custom account types registered to marshal them. We will follow the pattern established in `gov` for proposals.\\nAn example custom account definition:\\n```go\\nimport authtypes \"github.com\/cosmos\/cosmos-sdk\/x\/auth\/types\"\\n\/\/ Register the module account type with the auth module codec so it can decode module accounts stored in a genesis file\\nfunc init() {\\nauthtypes.RegisterAccountTypeCodec(ModuleAccount{}, \"cosmos-sdk\/ModuleAccount\")\\n}\\ntype ModuleAccount struct {\\n...\\n```\\nThe `auth` codec definition:\\n```go\\nvar ModuleCdc *codec.LegacyAmino\\nfunc init() {\\nModuleCdc = codec.NewLegacyAmino()\\n\/\/ register module msg's and Account interface\\n...\\n\/\/ leave the codec unsealed\\n}\\n\/\/ RegisterAccountTypeCodec registers an external account type defined in another module for the internal ModuleCdc.\\nfunc RegisterAccountTypeCodec(o interface{}, name string) {\\nModuleCdc.RegisterConcrete(o, name, nil)\\n}\\n```\\n### 3) Genesis validation for custom account types\\nModules implement a `ValidateGenesis` method. As `auth` does not know of account implementations, accounts will need to validate themselves.\\nWe will unmarshal accounts into a `GenesisAccount` interface that includes a `Validate` method.\\n```go\\ntype GenesisAccount interface {\\nexported.Account\\nValidate() error\\n}\\n```\\nThen the `auth` `ValidateGenesis` function becomes:\\n```go\\n\/\/ ValidateGenesis performs basic validation of auth genesis data returning an\\n\/\/ error for any failed validation criteria.\\nfunc ValidateGenesis(data GenesisState) error {\\n\/\/ Validate params\\n...\\n\/\/ Validate accounts\\naddrMap := make(map[string]bool, len(data.Accounts))\\nfor _, acc := range data.Accounts {\\n\/\/ check for duplicated accounts\\naddrStr := acc.GetAddress().String()\\nif _, ok := addrMap[addrStr]; ok {\\nreturn fmt.Errorf(\"duplicate account found in genesis state; address: %s\", addrStr)\\n}\\naddrMap[addrStr] = true\\n\/\/ check account specific validation\\nif err := acc.Validate(); err != nil {\\nreturn fmt.Errorf(\"invalid account found in genesis state; address: %s, error: %s\", addrStr, err.Error())\\n}\\n}\\nreturn nil\\n}\\n```\\n### 4) Move add-genesis-account cli to `auth`\\nThe `genaccounts` module contains a cli command to add base or vesting accounts to a genesis file.\\nThis will be moved to `auth`. We will leave it to projects to write their own commands to add custom accounts. An extensible cli handler, similar to `gov`, could be created but it is not worth the complexity for this minor use case.\\n### 5) Update module and vesting accounts\\nUnder the new scheme, module and vesting account types need some minor updates:\\n- Type registration on `auth`'s codec (shown above)\\n- A `Validate` method for each `Account` concrete type\\n"}
{"prompt":"## Context\\nInfrastructure deployment, in the traditional imperative way, is prone to oversights and errors or beholden to extremely rigorous processes that stifle change. Hunting down a small deviation from the design, such as having accidentally added a public IP to a machine, can spell disaster.\\nPerpendicular to imperative specifications there are declarative specifications of infrastructure. These declarative specifications describe the desired state in favour of the steps to reach such a state. Infrastructure as code is a way to script the declarative specification using a Domain Specific Language or a General Programming Language.\\n","completion":"## Decision\nInfrastructure as Code allows us to prevent infrastructure drift and allows us to run architectural fitness functions on the specification instead of the run-time environment. This in turn allows us to test many scenarios without actually setting the deployment of the infrastructure in motion.\\nWe are guaranteed to run the tests against what will in fact be deployed when we decide to execute the specification. Therefore, creating a very agile and evolvable ecosystem.\\nWe can recommend **Cloudformation** because it has built-in AWS specific features that assist the infrastructure engineer using visual designers but also checks on valid configurations. However, this is not officially part of the ADR as we recognize that an alternative, such as Terraform, could already be mastered by the implementation team.\\n"}
{"prompt":"## Context\\nMessages are denormalized in Cassandra in order to:\\n- access them by their unique identifier (messageId), for example through the JMAP protocol\\n- access them by their mailbox identifier and Unique IDentifier within that mailbox (mailboxId + uid), for example\\nthrough the IMAP protocol\\nHere is the table organisation:\\n- `messageIdTable` Holds mailbox and flags for each message, lookup by mailbox ID + UID\\n- `imapUidTable` Holds mailbox and flags for each message, lookup by message ID\\nFailures during the denormalization process will lead to inconsistencies between the two tables.\\nThis can lead to the following user experience:\\n```\\nBOB receives a message\\nThe denormalization process fails\\nBOB can read the message via JMAP\\nBOB cannot read the message via IMAP\\nBOB marks a message as SEEN\\nThe denormalization process fails\\nThe message is SEEN in JMAP\\nThe message is UNSEEN in IMAP\\n```\\n### Current operations\\n- Adding a message:\\n- (CassandraMessageMapper) First reference the message in `messageIdTable` then in `imapUidTable`.\\n- (CassandraMessageIdMapper) First reference the message in `imapUidTable` then in `messageIdTable`.\\n- Deleting a message:\\n- (CassandraMessageMapper) First delete the message in `imapUidTable` then in `messageIdTable`.\\n- (CassandraMessageIdMapper) Read the message metadata using `imapUidTable`, then first delete the message in\\n`imapUidTable` then in `messageIdTable`.\\n- Copying a message:\\n- (CassandraMessageMapper) Read the message first, then first reference the message in `messageIdTable` then\\nin `imapUidTable`.\\n- Moving a message:\\n- (CassandraMessageMapper) Logically copy then delete. A failure in the chain migh lead to duplicated message (present\\nin both source and destination mailbox) as well as different view in IMAP\/JMAP.\\n- (CassandraMessageIdMapper) First reference the message in `imapUidTable` then in `messageIdTable`.\\n- Updating a message flags:\\n- (CassandraMessageMapper) First update conditionally the message in `imapUidTable` then in `messageIdTable`.\\n- (CassandraMessageIdMapper) First update conditionally the message in `imapUidTable` then in `messageIdTable`.\\n","completion":"## Decision\nAdopt `imapUidTable` as a source of truth. Because `messageId` allows tracking changes to messages accross mailboxes\\nupon copy and moves. Furthermore, that is the table on which conditional flags updates are performed.\\nAll writes will be performed to `imapUidTable` then performed on `messageIdTable` if successful.\\nWe thus need to modify CassandraMessageMapper 'add' + 'copy' to first write to the source of truth (`imapUidTable`)\\nWe can adopt a retry policy of the `messageIdTable` projection update as a mitigation strategy.\\nUsing `imapUidTable` table as a source of truth, we can rebuild the `messageIdTable` projection:\\n- Iterating `imapUidTable` entries, we can rewrite entries in `messageIdTable`\\n- Iterating `messageIdTable` we can remove entries not referenced in `imapUidTable`\\n- Adding a delay and a re-check before the actual fix can decrease the occurrence of concurrency issues\\nWe will expose a webAdmin task for doing this.\\n"}
{"prompt":"## Context\\nTo provide a risk reduction method for double signing incidents mistakenly executed by validators\\n- Validators often mistakenly run duplicated validators to cause double-signing incident\\n- This proposed feature is to reduce the risk of mistaken double-signing incident by checking recent N blocks before voting begins\\n- When we think of such serious impact on double-signing incident, it is very reasonable to have multiple risk reduction algorithm built in node daemon\\n","completion":"## Decision\nWe would like to suggest a double signing risk reduction method.\\n- Methodology : query recent consensus results to find out whether node's consensus key is used on consensus recently or not\\n- When to check\\n- When the state machine starts `ConsensusReactor` after fully synced\\n- When the node is validator ( with privValidator )\\n- When `cs.config.DoubleSignCheckHeight > 0`\\n- How to check\\n1. When a validator is transformed from syncing status to fully synced status, the state machine check recent N blocks (`latest_height - double_sign_check_height`) to find out whether there exists consensus votes using the validator's consensus key\\n2. If there exists votes from the validator's consensus key, exit state machine program\\n- Configuration\\n- We would like to suggest by introducing `double_sign_check_height` parameter in `config.toml` and cli, how many blocks state machine looks back to check votes\\n- <span v-pre>`double_sign_check_height = {{ .Consensus.DoubleSignCheckHeight }}`<\/span> in `config.toml`\\n- `tendermint node --consensus.double_sign_check_height` in cli\\n- State machine ignore checking procedure when `double_sign_check_height == 0`\\n"}
{"prompt":"## Context\\nAt the moment, we have a set of scheduled tasks on Heroku that run every 10 minutes, hour, or day. These range from building the static site (every 10 minutes), to reporting stalled builds (every hour), to refreshing the database from production data (in dev\/staging, every night). The main job here, building the static site, has started to fail because it occasionally runs longer than 10 minutes. The Heroku scheduler will only allow a scheduled instance to live for as long as the interval on the task. Heroku is therefore killing the instance before it can finish, leading to the incomplete builds and frequent reports of 'stalled' (in this instance, killed) builds.\\nWe need to schedule static site builds in a way that removes this strict time constraint so that they will always finish building.\\n","completion":"## Decision\nWe will continue to use the Heroku scheduler on a ten-minute interval, but will instead use the Heroku CLI to run the management command (.\/manage.py build_static_site) in a separate, detached worker dyno. This worker dyno has a lifespan of 24 hours and is not limited to the interval of the scheduled job.\\nThe command we will use is `heroku run:detached -a <app_name> .\/manage.py build_static_site`.\\nPros\\n----\\n* Very easy to implement.\\n* Will allow static site builds that take up to 24 hours (which is greater than any reasonable build length).\\nCons\\n----\\n* We remain unable to responsively schedule static site builds at the moment they are needed and rely on the scheduler to kick off a build.\\n* We have to expose a Heroku API key and install the Heroku CLI buildpack on our main Publisher app.\\n"}
{"prompt":"## Context\\nThe Elm runtime uses `requestAnimationFrame` to keep the view updated. In order to\\nrun faster specs, elm-spec uses [@simon\/fake-timers](https:\/\/github.com\/sinonjs\/fake-timers)\\nto fake the `requestAnimationFrame` function (along with other timers). This provides\\nelm-spec with control over when to run this function, and thus when to update the view.\\nHowever, things are a little more complicated than they might seem. It turns out that the\\nElm runtime also uses `requestAnimationFrame` during some commands that involve the DOM,\\nin particular those like `Browser.Dom.getElement` in the `Browser.Dom` module. This means\\nthat sometimes a step might actually need multiple animation frames to complete and update\\nthe view as expected. This gets complicated when a program uses the `Browser.Events.onAnimationFrame`\\nsubscription, which sends an update message on each animation frame.\\nPreviously, elm-spec would run the next animation frame before any step or observer that\\ntouched the DOM (so that the view was updated for that step), and attempt to continue to\\nrun animation frame tasks until none remained at the end of that step. This was a little weird,\\nand had to try and only run *newly added* tasks so that the spec would not go into an\\ninfinite loop -- since the task that updates the view and the task that listens for\\nanimation frames each add a new request for the next animation frame when they are run.\\nOne major problem with this approach is that sometimes a program might need to run the next\\nanimation frame even if the step itself doesn't touch the DOM. For example, if `Browser.Dom.getElement`\\nis called when a port message is received, and the spec wants to see that the size of the\\nelement is retrieved and sent out on another port or something. The step and observer just\\ninvolves Ports but an animation frame needs to run for the command to complete.\\nThere also exists `Spec.Time.nextAnimationFrame` for manually running the next animation frame.\\nHowever, in the case described, something would happen where even that wouldn't trigger the\\nexpected behavior. And in any case, this would actually continue to run animation frames if\\nthey were added in the process of execiting a command, and so probably should have been called\\n'runAnimationFramesUntilComplete' or something.\\n","completion":"## Decision\nElm-spec will run an animation frame (or try to) at the *end* of each spec step, no matter what,\\nincluding the step that runs the initial command. And it will *only* run one animation frame.\\nIf any extra animation frame tasks are detected (that is, if there is more than one remaining\\nanimation frame task, which is the view animator), then the scenario will be rejected with a\\nmessage explaining that one of the steps triggers extra animation frames.\\nIn order to prevent the scenario from being rejected, one needs to add `Spec.Time.allowExtraAnimationFrames`\\nto the setup section of the scenario. The point of this really is just to let people know that\\nsomething funny could be going on. One can then use `Spec.Time.nextAnimationFrame` to\\nrun single animation frames until one triggers the behavior one wants.\\nThis seems to work much better. First, many common scenarios will just work as expected. If,\\nfor example, one triggers a `Browser.Dom.getElement` command based on some step that just\\nsimulates receiving a port message, then that will just work as expected since the animation frame\\nwill be run at the end of the step -- this will update the view and also run the `getElement`\\ncommand.\\nSecond, in the case of more complicated scenarios involving DOM commands that trigger other\\nDOM commands or `Browser.Events.onAnimationFrame` the spec writer will be alerted to the fact\\nthat things are complicated, and will have full control over how many animation frames to\\nexecute.\\n"}
{"prompt":"## Context\\n[#1776](https:\/\/github.com\/tendermint\/tendermint\/issues\/1776) was\\nopened in relation to implementation of a Plasma child chain using Tendermint\\nCore as consensus\/replication engine.\\nDue to the requirements of [Minimal Viable Plasma (MVP)](https:\/\/ethresear.ch\/t\/minimal-viable-plasma\/426) and [Plasma Cash](https:\/\/ethresear.ch\/t\/plasma-cash-plasma-with-much-less-per-user-data-checking\/1298), it is necessary for ABCI apps to have a mechanism to handle the following cases (more may emerge in the near future):\\n1. `deposit` transactions on the Root Chain, which must consist of a block\\nwith a single transaction, where there are no inputs and only one output\\nmade in favour of the depositor. In this case, a `block` consists of\\na transaction with the following shape:\\n```\\n[0, 0, 0, 0, #input1 - zeroed out\\n0, 0, 0, 0, #input2 - zeroed out\\n<depositor_address>, <amount>, #output1 - in favour of depositor\\n0, 0, #output2 - zeroed out\\n<fee>,\\n]\\n```\\n`exit` transactions may also be treated in a similar manner, wherein the\\ninput is the UTXO being exited on the Root Chain, and the output belongs to\\na reserved \"burn\" address, e.g., `0x0`. In such cases, it is favourable for\\nthe containing block to only hold a single transaction that may receive\\nspecial treatment.\\n2. Other \"internal\" transactions on the child chain, which may be initiated\\nunilaterally. The most basic example of is a coinbase transaction\\nimplementing validator node incentives, but may also be app-specific. In\\nthese cases, it may be favourable for such transactions to\\nbe ordered in a specific manner, e.g., coinbase transactions will always be\\nat index 0. In general, such strategies increase the determinism and\\npredictability of blockchain applications.\\nWhile it is possible to deal with the cases enumerated above using the\\nexisting ABCI, currently available result in suboptimal workarounds. Two are\\nexplained in greater detail below.\\n### Solution 1: App state-based Plasma chain\\nIn this work around, the app maintains a `PlasmaStore` with a corresponding\\n`Keeper`. The PlasmaStore is responsible for maintaing a second, separate\\nblockchain that complies with the MVP specification, including `deposit`\\nblocks and other \"internal\" transactions. These \"virtual\" blocks are then broadcasted\\nto the Root Chain.\\nThis naive approach is, however, fundamentally flawed, as it by definition\\ndiverges from the canonical chain maintained by Tendermint. This is further\\nexacerbated if the business logic for generating such transactions is\\npotentially non-deterministic, as this should not even be done in\\n`Begin\/EndBlock`, which may, as a result, break consensus guarantees.\\nAdditinoally, this has serious implications for \"watchers\" - independent third parties,\\nor even an auxilliary blockchain, responsible for ensuring that blocks recorded\\non the Root Chain are consistent with the Plasma chain's. Since, in this case,\\nthe Plasma chain is inconsistent with the canonical one maintained by Tendermint\\nCore, it seems that there exists no compact means of verifying the legitimacy of\\nthe Plasma chain without replaying every state transition from genesis (!).\\n### Solution 2: Broadcast to Tendermint Core from ABCI app\\nThis approach is inspired by `tendermint`, in which Ethereum transactions are\\nrelayed to Tendermint Core. It requires the app to maintain a client connection\\nto the consensus engine.\\nWhenever an \"internal\" transaction needs to be created, the proposer of the\\ncurrent block broadcasts the transaction or transactions to Tendermint as\\nneeded in order to ensure that the Tendermint chain and Plasma chain are\\ncompletely consistent.\\nThis allows \"internal\" transactions to pass through the full consensus\\nprocess, and can be validated in methods like `CheckTx`, i.e., signed by the\\nproposer, is the semantically correct, etc. Note that this involves informing\\nthe ABCI app of the block proposer, which was temporarily hacked in as a means\\nof conducting this experiment, although this should not be necessary when the\\ncurrent proposer is passed to `BeginBlock`.\\nIt is much easier to relay these transactions directly to the Root\\nChain smart contract and\/or maintain a \"compressed\" auxiliary chain comprised\\nof Plasma-friendly blocks that 100% reflect the canonical (Tendermint)\\nblockchain. Unfortunately, this approach not idiomatic (i.e., utilises the\\nTendermint consensus engine in unintended ways). Additionally, it does not\\nallow the application developer to:\\n- Control the _ordering_ of transactions in the proposed block (e.g., index 0,\\nor 0 to `n` for coinbase transactions)\\n- Control the _number_ of transactions in the block (e.g., when a `deposit`\\nblock is required)\\nSince determinism is of utmost importance in blockchain engineering, this approach,\\nwhile more viable, should also not be considered as fit for production.\\n","completion":"## Decision\n### `ProposeTx`\\nIn order to address the difficulties described above, the ABCI interface must\\nexpose an additional method, tentatively named `ProposeTx`.\\nIt should have the following signature:\\n```\\nProposeTx(RequestProposeTx) ResponseProposeTx\\n```\\nWhere `RequestProposeTx` and `ResponseProposeTx` are `message`s with the\\nfollowing shapes:\\n```\\nmessage RequestProposeTx {\\nint64 next_block_height = 1; \/\/ height of the block the proposed tx would be part of\\nValidator proposer = 2; \/\/ the proposer details\\n}\\nmessage ResponseProposeTx {\\nint64 num_tx = 1; \/\/ the number of tx to include in proposed block\\nrepeated bytes txs = 2; \/\/ ordered transaction data to include in block\\nbool exclusive = 3; \/\/ whether the block should include other transactions (from `mempool`)\\n}\\n```\\n`ProposeTx` would be called by before `mempool.Reap` at this\\n[line](https:\/\/github.com\/tendermint\/tendermint\/blob\/9cd9f3338bc80a12590631632c23c8dbe3ff5c34\/consensus\/state.go#L935).\\nDepending on whether `exclusive` is `true` or `false`, the proposed\\ntransactions are then pushed on top of the transactions received from\\n`mempool.Reap`.\\n### `DeliverTx`\\nSince the list of `tx` received from `ProposeTx` are _not_ passed through `CheckTx`,\\nit is probably a good idea to provide a means of differentiatiating \"internal\" transactions\\nfrom user-generated ones, in case the app developer needs\/wants to take extra measures to\\nensure validity of the proposed transactions.\\nTherefore, the `RequestDeliverTx` message should be changed to provide an additional flag, like so:\\n```\\nmessage RequestDeliverTx {\\nbytes tx = 1;\\nbool internal = 2;\\n}\\n```\\nAlternatively, an additional method `DeliverProposeTx` may be added as an accompanient to\\n`ProposeTx`. However, it is not clear at this stage if this additional overhead is necessary\\nto preserve consensus guarantees given that a simple flag may suffice for now.\\n"}
{"prompt":"# Context and Problem Statement\\nHow to convert image files properly to pdf?\\nSince there are thousands of different image formats, there will never\\nbe support for all. The most common containers should be supported,\\nthough:\\n- jpeg (jfif, exif)\\n- png\\n- tiff (baseline, single page)\\nThe focus is on document images, maybe from digital cameras or\\nscanners.\\n# Considered Options\\n* [pdfbox](https:\/\/pdfbox.apache.org\/) library\\n* [imagemagick](https:\/\/www.imagemagick.org\/) external command\\n* [img2pdf](https:\/\/github.com\/josch\/img2pdf) external command\\n* [tesseract](https:\/\/github.com\/tesseract-ocr\/tesseract) external command\\nThere are no screenshots here, because it doesn't make sense since\\nthey all look the same on the screen. Instead we look at the files\\nproperties.\\n**Input File**\\nThe input files are:\\n```\\n$ identify input\/*\\ninput\/jfif.jpg JPEG 2480x3514 2480x3514+0+0 8-bit sRGB 240229B 0.000u 0:00.000\\ninput\/letter-en.jpg JPEG 1695x2378 1695x2378+0+0 8-bit Gray 256c 467341B 0.000u 0:00.000\\ninput\/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000\\ninput\/letter-en.tiff TIFF 1695x2378 1695x2378+0+0 8-bit Grayscale Gray 4030880B 0.000u 0:00.000\\n```\\nSize:\\n- jfif.jpg 240k\\n- letter-en.jpg 467k\\n- letter-en.png 191k\\n- letter-en.tiff 4.0M\\n","completion":"## Decision\nTesseract.\\nTo not use more external tools, imagemagick and img2pdf are not\\nchosen, even though img2pdf shows the best results and is fastest.\\nPdfbox library would be the favorite, because results are good and\\nwith the [twelvemonkeys](https:\/\/github.com\/haraldk\/TwelveMonkeys)\\nlibrary there is support for many images. The priority is to avoid\\nmore external commands if possible.\\nBut since there already is a dependency to tesseract and it can create\\nsearchable pdfs, the decision is to use tesseract for this. Then PDFs\\nwith images can be converted to searchable PDFs with images. And text\\nextraction is required anyways.\\n"}
{"prompt":"## Context\\nWe need to record the architectural decisions made on this project.\\n","completion":"## Decision\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nWe will not burden the project with additional dependencies for creating\\nADRs: the format is simple enough to copy an existing ADR and\\nchange appropriate fields manually.\\nThe name of the file will contain the number of the ADR, which\\nwill be exactly copied into the title inside the ADR itself. For\\nexample, this file is 0001-record-architecture-decisions.md,\\nso the number \"0001\" (rather than \"1\") is in the title. (In other\\nteams, the number \"1\" is used, but we will use the full number.)\\nThe Date given in the decision is the date it was created.\\nEach status will be recorded on the decision, along with the date it came into force.\\nThis is to enable tracking of when decisions were proposed\/accepted\/superseded etc.,\\nwhich should help identify stale decisions (e.g. proposed a long time ago\\nbut never accepted). The latest status in the list is assumed current.\\n"}
