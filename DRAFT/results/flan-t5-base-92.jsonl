{"File Name":"macondo\/0007-automatic-aligning-of-user-host-user-for-better-interoperability.md","Context":"## Context\\nA well constructed macondo command should be easy to run directly (without\\nmacondo) by just executing it. This is hard to achieve unless the user in the\\ncontainer resembles as much as possible the host running it.\\nOne case where this is important is modifying files in the host. This is\\nachieved by mounting a volume into the running container. However, the files\\nwritten from the docker container are owned by the docker user, i.e. the user id\\nand group id of the file is that of whatever docker user happened to write it.\\nSo if the container runs with the root user, which is unfortunately common, then\\nthe files written to the host also are owned by root, making them innacessible\\nto the host user.\\nPotential ideas:\\n- A flag that enables user alignment and mounts HOME into docker's HOME.\\n- Customize home when running in OSX to be \/Users\/bla instead of \/home\/ble\\n- Even if mounting the whole home, it should be easy to mount current PWD into\\nsomething else \/mnt\/blablabla and use tha as working directory\\n","Decision":"Add a mechanism to align the user\/group of the host system with that of the docker container.\\n","tokens":237,"id":2193,"text":"## Context\\nA well constructed macondo command should be easy to run directly (without\\nmacondo) by just executing it. This is hard to achieve unless the user in the\\ncontainer resembles as much as possible the host running it.\\nOne case where this is important is modifying files in the host. This is\\nachieved by mounting a volume into the running container. However, the files\\nwritten from the docker container are owned by the docker user, i.e. the user id\\nand group id of the file is that of whatever docker user happened to write it.\\nSo if the container runs with the root user, which is unfortunately common, then\\nthe files written to the host also are owned by root, making them innacessible\\nto the host user.\\nPotential ideas:\\n- A flag that enables user alignment and mounts HOME into docker's HOME.\\n- Customize home when running in OSX to be \/Users\/bla instead of \/home\/ble\\n- Even if mounting the whole home, it should be easy to mount current PWD into\\nsomething else \/mnt\/blablabla and use tha as working directory\\n\n\n##Decision\nAdd a mechanism to align the user\/group of the host system with that of the docker container.\\n","Predictions":"## ContextnHistorically we have had \"two ways\" of configuring Verify Service Provider:n- Using environment variablesn- Using a YAML filenWhen using environment variables the application used the verify-service-provider-env.ymlnfile from the resources directory (so inside the jar). When using the YAML file you wouldnpass the path to a different file as a command line parameter - usually peoplenwould use the example one that's contained in the repo.nThere were a couple of reasons for the extra complexity of managing two files, both due tonrestrictions with the java buildpack used by cloudfoundry:n- It's not possible to specify command line arguments through the java buildpack,nso you can't specify a path to your config filen- We weren't confident in the way cloudfoundry manages static files, so we didn't wantnto rely on one.nThere was also a philosophical point that 12 factor applications should be configured throughntheir environment. This made the \"hide the configuration in the.jar and do everything throughnenv vars\" way appealing.n ## Decision We will remove the verify-service-provider-env.yml file from src\/main\/resourcesnThe application will default to the verify-service-provider.yml file that's included in the.zip if no command line arguments are provided.nIf the application is started without command line arguments specifying a yml filenAND no environment variables have been set, startup should error gracefully and tellnthe user that the configuration fields have not been specified for example:n\"ERROR - no configuration fields found, either set environment variables or specifyna configuration file using command line arguments server path\/to\/verify-service-provider.yml>\"We will establish the path to verify-service-provider.yml by asking java for thenpath to the.jar file containing the Application class and looking in the parentnfolder.n## Decision We will remove the verify-service-provider-env.yml file from src\/main\/resourcesnThe application will default to the verify-service-provider.yml file that's included in the.zip if no command line arguments are provided.nIf the application is started without command line arguments specifying a yml filenAND no environment variables have been set, startup should error gracefully and tellnthe user that the configuration fields have not been specified for example:n\"ERROR - no configuration fields found, either set environment variables or specifyna configuration file using command line arguments server path\/to\/verify-service-provider.yml>\"We will establish the path to verify-service-provider.yml by asking java for thenpath to the.jar file containing the Application class and looking in the parentnfolder.n## Decision We will remove the verify-service-provider-env.yml file from src\/main\/resourcesnThe application will default to the verify-service-provider.yml file that's included in the.zip if no command line arguments are provided.nIf the application is started without command line arguments specifying a yml filenAND no environment variables have been set, startup should error gracefully and tellnthe user that the configuration fields have not been specified for example:n\"ERROR - no configuration fields found, either set environment variables or specifyna configuration file using command line arguments server path\/to\/verify-service-provider.yml>\"We will establish the path to verify-service-provider.yml by asking java for thenpath to the.jar file containing the Application class and looking in the parentnfolder.n## Decision We will remove the verify-service-provider.yml file from src\/main\/resourcesnThe application will default to the verify-service-provider.yml file that's included in the.zip"}
{"File Name":"old-web-frontend\/0002-use-aws-s3-as-host-solution.md","Context":"## Context and Problem Statement\\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\\n## Decision Drivers\\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\n","Decision":"* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\nChosen option: \"AWS\", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\\n","tokens":53,"id":583,"text":"## Context and Problem Statement\\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\\n## Decision Drivers\\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\n\n\n##Decision\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\nChosen option: \"AWS\", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\\n","Predictions":"Driversn* Easy to updaten* High availabilityn* Easy to configure HTTPSn* Observabilityn"}
{"File Name":"git-en-boite\/0009-prefer-dugite-over-raw-git-commands-in-tests.md","Context":"## Context\\nThe build failed because the version of git used in CircleCI was not what we expected.\\n","Decision":"Use `GitProcess.exec` even in tests that need to lean on the git process.\\n","tokens":22,"id":3504,"text":"## Context\\nThe build failed because the version of git used in CircleCI was not what we expected.\\n\n\n##Decision\nUse `GitProcess.exec` even in tests that need to lean on the git process.\\n","Predictions":"The build failed because the version of git used in CircleCI was not what we expected.n ## ContextnWe were unable to address a security vulnerability in jQuery because we couldn't update Bootstrap. This was due to thenfact that it was present as both a gem and an npm package.n ## Decision We removed asset pipeline completely and moved all css and image assets to webpacker. This allowed us to update jQuerynvia yarn.n ## ContextnDRTv1 tried to normalise all ports into a single model. We frequently had requests for port specific featuresnor structures. Also itn ## Decision Use a jvm per airportn ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was not what we expected.n ## ContextnThe build failed because the version of git used in CircleCI was"}
{"File Name":"viplab-websocket-api\/0004-use-asymmetric-jwt-signing.md","Context":"## Context and Problem Statement\\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\\nJWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\\nWhen implementing JWTs one must decide which method to use.\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n","Decision":"* Multi tenant support with own keys for each tenant\\nChosen option: \"Asymmetric JWT signing\", because it the only option which allow to use different keys for different tenants.\\n### Positive Consequences\\n* multiple keys are supported\\n### Negative Consequences\\n* complex management of keys\\n","tokens":77,"id":1721,"text":"## Context and Problem Statement\\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\\nJWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\\nWhen implementing JWTs one must decide which method to use.\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n\n\n##Decision\n* Multi tenant support with own keys for each tenant\\nChosen option: \"Asymmetric JWT signing\", because it the only option which allow to use different keys for different tenants.\\n### Positive Consequences\\n* multiple keys are supported\\n### Negative Consequences\\n* complex management of keys\\n","Predictions":"is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be encrypted. A random salt _s(k)_ is generated. By applying the PBKDF2 (mentioned earlier) with _r_ and _s(k)_, the cryptographic key _e(k)_ and the initialization vector _i(k)_ is being derived. The value of _k_ will be encrypted using _e(k)_ and _i(k)_. The seed _s(k)_ will be encoded as prefix into the encrypted value.n During the **get phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_."}
{"File Name":"jabref\/0014-separate-URL-creation-to-enable-proper-logging.md","Context":"## Context and Problem Statement\\nFetchers are failing.\\nThe reason why they are failing needs to be investigated.\\n* Claim 1: Knowing the URL which was used to query the fetcher eases debugging\\n* Claim 2: Somehow logging the URL eases debugging (instead of showing it in the debugger only)\\nHow to properly log the URL used for fetching?\\n## Decision Drivers\\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\n","Decision":"* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\nChosen option: \"Separate URL creation\", because comes out best \\(see below\\).\\n","tokens":132,"id":4748,"text":"## Context and Problem Statement\\nFetchers are failing.\\nThe reason why they are failing needs to be investigated.\\n* Claim 1: Knowing the URL which was used to query the fetcher eases debugging\\n* Claim 2: Somehow logging the URL eases debugging (instead of showing it in the debugger only)\\nHow to properly log the URL used for fetching?\\n## Decision Drivers\\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\n\n\n##Decision\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\nChosen option: \"Separate URL creation\", because comes out best \\(see below\\).\\n","Predictions":"# ADR 0276: Problem Matchersn**Date** 2019-06-05n**Status** Acceptedn## ContextnCompilation failures during a CI build should surface good error messages.nFor example, the actual compile errors from the typescript compiler should bubble as issues in the UI. And not simply \"tsc exited with exit code 1\".nVSCode has an extensible model for solving this type of problem. VSCode allows users to configure which [problems matchers](https:\/\/code.visualstudio.com\/docs\/editor\/tasks#_defining-a-problem-matcher) to use, when scanning output. For example, a user can apply the tsc problem matcher to receive a rich error output experience in VSCode, when compiling their typescript project.nThe problem-matcher concept fits well with \"setup\" actions. For example, the setup-nodejs action will download node.js, add it to the PATH, and register the tsc problem matcher. For the duration of the job, the tsc problem matcher will be applied against the output.n ## Decision ### Registrationn#### Using :: commandn::add-matcher::path-to-problem-matcher-config.jsonnUsing a :: command allows for flexibility:n- Ad hoc scripts can register problem matchersn- Allows problem matchers to be conditionally registerednNote, if a matcher with the same name is registered a second time, it will clobber the first instance.n ## Context and Problem StatementnHow should Axiomatic ensure that dir2consul, when run as a Nomad batch job, has access to configuration repos hosted on GitHub?n## Decision Driversn* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.n* There is no appetite for spending money on Service User accounts.n* Option 2 needs a process for creating the ssh key."}
{"File Name":"gatemint-sdk\/adr-020-protobuf-transaction-encoding.md","Context":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nSpecifically, the client-side migration path primarily includes tx generation and\\nsigning, message construction and routing, in addition to CLI & REST handlers and\\nbusiness logic (i.e. queriers).\\nWith this in mind, we will tackle the migration path via two main areas, txs and\\nquerying. However, this ADR solely focuses on transactions. Querying should be\\naddressed in a future ADR, but it should build off of these proposals.\\nBased on detailed discussions ([\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)), the original\\ndesign for transactions was changed substantially from an `oneof` \/JSON-signing\\napproach to the approach described below.\\n","Decision":"### Transactions\\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\\n`sdk.Msg`s are encoding with `Any` in transactions.\\nOne of the main goals of using `Any` to encode interface values is to have a\\ncore set of types which is reused by apps so that\\nclients can safely be compatible with as many chains as possible.\\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\\nformat that can serve a wide variety of use cases without breaking client\\ncompatibility.\\nIn order to facilitate signing, transactions are separated into `TxBody`,\\nwhich will be re-used by `SignDoc` below, and `signatures`:\\n```proto\\n\/\/ types\/types.proto\\npackage cosmos_sdk.v1;\\nmessage Tx {\\nTxBody body = 1;\\nAuthInfo auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\n\/\/ A variant of Tx that pins the signer's exact binary represenation of body and\\n\/\/ auth_info. This is used for signing, broadcasting and verification. The binary\\n\/\/ `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\\n\/\/ becomes the \"txhash\", commonly used as the transaction ID.\\nmessage TxRaw {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in SignDoc.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\\nbytes auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\nmessage TxBody {\\n\/\/ A list of messages to be executed. The required signers of those messages define\\n\/\/ the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\\n\/\/ Each required signer address is added to the list only the first time it occurs.\\n\/\/\\n\/\/ By convention, the first required signer (usually from the first message) is referred\\n\/\/ to as the primary signer and pays the fee for the whole transaction.\\nrepeated google.protobuf.Any messages = 1;\\nstring memo = 2;\\nint64 timeout_height = 3;\\nrepeated google.protobuf.Any extension_options = 1023;\\n}\\nmessage AuthInfo {\\n\/\/ This list defines the signing modes for the required signers. The number\\n\/\/ and order of elements must match the required signers from TxBody's messages.\\n\/\/ The first element is the primary signer and the one which pays the fee.\\nrepeated SignerInfo signer_infos = 1;\\n\/\/ The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\\nFee fee = 2;\\n}\\nmessage SignerInfo {\\n\/\/ The public key is optional for accounts that already exist in state. If unset, the\\n\/\/ verifier can use the required signer address for this position and lookup the public key.\\nPublicKey public_key = 1;\\n\/\/ ModeInfo describes the signing mode of the signer and is a nested\\n\/\/ structure to support nested multisig pubkey's\\nModeInfo mode_info = 2;\\n\/\/ sequence is the sequence of the account, which describes the\\n\/\/ number of committed transactions signed by a given address. It is used to prevent\\n\/\/ replay attacks.\\nuint64 sequence = 3;\\n}\\nmessage ModeInfo {\\noneof sum {\\nSingle single = 1;\\nMulti multi = 2;\\n}\\n\/\/ Single is the mode info for a single signer. It is structured as a message\\n\/\/ to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\\nmessage Single {\\nSignMode mode = 1;\\n}\\n\/\/ Multi is the mode info for a multisig public key\\nmessage Multi {\\n\/\/ bitarray specifies which keys within the multisig are signing\\nCompactBitArray bitarray = 1;\\n\/\/ mode_infos is the corresponding modes of the signers of the multisig\\n\/\/ which could include nested multisig public keys\\nrepeated ModeInfo mode_infos = 2;\\n}\\n}\\nenum SignMode {\\nSIGN_MODE_UNSPECIFIED = 0;\\nSIGN_MODE_DIRECT = 1;\\nSIGN_MODE_TEXTUAL = 2;\\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\\n}\\n```\\nAs will be discussed below, in order to include as much of the `Tx` as possible\\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\\nraw signatures themselves live outside of what is signed over.\\nBecause we are aiming for a flexible, extensible cross-chain transaction\\nformat, new transaction processing options should be added to `TxBody` as soon\\nthose use cases are discovered, even if they can't be implemented yet.\\nBecause there is coordination overhead in this, `TxBody` includes an\\n`extension_options` field which can be used for any transaction processing\\noptions that are not already covered. App developers should, nevertheless,\\nattempt to upstream important improvements to `Tx`.\\n### Signing\\nAll of the signing modes below aim to provide the following guarantees:\\n- **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\\nis signed\\n- **Predictable Gas**: if I am signing a transaction where I am paying a fee,\\nthe final gas is fully dependent on what I am signing\\nThese guarantees give the maximum amount confidence to message signers that\\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\\n#### `SIGN_MODE_DIRECT`\\nThe \"direct\" signing behavior is to sign the raw `TxBody` bytes as broadcast over\\nthe wire. This has the advantages of:\\n- requiring the minimum additional client capabilities beyond a standard protocol\\nbuffers implementation\\n- leaving effectively zero holes for transaction malleability (i.e. there are no\\nsubtle differences between the signing and encoding formats which could\\npotentially be exploited by an attacker)\\nSignatures are structured using the `SignDoc` below which reuses the serialization of\\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\\n```proto\\n\/\/ types\/types.proto\\nmessage SignDoc {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in TxRaw.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\\nbytes auth_info = 2;\\nstring chain_id = 3;\\nuint64 account_number = 4;\\n}\\n```\\nIn order to sign in the default mode, clients take the following steps:\\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\\n2. Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n3. Sign the encoded `SignDoc` bytes.\\n4. Build a `TxRaw` and serialize it for broadcasting.\\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\\nbytes encoded in `TxRaw` not based on any [\"canonicalization\"](https:\/\/github.com\/regen-network\/canonical-proto3)\\nalgorithm which creates added complexity for clients in addition to preventing\\nsome forms of upgradeability (to be addressed later in this document).\\nSignature verifiers do:\\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\\n2. Create a list of required signer addresses from the messages.\\n3. For each required signer:\\n- Pull account number and sequence from the state.\\n- Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\\n- Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n- Verify the signature at the the same list position against the serialized `SignDoc`.\\n#### `SIGN_MODE_LEGACY_AMINO`\\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\\nsupported transaction signing. Once wallets and exchanges have had a\\nchance to upgrade to protobuf based signing, this option will be disabled. In\\nthe meantime, it is foreseen that disabling the current Amino signing would cause\\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\\nCosmos Hub and other chains may choose to disable Amino signing immediately.\\nLegacy clients will be able to sign a transaction using the current Amino\\nJSON format and have it encoded to protobuf using the REST `\/tx\/encode`\\nendpoint before broadcasting.\\n#### `SIGN_MODE_TEXTUAL`\\nAs was discussed extensively in [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078),\\nthere is a desire for a human-readable signing encoding, especially for hardware\\nwallets like the [Ledger](https:\/\/www.ledger.com) which display\\ntransaction contents to users before signing. JSON was an attempt at this but\\nfalls short of the ideal.\\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\\nencoding which will replace Amino JSON. This new encoding should be even more\\nfocused on readability than JSON, possibly based on formatting strings like\\n[MessageFormat](http:\/\/userguide.icu-project.org\/formatparse\/messages).\\nIn order to ensure that the new human-readable format does not suffer from\\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\\nto generate sign bytes.\\nMultiple human-readable formats (maybe even localized messages) may be supported\\nby `SIGN_MODE_TEXTUAL` when it is implemented.\\n### Unknown Field Filtering\\nUnknown fields in protobuf messages should generally be rejected by transaction\\nprocessors because:\\n- important data may be present in the unknown fields, that if ignored, will\\ncause unexpected behavior for clients\\n- they present a malleability vulnerability where attackers can bloat tx size\\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\\nnot `TxBody`)\\nThere are also scenarios where we may choose to safely ignore unknown fields\\n(https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078#issuecomment-624400188) to\\nprovide graceful forwards compatibility with newer clients.\\nWe propose that field numbers with bit 11 set (for most use cases this is\\nthe range of 1024-2047) be considered non-critical fields that can safely be\\nignored if unknown.\\nTo handle this we will need a unknown field filter that:\\n- always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\\nunsigned parts of `AuthInfo` if present based on the signing mode)\\n- rejects unknown fields in all messages (including nested `Any`s) other than\\nfields with bit 11 set\\nThis will likely need to be a custom protobuf parser pass that takes message bytes\\nand `FileDescriptor`s and returns a boolean result.\\n### Public Key Encoding\\nPublic keys in the Cosmos SDK implement Tendermint's `crypto.PubKey` interface,\\nso a natural solution might be to use `Any` as we are doing for other interfaces.\\nThere are, however, a limited number of public keys in existence and new ones\\naren't created overnight. The proposed solution is to use a `oneof` that:\\n- attempts to catalog all known key types even if a given app can't use them all\\n- has an `Any` member that can be used when a key type isn't present in the `oneof`\\nEx:\\n```proto\\nmessage PublicKey {\\noneof sum {\\nbytes secp256k1 = 1;\\nbytes ed25519 = 2;\\n...\\ngoogle.protobuf.Any any_pubkey = 15;\\n}\\n}\\n```\\nApps should only attempt to handle a registered set of public keys that they\\nhave tested. The provided signature verification ante handler decorators will\\nenforce this.\\n### CLI & REST\\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\\nin the client can be interfaces, similar to how we described in [ADR 019](.\/adr-019-protobuf-state-encoding.md),\\nthe client logic will now need to take a codec interface that knows not only how\\nto handle all the types, but also knows how to generate transactions, signatures,\\nand messages.\\n```go\\ntype AccountRetriever interface {\\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\\n}\\ntype Generator interface {\\nNewTx() TxBuilder\\nNewFee() ClientFee\\nNewSignature() ClientSignature\\nMarshalTx(tx types.Tx) ([]byte, error)\\n}\\ntype TxBuilder interface {\\nGetTx() sdk.Tx\\nSetMsgs(...sdk.Msg) error\\nGetSignatures() []sdk.Signature\\nSetSignatures(...sdk.Signature)\\nGetFee() sdk.Fee\\nSetFee(sdk.Fee)\\nGetMemo() string\\nSetMemo(string)\\n}\\n```\\nWe then update `Context` to have new fields: `JSONMarshaler`, `TxGenerator`,\\nand `AccountRetriever`, and we update `AppModuleBasic.GetTxCmd` to take\\na `Context` which should have all of these fields pre-populated.\\nEach client method should then use one of the `Init` methods to re-initialize\\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\\ngenerate or broadcast a transaction. For example:\\n```go\\nimport \"github.com\/spf13\/cobra\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\/tx\"\\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\\nreturn &cobra.Command{\\nRunE: func(cmd *cobra.Command, args []string) error {\\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\\nmsg := NewSomeMsg{...}\\ntx.GenerateOrBroadcastTx(clientCtx, msg)\\n},\\n}\\n}\\n```\\n","tokens":234,"id":21,"text":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nSpecifically, the client-side migration path primarily includes tx generation and\\nsigning, message construction and routing, in addition to CLI & REST handlers and\\nbusiness logic (i.e. queriers).\\nWith this in mind, we will tackle the migration path via two main areas, txs and\\nquerying. However, this ADR solely focuses on transactions. Querying should be\\naddressed in a future ADR, but it should build off of these proposals.\\nBased on detailed discussions ([\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)), the original\\ndesign for transactions was changed substantially from an `oneof` \/JSON-signing\\napproach to the approach described below.\\n\n\n##Decision\n### Transactions\\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\\n`sdk.Msg`s are encoding with `Any` in transactions.\\nOne of the main goals of using `Any` to encode interface values is to have a\\ncore set of types which is reused by apps so that\\nclients can safely be compatible with as many chains as possible.\\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\\nformat that can serve a wide variety of use cases without breaking client\\ncompatibility.\\nIn order to facilitate signing, transactions are separated into `TxBody`,\\nwhich will be re-used by `SignDoc` below, and `signatures`:\\n```proto\\n\/\/ types\/types.proto\\npackage cosmos_sdk.v1;\\nmessage Tx {\\nTxBody body = 1;\\nAuthInfo auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\n\/\/ A variant of Tx that pins the signer's exact binary represenation of body and\\n\/\/ auth_info. This is used for signing, broadcasting and verification. The binary\\n\/\/ `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\\n\/\/ becomes the \"txhash\", commonly used as the transaction ID.\\nmessage TxRaw {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in SignDoc.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\\nbytes auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\nmessage TxBody {\\n\/\/ A list of messages to be executed. The required signers of those messages define\\n\/\/ the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\\n\/\/ Each required signer address is added to the list only the first time it occurs.\\n\/\/\\n\/\/ By convention, the first required signer (usually from the first message) is referred\\n\/\/ to as the primary signer and pays the fee for the whole transaction.\\nrepeated google.protobuf.Any messages = 1;\\nstring memo = 2;\\nint64 timeout_height = 3;\\nrepeated google.protobuf.Any extension_options = 1023;\\n}\\nmessage AuthInfo {\\n\/\/ This list defines the signing modes for the required signers. The number\\n\/\/ and order of elements must match the required signers from TxBody's messages.\\n\/\/ The first element is the primary signer and the one which pays the fee.\\nrepeated SignerInfo signer_infos = 1;\\n\/\/ The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\\nFee fee = 2;\\n}\\nmessage SignerInfo {\\n\/\/ The public key is optional for accounts that already exist in state. If unset, the\\n\/\/ verifier can use the required signer address for this position and lookup the public key.\\nPublicKey public_key = 1;\\n\/\/ ModeInfo describes the signing mode of the signer and is a nested\\n\/\/ structure to support nested multisig pubkey's\\nModeInfo mode_info = 2;\\n\/\/ sequence is the sequence of the account, which describes the\\n\/\/ number of committed transactions signed by a given address. It is used to prevent\\n\/\/ replay attacks.\\nuint64 sequence = 3;\\n}\\nmessage ModeInfo {\\noneof sum {\\nSingle single = 1;\\nMulti multi = 2;\\n}\\n\/\/ Single is the mode info for a single signer. It is structured as a message\\n\/\/ to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\\nmessage Single {\\nSignMode mode = 1;\\n}\\n\/\/ Multi is the mode info for a multisig public key\\nmessage Multi {\\n\/\/ bitarray specifies which keys within the multisig are signing\\nCompactBitArray bitarray = 1;\\n\/\/ mode_infos is the corresponding modes of the signers of the multisig\\n\/\/ which could include nested multisig public keys\\nrepeated ModeInfo mode_infos = 2;\\n}\\n}\\nenum SignMode {\\nSIGN_MODE_UNSPECIFIED = 0;\\nSIGN_MODE_DIRECT = 1;\\nSIGN_MODE_TEXTUAL = 2;\\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\\n}\\n```\\nAs will be discussed below, in order to include as much of the `Tx` as possible\\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\\nraw signatures themselves live outside of what is signed over.\\nBecause we are aiming for a flexible, extensible cross-chain transaction\\nformat, new transaction processing options should be added to `TxBody` as soon\\nthose use cases are discovered, even if they can't be implemented yet.\\nBecause there is coordination overhead in this, `TxBody` includes an\\n`extension_options` field which can be used for any transaction processing\\noptions that are not already covered. App developers should, nevertheless,\\nattempt to upstream important improvements to `Tx`.\\n### Signing\\nAll of the signing modes below aim to provide the following guarantees:\\n- **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\\nis signed\\n- **Predictable Gas**: if I am signing a transaction where I am paying a fee,\\nthe final gas is fully dependent on what I am signing\\nThese guarantees give the maximum amount confidence to message signers that\\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\\n#### `SIGN_MODE_DIRECT`\\nThe \"direct\" signing behavior is to sign the raw `TxBody` bytes as broadcast over\\nthe wire. This has the advantages of:\\n- requiring the minimum additional client capabilities beyond a standard protocol\\nbuffers implementation\\n- leaving effectively zero holes for transaction malleability (i.e. there are no\\nsubtle differences between the signing and encoding formats which could\\npotentially be exploited by an attacker)\\nSignatures are structured using the `SignDoc` below which reuses the serialization of\\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\\n```proto\\n\/\/ types\/types.proto\\nmessage SignDoc {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in TxRaw.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\\nbytes auth_info = 2;\\nstring chain_id = 3;\\nuint64 account_number = 4;\\n}\\n```\\nIn order to sign in the default mode, clients take the following steps:\\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\\n2. Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n3. Sign the encoded `SignDoc` bytes.\\n4. Build a `TxRaw` and serialize it for broadcasting.\\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\\nbytes encoded in `TxRaw` not based on any [\"canonicalization\"](https:\/\/github.com\/regen-network\/canonical-proto3)\\nalgorithm which creates added complexity for clients in addition to preventing\\nsome forms of upgradeability (to be addressed later in this document).\\nSignature verifiers do:\\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\\n2. Create a list of required signer addresses from the messages.\\n3. For each required signer:\\n- Pull account number and sequence from the state.\\n- Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\\n- Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n- Verify the signature at the the same list position against the serialized `SignDoc`.\\n#### `SIGN_MODE_LEGACY_AMINO`\\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\\nsupported transaction signing. Once wallets and exchanges have had a\\nchance to upgrade to protobuf based signing, this option will be disabled. In\\nthe meantime, it is foreseen that disabling the current Amino signing would cause\\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\\nCosmos Hub and other chains may choose to disable Amino signing immediately.\\nLegacy clients will be able to sign a transaction using the current Amino\\nJSON format and have it encoded to protobuf using the REST `\/tx\/encode`\\nendpoint before broadcasting.\\n#### `SIGN_MODE_TEXTUAL`\\nAs was discussed extensively in [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078),\\nthere is a desire for a human-readable signing encoding, especially for hardware\\nwallets like the [Ledger](https:\/\/www.ledger.com) which display\\ntransaction contents to users before signing. JSON was an attempt at this but\\nfalls short of the ideal.\\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\\nencoding which will replace Amino JSON. This new encoding should be even more\\nfocused on readability than JSON, possibly based on formatting strings like\\n[MessageFormat](http:\/\/userguide.icu-project.org\/formatparse\/messages).\\nIn order to ensure that the new human-readable format does not suffer from\\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\\nto generate sign bytes.\\nMultiple human-readable formats (maybe even localized messages) may be supported\\nby `SIGN_MODE_TEXTUAL` when it is implemented.\\n### Unknown Field Filtering\\nUnknown fields in protobuf messages should generally be rejected by transaction\\nprocessors because:\\n- important data may be present in the unknown fields, that if ignored, will\\ncause unexpected behavior for clients\\n- they present a malleability vulnerability where attackers can bloat tx size\\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\\nnot `TxBody`)\\nThere are also scenarios where we may choose to safely ignore unknown fields\\n(https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078#issuecomment-624400188) to\\nprovide graceful forwards compatibility with newer clients.\\nWe propose that field numbers with bit 11 set (for most use cases this is\\nthe range of 1024-2047) be considered non-critical fields that can safely be\\nignored if unknown.\\nTo handle this we will need a unknown field filter that:\\n- always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\\nunsigned parts of `AuthInfo` if present based on the signing mode)\\n- rejects unknown fields in all messages (including nested `Any`s) other than\\nfields with bit 11 set\\nThis will likely need to be a custom protobuf parser pass that takes message bytes\\nand `FileDescriptor`s and returns a boolean result.\\n### Public Key Encoding\\nPublic keys in the Cosmos SDK implement Tendermint's `crypto.PubKey` interface,\\nso a natural solution might be to use `Any` as we are doing for other interfaces.\\nThere are, however, a limited number of public keys in existence and new ones\\naren't created overnight. The proposed solution is to use a `oneof` that:\\n- attempts to catalog all known key types even if a given app can't use them all\\n- has an `Any` member that can be used when a key type isn't present in the `oneof`\\nEx:\\n```proto\\nmessage PublicKey {\\noneof sum {\\nbytes secp256k1 = 1;\\nbytes ed25519 = 2;\\n...\\ngoogle.protobuf.Any any_pubkey = 15;\\n}\\n}\\n```\\nApps should only attempt to handle a registered set of public keys that they\\nhave tested. The provided signature verification ante handler decorators will\\nenforce this.\\n### CLI & REST\\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\\nin the client can be interfaces, similar to how we described in [ADR 019](.\/adr-019-protobuf-state-encoding.md),\\nthe client logic will now need to take a codec interface that knows not only how\\nto handle all the types, but also knows how to generate transactions, signatures,\\nand messages.\\n```go\\ntype AccountRetriever interface {\\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\\n}\\ntype Generator interface {\\nNewTx() TxBuilder\\nNewFee() ClientFee\\nNewSignature() ClientSignature\\nMarshalTx(tx types.Tx) ([]byte, error)\\n}\\ntype TxBuilder interface {\\nGetTx() sdk.Tx\\nSetMsgs(...sdk.Msg) error\\nGetSignatures() []sdk.Signature\\nSetSignatures(...sdk.Signature)\\nGetFee() sdk.Fee\\nSetFee(sdk.Fee)\\nGetMemo() string\\nSetMemo(string)\\n}\\n```\\nWe then update `Context` to have new fields: `JSONMarshaler`, `TxGenerator`,\\nand `AccountRetriever`, and we update `AppModuleBasic.GetTxCmd` to take\\na `Context` which should have all of these fields pre-populated.\\nEach client method should then use one of the `Init` methods to re-initialize\\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\\ngenerate or broadcast a transaction. For example:\\n```go\\nimport \"github.com\/spf13\/cobra\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\/tx\"\\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\\nreturn &cobra.Command{\\nRunE: func(cmd *cobra.Command, args []string) error {\\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\\nmsg := NewSomeMsg{...}\\ntx.GenerateOrBroadcastTx(clientCtx, msg)\\n},\\n}\\n}\\n```\\n","Predictions":"## ContextnIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse\/Panoptes-Front-End#4992] (https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/issues\/4992)).nThe default behavior in existing libraries like [JSON8](https:\/\/github.com\/sonnyp\/JSON8\/tree\/master\/packages\/json8#ooserialize) or MobX's [toJS](https:\/\/mobx.js.org\/refguide\/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.n ## Decision We decided to instead implement our own [type conversion utility function](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/store\/utils\/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:n jsn"}
{"File Name":"gsp\/ADR028-container-tools.md","Context":"## Context\\nWe are currently using Docker as our container runtime.\\nThere are needs for public docker images:\\n* so that master builds can be easily pulled and tested in the local development environments\\n* so that images can be easily shared between different teams\\nThere are needs for digitally verifying the provenance of images:\\n* so that production systems can trust that an image has not been tampered with\\n* so that production systems can authenticate the origin of a build\\nThere are needs for vulnerability scanning:\\n* so that production systems can warn or prevent exploitable software running in production\\nThe docker ecosystem provides tooling that can help us meet these needs:\\n* [Docker Content Trust] (Notary) can be used to sign images and prove provenance\\n* [Docker Registries][Docker Registry] can expose images publicly\\n* Scanning tools like [Clair] can periodically or at pull\/push time perform CVE scanning.\\nUnfortunately AWS [ECR] does not currently support public images or [Docker Content Trust], and there is no managed solution to image scanning from AWS as yet.\\n### Option 1: Wrap AWS ECR\\nWe could write\/manage a proxy to allow exposing [ECR] publicly and integrate the missing features.\\n* Potentially fragile implementation tied to the underlying AWS services\\n* Another thing to have to maintain\\n* Might offer ability to remove functionality as AWS support more features in future\\n### Option 2: Use an external SaaS offering\\nUse a SaaS service like [Quay] which offers most of these features.\\n* Additional configuration for cluster to pull from external source\\n* Reduces ability to automate provisioning (requires additional credential management)\\n* Procurement\\n### Option 3: Self-hosted Docker Tools in cluster\\nWe could deploy Docker Distribution, Notary & Clair into the cluster backed by a managed storage backend like S3\\n* Well integrated with the platform\\n* Would work for local GSP instance\\n","Decision":"We will run a self hosted set of Docker tools\\n","tokens":406,"id":3917,"text":"## Context\\nWe are currently using Docker as our container runtime.\\nThere are needs for public docker images:\\n* so that master builds can be easily pulled and tested in the local development environments\\n* so that images can be easily shared between different teams\\nThere are needs for digitally verifying the provenance of images:\\n* so that production systems can trust that an image has not been tampered with\\n* so that production systems can authenticate the origin of a build\\nThere are needs for vulnerability scanning:\\n* so that production systems can warn or prevent exploitable software running in production\\nThe docker ecosystem provides tooling that can help us meet these needs:\\n* [Docker Content Trust] (Notary) can be used to sign images and prove provenance\\n* [Docker Registries][Docker Registry] can expose images publicly\\n* Scanning tools like [Clair] can periodically or at pull\/push time perform CVE scanning.\\nUnfortunately AWS [ECR] does not currently support public images or [Docker Content Trust], and there is no managed solution to image scanning from AWS as yet.\\n### Option 1: Wrap AWS ECR\\nWe could write\/manage a proxy to allow exposing [ECR] publicly and integrate the missing features.\\n* Potentially fragile implementation tied to the underlying AWS services\\n* Another thing to have to maintain\\n* Might offer ability to remove functionality as AWS support more features in future\\n### Option 2: Use an external SaaS offering\\nUse a SaaS service like [Quay] which offers most of these features.\\n* Additional configuration for cluster to pull from external source\\n* Reduces ability to automate provisioning (requires additional credential management)\\n* Procurement\\n### Option 3: Self-hosted Docker Tools in cluster\\nWe could deploy Docker Distribution, Notary & Clair into the cluster backed by a managed storage backend like S3\\n* Well integrated with the platform\\n* Would work for local GSP instance\\n\n\n##Decision\nWe will run a self hosted set of Docker tools\\n","Predictions":"## ContextnAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) andnCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,netc.) use the APIs to manage or access IaaS resources.nThe most common mechanism for authenticating the API calls is to create annIdentify and Access Management (IAM) user with the appropriate permissions,ngenerate an Access Key ID and Secret Access Key for that user, and exportnthose as environment variables. AWS_ACCESS_KEY_ID andnAWS_SECRET_ACCESS_KEY are the standard environment variable names used bynmost utilities and libraries.nThe problem with this approach is that it's very easy to accidentally leaknthe plain text keys. They can appear in output from your shell, which younmight copy+paste into a gist or email when debugging a problem. You mightnadd them to your shell configuration or include them in a script, which cannbe pushed to a public code repository.nOur team have leaked keys like this on more than one occasion. It's worthnnoting that even if you realise that you've done this, delete the commit andnrevoke the keys, they may have already been used maliciously becausenautomated bots monitor sites like GitHub using the [events firehose][] tondetect any credentials.n[events firehose]: https:\/\/developer.github.com\/v3\/activity\/events\/nAs an alternative to using pre-generated keys, AWS recommends that you usen[IAM roles and instance profiles][] when accessing the API from EC2ninstances. You delegate permissions to the EC2 instance and temporaryncredentials are made available from the instance metadata service. Mostntools and libraries automatically support this. The credentials arenregularly rotated and never need to be stored in configuration files.n"}
{"File Name":"front-end-monorepo\/adr-21.md","Context":"## Context\\nAuthentication is currently handled by the existing auth client, which is bundled up as part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/).\\nWhile working on [#1306](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306), I ran into a few issues with it:\\n- Environment variables (env vars) aren't available on the client side. Next.js does have a method for sharing config on both the server and client, but it's academic since:\\n- The only way to configure `panoptes-javascript-client` is _directly_ via env vars, or by passing in query parameters to the URL. We can't get env vars on the client, so that's out, and query parameters become unwieldy very quickly.\\nThis hasn't been an issue when working with Single Page Apps, since env vars are baked in at transpilation time by the build tools. At the moment, we just avoid the problem by having the staging build of the project use the production API.\\nHowever, for the Next.js-based apps we're building right now, we need a different approach, and being able to configure the auth client from a single source of truth is required. Ideally, this would be source-agnostic: the client should be able to be configured from a config file, [env vars](https:\/\/12factor.net\/config), or whatever you want, but that's up to the consuming app to decide.\\n","Decision":"Rewrite the existing auth client as a separate package. The new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.\\nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.\\nThe [existing config](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/blob\/master\/lib\/config.js) will be turned into a separate package. Config settings can be imported wholesale from that for convenience.\\nFor use in the rebuild apps, we would create a `ConfigStore` which we populate with the relevant config settings. The `ConfigStore` snapshot is then used to rehydrate the client, so we get a single source of truth on both client and server.\\n","tokens":315,"id":527,"text":"## Context\\nAuthentication is currently handled by the existing auth client, which is bundled up as part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/).\\nWhile working on [#1306](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306), I ran into a few issues with it:\\n- Environment variables (env vars) aren't available on the client side. Next.js does have a method for sharing config on both the server and client, but it's academic since:\\n- The only way to configure `panoptes-javascript-client` is _directly_ via env vars, or by passing in query parameters to the URL. We can't get env vars on the client, so that's out, and query parameters become unwieldy very quickly.\\nThis hasn't been an issue when working with Single Page Apps, since env vars are baked in at transpilation time by the build tools. At the moment, we just avoid the problem by having the staging build of the project use the production API.\\nHowever, for the Next.js-based apps we're building right now, we need a different approach, and being able to configure the auth client from a single source of truth is required. Ideally, this would be source-agnostic: the client should be able to be configured from a config file, [env vars](https:\/\/12factor.net\/config), or whatever you want, but that's up to the consuming app to decide.\\n\n\n##Decision\nRewrite the existing auth client as a separate package. The new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.\\nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.\\nThe [existing config](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/blob\/master\/lib\/config.js) will be turned into a separate package. Config settings can be imported wholesale from that for convenience.\\nFor use in the rebuild apps, we would create a `ConfigStore` which we populate with the relevant config settings. The `ConfigStore` snapshot is then used to rehydrate the client, so we get a single source of truth on both client and server.\\n","Predictions":"## ContextnIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse\/Panoptes-Front-End#4992] (https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/issues\/4992)).nThe default behavior in existing libraries like [JSON8](https:\/\/github.com\/sonnyp\/JSON8\/tree\/master\/packages\/json8#ooserialize) or MobX's [toJS](https:\/\/mobx.js.org\/refguide\/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.n ## Decision We decided to instead implement our own [type conversion utility function](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/store\/utils\/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:n jsn"}
{"File Name":"openfido-app-service\/0002-project-structure.md","Context":"## Context\\nThe OpenFIDO project will mediate access to a workflow service that runs\\nGridLabD jobs. The frontend React application will need access to these\\nresources, but only if they have permission to access via an organization.\\n","Decision":"Create a Flask Rest service - since the other microservices in this suite of\\nservices are also Flask based, keep this using the same kind of infrastructure\\nso that common utilities can be shared (openfido-utils) and the maintenance will\\nbe simplified.\\nOrganize the database logic into a simplified CQRS-inspired style code\\nstructure. Since we anticipate many conceptual resources, each resource will\\nhave its own module:\\n* app\/RESOURCE\/models.py - contains all models for RESOURCE.\\n* app\/RESOURCE\/routes.py - contains all Rest routes specific to RESOURCE.\\n* app\/RESOURCE\/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\\n* app\/RESOURCE\/services.py - all db commands that modify database state.\\n* app\/RESOURCE\/queries.py - all db queries to the database.\\nAdditional libraries we anticipate using:\\n* [marshmallow](https:\/\/marshmallow.readthedocs.io\/en\/stable\/) will be used since there are many rest api endpoints that will take nested bodies.\\n* [alembic](https:\/\/alembic.sqlalchemy.org\/en\/latest\/) to manage database schema migrations.\\nOther thoughts on design:\\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\\nUse the structure of the [openfido-workflow-service](https:\/\/github.com\/slacgismo\/openfido-workflow-service) project as a reference.\\n","tokens":49,"id":2470,"text":"## Context\\nThe OpenFIDO project will mediate access to a workflow service that runs\\nGridLabD jobs. The frontend React application will need access to these\\nresources, but only if they have permission to access via an organization.\\n\n\n##Decision\nCreate a Flask Rest service - since the other microservices in this suite of\\nservices are also Flask based, keep this using the same kind of infrastructure\\nso that common utilities can be shared (openfido-utils) and the maintenance will\\nbe simplified.\\nOrganize the database logic into a simplified CQRS-inspired style code\\nstructure. Since we anticipate many conceptual resources, each resource will\\nhave its own module:\\n* app\/RESOURCE\/models.py - contains all models for RESOURCE.\\n* app\/RESOURCE\/routes.py - contains all Rest routes specific to RESOURCE.\\n* app\/RESOURCE\/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\\n* app\/RESOURCE\/services.py - all db commands that modify database state.\\n* app\/RESOURCE\/queries.py - all db queries to the database.\\nAdditional libraries we anticipate using:\\n* [marshmallow](https:\/\/marshmallow.readthedocs.io\/en\/stable\/) will be used since there are many rest api endpoints that will take nested bodies.\\n* [alembic](https:\/\/alembic.sqlalchemy.org\/en\/latest\/) to manage database schema migrations.\\nOther thoughts on design:\\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\\nUse the structure of the [openfido-workflow-service](https:\/\/github.com\/slacgismo\/openfido-workflow-service) project as a reference.\\n","Predictions":"We will support IndexedDB as a data store layer, sitting between the application and the API.n # Context and Problem StatementnHow should the application be structured into its main components? Thengoal is to be able to have multiple rest servers\/webapps and multiplendocument processor components working togehter.n# Decision OutcomenThe following are the \"main\" modules. There may be more helper modulesnand libraries that support implementing a feature.n ## ContextnThe OpenFIDO project will mediate access to a workflow service that runsnGridLabD jobs. The frontend React application will need access to thesenresources, but only if they have permission to access via an organization.n# Decision ## ContextnThe OpenFIDO project will mediate access to a workflow service that runsnGridLabD jobs. The frontend React application will need access to thesenresources, but only if they have permission to access via an organization.n # Context and Problem StatementnHow should the application be structured into its main components? Thengoal is to be able to have multiple rest servers\/webapps and multiplendocument processor components working togehter.n# Decision OutcomenThe following are the \"main\" modules. There may be more helper modulesnand libraries that support implementing a feature.n ## ContextnThe OpenFIDO project will mediate access to a workflow service that runsnGridLabD jobs. The frontend React application will need access to thesenresources, but only if they have permission to access via an organization.n# Decision ## ContextnThe OpenFIDO project will mediate access to a workflow service that runsnGridLabD jobs. The frontend React application will need access to thesenresources, but only if they have permission to access via an organization.n # Context and Problem StatementnHow should the application be structured into its main components? Thengoal is to be able to have multiple rest servers\/webapps and multiplendocument processor components working togehter.n# Decision OutcomenThe following are the \"main\" modules. There may be more helper modulesnand libraries that support implementing a feature.n ## ContextnThe OpenFIDO project will mediate access to a workflow service that runsnGridLabD jobs. The frontend React application will need access to thesenresources, but only if they have permission to access via an organization.n# Decision ## ContextnThe OpenFIDO project will mediate access to a workflow service that runsnGridLabD jobs. The frontend React application will need access to thesenresources, but only if they have permission to access via an organization.n # Context and Problem StatementnHow should the application be structured into its main components? Thengoal is to be able to have multiple rest servers\/webapps and multiplendocument processor components working togehter.n# Decision OutcomenThe following are the \"main\" modules. There may be more helper modulesnand libraries that support implementing a feature.n ## ContextnThe OpenFIDO project will mediate access to a workflow service that runsnGridLabD jobs. The frontend React application will need access to thesenresources, but only if they have permission to access via an organization.n# Decision ## ContextnThe OpenFIDO project will mediate access to a workflow service that runsnGridLabD jobs. The frontend React application will need access to thesenresources, but only if they have permission to access via an organization.n # Context and Problem StatementnHow should the application be structured into its main components? Thengoal is to be able to have multiple rest servers\/webapps and multiplendocument processor components working togehter.n# Decision OutcomenThe following are the \"main\" modules. There may be more helper modulesnand libraries that support implementing a feature.n ## ContextnThe OpenFIDO project will mediate access to "}
{"File Name":"winery\/0027-use-dasherization-for-filenames.md","Context":"## Context and Problem Statement\\nGraphics files have to take a consistent file name\\n## Decision Drivers <!-- optional -->\\n* Easy to process by Jekyll\\n* No WTFs at the creators\\n","Decision":"* Easy to process by Jekyll\\n* No WTFs at the creators\\nChosen option: \"Dasherization\", because\\n* clear separation of parts of the name\\n* consistent to other URLs (which are typically lowercase)\\n","tokens":40,"id":4308,"text":"## Context and Problem Statement\\nGraphics files have to take a consistent file name\\n## Decision Drivers <!-- optional -->\\n* Easy to process by Jekyll\\n* No WTFs at the creators\\n\n\n##Decision\n* Easy to process by Jekyll\\n* No WTFs at the creators\\nChosen option: \"Dasherization\", because\\n* clear separation of parts of the name\\n* consistent to other URLs (which are typically lowercase)\\n","Predictions":"Graphics files have to take a consistent file namen## Decision"}
