{"File Name":"libelektra\/ingroup_removal.md","Context":"## Problem\nCurrently, an error or warning message in elektra causes the following line to be shown:\n```\nIngroup: <group>\n```\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\n","Decision":"The `ingroup` message will be removed as it does not yield any notable benefit.\\nSee [Error concept](error_codes.md)\\n","tokens":83,"id":1308,"text":"## Problem\\nCurrently, an error or warning message in elektra causes the following line to be shown:\\n```\\nIngroup: <group>\\n```\\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\\n\n\n##Decision\nThe `ingroup` message will be removed as it does not yield any notable benefit.\\nSee [Error concept](error_codes.md)\\n","Predictions":"The ingroup message will be removed as it does not yield any notable benefit.nSee [Error concept](error_codes.md)n"}
{"File Name":"knot-documentation\/ARC-003-amqp-exc-name.md","Context":"## Context\nAs defined by CloudAMQP Blog, \"exchanges are message routing agents, defined by the virtual host within RabbitMQ. An exchange is responsible for routing the messages to different queues with the help of header attributes, bindings, and routing keys.\"\nThe Exchange name needs to be defined taking into account the architecture defined for the system. Thus, there will be a client that communicates with BabelTower, which has the Connector as one of its services (in this case BabelTower is the Connector client) or the client communicates directly with the Connector.\nThe goal is to define names that make it clear what this Exchange is used for.\n","Decision":"* The communication between two components will be done with two queues, where one component is consumer in one and producer in another and the other component follows the opposite.\\n* The reference for choosing the queue name will be the client.\\n* The queue where BabelTower publishes messages will be called 'fogOut'.\\n* The queue where BabelTower consumes messages will be called 'fogIn'.\\n* The queue where Connector publishes messages will be called 'connOut'.\\n* The queue where Connector consumes messages will be called 'connIn'.\\n","tokens":131,"id":3339,"text":"## Context\\nAs defined by CloudAMQP Blog, \"exchanges are message routing agents, defined by the virtual host within RabbitMQ. An exchange is responsible for routing the messages to different queues with the help of header attributes, bindings, and routing keys.\"\\nThe Exchange name needs to be defined taking into account the architecture defined for the system. Thus, there will be a client that communicates with BabelTower, which has the Connector as one of its services (in this case BabelTower is the Connector client) or the client communicates directly with the Connector.\\nThe goal is to define names that make it clear what this Exchange is used for.\\n\n\n##Decision\n* The communication between two components will be done with two queues, where one component is consumer in one and producer in another and the other component follows the opposite.\\n* The reference for choosing the queue name will be the client.\\n* The queue where BabelTower publishes messages will be called 'fogOut'.\\n* The queue where BabelTower consumes messages will be called 'fogIn'.\\n* The queue where Connector publishes messages will be called 'connOut'.\\n* The queue where Connector consumes messages will be called 'connIn'.\\n","Predictions":"* The communication between two components will be done with two queues, where one component is consumer in one and producer in another and the other component follows the opposite.n* The reference for choosing the queue name will be the client.n* The queue where BabelTower publishes messages will be called 'fogOut'.n* The queue where BabelTower consumes messages will be called 'fogIn'.n* The queue where Connector publishes messages will be called 'connOut'.n* The queue where Connector consumes messages will be called 'connIn'.n"}
{"File Name":"decodeweb\/0003-store-configuration-into-local-storage.md","Context":"## Context\nTo provide the required functionality for the pilot we need to store some\nconfiguration in a persistent way while just operating within a browser\nenvironment. This state could be written to a backend database, or we could\njust store into localstorage.\n","Decision":"We will store the user's configuration into localstorage protected by a PIN\\ncode.\\n### Proposed storage structure\\n```json\\n{\\n\"1234\": {\\n\"keypair\": {},\\n\"blindSignature\": {},\\n\"devices\": {\\n\"abc123\": {\\n\"longitude\": 12.2,\\n\"latitude\": 52.4,\\n\"exposure\": \"INDOOR\",\\n\"communities\": {\\n\"2a745dac-015a-4cde-a348-75bccc21502b\": {\\n\"authorizable_attribute_id\": \"6a1ba9b1-3f20-4e12-ae65-154c5eb7e00f\",\\n\"credential\": {},\\n\"blindproofCredential\": {}\\n}\\n}\\n}\\n}\\n}\\n}\\n```\\nPlease see `.\/localstorage.json` for a complete example including zenroom credentials.\\n","tokens":52,"id":2221,"text":"## Context\\nTo provide the required functionality for the pilot we need to store some\\nconfiguration in a persistent way while just operating within a browser\\nenvironment. This state could be written to a backend database, or we could\\njust store into localstorage.\\n\n\n##Decision\nWe will store the user's configuration into localstorage protected by a PIN\\ncode.\\n### Proposed storage structure\\n```json\\n{\\n\"1234\": {\\n\"keypair\": {},\\n\"blindSignature\": {},\\n\"devices\": {\\n\"abc123\": {\\n\"longitude\": 12.2,\\n\"latitude\": 52.4,\\n\"exposure\": \"INDOOR\",\\n\"communities\": {\\n\"2a745dac-015a-4cde-a348-75bccc21502b\": {\\n\"authorizable_attribute_id\": \"6a1ba9b1-3f20-4e12-ae65-154c5eb7e00f\",\\n\"credential\": {},\\n\"blindproofCredential\": {}\\n}\\n}\\n}\\n}\\n}\\n}\\n```\\nPlease see `.\/localstorage.json` for a complete example including zenroom credentials.\\n","Predictions":"We will use a stack library to store the user's configuration.n"}
{"File Name":"holochain-rust\/0006-splitting-agent-into-front-house-back-house-or-not.md","Context":"## Context\nFor Holo, we need to have user agent's keys and source chain on the client machine and the rest of the Holochain agent (the DHT shard etc.) be held by HoloPorts.\nIn February 2018, Arthur, Eric and Nico discussed this during a co-creative session in London and made the assumption to have the Holochain agent be split up into two pieces, called front-house and back-house (prior left and right hemisphere). The front-house was meant to entail the source chain and private key management as well as a ribosome to run the app and provide interfaces for the UI and in the non-Holo case for bridging - everything that is user\/agent facing. The back-house should have been the DHT shard, routing table, metrics, etc. Basically everything that is network facing.\nWith this separation, the reasoning in February was to rewrite (only) the front-house in JS to be able to run this piece in a web browser, as needed for Holo.\nEric and Nico continued to map out the specifics of how these two modules would interface. In that process it became apparent that dividing an agent in these two pieces and have them run on different remote machines has a huge problem:\n* every network communication (including the world model which happens on an ongoing basis) that the back-house is doing has to be signed with the agent's keys\n* the agent's keys are by definition part of the front-house\n* **-> the back-house can't live up to its main accountability without communicating with the front-house and requesting a signature for every packet of communication that might be even triggered from the network\/DHT while the user is offline and keys not accessible**\nFurther conversation including Arthur on May 17th 2018 makes it clear that thinking in terms of two different kinds of agency seems appropriate. We discussed separating the authoring or creative agency from the part that runs validations and holds DHT shards and world model information, and allows the later to proxy for the former, **with separate keys**.\n","Decision":"We decide to **not** emulate a single agency (as represented by one key) across two remote devices. In other words: we decide to not try to implement distributed agents.\\nInstead, we solve the initial Holo front-end problem by establishing **two sub-agencies with distinct keys**, where the Holo client's authoring agency explicitly grants proxy rights to a HoloPort's DHT agency.\\nIn other words: the Holo user uses their local key to sign a statement that grants another agent (the HoloPort) to act on their behalf for all the cases needed to have the HoloPort carry the DHT weight for this agent. But technically, it is another agent with its own key.\\n","tokens":423,"id":1500,"text":"## Context\\nFor Holo, we need to have user agent's keys and source chain on the client machine and the rest of the Holochain agent (the DHT shard etc.) be held by HoloPorts.\\nIn February 2018, Arthur, Eric and Nico discussed this during a co-creative session in London and made the assumption to have the Holochain agent be split up into two pieces, called front-house and back-house (prior left and right hemisphere). The front-house was meant to entail the source chain and private key management as well as a ribosome to run the app and provide interfaces for the UI and in the non-Holo case for bridging - everything that is user\/agent facing. The back-house should have been the DHT shard, routing table, metrics, etc. Basically everything that is network facing.\\nWith this separation, the reasoning in February was to rewrite (only) the front-house in JS to be able to run this piece in a web browser, as needed for Holo.\\nEric and Nico continued to map out the specifics of how these two modules would interface. In that process it became apparent that dividing an agent in these two pieces and have them run on different remote machines has a huge problem:\\n* every network communication (including the world model which happens on an ongoing basis) that the back-house is doing has to be signed with the agent's keys\\n* the agent's keys are by definition part of the front-house\\n* **-> the back-house can't live up to its main accountability without communicating with the front-house and requesting a signature for every packet of communication that might be even triggered from the network\/DHT while the user is offline and keys not accessible**\\nFurther conversation including Arthur on May 17th 2018 makes it clear that thinking in terms of two different kinds of agency seems appropriate. We discussed separating the authoring or creative agency from the part that runs validations and holds DHT shards and world model information, and allows the later to proxy for the former, **with separate keys**.\\n\n\n##Decision\nWe decide to **not** emulate a single agency (as represented by one key) across two remote devices. In other words: we decide to not try to implement distributed agents.\\nInstead, we solve the initial Holo front-end problem by establishing **two sub-agencies with distinct keys**, where the Holo client's authoring agency explicitly grants proxy rights to a HoloPort's DHT agency.\\nIn other words: the Holo user uses their local key to sign a statement that grants another agent (the HoloPort) to act on their behalf for all the cases needed to have the HoloPort carry the DHT weight for this agent. But technically, it is another agent with its own key.\\n","Predictions":"We decide to **not** emulate a single agency (as represented by one key) across two remote machines.n"}
{"File Name":"raster-foundry\/adr-0006-workflow-manager.md","Context":"## Context\nThis ADR has been superseded by `ADR-0019`.\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\ntools it seemed prudent to explore additional options as well.\n| Workflow Tool   | Pros | Cons |\n|-----------------|------|------|\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\n","Decision":"Raster Foundry will use Airflow as a task manager. There are a number of advantages over some of the\\nalternatives. First, Airflow has a large, active, user base that have used it in production. The\\nproject itself is in the Apache incubator, providing a strong signal that the project is of high quality\\nSecond, Airflow's UI for monitoring task and workflow progress is great. It provides\\nhigh-level relevant information that will enable us to diagnose issues quickly. Additionally, it\\nprovides a means to view log output of tasks directly in the admin interface. Third, Airflow\\nsupports both scheduled and ad hoc tasks. Lastly, Airflow's architecture would re-use many\\ncomponents that will already be a part of Raster Foundry's infrastructure - including a Postgres\\ndatabase and a redis cache.\\n","tokens":353,"id":1688,"text":"## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n\n\n##Decision\nRaster Foundry will use Airflow as a task manager. There are a number of advantages over some of the\\nalternatives. First, Airflow has a large, active, user base that have used it in production. The\\nproject itself is in the Apache incubator, providing a strong signal that the project is of high quality\\nSecond, Airflow's UI for monitoring task and workflow progress is great. It provides\\nhigh-level relevant information that will enable us to diagnose issues quickly. Additionally, it\\nprovides a means to view log output of tasks directly in the admin interface. Third, Airflow\\nsupports both scheduled and ad hoc tasks. Lastly, Airflow's architecture would re-use many\\ncomponents that will already be a part of Raster Foundry's infrastructure - including a Postgres\\ndatabase and a redis cache.\\n","Predictions":"------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"}
{"File Name":"occurrent\/0001-mongodb-database-structure.md","Context":"## Context\nWe need to record events in MongoDB in a specific structure\/schema.\n","Decision":"The [CloudEvents](https:\/\/cloudevents.io\/) are persisted like this in the \"events collection\" in the database (1):\\n```json\\n{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"dataschema\" : \"http:\/\/someschema.com\/schema.json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n},\\n\"streamid\" : \"streamid\"\\n}\\n```\\nNote that \"streamid\" is added as an extension by the MongoDB event stores in order to read all events for a particular stream.\\nIf stream consistency is enabled then another collection, the \"stream consistency\" collection is also written to the database (2):\\n```json\\n{\\n\"_id\" : \"streamid\",\\n\"version\" : 1\\n}\\n```\\nWhen appending cloud events to the stream the consistency of the stream is maintained by comparing the version supplied by the user\\nwith the version present in (2). If they don't match then the cloud events are not written. Also if there are two threads writing to the same\\nstream at once then one of them will run into an error which means it has to retry (optimistic locking). For this to work, transactions are required!\\nAnother previous approach was instead to store the events like this:\\n```json\\n{\\n\"_id\": \"streamid\",\\n\"version\" : 1,\\n\"events\": [{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n}\\n}]\\n}\\n```\\nI.e. the events were stored inside a single document. While there are several benefits of using this approach, such as:\\n1. No transactions required, just do;\\n```java\\neventCollection.updateOne(and(eq(\"_id\", streamId), eq(\"version\", expectedStreamVersion)),\\ncombine(pushEach(\"events\", serializedEvents), set(\"version\", expectedStreamVersion + 1)),\\nnew UpdateOptions().upsert(true));\\n```\\n1. Reads could be done in a streaming fashion even though the events were stored as a subarray using aggregations\\n1. Subscriptions could take a `List<CloudEvent>`, i.e. all events written in the same transaction to the event store.\\nWhen not using the approach subscriptions gets notified once for each event and the consumer needs to reassemble\\nthe \"transaction\" somehow. This is a major drawback when not using this approach.\\nThere are however two major drawbacks that lead to not using this approach:\\n1. There's 16Mb document size limit in MongoDB so this approach wouldn't work for large streams\\n1. It's much hard to implement queries\/filters for subscriptions. The aggregation support is\\n[limited](https:\/\/stackoverflow.com\/questions\/62846085\/remove-element-from-subarray-using-an-aggregation-stage-applied-to-a-change-stre)\\nwhen working with subscriptions preventing simple filters (it would have been much simpler if `unwind`\\nwas supported since then we could flatten out the `events` subarray before applying the queries, i.e. something like\\n`(unwind(\"$events\"), replaceRoot(\"$events\"), match(filter.apply(\"type\", item))`).\\nAnother problem with subscriptions is the format, when a document is _created_ the content is specified\\nin the `fullDocument` property but it's a different property when the document is updated. Thus a filter\/query\\nwould not need to consider both these cases which is very difficult. With the new approach a query\/filter is much\\neasier since we only need to care about inserts.\\n","tokens":17,"id":4385,"text":"## Context\\nWe need to record events in MongoDB in a specific structure\/schema.\\n\n\n##Decision\nThe [CloudEvents](https:\/\/cloudevents.io\/) are persisted like this in the \"events collection\" in the database (1):\\n```json\\n{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"dataschema\" : \"http:\/\/someschema.com\/schema.json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n},\\n\"streamid\" : \"streamid\"\\n}\\n```\\nNote that \"streamid\" is added as an extension by the MongoDB event stores in order to read all events for a particular stream.\\nIf stream consistency is enabled then another collection, the \"stream consistency\" collection is also written to the database (2):\\n```json\\n{\\n\"_id\" : \"streamid\",\\n\"version\" : 1\\n}\\n```\\nWhen appending cloud events to the stream the consistency of the stream is maintained by comparing the version supplied by the user\\nwith the version present in (2). If they don't match then the cloud events are not written. Also if there are two threads writing to the same\\nstream at once then one of them will run into an error which means it has to retry (optimistic locking). For this to work, transactions are required!\\nAnother previous approach was instead to store the events like this:\\n```json\\n{\\n\"_id\": \"streamid\",\\n\"version\" : 1,\\n\"events\": [{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n}\\n}]\\n}\\n```\\nI.e. the events were stored inside a single document. While there are several benefits of using this approach, such as:\\n1. No transactions required, just do;\\n```java\\neventCollection.updateOne(and(eq(\"_id\", streamId), eq(\"version\", expectedStreamVersion)),\\ncombine(pushEach(\"events\", serializedEvents), set(\"version\", expectedStreamVersion + 1)),\\nnew UpdateOptions().upsert(true));\\n```\\n1. Reads could be done in a streaming fashion even though the events were stored as a subarray using aggregations\\n1. Subscriptions could take a `List<CloudEvent>`, i.e. all events written in the same transaction to the event store.\\nWhen not using the approach subscriptions gets notified once for each event and the consumer needs to reassemble\\nthe \"transaction\" somehow. This is a major drawback when not using this approach.\\nThere are however two major drawbacks that lead to not using this approach:\\n1. There's 16Mb document size limit in MongoDB so this approach wouldn't work for large streams\\n1. It's much hard to implement queries\/filters for subscriptions. The aggregation support is\\n[limited](https:\/\/stackoverflow.com\/questions\/62846085\/remove-element-from-subarray-using-an-aggregation-stage-applied-to-a-change-stre)\\nwhen working with subscriptions preventing simple filters (it would have been much simpler if `unwind`\\nwas supported since then we could flatten out the `events` subarray before applying the queries, i.e. something like\\n`(unwind(\"$events\"), replaceRoot(\"$events\"), match(filter.apply(\"type\", item))`).\\nAnother problem with subscriptions is the format, when a document is _created_ the content is specified\\nin the `fullDocument` property but it's a different property when the document is updated. Thus a filter\/query\\nwould not need to consider both these cases which is very difficult. With the new approach a query\/filter is much\\neasier since we only need to care about inserts.\\n","Predictions":"We will use the [Schema](https:\/\/schema.io\/) schema to store the events.n"}
{"File Name":"adr-demo\/0002-use-microservices-architecture.md","Context":"## Context\nThe web application we want to develop and offer to our users is a picture service that allows :\n* Uploading images and gifs with a title and tags\n* Searching images by title or tag\n* Downloading an image in various file formats and sizes\nAs we want to quicly build a first version to be able to quickly go online and answer competitors, we choose\nto deploy our application to Google Cloud (as we do not have any bare metal servers).\nThe application should meet the following criteria:\n* it should be quick enough so the search answers in less than 500ms\n* it should be scalable so we can meet future usage\n","Decision":"In order to meet required criteria we choose to develop our application using a microservices approach :\\neach business need will be addressed by a specific microservice, as described in the following schema:\\n![](microservices-architecture.png)\\nEach microservice should take a simple input and produce a simple output, each microservice should act as a black box\\nregarding other microservices, meaning state should not be shared across different instances of same or differents microservices.\\n","tokens":137,"id":2516,"text":"## Context\\nThe web application we want to develop and offer to our users is a picture service that allows :\\n* Uploading images and gifs with a title and tags\\n* Searching images by title or tag\\n* Downloading an image in various file formats and sizes\\nAs we want to quicly build a first version to be able to quickly go online and answer competitors, we choose\\nto deploy our application to Google Cloud (as we do not have any bare metal servers).\\nThe application should meet the following criteria:\\n* it should be quick enough so the search answers in less than 500ms\\n* it should be scalable so we can meet future usage\\n\n\n##Decision\nIn order to meet required criteria we choose to develop our application using a microservices approach :\\neach business need will be addressed by a specific microservice, as described in the following schema:\\n![](microservices-architecture.png)\\nEach microservice should take a simple input and produce a simple output, each microservice should act as a black box\\nregarding other microservices, meaning state should not be shared across different instances of same or differents microservices.\\n","Predictions":"We will use the [Security & Security](https:\/\/github.com\/security-and-security) to provide a single, centralised, and unified security mechanism.n"}
{"File Name":"stamper\/0004-bootsptrap-as-frontend-framework.md","Context":"## Context\nWe need to choose a frontend framework.\n","Decision":"We don't have any specific requirements yet, so use Bootstrap because it is the most popular framework, and has a lot of documentation.\\n","tokens":12,"id":1240,"text":"## Context\\nWe need to choose a frontend framework.\\n\n\n##Decision\nWe don't have any specific requirements yet, so use Bootstrap because it is the most popular framework, and has a lot of documentation.\\n","Predictions":"We don't have any specific requirements yet, so use Bootstrap because it is the most popular framework, and has a lot of documentation.n"}
{"File Name":"tdr-dev-documentation\/0018-govkuk-notify-staging-configuration.md","Context":"## Context\nIt was decided to set the GovUK Notify Staging service to go live: [0015 GovUK Notify Staging Configuration](0015-govuuk-notify-staging-configuration.md)\n","Decision":"The GovUK Notify team recommended that usually they do not go live with non-production services.\\nIt was decided to not go live with the GovUK Notify Staging services following this advice from the GovUK Notify team.\\nIt is still possible to send emails to users participating with UAT via adding their email addresses to the service's guest list.\\nThe Staging service is sufficiently consistent with the Production service if it were to remain in \"trial mode\".\\nFor full details of the \"trial mode\" for GovUK Notify services see here: https:\/\/www.notifications.service.gov.uk\/using-notify\/trial-mode\\n","tokens":44,"id":1767,"text":"## Context\\nIt was decided to set the GovUK Notify Staging service to go live: [0015 GovUK Notify Staging Configuration](0015-govuuk-notify-staging-configuration.md)\\n\n\n##Decision\nThe GovUK Notify team recommended that usually they do not go live with non-production services.\\nIt was decided to not go live with the GovUK Notify Staging services following this advice from the GovUK Notify team.\\nIt is still possible to send emails to users participating with UAT via adding their email addresses to the service's guest list.\\nThe Staging service is sufficiently consistent with the Production service if it were to remain in \"trial mode\".\\nFor full details of the \"trial mode\" for GovUK Notify services see here: https:\/\/www.notifications.service.gov.uk\/using-notify\/trial-mode\\n","Predictions":"The GovUK Notify team recommended that usually they do not go live with non-production services.nIt was decided to not go live with the GovUK Notify Staging services following this advice from the GovUK Notify team.nIt is still possible to send emails to users participating with UAT via adding their email addresses to the service's guest list.nThe Staging service is sufficiently consistent with the Production service if it were to remain in \"trial mode\".nFor full details of the \"trial mode\" for GovUK Notify services see here: https:\/\/www.notifications.service.gov.uk\/using-notify\/trial-moden"}
{"File Name":"operational-data-hub\/0023-iso-8601-to-specify-date-and-time-with-timezone.md","Context":"## Context\nJSON does not specify how a date(time) string should be formatted. The ISO 8601 standard is widely used within the JSON community to specify date-time objects. [RFC 3339]([https:\/\/tools.ietf.org\/html\/rfc3339) describes the usage of the ISO-8601 standard.\n","Decision":"We will use the ISO-8601 (latest version) standard (as described in RFC-3339) for formatting date(time) objects whenever a date(time) object is serialized. This applies (but is not limited) to JSON messages, logging, data-store\/firestore timestamps.\\nAll date objects must have a time-zone included.\\n","tokens":66,"id":2755,"text":"## Context\\nJSON does not specify how a date(time) string should be formatted. The ISO 8601 standard is widely used within the JSON community to specify date-time objects. [RFC 3339]([https:\/\/tools.ietf.org\/html\/rfc3339) describes the usage of the ISO-8601 standard.\\n\n\n##Decision\nWe will use the ISO-8601 (latest version) standard (as described in RFC-3339) for formatting date(time) objects whenever a date(time) object is serialized. This applies (but is not limited) to JSON messages, logging, data-store\/firestore timestamps.\\nAll date objects must have a time-zone included.\\n","Predictions":"We will use the ISO-8601 (latest version) standard (as described in RFC-3339) for formatting date(time) objects whenever a date(time) object is serialized. This applies (but is not limited) to JSON messages, logging, data-store\/firestore timestamps.nAll date objects must have a time-zone included.n"}
{"File Name":"lcarsde\/display-server-library.md","Context":"## Context and Problem Statement\nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input\/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.\n","Decision":"Chosen option: \"Xlib\", because it was used already for the additional tooling and there's lots of documentations and examples.\\n### Positive Consequences <!-- optional -->\\n* Direct compatibility with additional tool libraries, e.g. for XPM handling\\n* Lots of resources and with Openbox a very complete WM (in sense of ICCCM and EWMH) as possible template\\n### Negative Consequences <!-- optional -->\\n* Refactoring code\\n* Wayland might be more modern\\n* XCB is faster at times due to asynchronous\\n","tokens":66,"id":142,"text":"## Context and Problem Statement\\nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input\/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.\\n\n\n##Decision\nChosen option: \"Xlib\", because it was used already for the additional tooling and there's lots of documentations and examples.\\n### Positive Consequences <!-- optional -->\\n* Direct compatibility with additional tool libraries, e.g. for XPM handling\\n* Lots of resources and with Openbox a very complete WM (in sense of ICCCM and EWMH) as possible template\\n### Negative Consequences <!-- optional -->\\n* Refactoring code\\n* Wayland might be more modern\\n* XCB is faster at times due to asynchronous\\n","Predictions":"* We need to start defining interfaces outside of their implementations.n* We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example, use POST \/get_projects BODY  \"index\": \"name\"  instead of GET \/projects?index=name.n"}
{"File Name":"fixcity\/0003-sys-use-modular-monolith.md","Context":"## Context\n- Constraints\n- Greenfield project\n- Quality attributes\n- Expected fast load\n","Decision":"We will not separate components in to separate deployment units,\\nwe will use modular monolith approach with single database.\\n","tokens":20,"id":1797,"text":"## Context\\n- Constraints\\n- Greenfield project\\n- Quality attributes\\n- Expected fast load\\n\n\n##Decision\nWe will not separate components in to separate deployment units,\\nwe will use modular monolith approach with single database.\\n","Predictions":"* Performance - decision should support high throughput scenariosn* Maintainability - amount of code needed to write should be minimizednChosen option: \"Optimistic concurrency\", because it is a better from the perfromance perspective and it will support our choice to [use Azure Table Storage](0006-use-azure-table-storage-for-persistence.md).n### Positive Consequences!-- optional -->n* Concurrency check will fail very rarely so it will have almost no overheadn* It is aligned with Azure Table storage Optimistic concurrencyn### Negative Consequences!-- optional -->n* Whole application will have a responsibility to manage versions properlyn* If we change a datastore we might need to have a larger changen"}
{"File Name":"remultiform\/0008-support-indexeddb-as-a-data-store.md","Context":"## Context\nWe need to persist the user data to the backend. Rather than building our\nfrontend to be tightly coupled to an API, we want a layer that can sit between\nthe application and the API.\nWe know that we will need to support offline use of these forms. We also want\nusers to be able to recover unsubmitted forms if they haven't gone online\nbetween sessions.\nWe also know that some forms will require the upload of images.\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\ndatabase that persists data across sessions. It supports storing blob data (such\nas images), and has decent storage limits.\n","Decision":"We will support IndexedDB as a data store layer, sitting between the application\\nand the API.\\n","tokens":144,"id":5257,"text":"## Context\\nWe need to persist the user data to the backend. Rather than building our\\nfrontend to be tightly coupled to an API, we want a layer that can sit between\\nthe application and the API.\\nWe know that we will need to support offline use of these forms. We also want\\nusers to be able to recover unsubmitted forms if they haven't gone online\\nbetween sessions.\\nWe also know that some forms will require the upload of images.\\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\\ndatabase that persists data across sessions. It supports storing blob data (such\\nas images), and has decent storage limits.\\n\n\n##Decision\nWe will support IndexedDB as a data store layer, sitting between the application\\nand the API.\\n","Predictions":"We will use Azure Blob storage for the uploads.n"}
{"File Name":"TOSCAna\/0018-cloudfoundry-no-deployment-in-runtime.md","Context":"## Problem\nMost of the credentials\/information of the environment (like a service address) are only available as soon the application is deployed.\n","Decision":"* Chosen Alternative: creating scripts\\n","tokens":29,"id":1745,"text":"## Problem\\nMost of the credentials\/information of the environment (like a service address) are only available as soon the application is deployed.\\n\n\n##Decision\n* Chosen Alternative: creating scripts\\n","Predictions":"* Chosen Alternative: creating scriptsn"}
{"File Name":"celestia-core\/adr-026-general-merkle-proof.md","Context":"## Context\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\nGoals:\n- Layer Merkle proofs without decoding\/reencoding\n- Provide general way to chain proofs\n- Make the proof format extensible, allowing thirdparty proof types\n","Decision":"### ProofOperator\\n`type ProofOperator` is an interface for Merkle proofs. The definition is:\\n```go\\ntype ProofOperator interface {\\nRun([][]byte) ([][]byte, error)\\nGetKey() []byte\\nProofOp() ProofOp\\n}\\n```\\nSince a proof can treat various data type, `Run()` takes `[][]byte` as the argument, not `[]byte`. For example, a range proof's `Run()` can take multiple key-values as its argument. It will then return the root of the tree for the further process, calculated with the input value.\\n`ProofOperator` does not have to be a Merkle proof - it can be a function that transforms the argument for intermediate process e.g. prepending the length to the `[]byte`.\\n### ProofOp\\n`type ProofOp` is a protobuf message which is a triple of `Type string`, `Key []byte`, and `Data []byte`. `ProofOperator` and `ProofOp`are interconvertible, using `ProofOperator.ProofOp()` and `OpDecoder()`, where `OpDecoder` is a function that each proof type can register for their own encoding scheme. For example, we can add an byte for encoding scheme before the serialized proof, supporting JSON decoding.\\n","tokens":99,"id":4130,"text":"## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding\/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n\n\n##Decision\n### ProofOperator\\n`type ProofOperator` is an interface for Merkle proofs. The definition is:\\n```go\\ntype ProofOperator interface {\\nRun([][]byte) ([][]byte, error)\\nGetKey() []byte\\nProofOp() ProofOp\\n}\\n```\\nSince a proof can treat various data type, `Run()` takes `[][]byte` as the argument, not `[]byte`. For example, a range proof's `Run()` can take multiple key-values as its argument. It will then return the root of the tree for the further process, calculated with the input value.\\n`ProofOperator` does not have to be a Merkle proof - it can be a function that transforms the argument for intermediate process e.g. prepending the length to the `[]byte`.\\n### ProofOp\\n`type ProofOp` is a protobuf message which is a triple of `Type string`, `Key []byte`, and `Data []byte`. `ProofOperator` and `ProofOp`are interconvertible, using `ProofOperator.ProofOp()` and `OpDecoder()`, where `OpDecoder` is a function that each proof type can register for their own encoding scheme. For example, we can add an byte for encoding scheme before the serialized proof, supporting JSON decoding.\\n","Predictions":"We will use ProofOperator as the interface for Merkle proofs. The definition is:ngontype ProofOperator interface nRun([][]byte) ([][]byte, error)nGetKey() []bytenProofOp() ProofOpnnnSince a proof can treat various data type, Run() takes [][]byte as the argument, not []byte. For example, a range proof's Run() can take multiple key-values as its argument, not []byte.nProofOperator does not have to be a Merkle proof - it can be a function that transforms the argument for intermediate process e.g. prepending the length to the []byte.n### ProofOpntype ProofOp is a protobuf message which is a triple of Type string, Key []byte, and Data []byte. ProofOperator and ProofOpare interconvertible, using ProofOperator.ProofOp() and OpDecoder(), where OpDecoder is a function that each proof type can register for their own encoding scheme. For example, we can add an byte for encoding scheme before the serialized proof, supporting JSON decoding.n"}
{"File Name":"corona-hackathon\/0011-postgres.md","Context":"## Context\nWe have to make a decision on how\/where to host the DB.\n","Decision":"We use a freely managed [Postgres on Heroku](https:\/\/www.heroku.com\/postgres)\\n","tokens":19,"id":3657,"text":"## Context\\nWe have to make a decision on how\/where to host the DB.\\n\n\n##Decision\nWe use a freely managed [Postgres on Heroku](https:\/\/www.heroku.com\/postgres)\\n","Predictions":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisionsn"}
{"File Name":"monocle\/0011-search-query-language.md","Context":"## Context and Problem Statement\nTo build custom dashboards we need to define queries that are too complex for the existing filter box form.\nWe would like to use flexible search expressions based on a proper query language.\n","Decision":"Chosen option: \"Monocle Query Language\", because it comes out best (see below).\\n### Positive Consequences\\n- We improve the user experience by replacing the clunky filter box with a simpler search bar.\\n- We create a re-usable component.\\n### Negative Consequences\\n- We need to maintain a language toolchain.\\n","tokens":44,"id":4637,"text":"## Context and Problem Statement\\nTo build custom dashboards we need to define queries that are too complex for the existing filter box form.\\nWe would like to use flexible search expressions based on a proper query language.\\n\n\n##Decision\nChosen option: \"Monocle Query Language\", because it comes out best (see below).\\n### Positive Consequences\\n- We improve the user experience by replacing the clunky filter box with a simpler search bar.\\n- We create a re-usable component.\\n### Negative Consequences\\n- We need to maintain a language toolchain.\\n","Predictions":"Chosen option: \"Use a syntax that is derived of the lucene query syntax\", because only option that is already known, and easy to implement.nFurthermore parsers for lucene already exist and are tested.nFor simplicity, and lack of universal capabilities across fetchers, only basic query features and therefor syntax is supported:n* All terms in the query are whitespace separated and will be ANDedn* Default and certain fielded terms are supportedn* Fielded Terms:n* authorn* titlen* journaln* year (for single year)n* year-range (for range e.g. year-range:2012-2015)n* The journal, year, and year-range fields should only be populated once in each queryn* The year and year-range fields are mutually exclusiven* Example:n* author:\"Igor Steinmacher\" author:\"Christn"}
{"File Name":"GoStudy\/0002-use-hexagonal-architecture.md","Context":"## Context\nThe SDARS application consists of 3 independent components that can be communicated in various ways.\nTo enable different communication ways we need to apply a proper architectural style.\n","Decision":"Adopt Hexagonal Architecture for project.\\n","tokens":37,"id":428,"text":"## Context\\nThe SDARS application consists of 3 independent components that can be communicated in various ways.\\nTo enable different communication ways we need to apply a proper architectural style.\\n\n\n##Decision\nAdopt Hexagonal Architecture for project.\\n","Predictions":"Adopt hexagonal architecture for project.n"}
{"File Name":"form-builder\/0010-testing-the-editor-pipeline.md","Context":"## Context\nOur pipeline follows a continuous delivery method with the manual gate to release to production.\n![alt text](0010-release-pipeline-context.png \"Diagram of how the editor is released through the pipeline.\")\nWe have one Editor in test which causes issues when the rest of the team want to test a feature. Also, while the Editor is tied up in testing the pipeline is blocked for deploying to live-production.\n","Decision":"![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")\\nThe main problem to solve is having a feature that is being developed available to the wider team to test. Running locally was not considered as the MOJ Macbooks are locked down to those outside of the technical disciplines.\\nChanging the pipeline to create an Editor (all plugged in to the dev and test environments). This will involve, building and deploying the editor code into a pod and attaching to a unique URL. Not every branch will need to create a new test editor, the developer will need to add \"testable-\" to the beginning of the branch name (i.e. `testable-my-really-awesome-feature`).\\nThe Editor will use the current auth mechanism unless the effort outweighs the investment. At a mimimum, basic auth of a set username and password must be added.\\nTest Editor must be cleanly removed after it has fullfilled it's purpose. This should ideally be automated either triggered by merging the branch or via commandline.\\n","tokens":87,"id":5102,"text":"## Context\\nOur pipeline follows a continuous delivery method with the manual gate to release to production.\\n![alt text](0010-release-pipeline-context.png \"Diagram of how the editor is released through the pipeline.\")\\nWe have one Editor in test which causes issues when the rest of the team want to test a feature. Also, while the Editor is tied up in testing the pipeline is blocked for deploying to live-production.\\n\n\n##Decision\n![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")\\nThe main problem to solve is having a feature that is being developed available to the wider team to test. Running locally was not considered as the MOJ Macbooks are locked down to those outside of the technical disciplines.\\nChanging the pipeline to create an Editor (all plugged in to the dev and test environments). This will involve, building and deploying the editor code into a pod and attaching to a unique URL. Not every branch will need to create a new test editor, the developer will need to add \"testable-\" to the beginning of the branch name (i.e. `testable-my-really-awesome-feature`).\\nThe Editor will use the current auth mechanism unless the effort outweighs the investment. At a mimimum, basic auth of a set username and password must be added.\\nTest Editor must be cleanly removed after it has fullfilled it's purpose. This should ideally be automated either triggered by merging the branch or via commandline.\\n","Predictions":"![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")nThe main problem to solve is having a feature that is being developed available to the wider team to test. Also, while the Editor is tied up in test the pipeline is blocked for deploying to live-production.n![alt text](0010-release-pipeline-context.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")nThe main problem to solve is having a feature that is being developed available to the wider team to test. Also, while the Editor is tied up in test the pipeline is blocked for deploying to live-production.n![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")nThe main problem to solve is having a feature that is being developed available to the wider team to test. Also, while the Editor is tied up in test the pipeline is blocked for deploying to live-production.n![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")nThe main problem to solve is having a feature that is being developed available to the wider team to test. Also, while the Editor is tied up in testing the pipeline is blocked for deploying to live-production.n![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")nThe main problem to solve is having a feature that is being developed available to the wider team to test. Also, while the Editor is tied up in testing the pipeline is blocked for deploying to live-production.n![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")nThe main problem to solve is having a feature that is being developed available to the wider team to test. Also, while the Editor is tied up in testing the pipeline is blocked for deploying to live-production.n![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")nThe main problem to solve is having a feature that is being developed available to the wider team to test. Also, while the Editor is tied up in testing the pipeline is blocked for deploying to live-production.n![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")nThe main problem to solve is having a feature that is being developed available to the wider team to test. Running locally was not considered as the MOJ Macbooks are locked down to those outside of the technical disciplines.nChanging the pipeline to create an Editor (all plugged in to the dev and test environments) will involve building and deploying the editor code into a pod and attaching to a unique URL. Not every branch will need to create a new test editor, the developer will need to add \"testable-\" to the beginning of the branch name (i.e. testable-my-really-awesome-feature).nThe Editor will use the current auth mechanism unless the effort outweighs the investment. At a mimum, basic auth of a set username and password must be added. Test Editor must be cleanly removed after it has fullfilled it's purpose. This should ideally be automated either triggered by merging the branch or via commandline.n"}
{"File Name":"iampeterbanjo.com\/0008-use-ava-test-runner.md","Context":"## Context\n[Lab][hapi-lab] is misreporting the code coverage stats because I have not set it up to [work with Typescript][lab-ts]. The other issue with Lab is that its community is quite small meaning less plug-and-play with other tools. I thought about using [Ava][ava-typescript] but [this review][dodds-jest] of Ava's performance issues doesn't sound great.\n","Decision":"Jest seems like the way to go:\\n- Typescript support\\n- Large community\\n- Familiar\\n","tokens":91,"id":1594,"text":"## Context\\n[Lab][hapi-lab] is misreporting the code coverage stats because I have not set it up to [work with Typescript][lab-ts]. The other issue with Lab is that its community is quite small meaning less plug-and-play with other tools. I thought about using [Ava][ava-typescript] but [this review][dodds-jest] of Ava's performance issues doesn't sound great.\\n\n\n##Decision\nJest seems like the way to go:\\n- Typescript support\\n- Large community\\n- Familiar\\n","Predictions":"We will use [ADR-007](007-define-goal-for-an-mvp.md) as our MVP.n"}
{"File Name":"handbook\/0003-css.md","Context":"## Context and Problem Statement\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\n## Decision Drivers\n- Should be performant, with code splitting, caching and minimal runtime overhead.\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\n- Should be type-safe to catch issues when refactoring.\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\n- We want a pattern for responsive props with atomic layout components.\n","Decision":"- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript jsx\\n\/\/ Good:\\n<Box padding\"small\" \/>\\n<Box padding={{xs: 'small', md: 'medium'}} \/>\\n<Input large \/>\\n<Text preset=\"heading3\" as=\"p\" \/>\\n\/\/ Bad:\\n<Box className={customLayout} \/>\\n<Input style={{ height: 50, padding: 16 }} \/>\\n<Text className={styles.heading} \/>\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n","tokens":126,"id":1992,"text":"## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n\n\n##Decision\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript jsx\\n\/\/ Good:\\n<Box padding\"small\" \/>\\n<Box padding={{xs: 'small', md: 'medium'}} \/>\\n<Input large \/>\\n<Text preset=\"heading3\" as=\"p\" \/>\\n\/\/ Bad:\\n<Box className={customLayout} \/>\\n<Input style={{ height: 50, padding: 16 }} \/>\\n<Text className={styles.heading} \/>\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n","Predictions":"- Should be performant, with code splitting, caching and minimal runtime overhead.n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.n- Should be type-safe to catch issues when refactoring.n- Reusable components should be closed, not accepting arbitrary styles\/classes.nWe want a pattern for responsive props with atomic layout components.nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.nExample:ntypescript jsxn\/\/ Good:nBox paddingsmall\" \/>nBox padding=xs:'small', md:'medium' \/>nInput style= height: 50, padding: 16  \/>nText className=styles.heading \/>nn### Positive Consequencesn- Treat is statically extracted at build time, so it has minimal runtime.n- Styles load in parallel with styles.load, so it has minimal runtime.n- Styles load in parallel with styles.load, so it has minimal runtime.n- Styles load in parallel with styles.load, so it has minimal runtime.n"}
{"File Name":"hello\/005-test-framework.md","Context":"## Context\nWe want our code to be well tested. What tools or frameworks can we\nleverage?\n","Decision":"We will use the [pytest] framework to test our [Python] code. In\\naddition, we will use the [mock] library to prevent our tests\\nfrom interacting with external services.\\n","tokens":23,"id":4359,"text":"## Context\\nWe want our code to be well tested. What tools or frameworks can we\\nleverage?\\n\n\n##Decision\nWe will use the [pytest] framework to test our [Python] code. In\\naddition, we will use the [mock] library to prevent our tests\\nfrom interacting with external services.\\n","Predictions":"We will use the [pytest](https:\/\/github.com\/pytest\/pytest) framework to test our [Python] code. Innaddition, we will use the [mock] library to prevent our testsnfrom interacting with external services.n"}
{"File Name":"lbh-adrs\/Event-Driven-Architecture.md","Context":"## **Context**\nWhen the microservices need to interact with each other (e.g. to sync common data), the preferable approach is to adopt an event-driven architecture. This approach guarantees loosely coupled services which can be run and deployed in isolation.\nIn order to implement an events-driven architecture we need to use a message broker which should be responsible for sending the messages.\nIn AWS, two options can be considered:\n1. SNS (Simple Notification Service)\nSNS implements pub\/sub messaging which allow applications to send messages to multiple subscribers through push mechanisms.\n2. SNS (Simple Notification Service) + SQS (Simple Queue Service)\nSQS is a message queue service used by distributed applications to exchange messages through a polling model and can be used to decouple sending and receiving components. Using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event, and also persisted in an Amazon SQS queue for other applications to process at a later time.\n","Decision":"**SNS (Simple Notification Service) + SQS (Simple Queue Service)**\\nBy coupling SQS with SNS, the subscriber can receive messages at \"their peace\". It allows subscribers to be offline, tolerant to network and host failures. Although SNS has got a 4-phase retry policy, the message can be lost if the consumer is not available. Instead if the subscriber uses a queue, we are able to achieve guaranteed delivery.\\n","tokens":209,"id":2309,"text":"## **Context**\\nWhen the microservices need to interact with each other (e.g. to sync common data), the preferable approach is to adopt an event-driven architecture. This approach guarantees loosely coupled services which can be run and deployed in isolation.\\nIn order to implement an events-driven architecture we need to use a message broker which should be responsible for sending the messages.\\nIn AWS, two options can be considered:\\n1. SNS (Simple Notification Service)\\nSNS implements pub\/sub messaging which allow applications to send messages to multiple subscribers through push mechanisms.\\n2. SNS (Simple Notification Service) + SQS (Simple Queue Service)\\nSQS is a message queue service used by distributed applications to exchange messages through a polling model and can be used to decouple sending and receiving components. Using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event, and also persisted in an Amazon SQS queue for other applications to process at a later time.\\n\n\n##Decision\n**SNS (Simple Notification Service) + SQS (Simple Queue Service)**\\nBy coupling SQS with SNS, the subscriber can receive messages at \"their peace\". It allows subscribers to be offline, tolerant to network and host failures. Although SNS has got a 4-phase retry policy, the message can be lost if the consumer is not available. Instead if the subscriber uses a queue, we are able to achieve guaranteed delivery.\\n","Predictions":"**Security**nSecurity is the responsibility of the service provider. It is the responsibility of the service provider to ensure that the service is up to date and meets the requirements of the service provider.nSecurity is the responsibility of the service provider to ensure that the service provider is up to date and meets the requirements of the service provider.nSecurity is the responsibility of the service provider to ensure that the service provider is up to date and meets the requirements of the service provider.n"}
{"File Name":"CICD-pipeline\/002-usage-and-import-of-shared-library.md","Context":"## Context\nWe need to decide how the shared library should be loaded for usage.\n- A library version can be a tag, a branch or a commit SHA.\n- A library can be loaded explicitly or implicitly\n- A library default version can be configured globally or not\n### Loading implicitly\n````groovy\npipeline {\nagent any\nstages {\nstage {\nsteps {\nsomeMagicallyAvailableLibraryFunction()\n}\n}\n}\n}\n````\n- a default version of the library needs to be defined globally\n- there is no need to import the library, as it will be loaded automatically\n- you can override the version by importing it manually: `@Library('my-shared-library@my-custom-version') _`\n### Loading explicitly\n````groovy\n@Library('my-shared-library@1.0.0') _\npipeline {\nagent any\nstages {\nstage {\nsteps {\nsomeMagicallyAvailableLibraryFunction()\n}\n}\n}\n}\n````\n- an import statement is always needed\n- a default version could be defined globally, then the version can be left out when importing\n### Using a global predefined version\n- can be overriden in the pipeline\n- does not force users to select a version\n### No globel predefined version\n- import will not work without version\n- consumers are aware of the version that they use\n- can be dynamic (auto updating) when using release channels with branches\n","Decision":"We will use explicit loading without a predefined version and release channels for releasing based on branches.\\n- reduces the magic of the used version\\n- can still be auto updating\\n- consumers know that a library is used and what the version is\\n","tokens":314,"id":2766,"text":"## Context\\nWe need to decide how the shared library should be loaded for usage.\\n- A library version can be a tag, a branch or a commit SHA.\\n- A library can be loaded explicitly or implicitly\\n- A library default version can be configured globally or not\\n### Loading implicitly\\n````groovy\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- a default version of the library needs to be defined globally\\n- there is no need to import the library, as it will be loaded automatically\\n- you can override the version by importing it manually: `@Library('my-shared-library@my-custom-version') _`\\n### Loading explicitly\\n````groovy\\n@Library('my-shared-library@1.0.0') _\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- an import statement is always needed\\n- a default version could be defined globally, then the version can be left out when importing\\n### Using a global predefined version\\n- can be overriden in the pipeline\\n- does not force users to select a version\\n### No globel predefined version\\n- import will not work without version\\n- consumers are aware of the version that they use\\n- can be dynamic (auto updating) when using release channels with branches\\n\n\n##Decision\nWe will use explicit loading without a predefined version and release channels for releasing based on branches.\\n- reduces the magic of the used version\\n- can still be auto updating\\n- consumers know that a library is used and what the version is\\n","Predictions":"We will use explicit loading without a predefined version and release channels for releasing based on branches.n- reduces the magic of the used versionn- can still be auto updatingn- consumers know that a library is used and what the version isn"}
{"File Name":"ios-architecture-decision-logs\/0006-inject-parameters-asArguments-toPresenter.md","Context":"## Context\nWe faced a problem while injecting new parameters to presenters' initializer. The list of init params are getting much longer.\n","Decision":"Each module has it's own `createModule` or `setupModule` static method in its router. If we have 3 or more properties to be injected to any of these methods, we must create `XYZArguments` structure that contains those properties and pass the argument object to router.\\nWe must also inject the same arguments variable to the presenters' initializer and use the variables inside through the arguments object, and not copy each of variables inside arguments to the presenter one by one.\\n**Note**: Argument object must only include the values to be injected from outer module to initialize the target module (e.g. any kind of `Config\/AB flags`, `Managers` and `Helpers` must not be stored inside arguments, and these variables must be injected to the presenters' initializer directly)\\n","tokens":28,"id":4983,"text":"## Context\\nWe faced a problem while injecting new parameters to presenters' initializer. The list of init params are getting much longer.\\n\n\n##Decision\nEach module has it's own `createModule` or `setupModule` static method in its router. If we have 3 or more properties to be injected to any of these methods, we must create `XYZArguments` structure that contains those properties and pass the argument object to router.\\nWe must also inject the same arguments variable to the presenters' initializer and use the variables inside through the arguments object, and not copy each of variables inside arguments to the presenter one by one.\\n**Note**: Argument object must only include the values to be injected from outer module to initialize the target module (e.g. any kind of `Config\/AB flags`, `Managers` and `Helpers` must not be stored inside arguments, and these variables must be injected to the presenters' initializer directly)\\n","Predictions":"We will store the current validator set in a state tree.n"}
{"File Name":"form-design-system\/components_ignore_classnames.md","Context":"## Context and Problem Statement\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\narbitrary style overrides on the component.\nWe want all components in FDS to follow a similar prop interface, so this decision should\napply to _all_ components.\n","Decision":"Chosen option: \"No components accept a `className`\", because...\\n- It's too risky to introduce style overrides to low-level layout components.\\n- Style customization should happen via props, where the component can be responsible for\\nstyle changes and variations.\\n### Positive Consequences <!-- optional -->\\n- Prop API consistency across all components.\\n### Negative Consequences <!-- optional -->\\n- Consumers of FDS components must handle the breaking change. Any custom `className`\\nprops on FDS components must be moved to a direct parent or direct child of the\\ncomponent.\\n","tokens":80,"id":802,"text":"## Context and Problem Statement\\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\\narbitrary style overrides on the component.\\nWe want all components in FDS to follow a similar prop interface, so this decision should\\napply to _all_ components.\\n\n\n##Decision\nChosen option: \"No components accept a `className`\", because...\\n- It's too risky to introduce style overrides to low-level layout components.\\n- Style customization should happen via props, where the component can be responsible for\\nstyle changes and variations.\\n### Positive Consequences <!-- optional -->\\n- Prop API consistency across all components.\\n### Negative Consequences <!-- optional -->\\n- Consumers of FDS components must handle the breaking change. Any custom `className`\\nprops on FDS components must be moved to a direct parent or direct child of the\\ncomponent.\\n","Predictions":"Chosen option: \"No components accept a className\", because...n- It's too risky to introduce style overrides to low-level layout components.n- Style customization should happen via props, where the component can be responsible fornstyle changes and variations.n### Positive Consequences!-- optional -->n- Prop API consistency across all components.n### Negative Consequences!-- optional -->n- Consumers of FDS components must handle the breaking change. Any custom classNamenprops on FDS components must be moved to a direct parent or direct child of the component.n"}
{"File Name":"green_log\/0003-decouple-generation-and-handling.md","Context":"## Context\nWe want the logging API used by applications to be consistent, while allowing for logs to be filed, forwarded, filtered and formatted in a variety of ways.\n","Decision":"De-couple generation of log message\/entries from how they are handled.\\n* A `logger` object provides an API that can be used to generate log entries.\\n* Log \"entries\" are strongly typed structures.\\n* Log entry \"handlers\" provide a simple, consistent interface.\\n```mermaid\\nsequenceDiagram\\nApp ->> Logger:       info(\"Message\")\\nLogger ->> Handler:   <<(entry)\\n```\\n","tokens":35,"id":1924,"text":"## Context\\nWe want the logging API used by applications to be consistent, while allowing for logs to be filed, forwarded, filtered and formatted in a variety of ways.\\n\n\n##Decision\nDe-couple generation of log message\/entries from how they are handled.\\n* A `logger` object provides an API that can be used to generate log entries.\\n* Log \"entries\" are strongly typed structures.\\n* Log entry \"handlers\" provide a simple, consistent interface.\\n```mermaid\\nsequenceDiagram\\nApp ->> Logger:       info(\"Message\")\\nLogger ->> Handler:   <<(entry)\\n```\\n","Predictions":"We will use a sequenceDiagram to generate a message.n"}
{"File Name":"publicservice-registry\/0013-handle-eventual-consistency-with-etags.md","Context":"## Context\nThe UI might request a resource after it's creation or alteration, while the async projections are not yet up to date with this change.\nIn that case, the user would see stale data without knowing why, perhaps thinking its changes were not accepted by the application.\nTo prevent this, the client needs a mechanism to check whether or not its latest update has already been processed by the projections.\n","Decision":"We will handle eventual consistency concerns using ETags. On succesful POST or PUT to the API, the client will receive a response containing an ETag header. This header will contain the highest event number caused by the client's request.\\nAll Projections will update their ProjectionState with this number.\\nIf a client wants to know if the resource is up to date with their changes, they will have to include the ETag in a `If-Match` header. Depending on whether the Projections is up to date or not, the API will return a `200 OK` or `412 PRECONDITION FAILED`.\\n","tokens":83,"id":3127,"text":"## Context\\nThe UI might request a resource after it's creation or alteration, while the async projections are not yet up to date with this change.\\nIn that case, the user would see stale data without knowing why, perhaps thinking its changes were not accepted by the application.\\nTo prevent this, the client needs a mechanism to check whether or not its latest update has already been processed by the projections.\\n\n\n##Decision\nWe will handle eventual consistency concerns using ETags. On succesful POST or PUT to the API, the client will receive a response containing an ETag header. This header will contain the highest event number caused by the client's request.\\nAll Projections will update their ProjectionState with this number.\\nIf a client wants to know if the resource is up to date with their changes, they will have to include the ETag in a `If-Match` header. Depending on whether the Projections is up to date or not, the API will return a `200 OK` or `412 PRECONDITION FAILED`.\\n","Predictions":"We will handle eventual consistency concerns using ETags. On succesful POST or PUT to the API, the client will receive a response containing an ETag header. This header will contain the highest event number caused by the client's request.nAll Projections will update their ProjectionState with this number.nIf a client wants to know if the resource is up to date with their changes, they will have to include the ETag in a If-Match header. Depending on whether the Projections is up to date or not, the API will return a 200 OK or 412 PRECONDITION FAILED.n"}
{"File Name":"libelektra\/holes.md","Context":"## Problem\nConfig files ideally do not copy any structure if they only want to\nset a single key.\n","Decision":"Support holes and values for non-leaves in a KeySet if the underlying format allows it.\\nIf the underlying format does not support it and there is also not an obvious\\nway how to circumvent it -- e.g., JSON which does not have comments -- holes and\\nvalues in non-leaves can be supported with key names starting with \u00aeelektra.\\n","tokens":23,"id":1307,"text":"## Problem\\nConfig files ideally do not copy any structure if they only want to\\nset a single key.\\n\n\n##Decision\nSupport holes and values for non-leaves in a KeySet if the underlying format allows it.\\nIf the underlying format does not support it and there is also not an obvious\\nway how to circumvent it -- e.g., JSON which does not have comments -- holes and\\nvalues in non-leaves can be supported with key names starting with \u00aeelektra.\\n","Predictions":"### General ApproachnThe introduction of a GPG interface enables the user to utilize her existing key-pairs for cryptographic operations in Elektra.nThe private key is used for encrypting a random sequence, which serves as seed for a key derivation function (KDF).nThis way we can safely derivate cryptographic keys for symmetric value encryption.nBoth OpenSSL and libgcrypt have built-in support for the PBKDF2 (see RFC 2898).nThe PBKDF2 needs an iteration number and a salt in the PBKDF2.n"}
{"File Name":"Nosedive\/0005-sqlite-database.md","Context":"## Context\nOne of the final stack canditate for persistence is [cockroachdb](https:\/\/www.cockroachlabs.com\/), but the windows version\n","Decision":"We remove all complexity at the beginnign using sqlite. As cockorachdb is SQL compatible, we not expect difficults to migrate\\n","tokens":33,"id":109,"text":"## Context\\nOne of the final stack canditate for persistence is [cockroachdb](https:\/\/www.cockroachlabs.com\/), but the windows version\\n\n\n##Decision\nWe remove all complexity at the beginnign using sqlite. As cockorachdb is SQL compatible, we not expect difficults to migrate\\n","Predictions":"We remove all complexity at the beginning using sqlite. As cockorachdb is SQL compatible, we not expect difficults to migraten"}
{"File Name":"content-publisher\/0008-restricted-deletion.md","Context":"## Context\nWe use foreign keys to help enforce database integrity e.g.\n```\n# ensure creator_id on a document matches a row in the users table\n# when we try to delete a user, raise an error if its the creator of a document\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :restrict\n```\nThere are other `on_delete` behaviours instead of `restrict`.\n```\n# when we delete a user, set creator_id to NULL on all of its documents\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :nullify\n# when we delete a user, also delete all of its document\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :cascade\n```\nDifferent options are appropriate depending on the foreign key.\n* `restrict` is a good choice in a child\/parent scenario, where deleting a child should be restricted if it would make the parent invalid. For example, we should not be able to delete an ActiveStorage blob if its parent image still exists.\n* `nullify` works well if the relationship is optional. For example, our code doesn't currently require a document to have a creator, so it might be reasonable to `creator_id` on document to NULL if we delete a user for some reason.\n* `cascade` is a good choice in a parent\/child scenario, where it makes sense to delete the child when we delete the parent. For example, if we delete a document for some reason, it also makes sense to delete the editions.\n","Decision":"Currently, we don't have any real-world scenarios for deleting stuff from the database. Therefore, we should do the simplest thing and `restrict` on all foreign key delete actions. `restrict` is actually the default, but we should continue to specify it explicitly to indicate a conscious decision.\\n","tokens":341,"id":4872,"text":"## Context\\nWe use foreign keys to help enforce database integrity e.g.\\n```\\n# ensure creator_id on a document matches a row in the users table\\n# when we try to delete a user, raise an error if its the creator of a document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :restrict\\n```\\nThere are other `on_delete` behaviours instead of `restrict`.\\n```\\n# when we delete a user, set creator_id to NULL on all of its documents\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :nullify\\n# when we delete a user, also delete all of its document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :cascade\\n```\\nDifferent options are appropriate depending on the foreign key.\\n* `restrict` is a good choice in a child\/parent scenario, where deleting a child should be restricted if it would make the parent invalid. For example, we should not be able to delete an ActiveStorage blob if its parent image still exists.\\n* `nullify` works well if the relationship is optional. For example, our code doesn't currently require a document to have a creator, so it might be reasonable to `creator_id` on document to NULL if we delete a user for some reason.\\n* `cascade` is a good choice in a parent\/child scenario, where it makes sense to delete the child when we delete the parent. For example, if we delete a document for some reason, it also makes sense to delete the editions.\\n\n\n##Decision\nCurrently, we don't have any real-world scenarios for deleting stuff from the database. Therefore, we should do the simplest thing and `restrict` on all foreign key delete actions. `restrict` is actually the default, but we should continue to specify it explicitly to indicate a conscious decision.\\n","Predictions":"Currently, we don't have any real-world scenarios for deleting stuff from the database. Therefore, we should do the simplest thing and restrict on all foreign key delete actions. restrict is actually the default, but it's not the only one.n"}
{"File Name":"amf\/0003-new-annotation-removal-stage-present-in-all-webapi-pipelines.md","Context":"## Context\nWhen referencing external files there are certain cases where AMF inlines the parsed content without creating a links (this is because the targeted elements are not present in the references of the base unit).\nFor these cases, when a emitting an unresolved model these references are being emitted inlined.\n","Decision":"In order to avoid emitting these references inlined for an unresolved model, we must make use of annotation to save the original reference.\\nWhen saving this reference, we must make sure that if the model is resolved this annotation is no longer present so that the emitter does not render references for a flattened model.\\nThis leads to the creation of a new resolution stage that removes specific annotations from the model that must not be present in a resolved base unit.\\n","tokens":61,"id":1393,"text":"## Context\\nWhen referencing external files there are certain cases where AMF inlines the parsed content without creating a links (this is because the targeted elements are not present in the references of the base unit).\\nFor these cases, when a emitting an unresolved model these references are being emitted inlined.\\n\n\n##Decision\nIn order to avoid emitting these references inlined for an unresolved model, we must make use of annotation to save the original reference.\\nWhen saving this reference, we must make sure that if the model is resolved this annotation is no longer present so that the emitter does not render references for a flattened model.\\nThis leads to the creation of a new resolution stage that removes specific annotations from the model that must not be present in a resolved base unit.\\n","Predictions":"In order to avoid emitting these references inlined for an unresolved model, we must make use of annotation to save the original reference.nWhen saving this reference, we must make sure that if the model is resolved this annotation is no longer present so that the emitter does not render references for a flattened model.nThis leads to the creation of a new resolution stage that removes specific annotations from the model that must not be present in a resolved base unit.n"}
{"File Name":"pace-developers\/0008-pace-syntax-guiding-principles.md","Context":"## Context\nWhilst the core framework of PACE is the Horace (and Herbert) projects,\nPACE involves many other codes, including [Euphonic](https:\/\/github.com\/pace-neutrons\/Euphonic),\n[Brille](https:\/\/github.com\/brille\/brille) and [SpinW](https:\/\/spinw.org).\nThe way in which these programs interact with each other is presently unclear.\nFurthermore the programs are written in several different languages.\nIn this ADR we propose some guiding principles in the design of the interfaces\nbetween the programs or components which make up PACE.\nThis was discussed at a meeting whose\n[minutes are here.](https:\/\/stfc365.sharepoint.com\/:w:\/r\/sites\/PACEProject\/Shared%20Documents\/Meetings\/PACE-General\/pace_syntax_meeting1_minutes.docx))\n","Decision":"The guiding principle for PACE-supported programs is **commonality**.\\nThat is, interacting with the different programs of PACE should be **seamless**\\nto the user, and the user interface presented to them should be **uniform**.\\nThe uniform interface means that function names and how users specify parameters\\nor arguments to functions in Matlab and Python flavours of PACE should align\\nas closely as possible.\\n","tokens":183,"id":5178,"text":"## Context\\nWhilst the core framework of PACE is the Horace (and Herbert) projects,\\nPACE involves many other codes, including [Euphonic](https:\/\/github.com\/pace-neutrons\/Euphonic),\\n[Brille](https:\/\/github.com\/brille\/brille) and [SpinW](https:\/\/spinw.org).\\nThe way in which these programs interact with each other is presently unclear.\\nFurthermore the programs are written in several different languages.\\nIn this ADR we propose some guiding principles in the design of the interfaces\\nbetween the programs or components which make up PACE.\\nThis was discussed at a meeting whose\\n[minutes are here.](https:\/\/stfc365.sharepoint.com\/:w:\/r\/sites\/PACEProject\/Shared%20Documents\/Meetings\/PACE-General\/pace_syntax_meeting1_minutes.docx))\\n\n\n##Decision\nThe guiding principle for PACE-supported programs is **commonality**.\\nThat is, interacting with the different programs of PACE should be **seamless**\\nto the user, and the user interface presented to them should be **uniform**.\\nThe uniform interface means that function names and how users specify parameters\\nor arguments to functions in Matlab and Python flavours of PACE should align\\nas closely as possible.\\n","Predictions":"The guiding principle for PACE-supported programs is **commonality**.nThat is, interacting with the different programs of PACE should be **seamless**nto the user, and the user interface presented to them should be **uniform**.nThe uniform interface means that function names and how users specify parameters to functions in Matlab and Python flavours of PACE should alignnas closely as possible.n"}
{"File Name":"TANF-app\/012-antivirus-strategy.md","Context":"## Context\nOur original plan was to upload files to S3 using signed URLs from S3, and checking the files' headers to determine whether or not it was, in fact, a text file. Further research has revealed that there is no fool proof way of doing this and a malicious user would be able to spoof a virus as a text file.\n","Decision":"Instead of using a signed URL and sending the file directly to S3, we will instead send the file to the backend and scan it with Clam AV before sending it to S3. In the event there is a virus, we will destroy the file on the backend immediately and return an error to the frontend.\\nBy using the [ClamAV REST server](https:\/\/github.com\/ajilaag\/clamav-rest) implementation we are able to scan files for viruses and malicious behavior. Additionally, Anti-Virus definitions are kept up to date automatically by use of the included [freshclam](https:\/\/www.clamav.net\/documents\/signature-testing-and-management#freshclam) tool which automatically downloads and updates an internal database of virus signatures using the official ClamAV source.\\nIn addition to this, the frontend is able to reliably determine if a file is a binary file. The client will check submitted files for this and immediately return an error to the user. These files will not be sent to the backend.\\n","tokens":72,"id":3797,"text":"## Context\\nOur original plan was to upload files to S3 using signed URLs from S3, and checking the files' headers to determine whether or not it was, in fact, a text file. Further research has revealed that there is no fool proof way of doing this and a malicious user would be able to spoof a virus as a text file.\\n\n\n##Decision\nInstead of using a signed URL and sending the file directly to S3, we will instead send the file to the backend and scan it with Clam AV before sending it to S3. In the event there is a virus, we will destroy the file on the backend immediately and return an error to the frontend.\\nBy using the [ClamAV REST server](https:\/\/github.com\/ajilaag\/clamav-rest) implementation we are able to scan files for viruses and malicious behavior. Additionally, Anti-Virus definitions are kept up to date automatically by use of the included [freshclam](https:\/\/www.clamav.net\/documents\/signature-testing-and-management#freshclam) tool which automatically downloads and updates an internal database of virus signatures using the official ClamAV source.\\nIn addition to this, the frontend is able to reliably determine if a file is a binary file. The client will check submitted files for this and immediately return an error to the user. These files will not be sent to the backend.\\n","Predictions":"We have decided to use basic authentication on the service in all environmentsnapart from the public facing one. This will only be applicable tonthe Rancher environment. The username and passwordnwill be the same as the password used by the user in thenpublic facing environment.n"}
{"File Name":"dos-server\/adr-2a-java.md","Context":"## Context\nAs DOS will be an important component of our collections platform,\nthe software stack (web server, application container, programming language, database)\nDOS uses to realize core functionality and to fulfill non-functional requirements\nof scalability, performance, reliability, security, and backward compatibility needs to be robust. Java can help meet various implicit non-functional requirements out of the box. Similarly, the use of proven and\nmature ecosystem libraries can help meet the functional requirements easily. The Spring\nframework also makes it relatively easy to create RESTful web services.\n","Decision":"The core service will rely on Java and Spring framework as the basic stack for implementing core services.\\n","tokens":114,"id":3374,"text":"## Context\\nAs DOS will be an important component of our collections platform,\\nthe software stack (web server, application container, programming language, database)\\nDOS uses to realize core functionality and to fulfill non-functional requirements\\nof scalability, performance, reliability, security, and backward compatibility needs to be robust. Java can help meet various implicit non-functional requirements out of the box. Similarly, the use of proven and\\nmature ecosystem libraries can help meet the functional requirements easily. The Spring\\nframework also makes it relatively easy to create RESTful web services.\\n\n\n##Decision\nThe core service will rely on Java and Spring framework as the basic stack for implementing core services.\\n","Predictions":"The decision is to allow the [React](https:\/\/reactjs.org) and [Angular](https:\/\/angular.io) frameworks in combination with the JavaScript and TypeScript programming languages to build the core views in the Kyma Console UI.n"}
{"File Name":"front-end-monorepo\/adr-21.md","Context":"## Context\nAuthentication is currently handled by the existing auth client, which is bundled up as part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/).\nWhile working on [#1306](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306), I ran into a few issues with it:\n- Environment variables (env vars) aren't available on the client side. Next.js does have a method for sharing config on both the server and client, but it's academic since:\n- The only way to configure `panoptes-javascript-client` is _directly_ via env vars, or by passing in query parameters to the URL. We can't get env vars on the client, so that's out, and query parameters become unwieldy very quickly.\nThis hasn't been an issue when working with Single Page Apps, since env vars are baked in at transpilation time by the build tools. At the moment, we just avoid the problem by having the staging build of the project use the production API.\nHowever, for the Next.js-based apps we're building right now, we need a different approach, and being able to configure the auth client from a single source of truth is required. Ideally, this would be source-agnostic: the client should be able to be configured from a config file, [env vars](https:\/\/12factor.net\/config), or whatever you want, but that's up to the consuming app to decide.\n","Decision":"Rewrite the existing auth client as a separate package. The new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.\\nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.\\nThe [existing config](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/blob\/master\/lib\/config.js) will be turned into a separate package. Config settings can be imported wholesale from that for convenience.\\nFor use in the rebuild apps, we would create a `ConfigStore` which we populate with the relevant config settings. The `ConfigStore` snapshot is then used to rehydrate the client, so we get a single source of truth on both client and server.\\n","tokens":315,"id":527,"text":"## Context\\nAuthentication is currently handled by the existing auth client, which is bundled up as part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/).\\nWhile working on [#1306](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306), I ran into a few issues with it:\\n- Environment variables (env vars) aren't available on the client side. Next.js does have a method for sharing config on both the server and client, but it's academic since:\\n- The only way to configure `panoptes-javascript-client` is _directly_ via env vars, or by passing in query parameters to the URL. We can't get env vars on the client, so that's out, and query parameters become unwieldy very quickly.\\nThis hasn't been an issue when working with Single Page Apps, since env vars are baked in at transpilation time by the build tools. At the moment, we just avoid the problem by having the staging build of the project use the production API.\\nHowever, for the Next.js-based apps we're building right now, we need a different approach, and being able to configure the auth client from a single source of truth is required. Ideally, this would be source-agnostic: the client should be able to be configured from a config file, [env vars](https:\/\/12factor.net\/config), or whatever you want, but that's up to the consuming app to decide.\\n\n\n##Decision\nRewrite the existing auth client as a separate package. The new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.\\nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.\\nThe [existing config](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/blob\/master\/lib\/config.js) will be turned into a separate package. Config settings can be imported wholesale from that for convenience.\\nFor use in the rebuild apps, we would create a `ConfigStore` which we populate with the relevant config settings. The `ConfigStore` snapshot is then used to rehydrate the client, so we get a single source of truth on both client and server.\\n","Predictions":"### AuthenticationnThe current auth client is bundled up as part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/).nThe new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.nThe [existing auth client](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306) is a separate package.nThe new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.nThe [existing auth client](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306) is a separate package.nThe new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.nThe [existing auth client](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306) is a separate package.nThe new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.nThe [existing auth client](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306) is a separate package.nThe new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.nThe [existing auth client](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306) is a separate package.nThe new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.nThe [existing auth client](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306) is a separate package.nThe new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.nThe [existing auth client](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306) is a separate package.nThe new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.nThe [existing auth client](https"}
{"File Name":"bfi-discovery\/0002-choice-of-auth-provider.md","Context":"## Context and Problem Statement\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\nidentity and access management solution must be provisioned which\nsupports user creation and registration, user management, and\nauthentication and authorisation.\n## Decision Drivers\n* Ease of initial deployment and configuration of the solution.\n* Ongoing hosting and maintenance costs of the solution.\n* Availability of core features to satisfy the project requirements with\nno \/ minimal costs.\n","Decision":"* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\nAuth0 is selected as the solution, due to its managed nature requiring\\nzero deployment and effort. After an evaluation of features compared\\nagainst the project requirements, it has been determined that Auth0's\\nfree tier will suffice and as a result there are no ongoing costs\\neither.\\n### Positive Consequences\\n* As a managed solution, there are no initial deployment or ongoing\\nhosting \/ infrastructure costs.\\n* Patching and maintenance of the solution is provided by Auth0 at no\\nadditional cost \/ interruption to the customer.\\n* It is expected that (at least initially) the free tier will suffice to\\nmeet the requirements of the project.\\n### Negative Consequences\\n* The features and customisations available are limited without\\nupgrading to a paid tier:\\n* Email templates cannot be customised on the free plan.\\n* A custom domain cannot be used to serve the Auth0 tenant.\\n* Enterprise integrations (AD, LDAP, etc) are not available.\\n* Multifactor authentication is not available.\\n* Auth0 hosted UI's can be customised but only to a limited extent.\\n* Limit of 7000 monthly active users.\\n","tokens":96,"id":4857,"text":"## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\n\n\n##Decision\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\nAuth0 is selected as the solution, due to its managed nature requiring\\nzero deployment and effort. After an evaluation of features compared\\nagainst the project requirements, it has been determined that Auth0's\\nfree tier will suffice and as a result there are no ongoing costs\\neither.\\n### Positive Consequences\\n* As a managed solution, there are no initial deployment or ongoing\\nhosting \/ infrastructure costs.\\n* Patching and maintenance of the solution is provided by Auth0 at no\\nadditional cost \/ interruption to the customer.\\n* It is expected that (at least initially) the free tier will suffice to\\nmeet the requirements of the project.\\n### Negative Consequences\\n* The features and customisations available are limited without\\nupgrading to a paid tier:\\n* Email templates cannot be customised on the free plan.\\n* A custom domain cannot be used to serve the Auth0 tenant.\\n* Enterprise integrations (AD, LDAP, etc) are not available.\\n* Multifactor authentication is not available.\\n* Auth0 hosted UI's can be customised but only to a limited extent.\\n* Limit of 7000 monthly active users.\\n","Predictions":"* Ease of initial deployment and configuration of the solution.n* Ongoing hosting and maintenance costs of the solution.n* Availability of core features to satisfy the project requirements withnno \/ minimal costs.nAuth0 is selected as the solution, due to its managed nature requiringnzero deployment and effort. After an evaluation of features comparednagainst the project requirements, it has been determined that Auth0'snfree tier will suffice and as a result there are no ongoing costsneither.n### Positive Consequencesn* As a managed solution, there are no initial deployment or ongoingnhosting \/ infrastructure costs.n* Patching and maintenance of the solution is provided by Auth0 at nonadditional cost \/ interruption to the customer.n* It is expected that (at least initially) the free tier will suffice tonmeet the requirements of the project.n### Negative Consequencesn* The features and customisations available are limited withoutnupgrading to a paid tier:n* Email templates cannot be customised on the free plan.n* A custom domain cannot be used to serve the Auth0 tenant.n* Enterprise integrations (AD, LDAP, etc) are not available.n* Multifactor authentication is not available.n* Auth0 hosted UI's can be customised but only to a limited extent.n* Limit of 7000 monthly active users.n"}
{"File Name":"copilot\/0003-use-mcp-api-for-pilot-config.md","Context":"## Context\nMesh Configuration Protocol (MCP) is a [protocol](https:\/\/github.com\/istio\/api\/tree\/master\/mcp) for transferring configuration among Istio components during runtime. MCP is meant to defer all the logics and complexities back to the server (copilot) as oppose to the original design which all the logic was embeded in the client (Pilot). Another goal of MCP is to create a unified contract for all the Custom Resource Definitions and Service Discovery and the way they are communicated with Pilot.\n","Decision":"Copilot will implement a MCP server to send configuration to Pilot. We will be sending definitions for Gateways, VirtualServices and DestinationRules over bi-directional GRPC.\\n","tokens":106,"id":49,"text":"## Context\\nMesh Configuration Protocol (MCP) is a [protocol](https:\/\/github.com\/istio\/api\/tree\/master\/mcp) for transferring configuration among Istio components during runtime. MCP is meant to defer all the logics and complexities back to the server (copilot) as oppose to the original design which all the logic was embeded in the client (Pilot). Another goal of MCP is to create a unified contract for all the Custom Resource Definitions and Service Discovery and the way they are communicated with Pilot.\\n\n\n##Decision\nCopilot will implement a MCP server to send configuration to Pilot. We will be sending definitions for Gateways, VirtualServices and DestinationRules over bi-directional GRPC.\\n","Predictions":"We will implement a MCP server to send configuration to Pilot. We will be sending definitions for Gateways, VirtualServices and DestinationRules over bi-directional GRPC.n"}
{"File Name":"content-data-api\/adr-006-track-metrics-via-time-dimension.md","Context":"## Context\nWe would benefit from having a central repository of integrated data from multiple sources that stores current and historical information, and use this data to create analytical reports and performance indicators to support the publishing workflow within GOV.UK.\nThis is actually difficult to achieve as we have the information dispersed across different applications that are currently designed to support transactional operations and not analytical reporting of integrated data.\n","Decision":"Build a data warehouse(*) that maintains a copy of the information of the transactional systems.\\n(*) We will be using a PostgreSQL database in the first iteration, we will be exploring other existing solutions for data warehouses in future iterations once we have validated this approach.\\n","tokens":80,"id":1863,"text":"## Context\\nWe would benefit from having a central repository of integrated data from multiple sources that stores current and historical information, and use this data to create analytical reports and performance indicators to support the publishing workflow within GOV.UK.\\nThis is actually difficult to achieve as we have the information dispersed across different applications that are currently designed to support transactional operations and not analytical reporting of integrated data.\\n\n\n##Decision\nBuild a data warehouse(*) that maintains a copy of the information of the transactional systems.\\n(*) We will be using a PostgreSQL database in the first iteration, we will be exploring other existing solutions for data warehouses in future iterations once we have validated this approach.\\n","Predictions":"We will use [Azure](https:\/\/azure.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and\/or enterprise artifacts with trail licenses).nWe\u2019d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we\u2019d prefer a SaaS solution.nThe REST API of Quay.io allows our organization to potentially automate user provisioning\/invitation and user\/group management which is not available for Docker Cloud at this time.nAdditionally, Quay \/ CoreOS seems strongly committed to their Enterprise \u2018on-prem\u2019 product while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product.n"}
{"File Name":"adr\/ADR-36-kernel-repository-separation.md","Context":"## Context and Problem Statement\nAs of today, we already decoupled the `renderer` (decentraland\/unity-renderer) from the `explorer` repository. And we are in the process of decoupling `website` (decentraland\/explorer-website). The ECS and building tools are also in the path of being migrated to their own repositories (decentraland\/js-sdk-toolchain).\nThe context of the problem is around the tactical\/execution aspect. Since the changes affect directly the publishing cycles and processes of every package. We must optimize not blocking development and also not breaking things in a way that prevent us from releasing hotfixes.\n","Decision":"We choose to create a separated repository to not block the releases and development of explorer during this migration stage.\\nAlso having separated repositories, will foster external contributions, ensuring clear scopes and responsibilities for each repository.\\n","tokens":135,"id":4609,"text":"## Context and Problem Statement\\nAs of today, we already decoupled the `renderer` (decentraland\/unity-renderer) from the `explorer` repository. And we are in the process of decoupling `website` (decentraland\/explorer-website). The ECS and building tools are also in the path of being migrated to their own repositories (decentraland\/js-sdk-toolchain).\\nThe context of the problem is around the tactical\/execution aspect. Since the changes affect directly the publishing cycles and processes of every package. We must optimize not blocking development and also not breaking things in a way that prevent us from releasing hotfixes.\\n\n\n##Decision\nWe choose to create a separated repository to not block the releases and development of explorer during this migration stage.\\nAlso having separated repositories, will foster external contributions, ensuring clear scopes and responsibilities for each repository.\\n","Predictions":"We choose to create a separated repository to not block the releases and development of explorer during this migration stage.nAlso having separated repositories, will foster external contributions, ensuring clear scopes and responsibilities for each repository.n"}
{"File Name":"gsp\/ADR011-build-artefacts.md","Context":"## Context\nAs part of our pipelines we will be building artefacts that will be used to test\nand deploy our applications. We will be deploying applications to Kubernetes. We\nwill need to build a container image of some kind.\nThere are some competing container image formats, namely:\n* [OCI]\n* [ACI]\nThe OCI image format is [based on the Docker v2][oci-standard] image format.\nThe Kubernetes project appears to [prefer Docker\/OCI][k8s-preferance] images\nover ACI.\n[rkt is moving to OCI][rkt-oci] and away from ACI. OCI will become the preferred\nimage format.\nDocker has wide industry adoption and appears to have wide understanding within\nGDS.\nDocker is the default container runtime for Kubernetes.\n","Decision":"We will build and store OCI images built using Docker.\\n","tokens":170,"id":3920,"text":"## Context\\nAs part of our pipelines we will be building artefacts that will be used to test\\nand deploy our applications. We will be deploying applications to Kubernetes. We\\nwill need to build a container image of some kind.\\nThere are some competing container image formats, namely:\\n* [OCI]\\n* [ACI]\\nThe OCI image format is [based on the Docker v2][oci-standard] image format.\\nThe Kubernetes project appears to [prefer Docker\/OCI][k8s-preferance] images\\nover ACI.\\n[rkt is moving to OCI][rkt-oci] and away from ACI. OCI will become the preferred\\nimage format.\\nDocker has wide industry adoption and appears to have wide understanding within\\nGDS.\\nDocker is the default container runtime for Kubernetes.\\n\n\n##Decision\nWe will build and store OCI images built using Docker.\\n","Predictions":"We will use the [ACI][ACI][ACI] for our CI\/CD pipelines.n"}
{"File Name":"clone_difitalcitizenship\/0003-follow-a-cloud-native-design-principle.md","Context":"## Context\nWe need to design the building blocks of the Digital Citizenship platform. We will make decisions about the overall architecture and the building blocks we're going to use to satisfy the functional and non-functional requirements.\nWhen deciding what technologies to use and how to satisfy those requirements, we're going to evaluate whether we want to build certain components from scratch, use open-source solutions managed by us or rely on existing public cloud components, managed by 3rd party providers.\nAs the world of cloud technologies continues to accelerate, we should absorb new developments into how we work. Leading technology organisations are rapidly embracing new tools like \"serverless\" computing.\nAt the infrastructure and application level we should expect our applications to be resilient, flexible and API-driven. We should have the tools and practices in place to manage and secure a distributed range of tools accessed over the internet.\nDecision criteria will also include:\n* elasticity and resilience\n* pay-as-you-go pricing\n* exploiting economies of scale\n* exploiting falling costs of infrastructure over time\n* quality of management tools\n* best of breed security\n* flexibility and opportunity costs\n","Decision":"* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\nWe decide to build the components of the Digital Citizenship initiative on top of scalable, managed and cost-effective components provided by the leading public cloud providers.\\n","tokens":227,"id":1182,"text":"## Context\\nWe need to design the building blocks of the Digital Citizenship platform. We will make decisions about the overall architecture and the building blocks we're going to use to satisfy the functional and non-functional requirements.\\nWhen deciding what technologies to use and how to satisfy those requirements, we're going to evaluate whether we want to build certain components from scratch, use open-source solutions managed by us or rely on existing public cloud components, managed by 3rd party providers.\\nAs the world of cloud technologies continues to accelerate, we should absorb new developments into how we work. Leading technology organisations are rapidly embracing new tools like \"serverless\" computing.\\nAt the infrastructure and application level we should expect our applications to be resilient, flexible and API-driven. We should have the tools and practices in place to manage and secure a distributed range of tools accessed over the internet.\\nDecision criteria will also include:\\n* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\n\n\n##Decision\n* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\nWe decide to build the components of the Digital Citizenship initiative on top of scalable, managed and cost-effective components provided by the leading public cloud providers.\\n","Predictions":"* elasticity and resiliencen* pay-as-you-go pricingn* exploiting economies of scalen* exploiting falling costs of infrastructure over timen* quality of management toolsn* best of breed securityn* flexibility and opportunity costsnWe decide to build the components of the Digital Citizen initiative on top of scalable, managed and cost-effective components provided by the leading public cloud providers.n"}
{"File Name":"openjdk-api-v3\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4699,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"openfido-app-service\/0002-project-structure.md","Context":"## Context\nThe OpenFIDO project will mediate access to a workflow service that runs\nGridLabD jobs. The frontend React application will need access to these\nresources, but only if they have permission to access via an organization.\n","Decision":"Create a Flask Rest service - since the other microservices in this suite of\\nservices are also Flask based, keep this using the same kind of infrastructure\\nso that common utilities can be shared (openfido-utils) and the maintenance will\\nbe simplified.\\nOrganize the database logic into a simplified CQRS-inspired style code\\nstructure. Since we anticipate many conceptual resources, each resource will\\nhave its own module:\\n* app\/RESOURCE\/models.py - contains all models for RESOURCE.\\n* app\/RESOURCE\/routes.py - contains all Rest routes specific to RESOURCE.\\n* app\/RESOURCE\/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\\n* app\/RESOURCE\/services.py - all db commands that modify database state.\\n* app\/RESOURCE\/queries.py - all db queries to the database.\\nAdditional libraries we anticipate using:\\n* [marshmallow](https:\/\/marshmallow.readthedocs.io\/en\/stable\/) will be used since there are many rest api endpoints that will take nested bodies.\\n* [alembic](https:\/\/alembic.sqlalchemy.org\/en\/latest\/) to manage database schema migrations.\\nOther thoughts on design:\\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\\nUse the structure of the [openfido-workflow-service](https:\/\/github.com\/slacgismo\/openfido-workflow-service) project as a reference.\\n","tokens":49,"id":2470,"text":"## Context\\nThe OpenFIDO project will mediate access to a workflow service that runs\\nGridLabD jobs. The frontend React application will need access to these\\nresources, but only if they have permission to access via an organization.\\n\n\n##Decision\nCreate a Flask Rest service - since the other microservices in this suite of\\nservices are also Flask based, keep this using the same kind of infrastructure\\nso that common utilities can be shared (openfido-utils) and the maintenance will\\nbe simplified.\\nOrganize the database logic into a simplified CQRS-inspired style code\\nstructure. Since we anticipate many conceptual resources, each resource will\\nhave its own module:\\n* app\/RESOURCE\/models.py - contains all models for RESOURCE.\\n* app\/RESOURCE\/routes.py - contains all Rest routes specific to RESOURCE.\\n* app\/RESOURCE\/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\\n* app\/RESOURCE\/services.py - all db commands that modify database state.\\n* app\/RESOURCE\/queries.py - all db queries to the database.\\nAdditional libraries we anticipate using:\\n* [marshmallow](https:\/\/marshmallow.readthedocs.io\/en\/stable\/) will be used since there are many rest api endpoints that will take nested bodies.\\n* [alembic](https:\/\/alembic.sqlalchemy.org\/en\/latest\/) to manage database schema migrations.\\nOther thoughts on design:\\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\\nUse the structure of the [openfido-workflow-service](https:\/\/github.com\/slacgismo\/openfido-workflow-service) project as a reference.\\n","Predictions":"We will use the [Semantic Query Language](https:\/\/semantic-search.org\/) library to query the Query Language.n"}
{"File Name":"buildit-all\/0008-database.md","Context":"## Context\nBookit needs to persist the locations, bookables, and bookings so that the data survives multiple instances and deployments over time.\n","Decision":"* Use SQL approach as opposed to NoSQL solution - the model is simple and ACID transactions keep multiple users separate\\n* Use H2 for unit testing & local development - speeds up execution time and reduces external dependencies\\n* Use AWS RDS Aurora (MySQL) for integration\/staging\/production - better HA & continuous snapshots (enabled for production)\\n* Use MariaDB JDBC driver - has native Aurora support for failover\\n","tokens":29,"id":4415,"text":"## Context\\nBookit needs to persist the locations, bookables, and bookings so that the data survives multiple instances and deployments over time.\\n\n\n##Decision\n* Use SQL approach as opposed to NoSQL solution - the model is simple and ACID transactions keep multiple users separate\\n* Use H2 for unit testing & local development - speeds up execution time and reduces external dependencies\\n* Use AWS RDS Aurora (MySQL) for integration\/staging\/production - better HA & continuous snapshots (enabled for production)\\n* Use MariaDB JDBC driver - has native Aurora support for failover\\n","Predictions":"We will use a Security database to store the configuration data.n"}
{"File Name":"deeplearning4j\/0003-dealing_with_inconsistencies_in_java_naming.md","Context":"## Context\nThere are slight inconsistencies in naming between existing op class definitions and factory methods. For example a\nfactory method called `bernoulli` in the `random` namespace with a corresponding op class called\n`BernoulliDistribution`.\nTwo possible solutions where suggested:\n1. Add an additional property that provides us with the correct class name\n2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses\n","Decision":"For now we will introduce a `javaOpClass` property which in cases of inconsistency provides us with the correct class\\nname.\\n","tokens":92,"id":2934,"text":"## Context\\nThere are slight inconsistencies in naming between existing op class definitions and factory methods. For example a\\nfactory method called `bernoulli` in the `random` namespace with a corresponding op class called\\n`BernoulliDistribution`.\\nTwo possible solutions where suggested:\\n1. Add an additional property that provides us with the correct class name\\n2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses\\n\n\n##Decision\nFor now we will introduce a `javaOpClass` property which in cases of inconsistency provides us with the correct class\\nname.\\n","Predictions":"* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.n* The feature configuration from Feature::getAll() is converted to a SCSS map inside ShopwareStorefrontThemeThemeCompiler::getFeatureConfigScssMap.n* This SCSS map is always added to the SCSS string which gets processed by ShopwareStorefrontThemeThemeCompiler::compileTheme.n* For webpack hot-proxy the var\/config_js_features.json is used instead.n* The SCSS map looks like this: $sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);n"}
{"File Name":"dotfiles\/0002-replace-travis-ci-with-github-actions.md","Context":"## Context\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\ngoing to be generally available on\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\nThey are a thing that will have a long term future. It is likely GitHub Actions\nwill become the default CI mechanism (and possibly more) for projects hosted on\nGitHub. Using them in this repository which has a very basic use case will\nprovide some exposure to the service.\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":116,"id":118,"text":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nThey are a thing that will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repository which has a very basic use case will\\nprovide some exposure to the service.\\n\n\n##Decision\nThe decision is to replace Travis CI with GitHub Actions.\\n","Predictions":"The decision is to use AWS Lambda for this repository.n"}
{"File Name":"signals-frontend\/0009-stop-mocking-fetch-in-tests.md","Context":"## Context\nFor mocking unittests we are using at this moment the `jest-fetch-mock` package. This works but there are new develoments on the market. As the creator of the [Testing Library](https:\/\/testing-library.com\/) suggests in this [article](https:\/\/kentcdodds.com\/blog\/stop-mocking-fetch), there is a new strategy available that would simplify testing of components that are making fetch calls.\n# Technical description\nThe `msw` server intercepts the fetch calls and returns the configured response. This is done outside the test file and only overrides of the standard responses from the default configuration need to be added to the respective test file. [Here is the documentation](https:\/\/github.com\/mswjs\/msw).\n","Decision":"The mocking will be done as much as possible by using the the `msw` package.\\n","tokens":156,"id":3050,"text":"## Context\\nFor mocking unittests we are using at this moment the `jest-fetch-mock` package. This works but there are new develoments on the market. As the creator of the [Testing Library](https:\/\/testing-library.com\/) suggests in this [article](https:\/\/kentcdodds.com\/blog\/stop-mocking-fetch), there is a new strategy available that would simplify testing of components that are making fetch calls.\\n# Technical description\\nThe `msw` server intercepts the fetch calls and returns the configured response. This is done outside the test file and only overrides of the standard responses from the default configuration need to be added to the respective test file. [Here is the documentation](https:\/\/github.com\/mswjs\/msw).\\n\n\n##Decision\nThe mocking will be done as much as possible by using the the `msw` package.\\n","Predictions":"We will use the [ADR-007](007-adr-007-steps) format for the workflow resource attribute.n"}
{"File Name":"tracking-consent-frontend\/0005-use-data-attribute-for-language.md","Context":"## Context and Problem Statement\nTracking consent reads the PLAY_LANG cookie to determine whether to render the banner in Welsh. PLAY_LANG is the cookie\nused by the Play Framework to persist a user's language preferences. It will be set to 'cy'\nwhen a user has selected Welsh using the language toggle in MDTP services using the Play Framework.\nTeams are increasingly setting PLAY_LANG to HttpOnly in an attempt to get green ZAP tests, even though there are no\nknown security concerns around keeping PLAY_LANG as a normal cookie. Setting a cookie to\nHttpOnly makes it unreadable within the client-side Javascript code that renders the tracking consent banner. The result\nof this is that the banner will not be translated into Welsh for these services.\nA related issue is that PLAY_LANG is not set for classic services written in Java, which means a Welsh version of the banner is not\ncurrently available for classic services.\nIt is worth noting that the only other known instance of our reading PLAY_LANG using Javascript is in the assets-frontend\n[timeout dialog](https:\/\/github.com\/hmrc\/assets-frontend\/blob\/97c638289e23bee255ac30724a8572c6efa96817\/assets\/patterns\/help-users-when-we-time-them-out-of-a-service\/timeoutDialog.js#L14) timeout dialog. All the new govuk-frontend and hmrc-frontend components use data attributes instead.\nShould we remove the reading of PLAY_LANG in tracking consent and accept a data-language attribute instead?\n## Decision Drivers\n* The need to support classic services\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\navoid services having to upgrade a second time.\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\n* The preference for consistency in language settings across our frontend components\n","Decision":"* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\nChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'\\ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG into Zap tests. We also agreed that our frontend\\ncomponents should be consistent in their treatment of language until such time as we are able to provide an\\nalternative approach that works for all components.\\n### Positive Consequences\\n* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner\\n* Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings\/errors\\n* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etc\\n### Negative Consequences\\n* We will need to make a small change to tracking consent and communicate this change to teams\\n* Teams will need to upgrade to the latest version of play-ui\/play-frontend-hmrc to get language translations\\n* Teams not using the tracking consent helpers will need to add a data-attribute for the language\\n","tokens":409,"id":3526,"text":"## Context and Problem Statement\\nTracking consent reads the PLAY_LANG cookie to determine whether to render the banner in Welsh. PLAY_LANG is the cookie\\nused by the Play Framework to persist a user's language preferences. It will be set to 'cy'\\nwhen a user has selected Welsh using the language toggle in MDTP services using the Play Framework.\\nTeams are increasingly setting PLAY_LANG to HttpOnly in an attempt to get green ZAP tests, even though there are no\\nknown security concerns around keeping PLAY_LANG as a normal cookie. Setting a cookie to\\nHttpOnly makes it unreadable within the client-side Javascript code that renders the tracking consent banner. The result\\nof this is that the banner will not be translated into Welsh for these services.\\nA related issue is that PLAY_LANG is not set for classic services written in Java, which means a Welsh version of the banner is not\\ncurrently available for classic services.\\nIt is worth noting that the only other known instance of our reading PLAY_LANG using Javascript is in the assets-frontend\\n[timeout dialog](https:\/\/github.com\/hmrc\/assets-frontend\/blob\/97c638289e23bee255ac30724a8572c6efa96817\/assets\/patterns\/help-users-when-we-time-them-out-of-a-service\/timeoutDialog.js#L14) timeout dialog. All the new govuk-frontend and hmrc-frontend components use data attributes instead.\\nShould we remove the reading of PLAY_LANG in tracking consent and accept a data-language attribute instead?\\n## Decision Drivers\\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\n\n\n##Decision\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\nChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'\\ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG into Zap tests. We also agreed that our frontend\\ncomponents should be consistent in their treatment of language until such time as we are able to provide an\\nalternative approach that works for all components.\\n### Positive Consequences\\n* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner\\n* Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings\/errors\\n* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etc\\n### Negative Consequences\\n* We will need to make a small change to tracking consent and communicate this change to teams\\n* Teams will need to upgrade to the latest version of play-ui\/play-frontend-hmrc to get language translations\\n* Teams not using the tracking consent helpers will need to add a data-attribute for the language\\n","Predictions":"* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, tonavoid services having to upgrade a second time.n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.n* The preference for not advising teams to add exemption.n* The preference for not advising teams to add exemption.n"}
{"File Name":"compliantkubernetes\/0013-configure-alerts-in-omt.md","Context":"## Context and Problem Statement\nAlerts are some noteworthy IT event, like a Node becoming un-ready, login failure or a disk getting full.\nTerminology differs across tooling and organizations, but one generally cares about:\n* P1 (critical) alerts, which require immediate human attention -- the person on-call needs to be notified immediately -- and;\n* P2 (high) alerts which require human attention with 24 hours -- the person on-call needs to be notified next morning;\n* P3 (moderate) alerts which do not require immediate human attention, but should be regularly reviewed.\nOther priorities (e.g., P4 and below) are generally used for informational purposes.\nDealing with alerts correctly entails prioritizing them (e.g., P1, P2, P3), deciding if someone should be notified, who should be notified, how they should be notified (e.g., SMS or email) and when.\n\"Who\", \"how\" and \"when\" should include escalation, if the previous notification was not acknowledged within a pre-configured time interval, then the same person if notified via a different channel or a new person is notified.\nUnder-alerting -- e.g., notifying an on-call person too late -- may lead to Service Level Agreement (SLA) violations and a general feeling of administrator anxiety: \"Is everything okay, or is alerting not working?\".\nOver-alerting -- e.g., notifying a person too often about low-priority alerts -- leads to alert fatigue and \"crying wolf\" where even important alerts are eventually ignored.\nHence, configuring the right level of alerting -- in particular notifications -- is extremely important both for SLA fulfillment and a happy on-call team.\nWhere should alerting be configured, so as to quickly converge to the optimal alerting level?\n## Decision Drivers\n* Allow to quickly silence, un-silence and re-prioritize alerts.\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\n* Leverage existing tools and processes.\n","Decision":"* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\nChosen option: Compliant Kubernetes \u201cover-alerts\u201d, i.e., forwards all alerts and all relevant information to an On-Call Management Tool (OMT, e.g., Opsgenie).\\nConfiguration of alerts happens in the OMT.\\n### Positive Consequences\\n* Clear separation of concerns.\\n* Alerting does not require per-customer configuration of Compliant Kubernetes.\\n* Leverages existing tools and processes.\\n* We do not need to implement complex alert filtering in Compliant Kubernetes, e.g., silence alerts during maintenance windows, silence alerts during Swedish holidays, etc.\\n### Negative Consequences\\n* Does not capture alerting know-how in Compliant Kubernetes.\\n* Migration to a new OMT means all alerting configuration needs to be migrated to the new tool. Fortunately, this can be done incrementally.\\n","tokens":445,"id":3110,"text":"## Context and Problem Statement\\nAlerts are some noteworthy IT event, like a Node becoming un-ready, login failure or a disk getting full.\\nTerminology differs across tooling and organizations, but one generally cares about:\\n* P1 (critical) alerts, which require immediate human attention -- the person on-call needs to be notified immediately -- and;\\n* P2 (high) alerts which require human attention with 24 hours -- the person on-call needs to be notified next morning;\\n* P3 (moderate) alerts which do not require immediate human attention, but should be regularly reviewed.\\nOther priorities (e.g., P4 and below) are generally used for informational purposes.\\nDealing with alerts correctly entails prioritizing them (e.g., P1, P2, P3), deciding if someone should be notified, who should be notified, how they should be notified (e.g., SMS or email) and when.\\n\"Who\", \"how\" and \"when\" should include escalation, if the previous notification was not acknowledged within a pre-configured time interval, then the same person if notified via a different channel or a new person is notified.\\nUnder-alerting -- e.g., notifying an on-call person too late -- may lead to Service Level Agreement (SLA) violations and a general feeling of administrator anxiety: \"Is everything okay, or is alerting not working?\".\\nOver-alerting -- e.g., notifying a person too often about low-priority alerts -- leads to alert fatigue and \"crying wolf\" where even important alerts are eventually ignored.\\nHence, configuring the right level of alerting -- in particular notifications -- is extremely important both for SLA fulfillment and a happy on-call team.\\nWhere should alerting be configured, so as to quickly converge to the optimal alerting level?\\n## Decision Drivers\\n* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\n\n\n##Decision\n* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\nChosen option: Compliant Kubernetes \u201cover-alerts\u201d, i.e., forwards all alerts and all relevant information to an On-Call Management Tool (OMT, e.g., Opsgenie).\\nConfiguration of alerts happens in the OMT.\\n### Positive Consequences\\n* Clear separation of concerns.\\n* Alerting does not require per-customer configuration of Compliant Kubernetes.\\n* Leverages existing tools and processes.\\n* We do not need to implement complex alert filtering in Compliant Kubernetes, e.g., silence alerts during maintenance windows, silence alerts during Swedish holidays, etc.\\n### Negative Consequences\\n* Does not capture alerting know-how in Compliant Kubernetes.\\n* Migration to a new OMT means all alerting configuration needs to be migrated to the new tool. Fortunately, this can be done incrementally.\\n","Predictions":"* Allow to quickly silence, un-silence and re-prioritize alerts.n* Allow arbitrary flexibility, e.g., who should be notified, when, who should be notified, and when.n* Allow arbitrary flexibility, e.g., who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be notified, when, who should be "}
{"File Name":"adr\/ADR-5-how-to-organize-adr-files.md","Context":"## Context and Problem Statement\n- How do we call the folder for assets?\n- What is the pattern of the filename where an ADR is stored?\n- How do we write titles in the ADR documents?\n","Decision":"### Asset folder\\nChosen option: `resources`\\n### Filename\\nChosen option: `docs\/ADR-NNNN-title-using-dashes.md` and `docs\/resources\/ADR-NNNN\/{filename}`, because\\n- Adding `arch` or `adr` folders is redundant.\\n- `NNNN` provides a unique number, which can be used for referencing using an immutable number (and generating permalinks if we render the repository to a static site).\\n- The creation time of an ADR is of historical interest only, if it gets updated somehow. It can be embeded in the file or extracted from git history.\\n- Having no spaces in filenames eases working in the command line.\\n- Prefixing with `docs\/ADR-` enables future new kinds of documents.\\n- Calling a folder `docs\/resources\/ADR-NNNN\/{filename}` (omiting the `title-with-dashes`) helps in refactors or changes in the file names.\\n### Title (markdown)\\nChosen option: `# Plain title` because:\\n- At the moment of writing the ADR the writer may not know the final ADR number.\\n- It can be embeded by rendering tools\\n- It would duplicate data from the filename\\n","tokens":46,"id":4619,"text":"## Context and Problem Statement\\n- How do we call the folder for assets?\\n- What is the pattern of the filename where an ADR is stored?\\n- How do we write titles in the ADR documents?\\n\n\n##Decision\n### Asset folder\\nChosen option: `resources`\\n### Filename\\nChosen option: `docs\/ADR-NNNN-title-using-dashes.md` and `docs\/resources\/ADR-NNNN\/{filename}`, because\\n- Adding `arch` or `adr` folders is redundant.\\n- `NNNN` provides a unique number, which can be used for referencing using an immutable number (and generating permalinks if we render the repository to a static site).\\n- The creation time of an ADR is of historical interest only, if it gets updated somehow. It can be embeded in the file or extracted from git history.\\n- Having no spaces in filenames eases working in the command line.\\n- Prefixing with `docs\/ADR-` enables future new kinds of documents.\\n- Calling a folder `docs\/resources\/ADR-NNNN\/{filename}` (omiting the `title-with-dashes`) helps in refactors or changes in the file names.\\n### Title (markdown)\\nChosen option: `# Plain title` because:\\n- At the moment of writing the ADR the writer may not know the final ADR number.\\n- It can be embeded by rendering tools\\n- It would duplicate data from the filename\\n","Predictions":"We decided to use docs\/resources\/ADR-NNNN-title-using-dashes.md instead of docs\/resources\/ADR-NNNN\/filename, becausen- Adding arch or adr folders is redundant.n- NNNN provides a unique number, which can be used for referencing using an immutable number (and generating permalinks if we render the repository to a static site).n- The creation time of an ADR is of historical interest only, if it gets updated somehow. It can be embeded in the file or extracted from git history.n- Having no spaces in filenames eases working in the command line.n- Prefixing with docs\/ADR- enables future new kinds of documents.n### Title (markdown)nChosen option: # Plain title because:n- At the moment of writing the ADR the writer may not know the final ADR number.n- It can be embeded by rendering toolsn- It would duplicate data from the filenamen"}
{"File Name":"james-project\/0025-cassandra-blob-store-cache.md","Context":"## Context\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\nAs an example:\n- Mailbox message headers are expected to be small and frequently accessed\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\n","Decision":"Implement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","tokens":160,"id":2896,"text":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n\n\n##Decision\nImplement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","Predictions":"Implement a write through cache to have better read latency for smaller objects.nSuch a cache needs to be distributed in order to be more efficient.nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,nand be queried with a consistency level of ONE.nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if thenblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.nFailure to read the cache, or cache miss will result in a read in the object storage.n"}
{"File Name":"datalab\/0036-replace-kong-with-nginx-ingress-controller.md","Context":"## Context\nWe have reached a point where we need to secure third party web applications that provide\nno security of their own. The Kong proxy does not offer a mechanism for this in the open\nsource version and we haven't had and response from our efforts to contact them.\nWe believe that the Nginx Ingress controller that has been available since Kubernetes 1.7\nwas released provides a Kubernetes native route for the same functionality.\n","Decision":"We have decided the replace the Kong proxy with an Nginx Ingress Controller in the\\nKubernetes cluster and an Nginx load balancer running on the proxy servers.\\nThis should provide all of the same functionality as Kong and in addition should provide\\na mechanism for token and cookie authentication using the `auth_url` annotation that\\nwraps the underlying Nginx `auth_request` module.\\n","tokens":91,"id":733,"text":"## Context\\nWe have reached a point where we need to secure third party web applications that provide\\nno security of their own. The Kong proxy does not offer a mechanism for this in the open\\nsource version and we haven't had and response from our efforts to contact them.\\nWe believe that the Nginx Ingress controller that has been available since Kubernetes 1.7\\nwas released provides a Kubernetes native route for the same functionality.\\n\n\n##Decision\nWe have decided the replace the Kong proxy with an Nginx Ingress Controller in the\\nKubernetes cluster and an Nginx load balancer running on the proxy servers.\\nThis should provide all of the same functionality as Kong and in addition should provide\\na mechanism for token and cookie authentication using the `auth_url` annotation that\\nwraps the underlying Nginx `auth_request` module.\\n","Predictions":"We have decided the replace the Kong proxy with an Nginx Ingress Controller in thenKubernetes cluster and an Nginx load balancer running on the proxy servers.nThis should provide all of the same functionality as Kong and in addition should providena mechanism for token and cookie authentication using the auth_url annotation thatnwraps the underlying Nginx auth_request module.n"}
{"File Name":"buildit-all\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4428,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"dl4j-dev-tools\/0006-op_specific_enums.md","Context":"## Context\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\nmakes usage and documentation easier.\n","Decision":"We allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values\\nfor this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from\\n`0`.\\nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default\\nvalues match one of the possible values (if applicable).\\nOn code generation, an appropriate representation of this enum will be generated in the target language. The name of\\nthe generated enum will be derived from the name of the arg.\\n### Example\\n```kotlin\\nArg(ENUM, \"padMode\"){\\npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")\\ndescription = \"padding mode\"\\n}\\n```\\n","tokens":31,"id":994,"text":"## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n\n\n##Decision\nWe allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values\\nfor this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from\\n`0`.\\nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default\\nvalues match one of the possible values (if applicable).\\nOn code generation, an appropriate representation of this enum will be generated in the target language. The name of\\nthe generated enum will be derived from the name of the arg.\\n### Example\\n```kotlin\\nArg(ENUM, \"padMode\"){\\npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")\\ndescription = \"padding mode\"\\n}\\n```\\n","Predictions":"We allow Arg sections to have an ENUM data type and add a possibleValues property to define the possible valuesnfor this arg. The ordinal number of the enum is the same as its position within the possibleValues list starting fromn0.nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that defaultnvalues match one of the possible values (if applicable).nOn code generation, an appropriate representation of this enum will be generated in the target language. The name ofnthe generated enum will be derived from the name of the arg.n### ExamplenkotlinnArg(ENUM, \"padMode\")npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")ndescription = \"padding mode\"nn"}
{"File Name":"govuk-design-system-architecture\/001-javascript-for-less-capable-browsers.md","Context":"## Context\nBefore GOV.UK Frontend, our projects used jQuery for DOM interactions, events and data manipulation.\nWe\u2019re taking a step back from jQuery due to its lack of support for the browsers we support, its large file size, lack of security updates and from conversations with the community.\n","Decision":"We\u2019re now writing standard ES5 JavaScript instead, that we polyfill where necessary.\\nThis means that in places where we would have previously used [`$.each`](http:\/\/api.jquery.com\/jquery.each\/) we\u2019re using [`.forEach`](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Array\/forEach) instead, and polyfilling the missing gaps.\\nWe use polyfills provided by the Financial Times\u2019 [Polyfill service](https:\/\/polyfill.io).\\nThis approach ensures that multiple polyfills can be sourced from this service with greater confidence that they\u2019ll work without conflicting with each other.\\nThe Polyfill service does not do runtime detection in browsers and instead opts to do this on the server via user-agent sniffing. It only ships the code needed for that browser, which means newer browsers don\u2019t have to run anything. We may investigate lazy-loading in the future, but for now we\u2019re using a bundled approach based on the lowest common denominator.\\nWe are vendoring these polyfills to avoid any [single point of failure](https:\/\/en.wikipedia.org\/wiki\/Single_point_of_failure) issues that could arise from relying on a CDN. By doing this, we can detect if polyfills are needed at runtime, which results in all browsers getting the same polyfill bundle.\\nWe hope that our approach can be automated or moved into a reusable npm package, based on the Financial Times [npm package](https:\/\/github.com\/Financial-Times\/polyfill-service#library).\\nHere is an [example of polyfilling `addEventListener`](https:\/\/github.com\/alphagov\/govuk-frontend\/blob\/master\/docs\/polyfilling.md).\\nAny polyfills included in GOV.UK Frontend will be tested to work in supported browsers and devices, including assistive technology. Any community discussion and documentation around potential bugs or downsides will also be considered before deciding to include a polyfill in GOV.UK Frontend.\\n","tokens":62,"id":3806,"text":"## Context\\nBefore GOV.UK Frontend, our projects used jQuery for DOM interactions, events and data manipulation.\\nWe\u2019re taking a step back from jQuery due to its lack of support for the browsers we support, its large file size, lack of security updates and from conversations with the community.\\n\n\n##Decision\nWe\u2019re now writing standard ES5 JavaScript instead, that we polyfill where necessary.\\nThis means that in places where we would have previously used [`$.each`](http:\/\/api.jquery.com\/jquery.each\/) we\u2019re using [`.forEach`](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Array\/forEach) instead, and polyfilling the missing gaps.\\nWe use polyfills provided by the Financial Times\u2019 [Polyfill service](https:\/\/polyfill.io).\\nThis approach ensures that multiple polyfills can be sourced from this service with greater confidence that they\u2019ll work without conflicting with each other.\\nThe Polyfill service does not do runtime detection in browsers and instead opts to do this on the server via user-agent sniffing. It only ships the code needed for that browser, which means newer browsers don\u2019t have to run anything. We may investigate lazy-loading in the future, but for now we\u2019re using a bundled approach based on the lowest common denominator.\\nWe are vendoring these polyfills to avoid any [single point of failure](https:\/\/en.wikipedia.org\/wiki\/Single_point_of_failure) issues that could arise from relying on a CDN. By doing this, we can detect if polyfills are needed at runtime, which results in all browsers getting the same polyfill bundle.\\nWe hope that our approach can be automated or moved into a reusable npm package, based on the Financial Times [npm package](https:\/\/github.com\/Financial-Times\/polyfill-service#library).\\nHere is an [example of polyfilling `addEventListener`](https:\/\/github.com\/alphagov\/govuk-frontend\/blob\/master\/docs\/polyfilling.md).\\nAny polyfills included in GOV.UK Frontend will be tested to work in supported browsers and devices, including assistive technology. Any community discussion and documentation around potential bugs or downsides will also be considered before deciding to include a polyfill in GOV.UK Frontend.\\n","Predictions":"We're now writing standard ES5 JavaScript instead, that we polyfill where necessary.nThis means that in places where we would have previously used [$.each](http:\/\/api.jquery.com\/jquery.each\/) we\u2019re using [.forEach](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Array\/forEach) instead, and polyfilling the missing gaps.nWe use polyfills provided by the Financial Times\u2019 [Polyfill service](https:\/\/polyfill.io).nThis approach ensures that multiple polyfills can be sourced from this service with greater confidence that they\u2019ll work without conflicting with each other.nThe Polyfill service does not do runtime detection in browsers and instead opts to do this on the server via user-agent sniffing. It only shipped the code needed for that browser, which means newer browsers don\u2019t have to run anything. We may investigate lazy-loading in the future, but for now we\u2019re using a bundled approach based on the lowest common denominator.nWe are vendoring these polyfills to avoid any [single point of failure](https:\/\/en.wikipedia.org\/wiki\/Single_point_of_failure) issues that could arise from relying on a CDN. By doing this, we can detect if polyfills are needed at runtime, which results in all browsers getting the same polyfill bundle.nWe hope that our approach can be automated or moved into a reusable npm package, based on the Financial Times [npm package](https:\/\/github.com\/Financial-Times\/polyfill-service#library).nHere is an example of how we're using this approach:njQuerynnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"service-stac\/2020_10_21_static_asset.md","Context":"## Context\n`service-stac` needs to serve some static assets for the admin pages (css, images, icons, ...). Django is not appropriate to serve static files on production environment. Currently Django is served directly by `gunicorn`. As a good practice to avoid issue with slow client and to avoid Denial of Service attacks, `gunicorn` should be served behind a Reversed proxy (e.g. Apache or Nginx).\n","Decision":"Because it is to us not clear yet if a Reverse Proxy is really necessary for our Architecture (CloudFront with Kubernetes Ingress), we decided to use WhiteNoise for static assets. This middleware seems to performs well with CDN (like CloudFront) therefore we will use it to serve static files as it is very simple to uses and take care of compressing and settings corrects Headers for caching.\\n","tokens":91,"id":3173,"text":"## Context\\n`service-stac` needs to serve some static assets for the admin pages (css, images, icons, ...). Django is not appropriate to serve static files on production environment. Currently Django is served directly by `gunicorn`. As a good practice to avoid issue with slow client and to avoid Denial of Service attacks, `gunicorn` should be served behind a Reversed proxy (e.g. Apache or Nginx).\\n\n\n##Decision\nBecause it is to us not clear yet if a Reverse Proxy is really necessary for our Architecture (CloudFront with Kubernetes Ingress), we decided to use WhiteNoise for static assets. This middleware seems to performs well with CDN (like CloudFront) therefore we will use it to serve static files as it is very simple to uses and take care of compressing and settings corrects Headers for caching.\\n","Predictions":"We will use stack to store the config file.n"}
{"File Name":"new-orbit\/0001-use-openshift-as-cloud-provider.md","Context":"## Context and Problem Statement\nWe want to deploy our application in docker containers that can be easily updated\n","Decision":"Chosen option: \"OpenShift\", because\\n* Built on Kubernetes.\\nThe bank has experience on it.\\nProvides a lot of added value tools for CI\/CD, automated builds.\\nIs supported by RedHat and we have a great support contract for it.\\n","tokens":21,"id":4096,"text":"## Context and Problem Statement\\nWe want to deploy our application in docker containers that can be easily updated\\n\n\n##Decision\nChosen option: \"OpenShift\", because\\n* Built on Kubernetes.\\nThe bank has experience on it.\\nProvides a lot of added value tools for CI\/CD, automated builds.\\nIs supported by RedHat and we have a great support contract for it.\\n","Predictions":"Chosen option: \"OpenShift\", becausen* Built on Kubernetes.nThe bank has experience on it.nProvides a lot of added value tools for CI\/CD, automated builds.nIs supported by RedHat and we have a great support contract for it.n"}
{"File Name":"google-cloud-cpp\/2019-01-04-error-reporting-with-statusor.md","Context":"**Context**: We know there will be users of these C++ libraries who want to use\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\nfor all of our users, regardless of their ability to use exceptions.\n**Decision**: None of our APIs will throw exceptions to indicate errors.\nInstead, our APIs will typically report errors to callers by returning a\n`Status` or `StatusOr<T>` object, unless the library we're using has another\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\nI\/O library).\n**Consequences**: This decision will result in a single set of APIs and a\nconsistent vocabulary for all users, whether or not they choose to compile with\nexceptions. This decision does not prevent callers from using exceptions in\ntheir own code.\nA downside of this decision is that our APIs will not be natural or idiomatic\nfor the [50+%][survey-link] of users who might prefer exceptions for error\nreporting.\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\nbreaking change. As of this writing (Jan 2019), this project has a\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\ntimeline to change this API in a separate document.\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\n","Decision":"Instead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","tokens":436,"id":2467,"text":"**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\\nfor all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n\n\n##Decision\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","Predictions":"We will use a StatusOrT> object to return anStatus or StatusOrT> object.nWe will use a StatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return anStatusOrT> object to return a"}
{"File Name":"beis-report-official-development-assistance\/0005-use-travis-for-ci.md","Context":"## Context\nTravis is actively used for many other projects within the BEIS GitHub account.\nWe know we will want to be using containers to run this application, so the tool we use must support Docker too.\n","Decision":"Use Travis to build and test the application.\\nUse Travis to automatically deploy the applications to live.\\n","tokens":45,"id":2393,"text":"## Context\\nTravis is actively used for many other projects within the BEIS GitHub account.\\nWe know we will want to be using containers to run this application, so the tool we use must support Docker too.\\n\n\n##Decision\nUse Travis to build and test the application.\\nUse Travis to automatically deploy the applications to live.\\n","Predictions":"Use Rollbar to collect and manage our application errors.n"}
{"File Name":"cloud-on-k8s\/0006-sidecar-health.md","Context":"## Context and Problem Statement\nThis proposal aims to outline possible approaches to report the Elasticsearch sidecar health in combination with the cluster state for cluster-wide monitoring and alerting.\nAs for now an Elasticsearch pod is composed of 2 containers:\n- a main container for Elasticsearch\n- a sidecar container for running the keystore-updater\nWhat is the keystore-updater in the sidecar doing?\nIt calls the Elasticsearch endpoint `\/_nodes\/reload_secure_settings` to decrypt and re-read the entire keystore used by the snapshotter job.\nTo connect to ES it depends on:\n- an environment variable for the username\n- secrets mounted as readonly files for the password and the CA certificate\n- the Elasticsearch readiness\nCurrently there is no health check based on the state of the sidecar. The sidecar can error without anyone ever noticing this state.\nSo there is a need to check that everything is correctly setup in the sidecar container and the call to the ES API succeeds.\nIf the sidecar container is not ready, the Elasticsearch container is impacted because the pod is considered not ready and\nKubernetes stops to send traffic to the pod. We must accept that the two containers are intimately linked. A sidecar failure\ncan impact the Elasticsearch availability by design.\nHowever Go binaries that do simple things are very fast to start and very reliable.\nFrom that we could admit that the probability to have a failure in the sidecar that runs a simple go binary is very low\ncompared to have an Elasticsearch failure.\nAnother challenge is to take into account that some sidecar errors are to be expected when ES is not ready yet.\nThis can be mitigated by considering a start-up delay during which it is accepted that ES is not ready and\ndo not report errors during this period. Then how to detect that ES has never started?\nThe ES readiness probe will fail if ES never becomes ready.\n## Decision Drivers\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\n* Promote reliability and simplicity because health-checking is a critical part of the system\n","Decision":"* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\nChosen option: option 4, because it gives us more flexibility to take decisions in case of failure, it does not depend on Kubernetes probes\/kubelet and it does not depend on external components.\\n### Positive Consequences\\n* Collecting the sidecar health from the operator side gives us more options to react to failures\\n* Having an HTTP server in the sidecar brings more flexibility to interact with the pod\\n* Does not depend on the Kubernetes probes or the Kubelet\\n* Minimize external dependencies\\n### Negative Consequences\\n* Increase a little the failure domain of the sidecar with the presence of the HTTP server\\n* Add complexity and responsibility to the operator\\n","tokens":455,"id":4705,"text":"## Context and Problem Statement\\nThis proposal aims to outline possible approaches to report the Elasticsearch sidecar health in combination with the cluster state for cluster-wide monitoring and alerting.\\nAs for now an Elasticsearch pod is composed of 2 containers:\\n- a main container for Elasticsearch\\n- a sidecar container for running the keystore-updater\\nWhat is the keystore-updater in the sidecar doing?\\nIt calls the Elasticsearch endpoint `\/_nodes\/reload_secure_settings` to decrypt and re-read the entire keystore used by the snapshotter job.\\nTo connect to ES it depends on:\\n- an environment variable for the username\\n- secrets mounted as readonly files for the password and the CA certificate\\n- the Elasticsearch readiness\\nCurrently there is no health check based on the state of the sidecar. The sidecar can error without anyone ever noticing this state.\\nSo there is a need to check that everything is correctly setup in the sidecar container and the call to the ES API succeeds.\\nIf the sidecar container is not ready, the Elasticsearch container is impacted because the pod is considered not ready and\\nKubernetes stops to send traffic to the pod. We must accept that the two containers are intimately linked. A sidecar failure\\ncan impact the Elasticsearch availability by design.\\nHowever Go binaries that do simple things are very fast to start and very reliable.\\nFrom that we could admit that the probability to have a failure in the sidecar that runs a simple go binary is very low\\ncompared to have an Elasticsearch failure.\\nAnother challenge is to take into account that some sidecar errors are to be expected when ES is not ready yet.\\nThis can be mitigated by considering a start-up delay during which it is accepted that ES is not ready and\\ndo not report errors during this period. Then how to detect that ES has never started?\\nThe ES readiness probe will fail if ES never becomes ready.\\n## Decision Drivers\\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\n\n\n##Decision\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\nChosen option: option 4, because it gives us more flexibility to take decisions in case of failure, it does not depend on Kubernetes probes\/kubelet and it does not depend on external components.\\n### Positive Consequences\\n* Collecting the sidecar health from the operator side gives us more options to react to failures\\n* Having an HTTP server in the sidecar brings more flexibility to interact with the pod\\n* Does not depend on the Kubernetes probes or the Kubelet\\n* Minimize external dependencies\\n### Negative Consequences\\n* Increase a little the failure domain of the sidecar with the presence of the HTTP server\\n* Add complexity and responsibility to the operator\\n","Predictions":"* The \/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/src\/sr"}
{"File Name":"terraform-aws-dynamodb\/0005-feature-toggles.md","Context":"## Context\nToggles are a useful aspect of Terraform modules that provide the ability to\nturn on\/off functionality.\nCapital One have written about\n[Building Feature Toggles into\nTerraform](https:\/\/medium.com\/capital-one-tech\/building-feature-toggles-into-terraform-d75806217647)\n. This build on top of the use of the count and ternary to provide a more robust\nfeature toggle.\nAs this module is envisioned to be part of a larger Data Storage Module there is\na concept where no DynamoDB table may need to be provisioned. In addition, when\nusing DynamoDB Autoscaling can be a useful feature. Providing the capabilities\nto turn this on and off based on requirements is also a useful requirement.\n","Decision":"Feature toggles will be leveraged to determine:\\n* If any resources should be created\\n* If a DynamoDB resource should be created\\n* If the Autoscaler should be enabled\\n","tokens":160,"id":2538,"text":"## Context\\nToggles are a useful aspect of Terraform modules that provide the ability to\\nturn on\/off functionality.\\nCapital One have written about\\n[Building Feature Toggles into\\nTerraform](https:\/\/medium.com\/capital-one-tech\/building-feature-toggles-into-terraform-d75806217647)\\n. This build on top of the use of the count and ternary to provide a more robust\\nfeature toggle.\\nAs this module is envisioned to be part of a larger Data Storage Module there is\\na concept where no DynamoDB table may need to be provisioned. In addition, when\\nusing DynamoDB Autoscaling can be a useful feature. Providing the capabilities\\nto turn this on and off based on requirements is also a useful requirement.\\n\n\n##Decision\nFeature toggles will be leveraged to determine:\\n* If any resources should be created\\n* If a DynamoDB resource should be created\\n* If the Autoscaler should be enabled\\n","Predictions":"We will use the Security module to provide a way to manage the Security module.n"}
{"File Name":"elasticsearch-updater\/0002-store-configurations-in-repository.md","Context":"## Context\nThe application loads data from a JSON file into an Elasticsearch instance. Along with the raw data the import\nalso needs to create mappings and transform data to improve search rankings or provide geolocation searches.\nThese are rich complex JSON objects or functions that cannot be passed in as environment variables.\n","Decision":"Given the small number of databases (currently only GP Profiles data is held in Elasticsearch) it is pragmatic to co-locate the Elasticsearch configuration alongside the `elasticsearch-updater` code, rather than creating a new repository and file hosting for each mappings and transform.\\n","tokens":61,"id":485,"text":"## Context\\nThe application loads data from a JSON file into an Elasticsearch instance. Along with the raw data the import\\nalso needs to create mappings and transform data to improve search rankings or provide geolocation searches.\\nThese are rich complex JSON objects or functions that cannot be passed in as environment variables.\\n\n\n##Decision\nGiven the small number of databases (currently only GP Profiles data is held in Elasticsearch) it is pragmatic to co-locate the Elasticsearch configuration alongside the `elasticsearch-updater` code, rather than creating a new repository and file hosting for each mappings and transform.\\n","Predictions":"We will use stack to generate help text.n"}
{"File Name":"klokwrk-project\/0002-strategic-project-structure.md","Context":"## Context\nExcluding the simplest hello-world-like cases, any useful project typically contains several modules. The traditional way to organize project modules is just to put them under the project root.\nWe can call that structure simply **flat structure**.\nWhile the flat structure is appropriate and sufficient for simpler projects, when the project grows and the number of modules increases, the flat structure starts suffering from many drawbacks:\n* Flat structure does not scale well when the number of modules grows.\n* Flat structure is difficult and confusing to navigate with numerous modules at the same hierarchy level.\n* Flat structure does not suggest a direction of dependencies between modules.\n* Flat structure does not suggest abstraction levels of modules.\n* Flat structure does not suggest where are the system's entry points.\n* Flat structure can use only module names to provide hints about relations between modules. Unfortunately, even that possibility is rarely leveraged.\n* Flat structure does not use any high-level constructs that may suggest how modules are organized and related.\n* Negative usage aspects are getting worse and worse as we add additional modules.\n* Flat structure often requires extracting modules in separate repositories just because confusion becomes unbearable with a larger number of modules.\n* When using microservices, the flat structure practically forces us to use one project per microservice.\n> Note: Terms **flat structure** and **strategic structure** (see below) are ad-hoc terms introduced just for this document. However, in the `klokwrk-project`, we may use them in other places for\n> convenience.\n### Architectural Context\n* System (`klokwrk-project`)\n","Decision":"**We'll organize project modules around strategic DDD (Domain Driven Design) constructs of bounded context and subdomains.**\\nOur project organization will follow principles and recommendations of **strategic structure** as defined below.\\n### Decision Details\\nWe'll start with a concrete example of the strategic structure used in the klokwrk at the time of writing this document. As a follow-up, we'll present a general scheme for creating the strategic\\nstructure focusing on the differences to the given concrete example.\\n#### Strategic structure in klokwrk\\nThe current project layout in the klokwrk looks like this:\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 asd\\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 booking\\n\u2502   \u2502       \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-commandside\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-view\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-rdbms-management\\n\u2502   \u2502       \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-boundary-web\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-out-customer\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502               cargotracking-booking-test-component\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-queryside\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-testcontainers\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502               cargotracking-lib-axon-cqrs\\n\u2502   \u2502               cargotracking-lib-axon-logging\\n\u2502   \u2502               cargotracking-lib-boundary-api\\n\u2502   \u2502               cargotracking-lib-boundary-query-api\\n\u2502   \u2502               cargotracking-lib-domain-model-command\\n\u2502   \u2502               cargotracking-lib-domain-model-event\\n\u2502   \u2502               cargotracking-lib-web\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-datasourceproxy-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-jackson-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-context\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-data-jpa\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-validation-springboot\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-archunit\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-datasourceproxy\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-hibernate\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-jackson\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-uom\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-constraint\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-validator\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-base\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-match\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-simple\\n\u2502   \u2502\\n\u2502   \u2514\u2500\u2500 other\\n\u2502       \u251c\u2500\u2500 platform\\n\u2502       \u2502       klokwrk-platform-base\\n\u2502       \u2502       klokwrk-platform-micronaut\\n\u2502       \u2502       klokwrk-platform-spring-boot\\n\u2502       \u2502\\n\u2502       \u2514\u2500\u2500 tool\\n\u2502               klokwrk-tool-gradle-source-repack\\n\u251c\u2500\u2500 support\\n\u2502   \u2514\u2500\u2500 ... (other files or directories)\\n\u2514\u2500\u2500 ... (other files or directories)\\nAt the top of the hierarchy, we have a project folder  - `klokwrk-project`. It is the equivalent of the whole system. In the strategic structure, the system name appears in the names of artifacts\\nconsidered to be conceptually at the level of a system.\\nRight below the root, we have `modules` and `support` folders. These should be the area of 99% of everyday work, with the `modules` folder taking a vast majority of that percentage.\\nThe `support` folder houses all kinds of supportive files like scripts, documentation, git hooks, etc. The `support` folder is free-form, and the strategic structure does not impose any\\nrecommendations or rules on its content. On the contrary, the strategic structure is applied to the content of the `modules` directory - the home of all source code modules in the system.\\nAt the 1st level of strategic structure - the system level, we have the content of the `modules` directory. It is divided into three subdirectories: `bc` (bounded context modules),\\n`lib` (system-level libraries), and `other` (miscellaneous helper modules).\\nAt the 2nd level - the bounded context level, we have the content of the `modules\/bc` directory that is further organized into three parts, `asd` (asd stands for **A** **S**ub**D**omain),\\n`domain-model` (bounded context domain model), and `lib` (bounded context libraries).\\nAt the 3rd level of a hierarchy, we have the content of the `modules\/bc\/[bounded-context-name]\/asd` directory that holds all bounded context's subdomains. The modules for each subdomain are further\\ndivided into `app` and `lib`. The `modules\/bc\/[bounded-context-name]\/asd\/[subdomain-name]\/app` directory contains the **subdomain applications** responsible for implementing concrete subdomain\\nscenarios. From the abstraction level and dependency perspectives, subdomain applications are at the top of the hierarchy. Subdomain applications speak the language of domain - the bounded context's\\nubiquitous language. They even contribute to it through the naming and meaning of use cases.\\nThe first thing that **subdomain libraries** (`modules\/bc\/[bounded-context-name]\/asd\/subdomain-name\/lib)` can hold is infrastructural code related to the technological choices made for that\\nparticular subdomain and are not reusable outside the subdomain. However, they can temporarily have infrastructural modules intended to be more reusable (either on the bounded context or system\\nlevels) at the end. Still, for whatever reason, it was more convenient to hold them at the subdomain level for a limited time.\\nThe second thing that can be found in subdomain libraries are business-related reusable modules that connect technological choices with the domain model. One characteristic example is the\\n`cargotracking-booking-lib-queryside-model-rdbms-jpa` module. Those kinds of modules do speak bounded context's ubiquitous language.\\nThe bounded context's **domain model** is implemented in `modules\/bc\/[bounded-context-name]\/domain-model`. Those modules contain the essence of the bounded context business logic. Implementation of\\nthe domain model should be free of technology as much as possible and practical. Adding external libraries is not strictly forbidden, but each addition should be conscious and must be carefully\\nevaluated. It is best to have tests that monitor and control the dependencies of a domain model. The domain model implements the majority of code-level representation of the bounded context's\\nubiquitous language and must be consistent across all bounded context's subdomains.\\nBy default, the directory `modules\/bc\/[bounded-context-name]\/lib` is the home of shareable **bounded context infrastructural libraries**. It contains modules with infrastructural code that is\\nreusable across the bounded context. Those modules are at a lower abstraction level than subdomain libraries. Bounded context infrastructural libraries do not speak domain language. However, they can\\nsupport the implementation of the domain model and other module groups higher in the hierarchy. Domain model should not generally depend on bounded context infrastructural libraries. Exceptions are\\nallowed but should be conscious and carefully managed.\\nDo note that another variant of bounded context libraries is also possible. It is a variant supporting the sharing of business logic at the bounded context level when necessary. In that case, instead\\nof a single `lib` directory, we would have `blib` and `ilib` directories. The `blib` directory would contain business-related modules that can depend on a domain model. On the contrary, the `ilib`\\ndirectory cannot use the domain model because it should contain infrastructural code only. The `ilib` directory role is the same as the role of `lib` directory from the default variant of bounded\\ncontext libraries.\\nLet's return to the `modules\/lib` directory containing general **system-level libraries**. It is divided into `hi`, `lo`, and `xlang` subdirectories. All system-level libraries are at lower\\ndependency and abstraction levels than any bounded context module.\\nAlthough separation on the high (`hi`) and low-level (`lo`) system libraries is somewhat arbitrary, it is helpful in practice. The `hi` directory is intended to contain\\n**high-level system libraries**, which are general infrastructural modules closer to the high-level technological frameworks (something like Spring, Spring Boot, or Axon frameworks) used in the\\nsystem. They could contain some specifics of our system, but usually, they do not. In that later case, they are general enough to be reused even outside of our system.\\nThe **low-level system libraries** from the `lo` directory deal with the customizations and extensions of widely used 3rd party libraries like Hibernate, Jackson, Java Bean validations, and similar.\\nBoth types of system-level libraries should not be, in general, dependencies of a domain model.\\nAt the lowest abstraction level, we have the **language extensions** (`modules\/lib\/xlang`). They focus on adding features to the programming language itself or its accompanying SDK (JDK in our case).\\nLanguage extensions can be used from everywhere, even from the domain model, without restrictions. Some of them are often written to ease the implementation of the domain model by making it more\\nexpressive and concise.\\n#### Characteristics of strategic structure\\nThe most important thing about strategic structure is not the structure itself but rather the distinguishing characteristics that it provides.\\nWe already mentioned abstraction levels and dependencies between groups of modules. If you look again at the example, you will notice that both of them are constantly flowing top to bottom through\\nthe strategic structure. For instance, subdomain applications depend on subdomain libraries. They both can depend on the domain model, which can depend on bounded context libraries and language\\nextensions. At the level of system libraries, high-level modules can depend on low-level modules, and they both can depend on the language extensions. However, none of the dependencies can come the\\nother way around. Dependencies are not allowed to flow from the bottom to the top.\\nWe have managed to do this because we applied strategic DDD concepts of bounded context and subdomains to the project structure. They provide sense and meaningfulness by connecting our code to the\\nbusiness. Without that business context, we will be left exclusively to the technical aspects, which are just insufficient. Technical aspects know nothing about the purpose of our system. They do not\\nknow anything about the business context.\\nDescribed characteristics bring important benefits when trying to understand or navigate through the system's code. Finding the desired functionality is much easier because we usually know, at least\\napproximately, where we should look for it. This can greatly reduce cognitive load while exploring unfamiliar (or even familiar) codebases.\\nIn addition, if you follow the proposed naming conventions for modules and their packages (see below), the same easy orientation can be applied at the package level or even if you pull out all\\nmodules into the flat structure. You will always know where to look for.\\n#### Naming conventions\\nYou have probably noticed that modules have very particular names reflecting their position in the strategic structure. The following table summarizes them as used in the example:\\n| Module group    | Naming scheme                                            | Example                                  |\\n|-----------------|----------------------------------------------------------|------------------------------------------|\\n| subdomain apps  | `[bounded-context-name]-[subdomain-name]-app-[app-name]` | `cargotracking-booking-app-commandside`  |\\n| subdomain libs  | `[bounded-context-name]-[subdomain-name]-lib-[lib-name]` | `cargotracking-booking-lib-boundary-web` |\\n| domain model    | `[bounded-context-name]-domain-model-[model-part-name]`  | `cargotracking-domain-model-aggregate`   |\\n| bc libs         | `[bounded-context-name]-lib-[lib-name]`                  | `cargotracking-lib-boundary-api`         |\\n| sys hi libs     | `[system-name]-lib-hi-[lib-name]`                        | `klokwrk-lib-hi-spring-context`          |\\n| sys lo libs     | `[system-name]-lib-lo-[lib-name]`                        | `klokwrk-lib-lo-jackson`                 |\\n| lang extensions | `[system-name]-lib-xlang-[lib-name]`                     | `klokwrk-lib-xlang-groovy-base`          |\\nModule naming conventions are essential because our modules are not always presented (i.e., try the Packages view in the IntelliJ IDEA's Project tool window) or used as a part of the hierarchy (think\\nof JAR names put in the same directory). For those reasons, our naming scheme closely follows the strategic structure hierarchy where parts of module names are directly pulled from corresponding\\nsubdirectory names. That way, we can keep the match between alphabetical order and the direction of dependencies.\\n> Note: When you have multiple bounded contexts and\/or multiple subdomains in the project, to get the exact match between alphabetical order and the direction of dependencies, you can use the `bc-`\\n> prefix in front of bounded context names and the `asd-` prefix for subdomain names.\\nThe same naming principles should also be applied to packages. Here are a few examples of package names:\\norg.klokwrk.cargotracking.booking.app.commandside.*\\norg.klokwrk.cargotracking.booking.lib.boundary.web.*\\norg.klokwrk.cargotracking.domain.model.aggregate.*\\norg.klokwrk.cargotracking.lib.boundary.api.*\\norg.klokwrk.lib.hi.spring.context.*\\norg.klokwrk.lib.lo.jackson.*\\norg.klokwrk.lib.xlang.groovy.base.*\\nWith those naming conventions, we should be able to avoid naming collisions on the module and package levels.\\n#### The general scheme of strategic structure\\nIn some circumstances, we may need additional elements in the strategic structure to deal with shared libraries at different levels. Examples of those, with sparse explanations, are given in the\\ngeneral scheme of strategic structure below:\\nmodules\\n\u251c\u2500\u2500 bc\\n\u2502   \u251c\u2500\u2500 my_food\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 restaurant\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 menu_management\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 zshared         \/\/ sharing code between subdomains if necessary\\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib                 \/\/ bounded context libraries - default variant\\n\u2502   \u2502           ... *           \/\/ Can be split into \"blib\" and \"ilib\" directories when the sharing of\\n\u2502   \u2502                           \/\/ business logic is necessary at the level of a single bounded context\\n\u2502   \u251c\u2500\u2500 my_carrier\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 zshared                 \/\/ shared code between multiple bounded contexts (if necessary).\\n\u2502       \u2502                       \/\/ \"z\" prefix - funny reference to \"zee Germans\" from Snatch movie.\\n\u2502       \u2502                       \/\/ Moves \"zshared\" at the last place alphabetically, which matches\\n\u2502       \u2502                       \/\/ the proper place in terms of dependencies and abstraction levels.\\n\u2502       \u251c\u2500\u2500 domain-model\\n\u2502       \u2502       ... *\\n\u2502       \u2514\u2500\u2500 lib\\n\u2502               ... *\\n\u251c\u2500\u2500 lib\\n\u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502       ... *\\n\u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502       ... *\\n\u2502   \u2514\u2500\u2500 xlang\\n\u2502           ... *\\n\u2514\u2500\u2500 other            \/\/ supportive project's code for various \"other\" purposes\\n\u251c\u2500\u2500 build\\n\u2502       ... *\\n\u251c\u2500\u2500 tool\\n\u2502       ... *\\n\u2514\u2500\u2500 ...\\n#### Simplification - the case of bounded context boundaries matching 1:1 with subdomain\\nThe one-to-one match between bounded context boundaries and corresponding subdomain is considered to be the \"ideal\" case, and it is relatively common in practice. When we know how a fully expanded\\nstrategic structure works and looks like, it is relatively easy to come up with simplification for this particular case.\\nHere are \"refactoring\" steps and the example based on our concrete example from the beginning of this document:\\n- move subdomain applications to the bounded context level\\n- merge subdomain libraries with bounded context libraries\\n- split bounded context libraries into `blib` and `ilib` directories if necessary\\n- rename corresponding modules and packages\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       cargotracking-app-commandside\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-view\\n\u2502   \u2502       \u2502       cargotracking-app-rdbms-management\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 blib\\n\u2502   \u2502       \u2502       cargotracking-blib-out-customer\\n\u2502   \u2502       \u2502       cargotracking-blib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 ilib\\n\u2502   \u2502               cargotracking-ilib-axon-cqrs\\n\u2502   \u2502               cargotracking-ilib-axon-logging\\n\u2502   \u2502               cargotracking-ilib-boundary-api\\n\u2502   \u2502               cargotracking-ilib-boundary-query-api\\n\u2502   \u2502               cargotracking-ilib-boundary-web\\n\u2502   \u2502               cargotracking-ilib-domain-model-command\\n\u2502   \u2502               cargotracking-ilib-domain-model-event\\n\u2502   \u2502               cargotracking-ilib-web\\n\u2502   \u2502               cargotracking-test-component\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502               cargotracking-test-support-queryside\\n\u2502   \u2502               cargotracking-test-support-testcontainers\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 other\\n\u2502           ... *\\n\u251c\u2500\u2500 support\\n\u2502       ... *\\n\u2514\u2500\u2500 ... *\\n","tokens":335,"id":4889,"text":"## Context\\nExcluding the simplest hello-world-like cases, any useful project typically contains several modules. The traditional way to organize project modules is just to put them under the project root.\\nWe can call that structure simply **flat structure**.\\nWhile the flat structure is appropriate and sufficient for simpler projects, when the project grows and the number of modules increases, the flat structure starts suffering from many drawbacks:\\n* Flat structure does not scale well when the number of modules grows.\\n* Flat structure is difficult and confusing to navigate with numerous modules at the same hierarchy level.\\n* Flat structure does not suggest a direction of dependencies between modules.\\n* Flat structure does not suggest abstraction levels of modules.\\n* Flat structure does not suggest where are the system's entry points.\\n* Flat structure can use only module names to provide hints about relations between modules. Unfortunately, even that possibility is rarely leveraged.\\n* Flat structure does not use any high-level constructs that may suggest how modules are organized and related.\\n* Negative usage aspects are getting worse and worse as we add additional modules.\\n* Flat structure often requires extracting modules in separate repositories just because confusion becomes unbearable with a larger number of modules.\\n* When using microservices, the flat structure practically forces us to use one project per microservice.\\n> Note: Terms **flat structure** and **strategic structure** (see below) are ad-hoc terms introduced just for this document. However, in the `klokwrk-project`, we may use them in other places for\\n> convenience.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n\n##Decision\n**We'll organize project modules around strategic DDD (Domain Driven Design) constructs of bounded context and subdomains.**\\nOur project organization will follow principles and recommendations of **strategic structure** as defined below.\\n### Decision Details\\nWe'll start with a concrete example of the strategic structure used in the klokwrk at the time of writing this document. As a follow-up, we'll present a general scheme for creating the strategic\\nstructure focusing on the differences to the given concrete example.\\n#### Strategic structure in klokwrk\\nThe current project layout in the klokwrk looks like this:\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 asd\\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 booking\\n\u2502   \u2502       \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-commandside\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-view\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-rdbms-management\\n\u2502   \u2502       \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-boundary-web\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-out-customer\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502               cargotracking-booking-test-component\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-queryside\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-testcontainers\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502               cargotracking-lib-axon-cqrs\\n\u2502   \u2502               cargotracking-lib-axon-logging\\n\u2502   \u2502               cargotracking-lib-boundary-api\\n\u2502   \u2502               cargotracking-lib-boundary-query-api\\n\u2502   \u2502               cargotracking-lib-domain-model-command\\n\u2502   \u2502               cargotracking-lib-domain-model-event\\n\u2502   \u2502               cargotracking-lib-web\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-datasourceproxy-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-jackson-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-context\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-data-jpa\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-validation-springboot\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-archunit\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-datasourceproxy\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-hibernate\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-jackson\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-uom\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-constraint\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-validator\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-base\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-match\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-simple\\n\u2502   \u2502\\n\u2502   \u2514\u2500\u2500 other\\n\u2502       \u251c\u2500\u2500 platform\\n\u2502       \u2502       klokwrk-platform-base\\n\u2502       \u2502       klokwrk-platform-micronaut\\n\u2502       \u2502       klokwrk-platform-spring-boot\\n\u2502       \u2502\\n\u2502       \u2514\u2500\u2500 tool\\n\u2502               klokwrk-tool-gradle-source-repack\\n\u251c\u2500\u2500 support\\n\u2502   \u2514\u2500\u2500 ... (other files or directories)\\n\u2514\u2500\u2500 ... (other files or directories)\\nAt the top of the hierarchy, we have a project folder  - `klokwrk-project`. It is the equivalent of the whole system. In the strategic structure, the system name appears in the names of artifacts\\nconsidered to be conceptually at the level of a system.\\nRight below the root, we have `modules` and `support` folders. These should be the area of 99% of everyday work, with the `modules` folder taking a vast majority of that percentage.\\nThe `support` folder houses all kinds of supportive files like scripts, documentation, git hooks, etc. The `support` folder is free-form, and the strategic structure does not impose any\\nrecommendations or rules on its content. On the contrary, the strategic structure is applied to the content of the `modules` directory - the home of all source code modules in the system.\\nAt the 1st level of strategic structure - the system level, we have the content of the `modules` directory. It is divided into three subdirectories: `bc` (bounded context modules),\\n`lib` (system-level libraries), and `other` (miscellaneous helper modules).\\nAt the 2nd level - the bounded context level, we have the content of the `modules\/bc` directory that is further organized into three parts, `asd` (asd stands for **A** **S**ub**D**omain),\\n`domain-model` (bounded context domain model), and `lib` (bounded context libraries).\\nAt the 3rd level of a hierarchy, we have the content of the `modules\/bc\/[bounded-context-name]\/asd` directory that holds all bounded context's subdomains. The modules for each subdomain are further\\ndivided into `app` and `lib`. The `modules\/bc\/[bounded-context-name]\/asd\/[subdomain-name]\/app` directory contains the **subdomain applications** responsible for implementing concrete subdomain\\nscenarios. From the abstraction level and dependency perspectives, subdomain applications are at the top of the hierarchy. Subdomain applications speak the language of domain - the bounded context's\\nubiquitous language. They even contribute to it through the naming and meaning of use cases.\\nThe first thing that **subdomain libraries** (`modules\/bc\/[bounded-context-name]\/asd\/subdomain-name\/lib)` can hold is infrastructural code related to the technological choices made for that\\nparticular subdomain and are not reusable outside the subdomain. However, they can temporarily have infrastructural modules intended to be more reusable (either on the bounded context or system\\nlevels) at the end. Still, for whatever reason, it was more convenient to hold them at the subdomain level for a limited time.\\nThe second thing that can be found in subdomain libraries are business-related reusable modules that connect technological choices with the domain model. One characteristic example is the\\n`cargotracking-booking-lib-queryside-model-rdbms-jpa` module. Those kinds of modules do speak bounded context's ubiquitous language.\\nThe bounded context's **domain model** is implemented in `modules\/bc\/[bounded-context-name]\/domain-model`. Those modules contain the essence of the bounded context business logic. Implementation of\\nthe domain model should be free of technology as much as possible and practical. Adding external libraries is not strictly forbidden, but each addition should be conscious and must be carefully\\nevaluated. It is best to have tests that monitor and control the dependencies of a domain model. The domain model implements the majority of code-level representation of the bounded context's\\nubiquitous language and must be consistent across all bounded context's subdomains.\\nBy default, the directory `modules\/bc\/[bounded-context-name]\/lib` is the home of shareable **bounded context infrastructural libraries**. It contains modules with infrastructural code that is\\nreusable across the bounded context. Those modules are at a lower abstraction level than subdomain libraries. Bounded context infrastructural libraries do not speak domain language. However, they can\\nsupport the implementation of the domain model and other module groups higher in the hierarchy. Domain model should not generally depend on bounded context infrastructural libraries. Exceptions are\\nallowed but should be conscious and carefully managed.\\nDo note that another variant of bounded context libraries is also possible. It is a variant supporting the sharing of business logic at the bounded context level when necessary. In that case, instead\\nof a single `lib` directory, we would have `blib` and `ilib` directories. The `blib` directory would contain business-related modules that can depend on a domain model. On the contrary, the `ilib`\\ndirectory cannot use the domain model because it should contain infrastructural code only. The `ilib` directory role is the same as the role of `lib` directory from the default variant of bounded\\ncontext libraries.\\nLet's return to the `modules\/lib` directory containing general **system-level libraries**. It is divided into `hi`, `lo`, and `xlang` subdirectories. All system-level libraries are at lower\\ndependency and abstraction levels than any bounded context module.\\nAlthough separation on the high (`hi`) and low-level (`lo`) system libraries is somewhat arbitrary, it is helpful in practice. The `hi` directory is intended to contain\\n**high-level system libraries**, which are general infrastructural modules closer to the high-level technological frameworks (something like Spring, Spring Boot, or Axon frameworks) used in the\\nsystem. They could contain some specifics of our system, but usually, they do not. In that later case, they are general enough to be reused even outside of our system.\\nThe **low-level system libraries** from the `lo` directory deal with the customizations and extensions of widely used 3rd party libraries like Hibernate, Jackson, Java Bean validations, and similar.\\nBoth types of system-level libraries should not be, in general, dependencies of a domain model.\\nAt the lowest abstraction level, we have the **language extensions** (`modules\/lib\/xlang`). They focus on adding features to the programming language itself or its accompanying SDK (JDK in our case).\\nLanguage extensions can be used from everywhere, even from the domain model, without restrictions. Some of them are often written to ease the implementation of the domain model by making it more\\nexpressive and concise.\\n#### Characteristics of strategic structure\\nThe most important thing about strategic structure is not the structure itself but rather the distinguishing characteristics that it provides.\\nWe already mentioned abstraction levels and dependencies between groups of modules. If you look again at the example, you will notice that both of them are constantly flowing top to bottom through\\nthe strategic structure. For instance, subdomain applications depend on subdomain libraries. They both can depend on the domain model, which can depend on bounded context libraries and language\\nextensions. At the level of system libraries, high-level modules can depend on low-level modules, and they both can depend on the language extensions. However, none of the dependencies can come the\\nother way around. Dependencies are not allowed to flow from the bottom to the top.\\nWe have managed to do this because we applied strategic DDD concepts of bounded context and subdomains to the project structure. They provide sense and meaningfulness by connecting our code to the\\nbusiness. Without that business context, we will be left exclusively to the technical aspects, which are just insufficient. Technical aspects know nothing about the purpose of our system. They do not\\nknow anything about the business context.\\nDescribed characteristics bring important benefits when trying to understand or navigate through the system's code. Finding the desired functionality is much easier because we usually know, at least\\napproximately, where we should look for it. This can greatly reduce cognitive load while exploring unfamiliar (or even familiar) codebases.\\nIn addition, if you follow the proposed naming conventions for modules and their packages (see below), the same easy orientation can be applied at the package level or even if you pull out all\\nmodules into the flat structure. You will always know where to look for.\\n#### Naming conventions\\nYou have probably noticed that modules have very particular names reflecting their position in the strategic structure. The following table summarizes them as used in the example:\\n| Module group    | Naming scheme                                            | Example                                  |\\n|-----------------|----------------------------------------------------------|------------------------------------------|\\n| subdomain apps  | `[bounded-context-name]-[subdomain-name]-app-[app-name]` | `cargotracking-booking-app-commandside`  |\\n| subdomain libs  | `[bounded-context-name]-[subdomain-name]-lib-[lib-name]` | `cargotracking-booking-lib-boundary-web` |\\n| domain model    | `[bounded-context-name]-domain-model-[model-part-name]`  | `cargotracking-domain-model-aggregate`   |\\n| bc libs         | `[bounded-context-name]-lib-[lib-name]`                  | `cargotracking-lib-boundary-api`         |\\n| sys hi libs     | `[system-name]-lib-hi-[lib-name]`                        | `klokwrk-lib-hi-spring-context`          |\\n| sys lo libs     | `[system-name]-lib-lo-[lib-name]`                        | `klokwrk-lib-lo-jackson`                 |\\n| lang extensions | `[system-name]-lib-xlang-[lib-name]`                     | `klokwrk-lib-xlang-groovy-base`          |\\nModule naming conventions are essential because our modules are not always presented (i.e., try the Packages view in the IntelliJ IDEA's Project tool window) or used as a part of the hierarchy (think\\nof JAR names put in the same directory). For those reasons, our naming scheme closely follows the strategic structure hierarchy where parts of module names are directly pulled from corresponding\\nsubdirectory names. That way, we can keep the match between alphabetical order and the direction of dependencies.\\n> Note: When you have multiple bounded contexts and\/or multiple subdomains in the project, to get the exact match between alphabetical order and the direction of dependencies, you can use the `bc-`\\n> prefix in front of bounded context names and the `asd-` prefix for subdomain names.\\nThe same naming principles should also be applied to packages. Here are a few examples of package names:\\norg.klokwrk.cargotracking.booking.app.commandside.*\\norg.klokwrk.cargotracking.booking.lib.boundary.web.*\\norg.klokwrk.cargotracking.domain.model.aggregate.*\\norg.klokwrk.cargotracking.lib.boundary.api.*\\norg.klokwrk.lib.hi.spring.context.*\\norg.klokwrk.lib.lo.jackson.*\\norg.klokwrk.lib.xlang.groovy.base.*\\nWith those naming conventions, we should be able to avoid naming collisions on the module and package levels.\\n#### The general scheme of strategic structure\\nIn some circumstances, we may need additional elements in the strategic structure to deal with shared libraries at different levels. Examples of those, with sparse explanations, are given in the\\ngeneral scheme of strategic structure below:\\nmodules\\n\u251c\u2500\u2500 bc\\n\u2502   \u251c\u2500\u2500 my_food\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 restaurant\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 menu_management\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 zshared         \/\/ sharing code between subdomains if necessary\\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib                 \/\/ bounded context libraries - default variant\\n\u2502   \u2502           ... *           \/\/ Can be split into \"blib\" and \"ilib\" directories when the sharing of\\n\u2502   \u2502                           \/\/ business logic is necessary at the level of a single bounded context\\n\u2502   \u251c\u2500\u2500 my_carrier\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 zshared                 \/\/ shared code between multiple bounded contexts (if necessary).\\n\u2502       \u2502                       \/\/ \"z\" prefix - funny reference to \"zee Germans\" from Snatch movie.\\n\u2502       \u2502                       \/\/ Moves \"zshared\" at the last place alphabetically, which matches\\n\u2502       \u2502                       \/\/ the proper place in terms of dependencies and abstraction levels.\\n\u2502       \u251c\u2500\u2500 domain-model\\n\u2502       \u2502       ... *\\n\u2502       \u2514\u2500\u2500 lib\\n\u2502               ... *\\n\u251c\u2500\u2500 lib\\n\u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502       ... *\\n\u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502       ... *\\n\u2502   \u2514\u2500\u2500 xlang\\n\u2502           ... *\\n\u2514\u2500\u2500 other            \/\/ supportive project's code for various \"other\" purposes\\n\u251c\u2500\u2500 build\\n\u2502       ... *\\n\u251c\u2500\u2500 tool\\n\u2502       ... *\\n\u2514\u2500\u2500 ...\\n#### Simplification - the case of bounded context boundaries matching 1:1 with subdomain\\nThe one-to-one match between bounded context boundaries and corresponding subdomain is considered to be the \"ideal\" case, and it is relatively common in practice. When we know how a fully expanded\\nstrategic structure works and looks like, it is relatively easy to come up with simplification for this particular case.\\nHere are \"refactoring\" steps and the example based on our concrete example from the beginning of this document:\\n- move subdomain applications to the bounded context level\\n- merge subdomain libraries with bounded context libraries\\n- split bounded context libraries into `blib` and `ilib` directories if necessary\\n- rename corresponding modules and packages\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       cargotracking-app-commandside\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-view\\n\u2502   \u2502       \u2502       cargotracking-app-rdbms-management\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 blib\\n\u2502   \u2502       \u2502       cargotracking-blib-out-customer\\n\u2502   \u2502       \u2502       cargotracking-blib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 ilib\\n\u2502   \u2502               cargotracking-ilib-axon-cqrs\\n\u2502   \u2502               cargotracking-ilib-axon-logging\\n\u2502   \u2502               cargotracking-ilib-boundary-api\\n\u2502   \u2502               cargotracking-ilib-boundary-query-api\\n\u2502   \u2502               cargotracking-ilib-boundary-web\\n\u2502   \u2502               cargotracking-ilib-domain-model-command\\n\u2502   \u2502               cargotracking-ilib-domain-model-event\\n\u2502   \u2502               cargotracking-ilib-web\\n\u2502   \u2502               cargotracking-test-component\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502               cargotracking-test-support-queryside\\n\u2502   \u2502               cargotracking-test-support-testcontainers\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 other\\n\u2502           ... *\\n\u251c\u2500\u2500 support\\n\u2502       ... *\\n\u2514\u2500\u2500 ... *\\n","Predictions":"**We'll organize project modules around strategic DDD (Domain Driven Design) constructs of bounded context and subdomains.**nOur project organization will follow principles and recommendations of **strategic structure** as defined below.n### Decision DetailsnWe'll start with a concrete example of the strategic structure used in the klokwrk at the time of writing this document. As a follow-up, we'll present a general scheme for creating the strategicnstructure focusing on domain context and subdomains.n"}
{"File Name":"james-project\/0041-replace-jcloud-with-s3.md","Context":"## Context\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\nserver.\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\nObject Storage technologies.\nObject Storage is known to be cheap and yet offers guarantee about durability.\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\nIt also contribute negatively to the project build time.\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\nour Swift code without dropping Swift support at the same time.\n","Decision":"* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\\n","tokens":209,"id":2897,"text":"## Context\\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\\nserver.\\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\\nObject Storage technologies.\\nObject Storage is known to be cheap and yet offers guarantee about durability.\\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\\nIt also contribute negatively to the project build time.\\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\\nour Swift code without dropping Swift support at the same time.\\n\n\n##Decision\n* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\\n","Predictions":"We will use a Status database to store the data.n"}
{"File Name":"tracking-consent-frontend\/0007-use-eslint-for-linting-and-formatting.md","Context":"## Context and Problem Statement\nWe found that without a standardised format our javascript files ended up with different\nformats in different files or even multiple formats in the same file.  We also found that\nour IDEs had different configurations which meant that using an autoformat tool would give\ndifferent results when each of us do it.\n## Decision Drivers\n* We wanted to spend less time doing manual formatting\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\n* We wanted to see easily which lines had actually changed when reviewing PRs\n* We wanted to avoid discussions about individual's preferences for particular\n","Decision":"* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\nChosen option: ESLint + Airbnb\\nWe decided to use ESLint with Airbnb because the Airbnb approach is [well documented](https:\/\/github.com\/airbnb\/javascript)\\nand adopting this meant we wouldn't have to make each decision ourselves.  We decided not to use Prettier as well, there\\nwas a discussion about the possibility of conflicting format changes between the two tools - we encountered one of these,\\nit was overcome with config updates.\\nESLint is compatible with both Javascript and Typescript projects which made it seem particularly suitable for us as we\\nchose to use Typescript for Tracking Consent but we usually use Javascript.\\nEach of the current team members use JetBrains IDEs and that has a built-in plugin which supports auto formatting on save,\\nthis is also a common feature which most IDEs have.\\nWe decided to add pre-commit and pre-push hooks which ensure that the style rules have been met but do not mutate code.\\nWe discussed the option of formatting the code on commit but we felt that this introduced an uncertainty about what we\\nwere committing.\\n### Positive Consequences\\n* Code style within a file will be consistent\\n* Files within a project will be consistent with each other\\n* When widely applied this Javascript\/Typescript files between projects will be consistent\\n* IDE will reformat for us which takes away the need for us to do it ourselves\\n* Code reviews will focus less on code format and more on the details of the changes made\\n* Builds will fail if format isn't followed which guarentees consistency\\n### Negative Consequences\\n* Builds will fail if format isn't followed which can disrupt focus\/workflow (mitigated by pre-commit hooks)\\n","tokens":133,"id":3521,"text":"## Context and Problem Statement\\nWe found that without a standardised format our javascript files ended up with different\\nformats in different files or even multiple formats in the same file.  We also found that\\nour IDEs had different configurations which meant that using an autoformat tool would give\\ndifferent results when each of us do it.\\n## Decision Drivers\\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\n\n\n##Decision\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\nChosen option: ESLint + Airbnb\\nWe decided to use ESLint with Airbnb because the Airbnb approach is [well documented](https:\/\/github.com\/airbnb\/javascript)\\nand adopting this meant we wouldn't have to make each decision ourselves.  We decided not to use Prettier as well, there\\nwas a discussion about the possibility of conflicting format changes between the two tools - we encountered one of these,\\nit was overcome with config updates.\\nESLint is compatible with both Javascript and Typescript projects which made it seem particularly suitable for us as we\\nchose to use Typescript for Tracking Consent but we usually use Javascript.\\nEach of the current team members use JetBrains IDEs and that has a built-in plugin which supports auto formatting on save,\\nthis is also a common feature which most IDEs have.\\nWe decided to add pre-commit and pre-push hooks which ensure that the style rules have been met but do not mutate code.\\nWe discussed the option of formatting the code on commit but we felt that this introduced an uncertainty about what we\\nwere committing.\\n### Positive Consequences\\n* Code style within a file will be consistent\\n* Files within a project will be consistent with each other\\n* When widely applied this Javascript\/Typescript files between projects will be consistent\\n* IDE will reformat for us which takes away the need for us to do it ourselves\\n* Code reviews will focus less on code format and more on the details of the changes made\\n* Builds will fail if format isn't followed which guarentees consistency\\n### Negative Consequences\\n* Builds will fail if format isn't followed which can disrupt focus\/workflow (mitigated by pre-commit hooks)\\n","Predictions":"* We wanted to spend less time doing manual formattingn* We wanted to spend less time undoing autoformatting which had been applied to unchanged linesn* We wanted to see easily which lines had actually changed when reviewing PRsnChosen option: ESLint + AirbnbnWe decided to use ESLint with Airbnb because the Airbnb approach is [well documented](https:\/\/github.com\/airbnb\/javascript)nand adopting this meant we wouldn't have to make each decision ourselves.nChosen option: Prettier as well, therenwas a discussion about the possibility of conflicting format changes between the two tools - we encountered one of these,nit was overcome with config updates.nESLint is compatible with both Javascript and Typescript projects which made it seem particularly suitable for us as wenchose to use Typescript for Tracking Consent but we usually use Javascript.nEach of the current team members use JetBrains IDEs and that has a built-in plugin which supports auto formatting on save,nthis is also a common feature which most IDEs have.nWe decided to add pre-commit and pre-push hooks which ensure that the style rules have been met but do not mutate code.nWe discussed the option of formatting the code on commit but we felt that this introduced an uncertain about what wenwere committing.n### Positive Consequencesn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within the code is consistentn* Code style within"}
{"File Name":"Data-Platform-Playbook\/002-ingest-google-sheets-data.md","Context":"## Context\nHackney currently have datasets distributed over a number of different Google Sheets Documents due their data recovery\nefforts. We need to get this information pulled into the data platform for processing.\n","Decision":"We will use a python based AWS Glue Job in conjunction with the gspread python library to pull the data onto platform\\n","tokens":41,"id":1016,"text":"## Context\\nHackney currently have datasets distributed over a number of different Google Sheets Documents due their data recovery\\nefforts. We need to get this information pulled into the data platform for processing.\\n\n\n##Decision\nWe will use a python based AWS Glue Job in conjunction with the gspread python library to pull the data onto platform\\n","Predictions":"We will use a [Semantic Data Warehouse](https:\/\/github.com\/alphagov\/semantic-data-warehouse) to store the data.n"}
{"File Name":"runner\/0274-step-outcome-and-conclusion.md","Context":"## Context\nThis ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.\nThis allows downstream a step to run based on whether a previous step succeeded or failed.\nReminder, currently the steps contains `steps.<id>.outputs`.\n","Decision":"For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:\\n- `success`\\n- `failure`\\n- `cancelled`\\n- `skipped`\\nWhen a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`.\\n### Example\\n```yaml\\nsteps:\\n- id: experimental\\ncontinue-on-error: true\\nrun: .\/build.sh experimental\\n- if: ${{ steps.experimental.outcome == 'success' }}\\nrun: .\/publish.sh experimental\\n```\\n### Terminology\\nThe runs API uses the term `conclusion`.\\nTherefore we use a different term `outcome` for the value prior to continue-on-error.\\nThe following is a snippet from the runs API response payload:\\n```json\\n\"steps\": [\\n{\\n\"name\": \"Set up job\",\\n\"status\": \"completed\",\\n\"conclusion\": \"success\",\\n\"number\": 1,\\n\"started_at\": \"2020-01-09T11:06:16.000-05:00\",\\n\"completed_at\": \"2020-01-09T11:06:18.000-05:00\"\\n},\\n```\\n","tokens":63,"id":3738,"text":"## Context\\nThis ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.\\nThis allows downstream a step to run based on whether a previous step succeeded or failed.\\nReminder, currently the steps contains `steps.<id>.outputs`.\\n\n\n##Decision\nFor steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:\\n- `success`\\n- `failure`\\n- `cancelled`\\n- `skipped`\\nWhen a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`.\\n### Example\\n```yaml\\nsteps:\\n- id: experimental\\ncontinue-on-error: true\\nrun: .\/build.sh experimental\\n- if: ${{ steps.experimental.outcome == 'success' }}\\nrun: .\/publish.sh experimental\\n```\\n### Terminology\\nThe runs API uses the term `conclusion`.\\nTherefore we use a different term `outcome` for the value prior to continue-on-error.\\nThe following is a snippet from the runs API response payload:\\n```json\\n\"steps\": [\\n{\\n\"name\": \"Set up job\",\\n\"status\": \"completed\",\\n\"conclusion\": \"success\",\\n\"number\": 1,\\n\"started_at\": \"2020-01-09T11:06:16.000-05:00\",\\n\"completed_at\": \"2020-01-09T11:06:18.000-05:00\"\\n},\\n```\\n","Predictions":"steps.id>.outcome and steps.id>.conclusion will be used to indicate whether a step has completed or failed.nThis will be done by steps.id>.outputs and steps.id>.conclusion.n"}
{"File Name":"editions\/07-\u2705-ophan-analytics.md","Context":"## Context\nWe need a way to send analytics data to Ophan (internal analytics analysis tool). This includes at its MVP level, screen views and events. This needs to work across iOS and Android.\n","Decision":"```sequence\\nOphan Analytics Library->Multiplatform Ophan: Thrift interface\\nMultiplatform Ophan->Native React Native Layer: Native iOS and Android modules\\nNative React Native Layer->Typescript Layer: Type definitions to mimic modules\\nTypescript Layer->App Code: Consume! \ud83c\udf54\\nApp Code-->Ophan Analytics Library: Sends events\\n```\\n### Key\\n**Ophan Analytics Library** - This is the current library that provides an interface to log events in Ophan.\\n**Multplatform Ophran** - Written in Kotlin, this uses the Ophan Analytics Library via its Thrift interface and exposes a subset of methods required by the apps.\\n**Native React Native Layer** - Takes the module created by Multiplatform Ophan and makes it \"React Nativey\" to expose methods to the JS.\\n**Typescript Layer** - Takes the React Native JS methods exposed and provides type definitions for them. This is what the App will use and it means we can capture errors at build time.\\n**App Code** - Consumes the typescript library where its needed.\\n","tokens":42,"id":669,"text":"## Context\\nWe need a way to send analytics data to Ophan (internal analytics analysis tool). This includes at its MVP level, screen views and events. This needs to work across iOS and Android.\\n\n\n##Decision\n```sequence\\nOphan Analytics Library->Multiplatform Ophan: Thrift interface\\nMultiplatform Ophan->Native React Native Layer: Native iOS and Android modules\\nNative React Native Layer->Typescript Layer: Type definitions to mimic modules\\nTypescript Layer->App Code: Consume! \ud83c\udf54\\nApp Code-->Ophan Analytics Library: Sends events\\n```\\n### Key\\n**Ophan Analytics Library** - This is the current library that provides an interface to log events in Ophan.\\n**Multplatform Ophran** - Written in Kotlin, this uses the Ophan Analytics Library via its Thrift interface and exposes a subset of methods required by the apps.\\n**Native React Native Layer** - Takes the module created by Multiplatform Ophan and makes it \"React Nativey\" to expose methods to the JS.\\n**Typescript Layer** - Takes the React Native JS methods exposed and provides type definitions for them. This is what the App will use and it means we can capture errors at build time.\\n**App Code** - Consumes the typescript library where its needed.\\n","Predictions":"We will use the Security API to provide a way to store the data in the Security API.n"}
{"File Name":"libelektra\/internal_cache.md","Context":"## Problem\nWhen doing kdbGet() possible more keys are returned which might be\nconfusing. When doing a second kdbGet() with a new keyset\nno keys might be returned, because it is up-to-date.\nWhen doing kdbSet() a deep duplication is needed.\nIdea: keep a duplicated keyset internally. Return (non-deep?)\nduplications exactly of the parts as requested.\n","Decision":"Not yet decided.\\n- [Global Validation](global_validation.md)\\n","tokens":90,"id":1289,"text":"## Problem\\nWhen doing kdbGet() possible more keys are returned which might be\\nconfusing. When doing a second kdbGet() with a new keyset\\nno keys might be returned, because it is up-to-date.\\nWhen doing kdbSet() a deep duplication is needed.\\nIdea: keep a duplicated keyset internally. Return (non-deep?)\\nduplications exactly of the parts as requested.\\n\n\n##Decision\nNot yet decided.\\n- [Global Validation](global_validation.md)\\n","Predictions":"Not yet decided.n- [Global Validation](global_validation.md)n**Context**: operations that change state in the server may complete with annindeterminate status code. For example: an error in the 500 range may benproduced by a middle-tier server after the operation was started by the backend.nRe-issuing the request may not work as expected for the application developer.n**Decision**: by default the library will retry all requests, includingnnon-idempotent ones. The library will allow application developers to overridenthe definition of what operations are automatically retried.n**Consequences**: most operations become easier to use for applicationndevelopers. In very rare cases the operation will result in double uploads, ornin a new generation of the object or metadata being created. In even more rarencases the operation may fail, for example, an operation to create an object withnIfGenerationMatch(0) would fail on the second attempt.n"}
{"File Name":"git-en-boite\/0009-prefer-dugite-over-raw-git-commands-in-tests.md","Context":"## Context\nThe build failed because the version of git used in CircleCI was not what we expected.\n","Decision":"Use `GitProcess.exec` even in tests that need to lean on the git process.\\n","tokens":22,"id":3504,"text":"## Context\\nThe build failed because the version of git used in CircleCI was not what we expected.\\n\n\n##Decision\nUse `GitProcess.exec` even in tests that need to lean on the git process.\\n","Predictions":"We will use github.com\/stack\/ to build the application.n"}
{"File Name":"Conduit\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4408,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"beis-report-official-development-assistance\/0006-use-auth0-for-authentication.md","Context":"## Context\nWe need to allow a number of users to sign in to the service in order to use it.\nIn order to implement this quickly, we'll use Auth0 to manage this.\nAs Auth0's authentication uses OAuth2, it should be straightforward to migrate\nto another service, if BEIS have a preference for something else.\nAuth0 provides views for authentication that we can use in our user journeys.\nThere are two versions of these views 'Classic' and 'New'.\nClassic uses JavaScript and is not progressively enhanced. New uses language\nthat results in a poor user experience.\n","Decision":"We will use the free tier and 'Classic' views of Auth0 for the private beta.\\n","tokens":127,"id":2405,"text":"## Context\\nWe need to allow a number of users to sign in to the service in order to use it.\\nIn order to implement this quickly, we'll use Auth0 to manage this.\\nAs Auth0's authentication uses OAuth2, it should be straightforward to migrate\\nto another service, if BEIS have a preference for something else.\\nAuth0 provides views for authentication that we can use in our user journeys.\\nThere are two versions of these views 'Classic' and 'New'.\\nClassic uses JavaScript and is not progressively enhanced. New uses language\\nthat results in a poor user experience.\\n\n\n##Decision\nWe will use the free tier and 'Classic' views of Auth0 for the private beta.\\n","Predictions":"We will use Rollbar to collect and manage our application errors.n"}
{"File Name":"report-a-defect\/0011-use-auth0-for-authentication.md","Context":"## Context\nAt the moment and for the short term future Hackney do not have a clear candidate for a single sign-on provider. They are reviewing the situation and will decide on a future solution before the end of 2019.\nWe have so far been using Basic Authentication.\nAs with other decisions we have attempted to follow the path set by the Repairs Hub which is another Hackney Rails app that is more mature. We asked what they were using and the answer was Microsoft Azure Active Directory using OpenID Connect (OAuth2). We believed we could get the exact same provisioning when the time came for Report a Defect to integrate with a single sign-on provider, however when the time came we learnt that it wasn't supported by Hackney and should be thought of as experimental.\nAs we had 2 weeks left we discussed with Hackney Architect Keith Gatt the possibility of using Auth0 as a temporary option to enable us to meet the user need of providing agent names within the service, as well as security needs through the use of a logged authentication service that provided one set of credentials per person, rather than a single set for basic auth.\n","Decision":"Use Auth0 as the single sign-on provider, and remove basic auth from production.\\n","tokens":234,"id":5211,"text":"## Context\\nAt the moment and for the short term future Hackney do not have a clear candidate for a single sign-on provider. They are reviewing the situation and will decide on a future solution before the end of 2019.\\nWe have so far been using Basic Authentication.\\nAs with other decisions we have attempted to follow the path set by the Repairs Hub which is another Hackney Rails app that is more mature. We asked what they were using and the answer was Microsoft Azure Active Directory using OpenID Connect (OAuth2). We believed we could get the exact same provisioning when the time came for Report a Defect to integrate with a single sign-on provider, however when the time came we learnt that it wasn't supported by Hackney and should be thought of as experimental.\\nAs we had 2 weeks left we discussed with Hackney Architect Keith Gatt the possibility of using Auth0 as a temporary option to enable us to meet the user need of providing agent names within the service, as well as security needs through the use of a logged authentication service that provided one set of credentials per person, rather than a single set for basic auth.\\n\n\n##Decision\nUse Auth0 as the single sign-on provider, and remove basic auth from production.\\n","Predictions":"We will use Auth0 as the single sign-on provider, and remove basic auth from production.n"}
{"File Name":"publicservice-registry\/0010-do-not-use-clr-type-names-for-event-types.md","Context":"## Context\nLooking at the SqlStreamStore code, we noticed a warning against using the CLR type name as the event type in your event store.\nThe reason behind this is that your message types will outlive your .net CLR types. Moving events along namespaces will break stuff.\n","Decision":"Use a dictionary\/map between your message types and the CLR type you want to deserialize your message into.\\n","tokens":57,"id":3129,"text":"## Context\\nLooking at the SqlStreamStore code, we noticed a warning against using the CLR type name as the event type in your event store.\\nThe reason behind this is that your message types will outlive your .net CLR types. Moving events along namespaces will break stuff.\\n\n\n##Decision\nUse a dictionary\/map between your message types and the CLR type you want to deserialize your message into.\\n","Predictions":"We will use the state package to store the current validator set.n"}
{"File Name":"react-library-template\/0006-use-jest.md","Context":"## Context\nWe want a test framework that has good support for React and TypeScript.\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\napps.\n","Decision":"We will use Jest as our testing framework.\\n","tokens":40,"id":4223,"text":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n\n\n##Decision\nWe will use Jest as our testing framework.\\n","Predictions":"We will use Jest as our testing framework.n"}
{"File Name":"paas-team-manual\/ADR016-end-to-end-encryption.md","Context":"Context\n=======\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\nThere are 3 main network sections between the user and the application:\n* User to ELB\n* ELB to router\n* Router to cells\nDecision\n========\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\nStatus\n======\nAccepted\nConsequences\n============\nThe traffic is encrypted end-to-end between the user and the applications.\n","Decision":"========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","tokens":247,"id":216,"text":"Context\\n=======\\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\\nThere are 3 main network sections between the user and the application:\\n* User to ELB\\n* ELB to router\\n* Router to cells\\nDecision\\n========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n\n\n##Decision\n========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","Predictions":"* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.nStatusn========nAcceptednConsequencesn=============nThe traffic is encrypted end-to-end between the user and the applications.n"}
{"File Name":"datalab\/0011-cluster-authentication-by-ssh-key.md","Context":"## Context\nWe need to configure access to the servers in the cluster and need to decide between\npassword authentication and SSH key pair authentication.\n","Decision":"We have decided that all server log on authentication will use SSH key pair\\nauthentication. The public keys will be provisioned onto the server at server creation\\nfor the default deploy user. A password will be required for sudo.\\n","tokens":29,"id":731,"text":"## Context\\nWe need to configure access to the servers in the cluster and need to decide between\\npassword authentication and SSH key pair authentication.\\n\n\n##Decision\nWe have decided that all server log on authentication will use SSH key pair\\nauthentication. The public keys will be provisioned onto the server at server creation\\nfor the default deploy user. A password will be required for sudo.\\n","Predictions":"We have decided that all server log on authentication will use SSH key pairnauthentication. The public keys will be provisioned onto the server at server creationnfor the default deploy user. A password will be required for sudo.n"}
{"File Name":"teaching-vacancies\/0011_use_devcontainers.md","Context":"## Context and Problem Statement\nSetting up the Teaching Vacancies app for local development is a frustrating process involving many\nsteps and multiple dependencies driven by manually updated setup documentation. A workflow based on\n[devcontainers](https:\/\/code.visualstudio.com\/docs\/remote\/create-dev-container) would alleviate\nmuch of this setup pain, and provide a trivially reproducible environment for local development,\nbenefitting both developers and non-developers on the team.\n## Decision Drivers\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\nenable users on restricted organisation-managed devices to contribute to the application\n","Decision":"- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\nAdd devcontainers as an option for now, with a view to iterate on it and improve it to the point\\nwhere we can consider it the \"official\" default way of running Teaching Vacancies (while still\\nallowing other development workflows for developers who prefer different ways of working).\\n### Positive Consequences\\n- Drastically easier onboarding and \"re-boarding\" (e.g. on a new device or after an OS upgrade\\ncausing developer tooling issues)\\n- Dependencies reduced to just Git, Docker, and VS Code\\n- A fully functioning development environment is ready in 10 minutes from scratch, with no user\\ninteraction beyond opening the repository in VS Code and selecting \"Reopen in container\"\\n- Moving entirety of development experience into a container fixes past Docker development workflow\\nissues experienced on the team (where tasks and services where executed from the host instead of\\ninteracting with a shell and an editor from inside the container itself)\\n- Developers and other team members can develop on any host OS (macOS\/Linux\/Windows) but we only\\nneed to support one single consistent environment\\n- Does away with all the Mac vs Linux vs WSL setup steps in our current documentation\\n- Reduces likelihood of \"works on my machine\" development environment issues\\n- \"Leave no trace\" on the host machine and complete isolation from other projects\\n- Removes possibility of \"dependency hell\" when working on multiple projects\\n- Removes need to clutter local environment with applications and dependencies that need to be\\nkept up to date and in sync (e.g. Google Chrome and `chromedriver`)\\n- Removes need for language version managers (`rbenv`, `nvm`)\\n- Provides _executable documentation_ of project setup and dependencies\\n- Removes need for manually updated setup documentation that can become stale\\n- Experienced developers who have a different preferred workflow can get a clear, in-code view\\nof setup steps and dependencies\\n- Good workflow for everyone, but excellent additional integration with Visual Studio Code\\n- Automatic passthrough of SSH and GPG keys\\n- Language extensions run within the container itself, and can be specified in the devcontainer\\nconfiguration file for instant setup of useful extensions for new users\\n- Automatic bootstrapping of personal dotfiles\\n- Ability to easily move to cloud-based workflows in the future\\n- Ability to easily propagate new tools and improved configuration to all developers on the team\\n- Trivial rebuilds to a known good state when performing \"dangerous\" operations in the container\\n### Negative Consequences\\n- Slightly reduced performance on some host OSs (non-Linux) due to Docker being Linux-native\\ntechnology (overhead of containers running in an intermediate VM)\\n- Somewhat mitigated by use of volumes for IO-intensive cache directories\\n- Can be worked around entirely by moving workspace root into the container, and we will continue\\nto investigate before we fully agree on devcontainers as our default workflow\\n- Container layers need occasional pruning on the host as Docker can fill up disk space quickly\\n- Some duplication of Docker configuration between production and development configuration (but\\nthat is to be expected given that use cases are very different)\\n","tokens":224,"id":4370,"text":"## Context and Problem Statement\\nSetting up the Teaching Vacancies app for local development is a frustrating process involving many\\nsteps and multiple dependencies driven by manually updated setup documentation. A workflow based on\\n[devcontainers](https:\/\/code.visualstudio.com\/docs\/remote\/create-dev-container) would alleviate\\nmuch of this setup pain, and provide a trivially reproducible environment for local development,\\nbenefitting both developers and non-developers on the team.\\n## Decision Drivers\\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\n\n\n##Decision\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\nAdd devcontainers as an option for now, with a view to iterate on it and improve it to the point\\nwhere we can consider it the \"official\" default way of running Teaching Vacancies (while still\\nallowing other development workflows for developers who prefer different ways of working).\\n### Positive Consequences\\n- Drastically easier onboarding and \"re-boarding\" (e.g. on a new device or after an OS upgrade\\ncausing developer tooling issues)\\n- Dependencies reduced to just Git, Docker, and VS Code\\n- A fully functioning development environment is ready in 10 minutes from scratch, with no user\\ninteraction beyond opening the repository in VS Code and selecting \"Reopen in container\"\\n- Moving entirety of development experience into a container fixes past Docker development workflow\\nissues experienced on the team (where tasks and services where executed from the host instead of\\ninteracting with a shell and an editor from inside the container itself)\\n- Developers and other team members can develop on any host OS (macOS\/Linux\/Windows) but we only\\nneed to support one single consistent environment\\n- Does away with all the Mac vs Linux vs WSL setup steps in our current documentation\\n- Reduces likelihood of \"works on my machine\" development environment issues\\n- \"Leave no trace\" on the host machine and complete isolation from other projects\\n- Removes possibility of \"dependency hell\" when working on multiple projects\\n- Removes need to clutter local environment with applications and dependencies that need to be\\nkept up to date and in sync (e.g. Google Chrome and `chromedriver`)\\n- Removes need for language version managers (`rbenv`, `nvm`)\\n- Provides _executable documentation_ of project setup and dependencies\\n- Removes need for manually updated setup documentation that can become stale\\n- Experienced developers who have a different preferred workflow can get a clear, in-code view\\nof setup steps and dependencies\\n- Good workflow for everyone, but excellent additional integration with Visual Studio Code\\n- Automatic passthrough of SSH and GPG keys\\n- Language extensions run within the container itself, and can be specified in the devcontainer\\nconfiguration file for instant setup of useful extensions for new users\\n- Automatic bootstrapping of personal dotfiles\\n- Ability to easily move to cloud-based workflows in the future\\n- Ability to easily propagate new tools and improved configuration to all developers on the team\\n- Trivial rebuilds to a known good state when performing \"dangerous\" operations in the container\\n### Negative Consequences\\n- Slightly reduced performance on some host OSs (non-Linux) due to Docker being Linux-native\\ntechnology (overhead of containers running in an intermediate VM)\\n- Somewhat mitigated by use of volumes for IO-intensive cache directories\\n- Can be worked around entirely by moving workspace root into the container, and we will continue\\nto investigate before we fully agree on devcontainers as our default workflow\\n- Container layers need occasional pruning on the host as Docker can fill up disk space quickly\\n- Some duplication of Docker configuration between production and development configuration (but\\nthat is to be expected given that use cases are very different)\\n","Predictions":"- Complex and time-consuming onboarding and \"re-boarding\" experience of the applicationn- Difficulties experiences by non-developers in getting the app set up locally, and getting itnrunning again after major dependency changes (e.g. our recent addition of PostGIS)n- Increasing adoption of devcontainers as a de-facto standard in the wider development communitynincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)n- Possible use of cloud-based development environments such as Github Codespaces in the future tonenable users on restricted organisation-managed devices to contribute to the applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with a view to iterate on it and improve it to the pointnwhere we can consider it the \"official\" applicationnAdd devcontainers as an option for now, with"}
{"File Name":"copilot\/0006-do-something-for-one-way-tls.md","Context":"## Context\nEnable one way TLS between front-end and envoy per host+domain that is\nspecified via gateway config.\n","Decision":"#### Gateway Configuration\\nThis is achieved by sending the following config from copilot.\\n```\\napiVersion: networking.istio.io\/v1alpha3\\nkind: Gateway\\nmetadata:\\nname: mygateway\\nspec:\\nselector:\\nistio: ingressgateway # use istio default ingress gateway\\nservers:\\n- port:\\nnumber: 443\\nname: https-httpbin\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-certs\/tls.key\\nhosts:\\n- \"httpbin.example.com\"\\n- port:\\nnumber: 443\\nname: https-bookinfo\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.key\\nhosts:\\n- \"bookinfo.com\"\\n```\\nIn the config above each cert and key in the array of servers represent a\\nhost+domain and the path to each cert and the key is arbitrarily chosen.\\nCopilot extracts the domain information from the cert chains provided in the bosh spec properties:\\n```\\nfrontend_tls_keypairs:\\nexample:\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n```\\n#### Cert Storage\\nThe placement of the certs and keys on the envoy VM is done using a separate\\nprocess specific to this purpose. This process will be in charge of knowing\\nwhere the certs are located and placing the certs on the correct paths. It is\\nimportant for the envoy VM and copilot to agree on a path where the cert and the keys\\nare stored, and having a specific process to manage this will reduce duplication\\nand mitigate skew.\\n","tokens":25,"id":50,"text":"## Context\\nEnable one way TLS between front-end and envoy per host+domain that is\\nspecified via gateway config.\\n\n\n##Decision\n#### Gateway Configuration\\nThis is achieved by sending the following config from copilot.\\n```\\napiVersion: networking.istio.io\/v1alpha3\\nkind: Gateway\\nmetadata:\\nname: mygateway\\nspec:\\nselector:\\nistio: ingressgateway # use istio default ingress gateway\\nservers:\\n- port:\\nnumber: 443\\nname: https-httpbin\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-certs\/tls.key\\nhosts:\\n- \"httpbin.example.com\"\\n- port:\\nnumber: 443\\nname: https-bookinfo\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.key\\nhosts:\\n- \"bookinfo.com\"\\n```\\nIn the config above each cert and key in the array of servers represent a\\nhost+domain and the path to each cert and the key is arbitrarily chosen.\\nCopilot extracts the domain information from the cert chains provided in the bosh spec properties:\\n```\\nfrontend_tls_keypairs:\\nexample:\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n```\\n#### Cert Storage\\nThe placement of the certs and keys on the envoy VM is done using a separate\\nprocess specific to this purpose. This process will be in charge of knowing\\nwhere the certs are located and placing the certs on the correct paths. It is\\nimportant for the envoy VM and copilot to agree on a path where the cert and the keys\\nare stored, and having a specific process to manage this will reduce duplication\\nand mitigate skew.\\n","Predictions":"- Enable one way TLS between front-end and envoy per host+domain that isnspecified via gateway config.n- APIVersion: networking.istio.io\/v1alpha3n- name: mygatewaynspec:nselector:nistio: ingressgateway # use istio default ingress gatewaynservers:n- port:nnumber: 443nname: https-httpbinnprotocol: HTTPSntls:nmode: SIMPLEnserverCertificate: \/etc\/istio\/ingressgateway-certs\/tls.crtnprivateKey: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.keynhosts:n- \"httpbin.example.com\"nnIn the config above each cert and key in the array of servers represent anhost+domain and the path to each cert and the key is arbitrarily chosen.nCopilot extracts the domain information from the cert chains provided in the bosh spec properties:nfrontend_tls_keypairs:nexample:n- cert_chain: |n----BEGIN CERTIFICATE-----n----END CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN CERTIFICATE-----n----BEGIN "}
{"File Name":"libelektra\/reference_counting.md","Context":"## Problem\n- locking is not reset when ref counting again gets 0 (adding to keyset and\npop again) #2202\n- C++ API for KeySet and Key has unexpected differences: also use ref counting\nfor KeySets (also suggested in #1332)\n","Decision":"- add second counter to Key\\n- One counter is for references, the other one is for locking the keyname. The keyname is thereby protected with a re-entrant lock.\\n- introduce reference counter for KeySets (for external keyset references, e.g. in bindings)\\n- limit number of references to `UINT16_MAX - 1` and use `UINT16_MAX` as an error value\\n- return error on reference count overflow\\n- no error on underflow (decrement when zero), instead stay at zero\\n- use fixed sized types (`uint16_t`) for reference counters\\n- increment\/decrement references before\/after passing instances to plugins\\n","tokens":59,"id":1309,"text":"## Problem\\n- locking is not reset when ref counting again gets 0 (adding to keyset and\\npop again) #2202\\n- C++ API for KeySet and Key has unexpected differences: also use ref counting\\nfor KeySets (also suggested in #1332)\\n\n\n##Decision\n- add second counter to Key\\n- One counter is for references, the other one is for locking the keyname. The keyname is thereby protected with a re-entrant lock.\\n- introduce reference counter for KeySets (for external keyset references, e.g. in bindings)\\n- limit number of references to `UINT16_MAX - 1` and use `UINT16_MAX` as an error value\\n- return error on reference count overflow\\n- no error on underflow (decrement when zero), instead stay at zero\\n- use fixed sized types (`uint16_t`) for reference counters\\n- increment\/decrement references before\/after passing instances to plugins\\n","Predictions":"- lock is reset when ref counting again gets 0 (adding to keyset andnpop again) #2202n- C++ API for KeySet and Key has unexpected differences: also use ref countingnfor KeySets (also suggested in #1332)n"}
{"File Name":"modernisation-platform\/0011-use-vpc-flow-logs-to-gain-insight-into-network-state.md","Context":"## Context\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence.\n","Decision":"[VPC flow flows](https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/flow-logs.html) contain information about data coming into and out of attached network interfaces. As such, flow log data could be collected, aggregated, analysed and visualised in order to provide insights into the traffic flowing (or not) through VPCs. VPC flow logs are already collected in environment accounts and at the platform-level in the core-logging account, within CloudWatch log groups.\\n","tokens":65,"id":3150,"text":"## Context\\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence.\\n\n\n##Decision\n[VPC flow flows](https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/flow-logs.html) contain information about data coming into and out of attached network interfaces. As such, flow log data could be collected, aggregated, analysed and visualised in order to provide insights into the traffic flowing (or not) through VPCs. VPC flow logs are already collected in environment accounts and at the platform-level in the core-logging account, within CloudWatch log groups.\\n","Predictions":"* elasticity and resiliencen* pay-as-you-go pricingn* exploiting economies of scalen* exploiting falling costs of infrastructure over timen* quality of management toolsn* best of breed securityn* flexibility and opportunity costsnWe will use the [CloudWatch](https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/flow-logs.html) to provide a single, centralised, unified view of the network.n"}
{"File Name":"old-web-frontend\/0002-use-aws-s3-as-host-solution.md","Context":"## Context and Problem Statement\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\n## Decision Drivers\n* Easy to update\n* High availability\n* Easy to configure HTTPS\n* Observability\n","Decision":"* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\nChosen option: \"AWS\", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\\n","tokens":53,"id":583,"text":"## Context and Problem Statement\\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\\n## Decision Drivers\\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\n\n\n##Decision\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\nChosen option: \"AWS\", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\\n","Predictions":"* Easy to usen* High availabilityn* Easy to configure HTTPSnChosen option: \"[option 1]\", because it is well known in thenindustry. It is not the most performant but it has good performancento serve WSGI app like Flask thanks to the uwsgi protocol used betweennNginx\/Apache.n### Negative Consequencesn* complexify the architecture.n"}
{"File Name":"js-sdk\/0008-add-kwargs-to-3bot-start.md","Context":"## Context\nAdding packages with kwargs has some limitations and hence kwargs are needed every time start is called not only once when adding package.\n","Decision":"Add kwargs passed to the package instance that will be saved locally, and can be retrieved everytime the threebot server restarts and starts the package.\\n","tokens":28,"id":5188,"text":"## Context\\nAdding packages with kwargs has some limitations and hence kwargs are needed every time start is called not only once when adding package.\\n\n\n##Decision\nAdd kwargs passed to the package instance that will be saved locally, and can be retrieved everytime the threebot server restarts and starts the package.\\n","Predictions":"* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.n* The feature configuration from Feature::getAll() is converted to a SCSS map inside ShopwareStorefrontThemeThemeCompiler::getFeatureConfigScssMap.n* This SCSS map is always added to the SCSS string which gets processed by ShopwareStorefrontThemeThemeCompiler::compileTheme.n* For webpack hot-proxy the var\/config_js_features.json is used instead.n* The SCSS map looks like this: $sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);n* See https:\/\/sass-lang.com\/documentation\/values\/mapsn* A globally available function feature() is used to read inside the SCSS map if a desired feature is active.nExample:nscssnbody n@if feature('FEATURE_NEXT_1') nbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svgnbackground-color.svg"}
{"File Name":"front-end-monorepo\/adr-19.md","Context":"## Context\nAs part of the lib-classifier, we are developing a component for displaying multi-frame subjects in the classification interface. Initially, we referred to this viewer as the PagesViewer, but changed the name to multiFrame Viewer to reflect more general use cases. In the first iteration, we will focus on multiImage features necessary for transcription projects. As projects with different requirements, such as ecology projects, move to the new classifier, we can add new features to the filmstrip component (see decisions 6 an 7).\nSee the [InVision document for the Pages Viewer](https:\/\/projects.invisionapp.com\/d\/main#\/console\/12924056\/393421254\/preview) and Issue #1142.\n","Decision":"1. **When to Render:** There will be an explicit workflow configuration (i.e., `{ subject_viewer: 'multiFrame' }`) for the multiFrame Viewer. However, as a fallback, the multiFrame subject viewer will render if (1) the subject delivered to the client has multiple frames and (2) the workflow is not configured to display all frames at once.\\n2. **Number of Frames:** The multiFrame Viewer will display up to ten frames. All frames after the tenth frame will be ignored.\\n3. **Frame Display**: For every frame that is an image, the filmstrip will render a thumbnail of the image. The fallback for non image frames will be a dot. Later, we will need to think about subjects with mixed media frames. HTML Inputs will be used instead of HTML buttons to display each frame.\\n4. **Selected Image Display**: The selected image will be displayed using the SingleImageViewer.\\n5. **Location of Filmstrip:** The filmstrip will be located to the left of the displayed image. This will help for images that are tall as well as make the interface easier to use for volunteers navigating by keyboard or screen reader (see https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1262).\\n6. **Playing Slideshow**: Although adding a slideshow play button will be necessary for non-transcription projects, at this time, the play button is a necessary enhancement\/future feature.\\n7. **Collapsing Filmstrip**: Adding a button to collapse the image previews into pagination dots is an enhancement\/future feature.\\n","tokens":147,"id":498,"text":"## Context\\nAs part of the lib-classifier, we are developing a component for displaying multi-frame subjects in the classification interface. Initially, we referred to this viewer as the PagesViewer, but changed the name to multiFrame Viewer to reflect more general use cases. In the first iteration, we will focus on multiImage features necessary for transcription projects. As projects with different requirements, such as ecology projects, move to the new classifier, we can add new features to the filmstrip component (see decisions 6 an 7).\\nSee the [InVision document for the Pages Viewer](https:\/\/projects.invisionapp.com\/d\/main#\/console\/12924056\/393421254\/preview) and Issue #1142.\\n\n\n##Decision\n1. **When to Render:** There will be an explicit workflow configuration (i.e., `{ subject_viewer: 'multiFrame' }`) for the multiFrame Viewer. However, as a fallback, the multiFrame subject viewer will render if (1) the subject delivered to the client has multiple frames and (2) the workflow is not configured to display all frames at once.\\n2. **Number of Frames:** The multiFrame Viewer will display up to ten frames. All frames after the tenth frame will be ignored.\\n3. **Frame Display**: For every frame that is an image, the filmstrip will render a thumbnail of the image. The fallback for non image frames will be a dot. Later, we will need to think about subjects with mixed media frames. HTML Inputs will be used instead of HTML buttons to display each frame.\\n4. **Selected Image Display**: The selected image will be displayed using the SingleImageViewer.\\n5. **Location of Filmstrip:** The filmstrip will be located to the left of the displayed image. This will help for images that are tall as well as make the interface easier to use for volunteers navigating by keyboard or screen reader (see https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1262).\\n6. **Playing Slideshow**: Although adding a slideshow play button will be necessary for non-transcription projects, at this time, the play button is a necessary enhancement\/future feature.\\n7. **Collapsing Filmstrip**: Adding a button to collapse the image previews into pagination dots is an enhancement\/future feature.\\n","Predictions":"We will use the [Semantic Graph](https:\/\/github.com\/SemanticGraph) library to generate Graphs.n"}
{"File Name":"govuk-docker\/0004-use-versioned-database-services.md","Context":"## Context\nGOV.UK Docker has services defined for various databases, such as PostgreSQL,\nMongoDB and Elasticsearch. These have their data persisted in\n[Docker volumes][volume] defined [for each service][defined-volumes].\nWhen GOV.UK Docker updated from [MongoDB 2.4 to 3.6][mongodb-update] users\nbegan receiving cryptic errors when trying to run applications that depended\non MongoDB. This was because they had a volume with data structured for MongoDB\n2.4 whereas MongoDB was expecting this to be structured for 3.6.\nWe felt that this upgrade path was more confusing and difficult than it needed\nbe and wanted to improve it for the future.\n[volume]: https:\/\/docs.docker.com\/storage\/volumes\/\n[defined-volumes]: https:\/\/github.com\/alphagov\/govuk-docker\/blob\/cb124a3a1d3353e777d4f777d77f03f93415d415\/docker-compose.yml#L5-L10\n[mongodb-update]: https:\/\/github.com\/alphagov\/govuk-docker\/pull\/356\n","Decision":"We decided that we would rename database services, and their respective\\nvolumes, to reflect the software version number. For example the `mongo`\\nservice, and namesake volume, have been renamed to `mongo-3.6`.\\nThe precision of the version number is chosen based on the backwards\\ncompatibility between versions of the software. For example to upgrade from\\nMySQL 5.5 to 5.6 requires an upgrade script and thus would cause problems for\\nGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not. For\\nservices that aren't sensitive to minor versions, such as Elasticsearch, we\\nhave specified the service and volume with respective to a major version:\\n`elasticsearch-6`.\\n","tokens":238,"id":1666,"text":"## Context\\nGOV.UK Docker has services defined for various databases, such as PostgreSQL,\\nMongoDB and Elasticsearch. These have their data persisted in\\n[Docker volumes][volume] defined [for each service][defined-volumes].\\nWhen GOV.UK Docker updated from [MongoDB 2.4 to 3.6][mongodb-update] users\\nbegan receiving cryptic errors when trying to run applications that depended\\non MongoDB. This was because they had a volume with data structured for MongoDB\\n2.4 whereas MongoDB was expecting this to be structured for 3.6.\\nWe felt that this upgrade path was more confusing and difficult than it needed\\nbe and wanted to improve it for the future.\\n[volume]: https:\/\/docs.docker.com\/storage\/volumes\/\\n[defined-volumes]: https:\/\/github.com\/alphagov\/govuk-docker\/blob\/cb124a3a1d3353e777d4f777d77f03f93415d415\/docker-compose.yml#L5-L10\\n[mongodb-update]: https:\/\/github.com\/alphagov\/govuk-docker\/pull\/356\\n\n\n##Decision\nWe decided that we would rename database services, and their respective\\nvolumes, to reflect the software version number. For example the `mongo`\\nservice, and namesake volume, have been renamed to `mongo-3.6`.\\nThe precision of the version number is chosen based on the backwards\\ncompatibility between versions of the software. For example to upgrade from\\nMySQL 5.5 to 5.6 requires an upgrade script and thus would cause problems for\\nGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not. For\\nservices that aren't sensitive to minor versions, such as Elasticsearch, we\\nhave specified the service and volume with respective to a major version:\\n`elasticsearch-6`.\\n","Predictions":"We decided that we would rename database services, and their respectivenvolumes, to reflect the software version number. For example the mongonservice, and namesake volume, have been renamed to mongo-3.6.nThe precision of the version number is chosen based on the backwardsncompatibility between versions of the software. For example to upgrade fromnMySQL 5.5 to 5.6 requires an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not require an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not require an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not require an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not require an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not require an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not require an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not require an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not require an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not require an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not require an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not require an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not require an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not require an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not require an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not require an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not require an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not require an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not require an upgrade script and thus would cause problems fornGOV.UK Docker users. However, upgrading from"}
{"File Name":"james\/0004-distributed-tasks-listing.md","Context":"## Context\nBy switching the task manager to a distributed implementation, we need to be able to `list` all `Task`s running on the cluster.\n","Decision":"* Read a Cassandra projection to get all `Task`s and their `Status`\\n","tokens":32,"id":2140,"text":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to `list` all `Task`s running on the cluster.\\n\n\n##Decision\n* Read a Cassandra projection to get all `Task`s and their `Status`\\n","Predictions":"* Creating a RabbitMQEventHandler which publish Events pushed to the task manager's event system to RabbitMQn* All the events which end a Task (Completed, Failed, and Canceled) have to be transmitted to other nodesn"}
{"File Name":"cosmos-sdk\/adr-043-nft-module.md","Context":"## Context\nNFTs are more than just crypto art, which is very helpful for accruing value to the Cosmos ecosystem. As a result, Cosmos Hub should implement NFT functions and enable a unified mechanism for storing and sending the ownership representative of NFTs as discussed in https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065.\nAs discussed in [#9065](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065), several potential solutions can be considered:\n* irismod\/nft and modules\/incubator\/nft\n* CW721\n* DID NFTs\n* interNFT\nSince functions\/use cases of NFTs are tightly connected with their logic, it is almost impossible to support all the NFTs' use cases in one Cosmos SDK module by defining and implementing different transaction types.\nConsidering generic usage and compatibility of interchain protocols including IBC and Gravity Bridge, it is preferred to have a generic NFT module design which handles the generic NFTs logic.\nThis design idea can enable composability that application-specific functions should be managed by other modules on Cosmos Hub or on other Zones by importing the NFT module.\nThe current design is based on the work done by [IRISnet team](https:\/\/github.com\/irisnet\/irismod\/tree\/master\/modules\/nft) and an older implementation in the [Cosmos repository](https:\/\/github.com\/cosmos\/modules\/tree\/master\/incubator\/nft).\n","Decision":"We create a `x\/nft` module, which contains the following functionality:\\n* Store NFTs and track their ownership.\\n* Expose `Keeper` interface for composing modules to transfer, mint and burn NFTs.\\n* Expose external `Message` interface for users to transfer ownership of their NFTs.\\n* Query NFTs and their supply information.\\nThe proposed module is a base module for NFT app logic. It's goal it to provide a common layer for storage, basic transfer functionality and IBC. The module should not be used as a standalone.\\nInstead an app should create a specialized module to handle app specific logic (eg: NFT ID construction, royalty), user level minting and burning. Moreover an app specialized module should handle auxiliary data to support the app logic (eg indexes, ORM, business data).\\nAll data carried over IBC must be part of the `NFT` or `Class` type described below. The app specific NFT data should be encoded in `NFT.data` for cross-chain integrity. Other objects related to NFT, which are not important for integrity can be part of the app specific module.\\n### Types\\nWe propose two main types:\\n* `Class` -- describes NFT class. We can think about it as a smart contract address.\\n* `NFT` -- object representing unique, non fungible asset. Each NFT is associated with a Class.\\n#### Class\\nNFT **Class** is comparable to an ERC-721 smart contract (provides description of a smart contract), under which a collection of NFTs can be created and managed.\\n```protobuf\\nmessage Class {\\nstring id          = 1;\\nstring name        = 2;\\nstring symbol      = 3;\\nstring description = 4;\\nstring uri         = 5;\\nstring uri_hash    = 6;\\ngoogle.protobuf.Any data = 7;\\n}\\n```\\n* `id` is used as the primary index for storing the class; _required_\\n* `name` is a descriptive name of the NFT class; _optional_\\n* `symbol` is the symbol usually shown on exchanges for the NFT class; _optional_\\n* `description` is a detailed description of the NFT class; _optional_\\n* `uri` is a URI for the class metadata stored off chain. It should be a JSON file that contains metadata about the NFT class and NFT data schema ([OpenSea example](https:\/\/docs.opensea.io\/docs\/contract-level-metadata)); _optional_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is app specific metadata of the class; _optional_\\n#### NFT\\nWe define a general model for `NFT` as follows.\\n```protobuf\\nmessage NFT {\\nstring class_id           = 1;\\nstring id                 = 2;\\nstring uri                = 3;\\nstring uri_hash           = 4;\\ngoogle.protobuf.Any data  = 10;\\n}\\n```\\n* `class_id` is the identifier of the NFT class where the NFT belongs; _required_\\n* `id` is an identifier of the NFT, unique within the scope of its class. It is specified by the creator of the NFT and may be expanded to use DID in the future. `class_id` combined with `id` uniquely identifies an NFT and is used as the primary index for storing the NFT; _required_\\n```text\\n{class_id}\/{id} --> NFT (bytes)\\n```\\n* `uri` is a URI for the NFT metadata stored off chain. Should point to a JSON file that contains metadata about this NFT (Ref: [ERC721 standard and OpenSea extension](https:\/\/docs.opensea.io\/docs\/metadata-standards)); _required_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is an app specific data of the NFT. CAN be used by composing modules to specify additional properties of the NFT; _optional_\\nThis ADR doesn't specify values that `data` can take; however, best practices recommend upper-level NFT modules clearly specify their contents.  Although the value of this field doesn't provide the additional context required to manage NFT records, which means that the field can technically be removed from the specification, the field's existence allows basic informational\/UI functionality.\\n### `Keeper` Interface\\n```go\\ntype Keeper interface {\\nNewClass(ctx sdk.Context,class Class)\\nUpdateClass(ctx sdk.Context,class Class)\\nMint(ctx sdk.Context,nft NFT\uff0creceiver sdk.AccAddress)   \/\/ updates totalSupply\\nBatchMint(ctx sdk.Context, tokens []NFT,receiver sdk.AccAddress) error\\nBurn(ctx sdk.Context, classId string, nftId string)    \/\/ updates totalSupply\\nBatchBurn(ctx sdk.Context, classID string, nftIDs []string) error\\nUpdate(ctx sdk.Context, nft NFT)\\nBatchUpdate(ctx sdk.Context, tokens []NFT) error\\nTransfer(ctx sdk.Context, classId string, nftId string, receiver sdk.AccAddress)\\nBatchTransfer(ctx sdk.Context, classID string, nftIDs []string, receiver sdk.AccAddress) error\\nGetClass(ctx sdk.Context, classId string) Class\\nGetClasses(ctx sdk.Context) []Class\\nGetNFT(ctx sdk.Context, classId string, nftId string) NFT\\nGetNFTsOfClassByOwner(ctx sdk.Context, classId string, owner sdk.AccAddress) []NFT\\nGetNFTsOfClass(ctx sdk.Context, classId string) []NFT\\nGetOwner(ctx sdk.Context, classId string, nftId string) sdk.AccAddress\\nGetBalance(ctx sdk.Context, classId string, owner sdk.AccAddress) uint64\\nGetTotalSupply(ctx sdk.Context, classId string) uint64\\n}\\n```\\nOther business logic implementations should be defined in composing modules that import `x\/nft` and use its `Keeper`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\nrpc Send(MsgSend)         returns (MsgSendResponse);\\n}\\nmessage MsgSend {\\nstring class_id = 1;\\nstring id       = 2;\\nstring sender   = 3;\\nstring reveiver = 4;\\n}\\nmessage MsgSendResponse {}\\n```\\n`MsgSend` can be used to transfer the ownership of an NFT to another address.\\nThe implementation outline of the server is as follows:\\n```go\\ntype msgServer struct{\\nk Keeper\\n}\\nfunc (m msgServer) Send(ctx context.Context, msg *types.MsgSend) (*types.MsgSendResponse, error) {\\n\/\/ check current ownership\\nassertEqual(msg.Sender, m.k.GetOwner(msg.ClassId, msg.Id))\\n\/\/ transfer ownership\\nm.k.Transfer(msg.ClassId, msg.Id, msg.Receiver)\\nreturn &types.MsgSendResponse{}, nil\\n}\\n```\\nThe query service methods for the `x\/nft` module are:\\n```protobuf\\nservice Query {\\n\/\/ Balance queries the number of NFTs of a given class owned by the owner, same as balanceOf in ERC721\\nrpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/balance\/{owner}\/{class_id}\";\\n}\\n\/\/ Owner queries the owner of the NFT based on its class and id, same as ownerOf in ERC721\\nrpc Owner(QueryOwnerRequest) returns (QueryOwnerResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/owner\/{class_id}\/{id}\";\\n}\\n\/\/ Supply queries the number of NFTs from the given class, same as totalSupply of ERC721.\\nrpc Supply(QuerySupplyRequest) returns (QuerySupplyResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/supply\/{class_id}\";\\n}\\n\/\/ NFTs queries all NFTs of a given class or owner,choose at least one of the two, similar to tokenByIndex in ERC721Enumerable\\nrpc NFTs(QueryNFTsRequest) returns (QueryNFTsResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\";\\n}\\n\/\/ NFT queries an NFT based on its class and id.\\nrpc NFT(QueryNFTRequest) returns (QueryNFTResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\/{class_id}\/{id}\";\\n}\\n\/\/ Class queries an NFT class based on its id\\nrpc Class(QueryClassRequest) returns (QueryClassResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\/{class_id}\";\\n}\\n\/\/ Classes queries all NFT classes\\nrpc Classes(QueryClassesRequest) returns (QueryClassesResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\";\\n}\\n}\\n\/\/ QueryBalanceRequest is the request type for the Query\/Balance RPC method\\nmessage QueryBalanceRequest {\\nstring class_id = 1;\\nstring owner    = 2;\\n}\\n\/\/ QueryBalanceResponse is the response type for the Query\/Balance RPC method\\nmessage QueryBalanceResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryOwnerRequest is the request type for the Query\/Owner RPC method\\nmessage QueryOwnerRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryOwnerResponse is the response type for the Query\/Owner RPC method\\nmessage QueryOwnerResponse {\\nstring owner = 1;\\n}\\n\/\/ QuerySupplyRequest is the request type for the Query\/Supply RPC method\\nmessage QuerySupplyRequest {\\nstring class_id = 1;\\n}\\n\/\/ QuerySupplyResponse is the response type for the Query\/Supply RPC method\\nmessage QuerySupplyResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryNFTstRequest is the request type for the Query\/NFTs RPC method\\nmessage QueryNFTsRequest {\\nstring                                class_id   = 1;\\nstring                                owner      = 2;\\ncosmos.base.query.v1beta1.PageRequest pagination = 3;\\n}\\n\/\/ QueryNFTsResponse is the response type for the Query\/NFTs RPC methods\\nmessage QueryNFTsResponse {\\nrepeated cosmos.nft.v1beta1.NFT        nfts       = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n\/\/ QueryNFTRequest is the request type for the Query\/NFT RPC method\\nmessage QueryNFTRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryNFTResponse is the response type for the Query\/NFT RPC method\\nmessage QueryNFTResponse {\\ncosmos.nft.v1beta1.NFT nft = 1;\\n}\\n\/\/ QueryClassRequest is the request type for the Query\/Class RPC method\\nmessage QueryClassRequest {\\nstring class_id = 1;\\n}\\n\/\/ QueryClassResponse is the response type for the Query\/Class RPC method\\nmessage QueryClassResponse {\\ncosmos.nft.v1beta1.Class class = 1;\\n}\\n\/\/ QueryClassesRequest is the request type for the Query\/Classes RPC method\\nmessage QueryClassesRequest {\\n\/\/ pagination defines an optional pagination for the request.\\ncosmos.base.query.v1beta1.PageRequest pagination = 1;\\n}\\n\/\/ QueryClassesResponse is the response type for the Query\/Classes RPC method\\nmessage QueryClassesResponse {\\nrepeated cosmos.nft.v1beta1.Class      classes    = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n```\\n### Interoperability\\nInteroperability is all about reusing assets between modules and chains. The former one is achieved by ADR-33: Protobuf client - server communication. At the time of writing ADR-33 is not finalized. The latter is achieved by IBC. Here we will focus on the IBC side.\\nIBC is implemented per module. Here, we aligned that NFTs will be recorded and managed in the x\/nft. This requires creation of a new IBC standard and implementation of it.\\nFor IBC interoperability, NFT custom modules MUST use the NFT object type understood by the IBC client. So, for x\/nft interoperability, custom NFT implementations (example: x\/cryptokitty) should use the canonical x\/nft module and proxy all NFT balance keeping functionality to x\/nft or else re-implement all functionality using the NFT object type understood by the IBC client. In other words: x\/nft becomes the standard NFT registry for all Cosmos NFTs (example: x\/cryptokitty will register a kitty NFT in x\/nft and use x\/nft for book keeping). This was [discussed](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065#discussioncomment-873206) in the context of using x\/bank as a general asset balance book. Not using x\/nft will require implementing another module for IBC.\\n","tokens":307,"id":827,"text":"## Context\\nNFTs are more than just crypto art, which is very helpful for accruing value to the Cosmos ecosystem. As a result, Cosmos Hub should implement NFT functions and enable a unified mechanism for storing and sending the ownership representative of NFTs as discussed in https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065.\\nAs discussed in [#9065](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065), several potential solutions can be considered:\\n* irismod\/nft and modules\/incubator\/nft\\n* CW721\\n* DID NFTs\\n* interNFT\\nSince functions\/use cases of NFTs are tightly connected with their logic, it is almost impossible to support all the NFTs' use cases in one Cosmos SDK module by defining and implementing different transaction types.\\nConsidering generic usage and compatibility of interchain protocols including IBC and Gravity Bridge, it is preferred to have a generic NFT module design which handles the generic NFTs logic.\\nThis design idea can enable composability that application-specific functions should be managed by other modules on Cosmos Hub or on other Zones by importing the NFT module.\\nThe current design is based on the work done by [IRISnet team](https:\/\/github.com\/irisnet\/irismod\/tree\/master\/modules\/nft) and an older implementation in the [Cosmos repository](https:\/\/github.com\/cosmos\/modules\/tree\/master\/incubator\/nft).\\n\n\n##Decision\nWe create a `x\/nft` module, which contains the following functionality:\\n* Store NFTs and track their ownership.\\n* Expose `Keeper` interface for composing modules to transfer, mint and burn NFTs.\\n* Expose external `Message` interface for users to transfer ownership of their NFTs.\\n* Query NFTs and their supply information.\\nThe proposed module is a base module for NFT app logic. It's goal it to provide a common layer for storage, basic transfer functionality and IBC. The module should not be used as a standalone.\\nInstead an app should create a specialized module to handle app specific logic (eg: NFT ID construction, royalty), user level minting and burning. Moreover an app specialized module should handle auxiliary data to support the app logic (eg indexes, ORM, business data).\\nAll data carried over IBC must be part of the `NFT` or `Class` type described below. The app specific NFT data should be encoded in `NFT.data` for cross-chain integrity. Other objects related to NFT, which are not important for integrity can be part of the app specific module.\\n### Types\\nWe propose two main types:\\n* `Class` -- describes NFT class. We can think about it as a smart contract address.\\n* `NFT` -- object representing unique, non fungible asset. Each NFT is associated with a Class.\\n#### Class\\nNFT **Class** is comparable to an ERC-721 smart contract (provides description of a smart contract), under which a collection of NFTs can be created and managed.\\n```protobuf\\nmessage Class {\\nstring id          = 1;\\nstring name        = 2;\\nstring symbol      = 3;\\nstring description = 4;\\nstring uri         = 5;\\nstring uri_hash    = 6;\\ngoogle.protobuf.Any data = 7;\\n}\\n```\\n* `id` is used as the primary index for storing the class; _required_\\n* `name` is a descriptive name of the NFT class; _optional_\\n* `symbol` is the symbol usually shown on exchanges for the NFT class; _optional_\\n* `description` is a detailed description of the NFT class; _optional_\\n* `uri` is a URI for the class metadata stored off chain. It should be a JSON file that contains metadata about the NFT class and NFT data schema ([OpenSea example](https:\/\/docs.opensea.io\/docs\/contract-level-metadata)); _optional_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is app specific metadata of the class; _optional_\\n#### NFT\\nWe define a general model for `NFT` as follows.\\n```protobuf\\nmessage NFT {\\nstring class_id           = 1;\\nstring id                 = 2;\\nstring uri                = 3;\\nstring uri_hash           = 4;\\ngoogle.protobuf.Any data  = 10;\\n}\\n```\\n* `class_id` is the identifier of the NFT class where the NFT belongs; _required_\\n* `id` is an identifier of the NFT, unique within the scope of its class. It is specified by the creator of the NFT and may be expanded to use DID in the future. `class_id` combined with `id` uniquely identifies an NFT and is used as the primary index for storing the NFT; _required_\\n```text\\n{class_id}\/{id} --> NFT (bytes)\\n```\\n* `uri` is a URI for the NFT metadata stored off chain. Should point to a JSON file that contains metadata about this NFT (Ref: [ERC721 standard and OpenSea extension](https:\/\/docs.opensea.io\/docs\/metadata-standards)); _required_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is an app specific data of the NFT. CAN be used by composing modules to specify additional properties of the NFT; _optional_\\nThis ADR doesn't specify values that `data` can take; however, best practices recommend upper-level NFT modules clearly specify their contents.  Although the value of this field doesn't provide the additional context required to manage NFT records, which means that the field can technically be removed from the specification, the field's existence allows basic informational\/UI functionality.\\n### `Keeper` Interface\\n```go\\ntype Keeper interface {\\nNewClass(ctx sdk.Context,class Class)\\nUpdateClass(ctx sdk.Context,class Class)\\nMint(ctx sdk.Context,nft NFT\uff0creceiver sdk.AccAddress)   \/\/ updates totalSupply\\nBatchMint(ctx sdk.Context, tokens []NFT,receiver sdk.AccAddress) error\\nBurn(ctx sdk.Context, classId string, nftId string)    \/\/ updates totalSupply\\nBatchBurn(ctx sdk.Context, classID string, nftIDs []string) error\\nUpdate(ctx sdk.Context, nft NFT)\\nBatchUpdate(ctx sdk.Context, tokens []NFT) error\\nTransfer(ctx sdk.Context, classId string, nftId string, receiver sdk.AccAddress)\\nBatchTransfer(ctx sdk.Context, classID string, nftIDs []string, receiver sdk.AccAddress) error\\nGetClass(ctx sdk.Context, classId string) Class\\nGetClasses(ctx sdk.Context) []Class\\nGetNFT(ctx sdk.Context, classId string, nftId string) NFT\\nGetNFTsOfClassByOwner(ctx sdk.Context, classId string, owner sdk.AccAddress) []NFT\\nGetNFTsOfClass(ctx sdk.Context, classId string) []NFT\\nGetOwner(ctx sdk.Context, classId string, nftId string) sdk.AccAddress\\nGetBalance(ctx sdk.Context, classId string, owner sdk.AccAddress) uint64\\nGetTotalSupply(ctx sdk.Context, classId string) uint64\\n}\\n```\\nOther business logic implementations should be defined in composing modules that import `x\/nft` and use its `Keeper`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\nrpc Send(MsgSend)         returns (MsgSendResponse);\\n}\\nmessage MsgSend {\\nstring class_id = 1;\\nstring id       = 2;\\nstring sender   = 3;\\nstring reveiver = 4;\\n}\\nmessage MsgSendResponse {}\\n```\\n`MsgSend` can be used to transfer the ownership of an NFT to another address.\\nThe implementation outline of the server is as follows:\\n```go\\ntype msgServer struct{\\nk Keeper\\n}\\nfunc (m msgServer) Send(ctx context.Context, msg *types.MsgSend) (*types.MsgSendResponse, error) {\\n\/\/ check current ownership\\nassertEqual(msg.Sender, m.k.GetOwner(msg.ClassId, msg.Id))\\n\/\/ transfer ownership\\nm.k.Transfer(msg.ClassId, msg.Id, msg.Receiver)\\nreturn &types.MsgSendResponse{}, nil\\n}\\n```\\nThe query service methods for the `x\/nft` module are:\\n```protobuf\\nservice Query {\\n\/\/ Balance queries the number of NFTs of a given class owned by the owner, same as balanceOf in ERC721\\nrpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/balance\/{owner}\/{class_id}\";\\n}\\n\/\/ Owner queries the owner of the NFT based on its class and id, same as ownerOf in ERC721\\nrpc Owner(QueryOwnerRequest) returns (QueryOwnerResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/owner\/{class_id}\/{id}\";\\n}\\n\/\/ Supply queries the number of NFTs from the given class, same as totalSupply of ERC721.\\nrpc Supply(QuerySupplyRequest) returns (QuerySupplyResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/supply\/{class_id}\";\\n}\\n\/\/ NFTs queries all NFTs of a given class or owner,choose at least one of the two, similar to tokenByIndex in ERC721Enumerable\\nrpc NFTs(QueryNFTsRequest) returns (QueryNFTsResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\";\\n}\\n\/\/ NFT queries an NFT based on its class and id.\\nrpc NFT(QueryNFTRequest) returns (QueryNFTResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\/{class_id}\/{id}\";\\n}\\n\/\/ Class queries an NFT class based on its id\\nrpc Class(QueryClassRequest) returns (QueryClassResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\/{class_id}\";\\n}\\n\/\/ Classes queries all NFT classes\\nrpc Classes(QueryClassesRequest) returns (QueryClassesResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\";\\n}\\n}\\n\/\/ QueryBalanceRequest is the request type for the Query\/Balance RPC method\\nmessage QueryBalanceRequest {\\nstring class_id = 1;\\nstring owner    = 2;\\n}\\n\/\/ QueryBalanceResponse is the response type for the Query\/Balance RPC method\\nmessage QueryBalanceResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryOwnerRequest is the request type for the Query\/Owner RPC method\\nmessage QueryOwnerRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryOwnerResponse is the response type for the Query\/Owner RPC method\\nmessage QueryOwnerResponse {\\nstring owner = 1;\\n}\\n\/\/ QuerySupplyRequest is the request type for the Query\/Supply RPC method\\nmessage QuerySupplyRequest {\\nstring class_id = 1;\\n}\\n\/\/ QuerySupplyResponse is the response type for the Query\/Supply RPC method\\nmessage QuerySupplyResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryNFTstRequest is the request type for the Query\/NFTs RPC method\\nmessage QueryNFTsRequest {\\nstring                                class_id   = 1;\\nstring                                owner      = 2;\\ncosmos.base.query.v1beta1.PageRequest pagination = 3;\\n}\\n\/\/ QueryNFTsResponse is the response type for the Query\/NFTs RPC methods\\nmessage QueryNFTsResponse {\\nrepeated cosmos.nft.v1beta1.NFT        nfts       = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n\/\/ QueryNFTRequest is the request type for the Query\/NFT RPC method\\nmessage QueryNFTRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryNFTResponse is the response type for the Query\/NFT RPC method\\nmessage QueryNFTResponse {\\ncosmos.nft.v1beta1.NFT nft = 1;\\n}\\n\/\/ QueryClassRequest is the request type for the Query\/Class RPC method\\nmessage QueryClassRequest {\\nstring class_id = 1;\\n}\\n\/\/ QueryClassResponse is the response type for the Query\/Class RPC method\\nmessage QueryClassResponse {\\ncosmos.nft.v1beta1.Class class = 1;\\n}\\n\/\/ QueryClassesRequest is the request type for the Query\/Classes RPC method\\nmessage QueryClassesRequest {\\n\/\/ pagination defines an optional pagination for the request.\\ncosmos.base.query.v1beta1.PageRequest pagination = 1;\\n}\\n\/\/ QueryClassesResponse is the response type for the Query\/Classes RPC method\\nmessage QueryClassesResponse {\\nrepeated cosmos.nft.v1beta1.Class      classes    = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n```\\n### Interoperability\\nInteroperability is all about reusing assets between modules and chains. The former one is achieved by ADR-33: Protobuf client - server communication. At the time of writing ADR-33 is not finalized. The latter is achieved by IBC. Here we will focus on the IBC side.\\nIBC is implemented per module. Here, we aligned that NFTs will be recorded and managed in the x\/nft. This requires creation of a new IBC standard and implementation of it.\\nFor IBC interoperability, NFT custom modules MUST use the NFT object type understood by the IBC client. So, for x\/nft interoperability, custom NFT implementations (example: x\/cryptokitty) should use the canonical x\/nft module and proxy all NFT balance keeping functionality to x\/nft or else re-implement all functionality using the NFT object type understood by the IBC client. In other words: x\/nft becomes the standard NFT registry for all Cosmos NFTs (example: x\/cryptokitty will register a kitty NFT in x\/nft and use x\/nft for book keeping). This was [discussed](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065#discussioncomment-873206) in the context of using x\/bank as a general asset balance book. Not using x\/nft will require implementing another module for IBC.\\n","Predictions":"We create a x\/nft module, which contains the following functionality:n* Store NFTs and track their ownership.n* Expose Keeper interface for composing modules to transfer, mint and burn NFTs.n* Expose external Message interface for users to transfer ownership of their NFTs.n* Query NFTs and their supply information.nThe proposed module is a base module, which is based on the work done by [IRISnet team](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065), and an older implementation in the [Cosmos repository](https:\/\/github.com\/cosmos\/modules\/tree\/master\/incubator\/nft).n"}
{"File Name":"ReportMI-service-manual\/0018-submission-state-machine.md","Context":"## Context\nAs outlined in [ADR-0016][adr-0016], the Data Submission Service will use\n\"submissions\" to describe something that a supplier has submitted to us in order\nto complete a \"task\".\nSubmissions will exist in a state machine that outlines what is happening with\nthem.\nCurrently, we expect there to be 6 states:\n* **pending** - a blank submission that is awaiting data\n* **processing** - a submission where data is being processed\n(eg a file is being ingested or data is being validated)\n* **validation_failed** - the submitted data has failed the validation process\nand needs to be corrected\n* **validation_passed** - the submitted data has passed the validation process,\nand the supplier now needs to review the results\n* **supplier_rejected** - the supplier has reviewed the result of the data\nprocessing and has rejected it (eg they have realised that their data needs to\nbe amended)\n* **supplier_accepted** - the supplier has reviewed the result of the data\nprocessing and are happy that it is accurate\nOther states may added in future to cover approval processes, and fixing\nmistakes in returns.\n","Decision":"The system will model the 6 states highlighted above.\\nSubmissions containing data will proceed through the states from `pending` to\\n`processing` to either `validation_failed` or `validation_passed`. If the\\nvalidation has passed, the supplier can reject the submission (move to\\n`supplier_rejected`) or accept the submission (move to `supplier_accepted`).\\nA 'no business' submission will proceed straight to `supplier_accepted` once\\nthe supplier has confirmed they wish to make this submission.\\nThis is shown in the following diagram.\\n![Submission state machine diagram](..\/diagrams\/0018-submission-states.jpg)\\n","tokens":250,"id":2039,"text":"## Context\\nAs outlined in [ADR-0016][adr-0016], the Data Submission Service will use\\n\"submissions\" to describe something that a supplier has submitted to us in order\\nto complete a \"task\".\\nSubmissions will exist in a state machine that outlines what is happening with\\nthem.\\nCurrently, we expect there to be 6 states:\\n* **pending** - a blank submission that is awaiting data\\n* **processing** - a submission where data is being processed\\n(eg a file is being ingested or data is being validated)\\n* **validation_failed** - the submitted data has failed the validation process\\nand needs to be corrected\\n* **validation_passed** - the submitted data has passed the validation process,\\nand the supplier now needs to review the results\\n* **supplier_rejected** - the supplier has reviewed the result of the data\\nprocessing and has rejected it (eg they have realised that their data needs to\\nbe amended)\\n* **supplier_accepted** - the supplier has reviewed the result of the data\\nprocessing and are happy that it is accurate\\nOther states may added in future to cover approval processes, and fixing\\nmistakes in returns.\\n\n\n##Decision\nThe system will model the 6 states highlighted above.\\nSubmissions containing data will proceed through the states from `pending` to\\n`processing` to either `validation_failed` or `validation_passed`. If the\\nvalidation has passed, the supplier can reject the submission (move to\\n`supplier_rejected`) or accept the submission (move to `supplier_accepted`).\\nA 'no business' submission will proceed straight to `supplier_accepted` once\\nthe supplier has confirmed they wish to make this submission.\\nThis is shown in the following diagram.\\n![Submission state machine diagram](..\/diagrams\/0018-submission-states.jpg)\\n","Predictions":"The data submission service will use the following state machines:n* **pending** - a blank submission that is awaiting datan* **validation_failed** - the submitted data has failed the validation processnand needs to be correctedn* **validation_passed** - the submitted data has passed the validation processnand the supplier now needs to review the resultsn* **supplier_rejected** - the supplier has reviewed the result of the datanprocessing and has rejected it (eg they have realised that their data needs to be amended)n* **supplier_accepted** - the supplier has reviewed the result of the datanprocessing and are happy that it is accuratenOther states may add in future to cover approval processes, and fixingnmistakes in returns.n"}
{"File Name":"operational-data-hub\/0005-build-open-source-solutions.md","Context":"## Context\nOpen source software is software with source code that anyone can inspect, modify, and enhance.\nBy design, open source software licenses promote collaboration and sharing because they permit other people to make modifications to source code and incorporate those changes into their own projects. They encourage computer programmers to access, view, and modify open source software whenever they like, as long as they let others do the same when they share their work.\nOpen source software development furthers several principles, described below.\nTransparency. Whether we're developing software or solving a business problem, we all have access to the information and materials necessary for doing our best work. And when these materials are accessible, we can build upon each other's ideas and discoveries. We can make more effective decisions and understand how decisions affect us.\nCollaboration. When we're free to participate, we can enhance each other's work in unanticipated ways. When we can modify what others have shared, we unlock new possibilities. By initiating new projects together, we can solve problems that no one can solve alone. And when we implement open standards, we enable others to contribute in the future.\nRelease early and often. Rapid prototypes can lead to rapid discoveries. An iterative approach leads to better solutions faster. When you're free to experiment, you can look at problems in new ways and seek answers in new places. You can learn by doing.\nInclusive meritocracy. Good ideas can come from anywhere, and the best ideas should win. Only by including diverse perspectives in our conversations can we be certain we've identified the best ideas, and decision-makers continually seek those perspectives. We may not operate by consensus, but successful work determines which projects gather support and effort from the community.\nCommunity. Communities form when different people unite around a common purpose. Shared values guide decision making, and community goals supersede individual interests and agendas.\n","Decision":"We make our code available as open source.\\n","tokens":382,"id":2759,"text":"## Context\\nOpen source software is software with source code that anyone can inspect, modify, and enhance.\\nBy design, open source software licenses promote collaboration and sharing because they permit other people to make modifications to source code and incorporate those changes into their own projects. They encourage computer programmers to access, view, and modify open source software whenever they like, as long as they let others do the same when they share their work.\\nOpen source software development furthers several principles, described below.\\nTransparency. Whether we're developing software or solving a business problem, we all have access to the information and materials necessary for doing our best work. And when these materials are accessible, we can build upon each other's ideas and discoveries. We can make more effective decisions and understand how decisions affect us.\\nCollaboration. When we're free to participate, we can enhance each other's work in unanticipated ways. When we can modify what others have shared, we unlock new possibilities. By initiating new projects together, we can solve problems that no one can solve alone. And when we implement open standards, we enable others to contribute in the future.\\nRelease early and often. Rapid prototypes can lead to rapid discoveries. An iterative approach leads to better solutions faster. When you're free to experiment, you can look at problems in new ways and seek answers in new places. You can learn by doing.\\nInclusive meritocracy. Good ideas can come from anywhere, and the best ideas should win. Only by including diverse perspectives in our conversations can we be certain we've identified the best ideas, and decision-makers continually seek those perspectives. We may not operate by consensus, but successful work determines which projects gather support and effort from the community.\\nCommunity. Communities form when different people unite around a common purpose. Shared values guide decision making, and community goals supersede individual interests and agendas.\\n\n\n##Decision\nWe make our code available as open source.\\n","Predictions":"We make our code available as open source.n"}
{"File Name":"TANF-app\/008-deployment-flow.md","Context":"## Context\nOur Cloud.gov organization currently has three Spaces -- `tanf-dev`, `tanf-staging`, and `tanf-prod`. The vendor team currently has access to the tanf-dev space only.\nSince the recent changes to our [Git workflow](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/009-git-workflow.md) we believe our current deploy strategy should be updated to more closely match the workflow. Previously, since we had approvals on two different repositories we decided that it made sense to maintain [two separate staging sites](https:\/\/github.com\/HHS\/TANF-app\/blob\/837574415af7c57e182684a75bbcf4d942d3b62a\/docs\/Architecture%20Decision%20Record\/008-deployment-flow.md). We would deploy to one with approval in the raft-tech repository, and another with approval to HHS. Since we now have all approvals made in raft-tech, the deploy after approval serves the same purpose as deploying to the Government staging site would have.\nAdditionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\n","Decision":"Additionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\nDeploy Environment | Cloud.gov Space | Cloud.gov Dev Access | Role                                             | Deploys when ...                                  |\\n-------------------|-----------------|----------------------|--------------------------------------------------|---------------------------------------------------|\\nDev                | Tanf-Dev        | Vendor & Gov      | Deploy code submitted for gov review                | Relevant github label assigned as shown below     |\\nDevelop            | Tanf-Staging    | Vendor & Gov      | Deploy code once gov-approved                       | Code merged to `raft-tech\/TANF-app:develop` |\\nStaging            | Tanf-Staging    | Gov               | Deploy code once gov-approved                       | Code merged to `HHS\/TANF-app:main` |\\nProduction         | Tanf-Prod       | Gov               | Deploy code tested in staging & ready for prod      | Code merged to `HHS\/TANF-app:master`                |\\n### Gitflow and Deployments\\nWe will be following the Gitflow process which is an industry standard. You can read more about it [in our ADR](.\/018-versioning-and-releases.md). I will just highlight the parts relevant for our deployment strategy. Release branches will be merged to `HHS\/TANF-app:master` which will deploy to our production sites. Code merged to `raft-tech\/TANF-app:develop` will be deployed to our staging sites.\\n### Dev deployments\\nWithin the dev space, there is no correlation for branch to environment as these feature or bugfix branches will constantly vary:\\n| Dev Site | Frontend URL | Backend URL | Purpose                                          |\\n| -------- | -------- | -------- |--------------------------------------------------|\\n| A11y | https:\/\/tdp-frontend-a11y.app.cloud.gov | https:\/\/tdp-backend-a11y.app.cloud.gov\/admin\/ | Space for accessibility testing                  |\\n| QASP | https:\/\/tdp-frontend-qasp.app.cloud.gov | https:\/\/tdp-backend-qasp.app.cloud.gov\/admin\/ | Space for QASP review                            |\\n| raft | https:\/\/tdp-frontend-raft.app.cloud.gov | https:\/\/tdp-backend-raft.app.cloud.gov\/admin\/ | Space for Raft review                          |\\n","tokens":396,"id":3789,"text":"## Context\\nOur Cloud.gov organization currently has three Spaces -- `tanf-dev`, `tanf-staging`, and `tanf-prod`. The vendor team currently has access to the tanf-dev space only.\\nSince the recent changes to our [Git workflow](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/009-git-workflow.md) we believe our current deploy strategy should be updated to more closely match the workflow. Previously, since we had approvals on two different repositories we decided that it made sense to maintain [two separate staging sites](https:\/\/github.com\/HHS\/TANF-app\/blob\/837574415af7c57e182684a75bbcf4d942d3b62a\/docs\/Architecture%20Decision%20Record\/008-deployment-flow.md). We would deploy to one with approval in the raft-tech repository, and another with approval to HHS. Since we now have all approvals made in raft-tech, the deploy after approval serves the same purpose as deploying to the Government staging site would have.\\nAdditionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\n\n\n##Decision\nAdditionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\nDeploy Environment | Cloud.gov Space | Cloud.gov Dev Access | Role                                             | Deploys when ...                                  |\\n-------------------|-----------------|----------------------|--------------------------------------------------|---------------------------------------------------|\\nDev                | Tanf-Dev        | Vendor & Gov      | Deploy code submitted for gov review                | Relevant github label assigned as shown below     |\\nDevelop            | Tanf-Staging    | Vendor & Gov      | Deploy code once gov-approved                       | Code merged to `raft-tech\/TANF-app:develop` |\\nStaging            | Tanf-Staging    | Gov               | Deploy code once gov-approved                       | Code merged to `HHS\/TANF-app:main` |\\nProduction         | Tanf-Prod       | Gov               | Deploy code tested in staging & ready for prod      | Code merged to `HHS\/TANF-app:master`                |\\n### Gitflow and Deployments\\nWe will be following the Gitflow process which is an industry standard. You can read more about it [in our ADR](.\/018-versioning-and-releases.md). I will just highlight the parts relevant for our deployment strategy. Release branches will be merged to `HHS\/TANF-app:master` which will deploy to our production sites. Code merged to `raft-tech\/TANF-app:develop` will be deployed to our staging sites.\\n### Dev deployments\\nWithin the dev space, there is no correlation for branch to environment as these feature or bugfix branches will constantly vary:\\n| Dev Site | Frontend URL | Backend URL | Purpose                                          |\\n| -------- | -------- | -------- |--------------------------------------------------|\\n| A11y | https:\/\/tdp-frontend-a11y.app.cloud.gov | https:\/\/tdp-backend-a11y.app.cloud.gov\/admin\/ | Space for accessibility testing                  |\\n| QASP | https:\/\/tdp-frontend-qasp.app.cloud.gov | https:\/\/tdp-backend-qasp.app.cloud.gov\/admin\/ | Space for QASP review                            |\\n| raft | https:\/\/tdp-frontend-raft.app.cloud.gov | https:\/\/tdp-backend-raft.app.cloud.gov\/admin\/ | Space for Raft review                          |\\n","Predictions":"We will use the [Security Gateway](https:\/\/github.com\/Security Gateway) to deploy to the tanf-prod space.n"}
{"File Name":"re-build-systems\/0002-use-containers-and-multiple-boxes.md","Context":"## Context\nThe aim of this project is to allow teams to quickly create their own Jenkins\nplatform, running on AWS.\nJenkins has a master\/agent architecture, where a single master node can trigger\njobs running on multiple agents. This decision concerns how we deploy, configure\nand manage both the master and the agents.\n","Decision":"We will make both the master and the agents Docker containers, running on\\nseparate managed EC2 instances. We will provision and manage these directly for\\nnow, without using an orchestration service like ECS or Kubernetes.\\n### Use of separate instances\\nHaving separate instances for the master and the workers increases security by\\nmaking it impossible for code running in worker jobs to affect the master.\\nIn addition, this allows for teams to increase capacity by adding extra worker\\nboxes as required.\\n### Use of Docker for master\\nRunning the master in Docker makes it easy to deploy and upgrade. This improves\\nteams' abilities to quickly respond when new Jenkins versions are released in\\nresponse to security vulnerabilities, for example.\\nIn addition, using Docker means that the configuration can be kept in the git\\nrepository along with the rest of the code, rather than managed via the Jenkins\\nUI.\\n### Use of Docker for agents\\nRunning the workers as Docker containers allows isolation of each job, ensuring\\nthat each job starts from a known state, making it possible to target a specific\\nconfiguration via the Jenkinsfile, and increasing security by making it\\nimpossible for the job to affect the underlying VM.\\n### Not using orchestration frameworks\\nAlthough some teams at GDS are experimenting with ECS, the Jenkins service is\\nsimple enough that it is not worth introducing the added complexity for this\\nproject.\\n","tokens":68,"id":319,"text":"## Context\\nThe aim of this project is to allow teams to quickly create their own Jenkins\\nplatform, running on AWS.\\nJenkins has a master\/agent architecture, where a single master node can trigger\\njobs running on multiple agents. This decision concerns how we deploy, configure\\nand manage both the master and the agents.\\n\n\n##Decision\nWe will make both the master and the agents Docker containers, running on\\nseparate managed EC2 instances. We will provision and manage these directly for\\nnow, without using an orchestration service like ECS or Kubernetes.\\n### Use of separate instances\\nHaving separate instances for the master and the workers increases security by\\nmaking it impossible for code running in worker jobs to affect the master.\\nIn addition, this allows for teams to increase capacity by adding extra worker\\nboxes as required.\\n### Use of Docker for master\\nRunning the master in Docker makes it easy to deploy and upgrade. This improves\\nteams' abilities to quickly respond when new Jenkins versions are released in\\nresponse to security vulnerabilities, for example.\\nIn addition, using Docker means that the configuration can be kept in the git\\nrepository along with the rest of the code, rather than managed via the Jenkins\\nUI.\\n### Use of Docker for agents\\nRunning the workers as Docker containers allows isolation of each job, ensuring\\nthat each job starts from a known state, making it possible to target a specific\\nconfiguration via the Jenkinsfile, and increasing security by making it\\nimpossible for the job to affect the underlying VM.\\n### Not using orchestration frameworks\\nAlthough some teams at GDS are experimenting with ECS, the Jenkins service is\\nsimple enough that it is not worth introducing the added complexity for this\\nproject.\\n","Predictions":"We will make both the master and the agents Docker containers, running on separate managed EC2 instances. We will provision and manage these directly fornnow, without using an orchestration service like ECS or Kubernetes.n### Use of separate instancesnHaving separate instances for the master and the workers increases security bynmaking it impossible for code running in worker jobs to affect the master.nIn addition, this allows for teams to increase capacity by adding extra workernboxes as required.n### Use of Docker for agentsnRunning the master in Docker makes it easy to deploy and upgrade. This improvesnteams' abilities to quickly respond when new Jenkins versions are released innresponse to security vulnerabilities, for example.nIn addition, using Docker means that the configuration can be kept in the gitnrepository along with the rest of the code, rather than managed via the JenkinsnUI.n### Not using orchestration frameworksnAlthough some teams at GDS are experimenting with ECS, the Jenkins service isnsimple enough that it is not worth introducing the added complexity for thisnproject.n"}
{"File Name":"jfluentvalidation\/0001-primitive-array-constraints.md","Context":"## Context and Problem Statement\nMy first pass at building out array constraint was to use a generic parameter `A` with `java.lang.reflect.Array` to obtain\nthe length of the property representing `A`.\nI was curious what the cost of using `java.lang.reflect.Array` compared to grabbing the `length` property from a known type was.\nAnything with a name like `reflect*` gives me nightmares about terrible performance.\nI decided to write a JMH benchmark to determine the performance impact `java.lang.reflect.Array` to assist in determining\nwhich implementation to use.\n## Decision Drivers\n1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\n","Decision":"1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\nI decided to choose option 1 as I prioritized performance above the overhead to maintain additional classes and having\\nduplicate logic.\\nWhile it might be premature optimization and such a small impact (1 - 2 ns) the benchmark results below still convinced me.\\nI'm sure someone can convince me that the overhead is insignificant or that I simply messed up the bencharmark at which point\\nit should be easier refactor to option 2.\\nI've included a rough [implementation of option 2](#option-2-implementation) just in case.\\n```java\\npackage jfluentvalidation.constraints.array;\\nimport jfluentvalidation.constraints.array.length.ArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraintAlternative;\\nimport jfluentvalidation.rules.PropertyRule;\\nimport jfluentvalidation.validators.RuleContext;\\nimport jfluentvalidation.validators.ValidationContext;\\nimport org.openjdk.jmh.annotations.*;\\nimport org.openjdk.jmh.runner.Runner;\\nimport org.openjdk.jmh.runner.options.OptionsBuilder;\\nimport java.util.concurrent.TimeUnit;\\n@BenchmarkMode(Mode.AverageTime)\\n@OutputTimeUnit(TimeUnit.NANOSECONDS)\\n@State(Scope.Benchmark)\\npublic class LengthBenchmark {\\npublic static class Foo {\\nprivate boolean[] bar;\\npublic Foo(boolean[] bar) {\\nthis.bar = bar;\\n}\\n}\\nRuleContext<Foo, boolean[]> ruleContext;\\nBooleanArrayExactLengthConstraintAlternative booleanArrayExactLengthConstraintAlternative;\\nArrayExactLengthConstraint arrayExactLengthConstraint;\\nBooleanArrayExactLengthConstraint booleanArrayExactLengthConstraint;\\n@Setup\\npublic void prepare() {\\nFoo f = new Foo(new boolean[5]);\\nPropertyRule propertyRule = new PropertyRule(foo -> f.bar, \"bar\");\\nruleContext = new RuleContext<>(new ValidationContext(f), propertyRule);\\nbooleanArrayExactLengthConstraintAlternative = new BooleanArrayExactLengthConstraintAlternative(5);\\narrayExactLengthConstraint = new ArrayExactLengthConstraint(5);\\n}\\n@Benchmark\\npublic void booleanArrayExactLengthConstraintAlternative() {\\nbooleanArrayExactLengthConstraintAlternative.isValid(ruleContext);\\n}\\n@Benchmark\\npublic void arrayExactLengthConstraint() {\\narrayExactLengthConstraint.isValid(ruleContext);\\n}\\npublic static void main(String[] args) throws Exception {\\nnew Runner(new OptionsBuilder()\\n.include(LengthBenchmark.class.getSimpleName())\\n.forks(1)\\n.warmupIterations(2)\\n.measurementIterations(5)\\n.build())\\n.run();\\n}\\n}\\n```\\nRun 1\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.504 | \u00b1 0.143 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.099 | \u00b1 0.022 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 2\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.436 | \u00b1 0.049 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.041 | \u00b1 0.013 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 3\\n| Benchmark                                                     | Mode  | Cnt | Score     | Error       | Units  |\\n|---------------------------------------------------------------|-------|-----|-----------|-------------|--------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | thrpt | 25  | 0.424     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | thrpt | 25  | 0.626     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt  | 25  | 2.362     | \u00b1 0.022     | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt  | 25  | 1.599     | \u00b1 0.007     | ns\/op  |\\n| LengthBenchmark.arrayExactLengthConstraint                    | ss    | 5   | 48261.200 | \u00b1 4493.186  | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | ss    | 5   | 18690.000 | \u00b1 23530.434 | ns\/op  |\\nA ~20% performance impact on throughput and even larger on average time is a bit too much for me to ignore.\\n### Positive Consequences\\n- Gain roughly a 20% performance instead in average time and throughput.\\n### Negative Consequences\\n- We need 36 classes (8 primitive types + 1 object type and 4 constraint classes for each type) compared to 4 constraint classes\\n(ExactLength, BetweenLength, MinimumLength, MaximumLength) to implement array length constraints for primitive arrays.\\n- Duplicate logic in constraint classes across types.\\n","tokens":164,"id":2591,"text":"## Context and Problem Statement\\nMy first pass at building out array constraint was to use a generic parameter `A` with `java.lang.reflect.Array` to obtain\\nthe length of the property representing `A`.\\nI was curious what the cost of using `java.lang.reflect.Array` compared to grabbing the `length` property from a known type was.\\nAnything with a name like `reflect*` gives me nightmares about terrible performance.\\nI decided to write a JMH benchmark to determine the performance impact `java.lang.reflect.Array` to assist in determining\\nwhich implementation to use.\\n## Decision Drivers\\n1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\n\n\n##Decision\n1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\nI decided to choose option 1 as I prioritized performance above the overhead to maintain additional classes and having\\nduplicate logic.\\nWhile it might be premature optimization and such a small impact (1 - 2 ns) the benchmark results below still convinced me.\\nI'm sure someone can convince me that the overhead is insignificant or that I simply messed up the bencharmark at which point\\nit should be easier refactor to option 2.\\nI've included a rough [implementation of option 2](#option-2-implementation) just in case.\\n```java\\npackage jfluentvalidation.constraints.array;\\nimport jfluentvalidation.constraints.array.length.ArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraintAlternative;\\nimport jfluentvalidation.rules.PropertyRule;\\nimport jfluentvalidation.validators.RuleContext;\\nimport jfluentvalidation.validators.ValidationContext;\\nimport org.openjdk.jmh.annotations.*;\\nimport org.openjdk.jmh.runner.Runner;\\nimport org.openjdk.jmh.runner.options.OptionsBuilder;\\nimport java.util.concurrent.TimeUnit;\\n@BenchmarkMode(Mode.AverageTime)\\n@OutputTimeUnit(TimeUnit.NANOSECONDS)\\n@State(Scope.Benchmark)\\npublic class LengthBenchmark {\\npublic static class Foo {\\nprivate boolean[] bar;\\npublic Foo(boolean[] bar) {\\nthis.bar = bar;\\n}\\n}\\nRuleContext<Foo, boolean[]> ruleContext;\\nBooleanArrayExactLengthConstraintAlternative booleanArrayExactLengthConstraintAlternative;\\nArrayExactLengthConstraint arrayExactLengthConstraint;\\nBooleanArrayExactLengthConstraint booleanArrayExactLengthConstraint;\\n@Setup\\npublic void prepare() {\\nFoo f = new Foo(new boolean[5]);\\nPropertyRule propertyRule = new PropertyRule(foo -> f.bar, \"bar\");\\nruleContext = new RuleContext<>(new ValidationContext(f), propertyRule);\\nbooleanArrayExactLengthConstraintAlternative = new BooleanArrayExactLengthConstraintAlternative(5);\\narrayExactLengthConstraint = new ArrayExactLengthConstraint(5);\\n}\\n@Benchmark\\npublic void booleanArrayExactLengthConstraintAlternative() {\\nbooleanArrayExactLengthConstraintAlternative.isValid(ruleContext);\\n}\\n@Benchmark\\npublic void arrayExactLengthConstraint() {\\narrayExactLengthConstraint.isValid(ruleContext);\\n}\\npublic static void main(String[] args) throws Exception {\\nnew Runner(new OptionsBuilder()\\n.include(LengthBenchmark.class.getSimpleName())\\n.forks(1)\\n.warmupIterations(2)\\n.measurementIterations(5)\\n.build())\\n.run();\\n}\\n}\\n```\\nRun 1\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.504 | \u00b1 0.143 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.099 | \u00b1 0.022 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 2\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.436 | \u00b1 0.049 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.041 | \u00b1 0.013 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 3\\n| Benchmark                                                     | Mode  | Cnt | Score     | Error       | Units  |\\n|---------------------------------------------------------------|-------|-----|-----------|-------------|--------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | thrpt | 25  | 0.424     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | thrpt | 25  | 0.626     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt  | 25  | 2.362     | \u00b1 0.022     | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt  | 25  | 1.599     | \u00b1 0.007     | ns\/op  |\\n| LengthBenchmark.arrayExactLengthConstraint                    | ss    | 5   | 48261.200 | \u00b1 4493.186  | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | ss    | 5   | 18690.000 | \u00b1 23530.434 | ns\/op  |\\nA ~20% performance impact on throughput and even larger on average time is a bit too much for me to ignore.\\n### Positive Consequences\\n- Gain roughly a 20% performance instead in average time and throughput.\\n### Negative Consequences\\n- We need 36 classes (8 primitive types + 1 object type and 4 constraint classes for each type) compared to 4 constraint classes\\n(ExactLength, BetweenLength, MinimumLength, MaximumLength) to implement array length constraints for primitive arrays.\\n- Duplicate logic in constraint classes across types.\\n","Predictions":"1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.nI decided to choose option 1 as I prioritized performance above the overhead to maintain additional classes and havingnduplicate logic.nWhile it might be premature optimization and such a small impact (1 - 2 ns) the benchmark results below still convinced me.nI'm sure someone can convince me that the overhead is insignificant or that I simply messed up the bencharmark at which pointnit should be easier refactor to option 2.nI've included a rough [implementation of option 2](#option-2-implementation) just in case.njavanpackage jfluentvalidation.constraints.array;nimport jfluentvalidation.constraints.array.length.ArrayExactLengthConstraint;nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraint;nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraintAlternative;nI've included a rough [implementation of option 2](#option-2-implementation) just in case.njavanpackage jfluentvalidation.constraints.array;nimport jfluentvalidation.constraints.array.length.ArrayExactLengthConstraint;nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraintAlternative;nI've included a rough [implementation of option 2](#option-2-implementation) just in case.njavanpackage jfluentvalidation.constraints.array;nimport jfluentvalidation.constraints.array.length.ArrayExactLengthConstraint;nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraintAlternative;n"}
{"File Name":"simple-server\/001-synchronization.md","Context":"## Context\nNetwork connectivity on phones in rural areas can be low, and\npatchy. So, the app needs to work offline, as much as possible. The\nsync mechanism exists to allow sharing records of patients, blood\npressures, etc across devices.\nWe need to accommodate cases where patients, and nurses move across\nfacilities multiple times during a week.\n","Decision":"### Mechanism\\n1. Send records from device to server\\nThe device needs to keep track of records that need to be\\nsynced. These can be new records, or records that have one or more\\nfields updated. These records need to be formatted into the payload\\nschemas as defined in the individual APIs below. The API will not\\ndifferentiate between new and updated records; this is handled by the\\nserver.  These payloads then need to be sent in batches, where each\\nbatch is inherently independent of the previous one. However, it is\\nimportant to _send_ all records that need syncing, before fetching\\nrecords from the server.\\n2. Fetch records from server to device\\nWhen fetching records for the first time, the `first_time` query\\nparameter should be set. The server will then send back a number of\\nrecords as defined by the `number_of_records` query parameter. This is\\nessentially the \\\"batch_size\\\". The first response also includes a\\n`latest_record_timestamp`, that needs to be sent with the next\\nrequest, in order to retrieve more records from the server. If there\\nare no more records to be sent from the server, it returns an empty\\nlist.\\n3. Merge records on device\\nAfter receiving records from the server, the device will match the\\nrecords in the local database using the ID field of the entity, and\\nupdate them to the server's version. If the local record in the\\ndatabase is pending a sync, it will not update it. The merging of\\nrecords will be handled only by the server.\\nThe server may send back the same record multiple times, so updating\\nrecords on the device needs to be idempotent.\\n### Levers\\nPhysical resource constraints such as battery life, or network are not\\nunder our control. In order for the app to adapt well to the\\nconstraints, we need to have knobs on:\\n1. Batch size: how many records to send or fetch\\n2. Sync frequency: how often sending or fetching happens\\nPreferably, these levers are modifiable at run-time, per device.\\n","tokens":77,"id":1712,"text":"## Context\\nNetwork connectivity on phones in rural areas can be low, and\\npatchy. So, the app needs to work offline, as much as possible. The\\nsync mechanism exists to allow sharing records of patients, blood\\npressures, etc across devices.\\nWe need to accommodate cases where patients, and nurses move across\\nfacilities multiple times during a week.\\n\n\n##Decision\n### Mechanism\\n1. Send records from device to server\\nThe device needs to keep track of records that need to be\\nsynced. These can be new records, or records that have one or more\\nfields updated. These records need to be formatted into the payload\\nschemas as defined in the individual APIs below. The API will not\\ndifferentiate between new and updated records; this is handled by the\\nserver.  These payloads then need to be sent in batches, where each\\nbatch is inherently independent of the previous one. However, it is\\nimportant to _send_ all records that need syncing, before fetching\\nrecords from the server.\\n2. Fetch records from server to device\\nWhen fetching records for the first time, the `first_time` query\\nparameter should be set. The server will then send back a number of\\nrecords as defined by the `number_of_records` query parameter. This is\\nessentially the \\\"batch_size\\\". The first response also includes a\\n`latest_record_timestamp`, that needs to be sent with the next\\nrequest, in order to retrieve more records from the server. If there\\nare no more records to be sent from the server, it returns an empty\\nlist.\\n3. Merge records on device\\nAfter receiving records from the server, the device will match the\\nrecords in the local database using the ID field of the entity, and\\nupdate them to the server's version. If the local record in the\\ndatabase is pending a sync, it will not update it. The merging of\\nrecords will be handled only by the server.\\nThe server may send back the same record multiple times, so updating\\nrecords on the device needs to be idempotent.\\n### Levers\\nPhysical resource constraints such as battery life, or network are not\\nunder our control. In order for the app to adapt well to the\\nconstraints, we need to have knobs on:\\n1. Batch size: how many records to send or fetch\\n2. Sync frequency: how often sending or fetching happens\\nPreferably, these levers are modifiable at run-time, per device.\\n","Predictions":"We will use the Sync method to sync the data between the devices.n"}
{"File Name":"adr-demo\/0003-use-cassandra-as-storage-backend.md","Context":"## Context\nAs the application offers uploads capabilities to our users, we need to be able to store uploaded images to some\nkind of storage. The storage backend should be able to do the following things:\n* Store raw images\/gifs\n* Store thumbnails of images\/gifs\n* Offer a basic search capability (search by title, search by tag)\n","Decision":"After a quick comparative study, we decide to go with a Cassandra storage backend. The reasons are the following:\\n* Cassandra is quick and easy to setup\\n* It should answer our needs without much custom configuration, even when scaling up\\n* Cassandra allows us to do simple search operations on the fields we're interested in\\n* The team is already familiar with Cassandra usage\\nHere is a quick picture of our architecture including Cassandra as the storage backend:\\n![](microservices-architecture-with-cassandra.png)\\n**Note**: Many services seem to point to the same Cassandra, however we need to setup multiple tables so we can respect\\nCQRS principles (command and query separation):\\n* One table will be used to store thumbnails\\n* One table will be used to store raw images\\n* Thumbnail and upload services are write services (but to different tables)\\n* Search service and download service are read-only services\\n### Other databases\\nIn our quest to the holy storage backend we also studied MongoDb but it didn't seem to be a good fit for our use case : its Json-like structure does not fill particularly well with what we are trying to achieve, and is not much appropriate for very large storage volumes.\\n","tokens":73,"id":2515,"text":"## Context\\nAs the application offers uploads capabilities to our users, we need to be able to store uploaded images to some\\nkind of storage. The storage backend should be able to do the following things:\\n* Store raw images\/gifs\\n* Store thumbnails of images\/gifs\\n* Offer a basic search capability (search by title, search by tag)\\n\n\n##Decision\nAfter a quick comparative study, we decide to go with a Cassandra storage backend. The reasons are the following:\\n* Cassandra is quick and easy to setup\\n* It should answer our needs without much custom configuration, even when scaling up\\n* Cassandra allows us to do simple search operations on the fields we're interested in\\n* The team is already familiar with Cassandra usage\\nHere is a quick picture of our architecture including Cassandra as the storage backend:\\n![](microservices-architecture-with-cassandra.png)\\n**Note**: Many services seem to point to the same Cassandra, however we need to setup multiple tables so we can respect\\nCQRS principles (command and query separation):\\n* One table will be used to store thumbnails\\n* One table will be used to store raw images\\n* Thumbnail and upload services are write services (but to different tables)\\n* Search service and download service are read-only services\\n### Other databases\\nIn our quest to the holy storage backend we also studied MongoDb but it didn't seem to be a good fit for our use case : its Json-like structure does not fill particularly well with what we are trying to achieve, and is not much appropriate for very large storage volumes.\\n","Predictions":"We will use a Security database to store the data.n"}
{"File Name":"exercise3-group-6\/0003-create-two-observation-comparators.md","Context":"## Context\nWhale observations have a variety of qualities and may require sorting for research purposes. Two\ndistinct sorting methods proposed for sorting these observations will require two unique comparison methods.\n","Decision":"Implement `Comparable<Observation>`, create default field `compareTo` method for `ObservationId` (long) and nested\\ncomparator class for field `sightingTime` (Date) in Observation.\\n","tokens":39,"id":2775,"text":"## Context\\nWhale observations have a variety of qualities and may require sorting for research purposes. Two\\ndistinct sorting methods proposed for sorting these observations will require two unique comparison methods.\\n\n\n##Decision\nImplement `Comparable<Observation>`, create default field `compareTo` method for `ObservationId` (long) and nested\\ncomparator class for field `sightingTime` (Date) in Observation.\\n","Predictions":"We will use a Data database to store the data and provide anreference to the buyer.n"}
{"File Name":"nr-arch\/2020-03-12-ARCH-record-architecture-decisions-simple-template.md","Context":"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4804,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","Predictions":"The change that we're proposing or have agreed to implement.n"}
{"File Name":"racing-tips\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3119,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"architecture\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1436,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"hmpps-interventions-ui\/0010-use-stylelint-for-linting-styles.md","Context":"## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [Stylelint](https:\/\/stylelint.io\/) is one of the more popular CSS\nlinters with support for SASS, and is easily configurable for our purposes.\n","Decision":"We will check SASS syntax using Stylelint.\\nWe will use the recommended configuration for plugins where possible.\\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\\n","tokens":75,"id":458,"text":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https:\/\/stylelint.io\/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n\n\n##Decision\nWe will check SASS syntax using Stylelint.\\nWe will use the recommended configuration for plugins where possible.\\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\\n","Predictions":"We will use serialize to serialize models.nWe will use serialize to serialize models.n"}
{"File Name":"log4brains\/20210113-distribute-log4brains-as-a-global-npm-package.md","Context":"## Context and Problem Statement\nLog4brains (`v1.0.0-beta.4`) installation procedure is currently optimized for JS projects and looks like this:\n- Run `npx init-log4brains`\n- Which installs locally `@log4brains\/cli` and `@log4brains\/web`\n- And creates custom entries in `package.json`'s `scripts` section:\n- `\"log4brains-preview\": \"log4brains-web preview\"`\n- `\"log4brains-build\": \"log4brains-web build\"`\n- `\"adr\": \"log4brains adr\"`\nFor non-JS projects, you have to install manually the packages and the `npx init-log4brains` script does not work.\nSince Log4brains is intended for all projects, not especially JS ones, we have to make the installation procedure simpler and language-agnostic.\n## Decision Drivers <!-- optional -->\n- Simplicity of the installation procedure\n- Language agnostic\n- Initialization script works on any kind of project\n- Faster \"getting started\"\n","Decision":"- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\nThe new installation procedure is now language agnostic and will be the following:\\n```bash\\nnpm install -g log4brains\\nlog4brains init\\n```\\nLog4brains will be distributed as a global NPM package named `log4brains`, which provides a global `log4brains` command.\\n- This global package will require the existing `@log4brains\/cli` and `@log4brains\/web` packages\\n- `init-log4brains` will be renamed to `@log4brains\/init` and required as a dependency\\n### Consequences\\nFor a JS project, it is now impossible to pin Log4brains to a specific version.\\nWe may implement a [xojs\/xo](https:\/\/github.com\/xojs\/xo)-like behavior later: \"the CLI will use your local install of XO when available, even when run globally.\"\\n","tokens":224,"id":1065,"text":"## Context and Problem Statement\\nLog4brains (`v1.0.0-beta.4`) installation procedure is currently optimized for JS projects and looks like this:\\n- Run `npx init-log4brains`\\n- Which installs locally `@log4brains\/cli` and `@log4brains\/web`\\n- And creates custom entries in `package.json`'s `scripts` section:\\n- `\"log4brains-preview\": \"log4brains-web preview\"`\\n- `\"log4brains-build\": \"log4brains-web build\"`\\n- `\"adr\": \"log4brains adr\"`\\nFor non-JS projects, you have to install manually the packages and the `npx init-log4brains` script does not work.\\nSince Log4brains is intended for all projects, not especially JS ones, we have to make the installation procedure simpler and language-agnostic.\\n## Decision Drivers <!-- optional -->\\n- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\n\n\n##Decision\n- Simplicity of the installation procedure\\n- Language agnostic\\n- Initialization script works on any kind of project\\n- Faster \"getting started\"\\nThe new installation procedure is now language agnostic and will be the following:\\n```bash\\nnpm install -g log4brains\\nlog4brains init\\n```\\nLog4brains will be distributed as a global NPM package named `log4brains`, which provides a global `log4brains` command.\\n- This global package will require the existing `@log4brains\/cli` and `@log4brains\/web` packages\\n- `init-log4brains` will be renamed to `@log4brains\/init` and required as a dependency\\n### Consequences\\nFor a JS project, it is now impossible to pin Log4brains to a specific version.\\nWe may implement a [xojs\/xo](https:\/\/github.com\/xojs\/xo)-like behavior later: \"the CLI will use your local install of XO when available, even when run globally.\"\\n","Predictions":"- Simplicity of the installation proceduren- Language agnosticn- Initialization script works on any kind of projectn- Faster \"getting started\"nThe new installation procedure is now language agnostic and will be the following:nbashnnpm install -g log4brainsnlog4brains initnnLog4brains will be distributed as a global NPM package named log4brains, which provides a global log4brains commandn- This global package will require the existing @log4brains\/cli and @log4brains\/web packagesn- init-log4brains will be renamed to @log4brains.n"}
{"File Name":"markmap\/splitting-and-bundling.md","Context":"## Context\ncoc-markmap \/ markmap-cli has an incredibly large size in `node_modules`. (~30MB)\n### Redundant dependencies\nIn markmap-lib@0.9.x transforming and rendering share the same package, but they have quite different dependencies and are used in quite different scenarios. In other words many of the dependencies are useless most of the time.\nFor example, transforming usually occurs locally when creating a markmap, while rendering happens in the browser when the markmap is opened. So if we are trying to create a markmap, it's likely that we don't need d3 and its friends. If we are exploring a markmap it's likely that we don't need all those transforming tools like remarkable and katex.\n### Extraneous bundles\nSome packages build multiple bundles to support different types of import. For example, `index.js` for `cjs`, and `index.min.js` for minimized `cjs` bundle, and several more. As a result, each of the bundle contains a copy of code, leading to a huge size.\nAfter bundling into a single package, we get a single copy of the code, and optionally minimized.\n","Decision":"- Split rendering code from `markmap-lib` into a new package `markmap-view`.\\nBy splitting the packages we can manage dependencies more independently, and get rid of unused ones.\\nIn most cases, `markmap-view` can be used as a prebuilt bundle, even without installation by leveraging CDNs.\\n- Use rollup to bundle code in `markmap-cli`.\\nUnder the hood, `coc-markmap` depends on `markmap-cli`, and `markmap-cli` depends on `markmap-lib`.\\n`markmap-lib` is supposed to be used in applications with bundlers, so it's better to keep simple and externalize the dependencies.\\n`markmap-cli` is a standalone command-line tool. It should be installed without dependency bloat. So it is a good idea to bundle everything in it and get rid of additional dependency packages.\\n`coc-markmap` is a thin layer on top of `markmap-cli` so we can just leave it as is.\\n","tokens":245,"id":5140,"text":"## Context\\ncoc-markmap \/ markmap-cli has an incredibly large size in `node_modules`. (~30MB)\\n### Redundant dependencies\\nIn markmap-lib@0.9.x transforming and rendering share the same package, but they have quite different dependencies and are used in quite different scenarios. In other words many of the dependencies are useless most of the time.\\nFor example, transforming usually occurs locally when creating a markmap, while rendering happens in the browser when the markmap is opened. So if we are trying to create a markmap, it's likely that we don't need d3 and its friends. If we are exploring a markmap it's likely that we don't need all those transforming tools like remarkable and katex.\\n### Extraneous bundles\\nSome packages build multiple bundles to support different types of import. For example, `index.js` for `cjs`, and `index.min.js` for minimized `cjs` bundle, and several more. As a result, each of the bundle contains a copy of code, leading to a huge size.\\nAfter bundling into a single package, we get a single copy of the code, and optionally minimized.\\n\n\n##Decision\n- Split rendering code from `markmap-lib` into a new package `markmap-view`.\\nBy splitting the packages we can manage dependencies more independently, and get rid of unused ones.\\nIn most cases, `markmap-view` can be used as a prebuilt bundle, even without installation by leveraging CDNs.\\n- Use rollup to bundle code in `markmap-cli`.\\nUnder the hood, `coc-markmap` depends on `markmap-cli`, and `markmap-cli` depends on `markmap-lib`.\\n`markmap-lib` is supposed to be used in applications with bundlers, so it's better to keep simple and externalize the dependencies.\\n`markmap-cli` is a standalone command-line tool. It should be installed without dependency bloat. So it is a good idea to bundle everything in it and get rid of additional dependency packages.\\n`coc-markmap` is a thin layer on top of `markmap-cli` so we can just leave it as is.\\n","Predictions":"- Split rendering code from markmap-lib into a new package markmap-view.nBy splitting the packages we can manage dependencies more independently, and get rid of unused ones.nIn most cases, markmap-view can be used as a prebuilt bundle, even without installation by leveraging CDNs.n- Use rollup to bundle code in markmap-cli.nUnder the hood, coc-markmap depends on markmap-cli, and markmap-cli depends on markmap-lib.nmarkmap-lib is supposed to be used in applications with bundlers, so it's better to keep simple and externalize the dependencies.nmarkmap-cli is a standalone command-line tool. It should be used in applications with bundlers.nmarkmap-cli is a standalone command-line tool. It should be used in applications with bundlers.n"}
{"File Name":"james\/0025-cassandra-blob-store-cache.md","Context":"## Context\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\nAs an example:\n- Mailbox message headers are expected to be small and frequently accessed\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\n","Decision":"Implement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","tokens":160,"id":2145,"text":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n\n\n##Decision\nImplement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","Predictions":"Implement a write through cache to have better read latency for smaller objects.nSuch a cache needs to be distributed in order to be more efficient.nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,nand be queried with a consistency level of ONE.nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if thenblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.nFailure to read the cache, or cache miss will result in a read in the object storage.n"}
{"File Name":"terraform\/Automation.md","Context":"## Context\nServices that we are in the process of migrating or that have recently competed migration are likely to go through a period of needing more maintenance that a mature service.\nOne answer to this is to handle all such problems on a case by case basis, fixing as we go in order to try to keep the velocity as high as possible. At the other end of the scale is to drop a significant cost on the project by automating all such processes.\n","Decision":"We will automate every process possible. Where an automated process may be applicable to more than one server, then we will attempt to write the solution to be DRY.\\n","tokens":93,"id":929,"text":"## Context\\nServices that we are in the process of migrating or that have recently competed migration are likely to go through a period of needing more maintenance that a mature service.\\nOne answer to this is to handle all such problems on a case by case basis, fixing as we go in order to try to keep the velocity as high as possible. At the other end of the scale is to drop a significant cost on the project by automating all such processes.\\n\n\n##Decision\nWe will automate every process possible. Where an automated process may be applicable to more than one server, then we will attempt to write the solution to be DRY.\\n","Predictions":"We will use a ADR format for all architectural decisions.n"}
{"File Name":"operational-data-hub\/0027-a-gcp-project-belongs-to-a-single-domain.md","Context":"## Context\nThe projects structure of the platform can be used to protect components. By the seperation into projects a modular, loosely coupled design is created. A project belongs to a single [domain](0025-domains-correspond-to-business-departments.md), a domain can consist of multiple projects. The project implements a coherent set of functions within a single domain.\n","Decision":"The set of functions implemented in one GCP project belongs to a single domain.\\n","tokens":75,"id":2756,"text":"## Context\\nThe projects structure of the platform can be used to protect components. By the seperation into projects a modular, loosely coupled design is created. A project belongs to a single [domain](0025-domains-correspond-to-business-departments.md), a domain can consist of multiple projects. The project implements a coherent set of functions within a single domain.\\n\n\n##Decision\nThe set of functions implemented in one GCP project belongs to a single domain.\\n","Predictions":"1. Teams are offered 'namespaces'. A namespace is the concept of an isolated environment for workloads\/resources.n2. A CP namespace is implemented as a Kubernetes namespace and AWS resources (e.g. RBAC instance, S3 bucket).n3. Isolation in Kubernetes namespaces is implemented using RBAC and NetworkPolicy:n* RBAC - teams can only administer k8s resources in their own namespacesn* NetworkPolicy - containers can only receive traffic from its ingresses and other containers in the same namespace (implemented with a NetworkPolicy, which teams can edit if needed)n4. Isolation between AWS resources is achieved using access control.nEach ECR repo, or S3 bucket, RDS bucket is made accessible to teams.n"}
{"File Name":"libelektra\/boolean.md","Context":"## Problem\nInconsistent use of booleans in various parts of Elektra.\n","Decision":"Only the strings `0` and `1` are allowed in the `KeySet` for `type = boolean`, for both values and defaults.\\nEverything else should lead to errors in checkers (in `kdbSet`).\\nA spec with `type = boolean` without a specified default should be interpreted as `default = 0`.\\nExample for an implementation in C in an application:\\n```c\\nif (k != NULL && strcmp(keyString(k), \"1\") == 0) {\/*true*\/} else {\/*false*\/}\\n```\\nStorage plugins are allowed any representation as suitable, e.g., a JSON plugin might render `1` as `true`.\\nThe type checker plugin should allow\\n- non-presence\\n- the string \"0\"\\n- the string \"1\"\\n","tokens":18,"id":1293,"text":"## Problem\\nInconsistent use of booleans in various parts of Elektra.\\n\n\n##Decision\nOnly the strings `0` and `1` are allowed in the `KeySet` for `type = boolean`, for both values and defaults.\\nEverything else should lead to errors in checkers (in `kdbSet`).\\nA spec with `type = boolean` without a specified default should be interpreted as `default = 0`.\\nExample for an implementation in C in an application:\\n```c\\nif (k != NULL && strcmp(keyString(k), \"1\") == 0) {\/*true*\/} else {\/*false*\/}\\n```\\nStorage plugins are allowed any representation as suitable, e.g., a JSON plugin might render `1` as `true`.\\nThe type checker plugin should allow\\n- non-presence\\n- the string \"0\"\\n- the string \"1\"\\n","Predictions":"* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build file that contains the build code.n* build is a build"}
{"File Name":"ftd-scratch3-offline\/0004-use-multiple-projects.md","Context":"## Context\nThe software could be developed in one big (Gradle) project.\nThis would make integration easier.\nAt the same time this would make re-use of the code outside of this project harder.\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\n","Decision":"We will try to modularize the software and will use multiple projects to achieve this goal.\\n","tokens":67,"id":2626,"text":"## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n\n\n##Decision\nWe will try to modularize the software and will use multiple projects to achieve this goal.\\n","Predictions":"We will use a Separate approach to the Separate folder.n"}
{"File Name":"phpadr\/0005-phpunit-as-testing-framework.md","Context":"## Context\nEnsure good code quality with ease for change, integration and error correction.\n","Decision":"It will be used the [PHPUnit](https:\/\/phpunit.de\/) as testing framework.\\n","tokens":18,"id":5070,"text":"## Context\\nEnsure good code quality with ease for change, integration and error correction.\\n\n\n##Decision\nIt will be used the [PHPUnit](https:\/\/phpunit.de\/) as testing framework.\\n","Predictions":"We acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.n"}
{"File Name":"FindMeFoodTrucks\/Choice of Compute for Ingestion.md","Context":"## :dart: Context\nAzure offers a number of ways to host your application code. The following are the considerations for choosing a compute option for the ingestion sub-system:\n* Should support scheduling\n* Should support running in background\n* Should be able to connect to backend Cosmos DB\n* Should support .net core framework\n* Service will only run once a day\n* Would prefer a managed service\n* Does not have portability requirements\nThe following options for compute are considered for this service:\n* App Services\n* Functions\n* Container Instances\n* Service Fabric\n* AKS\nChoosing the right compute model will help optimize the development experience and operations\n","Decision":"The recommended approach is to use Azure Functions considering the following points:\\n* Supports consumption plan (Pay-per-use) which is ideal for sparse usage\\n* Supports .net core framework and is well integrated with Visual Studio development experience\\n* Can leverage cosmos SDK (nuget) to connect back to the data store\\n* Has built-in scheduling capabilities\\n* Is a fully managed PaaS service\\n* No overheads related to cluster management or infrastructure provisioning.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/technology-choices\/compute-decision-tree\\n","tokens":135,"id":1095,"text":"## :dart: Context\\nAzure offers a number of ways to host your application code. The following are the considerations for choosing a compute option for the ingestion sub-system:\\n* Should support scheduling\\n* Should support running in background\\n* Should be able to connect to backend Cosmos DB\\n* Should support .net core framework\\n* Service will only run once a day\\n* Would prefer a managed service\\n* Does not have portability requirements\\nThe following options for compute are considered for this service:\\n* App Services\\n* Functions\\n* Container Instances\\n* Service Fabric\\n* AKS\\nChoosing the right compute model will help optimize the development experience and operations\\n\n\n##Decision\nThe recommended approach is to use Azure Functions considering the following points:\\n* Supports consumption plan (Pay-per-use) which is ideal for sparse usage\\n* Supports .net core framework and is well integrated with Visual Studio development experience\\n* Can leverage cosmos SDK (nuget) to connect back to the data store\\n* Has built-in scheduling capabilities\\n* Is a fully managed PaaS service\\n* No overheads related to cluster management or infrastructure provisioning.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/technology-choices\/compute-decision-tree\\n","Predictions":"The recommended approach is to use Azure Functions considering the following points:n* Supports consumption plan (Pay-per-use) which is ideal for sparse usagen* Supports.net core framework and is well integrated with Visual Studio development experiencen* Can leverage cosmos SDK (nuget) to connect back to the data storen* Has built-in scheduling capabilitiesn* Is a fully managed PaaS servicen* No overheads related to cluster management or infrastructure provisioningnThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/technology-choices\/compute-decision-treen"}
{"File Name":"jabref\/0015-support-an-abstract-query-syntax-for-query-conversion.md","Context":"## Context and Problem Statement\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\nWhich query syntax should be used for the abstract queries?\nWhich features should the syntax support?\n","Decision":"Chosen option: \"Use a syntax that is derived of the lucene query syntax\", because only option that is already known, and easy to implement.\\nFurthermore parsers for lucene already exist and are tested.\\nFor simplicity, and lack of universal capabilities across fetchers, only basic query features and therefor syntax is supported:\\n* All terms in the query are whitespace separated and will be ANDed\\n* Default and certain fielded terms are supported\\n* Fielded Terms:\\n* `author`\\n* `title`\\n* `journal`\\n* `year` (for single year)\\n* `year-range` (for range e.g. `year-range:2012-2015`)\\n* The `journal`, `year`, and `year-range` fields should only be populated once in each query\\n* The `year` and `year-range` fields are mutually exclusive\\n* Example:\\n* `author:\"Igor Steinmacher\" author:\"Christoph Treude\" year:2017` will be converted to\\n* `author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" AND year:2017`\\nThe supported syntax can be expressed in EBNF as follows:\\nQuery := {Clause} \\\\nClause:= \\[Field\\] Term \\\\nField := author: | title: | journal: | year: | year-range: | default:\\\\nTerm  := Word | Phrase \\\\nWord can be derived to any series of non-whitespace characters.\\nPhrases are multiple words wrapped in quotes and may contain white-space characters within the quotes.\\\\nNote: Even though this EBNF syntactically allows the creation of queries with year and year-range fields,\\nsuch a query does not make sense semantically and therefore will not be executed.\\n### Positive Consequences\\n* Already tested\\n* Well known\\n* Easy to implement\\n* Can use an existing parser\\n","tokens":84,"id":4738,"text":"## Context and Problem Statement\\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\\nWhich query syntax should be used for the abstract queries?\\nWhich features should the syntax support?\\n\n\n##Decision\nChosen option: \"Use a syntax that is derived of the lucene query syntax\", because only option that is already known, and easy to implement.\\nFurthermore parsers for lucene already exist and are tested.\\nFor simplicity, and lack of universal capabilities across fetchers, only basic query features and therefor syntax is supported:\\n* All terms in the query are whitespace separated and will be ANDed\\n* Default and certain fielded terms are supported\\n* Fielded Terms:\\n* `author`\\n* `title`\\n* `journal`\\n* `year` (for single year)\\n* `year-range` (for range e.g. `year-range:2012-2015`)\\n* The `journal`, `year`, and `year-range` fields should only be populated once in each query\\n* The `year` and `year-range` fields are mutually exclusive\\n* Example:\\n* `author:\"Igor Steinmacher\" author:\"Christoph Treude\" year:2017` will be converted to\\n* `author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" AND year:2017`\\nThe supported syntax can be expressed in EBNF as follows:\\nQuery := {Clause} \\\\nClause:= \\[Field\\] Term \\\\nField := author: | title: | journal: | year: | year-range: | default:\\\\nTerm  := Word | Phrase \\\\nWord can be derived to any series of non-whitespace characters.\\nPhrases are multiple words wrapped in quotes and may contain white-space characters within the quotes.\\\\nNote: Even though this EBNF syntactically allows the creation of queries with year and year-range fields,\\nsuch a query does not make sense semantically and therefore will not be executed.\\n### Positive Consequences\\n* Already tested\\n* Well known\\n* Easy to implement\\n* Can use an existing parser\\n","Predictions":"Chosen option: \"Use a syntax that is derived of the lucene query syntax\", because only option that is already known, and easy to implement.nFurthermore parsers for lucene already exist and are tested.nFor simplicity, and lack of universal capabilities across fetchers, only basic query features and therefor syntax is supported:n* All terms in the query are whitespace separated and will be ANDedn* Default and certain fielded terms are supportedn* Fielded Terms:n* authorn* titlen* journaln* year (for single year)n* year-range (for range e.g. year-range:2012-2015)n* The journal, year, and year-range fields should only be populated once in each queryn* The year and year-range fields are mutually exclusiven* Example:n* author:\"Igor Steinmacher\" author:\"Christoph Treude\" AND author:\"Igor Steinmacher\" AND year:2017nThe supported syntax can be expressed in EBNF as follows:nQuery := Clause nClause:= [Field] Term nField := author: | title: | journal: | year: | year-range: | default:nTerm := Word | Phrase nWord can be derived to any series of non-whitespace characters.nPhrases are multiple words.nWord can be derived to any series of non-whitespace characters.nPhrases are multiple words.n"}
{"File Name":"dos-server\/adr-11-exception-monitoring.md","Context":"## Context\nAn application is needed for real time production error discovery and reporting. Sentry is currently being\nused by DLS for various applications.\n","Decision":"We will use Sentry for application exception monitoring.\\n","tokens":30,"id":3370,"text":"## Context\\nAn application is needed for real time production error discovery and reporting. Sentry is currently being\\nused by DLS for various applications.\\n\n\n##Decision\nWe will use Sentry for application exception monitoring.\\n","Predictions":"We will use Sentry for application exception monitoring.n"}
{"File Name":"docspell\/0010_convert_image_files.md","Context":"# Context and Problem Statement\nHow to convert image files properly to pdf?\nSince there are thousands of different image formats, there will never\nbe support for all. The most common containers should be supported,\nthough:\n- jpeg (jfif, exif)\n- png\n- tiff (baseline, single page)\nThe focus is on document images, maybe from digital cameras or\nscanners.\n# Considered Options\n* [pdfbox](https:\/\/pdfbox.apache.org\/) library\n* [imagemagick](https:\/\/www.imagemagick.org\/) external command\n* [img2pdf](https:\/\/github.com\/josch\/img2pdf) external command\n* [tesseract](https:\/\/github.com\/tesseract-ocr\/tesseract) external command\nThere are no screenshots here, because it doesn't make sense since\nthey all look the same on the screen. Instead we look at the files\nproperties.\n**Input File**\nThe input files are:\n```\n$ identify input\/*\ninput\/jfif.jpg JPEG 2480x3514 2480x3514+0+0 8-bit sRGB 240229B 0.000u 0:00.000\ninput\/letter-en.jpg JPEG 1695x2378 1695x2378+0+0 8-bit Gray 256c 467341B 0.000u 0:00.000\ninput\/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000\ninput\/letter-en.tiff TIFF 1695x2378 1695x2378+0+0 8-bit Grayscale Gray 4030880B 0.000u 0:00.000\n```\nSize:\n- jfif.jpg 240k\n- letter-en.jpg 467k\n- letter-en.png 191k\n- letter-en.tiff 4.0M\n","Decision":"Tesseract.\\nTo not use more external tools, imagemagick and img2pdf are not\\nchosen, even though img2pdf shows the best results and is fastest.\\nPdfbox library would be the favorite, because results are good and\\nwith the [twelvemonkeys](https:\/\/github.com\/haraldk\/TwelveMonkeys)\\nlibrary there is support for many images. The priority is to avoid\\nmore external commands if possible.\\nBut since there already is a dependency to tesseract and it can create\\nsearchable pdfs, the decision is to use tesseract for this. Then PDFs\\nwith images can be converted to searchable PDFs with images. And text\\nextraction is required anyways.\\n","tokens":435,"id":273,"text":"# Context and Problem Statement\\nHow to convert image files properly to pdf?\\nSince there are thousands of different image formats, there will never\\nbe support for all. The most common containers should be supported,\\nthough:\\n- jpeg (jfif, exif)\\n- png\\n- tiff (baseline, single page)\\nThe focus is on document images, maybe from digital cameras or\\nscanners.\\n# Considered Options\\n* [pdfbox](https:\/\/pdfbox.apache.org\/) library\\n* [imagemagick](https:\/\/www.imagemagick.org\/) external command\\n* [img2pdf](https:\/\/github.com\/josch\/img2pdf) external command\\n* [tesseract](https:\/\/github.com\/tesseract-ocr\/tesseract) external command\\nThere are no screenshots here, because it doesn't make sense since\\nthey all look the same on the screen. Instead we look at the files\\nproperties.\\n**Input File**\\nThe input files are:\\n```\\n$ identify input\/*\\ninput\/jfif.jpg JPEG 2480x3514 2480x3514+0+0 8-bit sRGB 240229B 0.000u 0:00.000\\ninput\/letter-en.jpg JPEG 1695x2378 1695x2378+0+0 8-bit Gray 256c 467341B 0.000u 0:00.000\\ninput\/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000\\ninput\/letter-en.tiff TIFF 1695x2378 1695x2378+0+0 8-bit Grayscale Gray 4030880B 0.000u 0:00.000\\n```\\nSize:\\n- jfif.jpg 240k\\n- letter-en.jpg 467k\\n- letter-en.png 191k\\n- letter-en.tiff 4.0M\\n\n\n##Decision\nTesseract.\\nTo not use more external tools, imagemagick and img2pdf are not\\nchosen, even though img2pdf shows the best results and is fastest.\\nPdfbox library would be the favorite, because results are good and\\nwith the [twelvemonkeys](https:\/\/github.com\/haraldk\/TwelveMonkeys)\\nlibrary there is support for many images. The priority is to avoid\\nmore external commands if possible.\\nBut since there already is a dependency to tesseract and it can create\\nsearchable pdfs, the decision is to use tesseract for this. Then PDFs\\nwith images can be converted to searchable PDFs with images. And text\\nextraction is required anyways.\\n","Predictions":"* [pdfbox](https:\/\/pdfbox.apache.org\/) libraryn* [imagemagick](https:\/\/www.imagemagick.org\/) external commandn* [img2pdf](https:\/\/github.com\/josch\/img2pdf) external commandnThere are no screenshots here, becausenthey all look the same on the screen. Instead we look at the filesnproperties.n**Input File**nThe input files are:n$ identify input\/*ninput\/jfif.jpg JPEG 2480x3514 2480x3514+0+0 8-bit sRGB 240229B 0.000u 0:00.000ninput\/letter-en.jpg JPEG 1695x2378 1695x2378+0+0 8-bit Gray 256c 467341B 0.000u 0:00.000ninput\/letter-en.png PNG 1695x2378 1695x2378+0+0 8-bit Gray 256c 191571B 0.000u 0:00.000ninput\/letter-en.tiff TIFF 1695x2378 1695x2378+0+0 8-bit Grayscale Gray 4030880B 0.000u 0:00.000nSize:n- jfif.jpg 240kn- letter-en.jpg 467kn- letter-en.png 191kn- letter-en.png 191kn- letter-en.jpg"}
{"File Name":"html-diff-service\/0002-implement-using-spring-boot.md","Context":"## Context\nThe HTML Diff Service is born out of need to increase performance of HTML\ndiffing by not requiring a command line execution in Contently's web\napplication. Additionally, we wanted to remove the Java dependency from our\nRails application container to simplify and reduce the threat surface.\n","Decision":"Spring Boot is used to allow for the service to be standalone with support for\\nembedded servers. The decision allows portability to deploy the service without\\na J2EE server.\\n","tokens":60,"id":3249,"text":"## Context\\nThe HTML Diff Service is born out of need to increase performance of HTML\\ndiffing by not requiring a command line execution in Contently's web\\napplication. Additionally, we wanted to remove the Java dependency from our\\nRails application container to simplify and reduce the threat surface.\\n\n\n##Decision\nSpring Boot is used to allow for the service to be standalone with support for\\nembedded servers. The decision allows portability to deploy the service without\\na J2EE server.\\n","Predictions":"We will use swift-stack to store the data.n"}
{"File Name":"modernisation-platform\/0010-terraform-module-strategy.md","Context":"## Context\nThe Modernisation Platform uses [Terraform](https:\/\/www.terraform.io\/) for its infrastructure as code. To make infrastructure reusable, or to simply tidy up code you can use [Terraform Modules](https:\/\/www.terraform.io\/docs\/language\/modules\/). There are different use cases in the platform for using modules, and this ADR outlines how we plan to use them.\n","Decision":"Modules used only by the Modernisation Platform core infrastructure will remain in the [terraform\/modules](https:\/\/github.com\/ministryofjustice\/modernisation-platform\/tree\/main\/terraform\/modules) folder where they are currently located. These modules are mainly single use modules but created to keep the code tidier and easier to maintain. Modules used only by the core which currently have their own repository will remain where they are.\\nModules used by users will have their own repository per module which we link to from the main repo. These modules will be versioned with GitHub releases, and tested with [Terratest](https:\/\/terratest.gruntwork.io\/) against a test AWS account.\\n","tokens":82,"id":3144,"text":"## Context\\nThe Modernisation Platform uses [Terraform](https:\/\/www.terraform.io\/) for its infrastructure as code. To make infrastructure reusable, or to simply tidy up code you can use [Terraform Modules](https:\/\/www.terraform.io\/docs\/language\/modules\/). There are different use cases in the platform for using modules, and this ADR outlines how we plan to use them.\\n\n\n##Decision\nModules used only by the Modernisation Platform core infrastructure will remain in the [terraform\/modules](https:\/\/github.com\/ministryofjustice\/modernisation-platform\/tree\/main\/terraform\/modules) folder where they are currently located. These modules are mainly single use modules but created to keep the code tidier and easier to maintain. Modules used only by the core which currently have their own repository will remain where they are.\\nModules used by users will have their own repository per module which we link to from the main repo. These modules will be versioned with GitHub releases, and tested with [Terratest](https:\/\/terratest.gruntwork.io\/) against a test AWS account.\\n","Predictions":"The Modernisation Platform will use [Semantic Style Sheet](https:\/\/semantic-stylesheet.com\/) to provide a reusable, reusable, and reusable code base.n"}
{"File Name":"openlobby-server\/0006-use-flask.md","Context":"## Context\nWe need to choose webserver.\n","Decision":"We will use Flask. Server should be simple - pretty much just with a GraphQL\\nendpoint and GraphiQL.\\n","tokens":11,"id":469,"text":"## Context\\nWe need to choose webserver.\\n\n\n##Decision\nWe will use Flask. Server should be simple - pretty much just with a GraphQL\\nendpoint and GraphiQL.\\n","Predictions":"We will use Flask. Server should be simple - pretty much just with a GraphQLnendpoint and GraphiQL.n"}
{"File Name":"CCS-Architecture-Decision-Records\/0012-use-govuk-notify-for-bulk-communication-with-users.md","Context":"## Context\nMost public-facing digital services have a need to notify its users about something at a critical business process, or just to manage their expectations to prevent incoming contact.\nThe notifications can, for example, be:\n* Telling a supplier they have a task to complete,\n* Telling a supplier that a deadline is approaching,\n* Telling a supplier that a buyer has been shortlisted for a procurement,\n* Providing a receipt,\n* Providing status updates,\n* Etc...\nGenerally, the notifications will be email, but may also need to be SMS in some cases.\nTo reduce the need for each organisation or service team to build its own infrastructure for sending email and SMS based messages, the Government Digital Service (GDS) has built GOV.UK Notify.\nGOV.UK Notify can send notifications either via an API call, or by uploading a CSV to the website. Sending email is free, and the service provides feedback on what messages have failed to be delivered.\nThe service is already used in various parts of CCS, including MISO, Report MI, DigiTS etc.\nUsing GOV.UK Notify instead of other solutions (eg using the BCC field in an email) also reduces the potential for an accidental data breach.\n","Decision":"We will use GOV.UK Notify to send notifications to users of any newly built CCS digital services.\\nThis will make sure Notify offers the same user experience across all CCS digital services, and it is currently the most cost-effective (having no-cost for emails) bulk messaging 3rd party solution on the market.\\nThe dashboard to create new Notify templates and send bulk emails is very easy, and new Service Teams at CCS can easily be onboarded to use it, with minimal\/no training.\\nThis ADR does not apply to generic Email Newsletter communication, for which other, more market specific tools may be more appropriate.\\n","tokens":261,"id":996,"text":"## Context\\nMost public-facing digital services have a need to notify its users about something at a critical business process, or just to manage their expectations to prevent incoming contact.\\nThe notifications can, for example, be:\\n* Telling a supplier they have a task to complete,\\n* Telling a supplier that a deadline is approaching,\\n* Telling a supplier that a buyer has been shortlisted for a procurement,\\n* Providing a receipt,\\n* Providing status updates,\\n* Etc...\\nGenerally, the notifications will be email, but may also need to be SMS in some cases.\\nTo reduce the need for each organisation or service team to build its own infrastructure for sending email and SMS based messages, the Government Digital Service (GDS) has built GOV.UK Notify.\\nGOV.UK Notify can send notifications either via an API call, or by uploading a CSV to the website. Sending email is free, and the service provides feedback on what messages have failed to be delivered.\\nThe service is already used in various parts of CCS, including MISO, Report MI, DigiTS etc.\\nUsing GOV.UK Notify instead of other solutions (eg using the BCC field in an email) also reduces the potential for an accidental data breach.\\n\n\n##Decision\nWe will use GOV.UK Notify to send notifications to users of any newly built CCS digital services.\\nThis will make sure Notify offers the same user experience across all CCS digital services, and it is currently the most cost-effective (having no-cost for emails) bulk messaging 3rd party solution on the market.\\nThe dashboard to create new Notify templates and send bulk emails is very easy, and new Service Teams at CCS can easily be onboarded to use it, with minimal\/no training.\\nThis ADR does not apply to generic Email Newsletter communication, for which other, more market specific tools may be more appropriate.\\n","Predictions":"We will use GOV.UK Notify to send notifications to users of any newly built CCS digital services.nThis will make sure Notify offers the same user experience across all CCS digital services, and it is currently the most cost-effective (having no-cost for emails) bulk messaging 3rd party solution on the market.nThe dashboard to create new Notify templates and send bulk emails is very easy, and new Service Teams at CCS can easily be onboarded to use it, with minimal\/no training.nThis ADR does not apply to generic Email Newsletter communication, for which other, more market specific tools may be more appropriate.n"}
{"File Name":"FindMeFoodTrucks\/Hosting Model.md","Context":"## :dart: Context\nThe application can be hosted on a private datacenter or on a managed cloud datacenter.\nChoosing the right hosting model will impact the scalability cost and technology choices for the application.\n","Decision":"The application will be hosted on Microsoft Azure Cloud datacenter.\\n","tokens":43,"id":1089,"text":"## :dart: Context\\nThe application can be hosted on a private datacenter or on a managed cloud datacenter.\\nChoosing the right hosting model will impact the scalability cost and technology choices for the application.\\n\n\n##Decision\nThe application will be hosted on Microsoft Azure Cloud datacenter.\\n","Predictions":"The application will be hosted on Azure Cloud datacenter.n"}
{"File Name":"csc-swr-architecture\/007-Integration-Approaches.html.md.erb","Context":"## Context\nThe Children's Social Care Alpha Service is seeking to pull together key data assets\nsuch as Looked After Children and Foster Families. These will be sourced from Local\nAuthority (LA) and Independent Foster Agency (IFA) Care Management Systems (CMS'),\nas well as central Departmental data collections such as SSDA903.\nThe SSDA903 collection and the CMS at LAs highlight the significance of individual\npathways through the Care System. Each individuals pathway can be regarded as a\nseries of events and touchpoints that the individual has had with various other actors\nin the system, such as IFAs and Foster Families.\nThe existing SSDA903 dataset is analogous to an event stream, with the Local\nAuthorities (LAs) and Indedependent Fostering Agencies (IFAs) being event producers.\n","Decision":"* Develop an API Gateway (see [Kong], [NGinx], [AWS API Gateway]) and REST Proxy, available primarily\\nto LAs and IFAs, serving an [Event Architecture] available internally.\\n* Use an Event Backbone (see [Kafka], [AWS Event Bridge]) to store and publish these events internally.\\n* Publish example client code on the Internet to ease LA and IFA adoption.\\n* Events will become the durable and persistent data which can be used to hydrate\\nthe Department's services, such as Diary Log and Matchmaker Assistant,\\n* [Event Sourcing] should be trialled as a solution.\\n","tokens":169,"id":914,"text":"## Context\\nThe Children's Social Care Alpha Service is seeking to pull together key data assets\\nsuch as Looked After Children and Foster Families. These will be sourced from Local\\nAuthority (LA) and Independent Foster Agency (IFA) Care Management Systems (CMS'),\\nas well as central Departmental data collections such as SSDA903.\\nThe SSDA903 collection and the CMS at LAs highlight the significance of individual\\npathways through the Care System. Each individuals pathway can be regarded as a\\nseries of events and touchpoints that the individual has had with various other actors\\nin the system, such as IFAs and Foster Families.\\nThe existing SSDA903 dataset is analogous to an event stream, with the Local\\nAuthorities (LAs) and Indedependent Fostering Agencies (IFAs) being event producers.\\n\n\n##Decision\n* Develop an API Gateway (see [Kong], [NGinx], [AWS API Gateway]) and REST Proxy, available primarily\\nto LAs and IFAs, serving an [Event Architecture] available internally.\\n* Use an Event Backbone (see [Kafka], [AWS Event Bridge]) to store and publish these events internally.\\n* Publish example client code on the Internet to ease LA and IFA adoption.\\n* Events will become the durable and persistent data which can be used to hydrate\\nthe Department's services, such as Diary Log and Matchmaker Assistant,\\n* [Event Sourcing] should be trialled as a solution.\\n","Predictions":"We will use a Statement database to store the data of the stateful systems.n"}
{"File Name":"dlp-lux\/0002-Blacklight-Version.md","Context":"## Context\nIn order to facilitate work on dlp-lux, we need to make a decision about which verion of Blacklight to run.\n","Decision":"We will use Blacklight 7.\\nThis allows us to avoid problems deploying the application, and sets us up for the future when Blacklight 7 is\\nsupported by Hyrax.\\nThis decision accepts that minor styling differences may occur between Lux and Curate.\\n","tokens":31,"id":3215,"text":"## Context\\nIn order to facilitate work on dlp-lux, we need to make a decision about which verion of Blacklight to run.\\n\n\n##Decision\nWe will use Blacklight 7.\\nThis allows us to avoid problems deploying the application, and sets us up for the future when Blacklight 7 is\\nsupported by Hyrax.\\nThis decision accepts that minor styling differences may occur between Lux and Curate.\\n","Predictions":"We will use a SQL database for the backend.n"}
{"File Name":"compliantkubernetes\/0004-plan-for-usage-without-wrapper-scripts.md","Context":"## Context and Problem Statement\nWe frequently write wrapper scripts. They bring the following value:\n1. They bind together several tools and make them work together as a whole, e.g., `sops` and `kubectl`.\n2. They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files.\n3. They enforce best practices, e.g., encrypt secrets consumed or produced by tools.\nUnfortunately, wrapper scripts can also bring disadvantages:\n1. They make usages that are deviating from the \"good way\" difficult.\n2. They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked.\n3. They add overhead when adding new features or supporting new use-cases.\n4. They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the [Law of Leaky Abstractions](https:\/\/www.joelonsoftware.com\/2002\/11\/11\/the-law-of-leaky-abstractions\/).\n## Decision Drivers\n* We want to make operations simple, predictable, resilient to human error and scalable.\n* We want to have some predictability in how an environment is set up.\n* We want to make Compliant Kubernetes flexible and agile.\n","Decision":"* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\nWe have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value.\\nThis decision applies for new wrapper scripts. We will not rework old wrapper scripts.\\n### Positive Consequences\\n* The operations team can encode standard operating procedures and scale ways of working.\\n* Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort.\\n* Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability.\\n### Negative Consequences\\n* There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.\\n","tokens":301,"id":3109,"text":"## Context and Problem Statement\\nWe frequently write wrapper scripts. They bring the following value:\\n1. They bind together several tools and make them work together as a whole, e.g., `sops` and `kubectl`.\\n2. They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files.\\n3. They enforce best practices, e.g., encrypt secrets consumed or produced by tools.\\nUnfortunately, wrapper scripts can also bring disadvantages:\\n1. They make usages that are deviating from the \"good way\" difficult.\\n2. They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked.\\n3. They add overhead when adding new features or supporting new use-cases.\\n4. They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the [Law of Leaky Abstractions](https:\/\/www.joelonsoftware.com\/2002\/11\/11\/the-law-of-leaky-abstractions\/).\\n## Decision Drivers\\n* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\n\n\n##Decision\n* We want to make operations simple, predictable, resilient to human error and scalable.\\n* We want to have some predictability in how an environment is set up.\\n* We want to make Compliant Kubernetes flexible and agile.\\nWe have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value.\\nThis decision applies for new wrapper scripts. We will not rework old wrapper scripts.\\n### Positive Consequences\\n* The operations team can encode standard operating procedures and scale ways of working.\\n* Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort.\\n* Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability.\\n### Negative Consequences\\n* There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.\\n","Predictions":"* We want to make operations simple, predictable, resilient to human error and scalable.n* We want to have some predictability in how an environment is set up.n* We want to make Compliant Kubernetes flexible and agile.nWe have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g. Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. We should also be simple enough so they can be inspected.n"}
{"File Name":"ReportMI-service-manual\/0019-store-data-validation-rules-in-json-files.md","Context":"## Context\nEach framework has a specific description of what data must be submitted in the\nmonthly MI return. This description outlines what fields should be submitted,\nwhat data types are accepted and any validation rules which should be applied.\nAt the moment, these rules are encapsulated in the Excel template which\nsuppliers download, using drop-downs, tool-tips and sometimes macros.\nA separate copy of these rules is configured for each framework in MISO, which\nthe service uses to validate templates have been filed in correctly when\nuploaded.\n### Longer term approach\nIn future, we want a single source for the rules for framework data submissions\nwhich we can use for various tasks including:\n- generating submission templates in different formats (eg Excel, ODS, CSV)\n- validating submitted returns\n- documenting any APIs\nThese rules should ideally be stored in a central place and made available to\nany services which require them via an API. This will allow new services to\nshare the ruleset and description.\nThe high-level principle is that there should be an artefact that describes the\nrules that need to be applied.\nThe artefacts need to be version controlled so they can be changed during the\nlifetime of the framework. Services may need to access and used outdated\nversions of the rules.\n### Interim approach\nFor the initial waves of on-boarding, we expect only a small number of\nframeworks to be submitting monthly returns.\nWhile we develop our approach, and learn more about the varied rules for each\nframework, we will use an interim approach for storing the rules.\nWe will create a JSON file for each framework which outlines the data structure\nrequired and the validation rules for each field. This JSON file will be stored\nin a GitHub repository, and used to generate templates and validate submitted\nfiles.\nEventually, we will develop an API to provide access to this information.\n","Decision":"We will create a JSON file for each framework and store it in a GitHub\\nrepository.\\nThis JSON file will describe each field, it's data type and associated\\nvalidation rules.\\nWe will migrate these files to a longer-term solution once we have a better\\nunderstanding of the required rules for each framework.\\n","tokens":391,"id":2045,"text":"## Context\\nEach framework has a specific description of what data must be submitted in the\\nmonthly MI return. This description outlines what fields should be submitted,\\nwhat data types are accepted and any validation rules which should be applied.\\nAt the moment, these rules are encapsulated in the Excel template which\\nsuppliers download, using drop-downs, tool-tips and sometimes macros.\\nA separate copy of these rules is configured for each framework in MISO, which\\nthe service uses to validate templates have been filed in correctly when\\nuploaded.\\n### Longer term approach\\nIn future, we want a single source for the rules for framework data submissions\\nwhich we can use for various tasks including:\\n- generating submission templates in different formats (eg Excel, ODS, CSV)\\n- validating submitted returns\\n- documenting any APIs\\nThese rules should ideally be stored in a central place and made available to\\nany services which require them via an API. This will allow new services to\\nshare the ruleset and description.\\nThe high-level principle is that there should be an artefact that describes the\\nrules that need to be applied.\\nThe artefacts need to be version controlled so they can be changed during the\\nlifetime of the framework. Services may need to access and used outdated\\nversions of the rules.\\n### Interim approach\\nFor the initial waves of on-boarding, we expect only a small number of\\nframeworks to be submitting monthly returns.\\nWhile we develop our approach, and learn more about the varied rules for each\\nframework, we will use an interim approach for storing the rules.\\nWe will create a JSON file for each framework which outlines the data structure\\nrequired and the validation rules for each field. This JSON file will be stored\\nin a GitHub repository, and used to generate templates and validate submitted\\nfiles.\\nEventually, we will develop an API to provide access to this information.\\n\n\n##Decision\nWe will create a JSON file for each framework and store it in a GitHub\\nrepository.\\nThis JSON file will describe each field, it's data type and associated\\nvalidation rules.\\nWe will migrate these files to a longer-term solution once we have a better\\nunderstanding of the required rules for each framework.\\n","Predictions":"We will use the State method to generate a new State file for each framework.nThe State file will be a single file describing the data structurenrequired and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data structurenrequired and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be a single file describing the data type and the validation rules for each field.nThe State file will be "}
{"File Name":"tul_cob\/0001-use-feature-flags-to-toggle-features.md","Context":"## Context\nThere are features that exist in the codebase we are not yet ready to release in production. We would like to use feature flags to toggle the availability of certain features, which will help prevent development and production branches from drifting.\n","Decision":"We've decided to implement very simple feature flags that can be toggled with environment variables.\\n","tokens":49,"id":3171,"text":"## Context\\nThere are features that exist in the codebase we are not yet ready to release in production. We would like to use feature flags to toggle the availability of certain features, which will help prevent development and production branches from drifting.\\n\n\n##Decision\nWe've decided to implement very simple feature flags that can be toggled with environment variables.\\n","Predictions":"We've decided to implement very simple feature flags that can be toggled with environment variables.n"}
{"File Name":"community\/dr-017-Application-Integration-without-Wormhole-Connector.md","Context":"## Context\nThe Wormhole Connector is a part of the Application Integration. It is used by external systems deployed inside customers' private datacenter to establish a secure connection tunnel to the Kyma platform.\n","Decision":"The decision is to get rid of the Wormhole Connector at this stage of the project. It can be an optional component of the Application Connector but a secure tunnel. For now, the Application Connector should focus on stability and on providing the core business features.\\n","tokens":42,"id":3448,"text":"## Context\\nThe Wormhole Connector is a part of the Application Integration. It is used by external systems deployed inside customers' private datacenter to establish a secure connection tunnel to the Kyma platform.\\n\n\n##Decision\nThe decision is to get rid of the Wormhole Connector at this stage of the project. It can be an optional component of the Application Connector but a secure tunnel. For now, the Application Connector should focus on stability and on providing the core business features.\\n","Predictions":"The decision is to remove the SignIn component from the application.n[Option 2] The SignIn component should be removed from the application.n[Option 3] The SignIn component should be removed from the application.n[Option 4] The SignIn component should be removed from the application.n"}
{"File Name":"infrastructure-adrs\/0009-create-provenance-service.md","Context":"## Context and Problem Statement <!-- required -->\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\n## Decision Drivers <!-- optional -->\n* [driver 1, e.g., a force, facing concern, \u2026]\n* [driver 2, e.g., a force, facing concern, \u2026]\n* ... <!-- numbers of drivers can vary -->\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","tokens":100,"id":794,"text":"## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n","Predictions":"* [driver 1, e.g., a force, facing concern,...]n* [driver 2, e.g., a force, facing concern,...]n*...!-- numbers of drivers can vary -->nTBDn### Positive Consequences!-- optional -->n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required,...]n*...n### Negative Consequences!-- optional -->n* [e.g., compromising quality attribute, follow-up decisions required,...]n*...n"}
{"File Name":"opg-refunds\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":294,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"thing\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":2583,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"rails-template\/0006-use-simplecov-to-monitor-code-test-coverage.md","Context":"## Context\nWe want to keep our test coverage as high as possible without having to run\nmanual checks as these take time and are easy to forget.\n","Decision":"Use Simplecov with RSpec to monitor coverage changes on every test run\\n","tokens":32,"id":4179,"text":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n\n\n##Decision\nUse Simplecov with RSpec to monitor coverage changes on every test run\\n","Predictions":"We will use Simplecov with RSpec to monitor coverage changes on every test runn"}
{"File Name":"sexual-health-service-finder\/0005-calculate-distance-between-origin-and-result-items-within-the-application.md","Context":"## Context\nThe move to Azure search has introduced the need to calculate the distance\nbetween the search point and each result item. Previously, when using\nElasticsearch, the distance was returned within the query response. Azure\nsearch does not have this capability, it is currently a\n[feature request](https:\/\/feedback.azure.com\/forums\/263029-azure-search\/suggestions\/17760211-support-geo-distance-in-select-result).\n","Decision":"The decision is to calculate the distance between the search point and each\\nresult item within the consuming application i.e. the web app. The calculation\\nfor\\n[great-circle distance](https:\/\/en.wikipedia.org\/wiki\/Great-circle_distance)\\nis well known and available in numerous languages.\\n","tokens":90,"id":4274,"text":"## Context\\nThe move to Azure search has introduced the need to calculate the distance\\nbetween the search point and each result item. Previously, when using\\nElasticsearch, the distance was returned within the query response. Azure\\nsearch does not have this capability, it is currently a\\n[feature request](https:\/\/feedback.azure.com\/forums\/263029-azure-search\/suggestions\/17760211-support-geo-distance-in-select-result).\\n\n\n##Decision\nThe decision is to calculate the distance between the search point and each\\nresult item within the consuming application i.e. the web app. The calculation\\nfor\\n[great-circle distance](https:\/\/en.wikipedia.org\/wiki\/Great-circle_distance)\\nis well known and available in numerous languages.\\n","Predictions":"We will use the [Semantic Search](https:\/\/semanticsearch.io\/) to search for the following results:n- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-searchn- semantic-"}
{"File Name":"jskatas.org\/000-use-adrs.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4722,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"lbh-adrs\/Event-Driven-Architecture-Message-Types.md","Context":"## **Context**\nAlongside the decision to adopt an event driven architecture, there is a need to define what an event will look like. There are several options for events:\n- **Thin Events**\nA thin event consists of the minimum amount of data that is required that will allow a subscriber to retrieve everything it needs. This normally consists of an ID with which to make an API call back to the source publisher to gather the data it needs.\nThe benefits of thin events are:\n- The payload is small in size\n- Data is always up to date as it is retrieved at the point of consumption\n- If calls to APIs fail due to unavailability of APIs, the message can be replayed\n- Very little need for event versioning\nThe downsides are:\n- Consumers need to make API calls to gather the data they need\n- **Fat Events**\nA fat event contains all the data necessary for any subscriber to be able to perform its job.\nThe benefits of fat events are:\n- all the data needed for consumer processing is present in the event\n- no need to make any API calls to retrieve data\nThe downsides are:\n- Event payload could grow to be quite big\n- Data present in the payload may no longer be required by any consumer\n- It is difficult to version events easily (and multiple versions of the same event may need to be sent for backwards compatibility)\n**Hybrid Approach**\nIdeally, we should use thin events wherever possible,as this reduces the complexity around sharing events, updating consumers with new versions of events, etc. However, there are some instances where a thin event might not be possible - notably when updating an activity audit log with details of what has changed. Therefore, the best solution would be to have a thin event that contains an optional message body to hold a specific payload.\n","Decision":"**Hybrid Approach**\\nThe easiest solution is to use a hybrid approach, with consumers gathering the data they need using API calls. This has the benefit of reducing the need for event versioning, and event payloads that grow in size over time.\\nThe event payload will be:\\n```\\n{\\n\"id\": \"8e648f3d-9556-4896-8400-211cb1c5451b\",\\n\"eventType\": \"personCreated\",\\n\"sourceDomain\": \"person\",\\n\"sourceSystem\": \"personAPI\",\\n\"version\": \"v1\",\\n\"correlationId\": \"f4d541d0-7c07-4524-8296-2d0d50cb58f4\",\\n\"dateTime\": \"2021-05-17T11:59:57.25Z\",\\n\"user\": {\\n\"id\": \"ac703d87-c100-40ec-90a0-dabf183e7377\",\\n\"name\": \"Joe Bloggs\",\\n\"email\": \"joe.bloggs@hackney.gov.uk\"\\n},\\n\"entityId\": \"45c76564-2e38-48f3-bb31-6bab2fef8623\",\\n\"eventBody\": {\\n\"oldData\": {\\n\"optionalProperty1\": \"Property value\",\\n\"optionalProperty2\": \"Property value\",\\n\"optionalProperty3\": \"Property value\"\\n}\\n}\\n}\\n```\\n","tokens":384,"id":2316,"text":"## **Context**\\nAlongside the decision to adopt an event driven architecture, there is a need to define what an event will look like. There are several options for events:\\n- **Thin Events**\\nA thin event consists of the minimum amount of data that is required that will allow a subscriber to retrieve everything it needs. This normally consists of an ID with which to make an API call back to the source publisher to gather the data it needs.\\nThe benefits of thin events are:\\n- The payload is small in size\\n- Data is always up to date as it is retrieved at the point of consumption\\n- If calls to APIs fail due to unavailability of APIs, the message can be replayed\\n- Very little need for event versioning\\nThe downsides are:\\n- Consumers need to make API calls to gather the data they need\\n- **Fat Events**\\nA fat event contains all the data necessary for any subscriber to be able to perform its job.\\nThe benefits of fat events are:\\n- all the data needed for consumer processing is present in the event\\n- no need to make any API calls to retrieve data\\nThe downsides are:\\n- Event payload could grow to be quite big\\n- Data present in the payload may no longer be required by any consumer\\n- It is difficult to version events easily (and multiple versions of the same event may need to be sent for backwards compatibility)\\n**Hybrid Approach**\\nIdeally, we should use thin events wherever possible,as this reduces the complexity around sharing events, updating consumers with new versions of events, etc. However, there are some instances where a thin event might not be possible - notably when updating an activity audit log with details of what has changed. Therefore, the best solution would be to have a thin event that contains an optional message body to hold a specific payload.\\n\n\n##Decision\n**Hybrid Approach**\\nThe easiest solution is to use a hybrid approach, with consumers gathering the data they need using API calls. This has the benefit of reducing the need for event versioning, and event payloads that grow in size over time.\\nThe event payload will be:\\n```\\n{\\n\"id\": \"8e648f3d-9556-4896-8400-211cb1c5451b\",\\n\"eventType\": \"personCreated\",\\n\"sourceDomain\": \"person\",\\n\"sourceSystem\": \"personAPI\",\\n\"version\": \"v1\",\\n\"correlationId\": \"f4d541d0-7c07-4524-8296-2d0d50cb58f4\",\\n\"dateTime\": \"2021-05-17T11:59:57.25Z\",\\n\"user\": {\\n\"id\": \"ac703d87-c100-40ec-90a0-dabf183e7377\",\\n\"name\": \"Joe Bloggs\",\\n\"email\": \"joe.bloggs@hackney.gov.uk\"\\n},\\n\"entityId\": \"45c76564-2e38-48f3-bb31-6bab2fef8623\",\\n\"eventBody\": {\\n\"oldData\": {\\n\"optionalProperty1\": \"Property value\",\\n\"optionalProperty2\": \"Property value\",\\n\"optionalProperty3\": \"Property value\"\\n}\\n}\\n}\\n```\\n","Predictions":"* The payload is small in sizen* Data is always up to date as it is retrieved at the point of consumptionn* If calls to APIs fail due to unavailability of APIs, the message can be replayedn* Very little need for event versioningnThe downsides are:n* Consumers need to make API calls to gather the data they needn* All the data needed for consumer processing is present in the eventn* No need to make any API calls to retrieve datan* It is difficult to version events easily (and multiple versions of the same event may need to be sent for backwards compatibility)n* **Hybrid Approach**nIdeally, we should use thin events wherever possible,as this reduces the complexity around sharing events, updating consumers with new versions of events, etc. However, there are some instances where a thin event might not be possible - notably when updating an activity audit log with details of what has changed. Therefore, the best solution would be to have a thin event that contains an optional message body to hold a specific payload.n"}
{"File Name":"news\/0006-a-state-requirement.md","Context":"## Context\nJust trying to implement bookmarks and I have come across something interesting.\nWe have a ui element that looks like this:\n```html\n<a href=\"javascript:application.bookmarks.add('gbj1jf')\" class=\"bookmark\">bookmark<\/a>\n```\nWhich means \"on click, please bookmark the news item with id 'gbj1jf'\".\nThe bookmarking use case looks like this:\n```js\nbookmark = new Bookmark('id-a', 'http:\/\/abc', 'src-rnz');\nawait application.bookmarks.add(bookmark);\n```\nIt is expecting more information for bookmarks so that it can be used later without any further lookups.\nWe don't wish to have to look it up, especially since it may no longer exist in our sources -- and we don't want to store every single\narticle just for this purpose.\nWe don't really want to bake all the information into the link, either, that seems wrong for some reason.\nAnd we don't really want to add arguments to `application.bookmarks.add`, that also feels wrong.\n","Decision":"Introduce some idea of state in to `Application` so that it can handle notifications like this.\\nIt seems like simplifying UI elements is a good idea.\\nAnother option is giving this responsibility to the view element. But then how do we let it know?\\n","tokens":230,"id":4323,"text":"## Context\\nJust trying to implement bookmarks and I have come across something interesting.\\nWe have a ui element that looks like this:\\n```html\\n<a href=\"javascript:application.bookmarks.add('gbj1jf')\" class=\"bookmark\">bookmark<\/a>\\n```\\nWhich means \"on click, please bookmark the news item with id 'gbj1jf'\".\\nThe bookmarking use case looks like this:\\n```js\\nbookmark = new Bookmark('id-a', 'http:\/\/abc', 'src-rnz');\\nawait application.bookmarks.add(bookmark);\\n```\\nIt is expecting more information for bookmarks so that it can be used later without any further lookups.\\nWe don't wish to have to look it up, especially since it may no longer exist in our sources -- and we don't want to store every single\\narticle just for this purpose.\\nWe don't really want to bake all the information into the link, either, that seems wrong for some reason.\\nAnd we don't really want to add arguments to `application.bookmarks.add`, that also feels wrong.\\n\n\n##Decision\nIntroduce some idea of state in to `Application` so that it can handle notifications like this.\\nIt seems like simplifying UI elements is a good idea.\\nAnother option is giving this responsibility to the view element. But then how do we let it know?\\n","Predictions":"Introduce some idea of state in to Application so that it can handle notifications like this.nIt seems like simplifying UI elements is a good idea.nAnother option is giving this responsibility to the view element. But then how do we let it know?n"}
{"File Name":"meadow\/0029-npm.md","Context":"## Context\nThe latest upgrade of Yarn has introduced issues that we're finding difficult to overcome.\nSupersedes [11. Yarn](0011-yarn.md)\n","Decision":"Switch back to `npm` instead of `yarn` in all dev, test, and build environments.\\n","tokens":37,"id":3841,"text":"## Context\\nThe latest upgrade of Yarn has introduced issues that we're finding difficult to overcome.\\nSupersedes [11. Yarn](0011-yarn.md)\\n\n\n##Decision\nSwitch back to `npm` instead of `yarn` in all dev, test, and build environments.\\n","Predictions":"We will use a [simpledotorg](https:\/\/github.com\/simpledotorg\/simple-server\/blob\/master\/doc\/arch\/008-simpled-views-for-dashboards.md) to create a new set of dashboard reports. These reports will be updated in real-time as blood pressures are captured, removing the need for recurring batch jobs. We will denormalize as much data as we need to to make reporting queries simple and cheap to execute, to prevent the need for any complicated SQL queries or additional data layer caching.nOnce the current period has completed, we will treat the data in these rollup tables for that period as immutable. This will let us and our users have more confidence in reports from past periods, as they will not change due to patients moving to different facilities or changes in how we calculate things.nWe will focus on the District controlled blood pressure trends report as a first test case for this sort of approach, as it is a key report and one we'd like to gather feedback on as quickly as possible.n"}
{"File Name":"libelektra\/error_semantics.md","Context":"## Problem\nWhile we have a classification of errors and warnings, it remains\nunclear when plugins actually should emit errors and warnings.\n","Decision":"Provide guidelines in the form as tutorials, covering:\\n- prefer errors to warnings\\n- that any not understood metadata (e.g. types), should lead to an error\\n- that wrong specifications, like `kdb meta-set \/tests\/ipaddr\/ipv4 check\/ipaddr ipv8` should be rejected\\n- if the value does not confirm **exactly** to the specified type, an error should be emitted\\n(e.g. only `0` or `1` as boolean)\\n- anything else that is beyond the capabilities of a plugin (not implemented),\\nshould lead to an error\\nViolations against these guidelines can be reported as bug and then either:\\n- the bug gets fixed\\n- the plugin get a worse `infos\/status` but still get shipped with 1.0\\n- the plugin gets removed\\n- [Metadata in Spec Namespace](spec_metadata.md)\\n- [Capabilities](capabilities.md)\\n- [Boolean](boolean.md)\\n","tokens":28,"id":1302,"text":"## Problem\\nWhile we have a classification of errors and warnings, it remains\\nunclear when plugins actually should emit errors and warnings.\\n\n\n##Decision\nProvide guidelines in the form as tutorials, covering:\\n- prefer errors to warnings\\n- that any not understood metadata (e.g. types), should lead to an error\\n- that wrong specifications, like `kdb meta-set \/tests\/ipaddr\/ipv4 check\/ipaddr ipv8` should be rejected\\n- if the value does not confirm **exactly** to the specified type, an error should be emitted\\n(e.g. only `0` or `1` as boolean)\\n- anything else that is beyond the capabilities of a plugin (not implemented),\\nshould lead to an error\\nViolations against these guidelines can be reported as bug and then either:\\n- the bug gets fixed\\n- the plugin get a worse `infos\/status` but still get shipped with 1.0\\n- the plugin gets removed\\n- [Metadata in Spec Namespace](spec_metadata.md)\\n- [Capabilities](capabilities.md)\\n- [Boolean](boolean.md)\\n","Predictions":"We've decided to implement very simple feature flags that can be toggled with environment variables.n"}
{"File Name":"modular-monolith-with-ddd\/0016-create-ioc-container-per-module.md","Context":"## Context\nFor each module, when we process particular Command or Query, we need to resolve a graph of objects. We need to decide how dependencies of objects will be resolved.\n","Decision":"Solution number 2 - IoC Container per module<\/br>\\nIoC Container per module supports the autonomy of the module and louse coupling so this is a more important aspect for us than duplicated code in some places.\\n","tokens":37,"id":899,"text":"## Context\\nFor each module, when we process particular Command or Query, we need to resolve a graph of objects. We need to decide how dependencies of objects will be resolved.\\n\n\n##Decision\nSolution number 2 - IoC Container per module<\/br>\\nIoC Container per module supports the autonomy of the module and louse coupling so this is a more important aspect for us than duplicated code in some places.\\n","Predictions":"We will use a Security approach to manage the Security state.n"}
{"File Name":"govuk-aws\/0002-hosting-platforms.md","Context":"## Context\nWe need to decide upon a platform to host the future GOV.UK infrastructure. Long term, this will be primarily the GOV.UK PaaS but in the interim, we need to converge with that plan and also upgrade and modernise the current infrastructure.\nGDS policy for hosting of GDS internal services is PaaS first and AWS for anything that can not be run on the PaaS.\n","Decision":"We are using Amazon Web Services as our hosting provider of choice. This conforms to the [GDS Tech Forum Hosting Guide](https:\/\/github.com\/alphagov\/gds-tech\/pull\/7).\\nWe will initially be using the `eu-west-1` region, Ireland. This region has 3 availability zones and also contains the GDS PaaS which will allow easier sharing and peering.\\n","tokens":86,"id":4041,"text":"## Context\\nWe need to decide upon a platform to host the future GOV.UK infrastructure. Long term, this will be primarily the GOV.UK PaaS but in the interim, we need to converge with that plan and also upgrade and modernise the current infrastructure.\\nGDS policy for hosting of GDS internal services is PaaS first and AWS for anything that can not be run on the PaaS.\\n\n\n##Decision\nWe are using Amazon Web Services as our hosting provider of choice. This conforms to the [GDS Tech Forum Hosting Guide](https:\/\/github.com\/alphagov\/gds-tech\/pull\/7).\\nWe will initially be using the `eu-west-1` region, Ireland. This region has 3 availability zones and also contains the GDS PaaS which will allow easier sharing and peering.\\n","Predictions":"We are using Amazon Web Services as our hosting provider of choice. This conforms to the [GDS Tech Forum Hosting Guide](https:\/\/github.com\/alphagov\/gds-tech\/pull\/7).nWe will initially be using the eu-west-1 region, Ireland. This region has 3 availability zones and also contains the GDS PaaS which will allow easier sharing and peering.n"}
{"File Name":"libelektra\/cmake_spec.md","Context":"## Problem\nThe compilation variants of plugins blow up the number of plugins.\nAdditionally there is the concept of default storage + resolver that is\nneeded for bootstrapping plugins.\n","Decision":"Rejected: keep default plugins as-is\\n","tokens":37,"id":1279,"text":"## Problem\\nThe compilation variants of plugins blow up the number of plugins.\\nAdditionally there is the concept of default storage + resolver that is\\nneeded for bootstrapping plugins.\\n\n\n##Decision\nRejected: keep default plugins as-is\\n","Predictions":"* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.n* The feature configuration from Feature::getAll() is converted to a SCSS map inside ShopwareStorefrontThemeThemeCompiler::getFeatureConfigScssMap.n* This SCSS map is always added to the SCSS string which gets processed by ShopwareStorefrontThemeThemeCompiler::compileTheme.n* For webpack hot-proxy the var\/config_js_features.json is used instead.n* The SCSS map looks like this: $sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);n* See https:\/\/sass-lang.com\/documentation\/values\/mapsn* A globally available function feature() is used to read inside the SCSS map if a desired feature is active.nExample:nscssnbody n@if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n @else n"}
{"File Name":"hmpps-interventions-service\/0002-represent-validation-errors-at-field-level.md","Context":"## Context\nFor any user interface or client relying on our API, we need to define how we represent what was wrong with\ninvalid client requests.\n","Decision":"We will use field-level error validation.\\nWe will use meaningful codes per field.\\nExample:\\n```json\\n{\\n\"status\": 400,\\n\"error\": \"validation error\",\\n\"message\": \"draft referral update invalid\",\\n\"validationErrors\": [\\n{\\n\"field\": \"serviceUser.crn\",\\n\"error\": \"FIELD_CANNOT_BE_CHANGED\"\\n}\\n]\\n}\\n```\\n","tokens":31,"id":3159,"text":"## Context\\nFor any user interface or client relying on our API, we need to define how we represent what was wrong with\\ninvalid client requests.\\n\n\n##Decision\nWe will use field-level error validation.\\nWe will use meaningful codes per field.\\nExample:\\n```json\\n{\\n\"status\": 400,\\n\"error\": \"validation error\",\\n\"message\": \"draft referral update invalid\",\\n\"validationErrors\": [\\n{\\n\"field\": \"serviceUser.crn\",\\n\"error\": \"FIELD_CANNOT_BE_CHANGED\"\\n}\\n]\\n}\\n```\\n","Predictions":"We will use field-level error validation.nWe will use meaningful codes per field.nExample:njsonnn\"status\": 400,n\"error\": \"validation error\",n\"message\": \"draft referral update invalid\",n\"validationErrors\": [njsonnn\"field\": \"serviceUser.crn\",n\"error\": \"FIELD_CANNOT_BE_CHANGED\"n]nn"}
{"File Name":"unit-e-project\/ADR-0007.md","Context":"## Context\nWe need a way to provide initial supply. At the moment it is not possible to spend the genesis block coinbase because\neven if the block is added to the index, its transactions are not added to the txdb.\n","Decision":"We want to change the code so that we are able to create an initial supply but we MUST only use the coinbase of the\\ngenesis block and MUST NOT resort to further `imports` in subsequents blocks like for example Particl did.\\nAll the coins of the initial supply MUST be minted in the coinbase transaction of the genesis block.\\n","tokens":50,"id":3764,"text":"## Context\\nWe need a way to provide initial supply. At the moment it is not possible to spend the genesis block coinbase because\\neven if the block is added to the index, its transactions are not added to the txdb.\\n\n\n##Decision\nWe want to change the code so that we are able to create an initial supply but we MUST only use the coinbase of the\\ngenesis block and MUST NOT resort to further `imports` in subsequents blocks like for example Particl did.\\nAll the coins of the initial supply MUST be minted in the coinbase transaction of the genesis block.\\n","Predictions":"We want to change the code so that we are able to create an initial supply but we MUST only use the coinbase of thengenesis block and MUST NOT resort to further imports in subsequents blocks like for example Particl did.nAll the coins of the initial supply MUST be minted in the coinbase transaction of the genesis block.n"}
{"File Name":"ehoks-ui\/0002-use-finnish-as-the-domain-language.md","Context":"## Context\nThere are currently mixed conventions of translating domain words. For example mobx-state-tree-model properties are\nin Finnish but react component props in English even though data might be exactly the same.\n","Decision":"We will use Finnish as the domain language (e.g. osaamisenHankkimistapa, koulutuksenJarjestaja) and English when\\nthe word is not directly related to eHOKS domain. Words are refactored gradually so there might exist mixed\\nconventions quite some time.\\nHere is one expample:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisDemonstration?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\nperiodSpecifier?: string\\ndemonstrationEnviromentDescription?: string\\n}\\n```\\nclassName, title, size, description, startDate, endDate are technical or generic words and should be in English.\\nDemonstration and demonstrationEnvironment are domain and datamodel words naytto and nayttoYmparisto and should be in\\nFinnish. PeriodSpecifier is straight from the data model and thus should be ajanjaksonTarkenne. LearningEvent is used for\\nboth osaamisenHankkiminen and osaamisenOsoittaminen and there isn't domain word to describe these both so learningEvent\\nshould stay as is, although probably best option would be to split learningEvent component to OsaamisenHankkiminen and\\nOsaamisenOsoittaminen components.\\nExample fixed:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisNaytto?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\najanjaksonTarkenne?: string\\nnayttoymparistoDescription?: string\\n}\\n```\\n","tokens":41,"id":598,"text":"## Context\\nThere are currently mixed conventions of translating domain words. For example mobx-state-tree-model properties are\\nin Finnish but react component props in English even though data might be exactly the same.\\n\n\n##Decision\nWe will use Finnish as the domain language (e.g. osaamisenHankkimistapa, koulutuksenJarjestaja) and English when\\nthe word is not directly related to eHOKS domain. Words are refactored gradually so there might exist mixed\\nconventions quite some time.\\nHere is one expample:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisDemonstration?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\nperiodSpecifier?: string\\ndemonstrationEnviromentDescription?: string\\n}\\n```\\nclassName, title, size, description, startDate, endDate are technical or generic words and should be in English.\\nDemonstration and demonstrationEnvironment are domain and datamodel words naytto and nayttoYmparisto and should be in\\nFinnish. PeriodSpecifier is straight from the data model and thus should be ajanjaksonTarkenne. LearningEvent is used for\\nboth osaamisenHankkiminen and osaamisenOsoittaminen and there isn't domain word to describe these both so learningEvent\\nshould stay as is, although probably best option would be to split learningEvent component to OsaamisenHankkiminen and\\nOsaamisenOsoittaminen components.\\nExample fixed:\\n```typescript\\ninterface LearningEventProps {\\nclassName?: string\\ntitle?: React.ReactNode\\nisNaytto?: boolean\\nsize?: \"small\" | \"large\"\\ndescription?: string\\nstartDate?: string\\nendDate?: string\\najanjaksonTarkenne?: string\\nnayttoymparistoDescription?: string\\n}\\n```\\n","Predictions":"We will use Finnish as the domain language (e.g. osaamisenHankkimistapa, koulutuksenJarjestaja) and English whennthe word is not directly related to eHOKS domain. Words are refactored gradually so there might exist mixednconventions quite some time.nHere is one expaample:ntypescriptninterface LearningEventProps nclassName?: stringntitle?: React.ReactNodenisDemonstration?: booleannsize?: \"small\" | \"large\"ndescription?: stringnstartDate?: stringnendDate?: stringnperiodSpecifier?: stringndemonstrationEnvironmentDescription?: stringnnclassName, title, size, description, startDate, endDate are technical or generic words and should be in English.nDemonstration and demonstrationEnvironment are domain and datamodel words naytto and nayttoYmparisto and should be in Finnish. PeriodSpecifier is straight from the data model and thus should be ajanjaksonTarkenne. LearningEvent is used fornboth osaamisenHankkiminen and osaamisenOsoittaminen and there isn't domain word to describe these both so learningEventnshould stay as is, although probably best option would be to split learningEvent component to OsaamisenHankkiminen andnOsaamisenOsoittaminen components.nExample:ntypescriptninterface LearningEventProps nclassName?: stringntitle?: React.ReactNodenisNayttonisNaytto"}
{"File Name":"ng-kaart\/0005-links-in-tabel-tonen.md","Context":"## Context\nURL's worden gestyled zodat deze clickable zijn in het Identify paneel. We willen graag dat ze ook bruikbaar zijn in de tabel.\nUit de lagen.json heb ik afgeleid dat sommige van deze links veldtype URL hebben en andere veldtype String.\nWe willen deze URL's graag op dynamische wijze tonen in de tabel (niet gewoon de URL zelf, maar ook niet telkens gewoon LINK).\nEen groot deel kan getoond worden op basis van de waarde in een ander veld bv dossiernummer bij AV.\nVoor andere lagen mag er een vaste string getoond worden.\nEn voor sommige lagen een combinatie van 2 velden als label.\n","Decision":"Momenteel detecteren we links automatisch door naar de start van de string of naar veldtype te kijken. Als het met http of https begint of\\nveldtype 'url' heeft beelden we het af als een link.\\nWe houden voor de tabel vanaf nu rekening met 'html' veld. Indien ingevuld, dan gebruiken we de inhoud van dat html veld om de weergave af te beelden.\\nWe staan toe dat er tokens instaan om bepaalde velden dynamisch\\nin te vullen. Bvb:\\n{ \"isBasisVeld\": true, \"label\": \"Rapport\", \"naam\": \"rapport_url_1\", \"veldType\": \"url\", \"html\": \"{{periode_1_begin}} - {{periode_1_einde}}\" },\\nIndien 'html' leeg is, nemen we de waarde in 'label'.\\nMet het veld \"constante\" moet ook rekening gehouden worden, dat moet soms genomen worden ipv de waarde van het veld. Bvb:\\n{\\n\"isBasisVeld\": true,\\n\"label\": \"Open in Werf\",\\n\"naam\": \"externeurl\",\\n\"veldType\": \"string\",\\n\"html\": \"{werfid}\",\\n\"constante\": \"https:\/\/{domain.name}\/werf\/schermen\/werf\/{werfid};werf=werf%2Fapi%2Fwerf%2F{werfid}\"\\n},\\n","tokens":172,"id":2074,"text":"## Context\\nURL's worden gestyled zodat deze clickable zijn in het Identify paneel. We willen graag dat ze ook bruikbaar zijn in de tabel.\\nUit de lagen.json heb ik afgeleid dat sommige van deze links veldtype URL hebben en andere veldtype String.\\nWe willen deze URL's graag op dynamische wijze tonen in de tabel (niet gewoon de URL zelf, maar ook niet telkens gewoon LINK).\\nEen groot deel kan getoond worden op basis van de waarde in een ander veld bv dossiernummer bij AV.\\nVoor andere lagen mag er een vaste string getoond worden.\\nEn voor sommige lagen een combinatie van 2 velden als label.\\n\n\n##Decision\nMomenteel detecteren we links automatisch door naar de start van de string of naar veldtype te kijken. Als het met http of https begint of\\nveldtype 'url' heeft beelden we het af als een link.\\nWe houden voor de tabel vanaf nu rekening met 'html' veld. Indien ingevuld, dan gebruiken we de inhoud van dat html veld om de weergave af te beelden.\\nWe staan toe dat er tokens instaan om bepaalde velden dynamisch\\nin te vullen. Bvb:\\n{ \"isBasisVeld\": true, \"label\": \"Rapport\", \"naam\": \"rapport_url_1\", \"veldType\": \"url\", \"html\": \"{{periode_1_begin}} - {{periode_1_einde}}\" },\\nIndien 'html' leeg is, nemen we de waarde in 'label'.\\nMet het veld \"constante\" moet ook rekening gehouden worden, dat moet soms genomen worden ipv de waarde van het veld. Bvb:\\n{\\n\"isBasisVeld\": true,\\n\"label\": \"Open in Werf\",\\n\"naam\": \"externeurl\",\\n\"veldType\": \"string\",\\n\"html\": \"{werfid}\",\\n\"constante\": \"https:\/\/{domain.name}\/werf\/schermen\/werf\/{werfid};werf=werf%2Fapi%2Fwerf%2F{werfid}\"\\n},\\n","Predictions":"De tabel op voor tabel om een vladio te kijken om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om een vladio te voor tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om tabel om "}
{"File Name":"SearchServices\/0007-message-driven-content-tracker.md","Context":"## Context\nThe ability to search on content requires a content extraction process. This relies on repo getting the document, passing it to one or multiple transformers, and finally returning the plain text content. This process does not scale as the embedded transformation is unable to cope with large volumes or large documents. Embedded transformations in general come with multiple problems, security related and scaling, which led to the introduction of the transformation service with 6.1\nSince transformations to text for content indexing makes up a major portion of the transformation workload, it has always been intended to move these transformations to the new transformation service as well.\nThe following are the suggested approaches to indexing with Transform Service:\n* Refactor the current V0 API (in use by Search Services) to make use of RenditionService2.\n* Introduce a new microservice that sits between Solr and the transformation service. The content is off loaded to the transformation service asynchronously while providing the same synchronous API for Search Services.\n* Search Service to use the get rendition V1 Public API.\n* New content tracker that communicates with the repository asynchronously by messages.\n","Decision":"Based on the group discussion and design reviews, we have agreed to go with the asynchronous content tracker.\\nIn this design the Search Services will place a request in the message queue for the Repo to consume.\\nThe message will contain the NodeId and Solr identifier (name of the instance or Solr shard).\\nOnce the message is consumed by Repo it will start the process to obtain the text for the content.\\nWhen the content is ready a response message will be placed in the queue for Search Services to consume.\\nThe new content tracker will monitor the response queue and consume incoming messages. The message we expect to see in the queue will consist of an identifier, status and a URL. The status of the event can be used for handling errors. The handling of such errors prompting an abort or retry will be finalised during user story creation.\\nOn a successful completion the new content tracker will use the URL to obtain the content and retrieve the text for indexing.\\nWe use a URL in the response message rather than an identifier so that the repository can choose where to store the intermediate content at its own discretion. This will also provide the ability to leverage direct access URLs to cloud storage in the future (e.g. S3 signed URLs).\\nThe benefits of this solution gives ability to index content asynchronously. Unlike the current way which is based on a synchronous call to Repo using HTTP. This solution allows Alfresco to scale the transformation and adds the ability to index more content.\\n![Component Diagram](\/search-services\/alfresco-search\/doc\/architecture\/decisions\/diagrams\/AsyncContentTrackerComponentDiagram.png)\\nThe other options have been considered but did not full fill the requirements.\\nRefactor the current V0 API (in use by Search Services) to make use of RenditionService2:\\nThe thread in the repository will still be blocked. Although the new transform service has a higher throughput, it can have a slightly longer delay. This blocks HTTP threads even longer, or they could even time out.  Using async HTTP introduced with servlet 3.0 has been considered, but this would need to be implemented throughout the entire webscript framework.\\nUsing V1 API requires an authentication for SearchServices, which needs to be configured. There is currently no way for a system to call the V1 API without creating a new user. Creating a new user to represent the system is not the correct way to integrate systems and services. In addition, the V1 API uses the renditions for text which covered below.\\nUsing renditions for text extraction:\\nRenditions are stored long term in the repository as nodes. Using this mechanism for ephemeral text extractions would require ta new retention mechanism for renditions. All of this would put additional workload on the node tree, which defeats the design goal of handling high workloads.\\nIntroduce a new microservice:\\nThis has only been considered as an interim solution if it is not possible to change the content tracker in Search Services. It is essentially the solution above (blocking the sync request from Search Services until transformation is ready) but moved to its own micro service. This solution is slightly better as it does not block repository threads but suffers from the same problems.\\nCreating a microservice for the content tracker adds complexity which adds little benefit as part of this delivery. There are benefits for making the trackers into a microservice, but this would require a significant amount of work. The deployment of Search Services will become complicated as we would have to deploy SearchServices and a micorservice for the new ContentTracker.\\nThe current ContentTracker will remain so that the Community version continues to work with SearchServices.\\nThe V0 content tracking webscript and the ContentTracker will be removed with the next major upgrade in favour of the message base API. At this stage both Enterprise and Community will make use of the message base API. Please note that the communication via message queues between Repo and Search Service, will be configured by default in the ACS (reference) deployment templates. As for the Alfresco 6.x and SearchServices 1.x distribution zip files, will default to the legacy configuration. This would provide the customer different options to upgrade, as we have an overlap between versions.\\nAdding a new content tracker based on a message queue is the preferred solution as this provides the benefits of using the Transform Service.\\n","tokens":227,"id":5123,"text":"## Context\\nThe ability to search on content requires a content extraction process. This relies on repo getting the document, passing it to one or multiple transformers, and finally returning the plain text content. This process does not scale as the embedded transformation is unable to cope with large volumes or large documents. Embedded transformations in general come with multiple problems, security related and scaling, which led to the introduction of the transformation service with 6.1\\nSince transformations to text for content indexing makes up a major portion of the transformation workload, it has always been intended to move these transformations to the new transformation service as well.\\nThe following are the suggested approaches to indexing with Transform Service:\\n* Refactor the current V0 API (in use by Search Services) to make use of RenditionService2.\\n* Introduce a new microservice that sits between Solr and the transformation service. The content is off loaded to the transformation service asynchronously while providing the same synchronous API for Search Services.\\n* Search Service to use the get rendition V1 Public API.\\n* New content tracker that communicates with the repository asynchronously by messages.\\n\n\n##Decision\nBased on the group discussion and design reviews, we have agreed to go with the asynchronous content tracker.\\nIn this design the Search Services will place a request in the message queue for the Repo to consume.\\nThe message will contain the NodeId and Solr identifier (name of the instance or Solr shard).\\nOnce the message is consumed by Repo it will start the process to obtain the text for the content.\\nWhen the content is ready a response message will be placed in the queue for Search Services to consume.\\nThe new content tracker will monitor the response queue and consume incoming messages. The message we expect to see in the queue will consist of an identifier, status and a URL. The status of the event can be used for handling errors. The handling of such errors prompting an abort or retry will be finalised during user story creation.\\nOn a successful completion the new content tracker will use the URL to obtain the content and retrieve the text for indexing.\\nWe use a URL in the response message rather than an identifier so that the repository can choose where to store the intermediate content at its own discretion. This will also provide the ability to leverage direct access URLs to cloud storage in the future (e.g. S3 signed URLs).\\nThe benefits of this solution gives ability to index content asynchronously. Unlike the current way which is based on a synchronous call to Repo using HTTP. This solution allows Alfresco to scale the transformation and adds the ability to index more content.\\n![Component Diagram](\/search-services\/alfresco-search\/doc\/architecture\/decisions\/diagrams\/AsyncContentTrackerComponentDiagram.png)\\nThe other options have been considered but did not full fill the requirements.\\nRefactor the current V0 API (in use by Search Services) to make use of RenditionService2:\\nThe thread in the repository will still be blocked. Although the new transform service has a higher throughput, it can have a slightly longer delay. This blocks HTTP threads even longer, or they could even time out.  Using async HTTP introduced with servlet 3.0 has been considered, but this would need to be implemented throughout the entire webscript framework.\\nUsing V1 API requires an authentication for SearchServices, which needs to be configured. There is currently no way for a system to call the V1 API without creating a new user. Creating a new user to represent the system is not the correct way to integrate systems and services. In addition, the V1 API uses the renditions for text which covered below.\\nUsing renditions for text extraction:\\nRenditions are stored long term in the repository as nodes. Using this mechanism for ephemeral text extractions would require ta new retention mechanism for renditions. All of this would put additional workload on the node tree, which defeats the design goal of handling high workloads.\\nIntroduce a new microservice:\\nThis has only been considered as an interim solution if it is not possible to change the content tracker in Search Services. It is essentially the solution above (blocking the sync request from Search Services until transformation is ready) but moved to its own micro service. This solution is slightly better as it does not block repository threads but suffers from the same problems.\\nCreating a microservice for the content tracker adds complexity which adds little benefit as part of this delivery. There are benefits for making the trackers into a microservice, but this would require a significant amount of work. The deployment of Search Services will become complicated as we would have to deploy SearchServices and a micorservice for the new ContentTracker.\\nThe current ContentTracker will remain so that the Community version continues to work with SearchServices.\\nThe V0 content tracking webscript and the ContentTracker will be removed with the next major upgrade in favour of the message base API. At this stage both Enterprise and Community will make use of the message base API. Please note that the communication via message queues between Repo and Search Service, will be configured by default in the ACS (reference) deployment templates. As for the Alfresco 6.x and SearchServices 1.x distribution zip files, will default to the legacy configuration. This would provide the customer different options to upgrade, as we have an overlap between versions.\\nAdding a new content tracker based on a message queue is the preferred solution as this provides the benefits of using the Transform Service.\\n","Predictions":"We have decided to go with the asynchronous content tracker.nIn this design the Search Services will place a request in the message queue for the Repo to consume.nThe message will contain the NodeId and Solr identifier (name of the instance or Solr shard).nOnce the message is consumed by Repo it will start the process to obtain the text for the content.nWhen the content is ready a response message will be placed in the queue for Search Services to consume.nThe new content tracker will monitor the response queue and consume incoming messages. The message we expect to see in the queue will consist of an identifier, status and a URL. The status of the event can be used for handling errors. The handling of such errors prompting an abort or retry will be finalised during user story creation.nOn a successful completion the new content tracker will use the URL to obtain the content and retrieve the text for indexing.nWe use a URL in the response message rather than an identifier so that the search services can easily identify the content.nWe use a URL in the response message rather than an identifier so that we can easily identify the content.nWe use a URL in the response message rather than an identifier so that we can easily identify the content.nWe use a URL in the response message rather than an identifier so that we can easily identify the content.n"}
{"File Name":"react-transcript-editor\/2018-10-05-components-comunication.md","Context":"## Context and Problem Statement\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\n## Decision Drivers <!-- optional -->\n* Simple and straightforward way to reason around passing data between components\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\n","Decision":"* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\nStill evaluating, leaning torwards some light refactoring to enable parent component, option 1 to keep things simple.\\n<!-- ### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026 -->\\n","tokens":71,"id":3187,"text":"## Context and Problem Statement\\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\\n## Decision Drivers <!-- optional -->\\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n\n\n##Decision\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible\/when if needed.\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\nStill evaluating, leaning torwards some light refactoring to enable parent component, option 1 to keep things simple.\\n<!-- ### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026 -->\\n","Predictions":"* Easy to maintainn* Easy to maintainnChosen option: \"[option 2]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force |... | comes out best (see below)]. -->nStill evaluating, leaning torwards some light refactoring to enable parent component, option 1 to keep things simple.n!-- ### Positive Consequencesn* [e.g., improvement of quality attribute satisfaction, follow-up decisions required,...]n*...n### Negative consequencesn* [e.g., compromising quality attribute, follow-up decisions required,...]n*... -->n"}
{"File Name":"tendermint\/adr-015-crypto-encoding.md","Context":"## Context\nWe must standardize our method for encoding public keys and signatures on chain.\nCurrently we amino encode the public keys and signatures.\nThe reason we are using amino here is primarily due to ease of support in\nparsing for other languages.\nWe don't need its upgradability properties in cryptosystems, as a change in\nthe crypto that requires adapting the encoding, likely warrants being deemed\na new cryptosystem.\n(I.e. using new public parameters)\n","Decision":"### Public keys\\nFor public keys, we will continue to use amino encoding on the canonical\\nrepresentation of the pubkey.\\n(Canonical as defined by the cryptosystem itself)\\nThis has two significant drawbacks.\\nAmino encoding is less space-efficient, due to requiring support for upgradability.\\nAmino encoding support requires forking protobuf and adding this new interface support\\noption in the language of choice.\\nThe reason for continuing to use amino however is that people can create code\\nmore easily in languages that already have an up to date amino library.\\nIt is possible that this will change in the future, if it is deemed that\\nrequiring amino for interacting with Tendermint cryptography is unnecessary.\\nThe arguments for space efficiency here are refuted on the basis that there are\\nfar more egregious wastages of space in the SDK.\\nThe space requirement of the public keys doesn't cause many problems beyond\\nincreasing the space attached to each validator \/ account.\\nThe alternative to using amino here would be for us to create an enum type.\\nSwitching to just an enum type is worthy of investigation post-launch.\\nFor reference, part of amino encoding interfaces is basically a 4 byte enum\\ntype definition.\\nEnum types would just change that 4 bytes to be a variant, and it would remove\\nthe protobuf overhead, but it would be hard to integrate into the existing API.\\n### Signatures\\nSignatures should be switched to be `[]byte`.\\nSpatial efficiency in the signatures is quite important,\\nas it directly affects the gas cost of every transaction,\\nand the throughput of the chain.\\nSignatures don't need to encode what type they are for (unlike public keys)\\nsince public keys must already be known.\\nTherefore we can validate the signature without needing to encode its type.\\nWhen placed in state, signatures will still be amino encoded, but it will be the\\nprimitive type `[]byte` getting encoded.\\n#### Ed25519\\nUse the canonical representation for signatures.\\n#### Secp256k1\\nThere isn't a clear canonical representation here.\\nSignatures have two elements `r,s`.\\nThese bytes are encoded as `r || s`, where `r` and `s` are both exactly\\n32 bytes long, encoded big-endian.\\nThis is basically Ethereum's encoding, but without the leading recovery bit.\\n","tokens":100,"id":1962,"text":"## Context\\nWe must standardize our method for encoding public keys and signatures on chain.\\nCurrently we amino encode the public keys and signatures.\\nThe reason we are using amino here is primarily due to ease of support in\\nparsing for other languages.\\nWe don't need its upgradability properties in cryptosystems, as a change in\\nthe crypto that requires adapting the encoding, likely warrants being deemed\\na new cryptosystem.\\n(I.e. using new public parameters)\\n\n\n##Decision\n### Public keys\\nFor public keys, we will continue to use amino encoding on the canonical\\nrepresentation of the pubkey.\\n(Canonical as defined by the cryptosystem itself)\\nThis has two significant drawbacks.\\nAmino encoding is less space-efficient, due to requiring support for upgradability.\\nAmino encoding support requires forking protobuf and adding this new interface support\\noption in the language of choice.\\nThe reason for continuing to use amino however is that people can create code\\nmore easily in languages that already have an up to date amino library.\\nIt is possible that this will change in the future, if it is deemed that\\nrequiring amino for interacting with Tendermint cryptography is unnecessary.\\nThe arguments for space efficiency here are refuted on the basis that there are\\nfar more egregious wastages of space in the SDK.\\nThe space requirement of the public keys doesn't cause many problems beyond\\nincreasing the space attached to each validator \/ account.\\nThe alternative to using amino here would be for us to create an enum type.\\nSwitching to just an enum type is worthy of investigation post-launch.\\nFor reference, part of amino encoding interfaces is basically a 4 byte enum\\ntype definition.\\nEnum types would just change that 4 bytes to be a variant, and it would remove\\nthe protobuf overhead, but it would be hard to integrate into the existing API.\\n### Signatures\\nSignatures should be switched to be `[]byte`.\\nSpatial efficiency in the signatures is quite important,\\nas it directly affects the gas cost of every transaction,\\nand the throughput of the chain.\\nSignatures don't need to encode what type they are for (unlike public keys)\\nsince public keys must already be known.\\nTherefore we can validate the signature without needing to encode its type.\\nWhen placed in state, signatures will still be amino encoded, but it will be the\\nprimitive type `[]byte` getting encoded.\\n#### Ed25519\\nUse the canonical representation for signatures.\\n#### Secp256k1\\nThere isn't a clear canonical representation here.\\nSignatures have two elements `r,s`.\\nThese bytes are encoded as `r || s`, where `r` and `s` are both exactly\\n32 bytes long, encoded big-endian.\\nThis is basically Ethereum's encoding, but without the leading recovery bit.\\n","Predictions":"### Public keysnFor public keys, we will continue to use amino encoding on the canonicalnrepresentation of the pubkey.n(Canonical as defined by the cryptosystem itself)nThis has two significant drawbacks.nAmino encoding is less space-efficient, due to requiring support for upgradability.nAmino encoding support requires forking protobuf and adding this new interface supportnoption in the language of choice.nThis has two significant drawbacks.nAmino encoding is less space-efficient, due to requiring support for upgradability.nAmino encoding support requires forking protobuf and adding this new interface supportnoption in the language of choice.nThis has two significant drawbacks.nAmino encoding is less space-efficient, due to requiring support for upgradability.nAmino encoding support requires forking protobuf and adding this new interface supportnoption in the language of choice.nThis has two significant drawbacks.nAmino encoding is less space-efficient, due to requiring support for upgradability.nAmino encoding support requires forking protobuf and adding this new interface supportnoption in the language of choice.nThis has two significant drawbacks.nAmino encoding is less space-efficient, due to requiring support for upgradability.nAmino encoding support requires forking protobuf and adding this new interface supportnoption in the language of choice.nThis has two significant drawbacks.nAmino encoding is less space-efficient, due to requiring support for upgradability.nAmino encoding support requires forking protobuf and adding this new interface supportnoption in the language of choice.nThis has two significant drawbacks.nAmino encoding is less space-efficient, due to requiring support for upgradability.nAmino encoding support requires forking protobuf and adding this new interface supportnoption in the language of choice.nThis has two significant drawbacks.nAmino encoding is less space-efficient, due to requiring support for upgradability.nAmino encoding support requires forking protobuf and adding this new interface supportnoption in the language of choice.nThis has two significant drawbacks.nAmino encoding is less space-efficient, due to requiring support for upgradability.nAmino encoding support requires forking protobuf and adding this new interface supportnoption in the language of choice.nThis has two significant drawbacks.nAmino encoding is less space-efficient, due to requiring support for upgradability.nAmino encoding support requires forking protobuf and adding this new interface supportnoption in the language of choice.nThis has two significant drawbacks.nAmino encoding is less space-efficient, due to requiring support for upgradability.nAmino encoding support requires forking protobuf and adding this new interface supportnoption in the language of choice.nThis has two significant drawbacks.nAmino encoding is less space-efficient, due to requiring support for upgradability.nAmino encoding support requires forking protobuf and adding this new interface supportnoption in the language of choice.nThis has two significant drawbacks.nAmino encoding is less space-efficient, due to requiring support for upgradability.nAmino encoding support requires forking protobuf and adding this new interface supportnoption in the language of choice.nThis has two significant drawbacks.nAmino encoding is less space-efficient, due to requiring support for upgradability.nAmino encoding support requires forking protobuf and adding this new interface supportnoption in the language of choice.nThis has two significant drawbacks.nAmino encoding is less space-efficient, due to requiring support for upgradability.nAmino encoding is less space-efficient, due to requiring support for up"}
{"File Name":"gsp\/ADR032-sre-permissions.md","Context":"## Context\nAs mitigation for some risks raised in threat modelling it was agreed that day-to-day access to the cluster was to be read-only for everyone. Only the concourse running in the cluster could make changes that originated from Github, which required several approvals before merging.\nFollowing the gradual rollout of several applications onto the GSP it became clear there were issues with the deployment procedures. This caused conflicting and contending pods to attempt to execute, resulting in application failures and deployment pipeline blockages. This was happening up to several times a day, depending on the level of activity. The remedial procedure involves escalating one or more members to cluster admin to allow the resources to be deleted, before revoking the admin permissions again. This process requires 3 people to perform and could result in hours of wasted time for each occurrence.\n","Decision":"We will add to the SRE permissions map the ability to delete the following higher-level controllers so an escalation to cluster admin is no longer necessary:\\n* ConfigMap\\n* Deployment\\n* ReplicaSet\\n* Secret\\n* Service\\n* StatefulSet\\nWe will also raise a story to investigate the root cause of the deployment issues with a view to removing these permissions in the future.\\n","tokens":166,"id":3915,"text":"## Context\\nAs mitigation for some risks raised in threat modelling it was agreed that day-to-day access to the cluster was to be read-only for everyone. Only the concourse running in the cluster could make changes that originated from Github, which required several approvals before merging.\\nFollowing the gradual rollout of several applications onto the GSP it became clear there were issues with the deployment procedures. This caused conflicting and contending pods to attempt to execute, resulting in application failures and deployment pipeline blockages. This was happening up to several times a day, depending on the level of activity. The remedial procedure involves escalating one or more members to cluster admin to allow the resources to be deleted, before revoking the admin permissions again. This process requires 3 people to perform and could result in hours of wasted time for each occurrence.\\n\n\n##Decision\nWe will add to the SRE permissions map the ability to delete the following higher-level controllers so an escalation to cluster admin is no longer necessary:\\n* ConfigMap\\n* Deployment\\n* ReplicaSet\\n* Secret\\n* Service\\n* StatefulSet\\nWe will also raise a story to investigate the root cause of the deployment issues with a view to removing these permissions in the future.\\n","Predictions":"We will add to the SRE permissions map the ability to delete the following higher-level controllers so an escalation to cluster admin is no longer necessary:n* ConfigMapn* Deploymentn* ReplicaSetn* Secretn* Servicen* StatefulSetnWe will also raise a story to investigate the root cause of the deployment issues with a view to removing these permissions in the future.n"}
{"File Name":"texas-holdem-code-challenge\/0002-use-node-js.md","Context":"## Context\n- The problem instructions state \"C++ or node.js are preferred\".\n- I am most familiar with node.js out of those two.\n- I am completing this problem in my spare time and have a limited amount of\ntime to spend on it: it's convenient to work faster if possible.\n","Decision":"Use Node.js to complete this problem.\\n","tokens":67,"id":2294,"text":"## Context\\n- The problem instructions state \"C++ or node.js are preferred\".\\n- I am most familiar with node.js out of those two.\\n- I am completing this problem in my spare time and have a limited amount of\\ntime to spend on it: it's convenient to work faster if possible.\\n\n\n##Decision\nUse Node.js to complete this problem.\\n","Predictions":"-!-- What is the change that we're proposing and\/or doing? -->n- 1. The first parameter should be a scope of type GuStack.n- The second parameter should be a scope of type GuStack.n- The third parameter should be a scope of type GuStack.n- The first parameter should be a scope of type GuStack.n- The second parameter should be a scope of type GuStack.n- The third parameter should be a scope of type GuStack.n- The first parameter should be a scope of type GuStack.n- The second parameter should be a scope of type GuStack.n- The third parameter should be a scope of type GuStack.n- The first parameter should be a scope of type GuStack.n- The second parameter should be a scope of type GuStack.n- The third parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The third parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth parameter should be a scope of type GuStack.n- The fifth"}
{"File Name":"portfolio\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\nEven though this project is mainly a 1 engineer side-project, recording architectural decisions allows using the records as a reminder of the \"state of mind\" we were in when making a decision, as well as the motivations behind it.\nAdditionally, it's a good engineering practice that would be nice to try out, and this blog is a great \"guinea pig\" for this approach.\nSee this article on [GitHub's engineering blog](https:\/\/github.blog\/2020-08-13-why-write-adrs\/) for some addition reasons why they could be a good practice.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nADRs from here on out will be written in the first-person **singular** tense, since it's mainly just me building the app.\\n","tokens":135,"id":2091,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\nEven though this project is mainly a 1 engineer side-project, recording architectural decisions allows using the records as a reminder of the \"state of mind\" we were in when making a decision, as well as the motivations behind it.\\nAdditionally, it's a good engineering practice that would be nice to try out, and this blog is a great \"guinea pig\" for this approach.\\nSee this article on [GitHub's engineering blog](https:\/\/github.blog\/2020-08-13-why-write-adrs\/) for some addition reasons why they could be a good practice.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nADRs from here on out will be written in the first-person **singular** tense, since it's mainly just me building the app.\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).nADRs from here on out will be written in the first-person **singular** tense, since it's mainly just me building the app.n"}
{"File Name":"dogma\/0018-projection-compaction.md","Context":"## Context\nMany projections produce data that is only required for a limited period of\ntime.\nAn application developer needs to consider how long projection data will be\nretained and ideally implement measures to remove any unnecessary data.\nOften such data can be removed when some future event occurs. However, in some\ncases there is no future event that appropriately indicates the data is no\nlonger required.\nWe have encountered a genuine example of this when implementing an SQL\nprojection that inserts a row whenever a user performs a specific action. The\ndata is queried in order to enforce a \"cool down\" that prevents the user from\nrepeating that action again within a specific time frame.\nThe insert is triggered by the occurrence of an event, but the data becomes\nunnecessary whenever the \"cool down\" time has elapsed.\nIn this particular use case the \"cool down\" was not part of the business logic,\nbut rather an API level restriction. Hence, processes\/timeout messages were not\nthe appropriate solution.\n","Decision":"We have decided to add a `Compact()` method to `ProjectionMessageHandler`.\\nThe implementation of `Compact()` can modify the projection's data by whatever\\nmeans is appropriate such that unnecessary data is removed but the projection\\nstill serves its purpose.\\n","tokens":210,"id":1606,"text":"## Context\\nMany projections produce data that is only required for a limited period of\\ntime.\\nAn application developer needs to consider how long projection data will be\\nretained and ideally implement measures to remove any unnecessary data.\\nOften such data can be removed when some future event occurs. However, in some\\ncases there is no future event that appropriately indicates the data is no\\nlonger required.\\nWe have encountered a genuine example of this when implementing an SQL\\nprojection that inserts a row whenever a user performs a specific action. The\\ndata is queried in order to enforce a \"cool down\" that prevents the user from\\nrepeating that action again within a specific time frame.\\nThe insert is triggered by the occurrence of an event, but the data becomes\\nunnecessary whenever the \"cool down\" time has elapsed.\\nIn this particular use case the \"cool down\" was not part of the business logic,\\nbut rather an API level restriction. Hence, processes\/timeout messages were not\\nthe appropriate solution.\\n\n\n##Decision\nWe have decided to add a `Compact()` method to `ProjectionMessageHandler`.\\nThe implementation of `Compact()` can modify the projection's data by whatever\\nmeans is appropriate such that unnecessary data is removed but the projection\\nstill serves its purpose.\\n","Predictions":"We have decided to add a Compact() method to ProjectionMessageHandler.nThe implementation of Compact() can modify the projection's data by whatevernmeans is appropriate such that unnecessary data is removed but the projectionnstill serves its purpose.n"}
{"File Name":"GDD-app\/0003-use-bitrise-for-ci.md","Context":"## Context\nWe need an easy way to integrate and test out code that is fast and reliable.\n","Decision":"We choose Bitrise because it came from a suggestion from more senior devs and provides an easy interface to manage the workflows with good support for Android apps and testing.\\nIt also allows us to notify users in a easy way and with different roles.\\n","tokens":21,"id":4868,"text":"## Context\\nWe need an easy way to integrate and test out code that is fast and reliable.\\n\n\n##Decision\nWe choose Bitrise because it came from a suggestion from more senior devs and provides an easy interface to manage the workflows with good support for Android apps and testing.\\nIt also allows us to notify users in a easy way and with different roles.\\n","Predictions":"We choose Bitrise because it came from a suggestion from more senior devs and provides an easy interface to manage the workflows with good support for Android apps and testing.nIt also allows us to notify users in a easy way and with different roles.n"}
{"File Name":"scholarsphere\/0009-acl-actor-permissions.md","Context":"## Context\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\nWhen determining who has access to a given resource, both these sources may need to be consulted.\n","Decision":"Access controls and depositor or proxy depositor rights are independent from one another.\\nAccess controls should not include permissions granted by the Actor-to-resource arrangement, such as edit rights of the\\ndepositor. They are a separate form of permission structure and therefore independent of one another. Likewise,\\npermissions that come from a depositor should have no bearing on what access controls may be applied to a resource.\\n","tokens":79,"id":4554,"text":"## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n\n\n##Decision\nAccess controls and depositor or proxy depositor rights are independent from one another.\\nAccess controls should not include permissions granted by the Actor-to-resource arrangement, such as edit rights of the\\ndepositor. They are a separate form of permission structure and therefore independent of one another. Likewise,\\npermissions that come from a depositor should have no bearing on what access controls may be applied to a resource.\\n","Predictions":"We will use the [JSON Web Tokens](https:\/\/jwt.io) as a token for authentication and access control.n"}
{"File Name":"molgenis-js-auth\/0002-manage-user-permissions-in-fusion-auth.md","Context":"## Context\nIn Armadillo:\n* A cohort is an OpenID application\n* The researcher needs a `ROLE_<STUDY>_RESEARCHER` on the application to read data\nin shared folder `<study>`\nRequirements on who manages what:\n* User and permission management is done by representatives of the consortium and cohort\n* The identity of the user is federated to their local institution\n* Representatives of the cohort administer the study researchers\u2019 admission to their cohort\n* Representatives of the consortium can see which users have which roles on which cohort\n","Decision":"Implement this in Fusion Auth, plus a (modest) administration UI.\\n* The users register themselves.\\n* A cohort representative explicitly creates a registration for the researcher in the application,\\nwith one or more roles.\\n### Administration\\nApplication-specific permissions are not available in Fusion Auth.\\nIn other products they tend to be rather complicated to configure.\\nCreate an edge service that enforces the permissions and uses\\nan api key to administer them through the Fusion Auth API.\\n","tokens":116,"id":4292,"text":"## Context\\nIn Armadillo:\\n* A cohort is an OpenID application\\n* The researcher needs a `ROLE_<STUDY>_RESEARCHER` on the application to read data\\nin shared folder `<study>`\\nRequirements on who manages what:\\n* User and permission management is done by representatives of the consortium and cohort\\n* The identity of the user is federated to their local institution\\n* Representatives of the cohort administer the study researchers\u2019 admission to their cohort\\n* Representatives of the consortium can see which users have which roles on which cohort\\n\n\n##Decision\nImplement this in Fusion Auth, plus a (modest) administration UI.\\n* The users register themselves.\\n* A cohort representative explicitly creates a registration for the researcher in the application,\\nwith one or more roles.\\n### Administration\\nApplication-specific permissions are not available in Fusion Auth.\\nIn other products they tend to be rather complicated to configure.\\nCreate an edge service that enforces the permissions and uses\\nan api key to administer them through the Fusion Auth API.\\n","Predictions":"* The user is a member of the consortium.n* The researcher is a member of the cohort.n* The researcher is a member of the study.n* The researcher is a member of the cohort.n* The researcher is a member of the cohort.n* The researcher is a member of the study.n* The researcher is a member of the cohort.n* The researcher is a member of the study.n* The researcher is a member of the cohort.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the study.n* The researcher is a member of the"}
{"File Name":"origin\/0010-more-complete-database-type-and-adjust-migrations.md","Context":"## Context\nSo far, Origin has been using a development-friendly SQLite database on the backend to store data. While this has been beneficial in the early stages of development, in order to gain adoption and real-world use we should make it easier for anyone using the Origin SDK to get to production as fast as possible.\n","Decision":"A decision has been made to move the Origin SDK to a mode production-ready database. We've decided to go with **PostgreSQL** instead of SQLite, as we've seen that this is the database of choice for many users and setups.\\n","tokens":64,"id":1561,"text":"## Context\\nSo far, Origin has been using a development-friendly SQLite database on the backend to store data. While this has been beneficial in the early stages of development, in order to gain adoption and real-world use we should make it easier for anyone using the Origin SDK to get to production as fast as possible.\\n\n\n##Decision\nA decision has been made to move the Origin SDK to a mode production-ready database. We've decided to go with **PostgreSQL** instead of SQLite, as we've seen that this is the database of choice for many users and setups.\\n","Predictions":"We've decided to go with **PostgreSQL** instead of SQLite, as we've seen that this is the database of choice for many users and setups.n"}
{"File Name":"libelektra\/error_codes.md","Context":"## Problem\nThe current error concept has disadvantages in following regards:\n- A lot of redundant errors\nAt the moment, each new plugin introduces new error codes which led to about 210+ error codes. Many of those errors\nare duplicated because developers did not know or search for a similar error which is already present. This concept should\ngroup similar errors together so that there is one coherent and consistent state again.\n- Hard to manage specification file\nSince every developer adds its own error individually, a lot of merge conflicts happen which makes contributing to the codebase\nunpleasant. Additionally, if you want to reuse any error you have to scrape to the whole file with ~1300+ lines. As there is no\nsenseful ordering or scheme behind the errors (since they grew by time), it is a hassle to find the correct error code.\nThe new concept should standardize errors, making it easy to categorize errors from new plugins and avoid merge conflicts.\n- No senseful way for application developers to use error codes from elektra\nIf developers of plugins\/ external tools using elektra want to react to errors, they have to be very specific. At the moment there is\nno possibility to catch all errors easily which force a certain behavior. Eg. if there happens a temporary recoverable error, developers have to\ncatch for every specific error code rather than a general hierarchical error. The new concept should make it easy to react to errors as they are\nsensefully grouped together and are hierarchically structured.\n","Decision":"All \"fatal\" errors will be converted to \"errors\" as the distinction is not relevant.\\nUnused errors will be removed from the specification.\\nErrors will be categorized into logical groups with subgroups.\\nEach error will be made up of 5 characters, where the first 2 character indicate the highest level\\nand character 3 to 5 will be used for subgrouping. Errors are prepended with the letter `C` which\\nis the abbreviation for \"Code\".\\n- Permanent errors C01000\\n- Resource C01100\\n- Out of Memory C01110\\n- Installation C01200\\n- Logical C01300\\n- Internal C01310\\n- Interface C01320\\n- Plugin Misbehavior C01330\\n- Conflicting State C02000\\n- Validation C03000\\n- Syntactic C03100\\n- Semantic C03200\\nTo see an explanation of the categories along with a guideline on how to categorize please\\nsee the [Error Codes Guideline](..\/dev\/error-categorization.md)\\n- [Error Message Format](error_message_format.md)\\nShows the new format of the error message\\n- [Error Codes Guideline](..\/dev\/error-categorization.md)\\nShows how to categorize errors\\n","tokens":312,"id":1319,"text":"## Problem\\nThe current error concept has disadvantages in following regards:\\n- A lot of redundant errors\\nAt the moment, each new plugin introduces new error codes which led to about 210+ error codes. Many of those errors\\nare duplicated because developers did not know or search for a similar error which is already present. This concept should\\ngroup similar errors together so that there is one coherent and consistent state again.\\n- Hard to manage specification file\\nSince every developer adds its own error individually, a lot of merge conflicts happen which makes contributing to the codebase\\nunpleasant. Additionally, if you want to reuse any error you have to scrape to the whole file with ~1300+ lines. As there is no\\nsenseful ordering or scheme behind the errors (since they grew by time), it is a hassle to find the correct error code.\\nThe new concept should standardize errors, making it easy to categorize errors from new plugins and avoid merge conflicts.\\n- No senseful way for application developers to use error codes from elektra\\nIf developers of plugins\/ external tools using elektra want to react to errors, they have to be very specific. At the moment there is\\nno possibility to catch all errors easily which force a certain behavior. Eg. if there happens a temporary recoverable error, developers have to\\ncatch for every specific error code rather than a general hierarchical error. The new concept should make it easy to react to errors as they are\\nsensefully grouped together and are hierarchically structured.\\n\n\n##Decision\nAll \"fatal\" errors will be converted to \"errors\" as the distinction is not relevant.\\nUnused errors will be removed from the specification.\\nErrors will be categorized into logical groups with subgroups.\\nEach error will be made up of 5 characters, where the first 2 character indicate the highest level\\nand character 3 to 5 will be used for subgrouping. Errors are prepended with the letter `C` which\\nis the abbreviation for \"Code\".\\n- Permanent errors C01000\\n- Resource C01100\\n- Out of Memory C01110\\n- Installation C01200\\n- Logical C01300\\n- Internal C01310\\n- Interface C01320\\n- Plugin Misbehavior C01330\\n- Conflicting State C02000\\n- Validation C03000\\n- Syntactic C03100\\n- Semantic C03200\\nTo see an explanation of the categories along with a guideline on how to categorize please\\nsee the [Error Codes Guideline](..\/dev\/error-categorization.md)\\n- [Error Message Format](error_message_format.md)\\nShows the new format of the error message\\n- [Error Codes Guideline](..\/dev\/error-categorization.md)\\nShows how to categorize errors\\n","Predictions":"We will convert all \"fatal\" errors to \"errors\" as the distinction is not relevant.nUnused errors will be removed from the specification.nErrors will be grouped into logical groups with subgroups.nEach error will be made up of 5 characters, where the first 2 character indicate the highest levelnand character 3 to 5 will be used for subgrouping. Errors are prepended with the letter C whichnis the abbreviation for \"Code\".n- Permanent errors C01000n- Resource C01100n- Out of Memory C01110n- Installationn"}
{"File Name":"MoviesAndroid\/0002 Kotlin.md","Context":"## Context and Problem Statement\nWhen starting a new app you can now choose to build it in either Java or Kotlin. You can of course\ndon't choose and use both, but i want to make it consistent.\n## Decision Drivers\n* New empty project\n* Kotlin now fully supported for Android by Google\n","Decision":"* New empty project\\n* Kotlin now fully supported for Android by Google\\nChosen option: Kotlin, because it's way more modern than Java and fully supported in Android by\\nGoogle now, which eliminates the risk of being dropped any time soon.\\nPositive Consequences:\\n* Less code, more fun!\\n* Having to learn a new language\\n* Great interoperability with Java if needed\\nNegative consequences:\\n* Having to learn a new language\\n","tokens":64,"id":159,"text":"## Context and Problem Statement\\nWhen starting a new app you can now choose to build it in either Java or Kotlin. You can of course\\ndon't choose and use both, but i want to make it consistent.\\n## Decision Drivers\\n* New empty project\\n* Kotlin now fully supported for Android by Google\\n\n\n##Decision\n* New empty project\\n* Kotlin now fully supported for Android by Google\\nChosen option: Kotlin, because it's way more modern than Java and fully supported in Android by\\nGoogle now, which eliminates the risk of being dropped any time soon.\\nPositive Consequences:\\n* Less code, more fun!\\n* Having to learn a new language\\n* Great interoperability with Java if needed\\nNegative consequences:\\n* Having to learn a new language\\n","Predictions":"* New empty projectn* Kotlin now fully supported for Android by GooglenChosen option: Kotlin, because it's way more modern than Java and fully supported in Android by Google now, which eliminates the risk of being dropped any time soon.nPositive Consequences:n* Less code, more fun!n* Having to learn a new languagen* Great interoperability with Java if needednNegative consequences:n* Having to learn a new languagen"}
{"File Name":"PactSwift\/ADR-002-Coding_standard.md","Context":"# Context\nWe shouldn't feel bound by any pre-existing coding standards so this project and its code is written according to personal preferences based on practices that yielded good results acquired working in other projects with many collaborators. The code is relatively consistent but that might change once more developers contribute to the project.\nIn general, Swift code has a fairly strong styling, relative to C or C++, due to opinionated aspects of the language itself and the styling used by the official language guides. Formatting around brace placement, `if` and `for` styling is fairly clearly set by the language.\n# Decision\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\n","Decision":"[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n","tokens":150,"id":146,"text":"# Context\\nWe shouldn't feel bound by any pre-existing coding standards so this project and its code is written according to personal preferences based on practices that yielded good results acquired working in other projects with many collaborators. The code is relatively consistent but that might change once more developers contribute to the project.\\nIn general, Swift code has a fairly strong styling, relative to C or C++, due to opinionated aspects of the language itself and the styling used by the official language guides. Formatting around brace placement, `if` and `for` styling is fairly clearly set by the language.\\n# Decision\\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n\n\n##Decision\n[Swiftlint configuration](.\/..\/..\/.swiftlint.yml) is used to enforce us adhering to _most_ of code style conventions.\\n","Predictions":"We will use a swift-config file to configure the swift-config file.n"}
{"File Name":"ELEN4010\/ADR Sprint Planning and timeline.md","Context":"## Context\n16-25 April, Thabang is away, must work remotely. Major submission period over 29th April - 10th May, will impact productivity on software project.\n","Decision":"4 Sprints planned, consecutively. Will only start on 17th April, but have a \"Sprint 0\" from 9th to 16 April, with initial planning, research and folder structure creation, setup of Azure and Travis. Will not count towards actual number of sprints.\\n* Sprints will begin on Wednesdays, with a 1 hr review session in the morning.\\n* There will be a three hour sprint planning session in the afternooon each Wednesday.\\n* Release will be every Tuesday, by 8pm\\n* Product release window will be from 2pm - 8pm, with all pull requests done before 6pm to give time for any required last minute code review and testing\\n* Friday coding sessions together from 12pm - 4pm\\n* Standups via Whatsapp, or between lectures. Preferable to do in person but may not be possible.\\nRebecca: Product Manager\\nTyson: SCRUM Master\\nTheese roles will be alternated throughout the project each week.\\n","tokens":40,"id":57,"text":"## Context\\n16-25 April, Thabang is away, must work remotely. Major submission period over 29th April - 10th May, will impact productivity on software project.\\n\n\n##Decision\n4 Sprints planned, consecutively. Will only start on 17th April, but have a \"Sprint 0\" from 9th to 16 April, with initial planning, research and folder structure creation, setup of Azure and Travis. Will not count towards actual number of sprints.\\n* Sprints will begin on Wednesdays, with a 1 hr review session in the morning.\\n* There will be a three hour sprint planning session in the afternooon each Wednesday.\\n* Release will be every Tuesday, by 8pm\\n* Product release window will be from 2pm - 8pm, with all pull requests done before 6pm to give time for any required last minute code review and testing\\n* Friday coding sessions together from 12pm - 4pm\\n* Standups via Whatsapp, or between lectures. Preferable to do in person but may not be possible.\\nRebecca: Product Manager\\nTyson: SCRUM Master\\nTheese roles will be alternated throughout the project each week.\\n","Predictions":"* We will start on the first day of the month, with a 1 hr review session in the morning.n* We will start on the second day of the month, with a 3 hour sprint planning session in the afternoon.n* We will start on the third day of the month, with a 1 hr review session in the morning.n* We will start on the fifth day of the month, with a 3 hour sprint planning session in the afternoon.n* We will start on the fifth day of the month, with a 1 hr review session in the morning.n* We will start on the fifth day of the month, with a 3 hour sprint planning session in the afternoon.n* We will start on the fifth day of the month, with a 1 hr review session in the morning.n* We will start on the fifth day of the month, with a 3 hour sprint planning session in the afternoon.n* We will start on the fifth day of the month, with a 1 hr review session in the morning.n* We will start on the fifth day of the month, with a 3 hour sprint planning session in the afternoon.n* We will start on the fifth day of the month, with a 1 hr review session in the morning.n* We will start on the fifth day of the month, with a 3 hour sprint planning session in the afternoon.n* We will start on the fifth day of the month, with a 1 hr review session in the morning.n* We will start on the fifth day of the month, with a 3 hour sprint planning session in the afternoon.n* We will start on the fifth day of the month, with a 1 hr review session in the morning.n* We will start on the fifth day of the month, with a 3 hour sprint planning session in the afternoon.n* We will start on the fifth day of the month, with a 1 hr review session in the morning.n* We will start on the fifth day of the month, with a 3 hour sprint planning session in the afternoon.n* We will start on the fifth day of the month, with a 1 hr review session in the morning.n* We will start on the fifth day of the month, with a 3 hour sprint planning session in the afternoon.n* We will start on the fifth day of the month, with a 3 hour sprint planning session in the afternoon.n* We will start on the fifth day of the month, with a 1 hr review session in the morning.n* We will start on the fifth day of the month, with a 3 hour sprint planning session in the afternoon.n* We will start on the fifth day of the month, with a 3 hour sprint planning session in the afternoon.n* We will start on the fifth day of the month, with a 3 hour sprint planning session in the afternoon.n* We will start on the fifth day of the month, with a 3 hour sprint planning session in the afternoon.n* We will start on the fifth day of the month, with a 3 hour sprint planning session in the afternoon.n"}
{"File Name":"SAP-Cloud\/node-package-lock.md","Context":"## Context\nThe node package manager (npm) creates a so called [\"lock file\"](https:\/\/docs.npmjs.com\/files\/package-locks) when running `npm install`.\nThe recommended action is to commit this file to version control, as stated by the command itself:\n```\n$ npm install\nnpm notice created a lockfile as package-lock.json. You should commit this file.\n```\nSome npm commands, which are part of the pipeline, such as `ci` and `audit` require that a package lock file exists.\nThus, if a project lacks the package lock file, there are tree options:\n* Refuse to build the project, which enforces the recommended practice of committing the lock file\n* Silently create a package lock file as part of the pipeline\n* Avoid npm commands which require a package lock\n* Would disable the npm audit stage\n","Decision":"If a project has a `package.json` file, but no package lock, we create a package lock file as part of the pipeline and warn the user about this.\\nThe created package lock file is not archived as part of the pipeline build artifacts.\\n","tokens":183,"id":2924,"text":"## Context\\nThe node package manager (npm) creates a so called [\"lock file\"](https:\/\/docs.npmjs.com\/files\/package-locks) when running `npm install`.\\nThe recommended action is to commit this file to version control, as stated by the command itself:\\n```\\n$ npm install\\nnpm notice created a lockfile as package-lock.json. You should commit this file.\\n```\\nSome npm commands, which are part of the pipeline, such as `ci` and `audit` require that a package lock file exists.\\nThus, if a project lacks the package lock file, there are tree options:\\n* Refuse to build the project, which enforces the recommended practice of committing the lock file\\n* Silently create a package lock file as part of the pipeline\\n* Avoid npm commands which require a package lock\\n* Would disable the npm audit stage\\n\n\n##Decision\nIf a project has a `package.json` file, but no package lock, we create a package lock file as part of the pipeline and warn the user about this.\\nThe created package lock file is not archived as part of the pipeline build artifacts.\\n","Predictions":"* npm install will create a package lock file as part of the pipeline and warn the user about this.n* Silently create a package lock file as part of the pipelinen* Avoid npm commands which require a package lock filen* Would disable the npm audit stagen"}
{"File Name":"court-case-service\/0005-guiding-principles.md","Context":"## Context\nThe implementation of [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) raised questions about what is the appropriate approach for serving new data to the prepare-a-case app, and in particular stressed the need for error handling by `prepare-a-case` where graceful degradation from partial failures is needed. The question was raised as to whether `court-case-service` should be coupled closely with `prepare-a-case` and is notionally a dedicated gateway for it or whether it should be treated as a general purpose API which may be consumed by multiple consumers.\n","Decision":"1. All data stored by `court-case-service` is considered its own domain and should be treated as a general purpose api\\n2. All endpoints composing data on behalf of `prepare-a-case` are not intended for consumption by other clients and should provide all data needed on a given `prepare-a-case` page as efficiently as possible\\n3. The exception to point 2 is the case where errors in the retrieval of certain pieces of data are expected and need to be isolated. In this case data to be isolated should be served as a separate endpoint which will fail fast. `prepare-a-case` can then handle it simply as a failed HTTP call. See [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) for an example of this.\\n","tokens":129,"id":3579,"text":"## Context\\nThe implementation of [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) raised questions about what is the appropriate approach for serving new data to the prepare-a-case app, and in particular stressed the need for error handling by `prepare-a-case` where graceful degradation from partial failures is needed. The question was raised as to whether `court-case-service` should be coupled closely with `prepare-a-case` and is notionally a dedicated gateway for it or whether it should be treated as a general purpose API which may be consumed by multiple consumers.\\n\n\n##Decision\n1. All data stored by `court-case-service` is considered its own domain and should be treated as a general purpose api\\n2. All endpoints composing data on behalf of `prepare-a-case` are not intended for consumption by other clients and should provide all data needed on a given `prepare-a-case` page as efficiently as possible\\n3. The exception to point 2 is the case where errors in the retrieval of certain pieces of data are expected and need to be isolated. In this case data to be isolated should be served as a separate endpoint which will fail fast. `prepare-a-case` can then handle it simply as a failed HTTP call. See [PIC-1135](https:\/\/dsdmoj.atlassian.net\/browse\/PIC-1135?focusedCommentId=163988) for an example of this.\\n","Predictions":"We will use a Security folder for the Security component.n"}
{"File Name":"skypy\/adr-01.md","Context":"## Context\nWithin SkyPy all functions used to create a \"simulation\" will in practice be taking in some values (either parameters or columns from a table) and creating new column(s) in an output table *or* selecting specific rows from an input table.\nThe inputs and outputs of these functions are clearly defined so a directed acyclic graph (DAG) can be constructed to determine what order the functions should be run in.\nTo aid in the creation of the tables and the DAG a helper class or decorator should be used so the person writing the function does not have to worry about the implementation details. This class or decorator is what we are currently referring to as the `Model`.\nFor clarity in the options below we will assume the following example function:\n```python\ndef redshift_gamma(shape, scale, size):\n\"\"\"Gamma-distributed redshifts (Smail et al. 1994).\nSample `size` redshifts from a gamma distribution with the\ngiven `shape` and `scale` parameters. See `numpy.random.gamma`.\n\"\"\"\n# redshift distribution\nredshift = np.random.gamma(shape=shape, scale=scale, size=size)\nreturn redshift\n```\n## Decision Drivers\n- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\n","Decision":"- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/pull\/38) option 3 has been picked.  This will be easiest for developers to write new functions and write clean unit tests.  Within the example given above `pyungo` was just used as an example, other DAG frameworks exist and picking one should be the topic of a different ADR.\\n","tokens":301,"id":5147,"text":"## Context\\nWithin SkyPy all functions used to create a \"simulation\" will in practice be taking in some values (either parameters or columns from a table) and creating new column(s) in an output table *or* selecting specific rows from an input table.\\nThe inputs and outputs of these functions are clearly defined so a directed acyclic graph (DAG) can be constructed to determine what order the functions should be run in.\\nTo aid in the creation of the tables and the DAG a helper class or decorator should be used so the person writing the function does not have to worry about the implementation details. This class or decorator is what we are currently referring to as the `Model`.\\nFor clarity in the options below we will assume the following example function:\\n```python\\ndef redshift_gamma(shape, scale, size):\\n\"\"\"Gamma-distributed redshifts (Smail et al. 1994).\\nSample `size` redshifts from a gamma distribution with the\\ngiven `shape` and `scale` parameters. See `numpy.random.gamma`.\\n\"\"\"\\n# redshift distribution\\nredshift = np.random.gamma(shape=shape, scale=scale, size=size)\\nreturn redshift\\n```\\n## Decision Drivers\\n- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\n\n\n##Decision\n- Ease of use: if there is too much boiler plate `Model`s will be annoying to write\\n- Clarity of implementation: the base `Model` should be easy to read, understand, and debug\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/pull\/38) option 3 has been picked.  This will be easiest for developers to write new functions and write clean unit tests.  Within the example given above `pyungo` was just used as an example, other DAG frameworks exist and picking one should be the topic of a different ADR.\\n","Predictions":"- Ease of use: if there is too much boilerplate Models will be annoying to writen- Clarity of implementation: the base Model should be easy to read, understand, and debugnAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/pull\/38) option 4 has been picked. This will be easiest for developers to write new functions and write clean unit tests. Within the [discussion](https:\/\/github.com\/skypyproject\/skypy\/pull\/38) option 4 has been picked. This will be easiest for developers to write new functions and write clean unit tests.n"}
{"File Name":"archcolider\/008 At least once delivery for ready to pay order.md","Context":"## Context\n\"Ready to pay\" orders should be handled with special care. We'd like to guarantee \"at least once delivery\" for each order. Order payment processing is a business-critical scenario because selling meals is the whole point. In this case, there should be a guarantee, that the order store and payment processor can pick the order for execution.\nAt the same time, it's very important to avoid double payments, because of concurrency issues. When an order with a \"Ready to pay\" state arrives, in theory, it can be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.\n","Decision":"Delivery of \"ready to pay\" orders performed by a MessageQueue software with a message acknowledgment option. Additionally, we expect that the order comes with a unique id from client devices at the time of processing. During order processing, the existence of the order with the same id can be checked and the version number should be used for staleness validation. In this case, the event with the same version will be discarded by the processing service.\\n","tokens":139,"id":2672,"text":"## Context\\n\"Ready to pay\" orders should be handled with special care. We'd like to guarantee \"at least once delivery\" for each order. Order payment processing is a business-critical scenario because selling meals is the whole point. In this case, there should be a guarantee, that the order store and payment processor can pick the order for execution.\\nAt the same time, it's very important to avoid double payments, because of concurrency issues. When an order with a \"Ready to pay\" state arrives, in theory, it can be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.\\n\n\n##Decision\nDelivery of \"ready to pay\" orders performed by a MessageQueue software with a message acknowledgment option. Additionally, we expect that the order comes with a unique id from client devices at the time of processing. During order processing, the existence of the order with the same id can be checked and the version number should be used for staleness validation. In this case, the event with the same version will be discarded by the processing service.\\n","Predictions":"* We expect that the order will be delivered at least once. Additionally, we expect that the order will be processed once for each order. Additionally, we expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one or more times, but it should not lead to doubled, tripled, and so on charges from a user's account.n* We expect that the order will be processed one"}
{"File Name":"android-guidelines\/0009-gradle-rules.md","Context":"## Context and Problem Statement\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\n","Decision":"* Disable generating BuildConfig file if its not needed in module.\\n* Only enable *databinding* if you're going to use DataBinding in that module.\\n* Do not apply *kapt* plugin if you're not going to use.\\n* Do not create new variants other than *debug* and *release*.\\n* Use groovy scripts on *build.gradle* files.\\n","tokens":37,"id":3860,"text":"## Context and Problem Statement\\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\\n\n\n##Decision\n* Disable generating BuildConfig file if its not needed in module.\\n* Only enable *databinding* if you're going to use DataBinding in that module.\\n* Do not apply *kapt* plugin if you're not going to use.\\n* Do not create new variants other than *debug* and *release*.\\n* Use groovy scripts on *build.gradle* files.\\n","Predictions":"* Use a semantic versioning schema for major and minor version numbersn* Use a major.minor.patch schema for minor version numbersn* Use a patch schema for backwards compatible version numbersn"}
{"File Name":"verify-frontend\/0004-journey-hint-tracking-cookie.md","Context":"## Context\nThe encrypted `verify-front-journey-hint` cookie was originally created to enable the non-repudiation journey.\nWhen user selected an IDP, the entity_id was stored in the cookie and if the RP then sent a new Authn request\nwith a parameter `journey_hint=submission_confirmation` the `\/confirm-your-identity` page was rendered with the\nIDP from the cookie. The cookie was a simple JSON object:\n```\n{\nentity_id: \"https:\/\/idp-entity-id.com\"\n}\n```\nIn the early 2018 we introduced a sign-in hint to help users remind what IDP they used previously. The cookie has\nbeen repurposed and new properties were added to it to track the IDP for each status the user encountered.\nThe expiry date was also extended to 18 months. For example:\n```\n{\nentity_id: \"https:\/\/idp-entity-id.com\",\nATTEMPT: \"https:\/\/idp-entity-id.com\",\nSUCCESS: \"https:\/\/idp-entity-id.com\",\nFAILED: \"https:\/\/idp-entity-id-1.com\",\n...\n}\n```\nIf the user has any value in SUCCESS we show the user the sign-in hint for that IDP.\n","Decision":"While implementing a new Pause & Resume functionality, we came across a requirement when we needed to store the status,\\nthe selected IDP and also the RP user has paused with. Rather than creating a new cookie it was decided to re-factor the current\\njourney-hint cookie to support this. In order to know what the latest state was, a new `STATE` object was introduced in the schema.\\nThe `STATE` gets updated with every new Authn response from an IDP. The `ATTEMPT` and `SUCCESS` was kept to keep the cookie backwards\\ncompatible and to help with identifying if there ever was a successful verification. The `entity_id` property got removed and the\\nnon-repudiation journey now uses the `ATTEMPT` value. The cookie schema now looks like this:\\n```\\n{\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nSTATE:  {\\nIDP: \"https:\/\/idp-entity-id.com\",\\nRP: \"https:\/\/rp-entity-id.com\",\\nSTATUS: <SUCCESS | FAILED | FAILED_UPLIFT | CANCEL | PENDING>\\n}\\n}\\n```\\n","tokens":273,"id":1449,"text":"## Context\\nThe encrypted `verify-front-journey-hint` cookie was originally created to enable the non-repudiation journey.\\nWhen user selected an IDP, the entity_id was stored in the cookie and if the RP then sent a new Authn request\\nwith a parameter `journey_hint=submission_confirmation` the `\/confirm-your-identity` page was rendered with the\\nIDP from the cookie. The cookie was a simple JSON object:\\n```\\n{\\nentity_id: \"https:\/\/idp-entity-id.com\"\\n}\\n```\\nIn the early 2018 we introduced a sign-in hint to help users remind what IDP they used previously. The cookie has\\nbeen repurposed and new properties were added to it to track the IDP for each status the user encountered.\\nThe expiry date was also extended to 18 months. For example:\\n```\\n{\\nentity_id: \"https:\/\/idp-entity-id.com\",\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nFAILED: \"https:\/\/idp-entity-id-1.com\",\\n...\\n}\\n```\\nIf the user has any value in SUCCESS we show the user the sign-in hint for that IDP.\\n\n\n##Decision\nWhile implementing a new Pause & Resume functionality, we came across a requirement when we needed to store the status,\\nthe selected IDP and also the RP user has paused with. Rather than creating a new cookie it was decided to re-factor the current\\njourney-hint cookie to support this. In order to know what the latest state was, a new `STATE` object was introduced in the schema.\\nThe `STATE` gets updated with every new Authn response from an IDP. The `ATTEMPT` and `SUCCESS` was kept to keep the cookie backwards\\ncompatible and to help with identifying if there ever was a successful verification. The `entity_id` property got removed and the\\nnon-repudiation journey now uses the `ATTEMPT` value. The cookie schema now looks like this:\\n```\\n{\\nATTEMPT: \"https:\/\/idp-entity-id.com\",\\nSUCCESS: \"https:\/\/idp-entity-id.com\",\\nSTATE:  {\\nIDP: \"https:\/\/idp-entity-id.com\",\\nRP: \"https:\/\/rp-entity-id.com\",\\nSTATUS: <SUCCESS | FAILED | FAILED_UPLIFT | CANCEL | PENDING>\\n}\\n}\\n```\\n","Predictions":"We have implemented a new State object that is used to store the state of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP of the IDP.nThe State is a JSON object that contains the IDP.nThe State is a JSON object that contains the IDP.nThe State is a JSON object that contains the IDP.nThe State is a JSON object that contains the IDP.nThe State is a JSON object that contains the IDP.nThe State is a JSON object that contains the"}
{"File Name":"gp-redirect\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4401,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"dapr\/SDK-002-java-jdk-versions.md","Context":"## Context\nDapr offers a Java SDK. Java 11 is the latest LTS version. Java 8 is the previous LTS version but still the mainly used version by the Java community in 2019. What should be the minimum Java version supported by Dapr's Java SDK?\nSee https:\/\/github.com\/dapr\/java-sdk\/issues\/17\n","Decision":"* Java 8 should be the minimum version supported for Dapr's Java SDK.\\n* Java 11 should be used in samples and user documentation to encourage adoption.\\n* Java 8's commercial support ends in 2022. Dapr's Java SDK shoud migrate to Java 11 prior to that. The timeline still not decided.\\n","tokens":71,"id":73,"text":"## Context\\nDapr offers a Java SDK. Java 11 is the latest LTS version. Java 8 is the previous LTS version but still the mainly used version by the Java community in 2019. What should be the minimum Java version supported by Dapr's Java SDK?\\nSee https:\/\/github.com\/dapr\/java-sdk\/issues\/17\\n\n\n##Decision\n* Java 8 should be the minimum version supported for Dapr's Java SDK.\\n* Java 11 should be used in samples and user documentation to encourage adoption.\\n* Java 8's commercial support ends in 2022. Dapr's Java SDK shoud migrate to Java 11 prior to that. The timeline still not decided.\\n","Predictions":"* Java 8 should be the minimum version supported for Dapr's Java SDK.n* Java 11 should be used in samples and user documentation to encourage adoption.n* Java 8's commercial support ends in 2022. Dapr's Java SDK shoud migrate to Java 11 prior to that. The timeline still not decided.n"}
{"File Name":"hodder\/0002-use-docker-compose-v-2.md","Context":"## Context\nPeople are very used to traditional layered architectures and request\/response cycles which disguise a lot of the complexity of distributed systems. They also restrict extending systems as data is locked down in a specific form in specific systems. We want to give an alternative view with a clear demonstration of the trade-offs involved.\nGetting people to give it an initial attempt is going to be hard, so it needs to be super quick to get working on a local machine.\n","Decision":"We will use a docker-compose.yml (v.2) as the primary entry point of the project\\n","tokens":92,"id":311,"text":"## Context\\nPeople are very used to traditional layered architectures and request\/response cycles which disguise a lot of the complexity of distributed systems. They also restrict extending systems as data is locked down in a specific form in specific systems. We want to give an alternative view with a clear demonstration of the trade-offs involved.\\nGetting people to give it an initial attempt is going to be hard, so it needs to be super quick to get working on a local machine.\\n\n\n##Decision\nWe will use a docker-compose.yml (v.2) as the primary entry point of the project\\n","Predictions":"We will use a docker-compose.yml (v.2) as the primary entry point of the projectn"}
{"File Name":"link_platform\/0016-use-devise-for-admin-authentication.md","Context":"## Context\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\n","Decision":"[Devise](https:\/\/github.com\/plataformatec\/devise#starting-with-rails) is a very popular gem that integrates well with ActiveRecord.  It provides support for [a ridiculous amount of authentication providers](https:\/\/github.com\/omniauth\/omniauth\/wiki\/List-of-Strategies) through Omniauth as well as a variety of features such as password reset.\\n","tokens":37,"id":5024,"text":"## Context\\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\\n\n\n##Decision\n[Devise](https:\/\/github.com\/plataformatec\/devise#starting-with-rails) is a very popular gem that integrates well with ActiveRecord.  It provides support for [a ridiculous amount of authentication providers](https:\/\/github.com\/omniauth\/omniauth\/wiki\/List-of-Strategies) through Omniauth as well as a variety of features such as password reset.\\n","Predictions":"We will use a StubVerify service to provide a user journey that resembles a Verify journey.n"}
{"File Name":"content-publisher\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4886,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"cf-k8s-networking\/0016-job-for-upgrading-istio-sidecars-on-workloads.md","Context":"## Context\nIstio's service mesh capabilites are facilitated via sidecars injected into\nworkload pods. These sidecars run an Istio-patched version of Envoy that is tied\nto the version of Istio that injects them.\nTypically when new versions of Istio are released, new versions of the sidecars\nare released as well. Istio has been good so far about supporting older versions\nof sidecars that were deployed before Istio was upgraded, but it is still\n[documented best practice](https:\/\/istio.io\/latest\/docs\/setup\/upgrade\/) to roll\nall the pods after an Istio upgrade.\nAs an additional constraint, the operators of cf-for-k8s clusters expect to be\nable to perform upgrades in one `kapp deploy`, with no post-install hooks or\nother bash scripts. This limits our options considerably. See this [Slack\nthread](https:\/\/cloudfoundry.slack.com\/archives\/CH9LF6V1P\/p1592521879117400) on\nthat constraint.\n","Decision":"We will use the kubernetes\\n[Job](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/job\/) resource\\nto run the kubectl command needed to roll workload pods, after waiting for the\\nnew Istio control plane to be up and healthy.\\nTo that end, we will add the necessary minimal `ServiceAccounts` and `Roles`\\nneeded to list resources in the `istio-system` namespace, and restart resources\\nin the configured workload namespace. We will also build and maintain a\\ncontainer image that contains the Job's logic.\\nAll istio components will be tagged with their Istio version so that the job can\\npositively determine that the correct version of control plane components are\\nalive and healthy. We will also name the job according to it's Istio version, so\\nthat we can take advantage of `Jobs` inherent immutability in cases where a\\ncf-for-k8s upgrade does not contain a new Istio version (pushing the same job\\nagain will not cause it to rerun, preventing workloads from rolling\\nunnecessarily). Subsequent jobs will clean up previous ones.\\n","tokens":224,"id":2240,"text":"## Context\\nIstio's service mesh capabilites are facilitated via sidecars injected into\\nworkload pods. These sidecars run an Istio-patched version of Envoy that is tied\\nto the version of Istio that injects them.\\nTypically when new versions of Istio are released, new versions of the sidecars\\nare released as well. Istio has been good so far about supporting older versions\\nof sidecars that were deployed before Istio was upgraded, but it is still\\n[documented best practice](https:\/\/istio.io\/latest\/docs\/setup\/upgrade\/) to roll\\nall the pods after an Istio upgrade.\\nAs an additional constraint, the operators of cf-for-k8s clusters expect to be\\nable to perform upgrades in one `kapp deploy`, with no post-install hooks or\\nother bash scripts. This limits our options considerably. See this [Slack\\nthread](https:\/\/cloudfoundry.slack.com\/archives\/CH9LF6V1P\/p1592521879117400) on\\nthat constraint.\\n\n\n##Decision\nWe will use the kubernetes\\n[Job](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/job\/) resource\\nto run the kubectl command needed to roll workload pods, after waiting for the\\nnew Istio control plane to be up and healthy.\\nTo that end, we will add the necessary minimal `ServiceAccounts` and `Roles`\\nneeded to list resources in the `istio-system` namespace, and restart resources\\nin the configured workload namespace. We will also build and maintain a\\ncontainer image that contains the Job's logic.\\nAll istio components will be tagged with their Istio version so that the job can\\npositively determine that the correct version of control plane components are\\nalive and healthy. We will also name the job according to it's Istio version, so\\nthat we can take advantage of `Jobs` inherent immutability in cases where a\\ncf-for-k8s upgrade does not contain a new Istio version (pushing the same job\\nagain will not cause it to rerun, preventing workloads from rolling\\nunnecessarily). Subsequent jobs will clean up previous ones.\\n","Predictions":"We will use the kubernetesn[Job](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/job\/) resourcento run the kubectl command needed to roll workload pods, after waiting for thennew Istio control plane to be up and healthy.nTo that end, we will add the necessary minimal ServiceAccounts and Rolesnneeded to list resources in the istio-system namespace, and restart resourcesnin the configured workload namespace. We will also build and maintain ancontainer image that contains the Job's logic.nAll istio components will be tagged with their Istio version so that the job cannpositively determine that the correct version of control plane components arenalive and healthy. We will also name the job according to it's Istio version.nWe will also name the job according to its Istio version so thatnpositively determine that the correct version of control plane components arenalive and healthy. We will also name the job according to it's Istio version.n"}
{"File Name":"bananatabs\/0002-react-testing-library.md","Context":"## Context\nWe want to avoid testing implementation details in our integration tests.\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\ne.g. toggling the visibility of different tabs and window groups.\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\nIt could be more sensible to only verify the state\/model.\n","Decision":"We will assert against the session in the provider which is accessible through the context. (the BananaContext instance, not a React Context)\\n","tokens":111,"id":2816,"text":"## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state\/model.\\n\n\n##Decision\nWe will assert against the session in the provider which is accessible through the context. (the BananaContext instance, not a React Context)\\n","Predictions":"We will assert against the session in the provider which is accessible through the context. (the BananaContext instance, not a React Context)n"}
{"File Name":"blueprint\/0003-feature-selection-policy.md","Context":"## Context and Problem Statement\nOpen Data Hub has release v0.8.0, some of the Elyra features required by Thoth Station experiments are\nnot part of this ODH release. This would require to update certain components to the HEAD of main branch of ODH upstream\nproject.\n## Decision Drivers\n* Opertational complexity of an environment diverging from an upstream release\n* User needs of more current software components\n","Decision":"* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\nChosen option: \"deploy specific versions of components\", because this will give the most efficient deployment to\\nOperate First operators and users.\\n### Positive Consequences\\n* operators can gain a maximum of experience, enabling feedback on component versions that might have not been tested\\nby the upstream project\\n* users get the feature set they need to get the most out of an Operate First deployment\\n### Negative Consequences\\n* additional deployment\/manifest customizations that are not valuable to upstream project, as they are out of scope\\nfor them\\n<!-- markdownlint-disable-file MD013 -->\\n","tokens":90,"id":3551,"text":"## Context and Problem Statement\\nOpen Data Hub has release v0.8.0, some of the Elyra features required by Thoth Station experiments are\\nnot part of this ODH release. This would require to update certain components to the HEAD of main branch of ODH upstream\\nproject.\\n## Decision Drivers\\n* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\n\n\n##Decision\n* Opertational complexity of an environment diverging from an upstream release\\n* User needs of more current software components\\nChosen option: \"deploy specific versions of components\", because this will give the most efficient deployment to\\nOperate First operators and users.\\n### Positive Consequences\\n* operators can gain a maximum of experience, enabling feedback on component versions that might have not been tested\\nby the upstream project\\n* users get the feature set they need to get the most out of an Operate First deployment\\n### Negative Consequences\\n* additional deployment\/manifest customizations that are not valuable to upstream project, as they are out of scope\\nfor them\\n<!-- markdownlint-disable-file MD013 -->\\n","Predictions":"* Opertational complexity of an environment diverging from an upstream releasen* User needs of more current software componentsnChosen option: \"deploy specific versions of components\", because this will give the most efficient deployment tonOperate First operators and users.n### Positive Consequencesn* operators can gain a maximum of experience, enabling feedback on component versions that might have not been testednby the upstream projectn* users get the feature set they need to get the most out of an Operate First deploymentn### Negative Consequencesn* additional deployment\/manifest customizations that are not valuable to upstream project, as they are out of scopenfor themn!-- markdownlint-disable-file MD013 -->n"}
{"File Name":"atlasdb\/0009-load-and-read-streams-in-same-transaction.md","Context":"## Context\nReading streams from within the same transaction that calls loadStream() must\nbe supported. There are a number of benefits to doing this rather than reading\nstreams outside the transaction.\n1. If the transaction that calls loadStream() is run with retry, you risk\nleaking resources because multiple streams may be created that never get\nreturned to the caller (so the caller has no opportunity to close them).\nThe stream can not only be read (entirely) inside the transaction, but\nalso closed inside the transaction. This makes it safe to run with retries\n(since the retries can also close the stream they load).\n2. When the actual stream data is read outside of the transaction, that read\ncan fail (for example if the stream was then deleted). If instead the read is\nperformed inside the transaction, any failed reads would also cause the\ntransaction to fail, which could be desired. For example, if as a result of\nreading the stream you want to perform some writes based on the read data, it\nmakes sense to perform all of that in a single transaction.\n","Decision":"Reading from a stream inside the same transaction that calls loadStream()\\nshould be the preferred method unless you have a specific reason that the reads\\nmust be performed later outside the transaction. Reading from a stream should\\nbe supported both inside the same transaction where it was loaded and outside\\n(after the transaction commits or aborts).\\n","tokens":229,"id":3099,"text":"## Context\\nReading streams from within the same transaction that calls loadStream() must\\nbe supported. There are a number of benefits to doing this rather than reading\\nstreams outside the transaction.\\n1. If the transaction that calls loadStream() is run with retry, you risk\\nleaking resources because multiple streams may be created that never get\\nreturned to the caller (so the caller has no opportunity to close them).\\nThe stream can not only be read (entirely) inside the transaction, but\\nalso closed inside the transaction. This makes it safe to run with retries\\n(since the retries can also close the stream they load).\\n2. When the actual stream data is read outside of the transaction, that read\\ncan fail (for example if the stream was then deleted). If instead the read is\\nperformed inside the transaction, any failed reads would also cause the\\ntransaction to fail, which could be desired. For example, if as a result of\\nreading the stream you want to perform some writes based on the read data, it\\nmakes sense to perform all of that in a single transaction.\\n\n\n##Decision\nReading from a stream inside the same transaction that calls loadStream()\\nshould be the preferred method unless you have a specific reason that the reads\\nmust be performed later outside the transaction. Reading from a stream should\\nbe supported both inside the same transaction where it was loaded and outside\\n(after the transaction commits or aborts).\\n","Predictions":"The interface interface should be a single interface, and should be able to communicate with the processor and the interpreter in a single way.n"}
{"File Name":"dotcom-rendering\/009-static-types.md","Context":"## Context\nAdding static types to a JavaScript project can be incredibly useful to reduce the likelihood of introducing runtime type errors.\nWe have considered two methods of adding static types to dotcom-rendering.\n### Flow\nFlow makes it easy to retrofit a useful level of implicit type checking to a legacy project.\nHowever, it produces error messages that are difficult to understand.\nFlow requires developers to check in types for 3rd party libraries. These can be automatically generated based on the `package.json`, but developers must remember to keep these up to date.\nIts configuration is often too coarse-grained, meaning code gets littered with opt-outs and `any` types. It doesn't feel like a natural type system, as it is only annotations on top of a dynamic language.\nFlow's types are not exact by default.\nThe ecosystem is still immature.\n### TypeScript\nTypeScript produces more understandable error messages.\nIt allows developers to install versioned types for third party libraries.\nTypeScript is a language in its own right, and feels more natural. It can improve JavaScript language features to make them more type-friendly. Its configuration is more fine-grained.\nTypes are exact by default.\nThe ecosystem is more mature.\n","Decision":"Use TypeScript for static typing.\\n","tokens":252,"id":2652,"text":"## Context\\nAdding static types to a JavaScript project can be incredibly useful to reduce the likelihood of introducing runtime type errors.\\nWe have considered two methods of adding static types to dotcom-rendering.\\n### Flow\\nFlow makes it easy to retrofit a useful level of implicit type checking to a legacy project.\\nHowever, it produces error messages that are difficult to understand.\\nFlow requires developers to check in types for 3rd party libraries. These can be automatically generated based on the `package.json`, but developers must remember to keep these up to date.\\nIts configuration is often too coarse-grained, meaning code gets littered with opt-outs and `any` types. It doesn't feel like a natural type system, as it is only annotations on top of a dynamic language.\\nFlow's types are not exact by default.\\nThe ecosystem is still immature.\\n### TypeScript\\nTypeScript produces more understandable error messages.\\nIt allows developers to install versioned types for third party libraries.\\nTypeScript is a language in its own right, and feels more natural. It can improve JavaScript language features to make them more type-friendly. Its configuration is more fine-grained.\\nTypes are exact by default.\\nThe ecosystem is more mature.\\n\n\n##Decision\nUse TypeScript for static typing.\\n","Predictions":"Use TypeScript for static typing.n"}
{"File Name":"csw-backend\/0002-not-cloud-agnostic.md","Context":"## Context\nSince Cloud Security Watch specifically aims to monitor for\nmisconfigurations in AWS accounts it does not make sense to\nmake the tool cloud agnositic.\n","Decision":"Whilst we support the notion of writing cloud agnostic\\ncode in general. In this instance it is not appropriate\\nor desirable.\\n","tokens":36,"id":419,"text":"## Context\\nSince Cloud Security Watch specifically aims to monitor for\\nmisconfigurations in AWS accounts it does not make sense to\\nmake the tool cloud agnositic.\\n\n\n##Decision\nWhilst we support the notion of writing cloud agnostic\\ncode in general. In this instance it is not appropriate\\nor desirable.\\n","Predictions":"We will use [SecurityWatch](https:\/\/securitywatch.io\/) to provide a way to manage the security of our applications.n"}
{"File Name":"hmpps-interventions-ui\/0006-use-jest-for-testing.md","Context":"## Context\nWe want a test framework that has good support for TypeScript and Node. Jest is\na fast testing framework with good resources for mocking.\n","Decision":"We will use Jest as our testing framework.\\n","tokens":31,"id":452,"text":"## Context\\nWe want a test framework that has good support for TypeScript and Node. Jest is\\na fast testing framework with good resources for mocking.\\n\n\n##Decision\nWe will use Jest as our testing framework.\\n","Predictions":"We will use Jest as our testing framework.n"}
{"File Name":"content-publisher\/0009-modelling-history.md","Context":"## Context\nThe Content Publisher domain model is focused on storing current\ninformation. There is a documents table which stores the most recent content of\na document and an images table which stores the most recent version of\na document's images. History of these changes is stored using [Papertrail][],\nwhich is not [intended as a permanent store][papertrail-pr].\nThe presentation of a document's history is done via a TimelineEntry model\nwhich stores that a user did something, but lacks any further detail. In places\nwhere richer information was required there have been models such as Removal or\nWithdrawal associated with a TimelineEntry.\nThis has led to a number of pain points:\n- users cannot discard a new draft of a published document, because creating a\nnew draft overwrites the data stored for the published edition\n- Content Publisher can't show an accurate link or status for the live edition\nof a document when a new draft of a published document is created;\n- users cannot edit or remove images on a document once the first\nedition is published;\n- the TimelineEntry model stores aspects of a document's state, resulting in it\nneeding to be queried outside a timeline context which limits flexibility\nfor the timeline.\nAnd this prevents a number of intended features for Content Publisher:\n- comparing different editions of a document;\n- republishing live content if there are any problems (currently a common\nsupport task for Whitehall publisher);\n- showing users what changes a user made in a particular edit.\n","Decision":"This ADR proposes changes to the domain model to resolve the aforementioned\\npain points and provide a means to support the future intended features. These\\nchanges provide the means to store the individual editions of a document,\\neach revision of the content of a document and each status an edition has held.\\nAs per [ADR-3](0003-initial-domain-modelling.md) it does not consider the\\noption of sharing data between translations of a document as there are not\\nthe appropriate product decisions for this.\\nA common theme in this decision is\\n[immutablity in models](#approach-to-mutabilityimmutability), which is used\\nas an implicit means of storing a history. Immutability is a key consideration\\nin modelling [revisions of a document](#breakdown-of-revision) and\\n[images](#image-modelling). This ADR then considers the impacts of\\nstoring history for [timeline](#timeline) and [topics](#topics), both areas\\nwhere the usage\/need of history is less clear. Finally, this ADR concludes with\\na [collated diagram](#collated-diagram) of the domain model concepts.\\n### Core Concepts\\n![Main concepts](0009\/main-concepts-diagram.png)\\n**Document**: A record that represents all versions of a piece of content in a\\nparticular locale. It has many editions and at any time it will have a current\\nedition - shown on Content Publisher index - and potentially a live edition\\nwhich is currently on GOV.UK. The live and current edition can be\\nthe same. Each iteration of a document's content is represented as a revision\\non the current edition, thus a document has many revisions. Document is a\\nmutable entity that is used to store data common across all editions (such as\\nfirst publishing date) and it is expected to be a joining point for\\ndocument-related data that is not associated with a particular edition.\\n**Edition**: A numbered version of a document that has been, or is\\nexpected to be, published on GOV.UK. It is associated with a revision\\nand a status. It is mutable so that it can be a consistent object that\\njoins to immutable data. It is a place where any edition-level\\ndatabase constraints can be placed, such as the constraint that only one live\\nedition can exist per document. It is supported that two editions of the same\\ndocument share the same revision. This allows them to explicitly reference the\\nsame content, which supports a future ability to revert a document to past\\ncontent.\\n**Revision**: Represents an immutable snapshot of the content of a document at a\\nparticular point in time. It has a number to indicate which revision of the\\ndocument it is and stores who created it. Any request by a user that changes\\ncontent should result in a single new revision. This is to directly map the\\nconcept of a revision to each time a user revises a document. Data outside of\\ncontent, such as state, should not be stored in a revision to ensure that\\ndifferences between revisions can be represented to a user. The\\n[anatomy of a Revision model](#breakdown-of-revision) is explored further in\\nthis document.\\n**Status**: Represents a state that an edition can hold such as: \"draft\" or\\n\"submitted for review\". This model is coupled to the concept of status that is\\nshown and changed by a user. Each time a user changes the status of an edition\\na new Status model is created and the user who created it stored. An edition\\ncan only have one status at any one time. If a status has data specific to\\nthat status, such as an explanatory note for a withdrawal, this can be stored\\nin a specific model associated by a polymorphic relation. This allows for\\nmodels, such as Removal or Withdrawal, to no longer be the responsibility of\\nTimelineEntry. Initially this object is intended to be immutable, however this\\nmay be changed if status changes become asynchronous operations. This is so\\nthat a single status change performed by a user can still be represented by\\na single record.\\n### Approach to mutability\/immutability\\nA number of the models in Content Publisher are defined as immutable, most\\nsignificantly [Revision and associated models](#breakdown-of-revision). These\\nmodels should be persisted to the database once and never be updated or deleted.\\nAny need to change them requires creating a new record. This allows us to store\\na full history by only appending to the database.\\nFor simplicity, performance and consistency with Rails idioms the accessing\\nof immutable models is intended to be done by foreign key and not by the usage\\nof `SELECT MAX` style queries. This maintains the ability to use the regular\\napproach to ActiveRecord associations and the means to require the existence of\\na association (by specifying a foreign key cannot be null). An example of this\\nmodelling is the mutable Edition model which references an immutable model,\\nRevision, that stores the content. Edition is accessed by a\\nconsistent primary key and the revision accessed by a foreign key stored on\\nthe edition.\\nSince the data on a mutable model can be lost when the model is updated these\\nshould not be used for data where there is a need for history. For example, to\\nstore the statuses an edition has held there are individual status models that\\nreference the Edition. This allows an edition to reference a single status that\\nis replaced while a history is maintained.\\nThe choice of this immutability strategy is to store both present and\\nhistorical concerns in the same way, thus ensuring history remains a\\nfirst class citizen. A nice side effect of having immutable models is\\nthis opens options for caching. Since data for that\\nmodel will never change it can effectively be cached forever.\\n### Breakdown of Revision\\nAs Revision is an immutable model, used to store each edit of a Document, there\\nis likely to be a large amount of these with often only minor differences\\nbetween them. To address this a Revision is not stored as a single model but\\ninstead as a collection of models, where the Revision model stores little data\\nand joins to other models. This can be visualised as:\\n![Revision breakdown](0009\/revision-diagram.png)\\nThe intention of breaking this up is to be conservative with the amount of data\\nduplicated between consecutive revisions. For example when a user edits\\nthe title of an edition a new ContentRevision is created and the existing\\nTagsRevision, MetadataRevision and ImageRevisions models are associated with\\nthe next revision. An ImageRevision is modelled in a similar way to a Revision\\nand this is explained further in [Image modelling](#image-modelling).\\nIt is intended that [delegation][delegate] be used when interfacing with a\\nrevision so that the caller need not be concerned with which sub-revision\\nstores particular fields. This allows a revision to have a rich interface\\ndespite storing a low amount of data directly.\\n### Image modelling\\nContent Publisher supports a user uploading image files and referencing them\\nin a revision of a document. They have metadata and editable properties that a\\nuser can change, of which a history is stored. A single image file uploaded\\nproduces multiple files that are uploaded to Asset Manager for different sizing\\nvariations. Images are modelled in a similar way to Revision with an\\nimmutable Image::Revision model, as represented below:\\n![Image Revision breakdown](0009\/image-revision-diagram.png)\\nThe Image model itself is used for continuation between image revisions. It is\\nknown that two Image::Revisions are versions of the same item if they share the\\nsame Image association. The id of the Image is used in Content Publisher URLs\\nto consistently reference the Image no matter which revision it is.\\nThe data of an Image::Revision is stored between an Image::FileRevision and an\\nImage::MetadataRevision. Both are immutable and they differ by the fact that\\nany change to Image::FileRevision requires changes to the resultant Asset\\nManager files (such as crop dimensions), whereas Image::MetadataRevision stores\\naccompanying data that doesn't affect the Asset Manager files (such as alt\\ntext).\\nEach Image::FileRevision is associated with an ActiveStorage::Blob object that\\nis responsible for managing the storage of the source file. It also has a one\\nto many association with Image::Asset. Each Image::Asset represents resultant\\nfiles that are uploaded to Asset Manager for the various image sizes. The\\nImage::Asset model stores the URL to the Asset Manager file and what state the\\nfile is on Asset Manager.\\n### Timeline\\nThe TimelineEntry model represents an event that should be shown to a user as\\npart of a visual timeline of a document's history. In order for the timeline to\\nbe a flexible feature that can be iterated, this model should not be used\\noutside of the timeline context. Previously models such as Removal and\\nWithdrawal were associated directly with a TimelineEntry which\\nmeant state was accessed through the timeline. These are now suggested to be\\nassociated with a Status model.\\nAt the time of writing it wasn't yet determined what the\\ntimeline would show, and therefore it wasn't clear exactly how\\nbest to model an entry for it. Because of this TimelineEntry is modelled in a\\nspeculative way with a number of references to relevant data, including a\\npolymorphic association for flexibility.\\nThe TimelineEntry model should not store data which could not be\\nderived from other aspects of a document. This is the allow the ability to\\nrebuild TimelineEntry models if the needs of the timeline changed and to avoid\\ntimeline being an aspect of a document's state.\\n### Topics\\nNo data related to topics (otherwise known as GOV.UK taxonomy) is intended to\\nbe stored in Content Publisher at this current point in time. This is due to\\ntopics being accessed and edited by directly interacting with the Publishing\\nAPI. Other applications, notably Content Tagger, can also edit topics which has\\nthe consequence that the Publishing API is the source of truth for this data\\nrather than Content Publisher.\\nThis inconsistency makes it difficult to store the history of topics in a\\nreliable way. Thus, until needs are determined to store past topics, Content\\nPublisher will only know current topics (by querying Publishing API) and\\nthe history of topics will not be available.\\n### Collated diagram\\nThe various aspects of this decision can be collated together to be represented\\nin the following, somewhat daunting, diagram. It has been simplified for ease\\nof reading with noticeable omissions being a User model, since most models have\\na creator, and the speculative joins of a TimelineEntry.\\n![Domain model diagram](0009\/domain-model-diagram.png)\\n","tokens":317,"id":4875,"text":"## Context\\nThe Content Publisher domain model is focused on storing current\\ninformation. There is a documents table which stores the most recent content of\\na document and an images table which stores the most recent version of\\na document's images. History of these changes is stored using [Papertrail][],\\nwhich is not [intended as a permanent store][papertrail-pr].\\nThe presentation of a document's history is done via a TimelineEntry model\\nwhich stores that a user did something, but lacks any further detail. In places\\nwhere richer information was required there have been models such as Removal or\\nWithdrawal associated with a TimelineEntry.\\nThis has led to a number of pain points:\\n- users cannot discard a new draft of a published document, because creating a\\nnew draft overwrites the data stored for the published edition\\n- Content Publisher can't show an accurate link or status for the live edition\\nof a document when a new draft of a published document is created;\\n- users cannot edit or remove images on a document once the first\\nedition is published;\\n- the TimelineEntry model stores aspects of a document's state, resulting in it\\nneeding to be queried outside a timeline context which limits flexibility\\nfor the timeline.\\nAnd this prevents a number of intended features for Content Publisher:\\n- comparing different editions of a document;\\n- republishing live content if there are any problems (currently a common\\nsupport task for Whitehall publisher);\\n- showing users what changes a user made in a particular edit.\\n\n\n##Decision\nThis ADR proposes changes to the domain model to resolve the aforementioned\\npain points and provide a means to support the future intended features. These\\nchanges provide the means to store the individual editions of a document,\\neach revision of the content of a document and each status an edition has held.\\nAs per [ADR-3](0003-initial-domain-modelling.md) it does not consider the\\noption of sharing data between translations of a document as there are not\\nthe appropriate product decisions for this.\\nA common theme in this decision is\\n[immutablity in models](#approach-to-mutabilityimmutability), which is used\\nas an implicit means of storing a history. Immutability is a key consideration\\nin modelling [revisions of a document](#breakdown-of-revision) and\\n[images](#image-modelling). This ADR then considers the impacts of\\nstoring history for [timeline](#timeline) and [topics](#topics), both areas\\nwhere the usage\/need of history is less clear. Finally, this ADR concludes with\\na [collated diagram](#collated-diagram) of the domain model concepts.\\n### Core Concepts\\n![Main concepts](0009\/main-concepts-diagram.png)\\n**Document**: A record that represents all versions of a piece of content in a\\nparticular locale. It has many editions and at any time it will have a current\\nedition - shown on Content Publisher index - and potentially a live edition\\nwhich is currently on GOV.UK. The live and current edition can be\\nthe same. Each iteration of a document's content is represented as a revision\\non the current edition, thus a document has many revisions. Document is a\\nmutable entity that is used to store data common across all editions (such as\\nfirst publishing date) and it is expected to be a joining point for\\ndocument-related data that is not associated with a particular edition.\\n**Edition**: A numbered version of a document that has been, or is\\nexpected to be, published on GOV.UK. It is associated with a revision\\nand a status. It is mutable so that it can be a consistent object that\\njoins to immutable data. It is a place where any edition-level\\ndatabase constraints can be placed, such as the constraint that only one live\\nedition can exist per document. It is supported that two editions of the same\\ndocument share the same revision. This allows them to explicitly reference the\\nsame content, which supports a future ability to revert a document to past\\ncontent.\\n**Revision**: Represents an immutable snapshot of the content of a document at a\\nparticular point in time. It has a number to indicate which revision of the\\ndocument it is and stores who created it. Any request by a user that changes\\ncontent should result in a single new revision. This is to directly map the\\nconcept of a revision to each time a user revises a document. Data outside of\\ncontent, such as state, should not be stored in a revision to ensure that\\ndifferences between revisions can be represented to a user. The\\n[anatomy of a Revision model](#breakdown-of-revision) is explored further in\\nthis document.\\n**Status**: Represents a state that an edition can hold such as: \"draft\" or\\n\"submitted for review\". This model is coupled to the concept of status that is\\nshown and changed by a user. Each time a user changes the status of an edition\\na new Status model is created and the user who created it stored. An edition\\ncan only have one status at any one time. If a status has data specific to\\nthat status, such as an explanatory note for a withdrawal, this can be stored\\nin a specific model associated by a polymorphic relation. This allows for\\nmodels, such as Removal or Withdrawal, to no longer be the responsibility of\\nTimelineEntry. Initially this object is intended to be immutable, however this\\nmay be changed if status changes become asynchronous operations. This is so\\nthat a single status change performed by a user can still be represented by\\na single record.\\n### Approach to mutability\/immutability\\nA number of the models in Content Publisher are defined as immutable, most\\nsignificantly [Revision and associated models](#breakdown-of-revision). These\\nmodels should be persisted to the database once and never be updated or deleted.\\nAny need to change them requires creating a new record. This allows us to store\\na full history by only appending to the database.\\nFor simplicity, performance and consistency with Rails idioms the accessing\\nof immutable models is intended to be done by foreign key and not by the usage\\nof `SELECT MAX` style queries. This maintains the ability to use the regular\\napproach to ActiveRecord associations and the means to require the existence of\\na association (by specifying a foreign key cannot be null). An example of this\\nmodelling is the mutable Edition model which references an immutable model,\\nRevision, that stores the content. Edition is accessed by a\\nconsistent primary key and the revision accessed by a foreign key stored on\\nthe edition.\\nSince the data on a mutable model can be lost when the model is updated these\\nshould not be used for data where there is a need for history. For example, to\\nstore the statuses an edition has held there are individual status models that\\nreference the Edition. This allows an edition to reference a single status that\\nis replaced while a history is maintained.\\nThe choice of this immutability strategy is to store both present and\\nhistorical concerns in the same way, thus ensuring history remains a\\nfirst class citizen. A nice side effect of having immutable models is\\nthis opens options for caching. Since data for that\\nmodel will never change it can effectively be cached forever.\\n### Breakdown of Revision\\nAs Revision is an immutable model, used to store each edit of a Document, there\\nis likely to be a large amount of these with often only minor differences\\nbetween them. To address this a Revision is not stored as a single model but\\ninstead as a collection of models, where the Revision model stores little data\\nand joins to other models. This can be visualised as:\\n![Revision breakdown](0009\/revision-diagram.png)\\nThe intention of breaking this up is to be conservative with the amount of data\\nduplicated between consecutive revisions. For example when a user edits\\nthe title of an edition a new ContentRevision is created and the existing\\nTagsRevision, MetadataRevision and ImageRevisions models are associated with\\nthe next revision. An ImageRevision is modelled in a similar way to a Revision\\nand this is explained further in [Image modelling](#image-modelling).\\nIt is intended that [delegation][delegate] be used when interfacing with a\\nrevision so that the caller need not be concerned with which sub-revision\\nstores particular fields. This allows a revision to have a rich interface\\ndespite storing a low amount of data directly.\\n### Image modelling\\nContent Publisher supports a user uploading image files and referencing them\\nin a revision of a document. They have metadata and editable properties that a\\nuser can change, of which a history is stored. A single image file uploaded\\nproduces multiple files that are uploaded to Asset Manager for different sizing\\nvariations. Images are modelled in a similar way to Revision with an\\nimmutable Image::Revision model, as represented below:\\n![Image Revision breakdown](0009\/image-revision-diagram.png)\\nThe Image model itself is used for continuation between image revisions. It is\\nknown that two Image::Revisions are versions of the same item if they share the\\nsame Image association. The id of the Image is used in Content Publisher URLs\\nto consistently reference the Image no matter which revision it is.\\nThe data of an Image::Revision is stored between an Image::FileRevision and an\\nImage::MetadataRevision. Both are immutable and they differ by the fact that\\nany change to Image::FileRevision requires changes to the resultant Asset\\nManager files (such as crop dimensions), whereas Image::MetadataRevision stores\\naccompanying data that doesn't affect the Asset Manager files (such as alt\\ntext).\\nEach Image::FileRevision is associated with an ActiveStorage::Blob object that\\nis responsible for managing the storage of the source file. It also has a one\\nto many association with Image::Asset. Each Image::Asset represents resultant\\nfiles that are uploaded to Asset Manager for the various image sizes. The\\nImage::Asset model stores the URL to the Asset Manager file and what state the\\nfile is on Asset Manager.\\n### Timeline\\nThe TimelineEntry model represents an event that should be shown to a user as\\npart of a visual timeline of a document's history. In order for the timeline to\\nbe a flexible feature that can be iterated, this model should not be used\\noutside of the timeline context. Previously models such as Removal and\\nWithdrawal were associated directly with a TimelineEntry which\\nmeant state was accessed through the timeline. These are now suggested to be\\nassociated with a Status model.\\nAt the time of writing it wasn't yet determined what the\\ntimeline would show, and therefore it wasn't clear exactly how\\nbest to model an entry for it. Because of this TimelineEntry is modelled in a\\nspeculative way with a number of references to relevant data, including a\\npolymorphic association for flexibility.\\nThe TimelineEntry model should not store data which could not be\\nderived from other aspects of a document. This is the allow the ability to\\nrebuild TimelineEntry models if the needs of the timeline changed and to avoid\\ntimeline being an aspect of a document's state.\\n### Topics\\nNo data related to topics (otherwise known as GOV.UK taxonomy) is intended to\\nbe stored in Content Publisher at this current point in time. This is due to\\ntopics being accessed and edited by directly interacting with the Publishing\\nAPI. Other applications, notably Content Tagger, can also edit topics which has\\nthe consequence that the Publishing API is the source of truth for this data\\nrather than Content Publisher.\\nThis inconsistency makes it difficult to store the history of topics in a\\nreliable way. Thus, until needs are determined to store past topics, Content\\nPublisher will only know current topics (by querying Publishing API) and\\nthe history of topics will not be available.\\n### Collated diagram\\nThe various aspects of this decision can be collated together to be represented\\nin the following, somewhat daunting, diagram. It has been simplified for ease\\nof reading with noticeable omissions being a User model, since most models have\\na creator, and the speculative joins of a TimelineEntry.\\n![Domain model diagram](0009\/domain-model-diagram.png)\\n","Predictions":"The editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most recent version of a document.nThe editor table is a table that stores the most"}
{"File Name":"datalab\/0009-bastion-for-cluster-access.md","Context":"## Context\nIt will be necessary to access the servers that form the Datalabs network but we do not\nwant to expose any services outside of the JASMIN tenancy that we do not have to.\n","Decision":"We have decided that all access to the cluster will be via a Bastion server over an SSH\\nconnection on port 22. We will restrict access through the firewall to known IP address\\nranges including the development workstations, the STFC VPN and the Tessella public IP\\naddress.\\nThis excludes public facing services that should be available over HTTPS on port 443 via\\na different route.\\n","tokens":44,"id":745,"text":"## Context\\nIt will be necessary to access the servers that form the Datalabs network but we do not\\nwant to expose any services outside of the JASMIN tenancy that we do not have to.\\n\n\n##Decision\nWe have decided that all access to the cluster will be via a Bastion server over an SSH\\nconnection on port 22. We will restrict access through the firewall to known IP address\\nranges including the development workstations, the STFC VPN and the Tessella public IP\\naddress.\\nThis excludes public facing services that should be available over HTTPS on port 443 via\\na different route.\\n","Predictions":"We have decided that all access to the cluster will be via a Bastion server over an SSHnconnection on port 22. We will restrict access through the firewall to known IP addressnranges including the development workstations, the STFC VPN and the Tessella public IPnaddress.nThis excludes public facing services that should be available over HTTPS on port 443 viana different route.n"}
{"File Name":"opg-use-an-lpa\/0007-split-terraform-configuration.md","Context":"## Context\nEngineers in the team want to have their code changes built as real environments in the Development AWS account so that they can have their work evaluated.\nEvaluation includes automated and user acceptance testing.\nWhen evaluation is finished it should be possible to destroy the environment, while retaining important artifacts and data (logs for example).\nIt will be possible to host multiple PR environments in the Development AWS account\n","Decision":"### Terraform Configuration\\nWe will identify and manage separately, resources at an Account level and resources at an Environment level.\\nAccount level resources, such as S3 buckets for logs, the default VPC and networking will be used Environment level resources, such as ECS clusters, DNS routes and Load Balancers.\\nThis will enable us to use Terraform Workspaces to contain an Environment completely for it to be built and destroyed simply.\\nAccount level resources will remain in place after an Environment is destroyed.\\n![split terraform configuration diagram](..\/diagrams\/Use-An-LPA_split_Terraform_configurations.png)\\n### Name Spacing\\nWe will use the github PR number and the first few characters of the branch name to namespace the environment and it's resources\\nFor example, DNS records will be created for each environment using the namespace `https:\/\/view.025-UML-93.use-an-lpa.opg.service.justice.gov.uk`\\nwhile an ECS cluster will be named `025-UML-93-use-an-lpa`\\n### Environment lifecycle\\nAn Environment will be built when a PR is opened\\nOnce evaluation is complete, the Envirnoment can be destroyed.\\nTo begin with the pipeline will print instructions for destroying an Environment.\\nLater, destroying an Environment will be automated and scheduled with a maximum life span of a day or two.\\n### CI\/CD Pipeline\\nThe Terraform steps in the pipeline will always first provision the Account level shared resources, then provision the Environment level resources.\\n![pipeline terraform steps](..\/diagrams\/Use-An-LPA_Terraform_pipeline.png)\\n","tokens":82,"id":4847,"text":"## Context\\nEngineers in the team want to have their code changes built as real environments in the Development AWS account so that they can have their work evaluated.\\nEvaluation includes automated and user acceptance testing.\\nWhen evaluation is finished it should be possible to destroy the environment, while retaining important artifacts and data (logs for example).\\nIt will be possible to host multiple PR environments in the Development AWS account\\n\n\n##Decision\n### Terraform Configuration\\nWe will identify and manage separately, resources at an Account level and resources at an Environment level.\\nAccount level resources, such as S3 buckets for logs, the default VPC and networking will be used Environment level resources, such as ECS clusters, DNS routes and Load Balancers.\\nThis will enable us to use Terraform Workspaces to contain an Environment completely for it to be built and destroyed simply.\\nAccount level resources will remain in place after an Environment is destroyed.\\n![split terraform configuration diagram](..\/diagrams\/Use-An-LPA_split_Terraform_configurations.png)\\n### Name Spacing\\nWe will use the github PR number and the first few characters of the branch name to namespace the environment and it's resources\\nFor example, DNS records will be created for each environment using the namespace `https:\/\/view.025-UML-93.use-an-lpa.opg.service.justice.gov.uk`\\nwhile an ECS cluster will be named `025-UML-93-use-an-lpa`\\n### Environment lifecycle\\nAn Environment will be built when a PR is opened\\nOnce evaluation is complete, the Envirnoment can be destroyed.\\nTo begin with the pipeline will print instructions for destroying an Environment.\\nLater, destroying an Environment will be automated and scheduled with a maximum life span of a day or two.\\n### CI\/CD Pipeline\\nThe Terraform steps in the pipeline will always first provision the Account level shared resources, then provision the Environment level resources.\\n![pipeline terraform steps](..\/diagrams\/Use-An-LPA_Terraform_pipeline.png)\\n","Predictions":"* We will use the Security API to provide a way to manage the resources of the PR environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n* We will use the Security API to provide a way to manage the resources of the Environment.n*"}
{"File Name":"james\/0002-make-taskmanager-distributed.md","Context":"## Context\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\nTasks are scheduled and ran on the same node they are scheduled.\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\n","Decision":"Create a distribution-aware implementation of `TaskManager`.\\n","tokens":102,"id":2127,"text":"## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n\n\n##Decision\nCreate a distribution-aware implementation of `TaskManager`.\\n","Predictions":"Create a distribution-aware implementation of TaskManager.n"}
{"File Name":"js-sdk\/0016-update-threebot-deployer-flist.md","Context":"## Context\nDeploying hosted 3Bot takes a lot of time because of cloning and installation time.\nIt can make use of the new flist build by the CI to be always up to date and reduce the installation time.\n","Decision":"Update the flist with the up to date one\\n","tokens":48,"id":5201,"text":"## Context\\nDeploying hosted 3Bot takes a lot of time because of cloning and installation time.\\nIt can make use of the new flist build by the CI to be always up to date and reduce the installation time.\\n\n\n##Decision\nUpdate the flist with the up to date one\\n","Predictions":"### Terraform ConfigurationnWe will identify and manage separately, resources at an Account level and resources at an Environment level.nAccount level resources, such as S3 buckets for logs, the default VPC and networking will be used Environment level resources, such as ECS clusters, DNS routes and Load Balancers.nThis will enable us to use Terraform Workspaces to contain an Environment completely for it to be built and destroyed simply.n![split terraform configuration diagram](..\/diagrams\/Use-An-LPA_split_Terraform_configurations.png)n### Name SpacingnWe will use the github PR number and the first few characters of the branch name to namespace the environment and it's resourcesnFor example, DNS records will be created for each environment using the namespace https:\/\/view.025-UML-93.use-an-lpa.opg.service.justice.gov.uknwhile an ECS cluster will be named 025-UML-93-use-an-lpan### Environment lifecyclenAn Environment will be built when a PR is openednOnce evaluation is complete, the Envirnoment can be destroyed.nTo begin with the pipeline will print instructions for destroying an Environment.n"}
{"File Name":"james-project\/0032-distributed-mail-queue-cleanup.md","Context":"## Context\nRead [Distributed Mail Queue](0031-distributed-mail-queue.md) for full context.\n**enqueuedMailsV3** and **deletedMailsV2** is never cleaned up and the corresponding blobs are always referenced. This is not\nideal both from a privacy and space storage costs point of view.\nNote that **enqueuedMailsV3** and **deletedMailsV2** rely on timeWindowCompactionStrategy.\n","Decision":"Add a new `contentStart` table referencing the point in time from which a given mailQueue holds data, for each mail queue.\\nThe values contained between `contentStart` and `browseStart` can safely be deleted.\\nWe can perform this cleanup upon `browseStartUpdate`: once finished we can browse then delete content of **enqueuedMailsV3**\\nand **deletedMailsV2** contained between `contentStart` and the new `browseStart` then we can safely set `contentStart`\\nto the new `browseStart`.\\nContent before `browseStart` can safely be considered deletable, and is applicatively no longer exposed. We don't need an\\nadditional grace period mechanism for `contentStart`.\\nFailing cleanup will lead to the content being eventually updated upon next `browseStart` update.\\nWe will furthermore delete blobStore content upon dequeue, also when the mail had been deleted or purged via MailQueue\\nmanagement APIs.\\n","tokens":97,"id":2880,"text":"## Context\\nRead [Distributed Mail Queue](0031-distributed-mail-queue.md) for full context.\\n**enqueuedMailsV3** and **deletedMailsV2** is never cleaned up and the corresponding blobs are always referenced. This is not\\nideal both from a privacy and space storage costs point of view.\\nNote that **enqueuedMailsV3** and **deletedMailsV2** rely on timeWindowCompactionStrategy.\\n\n\n##Decision\nAdd a new `contentStart` table referencing the point in time from which a given mailQueue holds data, for each mail queue.\\nThe values contained between `contentStart` and `browseStart` can safely be deleted.\\nWe can perform this cleanup upon `browseStartUpdate`: once finished we can browse then delete content of **enqueuedMailsV3**\\nand **deletedMailsV2** contained between `contentStart` and the new `browseStart` then we can safely set `contentStart`\\nto the new `browseStart`.\\nContent before `browseStart` can safely be considered deletable, and is applicatively no longer exposed. We don't need an\\nadditional grace period mechanism for `contentStart`.\\nFailing cleanup will lead to the content being eventually updated upon next `browseStart` update.\\nWe will furthermore delete blobStore content upon dequeue, also when the mail had been deleted or purged via MailQueue\\nmanagement APIs.\\n","Predictions":"Add a new contentStart table referencing the point in time from which a given mailQueue holds data, for each mailQueue.nThe values contained between contentStart and browseStart can safely be deleted.nWe can perform this cleanup upon browseStartUpdate: once finished we can browse then delete content of **enqueuedMailsV3**nand **deletedMailsV2** contained between contentStart and the new browseStart then we can safely set contentStart to the new browseStart.nContent before browseStart can safely be considered deletable, and is applicatively no longer exposed. We don't need annadditional grace period mechanism for contentStart.nFailing cleanup will lead to the content being eventually updated upon next browseStart update.nWe will furthermore delete blobStore content upon dequeue, also when the mail had been deleted or purged via MailQueuenmanagement APIs.n"}
{"File Name":"dotfiles\/0004-use-vim-plug-in-favour-of-vundle.md","Context":"## Context\nVundle hasn't been updated for a while (Aug 2019 at time of writing) and I'd\nread a bit about how vim-plug was pretty good. And it felt like it was time for\na change.\n","Decision":"[vim-plug](https:\/\/github.com\/junegunn\/vim-plug) will be used to manage Vim\\nplugins in favour of [Vundle](https:\/\/github.com\/VundleVim\/Vundle.vim).\\n","tokens":51,"id":114,"text":"## Context\\nVundle hasn't been updated for a while (Aug 2019 at time of writing) and I'd\\nread a bit about how vim-plug was pretty good. And it felt like it was time for\\na change.\\n\n\n##Decision\n[vim-plug](https:\/\/github.com\/junegunn\/vim-plug) will be used to manage Vim\\nplugins in favour of [Vundle](https:\/\/github.com\/VundleVim\/Vundle.vim).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).nADRs from here on out will be written in the first-person **singular** tense, since it's mainly just me building the app.n"}
{"File Name":"cloud-platform\/015-Keeping-Terraform-modules-up-to-date.md","Context":"## Context\nWe maintain a lot of [terraform modules] which teams use to manage AWS\nresources for their namespaces.\nIn the past, lettings different namespaces use different versions of terraform\nmodules has caused problems because there has not always been a clear upgrade\npath from a module that is several versions old, to the latest version.\nIn these cases, it would have been easier to have upgraded every namespace when\nthe significant change was made to the module, because at that point in time\nthe relevant knowledge and understanding are fresh. Leaving it until later\nmakes the whole process much more difficult.\n","Decision":"We decided to ensure that all namespaces are always using the latest version of\\nevery module.\\n","tokens":124,"id":621,"text":"## Context\\nWe maintain a lot of [terraform modules] which teams use to manage AWS\\nresources for their namespaces.\\nIn the past, lettings different namespaces use different versions of terraform\\nmodules has caused problems because there has not always been a clear upgrade\\npath from a module that is several versions old, to the latest version.\\nIn these cases, it would have been easier to have upgraded every namespace when\\nthe significant change was made to the module, because at that point in time\\nthe relevant knowledge and understanding are fresh. Leaving it until later\\nmakes the whole process much more difficult.\\n\n\n##Decision\nWe decided to ensure that all namespaces are always using the latest version of\\nevery module.\\n","Predictions":"We decided to go for two branches. On the one hand, master being the branch that contains the code of the latest release and will be tagged with release versions. On the other hand, there will be a develop branch that hosts the current state of development and will be merged to master whenever we want to create a new release.nThis way we get rid of the current pain of releasing with every pull request at the cost a some overhead during release.nIn the process of automating most of the release process, we will run an integration test with locally built images for pull requests to master. Regarding choice 2, whenever a pull request is merged, whoever merged the PR has to tag this commit on the master branch with the most recent version. Right after the merge, everyone merged the PR has to tag this commit on the master branch with the most recent version. Right after the merge, everyone merged the PR has to tag this commit on the master branch with the most recent version. Right after the merge, everyone merged the PR has to tag this commit on the master branch with the most recent version. Right after the merge, everyone merged the PR has to tag this commit on the master branch with the most recent version.n"}
{"File Name":"galaxy-zoo-touch-table\/adr-3.md","Context":"### Context\nWe should structure files in a way that makes the codebase easy to navigate and intuitive when searching for certain components. Organization should adhere to accepted practices in the .NET community.\n### Decision\nThe MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\n### Status\nAccepted\n### Consequences\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\n","Decision":"The MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n","tokens":280,"id":5082,"text":"### Context\\nWe should structure files in a way that makes the codebase easy to navigate and intuitive when searching for certain components. Organization should adhere to accepted practices in the .NET community.\\n### Decision\\nThe MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n\n\n##Decision\nThe MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insight into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.\\nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.\\n### Status\\nAccepted\\n### Consequences\\nIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).\\n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?\\n","Predictions":"* The MVVM pattern is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insights into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.n### StatusnAcceptedn### ConsequencesnIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?n"}
{"File Name":"fxa\/0025-retire-fxa-auth-db-mysql.md","Context":"## Context and Problem Statement\nIn the original architecture for FxA, which was based on the Persona architecture, the database API was on a separate physical layer with more restricted permissions than the web frontend's that served the public API. This separation was deemed unimportant at some point, though the details are probably lost to history, and the db service was moved to be colocated with the auth-server on the frontend servers. New services like graphql-api-server are already making direct db calls via knex in fxa-shared, which auth-server could also use.\n## Decision Drivers\n- Reduce complexity\n- Improve performance\n- Share code\n","Decision":"- Reduce complexity\\n- Improve performance\\n- Share code\\nWe will incrementally expand the fxa-shared db API and use it in auth-server. Once the API implements all the fxa-auth-db-mysql calls we can decomission that service. Doing so will simplify the architecture and maintenance burden, make future changes easier, and should improve service performance.\\n","tokens":127,"id":362,"text":"## Context and Problem Statement\\nIn the original architecture for FxA, which was based on the Persona architecture, the database API was on a separate physical layer with more restricted permissions than the web frontend's that served the public API. This separation was deemed unimportant at some point, though the details are probably lost to history, and the db service was moved to be colocated with the auth-server on the frontend servers. New services like graphql-api-server are already making direct db calls via knex in fxa-shared, which auth-server could also use.\\n## Decision Drivers\\n- Reduce complexity\\n- Improve performance\\n- Share code\\n\n\n##Decision\n- Reduce complexity\\n- Improve performance\\n- Share code\\nWe will incrementally expand the fxa-shared db API and use it in auth-server. Once the API implements all the fxa-auth-db-mysql calls we can decomission that service. Doing so will simplify the architecture and maintenance burden, make future changes easier, and should improve service performance.\\n","Predictions":"- Reduce complexityn- Improve performancen- Share codenWe will incrementally expand the fxa-shared db API and use it in auth-server. Once the API implements all the fxa-auth-db-mysql calls we can decomission that service. Doing so will simplify the architecture and maintenance burden, make future changes easier, and should improve service performance.n"}
{"File Name":"boxwise-flask\/Python-ORM.md","Context":"## Context\nUp to now, database changes have been managed through [Phinx](https:\/\/phinx.org\/) in PHP, with SQL queries written as strings in PHP. Since we are migrating off of the old PHP code base of Dropapp and into the new codebase of Python \/ React, we needed to decide how to handle reading and writing to the DB going forward (GraphQL does not solve this, as you still have to hook the GraphQL interface into the DB somehow).\n","Decision":"1. Learning Curve\\n2. Community Support\\n3. Power and Reliability considering it will be used in a production environment\\nPeewee. Despite SQLAlchemy being the gold standard of ORMs, there are no long-term volunteers who have mastered that library aside from David C. The difficulty of ramping up everyone on SQLAlchemy and maintaining it outweighs the difficulty of having potential volunteers who are familiar with SQLAlchemy pick up one of the \"easier\" ORMs. Comparing the release notes of Peewee and PonyORM, David C. commented that Peewee's recent releases relate to support of edge cases and new technology stacks, whereas PonyORM releases still appear to involve developing functionality and fixing bugs around core uses cases. We therefore concluded that Peewee is the more production-ready solution.\\n","tokens":96,"id":3053,"text":"## Context\\nUp to now, database changes have been managed through [Phinx](https:\/\/phinx.org\/) in PHP, with SQL queries written as strings in PHP. Since we are migrating off of the old PHP code base of Dropapp and into the new codebase of Python \/ React, we needed to decide how to handle reading and writing to the DB going forward (GraphQL does not solve this, as you still have to hook the GraphQL interface into the DB somehow).\\n\n\n##Decision\n1. Learning Curve\\n2. Community Support\\n3. Power and Reliability considering it will be used in a production environment\\nPeewee. Despite SQLAlchemy being the gold standard of ORMs, there are no long-term volunteers who have mastered that library aside from David C. The difficulty of ramping up everyone on SQLAlchemy and maintaining it outweighs the difficulty of having potential volunteers who are familiar with SQLAlchemy pick up one of the \"easier\" ORMs. Comparing the release notes of Peewee and PonyORM, David C. commented that Peewee's recent releases relate to support of edge cases and new technology stacks, whereas PonyORM releases still appear to involve developing functionality and fixing bugs around core uses cases. We therefore concluded that Peewee is the more production-ready solution.\\n","Predictions":"We will use a Schema library to convert the data into a JSON format.n"}
{"File Name":"where-away\/0011-render-buttons-instead-of-anchor-elements-for-link-nodes.md","Context":"## Context\n- semantically, an html element that takes you to a new web page when you click\nit is an anchor -- `<a>`.\n- I'd like tab navigation and the Enter key to work for navigating between\nbookmarks.\n- buttons can be accessed via Tab and activated via Enter\n- not so with an anchor tag. See\nhttps:\/\/stackoverflow.com\/questions\/41476292\/how-to-give-focus-to-anchor-tag-without-href,\netc.\n","Decision":"Model the links as `<button>` elements to make the interaction more normal.\\n","tokens":100,"id":2280,"text":"## Context\\n- semantically, an html element that takes you to a new web page when you click\\nit is an anchor -- `<a>`.\\n- I'd like tab navigation and the Enter key to work for navigating between\\nbookmarks.\\n- buttons can be accessed via Tab and activated via Enter\\n- not so with an anchor tag. See\\nhttps:\/\/stackoverflow.com\/questions\/41476292\/how-to-give-focus-to-anchor-tag-without-href,\\netc.\\n\n\n##Decision\nModel the links as `<button>` elements to make the interaction more normal.\\n","Predictions":"We will use stackoverflow to create a new stack element.n"}
{"File Name":"laundromat\/0003-choice-of-entities.md","Context":"## Context\nThe choice of entities, i.e. what categories of information we want to anonymise and the specificity of these categories, will greatly impact the performance of our model. Additionally, time constraints mean that there is an upper limit to how many entities can be included. As such, we concluded that those entities which fulfill at least two of the following three categories should be included:\n* It is either directly identifying or a close proxy (e.g. names, phone numbers, etc.)\n* It is a so-called \u201cspecial categories\u201d of information (e.g. medical information)\n* It is present in the data in non-trivial quantities\n","Decision":"We have chosen the following NER entities:\\n* ORG (Organisation)\\n* LOC (Location)\\n* PER (Person)\\n* FNR (Personal number)\\n* MONEY\\n* DATE_TIME (Dates, time of day, name of day, and name of month)\\n* MEDICAL_CONDITIONS\\nEntities that will be left purely to RegEx are:\\n* NAV_YTELSE and NAV_OFFICE\\n* AGE\\n* TLF (Telephone number)\\n* BACC (Bank account number)\\nWe believe this list strikes the right balance between performance (fewer entities are better) and coverage.\\n","tokens":134,"id":4261,"text":"## Context\\nThe choice of entities, i.e. what categories of information we want to anonymise and the specificity of these categories, will greatly impact the performance of our model. Additionally, time constraints mean that there is an upper limit to how many entities can be included. As such, we concluded that those entities which fulfill at least two of the following three categories should be included:\\n* It is either directly identifying or a close proxy (e.g. names, phone numbers, etc.)\\n* It is a so-called \u201cspecial categories\u201d of information (e.g. medical information)\\n* It is present in the data in non-trivial quantities\\n\n\n##Decision\nWe have chosen the following NER entities:\\n* ORG (Organisation)\\n* LOC (Location)\\n* PER (Person)\\n* FNR (Personal number)\\n* MONEY\\n* DATE_TIME (Dates, time of day, name of day, and name of month)\\n* MEDICAL_CONDITIONS\\nEntities that will be left purely to RegEx are:\\n* NAV_YTELSE and NAV_OFFICE\\n* AGE\\n* TLF (Telephone number)\\n* BACC (Bank account number)\\nWe believe this list strikes the right balance between performance (fewer entities are better) and coverage.\\n","Predictions":"We have chosen the following NER entities:n* ORG (Organisation)n* LOC (Location)n* PER (Person)n* FNR (Personal number)n* MONEYn* DATE_TIME (Dates, time of day, name of day, and name of month)n* MEDICAL_CONDITIONSnEntities that will be left purely to RegEx are:n* NAV_YTELSE and NAV_OFFICEn* AGEn* TLF (Telephone number)n* BACC (Bank account number)nWe believe this list strikes the right balance between performance (fewer entities are better) and coverage.n"}
{"File Name":"ftd-scratch3-offline\/0011-implement-complex-scratch-functions-in-a-helper-function.md","Context":"## Context\nSome scratch\/ftduino functions take complex arguments that need to be verified.\nSome scratch\/ftduino functions are complex to implement.\nThe code for these functions could be directly generated in the java files for the specific function.\nThe code for these functions could also be written inside a helper function, such that only a single line - that calls the helper function - is generated for such a function.\nWriting these functions in a helper function also makes changes and code reuse easier.\n","Decision":"Complex scratch\/ftduino functions will be implemented in helper functions and those will be called by the generated code.\\n","tokens":99,"id":2629,"text":"## Context\\nSome scratch\/ftduino functions take complex arguments that need to be verified.\\nSome scratch\/ftduino functions are complex to implement.\\nThe code for these functions could be directly generated in the java files for the specific function.\\nThe code for these functions could also be written inside a helper function, such that only a single line - that calls the helper function - is generated for such a function.\\nWriting these functions in a helper function also makes changes and code reuse easier.\\n\n\n##Decision\nComplex scratch\/ftduino functions will be implemented in helper functions and those will be called by the generated code.\\n","Predictions":"We will keep the Processor as a Domain Service, because the three different kinds of execution we want to support really only differ for how data is handled in memory, and the domain doesn't concern itself with what is contained in memory (it doesn't even mention memory, since it's a detail of the interpreter, and we might as well use interpreters that don't use any memory):n- in the classic virtual machine, the Program is the entire program, that is executed from start to endn- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor: this is necessary because we might have defined procedures that we want to call in the future;n- in the future, we might need to define a new implementation of a procedure that we want to call in the domain;n- in the future, we might need to define a new implementation of a procedure that we want to call in the domain;n- in the future, we might need to define a new implementation of a procedure that we want to call in the domain;n- in the future, we might need to define a new implementation of a procedure that we want to call in the domain;n- in the future, we might need to define a new implementation of a procedure that we want to call in the domain;n- in the future, we might need to define a new implementation of a procedure that we want to call in the domain;n- in the future, we might need to define a new implementation of a procedure that we want to call in the domain;n- in the future, we might need to define a new implementation of a procedure that we want to call in the domain;n- in the future, we might need to define a new implementation of a procedure that we want to call in the domain;n- in the future, we might need to define a new implementation of a procedure that we want to call in the domain;n- in the future, we might need to define a new implementation of a procedure that we want to call in the domain;n- in the future, we might need to define a new implementation of a procedure that we want to call in the domain;n- in the future, we might need to define a new implementation of a procedure that we want to call in the domain;n- in the future, we might need to define a new implementation of a procedure that we want to call in the domain;n"}
{"File Name":"iampeterbanjo.com\/0007-module.exports-vs-export-default.md","Context":"## Context\nWhen HapiJS plugins are registered they expect an object like this:\n```JavaScript\n{\nname: 'plugin-name',\nversion: '1.0.0',\nregister: (server, options) => {}\n}\n```\nThe problem with `export default` is that it exports an object whose property `default` is the value of the exported object. This makes my convention of having the `index.ts` register the plugin not work because HapiJS still uses CommonJs and won't get the `default` value E.g.\n```JavaScript\nimport plugin from '.\/plugin';\nexport default {\nplugin,\n}\n```\nSwitching between `module.exports` and `export default` will make using my packages tricky. It won't be clear when to use CommonJS or ES6 modules so I'll pick one and stick with it.\nSo let's use `module.exports` instead of `export default`.\n","Decision":"Rejected because if I try and import a module that uses `module.exports` Typescript shows an error that the file `is not a module`. The implication of this is that I have to replace [Glue][hapi-glue] to compose the API.\\n","tokens":202,"id":1596,"text":"## Context\\nWhen HapiJS plugins are registered they expect an object like this:\\n```JavaScript\\n{\\nname: 'plugin-name',\\nversion: '1.0.0',\\nregister: (server, options) => {}\\n}\\n```\\nThe problem with `export default` is that it exports an object whose property `default` is the value of the exported object. This makes my convention of having the `index.ts` register the plugin not work because HapiJS still uses CommonJs and won't get the `default` value E.g.\\n```JavaScript\\nimport plugin from '.\/plugin';\\nexport default {\\nplugin,\\n}\\n```\\nSwitching between `module.exports` and `export default` will make using my packages tricky. It won't be clear when to use CommonJS or ES6 modules so I'll pick one and stick with it.\\nSo let's use `module.exports` instead of `export default`.\\n\n\n##Decision\nRejected because if I try and import a module that uses `module.exports` Typescript shows an error that the file `is not a module`. The implication of this is that I have to replace [Glue][hapi-glue] to compose the API.\\n","Predictions":"The config.exs file will be a config.exs file.n"}
{"File Name":"operational-data-hub\/0019-single-schema-per-topic.md","Context":"## Context\nA schema is a vocabulary that allows you to annotate and validate documents. Every topic has a schema that can validate the messages the topic receives.\n","Decision":"Since every topic only receives messages in JSON format (see [21. Messages are in JSON format](0021-messages-are-in-json-format.md) ), we define a JSON Schema for every topic that can validate the messages received by said topic.\\n","tokens":32,"id":2710,"text":"## Context\\nA schema is a vocabulary that allows you to annotate and validate documents. Every topic has a schema that can validate the messages the topic receives.\\n\n\n##Decision\nSince every topic only receives messages in JSON format (see [21. Messages are in JSON format](0021-messages-are-in-json-format.md) ), we define a JSON Schema for every topic that can validate the messages received by said topic.\\n","Predictions":"Chosen option: \"Use a syntax that is derived of the lucene query syntax\", because only option that is already known, and easy to implement.nFurthermore parsers for lucene already exist and are tested.nFor simplicity, and lack of universal capabilities across fetchers, only basic query features and therefor syntax is supported:n* All terms in the query are whitespace separated and will be ANDedn* Default and certain fielded terms are supportedn* Fielded Terms:n* authorn* titlen* journaln* year (for single year)n* year-range (for range e.g. year-range:2012-2015)n* The journal, year, and year-range fields should only be populated once in each queryn* The year and year-range fields are mutually exclusiven* Example:n* author:\"Igor Steinmacher\" author:\"Christoph Treude\" year:2017 will be converted ton* author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" year:2017 will be converted ton* author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" year:2017 will be converted ton* author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" year:2017 will be converted ton* author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" year:2017 will be converted ton* author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" year:2017 will be converted ton* author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" year:2017 will be converted ton* author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" year:2017 will be converted ton* author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" year:2017 will be converted ton* author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" year:2017 will be converted ton* author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" year:2017 will be converted ton* author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" year:2017 will be converted ton* author:\"Igor Steinmacher\" AND author:\""}
{"File Name":"iampeterbanjo.com\/0009-place-tests-next-to-source-files.md","Context":"## Context\nWhile migrating to Typescript, fixing tests means switching between the source file and their test files. Where the test files are in another folder, the journey is longer. If they were side-by-side then once I had the source or test, finding the related file would be much easier\n","Decision":"Moving test files next to the source code makes it much easier to switch between source and tests. It's also easier to see which test files do not have any unit tests.\\n","tokens":60,"id":1588,"text":"## Context\\nWhile migrating to Typescript, fixing tests means switching between the source file and their test files. Where the test files are in another folder, the journey is longer. If they were side-by-side then once I had the source or test, finding the related file would be much easier\\n\n\n##Decision\nMoving test files next to the source code makes it much easier to switch between source and tests. It's also easier to see which test files do not have any unit tests.\\n","Predictions":"We will remove the verify-service-provider-env.yml file from src\/main\/resourcesnThe application will default to the verify-service-provider.yml file that's included in the.zip if no command line arguments are provided.nIf the application is started without command line arguments specifying a yml filenAND no environment variables have been set, startup should error gracefully and tellnthe user that the configuration fields have been set.n"}
{"File Name":"insight\/0001-python.md","Context":"## Context\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\n","Decision":"We choose Python as our base programming language.\\n","tokens":33,"id":4761,"text":"## Context\\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\\n\n\n##Decision\nWe choose Python as our base programming language.\\n","Predictions":"!-- What is the change that we're proposing and\/or doing? -->n1. The first parameter should be a scope of type GuStack:n:white_check_mark: Validntsnclass MyConstruct nconstructor(scope: GuStack) n...nnn:x: Invalidntsnclass MyConstruct nconstructor(scope: Stack, props: MyConstructProps) n...nnnn:x: Invalidntsnclass MyConstruct nconstructor(scope: Stack, props: object) n...nnnnThe construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n3. They can also take an id from props as it will never change, for example InstanceTypeFor$props.app.n"}
{"File Name":"link_platform\/0002-use-the-adr-tools-kit-to-manage-adr-docs.md","Context":"## Context\nTo help automated the process of managing architectual decision records use a tool that stanadardizes the process.\nI found a reference to this tool on the thought works techninques review page.  The github repo is:\n`https:\/\/github.com\/npryce\/adr-tools`\nBut I installed it using brew:\n`brew install adr-tools`\n","Decision":"We will use the system adr-tools to manage the ADR documents\\n","tokens":79,"id":5033,"text":"## Context\\nTo help automated the process of managing architectual decision records use a tool that stanadardizes the process.\\nI found a reference to this tool on the thought works techninques review page.  The github repo is:\\n`https:\/\/github.com\/npryce\/adr-tools`\\nBut I installed it using brew:\\n`brew install adr-tools`\\n\n\n##Decision\nWe will use the system adr-tools to manage the ADR documents\\n","Predictions":"We will use the system adr-tools to manage the ADR documentsn"}
{"File Name":"bookit-api\/0010-jpa-manages-schema.md","Context":"## Context\nOriginally, we used Spring Boot's Database Initialization support to automatically create and intialize our database via schema.sql and data.sql scripts.  Each deployment (application initialization) would execute these scripts.  Our implementation would drop the database and recreate it each time.  While this accelerated our development (avoid data migrations), it's not sustainable\n","Decision":"* Leverage Hibernate's (our JPA implementation) ddl-auto feature to update the staging\/production databases (we will continue to drop\/recreate all other databases....local, integration).\\n* recreating in integration ensures a clean database for each run.  In addition, it validates that we can recreate a database from scratch\\n","tokens":71,"id":3221,"text":"## Context\\nOriginally, we used Spring Boot's Database Initialization support to automatically create and intialize our database via schema.sql and data.sql scripts.  Each deployment (application initialization) would execute these scripts.  Our implementation would drop the database and recreate it each time.  While this accelerated our development (avoid data migrations), it's not sustainable\\n\n\n##Decision\n* Leverage Hibernate's (our JPA implementation) ddl-auto feature to update the staging\/production databases (we will continue to drop\/recreate all other databases....local, integration).\\n* recreating in integration ensures a clean database for each run.  In addition, it validates that we can recreate a database from scratch\\n","Predictions":"* Leverage Hibernate's (our JPA implementation) ddl-auto feature to update the staging\/production databases (we will continue to drop\/recreate all other databases....local, integration).n* recreating in integration ensures a clean database for each run. In addition, it validates that we can recreate a database from scratchn"}
{"File Name":"launcher\/2018-06-15_request_enrollment_details.md","Context":"## Context\nWhen enrolling osquery into a remote server, it's desirable to also have a set of initial fields about the osquery node. Usually, servers like Fleet queue ad-hoc queries to get this information, but it's not always reliable. For example, a osquery host can succeed during the Enroll method, and then immediately crash, leaving the server operator with little context about which osquery host to troubleshoot. Another effect of populating host details via distributed query is that they can add some latency to the enrollment process depending on the query interval.\n","Decision":"Add a EnrollmentDetails structure to the RequestEnrollment method which sends the results of a population query as part of the enrollment. Below is a list of chose attributes that launcher will always send:\\n```\\nmessage EnrollmentDetails {\\nstring os_version = 1;\\nstring os_build = 2;\\nstring os_platform = 3;\\nstring hostname = 4;\\nstring hardware_vendor = 5;\\nstring hardware_model = 6;\\nstring hardware_serial = 7;\\nstring osquery_version = 8;\\nstring launcher_version = 9;\\n}\\n```\\n","tokens":116,"id":2546,"text":"## Context\\nWhen enrolling osquery into a remote server, it's desirable to also have a set of initial fields about the osquery node. Usually, servers like Fleet queue ad-hoc queries to get this information, but it's not always reliable. For example, a osquery host can succeed during the Enroll method, and then immediately crash, leaving the server operator with little context about which osquery host to troubleshoot. Another effect of populating host details via distributed query is that they can add some latency to the enrollment process depending on the query interval.\\n\n\n##Decision\nAdd a EnrollmentDetails structure to the RequestEnrollment method which sends the results of a population query as part of the enrollment. Below is a list of chose attributes that launcher will always send:\\n```\\nmessage EnrollmentDetails {\\nstring os_version = 1;\\nstring os_build = 2;\\nstring os_platform = 3;\\nstring hostname = 4;\\nstring hardware_vendor = 5;\\nstring hardware_model = 6;\\nstring hardware_serial = 7;\\nstring osquery_version = 8;\\nstring launcher_version = 9;\\n}\\n```\\n","Predictions":"Add a EnrollmentDetails structure to the RequestEnrollment method which sends the results of a population query as part of the enrollment. Below is a list of chose attributes that launcher will always send:nmessage EnrollmentDetails nstring os_version = 1;nstring os_build = 2;nstring os_platform = 3;nstring hostname = 4;nstring hardware_vendor = 5;nstring hardware_model = 6;nstring hardware_serial = 7;nstring osquery_version = 8;nstring launcher_version = 9;nn"}
{"File Name":"meadow\/0007-code-analysis.md","Context":"## Context\nWe need to make sure we adhere to our own designated code quality best practices.\n","Decision":"Use a code analysis tool (specifically, [credo](http:\/\/credo-ci.org\/) for Elixir\\nand [prettier](https:\/\/prettier.io\/) for JavaScript).\\n","tokens":20,"id":3853,"text":"## Context\\nWe need to make sure we adhere to our own designated code quality best practices.\\n\n\n##Decision\nUse a code analysis tool (specifically, [credo](http:\/\/credo-ci.org\/) for Elixir\\nand [prettier](https:\/\/prettier.io\/) for JavaScript).\\n","Predictions":"We will use [Azure](https:\/\/azure.io\/) for our database model.n"}
{"File Name":"gsp\/ADR043-k8s-resource-access.md","Context":"## Context\nSeveral different levels of access are required within a kubernetes cluster. The\nGSP uses role based access control so these levels are granted to users and\ngroups via roles.\n","Decision":"We will create two levels of access within each namespace:\\n* Operator\\n* Auditor\\nThe Operator is a relatively permissive read-write role within the namespace.\\nDevelopers working on branches that are not part of the release process may be\\ngranted this role in certain namespaces. This is also the role the in-cluster\\nconcourse team for each namespace will be granted.\\nThe Auditor will be given to all authenticated users in the cluster. This should\\nallow for debugging of issues and incidents and basic remedial actions without\\nneeding formal escalation procedures.\\nThe complete list of resource permissions is given in Appendix A.\\n","tokens":40,"id":3890,"text":"## Context\\nSeveral different levels of access are required within a kubernetes cluster. The\\nGSP uses role based access control so these levels are granted to users and\\ngroups via roles.\\n\n\n##Decision\nWe will create two levels of access within each namespace:\\n* Operator\\n* Auditor\\nThe Operator is a relatively permissive read-write role within the namespace.\\nDevelopers working on branches that are not part of the release process may be\\ngranted this role in certain namespaces. This is also the role the in-cluster\\nconcourse team for each namespace will be granted.\\nThe Auditor will be given to all authenticated users in the cluster. This should\\nallow for debugging of issues and incidents and basic remedial actions without\\nneeding formal escalation procedures.\\nThe complete list of resource permissions is given in Appendix A.\\n","Predictions":"We will create two levels of access within each namespace:n* Operatorn* AuditornThe Operator is a relatively permissive read-write role within the namespace.nDevelopers working on branches that are not part of the release process may bengranted this role in certain namespaces. This is also the role in-clusternconcourse team for each namespace will be granted.nThe Auditor will be given to all authenticated users in the cluster. This shouldnallow for debugging of issues and incidents and basic remedial actions withoutnneeding formal escalation procedures.nThe complete list of resource permissions is given in Appendix A.n"}
{"File Name":"jabref\/0021-keep-study-as-a-dto.md","Context":"## Context and Problem Statement\nThe study holds query and library entries that could be replaced respectively with complex query and fetcher instances.\nThis poses the question: should the study remain a pure DTO object or should it contain direct object instances?\n","Decision":"Chosen option: \"Keep study as DTO and use transformators\", because comes out best (see below).\\n","tokens":49,"id":4741,"text":"## Context and Problem Statement\\nThe study holds query and library entries that could be replaced respectively with complex query and fetcher instances.\\nThis poses the question: should the study remain a pure DTO object or should it contain direct object instances?\\n\n\n##Decision\nChosen option: \"Keep study as DTO and use transformators\", because comes out best (see below).\\n","Predictions":"Chosen option: \"Use a syntax that is derived of the lucene query syntax\", because only option that is already known, and easy to implement.nFurthermore parsers for lucene already exist and are tested.nFor simplicity, and lack of universal capabilities across fetchers, only basic query features and therefor syntax is supported:n* All terms in the query are whitespace separated and will be ANDedn* Default and certain fielded terms are supportedn* Fielded Terms:n* authorn* titlen* journaln* year (for single year)n* year-range (for range e.g. year-range:2012-2015)n* The journal, year, and year-range fields should only be populated once in each queryn* The year and year-range fields are mutually exclusiven* Example:n* author:\"Igor Steinmacher\" author:\"Christoph Treude\" AND author:\"Igor Steinmacher\" AND year:2017 will be converted ton* author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" AND year:2017nThe supported syntax can be expressed in EBNF as follows:nQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery := ClausenQuery :="}
{"File Name":"princeton_ansible\/0002-group-vars.md","Context":"## Context\nThere are a lot of different ways to set variables in Ansible and we should have\na consistent way to do it in all playbooks for each group.\nWe want to make sure that there's a smaller vault.yml file per group, instead of\na gigantic vault.yml file for everything. This is because one huge vault.yml\nfile causes merge conflicts in multiple Github Pull Requests.\n","Decision":"Unique group variables will be placed in `\/group_vars\/<groupname>`\\nEncrypted group variables will be placed in `\/group_vars\/<groupname>\/vault.yml`\\nShared group variables will be placed in `\/group_vars\/all\/`\\n","tokens":82,"id":2255,"text":"## Context\\nThere are a lot of different ways to set variables in Ansible and we should have\\na consistent way to do it in all playbooks for each group.\\nWe want to make sure that there's a smaller vault.yml file per group, instead of\\na gigantic vault.yml file for everything. This is because one huge vault.yml\\nfile causes merge conflicts in multiple Github Pull Requests.\\n\n\n##Decision\nUnique group variables will be placed in `\/group_vars\/<groupname>`\\nEncrypted group variables will be placed in `\/group_vars\/<groupname>\/vault.yml`\\nShared group variables will be placed in `\/group_vars\/all\/`\\n","Predictions":"- Split rendering code from markmap-lib into a new version of markmap-libn"}
{"File Name":"Wikibase\/0021-federated-properties-source-and-type-dispatching-services.md","Context":"## Context\nFederated Properties v2 aims to make it possible for users to enable Federated Properties even if their Wikibase already contains data, so that they can choose to use both remote & local Properties to make statements.\nDispatching by entity type is a widely used mechanism in Wikibase that allows the dispatching service to handle entity ids of different types by delegating to the service implementation defined in the [entity type definitions], thus enabling entity type specific behavior within a single service. With Federated Properties v2 the entity type (\"property\") no longer uniquely identifies the desired service implementation, since local Properties' services need to be handled by database-backed implementations, whereas Federated Properties use API-backed services. In order to work with local and remote Properties, dispatching services need to be aware of the entity's source as well as the type.\n","Decision":"Make all relevant services source and type dispatching.\\n","tokens":172,"id":1346,"text":"## Context\\nFederated Properties v2 aims to make it possible for users to enable Federated Properties even if their Wikibase already contains data, so that they can choose to use both remote & local Properties to make statements.\\nDispatching by entity type is a widely used mechanism in Wikibase that allows the dispatching service to handle entity ids of different types by delegating to the service implementation defined in the [entity type definitions], thus enabling entity type specific behavior within a single service. With Federated Properties v2 the entity type (\"property\") no longer uniquely identifies the desired service implementation, since local Properties' services need to be handled by database-backed implementations, whereas Federated Properties use API-backed services. In order to work with local and remote Properties, dispatching services need to be aware of the entity's source as well as the type.\\n\n\n##Decision\nMake all relevant services source and type dispatching.\\n","Predictions":"We will use the [Security & Security](https:\/\/github.com\/security-and-security\/security-security) to provide a single, centralised, and unified way to manage and manage the security of the Marain platform.n"}
{"File Name":"govuk-aws\/0029-combine-api-redis-into-backend-redis.md","Context":"## Context\nWe currently have 3 Redis instances that we run in Elasticache:\n- backend-redis\n- logs-redis\n- api-redis\nMost applications use backend-redis, the logging cluster uses logs-redis (which should\nsoon be replaced), and Rummager uses api-redis.\nThis was traditionally the case because Rummager lived in a different vDC to redis-1\/redis-2,\nbut this is no longer a concern in AWS.\n","Decision":"Update Rummager configuration so it uses backend-redis, and remove api-redis.\\n","tokens":101,"id":4044,"text":"## Context\\nWe currently have 3 Redis instances that we run in Elasticache:\\n- backend-redis\\n- logs-redis\\n- api-redis\\nMost applications use backend-redis, the logging cluster uses logs-redis (which should\\nsoon be replaced), and Rummager uses api-redis.\\nThis was traditionally the case because Rummager lived in a different vDC to redis-1\/redis-2,\\nbut this is no longer a concern in AWS.\\n\n\n##Decision\nUpdate Rummager configuration so it uses backend-redis, and remove api-redis.\\n","Predictions":"- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis, and remove api-redis.n- Update Redis configuration so it uses backend-redis.n- Update Redis configuration so it uses backend-redis.n- Update Redis configuration so it uses backend-redis.n- Update Redis configuration so it uses backend-redis.n- Update Redis configuration so it uses backend-redis.n- Update Redis configuration so it uses backend-redis.n- Update Redis configuration so it uses backend-redis.n- Update Redis configuration so it uses backend-redis.n- Update Redis configuration so it uses backend-redis.n- Update Redis configuration so it uses backend-redis.n- Update Redis configuration so it uses backend-redis.n- Update Redis configuration so it uses backend-redis.n- Update Redis configuration so it uses backend-redis.n- Update Redis configuration so it uses backend-redis.n- Update Redis configuration so"}
{"File Name":"drt-v2\/0013-use-lihaoyi-s-autowire.md","Context":"## Context\nWe've got a Single Page app, it needs to talk to the server. Our use of scala and scalajs means we can use [lihaoyi's autowire\nmacros](https:\/\/github.com\/lihaoyi\/autowire)\nAlthough this is essentially a 0 on the [Richardson maturity model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\nit has huge benefits in terms of speed of change. We also (at the moment) only have the one client of the SPA so we can afford the tight coupling.\nIt doesn't preclude moving toward something more restful, as we can just add routes when we recognise a need.\n","Decision":"Use autowire for now.\\n","tokens":147,"id":1907,"text":"## Context\\nWe've got a Single Page app, it needs to talk to the server. Our use of scala and scalajs means we can use [lihaoyi's autowire\\nmacros](https:\/\/github.com\/lihaoyi\/autowire)\\nAlthough this is essentially a 0 on the [Richardson maturity model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nit has huge benefits in terms of speed of change. We also (at the moment) only have the one client of the SPA so we can afford the tight coupling.\\nIt doesn't preclude moving toward something more restful, as we can just add routes when we recognise a need.\\n\n\n##Decision\nUse autowire for now.\\n","Predictions":"We will use Next.js for now.n"}
{"File Name":"twig-infrastructure\/0003-start-with-aws-codepipeline-and-codebuild.md","Context":"## Context\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer\/simpler riglet flavor\nand put newer approaches to the test.\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":40,"id":350,"text":"## Context\\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer\/simpler riglet flavor\\nand put newer approaches to the test.\\n\n\n##Decision\n* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","Predictions":"* We will use a Schema library to create a new Schema component.n* We will use a Schema library to create a new Schema component.n"}
{"File Name":"verify-onboarding-prototypes\/0006-we-will-build-a-js-client.md","Context":"## Context\nAt least one user is currently using node js and passport. We want to provide as\nfrictionless as possible an integration for them.\nOther users will be using other languages and frameworks.\n","Decision":"We will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","tokens":43,"id":577,"text":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n\n\n##Decision\nWe will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","Predictions":"We will initially build only a node \/ passport client. We will want to buildnanother client in another language as soon as possible to make sure the APInis well designed.nUsers should also be able to interact with the API directly if we haven't built an appropriate client for their use case.n"}
{"File Name":"mymove\/0061-use-opentelemetry-for-distributed-tracing.md","Context":"## Problem Statement\n### Why Distributed Tracing?\n> Observability is defined as the ability of the internal states of a system to\nbe determined by its external outputs.\nThere are several established techniques for gaining observability into complex\nsoftware systems.\n* Logging\n* helpful for known knowns\n* e.g. \"I want to know when a specific condition is reached\"\n* Metrics\n* helpful for assessing known unknowns\n* e.g. \"How many requests per second was the system handling last Tuesday?\"\n* Distributed Tracing\n* helpful for learning about unknown unknowns\n* e.g. \"What was the execution context for User X that caused their\ninteraction to timeout last Tuesday?\"\nSome of the benefits of distributed tracing, as outlined in\n[this](https:\/\/petabridge.com\/blog\/why-use-distributed-tracing\/) article are:\n* radically improves developer productivity and output\n* works across multiple applications, programming languages, and transports\n* improve time to market\n* facilitates excellent cross-team communication and cooperation\nHere are several example scenarios or questions that distributed tracing can\nhelp answer.\n* As a new engineer on the team, I want to understand how many separate systems\nare involved when a certain user type logs in and the first page is rendered.\n* As an operations engineer, I want to know how many SQL queries are executed\nfor a given endpoint or interaction.\n* As a product manager, I want to know if a new feature is being used by a\ncertain cohort of users on a regular basis.\n* As an engineer, I want to prove that an optimization I wrote is effective\nin a production environment.\n* As a load tester, after I have shown that a problem exists, I want to\nunderstand how the system is interacting so I can debug and fix the issue.\n### ADR Goals and Anti-goals\n* Goal: Choose which set of libraries to use at callsites (across programming\nlanguages) within the MilMove codebase, which will be used to generate\ndistributed tracing data\n* Anti-goal: Committing to a specific \"backend\", i.e. platform or service for\ngathering, exploring, and displaying trace information\n* Sub-goal: Leaving open as many options as possible for the backend\n","Decision":"* Chosen Alternative: _Use OpenTelemetry_\\n* OpenTelemetry is an emerging industry standard\\n* vendors find benefit of being in the OpenTelemetry ecosystem because they\\nno longer have to create or support instrumentation libraries in an ever\\ngrowing array of languages, i.e. as soon as language library exists for\\nOpenTelemetry, the vendors automatically become available to support that\\ngiven language.\\n* OpenTelemetry is vendor agnostic\\n* tracing information can be sent to hosted services (e.g. Honeycomb.io, AWS\\nX-Ray, etc) or self-hosted Open Source implementations (e.g. Zipkin, Jaeger,\\netc)\\n* if left unconfigured, OpenTelemetry instrumentation calls default to\\nlightweight\/noop executions\\n* OpenTelemetry has well-maintained libraries for the languages used in the\\nlayers of the MilMove project\\n* i.e. Go (back-end); JavaScript (front-end); Python (load testing); etc\\n* Easily swappable back-ends\\n* e.g. could choose a local Docker version of OpenZipkin for an all-local\\ndevelopment environment\\n* e.g. can use Honeycomb.io in the experimental commercial-cloud hosted\\nenvironment\\n* e.g. can swap in AWS X-Ray for use in GovCloud hosted environments\\n* Cons\\n* as an abstraction layer, OpenTelemetry may prohibit usage of vendor-\\nspecific capabilities\\n* some OpenTelemetry libraries and tools may trail their vendor-supported\\ncounterparts\\n* instrumentation for tracing may be a vector for performance overhead\\n","tokens":475,"id":3081,"text":"## Problem Statement\\n### Why Distributed Tracing?\\n> Observability is defined as the ability of the internal states of a system to\\nbe determined by its external outputs.\\nThere are several established techniques for gaining observability into complex\\nsoftware systems.\\n* Logging\\n* helpful for known knowns\\n* e.g. \"I want to know when a specific condition is reached\"\\n* Metrics\\n* helpful for assessing known unknowns\\n* e.g. \"How many requests per second was the system handling last Tuesday?\"\\n* Distributed Tracing\\n* helpful for learning about unknown unknowns\\n* e.g. \"What was the execution context for User X that caused their\\ninteraction to timeout last Tuesday?\"\\nSome of the benefits of distributed tracing, as outlined in\\n[this](https:\/\/petabridge.com\/blog\/why-use-distributed-tracing\/) article are:\\n* radically improves developer productivity and output\\n* works across multiple applications, programming languages, and transports\\n* improve time to market\\n* facilitates excellent cross-team communication and cooperation\\nHere are several example scenarios or questions that distributed tracing can\\nhelp answer.\\n* As a new engineer on the team, I want to understand how many separate systems\\nare involved when a certain user type logs in and the first page is rendered.\\n* As an operations engineer, I want to know how many SQL queries are executed\\nfor a given endpoint or interaction.\\n* As a product manager, I want to know if a new feature is being used by a\\ncertain cohort of users on a regular basis.\\n* As an engineer, I want to prove that an optimization I wrote is effective\\nin a production environment.\\n* As a load tester, after I have shown that a problem exists, I want to\\nunderstand how the system is interacting so I can debug and fix the issue.\\n### ADR Goals and Anti-goals\\n* Goal: Choose which set of libraries to use at callsites (across programming\\nlanguages) within the MilMove codebase, which will be used to generate\\ndistributed tracing data\\n* Anti-goal: Committing to a specific \"backend\", i.e. platform or service for\\ngathering, exploring, and displaying trace information\\n* Sub-goal: Leaving open as many options as possible for the backend\\n\n\n##Decision\n* Chosen Alternative: _Use OpenTelemetry_\\n* OpenTelemetry is an emerging industry standard\\n* vendors find benefit of being in the OpenTelemetry ecosystem because they\\nno longer have to create or support instrumentation libraries in an ever\\ngrowing array of languages, i.e. as soon as language library exists for\\nOpenTelemetry, the vendors automatically become available to support that\\ngiven language.\\n* OpenTelemetry is vendor agnostic\\n* tracing information can be sent to hosted services (e.g. Honeycomb.io, AWS\\nX-Ray, etc) or self-hosted Open Source implementations (e.g. Zipkin, Jaeger,\\netc)\\n* if left unconfigured, OpenTelemetry instrumentation calls default to\\nlightweight\/noop executions\\n* OpenTelemetry has well-maintained libraries for the languages used in the\\nlayers of the MilMove project\\n* i.e. Go (back-end); JavaScript (front-end); Python (load testing); etc\\n* Easily swappable back-ends\\n* e.g. could choose a local Docker version of OpenZipkin for an all-local\\ndevelopment environment\\n* e.g. can use Honeycomb.io in the experimental commercial-cloud hosted\\nenvironment\\n* e.g. can swap in AWS X-Ray for use in GovCloud hosted environments\\n* Cons\\n* as an abstraction layer, OpenTelemetry may prohibit usage of vendor-\\nspecific capabilities\\n* some OpenTelemetry libraries and tools may trail their vendor-supported\\ncounterparts\\n* instrumentation for tracing may be a vector for performance overhead\\n","Predictions":"* Easily integrates with other applicationsn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works across multiple languagesn* Works"}
{"File Name":"docspell\/0009_convert_office_docs.md","Context":"# Context and Problem Statement\nHow can office documents, like `docx` or `odt` be converted into a PDF\nfile that looks as much as possible like the original?\nIt would be nice to have a java-only solution. But if an external tool\nhas a better outcome, then an external tool is fine, too.\nSince Docspell is free software, the tools must also be free.\n# Considered Options\n* [Apache POI](https:\/\/poi.apache.org) together with\n[this](https:\/\/central.sonatype.com\/artifact\/fr.opensagres.xdocreport\/org.apache.poi.xwpf.converter.pdf\/1.0.6)\nlibrary\n* [pandoc](https:\/\/pandoc.org\/) external command\n* [abiword](https:\/\/www.abisource.com\/) external command\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\nTo choose an option, some documents are converted to pdf and compared.\nOnly the formats `docx` and `odt` are considered here. These are the\nmost used formats. They have to look well, if a `xlsx` or `pptx`\ndoesn't look so great, that is ok.\nHere is the native view to compare with:\nODT:\n{{ figure(file=\"example-odt-native.jpg\") }}\n","Decision":"Unoconv.\\nThe results from `unoconv` are really good.\\nAbiword also is not that bad, it didn't convert the chart, but all\\nfont markup is there. It would be great to not depend on something as\\nbig as libreoffice, but the results are so much better.\\nAlso pandoc deals very well with DOCX files (using the `context`\\nengine). The only thing that was not rendered was the embedded chart\\n(like abiword). But all images and font styling was present.\\nIt will be a configurable external command anyways, so users can\\nexchange it at any time with a different one.\\n","tokens":288,"id":284,"text":"# Context and Problem Statement\\nHow can office documents, like `docx` or `odt` be converted into a PDF\\nfile that looks as much as possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [Apache POI](https:\/\/poi.apache.org) together with\\n[this](https:\/\/central.sonatype.com\/artifact\/fr.opensagres.xdocreport\/org.apache.poi.xwpf.converter.pdf\/1.0.6)\\nlibrary\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [abiword](https:\/\/www.abisource.com\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nTo choose an option, some documents are converted to pdf and compared.\\nOnly the formats `docx` and `odt` are considered here. These are the\\nmost used formats. They have to look well, if a `xlsx` or `pptx`\\ndoesn't look so great, that is ok.\\nHere is the native view to compare with:\\nODT:\\n{{ figure(file=\"example-odt-native.jpg\") }}\\n\n\n##Decision\nUnoconv.\\nThe results from `unoconv` are really good.\\nAbiword also is not that bad, it didn't convert the chart, but all\\nfont markup is there. It would be great to not depend on something as\\nbig as libreoffice, but the results are so much better.\\nAlso pandoc deals very well with DOCX files (using the `context`\\nengine). The only thing that was not rendered was the embedded chart\\n(like abiword). But all images and font styling was present.\\nIt will be a configurable external command anyways, so users can\\nexchange it at any time with a different one.\\n","Predictions":"* [Apache POI](https:\/\/poi.apache.org) together withn[this](https:\/\/central.sonatype.com\/artifact\/fr.opensagres.xdocreport\/org.apache.poi.xwpf.converter.pdf\/1.0.6)nlibraryn* [pandoc](https:\/\/pandoc.org\/) external commandn* [abiword](https:\/\/www.abisource.com\/) external commandn* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external commandnTo choose an option, some documents are converted to pdf and compared.nOnly the formats docx and odt are considered here. These are thenmost used formats. They have to look well, if a xlsx or pptxndoesn't look so great, that is ok, that is ok, too.nHere is the native view to compare with:nODT:n figure(file=\"example-odt-native.jpg\") n unoconv.nThe results from unoconv are really good.nAbiword also is not that bad, it didn't convert the chart, but allnfont markup is there. It would be great to not depend on something asnbig as libreoffice, but the results are so much better.nAlso pandoc deals very well with docx files (using the contextengine). The only thing that was not rendered was the embedded chart. The only thing that was not rendered was the embedded chart.nAlso pandoc deals very well with docx files (using the contextengine). The only thing that was not rendered was the embedded chart.nThe only thing that was not rendered was the embedded chart.nThe only thing that was not rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was not rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was not rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was the embedded chart.nThe only thing that was rendered was"}
{"File Name":"libelektra\/elektra_web.md","Context":"## Problem\nFor Elektra Web, there needs to be a way to remotely manage instances and groups\nof instances (clusters). The remote configuration of a single instance is\nsimple. However, to manage multiple instances, we need to store the information\nto access the daemons, as well as information about the grouping (clusters) of\ndaemons.\n","Decision":"Use one cluster daemon (clusterd) to manage all clusters and instances.\\n- [Elektra Web Recursive Structure decision](elektra_web_recursive.md)\\n","tokens":75,"id":1306,"text":"## Problem\\nFor Elektra Web, there needs to be a way to remotely manage instances and groups\\nof instances (clusters). The remote configuration of a single instance is\\nsimple. However, to manage multiple instances, we need to store the information\\nto access the daemons, as well as information about the grouping (clusters) of\\ndaemons.\\n\n\n##Decision\nUse one cluster daemon (clusterd) to manage all clusters and instances.\\n- [Elektra Web Recursive Structure decision](elektra_web_recursive.md)\\n","Predictions":"Use one cluster daemon (clusterd) to manage all clusters and instances.n- [Elektra Web Recursive Structure decision](elektra_web_recursive.md)n"}
{"File Name":"katas\/000-use-adrs.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4333,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this\\narticle: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"offender-management-architecture-decisions\/0003-use-progressive-enhancement.md","Context":"## Context\nThe service manual clearly states that teams should use progressive enhancement\nwhen building services: https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancement\nThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/service-standard\/create-a-service-thats-simple),\nwhich is about ensuring that all users can use the service successfully the\nfirst time they try - including users with accessibility needs.\nThe service manual is also clear that [internal services should be held to the\nsame standard as public-facing services](https:\/\/www.gov.uk\/service-manual\/design\/services-for-government-users).\nSome of the services for prison and probation staff which have been built over\nthe last couple of years are not progressively enhanced. Without JavaScript\nthey display no content.\nSince these services are in a similar space to our work and have overlapping\nuser bases with ours (although they are not the only existing services in this\nspace), we have considered whether we should take a similar approach to them.\n","Decision":"We will use progressive enhancement for all our user-facing applications.\\n","tokens":210,"id":263,"text":"## Context\\nThe service manual clearly states that teams should use progressive enhancement\\nwhen building services: https:\/\/www.gov.uk\/service-manual\/technology\/using-progressive-enhancement\\nThis supports [point 12 of the service standard](https:\/\/www.gov.uk\/service-manual\/service-standard\/create-a-service-thats-simple),\\nwhich is about ensuring that all users can use the service successfully the\\nfirst time they try - including users with accessibility needs.\\nThe service manual is also clear that [internal services should be held to the\\nsame standard as public-facing services](https:\/\/www.gov.uk\/service-manual\/design\/services-for-government-users).\\nSome of the services for prison and probation staff which have been built over\\nthe last couple of years are not progressively enhanced. Without JavaScript\\nthey display no content.\\nSince these services are in a similar space to our work and have overlapping\\nuser bases with ours (although they are not the only existing services in this\\nspace), we have considered whether we should take a similar approach to them.\\n\n\n##Decision\nWe will use progressive enhancement for all our user-facing applications.\\n","Predictions":"We will use progressive enhancement for all our user-facing applications.n"}
{"File Name":"platform\/2021-09-06-make-core-mail-templates-independent-from-storefront-urls.md","Context":"## Context\nSome mail templates of the core component (Newsletter, Registration, Password Recovery, Order Status mails) depend on storefront Urls to be included in the mails.\nThose Urls are not available when shopware is used in \"headless\" mode, without the storefront bundle being installed.\nFor some mails (Newsletter subscription, Double Opt-In, Password recovery), the Url was made configurable over the system config and over the settings inside the administration.\nThe default values for those Urls are the ones that the storefront bundle would use.\nThis option does not really scale well as each Url that should be used, needs to be configurable in the administration and this can grow quickly out of hand.\nAdditionally, it is not clear when and where those configs should be used to generate the absolute Urls, as with the BusinessEvent system and the upcoming FlowBuilder, the sending of mails is not necessarily triggered by the same entry point all the times, but different trigger can lead to sending the same mails.\n","Decision":"There shouldn't be any links generated on PHP-side as that can be hard to override per sales-channel and can not easily be changed by apps, and links should be generated inside the mailTemplates with string concatenation instead of `raw_url`-twig functions, so the links can still be generated even if the route is not registered in the system.\\nTo make generation of urls inside the mail templated easier, we will add a `{{ domain }}` variable to the twig context, that contains the domain of the current salesChannelContext, of the order in question etc.\\nThe URLs we use in the core mail templates become part of the public API, and custom frontends should adhere to theme and provide routes under the same path, or create redirects so that the default URLs work for their frontend implementation.\\nThe default urls are:\\n```\\n\/account\/order\/{deepLinkCode} -> opens the order details of the given order\\n\/account\/recover\/password?hash={recoverHash} -> start password recovery process\\n\/newsletter-subscribe?em={emailHash}&hash={subscribeHash} -> Subscribe email with given hash to the newsletter (for douple-opt in)\\n\/registration\/confirm?em={emailHash}&hash={subscribeHash} -> Confirm registration for user eith the given mail hash (for douple-opt in)\\n```\\nIf the custom frontends can't or don't want to use our default URLs they can use the possibility to override the existing mail templates to generate custom URLs.\\nWe will deprecate the usage of the system-config configuration values and the events thrown when the links are generated on PHP-side and remove those in the next major version.\\nTo be forward compatible we will already pass the necessary data needed for generating the links into the templates, so the urls can be already generated inside the mail templates.\\nThird party clients (like the PWA) should either adhere to our default URLs or add additional mail templates, that generate the correct urls for their client.\\nIn addition to that the third party client could extend the core mail template, rather than providing a new one, and then deciding in an `IF\/ELSE` what url needs to be generated based on the salesChannel or domain.\\n","tokens":204,"id":4500,"text":"## Context\\nSome mail templates of the core component (Newsletter, Registration, Password Recovery, Order Status mails) depend on storefront Urls to be included in the mails.\\nThose Urls are not available when shopware is used in \"headless\" mode, without the storefront bundle being installed.\\nFor some mails (Newsletter subscription, Double Opt-In, Password recovery), the Url was made configurable over the system config and over the settings inside the administration.\\nThe default values for those Urls are the ones that the storefront bundle would use.\\nThis option does not really scale well as each Url that should be used, needs to be configurable in the administration and this can grow quickly out of hand.\\nAdditionally, it is not clear when and where those configs should be used to generate the absolute Urls, as with the BusinessEvent system and the upcoming FlowBuilder, the sending of mails is not necessarily triggered by the same entry point all the times, but different trigger can lead to sending the same mails.\\n\n\n##Decision\nThere shouldn't be any links generated on PHP-side as that can be hard to override per sales-channel and can not easily be changed by apps, and links should be generated inside the mailTemplates with string concatenation instead of `raw_url`-twig functions, so the links can still be generated even if the route is not registered in the system.\\nTo make generation of urls inside the mail templated easier, we will add a `{{ domain }}` variable to the twig context, that contains the domain of the current salesChannelContext, of the order in question etc.\\nThe URLs we use in the core mail templates become part of the public API, and custom frontends should adhere to theme and provide routes under the same path, or create redirects so that the default URLs work for their frontend implementation.\\nThe default urls are:\\n```\\n\/account\/order\/{deepLinkCode} -> opens the order details of the given order\\n\/account\/recover\/password?hash={recoverHash} -> start password recovery process\\n\/newsletter-subscribe?em={emailHash}&hash={subscribeHash} -> Subscribe email with given hash to the newsletter (for douple-opt in)\\n\/registration\/confirm?em={emailHash}&hash={subscribeHash} -> Confirm registration for user eith the given mail hash (for douple-opt in)\\n```\\nIf the custom frontends can't or don't want to use our default URLs they can use the possibility to override the existing mail templates to generate custom URLs.\\nWe will deprecate the usage of the system-config configuration values and the events thrown when the links are generated on PHP-side and remove those in the next major version.\\nTo be forward compatible we will already pass the necessary data needed for generating the links into the templates, so the urls can be already generated inside the mail templates.\\nThird party clients (like the PWA) should either adhere to our default URLs or add additional mail templates, that generate the correct urls for their client.\\nIn addition to that the third party client could extend the core mail template, rather than providing a new one, and then deciding in an `IF\/ELSE` what url needs to be generated based on the salesChannel or domain.\\n","Predictions":"We will use the Security property to store the data in the Security property.n"}
{"File Name":"mediawiki-extensions-Popups\/0008-enable-minification-of-bundle-with-uglifyjs.md","Context":"## Context\nSources are currently minified by JSMin producing a bundle bigger than\nnecessary. We want to produce a JS bundle as small as possible to ship less\ncode to clients.\nGiven we have a pre-compilation step in Popups, we can leverage node based\ntooling without penalty, so using UglifyJS to minify the code is an option.\nWe performed some analysis to see the differences in asset size between the two\ntools. See:\n* [Minifying assets with uglifyjs][1]\nResults with the current codebase as of today 12th of May 2017 are that uglify\nproduces a 40% smaller bundle, and a 25% smaller bundle when using gzip.\nThis results are stable through the last 3 months, the same test has been\nperformed multiple times. See [Compressed JS comparison][2]\n","Decision":"Webpack's production mode has been enabled for `npm run build`, and thus the\\ncompiled bundle will be minified in production with UglifyJS.\\nSee [Enable production settings for the production bundle][3]\\n","tokens":189,"id":3428,"text":"## Context\\nSources are currently minified by JSMin producing a bundle bigger than\\nnecessary. We want to produce a JS bundle as small as possible to ship less\\ncode to clients.\\nGiven we have a pre-compilation step in Popups, we can leverage node based\\ntooling without penalty, so using UglifyJS to minify the code is an option.\\nWe performed some analysis to see the differences in asset size between the two\\ntools. See:\\n* [Minifying assets with uglifyjs][1]\\nResults with the current codebase as of today 12th of May 2017 are that uglify\\nproduces a 40% smaller bundle, and a 25% smaller bundle when using gzip.\\nThis results are stable through the last 3 months, the same test has been\\nperformed multiple times. See [Compressed JS comparison][2]\\n\n\n##Decision\nWebpack's production mode has been enabled for `npm run build`, and thus the\\ncompiled bundle will be minified in production with UglifyJS.\\nSee [Enable production settings for the production bundle][3]\\n","Predictions":"We will use Rail to manage our application errors.n"}
{"File Name":"frontend-library\/0003-use-brunch.md","Context":"## Context\nThe styling of sites developed for NHSUK need to be as consistent as possible.\nCurrently each new site starts by copying what the last developed site had done\nand building on that. This results in the newer sites looking different than\nthe older sites, which would not be a problem if each site was easily updated\nto use the same, most recent, set of styles. Currently there is no central\nstore for the styles which means there is no easy way to find and use them\neither.\nThe ability to review the styles in a development environment before they are\nmade available to a wider audience is important. As is the ability to use well\nknown technologies such as [SASS](http:\/\/sass-lang.com\/).\n","Decision":"We will use [Brunch](http:\/\/brunch.io\/) to accomplish a number of tasks\\nincluding:\\n* Running the site used in the development environment\\n* Generating the compiled assets\\n","tokens":153,"id":783,"text":"## Context\\nThe styling of sites developed for NHSUK need to be as consistent as possible.\\nCurrently each new site starts by copying what the last developed site had done\\nand building on that. This results in the newer sites looking different than\\nthe older sites, which would not be a problem if each site was easily updated\\nto use the same, most recent, set of styles. Currently there is no central\\nstore for the styles which means there is no easy way to find and use them\\neither.\\nThe ability to review the styles in a development environment before they are\\nmade available to a wider audience is important. As is the ability to use well\\nknown technologies such as [SASS](http:\/\/sass-lang.com\/).\\n\n\n##Decision\nWe will use [Brunch](http:\/\/brunch.io\/) to accomplish a number of tasks\\nincluding:\\n* Running the site used in the development environment\\n* Generating the compiled assets\\n","Predictions":"We will use [Semantic Styles](http:\/\/semantic-styles.com\/) to achieve a number of tasksnincluding:n* Running the site used in the development environmentn* Generating the compiled assetsn"}
{"File Name":"backdrop\/adr-002-persistent-storage.md","Context":"# Context\nThe way the Government Digital Service (GDS) [makes technology choices is\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which technology will to use to provide\npersistence for the Performance Platform.\nGDS has experience in running MongoDB and MySQL in production.\nWe envisage the Performance Platform as taking in unstructured data from a\nvariety of data sources (spreadsheets, analytics, logs, other databases and\napplications) and allowing people to collect this data in a single place. This\nshould enable service managers to:\n- make comparisons\n- see how well their service is performing\n- see how the performance changes over time, as they iterate the service\nSo we want a persistent data store that will store unstructured data, and\nallow us to apply a structure either by post-processing the data, or at query\ntime.\nThe volume of the data that we are envisaging at this stage is pretty small.\nWe will be building a small thing to start; as we learn more about the\nuser needs and problem space, then we will revisit this decision. Since the\nvolume is small, it does not seem likely that we need Hadoop \/ HDFS or\nCassandra.\nWe are not the canonical source of this data. We are an aggregator; the\ncanonical source remains the data sources which will be providing feeds or\npushing the data into the Performance Platform.\nBecause of this position, we do not need ACID properties for this data, nor\nneed worry about the CAP theorem in any detail.\n# Decision\nWe will use MongoDB. We are comfortable operating it in production,\nit will allow unstructured data (in the form of JSON documents) and we can\napply structure at query time.\n# Status\nAccepted.\n# Consequences\nUse MongoDB with an appropriate replica-set configuration.\n","Decision":"We will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n","tokens":403,"id":4266,"text":"# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop \/ HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n\n\n##Decision\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n","Predictions":"We will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status database to store the data we need tonprocess.nWe will use a Status"}
{"File Name":"origin\/0005-simplify-off-chain-storage.md","Context":"## Context\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\n","Decision":"We decided to store off-chain data in a way which doesn't reference the ID of any of the entities, but purely stores the precise proof data under the hash of the off-chain data.\\n","tokens":52,"id":1558,"text":"## Context\\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\\n\n\n##Decision\nWe decided to store off-chain data in a way which doesn't reference the ID of any of the entities, but purely stores the precise proof data under the hash of the off-chain data.\\n","Predictions":"We decided to store off-chain data in a way which doesn't reference the ID of any of the entities, but purely stores the precise proof data under the hash of the off-chain data.n"}
{"File Name":"educational-platform\/0009-architecture-tests.md","Context":"## Context\nWe need to have the mechanism for supporting and validating common architecture principles in all application.\n","Decision":"Architecture tests with using Archunit should be implemented.\\n","tokens":21,"id":2502,"text":"## Context\\nWe need to have the mechanism for supporting and validating common architecture principles in all application.\\n\n\n##Decision\nArchitecture tests with using Archunit should be implemented.\\n","Predictions":"Architecture tests with using Archunit should be implemented.n"}
{"File Name":"molgenis-frontend\/0003-have-app-create-settings.md","Context":"## Context\nApps (for example the data-explorer-2 or the biobank-explorer) may need settings data to function properly. As these settings are dynamic (may be changed at run time), and may change between deployments these settings need to be stored somewhere (molgenis database). This poses two problems; 1) how to link the app with the settings data, 2) how to control access to the settings.\n","Decision":"We leave it up to the app to create the settings entity. Each app creates 0 or 1 settings entities. By default everyone can read the settings data. Depending on the app the entity contains 1 or more rows ( in case of multiple rows the app is responsible using the correct row(s) in each context).\\nRejected alternatives:\\n- Add settings via bootstrapped entity in molgenis core; This was deemed as insufficiently flexible due to the dependance on Java knowledge and (possible major version) core release.\\n- Use of app manager; App-manager does not facilitate an automated deployment process. App manager restricts the app architecture and settings structure.\\n- Use of entity meta data: Does not allow for settings per 'entity and app' combination. For instance the dataexplorer needs n settings rows or n tables.\\n- Proxy settings request to external repository; Deemed as hard to manage, app and proxy settings need to be kept in sync, also requires knowledge of proxy (nginx, apache) to configure.\\n","tokens":90,"id":1514,"text":"## Context\\nApps (for example the data-explorer-2 or the biobank-explorer) may need settings data to function properly. As these settings are dynamic (may be changed at run time), and may change between deployments these settings need to be stored somewhere (molgenis database). This poses two problems; 1) how to link the app with the settings data, 2) how to control access to the settings.\\n\n\n##Decision\nWe leave it up to the app to create the settings entity. Each app creates 0 or 1 settings entities. By default everyone can read the settings data. Depending on the app the entity contains 1 or more rows ( in case of multiple rows the app is responsible using the correct row(s) in each context).\\nRejected alternatives:\\n- Add settings via bootstrapped entity in molgenis core; This was deemed as insufficiently flexible due to the dependance on Java knowledge and (possible major version) core release.\\n- Use of app manager; App-manager does not facilitate an automated deployment process. App manager restricts the app architecture and settings structure.\\n- Use of entity meta data: Does not allow for settings per 'entity and app' combination. For instance the dataexplorer needs n settings rows or n tables.\\n- Proxy settings request to external repository; Deemed as hard to manage, app and proxy settings need to be kept in sync, also requires knowledge of proxy (nginx, apache) to configure.\\n","Predictions":"We leave it up to the app to create the settings entity. Each app creates 0 or 1 settings entities. By default everyone can read the settings data. Depending on the app the entity contains 1 or more rows ( in case of multiple rows the app is responsible using the correct row(s) in each context).nRejected alternatives:n- Add settings via bootstrapped entity in molgenis core; This was deemed as insufficiently flexible due to the depended on Java knowledge and (possible major version) core release.n- Use of app manager; App-manager does not facilitate an automated deployment process. App manager restricts the app architecture and settings structure.n- Use of entity meta data: Does not allow for settings per 'entity and app' combination. For instance the dataexplorer needs n settings rows or n tables.n- Proxy settings request to external repository; Deemed as hard to manage, app and proxy settings need to be kept in sync, also requires knowledge of proxy (nginx, apache) to configure.n"}
{"File Name":"paas-team-manual\/ADR002-concourse-pool-resource.html.md","Context":"## Context\nWhen building pipelines using concourse, we investigated using the [pool\nresource](https:\/\/github.com\/concourse\/pool-resource) in order to control flow\nthrough jobs. This was an alternative to the use of the\n[semver resource](https:\/\/github.com\/concourse\/semver-resource).\nThese 2 resources are both workarounds to solve the problem of triggering jobs\nwhen we haven't made changes to a resource.\nThe problem is that the pool resource relies on write access to a github repo,\nwhich means we must pass public keys that allow this access into the pipeline\nand deployed concourse instance - we want to minimise the number of credentials\nwe pass, and the semver resource relies on AWS credentials that are already\npassed.\n","Decision":"We will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\n","tokens":158,"id":210,"text":"## Context\\nWhen building pipelines using concourse, we investigated using the [pool\\nresource](https:\/\/github.com\/concourse\/pool-resource) in order to control flow\\nthrough jobs. This was an alternative to the use of the\\n[semver resource](https:\/\/github.com\/concourse\/semver-resource).\\nThese 2 resources are both workarounds to solve the problem of triggering jobs\\nwhen we haven't made changes to a resource.\\nThe problem is that the pool resource relies on write access to a github repo,\\nwhich means we must pass public keys that allow this access into the pipeline\\nand deployed concourse instance - we want to minimise the number of credentials\\nwe pass, and the semver resource relies on AWS credentials that are already\\npassed.\\n\n\n##Decision\nWe will not use the pool resource for flow between jobs - instead we will use\\nthe semver resource\\n","Predictions":"We will not use the pool resource for flow between jobs - instead we will usenthe semver resourcen"}
{"File Name":"libelektra\/empty_files.md","Context":"## Problem\nAn empty KeySet is passed to kdbSet(). What is the correct persistent\nrepresentation?\n","Decision":"Remove files on empty KeySet.\\n","tokens":23,"id":1271,"text":"## Problem\\nAn empty KeySet is passed to kdbSet(). What is the correct persistent\\nrepresentation?\\n\n\n##Decision\nRemove files on empty KeySet.\\n","Predictions":"We will use stack to create a new stack in the stack directory.n"}
{"File Name":"adrflow\/8-Remove_ADR_Util_Sync_Module.md","Context":"supersedes [5](5-Utility_Context_Lazily_Initialized.md)\nmentions [6](6-Centralize_Definition_of_Filename.md)\n## Context\nThe ADR Util module grew as an attempt to centralize a list of utility functions that require the context of the ADR file and directory.\nThe resulting module was very central to the logic of the entire program, but resulted in a module that had too much differing concerns (too many responsibilities\/reasons to change).\nThis has eventually resulted in tangled code, and sometimes to (unintentional) circular dependencies, e.g. as when trying to isolate the handling of links.\nAt the same time, the usage of the shared context - ADR files and directory - hasn't proved that useful in most case.\nIn the cases where it should be used, it can be defined and cached.\n","Decision":"Divide the utilities defined in this module to other, more focused modules (under `core` directory):\\n1. `files.js`: handles all file reading and writing\\n2. `adrobj.js`: handles all ADR logical-level operations. Essentially linking the ID to the file.\\n3. `links.js`: handles logic around linking ADRs\\nIn cases where caching the ADR files is needed, e.g. in `adrobj.js`, it is done in that module.\\n","tokens":180,"id":3637,"text":"supersedes [5](5-Utility_Context_Lazily_Initialized.md)\\nmentions [6](6-Centralize_Definition_of_Filename.md)\\n## Context\\nThe ADR Util module grew as an attempt to centralize a list of utility functions that require the context of the ADR file and directory.\\nThe resulting module was very central to the logic of the entire program, but resulted in a module that had too much differing concerns (too many responsibilities\/reasons to change).\\nThis has eventually resulted in tangled code, and sometimes to (unintentional) circular dependencies, e.g. as when trying to isolate the handling of links.\\nAt the same time, the usage of the shared context - ADR files and directory - hasn't proved that useful in most case.\\nIn the cases where it should be used, it can be defined and cached.\\n\n\n##Decision\nDivide the utilities defined in this module to other, more focused modules (under `core` directory):\\n1. `files.js`: handles all file reading and writing\\n2. `adrobj.js`: handles all ADR logical-level operations. Essentially linking the ID to the file.\\n3. `links.js`: handles logic around linking ADRs\\nIn cases where caching the ADR files is needed, e.g. in `adrobj.js`, it is done in that module.\\n","Predictions":"- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Contentn- Con"}
{"File Name":"content-publisher\/0004-editing-microcopy.md","Context":"## Context\nEvery feature we add to the app comes with its own static text, which is either embedded in the code (Ruby or JavaScript) or in the HTML. Static text can be anything from the page title, to the text of a button, to an entire page of guidance.\nWriting text 'inline' makes it hard for us to audit all of strings in our application, some of which can only be seen under special conditions e.g. error messages. It also makes it hard to change strings consistently across the application - a task which has to be done by a developer. Finally, using inline strings in code distracts from the logical flow of that code.\n[Rails Internationalization](https:\/\/guides.rubyonrails.org\/i18n.html) (also referred to as 'translations') are a way to extract all of the strings in the application to a central location in `config\/locales\/en`. The strings can be organized in a hierarchy over one or more files, as below, where we can refer to the reviewed title by writing `I18n.t(\"publish.published.reviewed.title\")`.\n```\n# publish_document\/published.yml\nen:\npublish_document:\npublished:\nreviewed:\ntitle: Content has been published\nbody: |\n\u2018%{title}\u2019 has been published on GOV.UK.\nIt may take 5 minutes to appear live.\n```\nRails translations have a few special behaviours, such as pluralization, raw HTML, and variables. The `%{title}` string in the above is an example of a variable, which a developer will set to the title of the document being published.\n","Decision":"Although we could use translations to extract all of the strings in the application, in some cases we felt this wasn't necessary, or that a different method should be used. The following is a summary of the rules we currently use.\\n* **Link and button labels** are not extracted. We think link and button labels are unlikely to change, and extracting them made the application tests harder to read by obfuscating some of the crucial steps in the test with translation keys.\\n* **Publishing component strings** are not extracted. This ensures we are able to migrate these components to the [govuk_publishing_components](https:\/\/github.com\/alphagov\/govuk_publishing_components) repo, which wouldn't be able to access our local translations.\\n* **Big guidance** is extracted into it's own Markdown files and stored alongside the corresponding HTML page that shows it. For example, the guidance for creating a new document is stored in `app\/views\/new_document\/guidance.md`.\\n* **Domain data** that's static is stored in a number of custom YAML files. This application has two static models (for document types and their selection) that encapsulate domain concepts where the data is part of the application. We have split up domain data based on whether it's used in a backend setting or as a string for use in the frontend. The latter are extracted at the top-level of the translation hierarchy.\\n* **Global strings** (states and validation messages) are extracted using translations. As these strings aren't page-specific, we put them at the top-level of the translation hierarchy (in `states.yml` and `validations.yml`).\\n* **All other strings** are extracted using translations, in a hierarchy that follows the structure of the `app\/views` directory. For example, the above example relates to `app\/views\/publish_document\/published.html.erb`.\\n* **Small amounts of govspeak and HTML** are extracted using translations as for other strings, with '\\_html' or '\\_govspeak' appended to the final component of the key to indicate they support rich text.\\nEvery instance of a string in the tests has been replaced according to the above rules, such that the tests continue to pass when an extracted string is changed. **Link and button labels** are not replaced, as they are not extracted in the code.\\nWe also configured Rails to raise an exception when we hit a page where a translation is missing, as we don't test all of the translations; this is done by setting `config.action_view.raise_on_missing_translations = true` in `application.rb`.\\n","tokens":343,"id":4878,"text":"## Context\\nEvery feature we add to the app comes with its own static text, which is either embedded in the code (Ruby or JavaScript) or in the HTML. Static text can be anything from the page title, to the text of a button, to an entire page of guidance.\\nWriting text 'inline' makes it hard for us to audit all of strings in our application, some of which can only be seen under special conditions e.g. error messages. It also makes it hard to change strings consistently across the application - a task which has to be done by a developer. Finally, using inline strings in code distracts from the logical flow of that code.\\n[Rails Internationalization](https:\/\/guides.rubyonrails.org\/i18n.html) (also referred to as 'translations') are a way to extract all of the strings in the application to a central location in `config\/locales\/en`. The strings can be organized in a hierarchy over one or more files, as below, where we can refer to the reviewed title by writing `I18n.t(\"publish.published.reviewed.title\")`.\\n```\\n# publish_document\/published.yml\\nen:\\npublish_document:\\npublished:\\nreviewed:\\ntitle: Content has been published\\nbody: |\\n\u2018%{title}\u2019 has been published on GOV.UK.\\nIt may take 5 minutes to appear live.\\n```\\nRails translations have a few special behaviours, such as pluralization, raw HTML, and variables. The `%{title}` string in the above is an example of a variable, which a developer will set to the title of the document being published.\\n\n\n##Decision\nAlthough we could use translations to extract all of the strings in the application, in some cases we felt this wasn't necessary, or that a different method should be used. The following is a summary of the rules we currently use.\\n* **Link and button labels** are not extracted. We think link and button labels are unlikely to change, and extracting them made the application tests harder to read by obfuscating some of the crucial steps in the test with translation keys.\\n* **Publishing component strings** are not extracted. This ensures we are able to migrate these components to the [govuk_publishing_components](https:\/\/github.com\/alphagov\/govuk_publishing_components) repo, which wouldn't be able to access our local translations.\\n* **Big guidance** is extracted into it's own Markdown files and stored alongside the corresponding HTML page that shows it. For example, the guidance for creating a new document is stored in `app\/views\/new_document\/guidance.md`.\\n* **Domain data** that's static is stored in a number of custom YAML files. This application has two static models (for document types and their selection) that encapsulate domain concepts where the data is part of the application. We have split up domain data based on whether it's used in a backend setting or as a string for use in the frontend. The latter are extracted at the top-level of the translation hierarchy.\\n* **Global strings** (states and validation messages) are extracted using translations. As these strings aren't page-specific, we put them at the top-level of the translation hierarchy (in `states.yml` and `validations.yml`).\\n* **All other strings** are extracted using translations, in a hierarchy that follows the structure of the `app\/views` directory. For example, the above example relates to `app\/views\/publish_document\/published.html.erb`.\\n* **Small amounts of govspeak and HTML** are extracted using translations as for other strings, with '\\_html' or '\\_govspeak' appended to the final component of the key to indicate they support rich text.\\nEvery instance of a string in the tests has been replaced according to the above rules, such that the tests continue to pass when an extracted string is changed. **Link and button labels** are not replaced, as they are not extracted in the code.\\nWe also configured Rails to raise an exception when we hit a page where a translation is missing, as we don't test all of the translations; this is done by setting `config.action_view.raise_on_missing_translations = true` in `application.rb`.\\n","Predictions":"We will use stack to store all of the strings in the application.n"}
{"File Name":"nhsuk-prototype-kit-version-one\/0001-use-node-and-express.md","Context":"## Context\nThe prototype kit application will need to operate similarly to a live service, depending on the level of fidelity required for testing. This means that a server is required. To make the prototype kit easy to use for different levels of coding experience we want to implement auto routing (or route matching) for template files, the simplest way to achieve this this dynamically is to have an application running on a server.\n","Decision":"We will use Node.js with the Express framework to run a Node.js server for the prototype kit on both local development environments and production environment when published on Heroku. Node.js is written in javascript and is therefore one the most common languages between designers who have some coding experience and software engineers. It is also the easiest runtime to install on both Mac and PC.\\n","tokens":83,"id":3681,"text":"## Context\\nThe prototype kit application will need to operate similarly to a live service, depending on the level of fidelity required for testing. This means that a server is required. To make the prototype kit easy to use for different levels of coding experience we want to implement auto routing (or route matching) for template files, the simplest way to achieve this this dynamically is to have an application running on a server.\\n\n\n##Decision\nWe will use Node.js with the Express framework to run a Node.js server for the prototype kit on both local development environments and production environment when published on Heroku. Node.js is written in javascript and is therefore one the most common languages between designers who have some coding experience and software engineers. It is also the easiest runtime to install on both Mac and PC.\\n","Predictions":"Use Rollbar to collect and manage our application errors.n"}
{"File Name":"klokwrk-project\/0007-git-workflow-with-linear-history.md","Context":"## Context\nThe value of tidy and [semi-linear commit history](https:\/\/fangpenlin.com\/images\/2013-09-30-keep-a-readable-git-history\/source_tree_new_branch_rebase_merge.png) is often overlooked in many Git-based\nprojects. This is unfortunate since non-linear git commit history might be a [horrible mess](https:\/\/tugberkugurlu.blob.core.windows.net\/bloggyimages\/d773c1fe-4db8-4d2f-a994-c60f3f8cb6f0.png) that\ndoes not provide any useful information. We want to use as simple as possible git workflow that promotes and ensures a semi-linear history.\n> * **Semi-linear** commit history usually refers to a history that uses merge commits (git \"no-fast-forward\" merge option) to clearly denote which commits are meant to be together and represent a\n>   coherent whole.\n> * **Linear** commit history usually refers to completely flat history (git default \"fast-forward\") where it is impossible to tell at first glance which commits belong together.\nWhen working on individual features, related git commits can be organized either as \"work log\" or as a \"recipe\". When working in a team, it is crucial that team members and\/or reviewers can easily\ncomprehend what is going on in a particular feature. For this reason, we prefer features to be organized as \"recipes\".\n> * **Work log** style of organizing feature commits refers to the style without any organization. Commits are added solely as they are developed through time.\n> * **Recipe** style of organizing feature commits refers to the style where commits have a sensible organization where peer developers can clearly see and learn how the feature is created. This\n>   style requires some additional work as its primary goal is communication, instead of just implementing a feature.\nVery often, in bigger teams, common git workflows have a problem of broken continuous integration builds. We want to embrace and use as simple as possible workflow that resolves that problem.\nChosen git workflow should seamlessly support release versioning and, if needed, related work on release branches.\n### Architectural Context\n* System (`klokwrk-project`)\n","Decision":"**We will use a [stable mainline branching model for Git](https:\/\/www.bitsnbites.eu\/a-stable-mainline-branching-model-for-git\/).** It\\n[supports semi-linear Git history](https:\/\/www.bitsnbites.eu\/a-tidy-linear-git-history\/) and helps to resolve the problem of broken continuous integration builds.\\n**We will, however, introduce several tweaks to the \"stable mainline branching model\":**\\n* We will use the following naming pattern for feature branches: **`feature_<name>`** instead of `feature\/name`.\\n* We will not use new branches when remote feature branches need rebasing. Instead, we will just inform all collaborators that rebasing is pending for a feature branch. It is important\\nto communicate with collaborators **before** rebasing and force-pushing.\\n* We will not create a release branch for each release. In general, we will just tag a release. However, we will create a release branch when a particular release needs fixing.\\nWe will organize our [feature commits as recipes](https:\/\/www.bitsnbites.eu\/git-history-work-log-vs-recipe\/) because we want to promote the team's learning and communication.\\nWe will also use specific commit message format as described in [ADR-0014 - Commit Message Format](.\/0014-commit-message-format.md)\\n","tokens":461,"id":4899,"text":"## Context\\nThe value of tidy and [semi-linear commit history](https:\/\/fangpenlin.com\/images\/2013-09-30-keep-a-readable-git-history\/source_tree_new_branch_rebase_merge.png) is often overlooked in many Git-based\\nprojects. This is unfortunate since non-linear git commit history might be a [horrible mess](https:\/\/tugberkugurlu.blob.core.windows.net\/bloggyimages\/d773c1fe-4db8-4d2f-a994-c60f3f8cb6f0.png) that\\ndoes not provide any useful information. We want to use as simple as possible git workflow that promotes and ensures a semi-linear history.\\n> * **Semi-linear** commit history usually refers to a history that uses merge commits (git \"no-fast-forward\" merge option) to clearly denote which commits are meant to be together and represent a\\n>   coherent whole.\\n> * **Linear** commit history usually refers to completely flat history (git default \"fast-forward\") where it is impossible to tell at first glance which commits belong together.\\nWhen working on individual features, related git commits can be organized either as \"work log\" or as a \"recipe\". When working in a team, it is crucial that team members and\/or reviewers can easily\\ncomprehend what is going on in a particular feature. For this reason, we prefer features to be organized as \"recipes\".\\n> * **Work log** style of organizing feature commits refers to the style without any organization. Commits are added solely as they are developed through time.\\n> * **Recipe** style of organizing feature commits refers to the style where commits have a sensible organization where peer developers can clearly see and learn how the feature is created. This\\n>   style requires some additional work as its primary goal is communication, instead of just implementing a feature.\\nVery often, in bigger teams, common git workflows have a problem of broken continuous integration builds. We want to embrace and use as simple as possible workflow that resolves that problem.\\nChosen git workflow should seamlessly support release versioning and, if needed, related work on release branches.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n\n##Decision\n**We will use a [stable mainline branching model for Git](https:\/\/www.bitsnbites.eu\/a-stable-mainline-branching-model-for-git\/).** It\\n[supports semi-linear Git history](https:\/\/www.bitsnbites.eu\/a-tidy-linear-git-history\/) and helps to resolve the problem of broken continuous integration builds.\\n**We will, however, introduce several tweaks to the \"stable mainline branching model\":**\\n* We will use the following naming pattern for feature branches: **`feature_<name>`** instead of `feature\/name`.\\n* We will not use new branches when remote feature branches need rebasing. Instead, we will just inform all collaborators that rebasing is pending for a feature branch. It is important\\nto communicate with collaborators **before** rebasing and force-pushing.\\n* We will not create a release branch for each release. In general, we will just tag a release. However, we will create a release branch when a particular release needs fixing.\\nWe will organize our [feature commits as recipes](https:\/\/www.bitsnbites.eu\/git-history-work-log-vs-recipe\/) because we want to promote the team's learning and communication.\\nWe will also use specific commit message format as described in [ADR-0014 - Commit Message Format](.\/0014-commit-message-format.md)\\n","Predictions":"We will use a simple git-log format for logging and renaming.n"}
{"File Name":"gsp\/ADR019-service-mesh.md","Context":"## Context\nVerify have a need to restrict exfiltration of data, enforce strict authentication between microservices and to use mutual TLS.\nA service mesh gives us a way of meeting these needs.\n### Option 1: Istio\nIstio makes it easy to create a network of deployed services with load balancing, service-to-service authentication, monitoring, and more, with few or no code changes in service code.\nIstio supports services by deploying a special sidecar proxy throughout your environment that intercepts all network communication between microservices, you then configure and manage Istio using its control plane functionality, which includes:\n- Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic.\n- Fine-grained control of traffic behaviour with rich routing rules, retries, fail-overs, and fault injection.\n- A pluggable policy layer and configuration API supporting access controls, rate limits and quotas.\n- Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress.\n- Secure service-to-service communication in a cluster with strong identity-based authentication and authorisation.\nPros\/cons:\n- an emerging standard (installed by default on GKE)\n- a large community of contributors\n### Option 2: AWS App Mesh (Istio from AWS)\n[AWS App Mesh](https:\/\/aws.amazon.com\/app-mesh\/) is a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. App Mesh standardizes how your services communicate, giving you end-to-end visibility and ensuring high-availability for your applications\npros\/cons:\n- Unavailable in London region\n- Did not support automatic sidecar injection (meaning service teams would have to add lots of extra configuration to their Deployments)\n- Appears to be abstraction over Istio\n### Option 3: Linkerd 1.x & 2.0\n[Linkerd](https:\/\/linkerd.io\/) is an ultra light service mesh for Kubernetes. It gives you observability, reliability, and security without requiring any code changes.\nPros\/cons:\n- 1.0 has a richer feature set but poorer kubernetes support\n- 2.0 has a very minimal feature set but native kubernetes support\n- Going through major rewrite for improved Kubernetes support\n- Smaller community\n- Fewer features around\n","Decision":"We will use [Istio](https:\/\/istio.io\/) to provide a service mesh in the GDS Supported Platform.\\n","tokens":498,"id":3909,"text":"## Context\\nVerify have a need to restrict exfiltration of data, enforce strict authentication between microservices and to use mutual TLS.\\nA service mesh gives us a way of meeting these needs.\\n### Option 1: Istio\\nIstio makes it easy to create a network of deployed services with load balancing, service-to-service authentication, monitoring, and more, with few or no code changes in service code.\\nIstio supports services by deploying a special sidecar proxy throughout your environment that intercepts all network communication between microservices, you then configure and manage Istio using its control plane functionality, which includes:\\n- Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic.\\n- Fine-grained control of traffic behaviour with rich routing rules, retries, fail-overs, and fault injection.\\n- A pluggable policy layer and configuration API supporting access controls, rate limits and quotas.\\n- Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress.\\n- Secure service-to-service communication in a cluster with strong identity-based authentication and authorisation.\\nPros\/cons:\\n- an emerging standard (installed by default on GKE)\\n- a large community of contributors\\n### Option 2: AWS App Mesh (Istio from AWS)\\n[AWS App Mesh](https:\/\/aws.amazon.com\/app-mesh\/) is a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. App Mesh standardizes how your services communicate, giving you end-to-end visibility and ensuring high-availability for your applications\\npros\/cons:\\n- Unavailable in London region\\n- Did not support automatic sidecar injection (meaning service teams would have to add lots of extra configuration to their Deployments)\\n- Appears to be abstraction over Istio\\n### Option 3: Linkerd 1.x & 2.0\\n[Linkerd](https:\/\/linkerd.io\/) is an ultra light service mesh for Kubernetes. It gives you observability, reliability, and security without requiring any code changes.\\nPros\/cons:\\n- 1.0 has a richer feature set but poorer kubernetes support\\n- 2.0 has a very minimal feature set but native kubernetes support\\n- Going through major rewrite for improved Kubernetes support\\n- Smaller community\\n- Fewer features around\\n\n\n##Decision\nWe will use [Istio](https:\/\/istio.io\/) to provide a service mesh in the GDS Supported Platform.\\n","Predictions":"We will use the [Azure Service Mesh](https:\/\/azure.amazon.com\/app-mesh\/) as a service mesh.n"}
{"File Name":"verify-service-provider\/0008-provide-an-end-to-end-stub.md","Context":"## Context\nWe wish to run regular user research and testing against the prototype Service Provider.\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\nThe area we are most interested in is the interface between the Service and the Hub.\n","Decision":"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","tokens":55,"id":4454,"text":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n\n\n##Decision\nWe will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","Predictions":"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in theirnjourney. Continue from this page will take the user to a page containing a series of possible response scenarios thatncan be selected.nAny information that needs to be stored from the original request or between pages will be stored in a session cookie. The session cookie does not need to be signed or encrypted initially.nSome scenarios will require additional user input such as a providing a pid value or user account creation attributesnthese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should send its responses to.nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SPnprototype.nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.nThe prototype service provider will be able to understand each canned response and produce a specific response to thenService\/client.nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.nThe Stub Verify Hub will provide the following responses:n| Response Scenario | Message |n| -- | --n| -- |n| Successful Match | scenario: SUCCESS_MATCH, levelOfAssurance: $specified loa, pid: $specified PID |n| Account Creation | scenario: ACCOUNT_CREATION, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOf"}
{"File Name":"form-design-system\/zindex-values.md","Context":"## Context and Problem Statement\nOur public-facing application is currently suffering from a \"zindex war\". In a \"zindex\nwar\", there are no winners. Engineers must regularly set insanely high and arbitrary\nzindex values to build our front end features.\n","Decision":"We've decided to go with a 2 tier approach:\\n- Use pre-defined z-index values & ranges that are defined in FDS.\\n- Use values between `1` and `9` for fine layering control (usualy when pre-defined z-index\\nvalues are not useful)\\n- Rely on DOM order to set stacking order for elements of the same `z-index` (for example, a popover menu within a modal)\\n### Use values between `1` and `9` for fine layering control\\nIn cases where one element needs to appear above another, use integers below 10. Rely on\\nsurrounding stacking context to do the rest.\\nFor example, the `ButtonGroup` component needs to manage layering of buttons so that the\\nfocus ring is always visible. The surrounding stacking context does not matter - it uses\\nvalues `1` through `5` to accomplish this.\\nNote: It's helpful to understand what stacking context is to better utilize this approach. `z-index` values [are not absolute](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/CSS\/CSS_Positioning\/Understanding_z_index\/The_stacking_context).\\n### Pre-defined FDS z-index values \/ ranges\\nWe currently have 3 layers: navigation (10), modal (20), and crazy (50).\\n#### Navigation\\nYou want to use navigation z-index for headers, sidebars, etc. Anything that generally serves to \"frame\" the content on a page.\\nFor navigation, you will need often manage the relationship between navigation elements. You can do this by using calc. To note: the navigation z-index should never exceed 19, as these are z-index values for modals.\\n```\\n.mainheader {\\nz-index: calc(var(--zindex-navigation) + 9);\\n}\\n.subheader {\\nz-index: var(--zindex-navigation)\\n}\\n```\\n#### Modal\\nUse modal z-index for shims, popovers, and modals. To control stacking between these, we rely on document order and portaling. Shims come before modals, modals and popovers stack based on which one comes last (which you can help control by portaling, though we can get away with not portaling from time to time).\\n#### Crazy\\nUse crazy for the unexpected situations where you know something should go above everything else. This ideally would never need to be used but is there as an escape hatch and for debugging purposes even.\\n### For standard values, continue using increments of `10`\\nFor base stacking context, standard values are most helpful at increments of 10 because it\\ngives engineers flexibility to insert layers between defined stack levels. For example:\\n","tokens":53,"id":798,"text":"## Context and Problem Statement\\nOur public-facing application is currently suffering from a \"zindex war\". In a \"zindex\\nwar\", there are no winners. Engineers must regularly set insanely high and arbitrary\\nzindex values to build our front end features.\\n\n\n##Decision\nWe've decided to go with a 2 tier approach:\\n- Use pre-defined z-index values & ranges that are defined in FDS.\\n- Use values between `1` and `9` for fine layering control (usualy when pre-defined z-index\\nvalues are not useful)\\n- Rely on DOM order to set stacking order for elements of the same `z-index` (for example, a popover menu within a modal)\\n### Use values between `1` and `9` for fine layering control\\nIn cases where one element needs to appear above another, use integers below 10. Rely on\\nsurrounding stacking context to do the rest.\\nFor example, the `ButtonGroup` component needs to manage layering of buttons so that the\\nfocus ring is always visible. The surrounding stacking context does not matter - it uses\\nvalues `1` through `5` to accomplish this.\\nNote: It's helpful to understand what stacking context is to better utilize this approach. `z-index` values [are not absolute](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/CSS\/CSS_Positioning\/Understanding_z_index\/The_stacking_context).\\n### Pre-defined FDS z-index values \/ ranges\\nWe currently have 3 layers: navigation (10), modal (20), and crazy (50).\\n#### Navigation\\nYou want to use navigation z-index for headers, sidebars, etc. Anything that generally serves to \"frame\" the content on a page.\\nFor navigation, you will need often manage the relationship between navigation elements. You can do this by using calc. To note: the navigation z-index should never exceed 19, as these are z-index values for modals.\\n```\\n.mainheader {\\nz-index: calc(var(--zindex-navigation) + 9);\\n}\\n.subheader {\\nz-index: var(--zindex-navigation)\\n}\\n```\\n#### Modal\\nUse modal z-index for shims, popovers, and modals. To control stacking between these, we rely on document order and portaling. Shims come before modals, modals and popovers stack based on which one comes last (which you can help control by portaling, though we can get away with not portaling from time to time).\\n#### Crazy\\nUse crazy for the unexpected situations where you know something should go above everything else. This ideally would never need to be used but is there as an escape hatch and for debugging purposes even.\\n### For standard values, continue using increments of `10`\\nFor base stacking context, standard values are most helpful at increments of 10 because it\\ngives engineers flexibility to insert layers between defined stack levels. For example:\\n","Predictions":"We've decided to go with a 2 tier approach:n- Use pre-defined z-index values & ranges that are defined in FDS.n- Use values between 1 and 9 for fine layering control (usualy when pre-defined z-indexnvalues are not useful)n- Rely on DOM order to set stacking order for elements of the same z-index (for example, a popover menu within a modal)n### Use values between 1 and 9 for fine layering controlnIn cases where one element needs to appear above another, use integers below 10. Rely onnsurrounding stacking context to do the rest.nFor example, the ButtonGroup component needs to manage layering of buttons so that thenfocus ring is always visible. The surrounding stacking context does not matter - it usesnvalues 1 through 5 to accomplish this.n### Pre-defined FDS z-index values \/ rangesnWe currently have 3 layers: navigation (10), modal (20), and crazy (50).n### NavigationnYou want to use navigation z-index for headers, sidebars, etc. Anything that generally serves to \"frame\" the content on a page.nFor navigation, you will need often manage the relationship between navigation elements. You can do this by using calc. To note: the navigation z-index should never exceed 19, as these are z-index.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements.nFor navigation, you will need often manage the relationship between navigation elements."}
{"File Name":"james\/0042-james-cli-based-on-webadmin.md","Context":"## Context\nJames servers offer a command-line interface in order to interact with the server. However, it relies on the JMX protocol, which is known to be insecure. The JMX server embedded in Apache James, also used by the command line client is exposed to a java de-serialization issue according to [NVD-CVE-2017-12628 Detail](https:\/\/nvd.nist.gov\/vuln\/detail\/CVE-2017-12628), and thus can be used to execute arbitrary commands.\nBesides, the current CLI interface is also not optimal for users. It places actions in front of entities with contiguous syntax, making it harder for the user to remember the command (for example, which entity the GET action command can interact with). If we design to place the entity first and the outgoing actions can interact with that entity afterward, the user will easily imagine what he\/she can do with each entity. This creates an intuitive interface that is easier to remember.\nWebadmin APIs use HTTP protocol, which is more secure than JMX protocol to interact with James servers.\nWebadmin command-line interface is an upcoming replacement for the outdated, security-vulnerable JMX command-line interface.\n","Decision":"We decided to write a new CLI client, running on top of the JVM, communicating with James via the webadmin protocol, using http.\\n* What libraries will we use?\\n* http client: ***Feign library***. We used it as an http client in other parts of James so we continue to use it.\\n* CLI: ***Picocli library***. Picocli is a one-file command line parsing framework written in Java that allows us to create command line applications with almost no code. It allows mixing Options with positional Parameters (Eg: no need to the follow order Options then Parameters), [automatic type conversion](https:\/\/picocli.info\/#_strongly_typed_everything) of command line arguments to the type of the annotated field, provide Automatic Help and better Subcommand Support, easily handle Exceptions.\\n* How will we limit breaking changes this new CLI will cause?\\n* Work on a wrapper to adapt the old CLI API.\\n* Where will we locate this cli code?\\n* server\/protocols\/webadmin-cli\\n* Write a man page.\\n* Picocli generates beautiful documentation for our CLI (HTML, PDF and Unix man pages).\\n* We decided to adopt a more modern, modular CLI syntax:\\n```\\n$ .\/james-cli [OPTION] ENTITY ACTION {ARGUMENT}\\n```\\nwhere\\nOPTION: optional parameter when running the command line,\\nENTITY: represents the entity to perform action on,\\nACTION: name of the action to perform,\\nARGUMENT: arguments needed for the action.\\n#### Examples\\nAdd a domain to the domain list.\\n```\\n$ .\/james-cli --url http:\/\/127.0.0.1:9999 domain create domainNameToBeCreated\\n```\\nIn above command-line\\nOPTION: --url http:\/\/127.0.0.1:9999\\nENTITY: domain\\nACTION: create\\nARGUMENT: domainNameToBeCreated\\n","tokens":244,"id":2133,"text":"## Context\\nJames servers offer a command-line interface in order to interact with the server. However, it relies on the JMX protocol, which is known to be insecure. The JMX server embedded in Apache James, also used by the command line client is exposed to a java de-serialization issue according to [NVD-CVE-2017-12628 Detail](https:\/\/nvd.nist.gov\/vuln\/detail\/CVE-2017-12628), and thus can be used to execute arbitrary commands.\\nBesides, the current CLI interface is also not optimal for users. It places actions in front of entities with contiguous syntax, making it harder for the user to remember the command (for example, which entity the GET action command can interact with). If we design to place the entity first and the outgoing actions can interact with that entity afterward, the user will easily imagine what he\/she can do with each entity. This creates an intuitive interface that is easier to remember.\\nWebadmin APIs use HTTP protocol, which is more secure than JMX protocol to interact with James servers.\\nWebadmin command-line interface is an upcoming replacement for the outdated, security-vulnerable JMX command-line interface.\\n\n\n##Decision\nWe decided to write a new CLI client, running on top of the JVM, communicating with James via the webadmin protocol, using http.\\n* What libraries will we use?\\n* http client: ***Feign library***. We used it as an http client in other parts of James so we continue to use it.\\n* CLI: ***Picocli library***. Picocli is a one-file command line parsing framework written in Java that allows us to create command line applications with almost no code. It allows mixing Options with positional Parameters (Eg: no need to the follow order Options then Parameters), [automatic type conversion](https:\/\/picocli.info\/#_strongly_typed_everything) of command line arguments to the type of the annotated field, provide Automatic Help and better Subcommand Support, easily handle Exceptions.\\n* How will we limit breaking changes this new CLI will cause?\\n* Work on a wrapper to adapt the old CLI API.\\n* Where will we locate this cli code?\\n* server\/protocols\/webadmin-cli\\n* Write a man page.\\n* Picocli generates beautiful documentation for our CLI (HTML, PDF and Unix man pages).\\n* We decided to adopt a more modern, modular CLI syntax:\\n```\\n$ .\/james-cli [OPTION] ENTITY ACTION {ARGUMENT}\\n```\\nwhere\\nOPTION: optional parameter when running the command line,\\nENTITY: represents the entity to perform action on,\\nACTION: name of the action to perform,\\nARGUMENT: arguments needed for the action.\\n#### Examples\\nAdd a domain to the domain list.\\n```\\n$ .\/james-cli --url http:\/\/127.0.0.1:9999 domain create domainNameToBeCreated\\n```\\nIn above command-line\\nOPTION: --url http:\/\/127.0.0.1:9999\\nENTITY: domain\\nACTION: create\\nARGUMENT: domainNameToBeCreated\\n","Predictions":"We decided to write a new CLI client, running on top of the JVM, communicating with James via the webadmin protocol, using http.n* What libraries will we use?n* http client: ***Feign library***. We used it as an HTTP client in other parts of James so we continue to use it.n* CLI: ***Picocli library***. Picocli is a one-file command line parsing framework written in Java that allows us to create command line applications with almost no code. It allows mixing Options with positional Parameters (Eg: no need to follow order Options then Parameters), [automatic type conversion](https:\/\/picocli.info\/#_strongly_typed_everything) of command line arguments to the type of the annotated field, provide Automatic Help and better Subcommand Support, easily handle Exceptions.n* How will we limit breaking changes this new CLI will will include:n- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/jsonn- \/usr\/bin\/env \/j"}
{"File Name":"NorthwindCore\/0001-record-architecture-decision.md","Context":"## Context\nAs the project is an example of a clean code and best practices of software development, it is necessary to save all architectural decisions in one place.\n","Decision":"For all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\\nEach ADR will be recorded using [Michael Nygard template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\\n","tokens":33,"id":1792,"text":"## Context\\nAs the project is an example of a clean code and best practices of software development, it is necessary to save all architectural decisions in one place.\\n\n\n##Decision\nFor all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\\nEach ADR will be recorded using [Michael Nygard template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\\n","Predictions":"For all architectural decisions Architecture Decision Log (ADR) is created. All decisions will be recorded as Architecture Decision Records (ADR).nEach ADR will be recorded using [Michael Nygard template](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.n"}
{"File Name":"reaction-component-library\/0003-choose-a-style-guide-generator-framework.md","Context":"## Context\nWe want:\n- Write all simple React components in one repository\n- Document the React components with code comments\n- Add additional markdown documentation for components when necessary\n- Allow both designers and engineers to edit the docs\n- Run the tool locally to make component development and testing easier\n- Build into a hostable web app, which can be used by anyone to learn our style, pick an appropriate component, and edit the component on the page\n- Be able to style\/theme anything about the style guide app as a whole to match our other docs\n### Options\n[React Storybook](https:\/\/storybook.js.org\/)\n[React Styleguidist](https:\/\/react-styleguidist.js.org\/)\n","Decision":"Use Styleguidist. They way it is built from markdown is more user-friendly for designers to edit vs. React Storybook. Also, it is more aimed at generating a living style guide, whereas Storybook is more of a developer's tool.\\n","tokens":147,"id":3035,"text":"## Context\\nWe want:\\n- Write all simple React components in one repository\\n- Document the React components with code comments\\n- Add additional markdown documentation for components when necessary\\n- Allow both designers and engineers to edit the docs\\n- Run the tool locally to make component development and testing easier\\n- Build into a hostable web app, which can be used by anyone to learn our style, pick an appropriate component, and edit the component on the page\\n- Be able to style\/theme anything about the style guide app as a whole to match our other docs\\n### Options\\n[React Storybook](https:\/\/storybook.js.org\/)\\n[React Styleguidist](https:\/\/react-styleguidist.js.org\/)\\n\n\n##Decision\nUse Styleguidist. They way it is built from markdown is more user-friendly for designers to edit vs. React Storybook. Also, it is more aimed at generating a living style guide, whereas Storybook is more of a developer's tool.\\n","Predictions":"We will use a [semantic UI](https:\/\/semantic-UI.org\/) to create a UI that is easy to use and maintain.n"}
{"File Name":"smjs\/2018121201-stop-using-registers-in-processor.md","Context":"## Context\nCommunication between processor and interpreter must go both ways: the processor needs to pass the interpreter the instruction to execute, and the interpreter is supposed to be able to update the Instruction Pointer (IP), and to tell the processor it the execution needs to stop, and with which exit status.\nCurrently we are using an intermediate component, registers, partially like how real processors work, containing both the IP and the information about the exit status. However, the communication between the processor and the interpreter is so straightforward that a much simpler solution would be to just use classical call argument, and call return value. Registers could be used as a mediator, but for this to have sense the processor and the interpreter would have to be unaware of each other, and registers should implement a mechanism to automatically notify involved parties when a change is made, which seems overkill.\n","Decision":"The interpreter will keep taking an instruction from the processor, but this time it will return a data structure, which tells if an exit has been triggered, and in this case what's the exit status.\\n","tokens":172,"id":3972,"text":"## Context\\nCommunication between processor and interpreter must go both ways: the processor needs to pass the interpreter the instruction to execute, and the interpreter is supposed to be able to update the Instruction Pointer (IP), and to tell the processor it the execution needs to stop, and with which exit status.\\nCurrently we are using an intermediate component, registers, partially like how real processors work, containing both the IP and the information about the exit status. However, the communication between the processor and the interpreter is so straightforward that a much simpler solution would be to just use classical call argument, and call return value. Registers could be used as a mediator, but for this to have sense the processor and the interpreter would have to be unaware of each other, and registers should implement a mechanism to automatically notify involved parties when a change is made, which seems overkill.\\n\n\n##Decision\nThe interpreter will keep taking an instruction from the processor, but this time it will return a data structure, which tells if an exit has been triggered, and in this case what's the exit status.\\n","Predictions":"We will use the stack method to implement the stack interface.n"}
{"File Name":"pace-developers\/0007-developer-scripts-storage-location.md","Context":"## Context\nWhen developing new algorithms and features for PACE-related software,\ndevelopers often create useful demo\/visualisation scripts for their own use.\nThese scripts could be useful or interesting for other developers, and are\nimportant for reproducibility or justifying design decisions. They should be\nstored somewhere in version control so that they can be easily accessed by any\ndevelopers and referred to later. However, they are not intended for general\nuse and will not be actively maintained or tested. There are 2 main options:\n* Store them in a `dev_scripts` directory in each separate project repository\n* Store them in a `scripts` directory in `pace-developers`\nIf they're in the `dev_scripts` directory for each project repository:\n+ All in one place\n+ Scripts will be close to the code they are used for\n- Scripts may not work with the version of the code they are distributed with\n- It's unclear where scripts that use more than one project would go\n- Despite the folder being called `dev_scripts` people might expect the scripts\nto actually work as they're in the main project repository\nIf they're in a `scripts` directory in `pace-developers`:\n+ They can be kept close to the decision-making developer documentation that\nthey support\n+ A version can be specified for any project dependencies\n","Decision":"Developer scripts will be stored in an appropriately placed `scripts`\\ndirectory in the `pace-developers` repository. Depending on whether the\\nscript is tied to a particular software, or general algorithm development\\nit could be stored in `pace-developers\/euphonic\/scripts` or\\n`pace-developers\/powder_averaging\/scripts` for example.\\n","tokens":283,"id":5176,"text":"## Context\\nWhen developing new algorithms and features for PACE-related software,\\ndevelopers often create useful demo\/visualisation scripts for their own use.\\nThese scripts could be useful or interesting for other developers, and are\\nimportant for reproducibility or justifying design decisions. They should be\\nstored somewhere in version control so that they can be easily accessed by any\\ndevelopers and referred to later. However, they are not intended for general\\nuse and will not be actively maintained or tested. There are 2 main options:\\n* Store them in a `dev_scripts` directory in each separate project repository\\n* Store them in a `scripts` directory in `pace-developers`\\nIf they're in the `dev_scripts` directory for each project repository:\\n+ All in one place\\n+ Scripts will be close to the code they are used for\\n- Scripts may not work with the version of the code they are distributed with\\n- It's unclear where scripts that use more than one project would go\\n- Despite the folder being called `dev_scripts` people might expect the scripts\\nto actually work as they're in the main project repository\\nIf they're in a `scripts` directory in `pace-developers`:\\n+ They can be kept close to the decision-making developer documentation that\\nthey support\\n+ A version can be specified for any project dependencies\\n\n\n##Decision\nDeveloper scripts will be stored in an appropriately placed `scripts`\\ndirectory in the `pace-developers` repository. Depending on whether the\\nscript is tied to a particular software, or general algorithm development\\nit could be stored in `pace-developers\/euphonic\/scripts` or\\n`pace-developers\/powder_averaging\/scripts` for example.\\n","Predictions":"We will store the dev_scripts directory in a separate pace-developers repository.nWe will store the dev_scripts directory in pace-developersnIf they're in the dev_scripts directory for each project repository:n* All in one placen* Scripts will be close to the code they are used forn* Scripts may not work with the version of the code they are distributed withn* It's unclear where scripts that use more than one project would gonIf they're in a scripts directory in pace-developers:n* They can be kept close to the decision-making developer documentation thatnthey supportn* A version can be specified for any project dependenciesn"}
{"File Name":"inner-source_kotlin_spark_template\/20180617-flyway.md","Context":"## Context\nDevelopers need some tool for DB versioning and safe migrations.\n","Decision":"We will introduce dockerized flyway setup for local development.\\nMigration scripts will also be provided.\\n","tokens":17,"id":1462,"text":"## Context\\nDevelopers need some tool for DB versioning and safe migrations.\\n\n\n##Decision\nWe will introduce dockerized flyway setup for local development.\\nMigration scripts will also be provided.\\n","Predictions":"We will introduce dockerized flyway setup for local development.nMigration scripts will also be provided.n"}
{"File Name":"nso.aurora\/MultipleQueues.md","Context":"## Context\nThe messages from Order Management is probably more important than those that are coming from Rating Manager and Recommendation Manager.\n","Decision":"The decision is to introduce another queue for Order Management\\n","tokens":25,"id":306,"text":"## Context\\nThe messages from Order Management is probably more important than those that are coming from Rating Manager and Recommendation Manager.\\n\n\n##Decision\nThe decision is to introduce another queue for Order Management\\n","Predictions":"The decision is to introduce another queue for Order Managementn"}
{"File Name":"digitalrig-metal-aws\/0006-create-reference-implementation-repository.md","Context":"## Context\nThe rig defined at [Bookit Infrastructure](https:\/\/github.com\/buildit\/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\nThe only way to capture that is via branches which can be hard to discover.\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\n","Decision":"Create a digitalrig-metal-aws repo (https:\/\/github.com\/buildit\/digitalrig-metal-aws) that demonstrates all of the options and latest features of the AWS Bare Metal Rig and removes any Bookit specific wording\/concepts.\\n","tokens":162,"id":1742,"text":"## Context\\nThe rig defined at [Bookit Infrastructure](https:\/\/github.com\/buildit\/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\\nThe only way to capture that is via branches which can be hard to discover.\\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\\n\n\n##Decision\nCreate a digitalrig-metal-aws repo (https:\/\/github.com\/buildit\/digitalrig-metal-aws) that demonstrates all of the options and latest features of the AWS Bare Metal Rig and removes any Bookit specific wording\/concepts.\\n","Predictions":"Create a digitalrig-metal-aws repo (https:\/\/github.com\/buildit\/digitalrig-metal-aws) that demonstrates all of the options and latest features of the AWS Bare Metal Rig and removes any Bookit specific wording\/concepts.n"}
{"File Name":"log430-dashview-architecture\/0002-use-java-to-implement-complete-solution.md","Context":"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":3820,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","Predictions":"The change that we're proposing or have agreed to implement.n"}
{"File Name":"opg-data\/0008-hateoas.md","Context":"## Context\nWe wish to build a RESTful API, and to be technically RESTful, we must implement HATEOAS. This gets us to stage 3 of the [Richardson Maturity Model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\n","Decision":"Our API is to be as RESTful as it can be.\\nWe recognise that we must implement Hypermedia as the Engine of Application State.\\nThe content structure already discussed in [0005-content-structure.md](0005-content-structure.md) implements links within our resource object... this ADR is to explicitly state our aim is to implement HATEOAS.\\nFurthermore, for every endpoint on the API we will implement an OPTIONS verb, returning every action possible at that endpoint.\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n","tokens":88,"id":2184,"text":"## Context\\nWe wish to build a RESTful API, and to be technically RESTful, we must implement HATEOAS. This gets us to stage 3 of the [Richardson Maturity Model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n\n\n##Decision\nOur API is to be as RESTful as it can be.\\nWe recognise that we must implement Hypermedia as the Engine of Application State.\\nThe content structure already discussed in [0005-content-structure.md](0005-content-structure.md) implements links within our resource object... this ADR is to explicitly state our aim is to implement HATEOAS.\\nFurthermore, for every endpoint on the API we will implement an OPTIONS verb, returning every action possible at that endpoint.\\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\\n","Predictions":"We will use the [HATEOAS](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html) as the Engine of Application State.n"}
{"File Name":"konfetti\/0004-use-lazy-loading.md","Context":"## Context\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\n","Decision":"We will use a lazy evaluation approach, similar to [implemented in Django](https:\/\/github.com\/django\/django\/blob\/master\/django\/conf\/__init__.py#L42)\\n","tokens":37,"id":3572,"text":"## Context\\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\\n\n\n##Decision\nWe will use a lazy evaluation approach, similar to [implemented in Django](https:\/\/github.com\/django\/django\/blob\/master\/django\/conf\/__init__.py#L42)\\n","Predictions":"We will use a lazy evaluation approach, similar to [implemented in Django](https:\/\/github.com\/django\/django\/blob\/master\/django\/conf\/__init__.py#L42)n"}
{"File Name":"gp-data-etl\/0002-store-local-copy-of-syndication-data.md","Context":"## Context\nThe [NHS's Syndicated Content](http:\/\/www.nhs.uk\/aboutNHSChoices\/professionals\/syndication\/Pages\/Webservices.aspx)\nholds information about GP surgeries.\nSeveral of Connecting to Services applications need to make use of GP data, including the\n[GP Finder](https:\/\/github.com\/nhsuk\/gp-finder) and [Profiles](https:\/\/github.com\/nhsuk\/profiles) projects.\nThe Syndication API is not intended for direct access in production environments.\nThe Syndication API returns data in XML, and information is spread across several subpages,\ni.e. overview, services, facilities.\n","Decision":"The syndication XML API should be scraped nightly to create a local copy of the data.\\nWhat were multiple pages for a practice on the Syndication API will be merged into a single record per practice and\\nconverted into the Connecting to Services teams preferred format, JSON.\\n","tokens":137,"id":2530,"text":"## Context\\nThe [NHS's Syndicated Content](http:\/\/www.nhs.uk\/aboutNHSChoices\/professionals\/syndication\/Pages\/Webservices.aspx)\\nholds information about GP surgeries.\\nSeveral of Connecting to Services applications need to make use of GP data, including the\\n[GP Finder](https:\/\/github.com\/nhsuk\/gp-finder) and [Profiles](https:\/\/github.com\/nhsuk\/profiles) projects.\\nThe Syndication API is not intended for direct access in production environments.\\nThe Syndication API returns data in XML, and information is spread across several subpages,\\ni.e. overview, services, facilities.\\n\n\n##Decision\nThe syndication XML API should be scraped nightly to create a local copy of the data.\\nWhat were multiple pages for a practice on the Syndication API will be merged into a single record per practice and\\nconverted into the Connecting to Services teams preferred format, JSON.\\n","Predictions":"We will use the [Security Manager](https:\/\/docs.aws.amazon.com\/systems-manager\/latest\/userguide\/systems-manager-parameter-store.html) for our secrets storage.n"}
{"File Name":"Head-Start-TTADP\/0003-configuration-by-environment-variable.md","Context":"## Context\nApplications need to be configured differently depending on where they are running. For example, the backend running locally will have different configuration then the backend running in production.\n","Decision":"We will use environment variables to configure applications.\\n","tokens":35,"id":1173,"text":"## Context\\nApplications need to be configured differently depending on where they are running. For example, the backend running locally will have different configuration then the backend running in production.\\n\n\n##Decision\nWe will use environment variables to configure applications.\\n","Predictions":"We will use the Procesor as the domain service, because the domain service is the only one that is responsible for running the application.n"}
{"File Name":"verify-service-provider\/0025-we-will-only-release-one-configuration-file.md","Context":"## Context\nHistorically we have had \"two ways\" of configuring Verify Service Provider:\n- Using environment variables\n- Using a YAML file\nWhen using environment variables the application used the verify-service-provider-env.yml\nfile from the resources directory (so inside the jar). When using the YAML file you would\npass the path to a different file as a command line parameter - usually people\nwould use the example one that's contained in the repo.\nThere were a couple of reasons for the extra complexity of managing two files, both due to\nrestrictions with the java buildpack used by cloudfoundry:\n- It's not possible to specify command line arguments through the java buildpack,\nso you can't specify a path to your config file\n- We weren't confident in the way cloudfoundry manages static files, so we didn't want\nto rely on one.\nThere was also a philosophical point that 12 factor applications should be configured through\ntheir environment. This made the \"hide the configuration in the .jar and do everything through\nenv vars\" way appealing.\n","Decision":"We will remove the verify-service-provider-env.yml file from src\/main\/resources\\nThe application will default to the verify-service-provider.yml\\nfile that's included in the .zip if no command line arguments are provided.\\nIf the application is started without command line arguments specifying a yml file\\nAND no environment variables have been set, startup should error gracefully and tell\\nthe user that the configuration fields have not been specified for example:\\n\"ERROR - no configuration fields found, either set environment variables or specify\\na configuration file using command line arguments ```server <path\/to\/verify-service-provider.yml>```\"\\nWe will establish the path to verify-service-provider.yml by asking java for the\\npath to the .jar file containing the Application class and looking in the parent\\nfolder.\\n","tokens":228,"id":4448,"text":"## Context\\nHistorically we have had \"two ways\" of configuring Verify Service Provider:\\n- Using environment variables\\n- Using a YAML file\\nWhen using environment variables the application used the verify-service-provider-env.yml\\nfile from the resources directory (so inside the jar). When using the YAML file you would\\npass the path to a different file as a command line parameter - usually people\\nwould use the example one that's contained in the repo.\\nThere were a couple of reasons for the extra complexity of managing two files, both due to\\nrestrictions with the java buildpack used by cloudfoundry:\\n- It's not possible to specify command line arguments through the java buildpack,\\nso you can't specify a path to your config file\\n- We weren't confident in the way cloudfoundry manages static files, so we didn't want\\nto rely on one.\\nThere was also a philosophical point that 12 factor applications should be configured through\\ntheir environment. This made the \"hide the configuration in the .jar and do everything through\\nenv vars\" way appealing.\\n\n\n##Decision\nWe will remove the verify-service-provider-env.yml file from src\/main\/resources\\nThe application will default to the verify-service-provider.yml\\nfile that's included in the .zip if no command line arguments are provided.\\nIf the application is started without command line arguments specifying a yml file\\nAND no environment variables have been set, startup should error gracefully and tell\\nthe user that the configuration fields have not been specified for example:\\n\"ERROR - no configuration fields found, either set environment variables or specify\\na configuration file using command line arguments ```server <path\/to\/verify-service-provider.yml>```\"\\nWe will establish the path to verify-service-provider.yml by asking java for the\\npath to the .jar file containing the Application class and looking in the parent\\nfolder.\\n","Predictions":"We will remove the verify-service-provider-env.yml file from src\/main\/resourcesnThe application will default to the verify-service-provider.yml file that's included in the.zip if no command line arguments are provided.nIf the application is started without command line arguments specifying a yml filenAND no environment variables have been set, startup should error gracefully and tellnthe user that the configuration fields have not been specified for example:n\"ERROR - no configuration fields found, either set environment variables or specifyna configuration file using command line arguments server path\/to\/verify-service-provider.yml>\"nWe will establish the path to verify-service-provider.yml by asking java for thenpath to the.jar file containing the Application class and looking in the parentnfolder.n"}
{"File Name":"remultiform\/0004-use-prettier-to-format-code.md","Context":"## Context\nWe want to ensure we're all using one code style, that is familiar across\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\nof writing, it is used by over\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\non GitHub, and has become a standard.\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":5255,"text":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n\n##Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","Predictions":"We will enforce that everything supported by Prettier has its style enforced bynit.nWe will set up Git hooks to automatically run the formatter before committing.nWe will set continuous integration up to reject commits that are not correctlynformatted.n"}
{"File Name":"latis3\/0001-Scala-programming-language.md","Context":"## Context and Problem Statement\nThe hybrid object-oriented\/functional language Scala is ideally suited for developing LaTiS.\n## Decision Drivers <!-- optional -->\n* merits of functional programming languages\n* merits of strongly typed languages\n* availability of developers\n* consensus of community of scientific programmers\n* suitability for processing large data sets\n* concepts involving code correctnes\n* transformations to data are explicit and can be reasoned about\n* support for DSLs (Domain Specific Languages)\n","Decision":"* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\nChosen option: Scala is the language-of-choice for LaTiS development.  The only decision driver above that is not in alignment with this decision is the fact that Python is the preferred language for most scientific programmers.\\n### Positive Consequences <!-- optional -->\\n* immutable data types in Scala reduce the challenges of introducing parallelism\\n* for an extensible library like LaTiS a functional programming language like Scala provides better abstractions and tools to reason about transformations of large data sets\\n* a strongly typed language like Scala helps to prevent code rot and reduces the chance that defects will slip into the code base\\n* well designed Scala applications are based on sound software engineering principles even though they may take more effort to build than similar Java or Python programs\\n* that being said, Scala programs tend to be easier to maintain and refactor which actually minimizes effort over the life of a project\\n* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather than creating specific recipes for specific use cases\\n* Apache Spark, written in Scala, is the leading framework for processing large amounts of data and is becoming on of the primary framework for implementing machine learning algorithms\\n* Scala supports the creation of DSLs which will allow users of LaTiS to specify processing instructions for datasets without themselves nedding to learn Scala\\n* the full gamut of Java libraies is available to Scala developers\\n* many successful Java libraries eventually get re-written in Scala\\n* Scala encourages software craft-people to follow sound engineering principles instead of just developing something that just gets the job done, this is probably the most important factor in choosing Scala, but the hardest to document\\n### Negative Consequences\\n* Scala developers are not as common as Python developers, especially in the field of scientific computing\\n* Python is clearly the dominant language used by scientific programmers\\n","tokens":97,"id":4716,"text":"## Context and Problem Statement\\nThe hybrid object-oriented\/functional language Scala is ideally suited for developing LaTiS.\\n## Decision Drivers <!-- optional -->\\n* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\n\n\n##Decision\n* merits of functional programming languages\\n* merits of strongly typed languages\\n* availability of developers\\n* consensus of community of scientific programmers\\n* suitability for processing large data sets\\n* concepts involving code correctnes\\n* transformations to data are explicit and can be reasoned about\\n* support for DSLs (Domain Specific Languages)\\nChosen option: Scala is the language-of-choice for LaTiS development.  The only decision driver above that is not in alignment with this decision is the fact that Python is the preferred language for most scientific programmers.\\n### Positive Consequences <!-- optional -->\\n* immutable data types in Scala reduce the challenges of introducing parallelism\\n* for an extensible library like LaTiS a functional programming language like Scala provides better abstractions and tools to reason about transformations of large data sets\\n* a strongly typed language like Scala helps to prevent code rot and reduces the chance that defects will slip into the code base\\n* well designed Scala applications are based on sound software engineering principles even though they may take more effort to build than similar Java or Python programs\\n* that being said, Scala programs tend to be easier to maintain and refactor which actually minimizes effort over the life of a project\\n* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather than creating specific recipes for specific use cases\\n* Apache Spark, written in Scala, is the leading framework for processing large amounts of data and is becoming on of the primary framework for implementing machine learning algorithms\\n* Scala supports the creation of DSLs which will allow users of LaTiS to specify processing instructions for datasets without themselves nedding to learn Scala\\n* the full gamut of Java libraies is available to Scala developers\\n* many successful Java libraries eventually get re-written in Scala\\n* Scala encourages software craft-people to follow sound engineering principles instead of just developing something that just gets the job done, this is probably the most important factor in choosing Scala, but the hardest to document\\n### Negative Consequences\\n* Scala developers are not as common as Python developers, especially in the field of scientific computing\\n* Python is clearly the dominant language used by scientific programmers\\n","Predictions":"* merits of functional programming languagesn* merits of strongly typed languagesn* availability of developersn* consensus of community of scientific programmersn* suitability for processing large data setsn* concepts involving code correctnesn* transformations to data are explicit and can be reasoned aboutn* support for DSLs (Domain Specific Languages)nChosen option: Scala is the language-of-choice for LaTiS development. The only decision driver above that is not in alignment with this decision is the fact that Python is the preferred language for most scientific programmers.n### Positive Consequences!-- optional -->n* immutable data types in Scala reduce the challenges of introducing parallelismn* for an extensible library like LaTiS a functional programming language like Scala provides better abstractions and tools to reason about transformations of large data setsn* a strongly typed language like Scala helps to prevent code rot and reduces the chance that defects will slip into the code basen* well designed Scala applications are based on sound software engineering principles even though they may take more effort to build than similar Java or Python programsn* that being said, Scala programs tend to be easier to maintain and refactor which actually minimizes effort over the life of a projectn* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather than creating specific recipes for specific use casesn* Apache Spark, written in Scala, is the leading framework for processing large datasets.n* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather than creating specific recipes for specific use casesn* Apache Spark, written in Scala, is the leading framework for processing large datasets.n* Apache Spark, written in Scala, is the leading framework for processing large datasets.n* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather than creating specific recipes for specific use casesn* Apache Spark, written in Scala, is the leading framework for processing large datasets.n* Apache Spark, written in Scala, is the leading framework for processing large datasets.n* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather than creating specific recipes for specific use casesn* Apache Spark, written in Scala, is the leading framework for processing large datasets.n* Apache Spark, written in Scala, is the leading framework for processing large datasets.n* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather than creating specific recipes for specific use casesn* Apache Spark, written in Scala, is the leading framework for processing large datasets.n* Apache Spark, written in Scala, is the leading framework for processing large datasets.n* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather than creating specific recipes for specific use casesn* Apache Spark, written in Scala, is the leading framework for processing large datasets.n* Apache Spark, written in Scala, is the leading framework for processing large datasets.n* LaTiS is primarily a framework for transforming large datasets in a performant manner, this meshes nicely with the functional approach that emphasizes creating abstractions that do things to data rather"}
{"File Name":"disco-poc-vue\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":165,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"cf-k8s-networking\/0013-rename-master-branch.md","Context":"## Context\nA [proposal](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/topic\/75070528#9059) was\nmade on the cf-dev mailing list to rename our `master` branch to `main` to make\nthe CF community a more welcoming and inclusive environment.\n","Decision":"We are going to rename our `master` branch to `develop`. `develop` was chosen\\ninstead of `main` by team consensus because it better describes the use of the\\nbranch.\\n","tokens":63,"id":2251,"text":"## Context\\nA [proposal](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/topic\/75070528#9059) was\\nmade on the cf-dev mailing list to rename our `master` branch to `main` to make\\nthe CF community a more welcoming and inclusive environment.\\n\n\n##Decision\nWe are going to rename our `master` branch to `develop`. `develop` was chosen\\ninstead of `main` by team consensus because it better describes the use of the\\nbranch.\\n","Predictions":"We are going to rename our master branch to develop. develop was chosenninstead of main by team consensus because it better describes the use of the branch.n"}
{"File Name":"road-registry\/002-track-municipalities.md","Context":"## Context\nWe need the geometries (boundaries) of a municipality to derive the municipality of road segments that do not have an associated left or right street.\nWe can't rely on the geometries used in the municipality registry because they are (1) not exposed and (2) they do not match the boundary along which road segments have been split up.\nWhen the boundaries in the municipality registry change, one can't expect all segments to change as well.\n","Decision":"We keep track of municipalities as defined in the legacy road registry database because those are the ones along which road segments have been split up.\\nWe use `ImportedMunicipality` as the main event to track this. It contains all the data we need for now. This event is assembled while extracting events and imported during deployment.\\n","tokens":96,"id":4930,"text":"## Context\\nWe need the geometries (boundaries) of a municipality to derive the municipality of road segments that do not have an associated left or right street.\\nWe can't rely on the geometries used in the municipality registry because they are (1) not exposed and (2) they do not match the boundary along which road segments have been split up.\\nWhen the boundaries in the municipality registry change, one can't expect all segments to change as well.\\n\n\n##Decision\nWe keep track of municipalities as defined in the legacy road registry database because those are the ones along which road segments have been split up.\\nWe use `ImportedMunicipality` as the main event to track this. It contains all the data we need for now. This event is assembled while extracting events and imported during deployment.\\n","Predictions":"We keep track of municipalities as defined in the legacy road registry database because those are the ones along which road segments have been split up.nWe use ImportedMunicipality as the main event to track this. It contains all the data we need for now. This event is assembled while extracting events and imported during deployment.n"}
{"File Name":"external-service-operator\/0004-ports-are-never-stored-as-named-ports.md","Context":"## Context\nActually it does not make sense to store ports as strings, as those Backends are not in the cluster so they don't have names and can only be referenced by a port number. Nevertheless the Kuberenetes API makes it possible to store ports as string which will not be used at the endpoints generated by the External Service Operator.\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":70,"id":1735,"text":"## Context\\nActually it does not make sense to store ports as strings, as those Backends are not in the cluster so they don't have names and can only be referenced by a port number. Nevertheless the Kuberenetes API makes it possible to store ports as string which will not be used at the endpoints generated by the External Service Operator.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","Predictions":"* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.n* The feature configuration from Feature::getAll() is converted to a SCSS map inside ShopwareStorefrontThemeThemeCompiler::getFeatureConfigScssMap.n* This SCSS map is always added to the SCSS string which gets processed by ShopwareStorefrontThemen"}
{"File Name":"fxa\/0005-minimize-password-entry.md","Context":"## Context and Problem Statement\nSee [Github Issue 1371][#gh-issue-1371]. The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff. Minimizing the need for a password in an authorization flow should increase flow completion rates.\nWhen and where passwords are asked for has been a repeated source of confusion amongst both users and Firefox Accounts developers. If a user is signed into Sync, passwords are only _supposed_ to be required for authorization flows for RPs that require encryption keys. However, there is a bug in the state management logic that forces users to enter their password more often than expected.\nTechnically, we _must always_ ask the user to enter their password any time encryption keys are needed by an RP, e.g., Sync, Lockwise, and Send. For RPs that do not require encryption keys, e.g., Monitor and AMO, there is no technical reason why authenticated users must enter their password again, the existing sessionToken is capable of requesting new OAuth tokens.\n## Decision Drivers\n- User happiness via fewer keystrokes, less confusion\n- Improved signin rates\n","Decision":"- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\nChosen option: \"option 2\", because it minimizes the number of places the user must enter their password.\\n### Positive Consequences\\n- User will need to type their password in fewer places.\\n- Signin completion rates should increase.\\n### Negative Consequences\\n- There may be user confusion around what it means to sign out.\\n### [option 1] Keep the existing flow\\nIf a user signs in to Sync first and is not signing into an OAuth\\nRP that requires encryption keys, then no password is required.\\nIf a user does not sign into Sync and instead signs into an\\nOAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not\\nrequire encryption keys, e.g., Monitor, then they must enter their password.\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _ask_ for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because we already have it and no effort is required to keep it.\\n- Bad because there is no technical reason why we cannot re-use existing sessionTokens created when signing into OAuth RPs to generate OAuth tokens for other non-key requesting OAuth RPs.\\n- Bad, because users need to enter their password more than they need to.\\n- Bad, because due to a bug in the code, users that are currently signed into Sync are sometimes asked for their password to sign into services such as Monitor that do not require keys.\\n### [option 2] Only ask authenticated users for a password if encryption keys are required\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _do not_ ask for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because case 1 _does not_ ask for a password whereas it _does_ with option 1.\\n- Bad, because there is potential for user confusion about expected behavior when destroying the sessionToken - should destroying the sessionToken sign the user out of the RP too? See [Github issue 640][#gh-issue-640].\\n- Support for [RP initiated logout][#gh-issue-1979] will largely mitigate this.\\n","tokens":254,"id":377,"text":"## Context and Problem Statement\\nSee [Github Issue 1371][#gh-issue-1371]. The FxA authorization flow sometimes asks already authenticated users to enter their password, sometimes it does not. Password entry, especially on mobile devices, is difficult and a source of user dropoff. Minimizing the need for a password in an authorization flow should increase flow completion rates.\\nWhen and where passwords are asked for has been a repeated source of confusion amongst both users and Firefox Accounts developers. If a user is signed into Sync, passwords are only _supposed_ to be required for authorization flows for RPs that require encryption keys. However, there is a bug in the state management logic that forces users to enter their password more often than expected.\\nTechnically, we _must always_ ask the user to enter their password any time encryption keys are needed by an RP, e.g., Sync, Lockwise, and Send. For RPs that do not require encryption keys, e.g., Monitor and AMO, there is no technical reason why authenticated users must enter their password again, the existing sessionToken is capable of requesting new OAuth tokens.\\n## Decision Drivers\\n- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\n\n\n##Decision\n- User happiness via fewer keystrokes, less confusion\\n- Improved signin rates\\nChosen option: \"option 2\", because it minimizes the number of places the user must enter their password.\\n### Positive Consequences\\n- User will need to type their password in fewer places.\\n- Signin completion rates should increase.\\n### Negative Consequences\\n- There may be user confusion around what it means to sign out.\\n### [option 1] Keep the existing flow\\nIf a user signs in to Sync first and is not signing into an OAuth\\nRP that requires encryption keys, then no password is required.\\nIf a user does not sign into Sync and instead signs into an\\nOAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not\\nrequire encryption keys, e.g., Monitor, then they must enter their password.\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _ask_ for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because we already have it and no effort is required to keep it.\\n- Bad because there is no technical reason why we cannot re-use existing sessionTokens created when signing into OAuth RPs to generate OAuth tokens for other non-key requesting OAuth RPs.\\n- Bad, because users need to enter their password more than they need to.\\n- Bad, because due to a bug in the code, users that are currently signed into Sync are sometimes asked for their password to sign into services such as Monitor that do not require keys.\\n### [option 2] Only ask authenticated users for a password if encryption keys are required\\n**example 1** User performs the initial authorization flow for an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does not require encryption keys, e.g., Monitor, then _do not_ ask for the password.\\n**example 2** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that does not require encryption keys, e.g., Monitor, _do not_ ask for the password.\\n**example 3** User performs the initial authorization flow for an OAuth RP, e.g., Monitor, and then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 4** User performs the initial authorization flow for Sync, then a subsequent authorization flow for an OAuth RP that _does_ require encryption keys, e.g., Send, then _ask_ for the password.\\n**example 5** User performs the initial authorization flow for an OAuth RP that does not require keys, e.g., Monitor, and then performs an authorization flow for Sync, then _ask_ for the password.\\n**example 6** User performs the initial authorization flow for an OAuth RP that does does require keys, e.g., Send, and then performs an authorization flow for Sync, then _ask_ for the password.\\n- Good, because case 1 _does not_ ask for a password whereas it _does_ with option 1.\\n- Bad, because there is potential for user confusion about expected behavior when destroying the sessionToken - should destroying the sessionToken sign the user out of the RP too? See [Github issue 640][#gh-issue-640].\\n- Support for [RP initiated logout][#gh-issue-1979] will largely mitigate this.\\n","Predictions":"- User happiness via fewer keystrokes, less confusionn- Improved signin ratesnChosen option: \"option 2\", because it minimizes the number of places the user must enter their password.n### Positive Consequencesn- User will need to type their password in fewer places.n- Signin completion rates should increase.n### Negative Consequencesn- There may be user confusion around what it means to sign out.n### [option 1] Keep the existing flownIf a user signs in to Sync first and is not signing into an OAuthnRP that requires encryption keys, then no password is required.nIf a user does not sign into Sync and instead signs into an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does notn require encryption keys, then no password is required.nIf a user does not sign into Sync and instead signs into an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does notnrequire encryption keys, then no password is required.n### [option 1] Keep the existing flownIf a user signs in to Sync first and is not signing into an OAuthnRP that requires encryption keys, then no password is required.nIf a user does not sign into Sync and instead signing into an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does notnrequire encryption keys, then no password is required.nIf a user does not sign into Sync and instead signing into an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does notnrequire encryption keys, then no password is required.n### [option 1] Keep the existing flownIf a user signed into Sync first and is not signing into an OAuthnRP that requires encryption keys, then no password is required.nIf a user does not sign into Sync and instead signing into an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does notnrequire encryption keys, then no password is required.nIf a user does not sign into Sync and instead signing into an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does notnrequire encryption keys, then no password is required.nIf a user does not sign into Sync and instead signing into an OAuth RP, e.g., Send, and then visits a 2nd OAuth RP that does notnrequire encryption keys, then no password is required.n"}
{"File Name":"unfinished-design-system\/006-yarn-workspaces.md","Context":"* [Context](#context)\n* [Decision](#decision)\n* [Status](#status)\n* [Consequences](#consequences)\n* [Experience Report](#experience-report)\n* [More reading](#more-reading)\n* [Updates](#updates)\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Yarn Workspaces](https:\/\/classic.yarnpkg.com\/en\/docs\/workspaces\/) integrated with Lerna to solve those problems. It has some promising features that can improve our experience and create more sustainable development environments.\\n","tokens":63,"id":4549,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [Experience Report](#experience-report)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Yarn Workspaces](https:\/\/classic.yarnpkg.com\/en\/docs\/workspaces\/) integrated with Lerna to solve those problems. It has some promising features that can improve our experience and create more sustainable development environments.\\n","Predictions":"* [Status](#status)n* [Consequences](#consequences)n* [Experience Report](#experience-report)n* [More reading](#more-reading)n* [Updates](#updates)nWe've decided to use [Yarn Workspaces](https:\/\/classic.yarnpkg.com\/en\/docs\/workspaces\/) integrated with Lerna to solve those problems. It has some promising features that can improve our experience and create more sustainable development environments.n* [Context](#context)n* [Decision](#decision)n* [Status](#status)n* [Consequences](#consequences)n* [Experience Report](#experience-report)n* [More reading](#more-reading)n* [Updates](#updates)nWe've decided to use [Yarn Workspaces](https:\/\/classic.yarnpkg.com\/en\/docs\/workspaces\/) integrated with Lerna to solve those problems. It has some promising features that can improve our experience and create more sustainable development environments.n"}
{"File Name":"kuona\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":573,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"verify-service-provider\/0006-we-will-build-a-js-client.md","Context":"## Context\nAt least one user is currently using node js and passport. We want to provide as\nfrictionless as possible an integration for them.\nOther users will be using other languages and frameworks.\n","Decision":"We will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","tokens":43,"id":4443,"text":"## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n\n\n##Decision\nWe will initially build only a node \/ passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n","Predictions":"We will initially build only a node \/ passport client. We will want to buildnanother client in another language as soon as possible to make sure the APInis well designed.nUsers should also be able to interact with the API directly if we haven't built an appropriate client for their use case.n"}
{"File Name":"beis-report-official-development-assistance\/0030-run-data-migrations-manually.md","Context":"## Context\nThe Data Migrate gem has caused us a number of issues in the past, it runs\nsilently as part of a deploy, and this can result in surprising errors\nduring a deploy. We've also had issues with the gem itself - most recently\na bug in a new version causing strange errors in deployment.\n","Decision":"With this in mind, we've decided to drop the use of the Data Migrate gem,\\nand instead run any data migrations manually. We have easy access to the\\nconsole via GOV.UK PaaS, so this is relatively painless.\\nWe have added a generator which creates an empty Ruby file with a timestamp and the name of the migration as the filename. Once the code is\\ndeployed, we run the migration on the production server.\\nAs the code is plain old Ruby, it's also easier for us to write tests, if\\nwe think it's worth doing (for example, the migrations code is suitably\\ncomplex).\\n","tokens":69,"id":2396,"text":"## Context\\nThe Data Migrate gem has caused us a number of issues in the past, it runs\\nsilently as part of a deploy, and this can result in surprising errors\\nduring a deploy. We've also had issues with the gem itself - most recently\\na bug in a new version causing strange errors in deployment.\\n\n\n##Decision\nWith this in mind, we've decided to drop the use of the Data Migrate gem,\\nand instead run any data migrations manually. We have easy access to the\\nconsole via GOV.UK PaaS, so this is relatively painless.\\nWe have added a generator which creates an empty Ruby file with a timestamp and the name of the migration as the filename. Once the code is\\ndeployed, we run the migration on the production server.\\nAs the code is plain old Ruby, it's also easier for us to write tests, if\\nwe think it's worth doing (for example, the migrations code is suitably\\ncomplex).\\n","Predictions":"We will use a stack command to create a new stack file.n"}
{"File Name":"nozama\/adr-001-simulator-webserver.md","Context":"## Context\nSince we decided to design the simulator as a separated component we didn't thought how it would communicate with the WebApp. So, the simulator is a simple java program that can't do anything to communicate with Nozama since it is a web application and has a different ecosystem (spring framework).\n","Decision":"As of now we decided to create a simple webserver to expose simulator to our main WebApp. As it main functionality is to just pass some data when required and send notifications to Nozama's backend when some task is done.\\n","tokens":63,"id":97,"text":"## Context\\nSince we decided to design the simulator as a separated component we didn't thought how it would communicate with the WebApp. So, the simulator is a simple java program that can't do anything to communicate with Nozama since it is a web application and has a different ecosystem (spring framework).\\n\n\n##Decision\nAs of now we decided to create a simple webserver to expose simulator to our main WebApp. As it main functionality is to just pass some data when required and send notifications to Nozama's backend when some task is done.\\n","Predictions":"As of now we decided to create a simple webserver to expose simulator to our main WebApp. As it main functionality is to just pass some data when required and send notifications to Nozama's backend when some task is done.n"}
{"File Name":"godspeed-you-blocked-developer\/001. TypeScript, React, Redux, and Webpack.md","Context":"## Context\nI was once a web developer when jQuery and CSS 2 were the new hotness. I did a lot of web 1.0, single-server work. Then I spent many years building iOS apps. These days I'm interested to be more of a generalist.\nIn the fall of 2017, I learned some TypeScript and thought it was great. JavaScript is JavaScript, and I like strong types, and TypeScript makes that happen, so it was an instant favorite.\nI decided to learn some more web technologies. For browser apps that means React and Redux, which is commonly used with Node, NPM, and Webpack. Learning the common stuff is a good way to get my bearings in the field.\nWhen the idea for this project came to mind, it was a good chance to learn that web stack and catch up my knowledge.\n","Decision":"I decided to build this site in TypeScript, with React and Redux, on Node, NPM, and Webpack.\\n","tokens":177,"id":4940,"text":"## Context\\nI was once a web developer when jQuery and CSS 2 were the new hotness. I did a lot of web 1.0, single-server work. Then I spent many years building iOS apps. These days I'm interested to be more of a generalist.\\nIn the fall of 2017, I learned some TypeScript and thought it was great. JavaScript is JavaScript, and I like strong types, and TypeScript makes that happen, so it was an instant favorite.\\nI decided to learn some more web technologies. For browser apps that means React and Redux, which is commonly used with Node, NPM, and Webpack. Learning the common stuff is a good way to get my bearings in the field.\\nWhen the idea for this project came to mind, it was a good chance to learn that web stack and catch up my knowledge.\\n\n\n##Decision\nI decided to build this site in TypeScript, with React and Redux, on Node, NPM, and Webpack.\\n","Predictions":"We decided to use json as the base for the app.n"}
{"File Name":"lbh-frontend-react\/0004-use-prettier-to-format-code.md","Context":"## Context\nWe want to ensure we're all using one code style, that is familiar across\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\nof writing, it is used by over\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\non GitHub, and has become a standard.\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":1408,"text":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n\n##Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","Predictions":"We will enforce that everything supported by Prettier has its style enforced bynit.nWe will set up Git hooks to automatically run the formatter before committing.nWe will set continuous integration up to reject commits that are not correctlynformatted.n"}
{"File Name":"skypy\/adr-02.md","Context":"## Context\nWe need to decide on a unit convention as to whether units include the factor \/h or not (for instance Mpc or Mpc\/h as a unit of distance). For further discussion see e.g. 10.1017\/pasa.2013.31\n## Decision Drivers\n- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\n- Consistency \/ least surprise: the default for astropy is Mpc\n","Decision":"- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/issues\/23) and offline, Mpc has been chosen to ensure the closest integration and least surprise for astropy.\\n","tokens":112,"id":5148,"text":"## Context\\nWe need to decide on a unit convention as to whether units include the factor \/h or not (for instance Mpc or Mpc\/h as a unit of distance). For further discussion see e.g. 10.1017\/pasa.2013.31\\n## Decision Drivers\\n- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\n\n\n##Decision\n- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).\\n- Consistency \/ least surprise: the default for astropy is Mpc\\nAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/issues\/23) and offline, Mpc has been chosen to ensure the closest integration and least surprise for astropy.\\n","Predictions":"- Flexibility: Mpc\/h allows results to be easily propagated across the 'unknown' value of h (0.68 or 0.74 or something else).n- Consistency \/ least surprise: the default for astropy is MpcnAfter [discussion](https:\/\/github.com\/skypyproject\/skypy\/issues\/23) and offline, Mpc has been chosen to ensure the closest integration and least surprise for astropy.n"}
{"File Name":"verify-self-service\/0003-use-pundit-for-rbac.md","Context":"## Context\nAs with any modern web system there is a need to secure the application with\nstrong authentication and Role Based Access Control (RBAC). RBAC allows us to\ncreate permissions, apply them to roles and associates roles with users. From\nthis we can restrict what users can do and see within the application based on\ntheir role. In addition to this we also had to investigate where to store role\ninformation and define teams to support our use of RBAC.\n","Decision":"After some investigation and discussion into various ways of implementing RBAC\\nwithin Ruby we have decided to implement the Pundit Ruby gem within the Verify\\nSelf Service application. This gem provides us with a way of creating policies\\nwhich apply to the app as a whole and to individual classes within the\\napplication.\\nWe have chosen pundit for a number of reasons. The first is that it seems\\nrelatively easy to implement within our application. A proof of concept was\\ncreated in a few hours which restricted creating new components within the\\napplication to those who hold the `gds` or `dev` roles. Policies and permissions\\nare defined in code as plain Ruby objects which are easy to understand and\\nsimple to implement. This ease of implementation and an appearance of providing\\nthe fuctionality we are seeking we believe makes pundit a good candidate for\\nimplementing RBAC within our application.\\nIn addition to this we are aware of prior-art use within GDS on the GOV.UK\\nproject. Finally we have found a lot of documentation and articles about how to\\nimplement and use pundit.\\nRole information we have decided to store in a custom field within our cognito\\ninstance. This will take the form of a comma seperated list which will be split\\ninto an array of roles for use within the application. This method allows us to\\nkeep all use information together within AWS Cognito and means that the only way\\nrole information can be changed is via SDK calls to AWS Cognito.\\nFinally we understand there is a need to define teams within our application. We\\nhave decided that a team is analogous to an AWS Cognito group. This allows a\\nuser to be part of one more groups and for a group to hold one or more users. We\\ncan easily query group membership using the AWS Cognito SDK and check for a\\nusers membership to a specific group.\\n","tokens":97,"id":3950,"text":"## Context\\nAs with any modern web system there is a need to secure the application with\\nstrong authentication and Role Based Access Control (RBAC). RBAC allows us to\\ncreate permissions, apply them to roles and associates roles with users. From\\nthis we can restrict what users can do and see within the application based on\\ntheir role. In addition to this we also had to investigate where to store role\\ninformation and define teams to support our use of RBAC.\\n\n\n##Decision\nAfter some investigation and discussion into various ways of implementing RBAC\\nwithin Ruby we have decided to implement the Pundit Ruby gem within the Verify\\nSelf Service application. This gem provides us with a way of creating policies\\nwhich apply to the app as a whole and to individual classes within the\\napplication.\\nWe have chosen pundit for a number of reasons. The first is that it seems\\nrelatively easy to implement within our application. A proof of concept was\\ncreated in a few hours which restricted creating new components within the\\napplication to those who hold the `gds` or `dev` roles. Policies and permissions\\nare defined in code as plain Ruby objects which are easy to understand and\\nsimple to implement. This ease of implementation and an appearance of providing\\nthe fuctionality we are seeking we believe makes pundit a good candidate for\\nimplementing RBAC within our application.\\nIn addition to this we are aware of prior-art use within GDS on the GOV.UK\\nproject. Finally we have found a lot of documentation and articles about how to\\nimplement and use pundit.\\nRole information we have decided to store in a custom field within our cognito\\ninstance. This will take the form of a comma seperated list which will be split\\ninto an array of roles for use within the application. This method allows us to\\nkeep all use information together within AWS Cognito and means that the only way\\nrole information can be changed is via SDK calls to AWS Cognito.\\nFinally we understand there is a need to define teams within our application. We\\nhave decided that a team is analogous to an AWS Cognito group. This allows a\\nuser to be part of one more groups and for a group to hold one or more users. We\\ncan easily query group membership using the AWS Cognito SDK and check for a\\nusers membership to a specific group.\\n","Predictions":"After some investigation and discussion into various ways of implementing RBACnwithin Ruby we have decided to implement the Pundit Ruby gem within the VerifynSelf Service application. This gem provides us with a way of creating policiesnwhich apply to the app as a whole and to individual classes within thenapplication.nWe have chosen pundit for a number of reasons. The first is that it seemsnrelatively easy to implement within our application. A proof of concept wasncreated in a few hours which restricted creating new components within thenapplication to those who hold the gds or dev roles. Policies and permissionsnare defined in code as plain Ruby objects which are easy to understand andnsimple to implement. This ease of implementation and an appearance of providingnthe fuctionality we are seeking we believe makes pundit a good candidate for implementing RBAC within our application.nIn addition to this we are aware of prior-art use within GDS on the GOV.UKnproject. Finally we have found a lot of documentation and articles about how tonimplement and use pundit.nRole information we have decided to store in a custom field within our cognitoninstance. This will take the form of a comma seperated list which will be splitninto an array of roles for use within the application. This method allows us tonkeep all use information together within AWS Cognito and means that the only waynrole information can be changed is via SDK calls to AWS Cognito.nFinally we understand there is a need to define teams within our application. Wenhave decided that teams will be the name of the application. This is a teams field which will be a comma separated list which will be split into an array of roles for use within the application.nWe have decided that teams will be the name of the application. This will be the name of the application.nWe have decided that teams will be the name of the application. This will be the name of the application.nWe have decided that teams will be the name of the application. This will be the name of the application.nWe have decided that teams will be the name of the application. This will be the name of the application.nWe have decided that teams will be the name of the application. This will be the name of the application.nWe have decided that teams will be the name of the application. This will be the name of the application.nWe have decided that teams will be the name of the application. This will be the name of the application.nWe have decided that teams will be the name of the application. This will be the name of the application.n"}
{"File Name":"TANF-app\/010-assign-superuser.md","Context":"## Context\nUsually in Django we would assign the first superuser through the CLI, but because this will not\nbe available in production, we will need another method.\n","Decision":"The Django Admin provides the easiest way to assign superuser status, so for most users that will\\nbe the method employed. However, we still need a method for creating the first superuser, so that\\nuser will be able to assign others as needed. We will assign this with a data migration using a\\nusername defined in environment variables.\\n","tokens":35,"id":3792,"text":"## Context\\nUsually in Django we would assign the first superuser through the CLI, but because this will not\\nbe available in production, we will need another method.\\n\n\n##Decision\nThe Django Admin provides the easiest way to assign superuser status, so for most users that will\\nbe the method employed. However, we still need a method for creating the first superuser, so that\\nuser will be able to assign others as needed. We will assign this with a data migration using a\\nusername defined in environment variables.\\n","Predictions":"We will use stack to store the data.n"}
{"File Name":"govuk-aws\/0032-transfer-artefact-binary.md","Context":"## Context\nWe create binary artifacts for applications when we execute the **Deploy_App**\nJenkins job. The binary file (artifact) gets stored in an AWS S3 bucket\n(For example **govuk-integration-artefact**). We need the binary file to deploy\nthe application to the next environment. For example, we will use the binary\nfile from **Integration** to deploy the application in the **Staging**\nenvironment.\n","Decision":"We decided to use a mechanism that is located outside the environments involved\\nand pass the contents to the correct locations when the package becomes\\navailable.\\n![Transfer Mechanism](.\/0032-transfer-artefact-binary-img01.png?raw=true \"Transfer Mechanism\")\\n### Example\\nWe will use the **Integration** and **Staging** environments as examples.\\n1. We execute the **Deploy_Apps -> router** Jenkins job in the Integration environment.\\n2. A binary file is created in the **govuk-integration-artefact** AWS S3 bucket.\\n3. We have an AWS SNS Topic called **govuk-integration-artefact**.\\n4. We have an AWS S3 **govuk-integration-artefact** bucket event notification. This event sends a notification to the AWS SNS Topic when an **ObjectCreate** event is triggered.\\n5. The **Staging** environment has an AWS S3 bucket called **govuk-staging-artefact**.\\n6. We have an AWS SNS Subscription that subscribes to the **govuk-integration-artefact** AWS SNS Topic. This subscription will enable Staging to know when a new object gets created in the **govuk-integration-artefact** AWS S3 bucket.\\n7. We have an AWS Lambda function called **govuk-staging-artefact**. This function copies objects from the destination bucket to the target bucket. The trigger for the function is the AWS SNS Subscription.\\n","tokens":95,"id":4057,"text":"## Context\\nWe create binary artifacts for applications when we execute the **Deploy_App**\\nJenkins job. The binary file (artifact) gets stored in an AWS S3 bucket\\n(For example **govuk-integration-artefact**). We need the binary file to deploy\\nthe application to the next environment. For example, we will use the binary\\nfile from **Integration** to deploy the application in the **Staging**\\nenvironment.\\n\n\n##Decision\nWe decided to use a mechanism that is located outside the environments involved\\nand pass the contents to the correct locations when the package becomes\\navailable.\\n![Transfer Mechanism](.\/0032-transfer-artefact-binary-img01.png?raw=true \"Transfer Mechanism\")\\n### Example\\nWe will use the **Integration** and **Staging** environments as examples.\\n1. We execute the **Deploy_Apps -> router** Jenkins job in the Integration environment.\\n2. A binary file is created in the **govuk-integration-artefact** AWS S3 bucket.\\n3. We have an AWS SNS Topic called **govuk-integration-artefact**.\\n4. We have an AWS S3 **govuk-integration-artefact** bucket event notification. This event sends a notification to the AWS SNS Topic when an **ObjectCreate** event is triggered.\\n5. The **Staging** environment has an AWS S3 bucket called **govuk-staging-artefact**.\\n6. We have an AWS SNS Subscription that subscribes to the **govuk-integration-artefact** AWS SNS Topic. This subscription will enable Staging to know when a new object gets created in the **govuk-integration-artefact** AWS S3 bucket.\\n7. We have an AWS Lambda function called **govuk-staging-artefact**. This function copies objects from the destination bucket to the target bucket. The trigger for the function is the AWS SNS Subscription.\\n","Predictions":"We decided to use a mechanism that is located outside the environments involvednand pass the contents to the correct locations when the package becomesnavailable.n![Transfer Mechanism](.\/0032-transfer-artefact-binary-img01.png?raw=true \"Transfer Mechanism\")n### ExamplenWe will use the **Integration** and **Staging** environments as examples.n1. We execute the **Deploy_Apps -> router** Jenkins job in the Integration environment.n2. A binary file is created in the **govuk-integration-artefact** AWS S3 bucket.n3. We have an AWS SNS Topic called **govuk-integration-artefact**.n4. We have an AWS S3 **govuk-integration-artefact** bucket event notification. This event sends a notification to the AWS SNS Topic when an **ObjectCreate** event is triggered.n5. The **Staging** environment has an AWS S3 bucket called **govuk-staging-artefact**.n6. We have an AWS SNS Subscription that subscribes to the **govuk-integration-artefact** AWS SNS Topic. This subscription will enable Staging to know when a new object gets created in the **govuk-integration-artefact** AWS S3 bucket.n7. We have an AWS Lambda function called **govuk-staging-artefact**. This function copies objects from the destination bucket to the target bucket. The trigger for the function is the AWS SNS Subscription.n### DecisionnWe decided to use a mechanism that is located outside the environments involvednand pass the contents to the correct locations when the package becomesnavailable.n![Transfer Mechanism](.\/0032-transfer-artefact-binary-img01.png?raw=true \"Transfer Mechanism\")n### ExamplenWe will use the **Integration** and **Staging** environments as examples.n1. We execute the **Deploy_Apps -> router** Jenkins job in the Integration environment.n2. A binary file is created in the **govuk-integration-artefact** AWS S3 bucket.n3. We have an AWS SNS Topic called **govuk-integration-artefact**.n4. We have an AWS SNS Subscription that subscribes to the **govuk-integration-artefact** AWS SNS Topic. This subscription will enable Staging to know when a new object gets created in the **govuk-integration-artefact** AWS S3 bucket.n5. The **Staging** environment has an AWS S3 bucket called **govuk-staging-artefact**.n6. We have an AWS Lambda function called **govuk-staging-artefact**. This function copies objects from the destination bucket to the target bucket. The trigger for the function is the AWS SNS Subscription.n### DecisionnWe will use the"}
{"File Name":"status-react\/0011-tweak-pr-process.md","Context":"## Context\nThere was a generally dissatisfaction with our PR flow process from multiple stakeholders, including devs, QA and design. These largely centered around size, speed of integration and quality of PRs.\nFor more details, please see pain points in the meeting notes held end of February here: https:\/\/notes.status.im\/C5pj8g7gQOu9Wo8PtDZsMw?edit# as well as the preceeding Discuss thread: https:\/\/discuss.status.im\/t\/better-pull-requests-process\/1044\nAlso see conversations in Core Dev Call #12 and #13: https:\/\/github.com\/status-im\/pm\/\n","Decision":"These specific policy changes were agreed upon (a la 'rough consensus' model). These are heuristics, and should be more of a guide than strict rules. Exception to these should be minimal though.\\n**1. Reject PRs that don't have tests**\\nAs a rule of thumb, coverage should increase.\\n**2. Don't use or promote nightly as something for end users.**\\n**3. Reject PRs that don't write what tests the author have done and what platforms you used to test, including screenshots, videos or logs**\\nThis means QA and dev have to spend less time catching obvious problems.\\n**4. Run tests when a PR is at \"Review\" stage on https:\/\/github.com\/status-im\/status-react\/projects\/7 (or add new 'e2e stage' between review and `TO TEST`)**\\n**5. Dev asks for 2-3 reviewers, including a designer if the change affects UI, and these reviewers review within a day.**\\nThis ensures we don't have *diffusion of responsibility* and that PRs are reviewed quickly.\\n","tokens":135,"id":2964,"text":"## Context\\nThere was a generally dissatisfaction with our PR flow process from multiple stakeholders, including devs, QA and design. These largely centered around size, speed of integration and quality of PRs.\\nFor more details, please see pain points in the meeting notes held end of February here: https:\/\/notes.status.im\/C5pj8g7gQOu9Wo8PtDZsMw?edit# as well as the preceeding Discuss thread: https:\/\/discuss.status.im\/t\/better-pull-requests-process\/1044\\nAlso see conversations in Core Dev Call #12 and #13: https:\/\/github.com\/status-im\/pm\/\\n\n\n##Decision\nThese specific policy changes were agreed upon (a la 'rough consensus' model). These are heuristics, and should be more of a guide than strict rules. Exception to these should be minimal though.\\n**1. Reject PRs that don't have tests**\\nAs a rule of thumb, coverage should increase.\\n**2. Don't use or promote nightly as something for end users.**\\n**3. Reject PRs that don't write what tests the author have done and what platforms you used to test, including screenshots, videos or logs**\\nThis means QA and dev have to spend less time catching obvious problems.\\n**4. Run tests when a PR is at \"Review\" stage on https:\/\/github.com\/status-im\/status-react\/projects\/7 (or add new 'e2e stage' between review and `TO TEST`)**\\n**5. Dev asks for 2-3 reviewers, including a designer if the change affects UI, and these reviewers review within a day.**\\nThis ensures we don't have *diffusion of responsibility* and that PRs are reviewed quickly.\\n","Predictions":"We will use the [Semantic Repository](https:\/\/github.com\/SemanticRepository) to store the data we store in the database.n"}
{"File Name":"adr\/ADR-40-ui-dependencies-upgrades.md","Context":"## Context and Problem Statement\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https:\/\/github.com\/Semantic-Org\/Semantic-UI-React\/pull\/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https:\/\/github.com\/decentraland\/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\n- The `@artsy\/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\n- Doesn't have hooks support.\n#### Second alternative (react-semantic-ui)\n##### Advantages\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\n##### Disadvantages\n- Bad SSR support.\n","Decision":"The option to keep the an exact copy of the `Responsive` component (from the old `react-semantic-ui` lib version) was chosen in order to have a frictionless upgrade of the library.\\nThe procedure in which we'll be handling the upgrade is the following:\\n1. A non breaking change upgrade will be provided to our [UI library](https:\/\/github.com\/decentraland\/ui), keeping the `Responsive` component as a deprecated component and an alternative (describe below) will be provided to replace it.\\n2. A breaking change upgrade will be applied to our [UI library](https:\/\/github.com\/decentraland\/ui), whenever all of our dependencies are updated, removing the `Responsive` component.\\nWe\u2019ll be providing, alongside the `Responsive` component a set of components and hooks to replace it, using the `react-responsive`, library. This library was chosen in favor of the recommended `@artsy\/fresnel` mainly because of its versatility. The need of having to set a provider at the application's root level, (coupling the users of this dependency to `@artsy\/fresnel`) to have better SSR support that we don't currently need, made us decide not to go with it.\\nThe components built with the `react-responsive` and exposed to the consumers of our [UI library](https:\/\/github.com\/decentraland\/ui) will be the following:\\n- **Desktop** (for devices with `min width: 992`)\\n- **Tablet** (for devices with `min width: 768 and max width: 991`)\\n- **TabletAndBelow** (for devices with `max width: 991`, that is taking into consideration tablets and mobile devices)\\n- **Mobile** (for devices with `max width: 767`)\\n- **NotMobile** (for devices that don't comply with the requirements specified in Mobile)\\nThese components describe a conditional rendering based on the media the page in being rendered.\\nWhere we had:\\n```tsx\\n<Responsive\\nas={Menu}\\nsecondary\\nstackable\\nminWidth={Responsive.onlyTablet.minWidth}\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Responsive>\\n<Responsive\\n{...Responsive.onlyMobile}\\nclassName=\"dcl navbar-mobile-menu\"\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/Responsive>\\n```\\nWe now have:\\n```tsx\\n<NotMobile>\\n<Menu secondary stackable>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Menu>\\n<\/NotMobile>\\n<Mobile>\\n<div className=\"dcl navbar-mobile-menu\">\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/div>\\n<\/Mobile>\\n```\\nAnd, alongside these components, as explained before, we're exposing the following set of hooks:\\n- **useDesktopMediaQuery**\\n- **useTabletMediaQuery**\\n- **useTabletAndBelowMediaQuery**\\n- **useMobileMediaQuery**\\n- **useNotMobileMediaQuery**\\nWhich return true if the device is the one defined as the name of the hook.\\nThese types of hooks will provide us with newer functionality, being able to customize small portions of our code instead of forking our components into two.\\nAs an example, we can apply certain styles by simply:\\n```tsx\\nconst isMobile = useMobileMediaQuery()\\nconst classes = isMobile ? \"dcl mobile\" : \"dcl\"\\n<div className={classes}>\\n...\\n<\/div>\\n```\\n","tokens":379,"id":4613,"text":"## Context and Problem Statement\\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https:\/\/github.com\/Semantic-Org\/Semantic-UI-React\/pull\/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https:\/\/github.com\/decentraland\/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\\n- The `@artsy\/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\\n- Doesn't have hooks support.\\n#### Second alternative (react-semantic-ui)\\n##### Advantages\\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\\n##### Disadvantages\\n- Bad SSR support.\\n\n\n##Decision\nThe option to keep the an exact copy of the `Responsive` component (from the old `react-semantic-ui` lib version) was chosen in order to have a frictionless upgrade of the library.\\nThe procedure in which we'll be handling the upgrade is the following:\\n1. A non breaking change upgrade will be provided to our [UI library](https:\/\/github.com\/decentraland\/ui), keeping the `Responsive` component as a deprecated component and an alternative (describe below) will be provided to replace it.\\n2. A breaking change upgrade will be applied to our [UI library](https:\/\/github.com\/decentraland\/ui), whenever all of our dependencies are updated, removing the `Responsive` component.\\nWe\u2019ll be providing, alongside the `Responsive` component a set of components and hooks to replace it, using the `react-responsive`, library. This library was chosen in favor of the recommended `@artsy\/fresnel` mainly because of its versatility. The need of having to set a provider at the application's root level, (coupling the users of this dependency to `@artsy\/fresnel`) to have better SSR support that we don't currently need, made us decide not to go with it.\\nThe components built with the `react-responsive` and exposed to the consumers of our [UI library](https:\/\/github.com\/decentraland\/ui) will be the following:\\n- **Desktop** (for devices with `min width: 992`)\\n- **Tablet** (for devices with `min width: 768 and max width: 991`)\\n- **TabletAndBelow** (for devices with `max width: 991`, that is taking into consideration tablets and mobile devices)\\n- **Mobile** (for devices with `max width: 767`)\\n- **NotMobile** (for devices that don't comply with the requirements specified in Mobile)\\nThese components describe a conditional rendering based on the media the page in being rendered.\\nWhere we had:\\n```tsx\\n<Responsive\\nas={Menu}\\nsecondary\\nstackable\\nminWidth={Responsive.onlyTablet.minWidth}\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Responsive>\\n<Responsive\\n{...Responsive.onlyMobile}\\nclassName=\"dcl navbar-mobile-menu\"\\n>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/Responsive>\\n```\\nWe now have:\\n```tsx\\n<NotMobile>\\n<Menu secondary stackable>\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n{this.renderLeftMenu()}\\n<\/Menu>\\n<\/NotMobile>\\n<Mobile>\\n<div className=\"dcl navbar-mobile-menu\">\\n<a className=\"dcl navbar-logo\" href=\"https:\/\/decentraland.org\">\\n<Logo \/>\\n<\/a>\\n<Header\\nsize=\"small\"\\nclassName={`dcl active-page ${\\nthis.state.toggle ? 'caret-up' : 'caret-down'\\n}`}\\nonClick={this.handleToggle}\\n>\\n{activePage}\\n<\/Header>\\n<\/div>\\n<\/Mobile>\\n```\\nAnd, alongside these components, as explained before, we're exposing the following set of hooks:\\n- **useDesktopMediaQuery**\\n- **useTabletMediaQuery**\\n- **useTabletAndBelowMediaQuery**\\n- **useMobileMediaQuery**\\n- **useNotMobileMediaQuery**\\nWhich return true if the device is the one defined as the name of the hook.\\nThese types of hooks will provide us with newer functionality, being able to customize small portions of our code instead of forking our components into two.\\nAs an example, we can apply certain styles by simply:\\n```tsx\\nconst isMobile = useMobileMediaQuery()\\nconst classes = isMobile ? \"dcl mobile\" : \"dcl\"\\n<div className={classes}>\\n...\\n<\/div>\\n```\\n","Predictions":"We will use the src\/src\/react\/ library for the react-semantic-ui component.n"}
{"File Name":"smjs\/2018121601-multiple-usages-of-the-processor.md","Context":"## Context\nThe domain our virtual machine is supporting is concerned with running programs: as such, how programs should be run is a business rule, and the Processor is the Domain Service encapsulating that rule.\nAt the application level, we can think of several features that can support this domain:\n- classic virtual machine, running an entire program, perhaps loaded from a file, until its termination\n- REPL, where instructions are loaded one at a time from an interactive prompt, and executed, printing some kind of output each time\n- debug, where we still have an entire program, but this time we want to support debugging features, like running step-by-step\nOf course there could be many others. Can we use the same definition of a Processor in the domain to support all this cases?\n","Decision":"We will keep the Processor as a Domain Service, because the three different kinds of execution we want to support really only differ for how data is handled in memory, and the domain doesn't concern itself with what is contained in memory (it doesn't even mention memory, since it's a detail of the interpreter, and we might as well use interpreters that don't use any memory):\\n- in the classic virtual machine, the Program is the entire program, that is executed from start to end\\n- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor: this is necessary because we might have defined procedures that we want to call in the future; the state of memory is saved between different executions (this doesn't concern the domain), and to make the program start at the latest instruction, we add a new unconditional jump instruction at the beginning of it: this actually means that we don't run the entire program again (it isn't necessary since memory is preserved), but only the last inserted instruction\\n- in the debug situation, the Interpreter implementation can avoid returning control back to the Processor until requested to: this still doesn't concern the Processor implementation in the domain\\n","tokens":161,"id":3981,"text":"## Context\\nThe domain our virtual machine is supporting is concerned with running programs: as such, how programs should be run is a business rule, and the Processor is the Domain Service encapsulating that rule.\\nAt the application level, we can think of several features that can support this domain:\\n- classic virtual machine, running an entire program, perhaps loaded from a file, until its termination\\n- REPL, where instructions are loaded one at a time from an interactive prompt, and executed, printing some kind of output each time\\n- debug, where we still have an entire program, but this time we want to support debugging features, like running step-by-step\\nOf course there could be many others. Can we use the same definition of a Processor in the domain to support all this cases?\\n\n\n##Decision\nWe will keep the Processor as a Domain Service, because the three different kinds of execution we want to support really only differ for how data is handled in memory, and the domain doesn't concern itself with what is contained in memory (it doesn't even mention memory, since it's a detail of the interpreter, and we might as well use interpreters that don't use any memory):\\n- in the classic virtual machine, the Program is the entire program, that is executed from start to end\\n- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor: this is necessary because we might have defined procedures that we want to call in the future; the state of memory is saved between different executions (this doesn't concern the domain), and to make the program start at the latest instruction, we add a new unconditional jump instruction at the beginning of it: this actually means that we don't run the entire program again (it isn't necessary since memory is preserved), but only the last inserted instruction\\n- in the debug situation, the Interpreter implementation can avoid returning control back to the Processor until requested to: this still doesn't concern the Processor implementation in the domain\\n","Predictions":"We will keep the Processor as a Domain Service, because the three different kinds of execution we want to support really only differ for how data is handled in memory, and the domain doesn't concern itself with what is contained in memory (it doesn't even mention memory, since it's a detail of the interpreter, and we might as well use interpreters that don't use any memory):n- in the classic virtual machine, the Program is the entire program, that is executed from start to endn- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor: this is necessary because we might have defined procedures that we want to call in the future; the state of memory is saved between different executions (this doesn't concern the domain), and to make the program start at the latest instruction, we add a new unconditional jump instruction at the beginning of it: this actually means that we don't run the entire program again (it isn't necessary since memory is preserved), but only the last inserted instructionn- in the debug situation, the Interpreter implementation can avoid returning control back to the Processor until requested to: this still doesn't concern the Processor implementation in the domainn"}
{"File Name":"kafka\/0002-require-php-7-2-64-bit.md","Context":"## Context\nDeciding which minimum PHP version to require is quite important because it\nmight restrict people from using the library, at the same time supporting many\nversions increases the cost to maintain the library.\n","Decision":"For the first releases of this library we'll be requiring PHP 7.2+ (64-bit build\\nonly).\\nThe main motivation for only allowing it to be installed in 64-bit systems is\\nthat Kafka's protocol uses signed longs (64-bit integers) for some fields, which\\ncan't be achieve in 32-bit systems.\\nI believe that libraries' maintainers also have the role to help the evolution\\nof the PHP ecosystem, which makes me want to require a more up-to-date version\\nof the language.\\n","tokens":42,"id":641,"text":"## Context\\nDeciding which minimum PHP version to require is quite important because it\\nmight restrict people from using the library, at the same time supporting many\\nversions increases the cost to maintain the library.\\n\n\n##Decision\nFor the first releases of this library we'll be requiring PHP 7.2+ (64-bit build\\nonly).\\nThe main motivation for only allowing it to be installed in 64-bit systems is\\nthat Kafka's protocol uses signed longs (64-bit integers) for some fields, which\\ncan't be achieve in 32-bit systems.\\nI believe that libraries' maintainers also have the role to help the evolution\\nof the PHP ecosystem, which makes me want to require a more up-to-date version\\nof the language.\\n","Predictions":"The main motivation for only supporting a single version of the library is that it's a de facto standard, and I chose springfox-swagger2 and springfox-swagger-ui because of its smooth integration with Spring Boot.n"}
{"File Name":"PIMS\/database.md","Context":"## Context\nPIMS requires a database to store all property information.\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\nAdditionally it must be supported by Entity Framework Core 3.1.\n","Decision":"Originally the database generated for the SWU was with PostgreSQL, after further consideration it made more sense to tightly couple both MS-SQL with .NET Core.\\nThis will give us better performance and tighter integration with Entity Framework Core.\\nIt was decided to create a Linux docker container to host the MS-SQL 2019 database.\\n","tokens":49,"id":3966,"text":"## Context\\nPIMS requires a database to store all property information.\\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\\nAdditionally it must be supported by Entity Framework Core 3.1.\\n\n\n##Decision\nOriginally the database generated for the SWU was with PostgreSQL, after further consideration it made more sense to tightly couple both MS-SQL with .NET Core.\\nThis will give us better performance and tighter integration with Entity Framework Core.\\nIt was decided to create a Linux docker container to host the MS-SQL 2019 database.\\n","Predictions":"The application will be hosted on a Kubernetes cluster.n"}
{"File Name":"simple-server\/014-region-level-sync.md","Context":"## Context\n[PRD](https:\/\/docs.google.com\/document\/d\/1Cflct0Y-44IRUVw_5-NptcnNSX1UgAPBiXqoXHq22io\/edit)\nUsers in large districts reported that the Simple app was running very slow, making the app near-unusable.\nThe slowdown was caused by the volume of patient data synced to the user\u2019s phone. We realised that the amount of data\nbeing stored on the device had to be reduced for better long-term performance.\nCurrently we sync the entire district's records to a user's phone. Some of the large districts have upto 50,000 patients,\nwhich can amount to 400-500 MB of data. On lower-end phones we noticed the app started slowing down when the DB size grew beyond 250 MB.\nA district typically has between 1-20 blocks. From trends in IHCI, we found it's uncommon for patients to visit facilities across blocks.\nPatients that have a BP taken in more than 1 block is around 2%, with the exceptions of:\nSindhudurg (9.8%), Hoshiarpur (5.3%), Bathinda (3.1%).\nThis means that we can sync only a block's data to the user's phone and be reasonably confident about finding patients on the app.\n","Decision":"- The server will sync records from the user's block instead of the entire district.\\nSpecifically the following patients will be synced:\\n- patients that registered at a facility in the same block,\\n- patients that are assigned to a facility in the same block, and\\n- patients that have an appointment scheduled at a facility in the same block.\\n- All other sync resources will return records belonging to these patients only.\\n- The sync mechanism should support the ability to adjust the sync radius to any sync region.\\nThis is important in case we need to change the kind of records that are synced to the app in the future.\\nSee the [wiki entry on Region level sync](..\/wiki\/adjusting-sync-boundaries.md) for how it works.\\n### On the app\\n- Users can continue selecting any facility in their district when switching facilities.\\n- Users can continue selecting any facility in their district when scheduling a patient\u2019s next visit or preferred facility.\\n- It is possible that a patient will visit a facility outside their block and their record will not be found on the user\u2019s device. In this case the user should\\n- Scan the patient\u2019s BP passport if they have one.\\n- Register the patient again, as if they were new. Make sure to attach their existing BP passport to the registration.\\n- The duplicate patient records will be merged by the Simple team later.\\n","tokens":282,"id":1708,"text":"## Context\\n[PRD](https:\/\/docs.google.com\/document\/d\/1Cflct0Y-44IRUVw_5-NptcnNSX1UgAPBiXqoXHq22io\/edit)\\nUsers in large districts reported that the Simple app was running very slow, making the app near-unusable.\\nThe slowdown was caused by the volume of patient data synced to the user\u2019s phone. We realised that the amount of data\\nbeing stored on the device had to be reduced for better long-term performance.\\nCurrently we sync the entire district's records to a user's phone. Some of the large districts have upto 50,000 patients,\\nwhich can amount to 400-500 MB of data. On lower-end phones we noticed the app started slowing down when the DB size grew beyond 250 MB.\\nA district typically has between 1-20 blocks. From trends in IHCI, we found it's uncommon for patients to visit facilities across blocks.\\nPatients that have a BP taken in more than 1 block is around 2%, with the exceptions of:\\nSindhudurg (9.8%), Hoshiarpur (5.3%), Bathinda (3.1%).\\nThis means that we can sync only a block's data to the user's phone and be reasonably confident about finding patients on the app.\\n\n\n##Decision\n- The server will sync records from the user's block instead of the entire district.\\nSpecifically the following patients will be synced:\\n- patients that registered at a facility in the same block,\\n- patients that are assigned to a facility in the same block, and\\n- patients that have an appointment scheduled at a facility in the same block.\\n- All other sync resources will return records belonging to these patients only.\\n- The sync mechanism should support the ability to adjust the sync radius to any sync region.\\nThis is important in case we need to change the kind of records that are synced to the app in the future.\\nSee the [wiki entry on Region level sync](..\/wiki\/adjusting-sync-boundaries.md) for how it works.\\n### On the app\\n- Users can continue selecting any facility in their district when switching facilities.\\n- Users can continue selecting any facility in their district when scheduling a patient\u2019s next visit or preferred facility.\\n- It is possible that a patient will visit a facility outside their block and their record will not be found on the user\u2019s device. In this case the user should\\n- Scan the patient\u2019s BP passport if they have one.\\n- Register the patient again, as if they were new. Make sure to attach their existing BP passport to the registration.\\n- The duplicate patient records will be merged by the Simple team later.\\n","Predictions":"We will use the SyncRail server to sync records from the user's block instead of the entire district.nSpecifically the following patients will be synced:n- patients that registered at a facility in the same block,n- patients that are assigned to a facility in the same block, andn- patients that have an appointment scheduled at a facility in the same block.n- All other sync resources will return records belonging to these patients only.nThe sync mechanism should support the ability to adjust the sync radius to any sync region.nThis is important in case we need to change the kind of records that are synced to the app in the future.nSee the [wiki entry on Region level sync](..\/wiki\/adjusting the sync radius to any sync region).nThis is important in case we need to change the kind of records that are synced to the app in the future.nSee the [wiki entry on Region level sync](..\/wiki\/adjusting the sync radius to any sync region).n"}
{"File Name":"protagonist\/0001-composite-handler-design.md","Context":"## Context and Problem Statement\n### Context\nA design for the implementation and delivery of a component of the DLCS\nwhich can process \"composite\" artifacts. Such artifacts typically take\nthe form of a single object containing multiple embedded resources, each\nof which can be rasterized and treated as an individual resource within\nthe DLCS.\n","Decision":"Delivery of a standalone component which consumes a DLCS ingest payload\\ntailored to \"composite\" artifacts, and triggers an ingestion workflow\\nwhich rasterizes each of the constituent parts and invokes the existing\\nDLCS ingestion workflow.\\n![0001-flowchart.png](images\/0001-flowchart.png)\\n","tokens":69,"id":2859,"text":"## Context and Problem Statement\\n### Context\\nA design for the implementation and delivery of a component of the DLCS\\nwhich can process \"composite\" artifacts. Such artifacts typically take\\nthe form of a single object containing multiple embedded resources, each\\nof which can be rasterized and treated as an individual resource within\\nthe DLCS.\\n\n\n##Decision\nDelivery of a standalone component which consumes a DLCS ingest payload\\ntailored to \"composite\" artifacts, and triggers an ingestion workflow\\nwhich rasterizes each of the constituent parts and invokes the existing\\nDLCS ingestion workflow.\\n![0001-flowchart.png](images\/0001-flowchart.png)\\n","Predictions":"* A single component which consumes a DLCS ingest payloadntailored to \"composite\" artifacts, and triggers an ingestion workflownwhich rasterizes each of the constituent parts and invokes the existingnDLCS ingestion workflow.n![0001-flowchart.png](images\/0001-flowchart.png)n"}
{"File Name":"alfresco-anaxes-shipyard\/0004-k8s-cluster-creation.md","Context":"## Context\nWe need to recommend a tool to those customers that do not have an existing Kubernetes cluster.\nA tool called minikube can be used on laptops for development and evaluation purposes. For a production ready stack we need to provide a way for customers to provision a cluster in AWS using a mechanism a less technically literate person can follow.\nA list of candidate solutions can be found [here](https:\/\/github.com\/kubernetes\/community\/blob\/master\/sig-aws\/kubernetes-on-aws.md).\n","Decision":"We will be informing customers that we use kops internally and it's the cluster upon which we perform all our testing.\\nWe have chosen kops as it's the tool the most closely fulfils our current and future requirements, it\u2019s produced by the same team that build Kubernetes and it\u2019s been [endorsed](https:\/\/aws.amazon.com\/blogs\/compute\/kubernetes-clusters-aws-kops) by AWS.\\n","tokens":101,"id":3600,"text":"## Context\\nWe need to recommend a tool to those customers that do not have an existing Kubernetes cluster.\\nA tool called minikube can be used on laptops for development and evaluation purposes. For a production ready stack we need to provide a way for customers to provision a cluster in AWS using a mechanism a less technically literate person can follow.\\nA list of candidate solutions can be found [here](https:\/\/github.com\/kubernetes\/community\/blob\/master\/sig-aws\/kubernetes-on-aws.md).\\n\n\n##Decision\nWe will be informing customers that we use kops internally and it's the cluster upon which we perform all our testing.\\nWe have chosen kops as it's the tool the most closely fulfils our current and future requirements, it\u2019s produced by the same team that build Kubernetes and it\u2019s been [endorsed](https:\/\/aws.amazon.com\/blogs\/compute\/kubernetes-clusters-aws-kops) by AWS.\\n","Predictions":"We will be informing customers that we use kops internally and it's the cluster upon which we perform all our testing.nWe have chosen kops as it's the tool the most closely fulfils our current and future requirements, it's produced by the same team that build Kubernetes and it's been [endorsed](https:\/\/aws.amazon.com\/blogs\/compute\/kubernetes-clusters-aws-kops) by AWS.n"}
{"File Name":"web-mapviewer\/2020_05_28_test_framework.md","Context":"## Context\nAn integration test framework (and possibly a combo unit\/integration test) has to be chosen for this project.\nAfter reviewing what's the state of testing with JS in 2020, here's a list of contenders.\n### Selenium\nHas been the standard of the industry for many years. Start to struggle with single page application framework (SPA) like Angular, Vue.js or React.js.\n- Desktop browsers, emulators \/ simulators and real devices (E2E testing all the way)\n- Highly scalable\n- Has many \"sister frameworks\" that enhance the feature set (like Appium for mobile testing, Nightwach or WebdriverIO)\nWhile very efficient for E2E testing, this framework would be cumbersome to have in development on a local machine.\nCould be used for E2E testing for cross browser compatibility tests.\n### TestCafe\nGood alternative to Selenium, is compatible with Saucelab (we already have some account with them).\n- Fast to set up\n- Cross Browser and Devices (with SauceLabs or BrowserStack)\nThis could also be a good cross browser testing framework, but the cross browser feature will be hard to have deployed on every developers' machine.\n### Cypress\nMade by devs for devs.\n- Very convenient running and debugging tools\n- Native access to all your application\u2019s variables\n- Solid and clear documentation\n- Newest addition (2017) to the big frameworks (very active community)\n","Decision":"After playing a bit with these frameworks, Cypress clearly had something for it.\\nThe documentation is well maintained and complete, tests are described with Mocha which is already known by many in the team (it's what we used to write our tests on the older viewer)\\nCypress will be a great help during the development phase of the new viewer.\\nBut we have to keep in mind that it doesn't really do cross browser testing.\\n","tokens":307,"id":4785,"text":"## Context\\nAn integration test framework (and possibly a combo unit\/integration test) has to be chosen for this project.\\nAfter reviewing what's the state of testing with JS in 2020, here's a list of contenders.\\n### Selenium\\nHas been the standard of the industry for many years. Start to struggle with single page application framework (SPA) like Angular, Vue.js or React.js.\\n- Desktop browsers, emulators \/ simulators and real devices (E2E testing all the way)\\n- Highly scalable\\n- Has many \"sister frameworks\" that enhance the feature set (like Appium for mobile testing, Nightwach or WebdriverIO)\\nWhile very efficient for E2E testing, this framework would be cumbersome to have in development on a local machine.\\nCould be used for E2E testing for cross browser compatibility tests.\\n### TestCafe\\nGood alternative to Selenium, is compatible with Saucelab (we already have some account with them).\\n- Fast to set up\\n- Cross Browser and Devices (with SauceLabs or BrowserStack)\\nThis could also be a good cross browser testing framework, but the cross browser feature will be hard to have deployed on every developers' machine.\\n### Cypress\\nMade by devs for devs.\\n- Very convenient running and debugging tools\\n- Native access to all your application\u2019s variables\\n- Solid and clear documentation\\n- Newest addition (2017) to the big frameworks (very active community)\\n\n\n##Decision\nAfter playing a bit with these frameworks, Cypress clearly had something for it.\\nThe documentation is well maintained and complete, tests are described with Mocha which is already known by many in the team (it's what we used to write our tests on the older viewer)\\nCypress will be a great help during the development phase of the new viewer.\\nBut we have to keep in mind that it doesn't really do cross browser testing.\\n","Predictions":"We will use js-stack to store the data.n"}
{"File Name":"jupyter-nbrequirements\/0000-dependencies-management-jupyter-notebooks.md","Context":"## Context and Problem Statement\nHow to guarantee reproducibility of Jupyter Notebooks?\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\nNotebooks should be treated as component\/service that use their own dependencies, therefore when storing notebooks,\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\n## Decision Drivers <!-- optional -->\n* user prospective\n* reproducibility\n* traecability\n","Decision":"* user prospective\\n* reproducibility\\n* traecability\\nThe option selected is 3. because:\\n* enforce reproducibility\\n* enforce traceability between notebook\\n### Positive Consequences <!-- optional -->\\n* Satisfy reproducibility, traecability, shareability.\\n* Notebooks are coupled with dependencies in their metadata.\\n* If more notebooks are present, a common Pipfile can be created with a button that can automatically extract from all notebook dependencies and new common Pipfile.lock will be created. This would allow creation of an image that can run the notebooks.\\n","tokens":140,"id":1228,"text":"## Context and Problem Statement\\nHow to guarantee reproducibility of Jupyter Notebooks?\\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\\nNotebooks should be treated as component\/service that use their own dependencies, therefore when storing notebooks,\\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\\n## Decision Drivers <!-- optional -->\\n* user prospective\\n* reproducibility\\n* traecability\\n\n\n##Decision\n* user prospective\\n* reproducibility\\n* traecability\\nThe option selected is 3. because:\\n* enforce reproducibility\\n* enforce traceability between notebook\\n### Positive Consequences <!-- optional -->\\n* Satisfy reproducibility, traecability, shareability.\\n* Notebooks are coupled with dependencies in their metadata.\\n* If more notebooks are present, a common Pipfile can be created with a button that can automatically extract from all notebook dependencies and new common Pipfile.lock will be created. This would allow creation of an image that can run the notebooks.\\n","Predictions":"* user prospectiven* reproducibilityn* traecabilitynThe option selected is 3. because:n* enforce reproducibilityn* enforce traceability between notebookn### Positive Consequences!-- optional -->n* Satisfy reproducibility, traecability, shareability.n* Notebooks are coupled with dependencies in their metadata.n* If more notebooks are present, a common Pipfile can be created with a button that can automatically extract from all notebook dependencies and new common Pipfile.lock will be created. This would allow creation of an image that can run the notebooks.n"}
{"File Name":"pace-developers\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5182,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"stac-ml-aoi-extension\/0004-multiple-label-items.md","Context":"## Context\nShould each `ml-aoi` Item be able to bring in multiple labels?\nThis would be a useful feature for training multi-class classifiers.\nOne can imagine having a label STAC item for buildings and separate STAC item for fields.\nSTAC Items Links object is an array, so many label items could be linked to from a single `ml-aoi` STAC Item.\n#### Limiting to single label link\nLimiting to single label link however is appealing because the label item metadata could be copied over to `ml-aoi` Item.\nThis would remove the need to follow the link for the label item during processing.\nIn practice this would make each `ml-aoi` Item also a `label` Item, allowing for its re-use by tooling that understands `label`.\nIf multi-class label dataset would be required there would have to be a mechanical pre-processing step of combining\nexisting labels into a single STAC `label` item. This could mean either union of GeoJSON FeatureCollections per item or\na configuration of a more complex STAC `label` Item that links to multiple label assets.\n#### Allowing multiple labels\nThe main appeal of consuming multi-label `ml-aoi` items is that it would allow referencing multiple label sources,\nsome which could be external, without the need for pre-processing and thus minimizing data duplication.\nIf multiple labels were to be allowed the `ml-aoi` the pre-processing step above would be pushed into `ml-aoi` consumer.\nThe consumer would need appropriate metadata in order to decipher how the label structure.\nThis would require either crawling the full catalog or some kind of meta-label structure that combines the metadata\nfrom all the included labels into a single structure that could be interpreted by the consumer.\n","Decision":"`ml-aoi` Items should be limited to linking to only a single label item.\\nRequiring the consumer to interpret multiple label items pushed unreasonable complexity on the user.\\nAdditionally combining labels likely requires series of processing and validation steps.\\nEach one of those would likely require judgment calls and exceptions.\\nFor instance when combining building and fields label datasets the user should check that no building and field polygons overlap.\\nIt is not realistic to expect all possible requirements of that process to be expressed by a simple metadata structure.\\nTherefore it is better to explicitly require the label combination as a separate process done by the user.\\nThe resulting label catalog can capture that design and iteration required for that process anyway.\\n","tokens":375,"id":2286,"text":"## Context\\nShould each `ml-aoi` Item be able to bring in multiple labels?\\nThis would be a useful feature for training multi-class classifiers.\\nOne can imagine having a label STAC item for buildings and separate STAC item for fields.\\nSTAC Items Links object is an array, so many label items could be linked to from a single `ml-aoi` STAC Item.\\n#### Limiting to single label link\\nLimiting to single label link however is appealing because the label item metadata could be copied over to `ml-aoi` Item.\\nThis would remove the need to follow the link for the label item during processing.\\nIn practice this would make each `ml-aoi` Item also a `label` Item, allowing for its re-use by tooling that understands `label`.\\nIf multi-class label dataset would be required there would have to be a mechanical pre-processing step of combining\\nexisting labels into a single STAC `label` item. This could mean either union of GeoJSON FeatureCollections per item or\\na configuration of a more complex STAC `label` Item that links to multiple label assets.\\n#### Allowing multiple labels\\nThe main appeal of consuming multi-label `ml-aoi` items is that it would allow referencing multiple label sources,\\nsome which could be external, without the need for pre-processing and thus minimizing data duplication.\\nIf multiple labels were to be allowed the `ml-aoi` the pre-processing step above would be pushed into `ml-aoi` consumer.\\nThe consumer would need appropriate metadata in order to decipher how the label structure.\\nThis would require either crawling the full catalog or some kind of meta-label structure that combines the metadata\\nfrom all the included labels into a single structure that could be interpreted by the consumer.\\n\n\n##Decision\n`ml-aoi` Items should be limited to linking to only a single label item.\\nRequiring the consumer to interpret multiple label items pushed unreasonable complexity on the user.\\nAdditionally combining labels likely requires series of processing and validation steps.\\nEach one of those would likely require judgment calls and exceptions.\\nFor instance when combining building and fields label datasets the user should check that no building and field polygons overlap.\\nIt is not realistic to expect all possible requirements of that process to be expressed by a simple metadata structure.\\nTherefore it is better to explicitly require the label combination as a separate process done by the user.\\nThe resulting label catalog can capture that design and iteration required for that process anyway.\\n","Predictions":"ml-aoi Items should be limited to linking to only a single label item.nRequiring that metadata be merged into ml-aoi item.n"}
{"File Name":"digital-paper-edit-client\/2019-09-28-time-estimation-for-links.md","Context":"## Context and Problem Statement\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\n## Decision Drivers <!-- optional -->\n* Ease of implementation\n* Accuracy of time estimated\n* Extensibility\n","Decision":"* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","tokens":49,"id":4171,"text":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n\n\n##Decision\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","Predictions":"* Ease of implementationn* Accuracy of time estimatedn* ExtensibilitynChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.n### Positive Consequences!-- optional -->n* A fairly easy implementation.n### Negative consequences!-- optional -->n* Less accurate than other options.n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))n"}
{"File Name":"adr\/ADR-8-dao-content-servers-and-local-content-servers.md","Context":"## Context and Problem Statement\n* Today Catalyst servers are split in two groups listed and unlisted in the DAO\nListed servers sync with eachother by a pull mechanism (between each other, listed in the DAO).\nUnlisted servers, sync again those who are listed but. Listed servers never pull changes from unlisted servers.\n* Big scenes are constrained by server configurations or proxy limitations of the deployed nodes (i.e. body post size). Nowadays, those limitations are workarounded by selecting another server with a different configuration (Kyllian's).\n### Use cases\n* I want to push my scene to my own server and I expect it to replicate to other servers.\n* Create a \"local-staging\" environment in which land owners or creators can synchronize the whole world from a DAO node and test locally.\n* Broadcast of changes should be optional.\n","Decision":"* Enable custom Catalyst servers from Explorer\\n* Define action plan for broadcasting\/commiting local deployments to the DAO servers. [issue](https:\/\/github.com\/decentraland\/catalyst\/issues\/195)\\n","tokens":178,"id":4601,"text":"## Context and Problem Statement\\n* Today Catalyst servers are split in two groups listed and unlisted in the DAO\\nListed servers sync with eachother by a pull mechanism (between each other, listed in the DAO).\\nUnlisted servers, sync again those who are listed but. Listed servers never pull changes from unlisted servers.\\n* Big scenes are constrained by server configurations or proxy limitations of the deployed nodes (i.e. body post size). Nowadays, those limitations are workarounded by selecting another server with a different configuration (Kyllian's).\\n### Use cases\\n* I want to push my scene to my own server and I expect it to replicate to other servers.\\n* Create a \"local-staging\" environment in which land owners or creators can synchronize the whole world from a DAO node and test locally.\\n* Broadcast of changes should be optional.\\n\n\n##Decision\n* Enable custom Catalyst servers from Explorer\\n* Define action plan for broadcasting\/commiting local deployments to the DAO servers. [issue](https:\/\/github.com\/decentraland\/catalyst\/issues\/195)\\n","Predictions":"* Enable custom Catalyst servers from Explorern* Define action plan for broadcasting\/commiting local deployments to the DAO servers. [issue](https:\/\/github.com\/decentraland\/catalyst\/issues\/195)n"}
{"File Name":"docspell\/0004_iso8601vsEpoch.md","Context":"# Context and Problem Statement\nThe question is whether the REST Api should return an ISO8601\nformatted string in UTC timezone, or the unix time (number of\nmilliseconds since 1970-01-01).\nThere is quite some controversy about it.\n- <https:\/\/stackoverflow.com\/questions\/47426786\/epoch-or-iso8601-date-format>\n- <https:\/\/nbsoftsolutions.com\/blog\/designing-a-rest-api-unix-time-vs-iso-8601>\nIn my opinion, the ISO8601 format (always UTC) is better. The reason\nis the better readability. But elm folks are on the other side:\n- <https:\/\/package.elm-lang.org\/packages\/elm\/time\/1.0.0#iso-8601>\n- <https:\/\/package.elm-lang.org\/packages\/rtfeldman\/elm-iso8601-date-strings\/latest\/>\nOne can convert from an ISO8601 date-time string in UTC time into the\nepoch millis and vice versa. So it is the same to me. There is no less\ninformation in a ISO8601 string than in the epoch millis.\nTo avoid confusion, all date\/time values should use the same encoding.\n# Decision Outcome\nI go with the epoch time. Every timestamp\/date-time values is\ntransfered as Unix timestamp.\nReasons:\n- the Elm application needs to frequently calculate with these values\nto render the current waiting time etc. This is better if there are\nnumbers without requiring to parse dates first\n- Since the UI is written with Elm, it's probably good to adopt their\nstyle\n","Decision":"I go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n","tokens":337,"id":280,"text":"# Context and Problem Statement\\nThe question is whether the REST Api should return an ISO8601\\nformatted string in UTC timezone, or the unix time (number of\\nmilliseconds since 1970-01-01).\\nThere is quite some controversy about it.\\n- <https:\/\/stackoverflow.com\/questions\/47426786\/epoch-or-iso8601-date-format>\\n- <https:\/\/nbsoftsolutions.com\/blog\/designing-a-rest-api-unix-time-vs-iso-8601>\\nIn my opinion, the ISO8601 format (always UTC) is better. The reason\\nis the better readability. But elm folks are on the other side:\\n- <https:\/\/package.elm-lang.org\/packages\/elm\/time\/1.0.0#iso-8601>\\n- <https:\/\/package.elm-lang.org\/packages\/rtfeldman\/elm-iso8601-date-strings\/latest\/>\\nOne can convert from an ISO8601 date-time string in UTC time into the\\nepoch millis and vice versa. So it is the same to me. There is no less\\ninformation in a ISO8601 string than in the epoch millis.\\nTo avoid confusion, all date\/time values should use the same encoding.\\n# Decision Outcome\\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n\n\n##Decision\nI go with the epoch time. Every timestamp\/date-time values is\\ntransfered as Unix timestamp.\\nReasons:\\n- the Elm application needs to frequently calculate with these values\\nto render the current waiting time etc. This is better if there are\\nnumbers without requiring to parse dates first\\n- Since the UI is written with Elm, it's probably good to adopt their\\nstyle\\n","Predictions":"- The Elm application needs to frequently calculate with these valuesnto render the current waiting time etc. This is better if there arennumbers without requiring to parse dates firstn- Since the UI is written with Elm, it's probably good to adopt theirnstylen"}
{"File Name":"community\/0007-github-actions.md","Context":"## [Context](https:\/\/docs.google.com\/document\/d\/1zRy8OCZ_JJpbs6scwrvx5PYkWwaWbR-5DLuq3AujhBA\/edit)\nLibero products need automated builds for pull requests and release candidates.\nTravis CI has been [acquired](https:\/\/techcrunch.com\/2019\/01\/23\/idera-acquires-travis-ci\/) by a private equity firm and has an [uncertain future](https:\/\/twitter.com\/ReinH\/status\/1098663375985229825). We also have run into performance bottlenecks of [5 concurrent jobs](https:\/\/travis-ci.com\/plans) in the open source offer.\nGithub Actions is a Github-native general workflow system that can provide CI\/CD capabilities; it has been in general availability [since November](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/). It offers fully managed, sandboxed environments and a per-repository limit to concurrency.\n","Decision":"We will create Github Actions builds for all new projects.\\nWe will port existing Travis CI builds of maintained projects onto Github Actions, on an as-needed basis.\\n","tokens":222,"id":3456,"text":"## [Context](https:\/\/docs.google.com\/document\/d\/1zRy8OCZ_JJpbs6scwrvx5PYkWwaWbR-5DLuq3AujhBA\/edit)\\nLibero products need automated builds for pull requests and release candidates.\\nTravis CI has been [acquired](https:\/\/techcrunch.com\/2019\/01\/23\/idera-acquires-travis-ci\/) by a private equity firm and has an [uncertain future](https:\/\/twitter.com\/ReinH\/status\/1098663375985229825). We also have run into performance bottlenecks of [5 concurrent jobs](https:\/\/travis-ci.com\/plans) in the open source offer.\\nGithub Actions is a Github-native general workflow system that can provide CI\/CD capabilities; it has been in general availability [since November](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/). It offers fully managed, sandboxed environments and a per-repository limit to concurrency.\\n\n\n##Decision\nWe will create Github Actions builds for all new projects.\\nWe will port existing Travis CI builds of maintained projects onto Github Actions, on an as-needed basis.\\n","Predictions":"We will use a serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized serialized"}
{"File Name":"verify-event-store-schema\/0003-database-migrations-should-be-designed-for-zero-downtime.md","Context":"## Context\nSee also ADR 0002 \"Database migrations are standalone releases\"\nAs our system is designed for zero downtime, we have to be careful that\nwe don't change the database in a way that causes production issues\n","Decision":"Where possible, we should avoid database migrations that will lock the database\\nfor any significant amount of time.  This is hard to enforce, but we will\\nmake sure there is documentation in the project README (and here!) on ways\\nto achieve this.\\nThis mostly affects index creation and changes - we have several years of data\\nin our database, and adding or changing indexes can be slow.  In general,\\nyou should use the `CREATE INDEX CONCURRENTLY` option to let indexes be\\ncreated in a non-blocking way.  See https:\/\/www.postgresql.org\/docs\/current\/static\/sql-createindex.html\\nIf you want to `ALTER INDEX` or `REINDEX`, they can't be concurrent - in this\\ncase you'll need to look at stopping the Event Recorder lambda, allowing messages\\nto queue up while the index change is made.  *BEWARE* however that SQS queues\\nonly allow 100,000 messages, and at peak load we have historically sent 75,000\\nmessages an hour, so you have a somewhat limited amount of time to run such a change.\\nIf you have a very complex change, you should consider:\\n- Dropping the index then running `CREATE INDEX CONCURRENTLY` rather than\\naltering indexes - generally our reports run intermittently, so it is safe to have\\nno indexes for a period of time, data will still be appended with no problems\\n- Performance testing the change - we have a large fake dataset available that\\ncan be used to simulate a production database in a test environment\\n- Duplicating the database - you could apply the change to a new database containing\\na copy of production data, then switch databases, and migrate any missed changes\\nfrom the old database to the new.\\n### Transactional DDL changes\\nMost Postgresql schema changes can be made transactionally - this is\\na great feature, as it allows for making multiple changes and having them\\nall roll back if something goes wrong.  For example:\\n```\\nBEGIN;\\nALTER TABLE fizzbuzz RENAME COLUMN foo TO bar;\\nUPDATE TABLE fizzbuzz set foo = 'splat';\\nCOMMIT;\\n```\\nIn this case the `UPDATE` will fail, so the column rename will be reverted.\\n*However* note that `CREATE INDEX CONCURRENTLY` does not work in a transaction -\\nit depends on being able to change the table incrementally, which doesn't fit\\nthe transaction model.  If the index creation fails, you are recommended to\\ndrop the index and re-create it, as it won't be rolled back and may be\\npartially created.\\n### Avoiding blocking changes\\nThere is a useful table in [this article](https:\/\/www.citusdata.com\/blog\/2018\/02\/15\/when-postgresql-blocks\/) which I've reproduced below (as the article may disappear):\\n| Runs concurrently with           | SELECT | INSERT UPDATE DELETE | CREATE INDEX CONC VACUUM ANALYZE | CREATE INDEX | CREATE TRIGGER | ALTER TABLE DROP TABLE TRUNCATE VACUUM FULL |\\n|----------------------------------|--------|----------------------|----------------------------------|--------------|----------------|---------------------------------------------|\\n| SELECT                           | Y      | Y                    | Y                                | Y            | Y              | No                                          |\\n| INSERT UPDATE DELETE             | Y      | Y                    | Y                                | No           | No             | No                                          |\\n| CREATE INDEX CONC VACUUM ANALYZE | Y      | Y                    | No                               | No           | No             | No                                          |\\n| CREATE INDEX                     | Y      | No                   | No                               | Y            | No             | No                                          |\\n| CREATE TRIGGER                   | Y      | No                   | No                               | No           | No             | No                                          |\\n| ALTER TABLE etc                  | No     | No                   | No                               | No           | No             | No                                          |\\nOur reports should only use SELECT so most operations won't block them.\\nThe Event Recorder however needs to insert data, so you can't run\\nany of the index modification changes (apart from `CREATE INDEX CONCURRENTLY`) without risking blocking.\\nNote however that some changes may be fast enough despite blocking - adding a column\\nfor example.  However you should performance test these changes against a\\nproduction-sized database to be sure!\\n","tokens":48,"id":317,"text":"## Context\\nSee also ADR 0002 \"Database migrations are standalone releases\"\\nAs our system is designed for zero downtime, we have to be careful that\\nwe don't change the database in a way that causes production issues\\n\n\n##Decision\nWhere possible, we should avoid database migrations that will lock the database\\nfor any significant amount of time.  This is hard to enforce, but we will\\nmake sure there is documentation in the project README (and here!) on ways\\nto achieve this.\\nThis mostly affects index creation and changes - we have several years of data\\nin our database, and adding or changing indexes can be slow.  In general,\\nyou should use the `CREATE INDEX CONCURRENTLY` option to let indexes be\\ncreated in a non-blocking way.  See https:\/\/www.postgresql.org\/docs\/current\/static\/sql-createindex.html\\nIf you want to `ALTER INDEX` or `REINDEX`, they can't be concurrent - in this\\ncase you'll need to look at stopping the Event Recorder lambda, allowing messages\\nto queue up while the index change is made.  *BEWARE* however that SQS queues\\nonly allow 100,000 messages, and at peak load we have historically sent 75,000\\nmessages an hour, so you have a somewhat limited amount of time to run such a change.\\nIf you have a very complex change, you should consider:\\n- Dropping the index then running `CREATE INDEX CONCURRENTLY` rather than\\naltering indexes - generally our reports run intermittently, so it is safe to have\\nno indexes for a period of time, data will still be appended with no problems\\n- Performance testing the change - we have a large fake dataset available that\\ncan be used to simulate a production database in a test environment\\n- Duplicating the database - you could apply the change to a new database containing\\na copy of production data, then switch databases, and migrate any missed changes\\nfrom the old database to the new.\\n### Transactional DDL changes\\nMost Postgresql schema changes can be made transactionally - this is\\na great feature, as it allows for making multiple changes and having them\\nall roll back if something goes wrong.  For example:\\n```\\nBEGIN;\\nALTER TABLE fizzbuzz RENAME COLUMN foo TO bar;\\nUPDATE TABLE fizzbuzz set foo = 'splat';\\nCOMMIT;\\n```\\nIn this case the `UPDATE` will fail, so the column rename will be reverted.\\n*However* note that `CREATE INDEX CONCURRENTLY` does not work in a transaction -\\nit depends on being able to change the table incrementally, which doesn't fit\\nthe transaction model.  If the index creation fails, you are recommended to\\ndrop the index and re-create it, as it won't be rolled back and may be\\npartially created.\\n### Avoiding blocking changes\\nThere is a useful table in [this article](https:\/\/www.citusdata.com\/blog\/2018\/02\/15\/when-postgresql-blocks\/) which I've reproduced below (as the article may disappear):\\n| Runs concurrently with           | SELECT | INSERT UPDATE DELETE | CREATE INDEX CONC VACUUM ANALYZE | CREATE INDEX | CREATE TRIGGER | ALTER TABLE DROP TABLE TRUNCATE VACUUM FULL |\\n|----------------------------------|--------|----------------------|----------------------------------|--------------|----------------|---------------------------------------------|\\n| SELECT                           | Y      | Y                    | Y                                | Y            | Y              | No                                          |\\n| INSERT UPDATE DELETE             | Y      | Y                    | Y                                | No           | No             | No                                          |\\n| CREATE INDEX CONC VACUUM ANALYZE | Y      | Y                    | No                               | No           | No             | No                                          |\\n| CREATE INDEX                     | Y      | No                   | No                               | Y            | No             | No                                          |\\n| CREATE TRIGGER                   | Y      | No                   | No                               | No           | No             | No                                          |\\n| ALTER TABLE etc                  | No     | No                   | No                               | No           | No             | No                                          |\\nOur reports should only use SELECT so most operations won't block them.\\nThe Event Recorder however needs to insert data, so you can't run\\nany of the index modification changes (apart from `CREATE INDEX CONCURRENTLY`) without risking blocking.\\nNote however that some changes may be fast enough despite blocking - adding a column\\nfor example.  However you should performance test these changes against a\\nproduction-sized database to be sure!\\n","Predictions":"We will use the SQL database for all database migrations.n"}
{"File Name":"edgex-docs\/014-Secret-Provider-For-All.md","Context":"- [Context](#context)\n* [Existing Implementations](#existing-implementations)\n+ [What is a Secret?](#what-is-a-secret)\n+ [Service Exclusive vs Service Shared Secrets](#service-exclusive-vs-service-shared-secrets)\n+ [Known and Unknown Services](#known-and-unknown-services)\n+ [Static Secrets and Runtime Secrets](#static-secrets-and-runtime-secrets)\n+ [Interfaces and factory methods](#interfaces-and-factory-methods)\n- [Bootstrap's current implementation](#bootstraps-current-implementation)\n* [Interfaces](#interfaces)\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\n- [App SDK's current implementation](#app-sdks-current-implementation)\n* [Interface](#interface)\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\n+ [Secret Store for non-secure mode](#secret-store-for-non-secure-mode)\n- [InsecureSecrets Configuration](#insecuresecrets-configuration)\n- [Decision](#decision)\n* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\n* [Abstraction Interface](#abstraction-interface)\n* [Implementation](#implementation)\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\n+ [Caching of Secrets](#caching-of-secrets)\n+ [Insecure Secrets](#insecure-secrets)\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\n+ [Mocks](#mocks)\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\n- [Go Services](#go-services)\n- [C Device Service](#c-device-service)\n* [Consequences](#consequences)\n","Decision":"* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\nThe new `SecretProvider` abstraction defined by this ADR is a combination of the two implementations described above in the [Existing Implementations](#existing-implementations) section.\\n### Only Exclusive Secret Stores\\nTo simplify the `SecretProvider` abstraction, we need to reduce to using only exclusive `SecretStores`. This allows all the APIs to deal with a single `SecretClient`, rather than the split up way we currently have in Application Services. This requires that the current Application Service shared secrets (database credentials) must be copied into each Application Service's exclusive `SecretStore` when it is created.\\nThe challenge is how do we seed static secrets for unknown services when they become known.  As described above in the [Known and Unknown Services](#known-and-unknown-services) section above,  services currently identify themselves for exclusive `SecretStore` creation via the `EDGEX_ADD_SECRETSTORE_TOKENS` environment variable on security-secretstore-setup. This environment variable simply takes a comma separated list of service names.\\n```yaml\\nEDGEX_ADD_SECRETSTORE_TOKENS: \"<service-name1>,<service-name2>\"\\n```\\nIf we expanded this to add an optional list of static secret identifiers for each service, i.e.  `appservice\/redisdb`, the exclusive store could also be seeded with a copy of static shared secrets. In this case the Redis database credentials for the Application Services' shared database. The environment variable name will change to `ADD_SECRETSTORE` now that it is more than just tokens.\\n```yaml\\nADD_SECRETSTORE: \"app-service-xyz[appservice\/redisdb]\"\\n```\\n> *Note: The secret identifier here is the short path to the secret in the existing **appservice**  `SecretStore`. In the above example this expands to the full path of `\/secret\/edgex\/appservice\/redisdb`*\\nThe above example results in the Redis credentials being copied into app-service-xyz's `SecretStore` at `\/secret\/edgex\/app-service-xyz\/redis`.\\nSimilar approach could be taken for Message Bus credentials where a common `SecretStore` is created with the Message Bus credentials saved. The services request the credentials are copied into their exclusive `SecretStore` using `common\/messagebus` as the secret identifier.\\nFull specification for the environment variable's value is a comma separated list of service entries defined as:\\n```\\n<service-name1>[optional list of static secret IDs sperated by ;],<service-name2>[optional list of static secret IDs sperated by ;],...\\n```\\nExample with one service specifying IDs for static secrets and one without static secrets\\n```yaml\\nADD_SECRETSTORE: \"appservice-xyz[appservice\/redisdb; common\/messagebus], appservice-http-export\"\\n```\\nWhen the `ADD_SECRETSTORE` environment variable is processed to create these `SecretStores`, it will copy the specified saved secrets from the initial `SecretStore` into the service's `SecretStore`. This all depends on the completion of database or other credential bootstrapping and the secrets having been stored prior to the environment variable being processed. security-secretstore-setup will need to be refactored to ensure this sequencing.\\n### Abstraction Interface\\nThe following will be the new `SecretProvider` abstraction interface used by all Edgex services\\n```go\\ntype SecretProvider interface {\\n\/\/ Stores new secrets into the service's exclusive SecretStore at the specified path.\\nStoreSecrets(path string, secrets map[string]string) error\\n\/\/ Retrieves secrets from the service's exclusive SecretStore at the specified path.\\nGetSecrets(path string, _ ...string) (map[string]string, error)\\n\/\/ Sets the secrets lastupdated time to current time.\\nSecretsUpdated()\\n\/\/ Returns the secrets last updated time\\nSecretsLastUpdated() time.Time\\n}\\n```\\n> *Note: The `GetDatabaseCredentials` and `GetCertificateKeyPair` APIs have been removed. These are no longer needed since insecure database credentials will no longer be stored in the `DatabaseInfo` configuration and certificate key pairs are secrets like any others. This allows these secrets to be retrieved via the `GetSecrets` API.*\\n### Implementation\\n#### Factory Method and Bootstrap Handler\\nThe factory method and bootstrap handler will follow that currently in the Bootstrap implementation with some tweaks. Rather than putting the two split interfaces into the DIC, it will put just the single interface instance into the DIC. See details in the [Interfaces and factory methods](#interfaces-and-factory-methods) section above under **Existing Implementations**.\\n#### Caching of Secrets\\nSecrets will be cached as they are currently in the Application Service implementation\\n#### Insecure Secrets\\nInsecure Secrets will be handled as they are currently in the Application Service implementation. `DatabaseInfo` configuration will no longer be an option for storing the insecure database credentials. They will be stored in the `InsecureSecrets` configuration only.\\n```toml\\n[Writable.InsecureSecrets]\\n[Writable.InsecureSecrets.DB]\\npath = \"redisdb\"\\n[Writable.InsecureSecrets.DB.Secrets]\\nusername = \"\"\\npassword = \"\"\\n```\\n##### Handling on-the-fly changes to `InsecureSecrets`\\nAll services will need to handle the special processing when `InsecureSecrets` are changed on-the-fly via Consul. Since this will now be a common configuration item in `Writable` it can be handled in `go-mod-bootstrap` along with existing log level processing. This special processing will be taken from App SDK.\\n#### Mocks\\nProper mock of the `SecretProvider` interface will be created with `Mockery` to be used in unit tests. Current mock in App SDK is hand written rather then generated with `Mockery`.\\n#### Where will `SecretProvider` reside?\\n##### Go Services\\nThe final decision to make is where will this new `SecretProvider` abstraction reside? Originally is was assumed that it would reside in `go-mod-secrets`, which seems logical. If we were to attempt this with the implementation including the bootstrap handler, `go-mod-secrets` would have a dependency on `go-mod-bootstrap` which will likely create a circular dependency.\\nRefactoring the existing implementation in `go-mod-bootstrap` and have it reside there now seems to be the best choice.\\n##### C Device Service\\nThe C Device SDK will implement the same `SecretProvider` abstraction, InsecureSercets configuration and the underling `SecretStore` client.\\n### Consequences\\n- All service's will have `Writable.InsecureSecrets` section added to their configuration\\n- `InsecureSecrets` definition will be moved from App SDK to go-mod-bootstrap\\n- Go Device SDK will add the SecretProvider to it's bootstrapping\\n- C Device SDK implementation could be big lift?\\n- ` SecretStore`configuration section will be added to all Device Services\\n- edgex-go services will be modified to use the single `SecretProvider` interface from the DIC in place of current usage of the `GetDatabaseCredentials` and `GetCertificateKeyPair` interfaces.\\n- Calls to `GetDatabaseCredentials` and `GetCertificateKeyPair` will be replaced with calls to `GetSecrets` API and appropriate processing of the returned secrets will be added.\\n- App SDK will be modified to use `GetSecrets` API in place of the `GetDatabaseCredentials` API\\n- App SDK will be modified to use the new `SecretProvider` bootstrap handler\\n- app-service-configurable's configuration profiles as well as all the Application Service examples configurations will be updated to remove the `SecretStoreExclusive` configuration and just use the existing `SecretStore` configuration\\n- security-secretstore-setup will be enhanced as described in the [Only Exclusive Secret Stores](#only-exclusive-secret-stores) section above\\n- Adding new services that need static secrets added to their `SecretStore` requires stopping and restarting all the services. The is because security-secretstore-setup has completed but not stopped. If it is rerun without stopping the other services, there tokens and static secrets will have changed. The planned refactor of `security-secretstore-setup` will attempt to resolve this.\\n- Snaps do not yet support setting the environment variable for adding SecretStore. It is planned for Ireland release.\\n","tokens":402,"id":947,"text":"- [Context](#context)\\n* [Existing Implementations](#existing-implementations)\\n+ [What is a Secret?](#what-is-a-secret)\\n+ [Service Exclusive vs Service Shared Secrets](#service-exclusive-vs-service-shared-secrets)\\n+ [Known and Unknown Services](#known-and-unknown-services)\\n+ [Static Secrets and Runtime Secrets](#static-secrets-and-runtime-secrets)\\n+ [Interfaces and factory methods](#interfaces-and-factory-methods)\\n- [Bootstrap's current implementation](#bootstraps-current-implementation)\\n* [Interfaces](#interfaces)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n- [App SDK's current implementation](#app-sdks-current-implementation)\\n* [Interface](#interface)\\n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)\\n+ [Secret Store for non-secure mode](#secret-store-for-non-secure-mode)\\n- [InsecureSecrets Configuration](#insecuresecrets-configuration)\\n- [Decision](#decision)\\n* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\n\n\n##Decision\n* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)\\n* [Abstraction Interface](#abstraction-interface)\\n* [Implementation](#implementation)\\n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)\\n+ [Caching of Secrets](#caching-of-secrets)\\n+ [Insecure Secrets](#insecure-secrets)\\n- [Handling on-the-fly changes to `InsecureSecrets`](#handling-on-the-fly-changes-to-insecuresecrets)\\n+ [Mocks](#mocks)\\n+ [Where will `SecretProvider` reside?](#where-will-secretprovider-reside)\\n- [Go Services](#go-services)\\n- [C Device Service](#c-device-service)\\n* [Consequences](#consequences)\\nThe new `SecretProvider` abstraction defined by this ADR is a combination of the two implementations described above in the [Existing Implementations](#existing-implementations) section.\\n### Only Exclusive Secret Stores\\nTo simplify the `SecretProvider` abstraction, we need to reduce to using only exclusive `SecretStores`. This allows all the APIs to deal with a single `SecretClient`, rather than the split up way we currently have in Application Services. This requires that the current Application Service shared secrets (database credentials) must be copied into each Application Service's exclusive `SecretStore` when it is created.\\nThe challenge is how do we seed static secrets for unknown services when they become known.  As described above in the [Known and Unknown Services](#known-and-unknown-services) section above,  services currently identify themselves for exclusive `SecretStore` creation via the `EDGEX_ADD_SECRETSTORE_TOKENS` environment variable on security-secretstore-setup. This environment variable simply takes a comma separated list of service names.\\n```yaml\\nEDGEX_ADD_SECRETSTORE_TOKENS: \"<service-name1>,<service-name2>\"\\n```\\nIf we expanded this to add an optional list of static secret identifiers for each service, i.e.  `appservice\/redisdb`, the exclusive store could also be seeded with a copy of static shared secrets. In this case the Redis database credentials for the Application Services' shared database. The environment variable name will change to `ADD_SECRETSTORE` now that it is more than just tokens.\\n```yaml\\nADD_SECRETSTORE: \"app-service-xyz[appservice\/redisdb]\"\\n```\\n> *Note: The secret identifier here is the short path to the secret in the existing **appservice**  `SecretStore`. In the above example this expands to the full path of `\/secret\/edgex\/appservice\/redisdb`*\\nThe above example results in the Redis credentials being copied into app-service-xyz's `SecretStore` at `\/secret\/edgex\/app-service-xyz\/redis`.\\nSimilar approach could be taken for Message Bus credentials where a common `SecretStore` is created with the Message Bus credentials saved. The services request the credentials are copied into their exclusive `SecretStore` using `common\/messagebus` as the secret identifier.\\nFull specification for the environment variable's value is a comma separated list of service entries defined as:\\n```\\n<service-name1>[optional list of static secret IDs sperated by ;],<service-name2>[optional list of static secret IDs sperated by ;],...\\n```\\nExample with one service specifying IDs for static secrets and one without static secrets\\n```yaml\\nADD_SECRETSTORE: \"appservice-xyz[appservice\/redisdb; common\/messagebus], appservice-http-export\"\\n```\\nWhen the `ADD_SECRETSTORE` environment variable is processed to create these `SecretStores`, it will copy the specified saved secrets from the initial `SecretStore` into the service's `SecretStore`. This all depends on the completion of database or other credential bootstrapping and the secrets having been stored prior to the environment variable being processed. security-secretstore-setup will need to be refactored to ensure this sequencing.\\n### Abstraction Interface\\nThe following will be the new `SecretProvider` abstraction interface used by all Edgex services\\n```go\\ntype SecretProvider interface {\\n\/\/ Stores new secrets into the service's exclusive SecretStore at the specified path.\\nStoreSecrets(path string, secrets map[string]string) error\\n\/\/ Retrieves secrets from the service's exclusive SecretStore at the specified path.\\nGetSecrets(path string, _ ...string) (map[string]string, error)\\n\/\/ Sets the secrets lastupdated time to current time.\\nSecretsUpdated()\\n\/\/ Returns the secrets last updated time\\nSecretsLastUpdated() time.Time\\n}\\n```\\n> *Note: The `GetDatabaseCredentials` and `GetCertificateKeyPair` APIs have been removed. These are no longer needed since insecure database credentials will no longer be stored in the `DatabaseInfo` configuration and certificate key pairs are secrets like any others. This allows these secrets to be retrieved via the `GetSecrets` API.*\\n### Implementation\\n#### Factory Method and Bootstrap Handler\\nThe factory method and bootstrap handler will follow that currently in the Bootstrap implementation with some tweaks. Rather than putting the two split interfaces into the DIC, it will put just the single interface instance into the DIC. See details in the [Interfaces and factory methods](#interfaces-and-factory-methods) section above under **Existing Implementations**.\\n#### Caching of Secrets\\nSecrets will be cached as they are currently in the Application Service implementation\\n#### Insecure Secrets\\nInsecure Secrets will be handled as they are currently in the Application Service implementation. `DatabaseInfo` configuration will no longer be an option for storing the insecure database credentials. They will be stored in the `InsecureSecrets` configuration only.\\n```toml\\n[Writable.InsecureSecrets]\\n[Writable.InsecureSecrets.DB]\\npath = \"redisdb\"\\n[Writable.InsecureSecrets.DB.Secrets]\\nusername = \"\"\\npassword = \"\"\\n```\\n##### Handling on-the-fly changes to `InsecureSecrets`\\nAll services will need to handle the special processing when `InsecureSecrets` are changed on-the-fly via Consul. Since this will now be a common configuration item in `Writable` it can be handled in `go-mod-bootstrap` along with existing log level processing. This special processing will be taken from App SDK.\\n#### Mocks\\nProper mock of the `SecretProvider` interface will be created with `Mockery` to be used in unit tests. Current mock in App SDK is hand written rather then generated with `Mockery`.\\n#### Where will `SecretProvider` reside?\\n##### Go Services\\nThe final decision to make is where will this new `SecretProvider` abstraction reside? Originally is was assumed that it would reside in `go-mod-secrets`, which seems logical. If we were to attempt this with the implementation including the bootstrap handler, `go-mod-secrets` would have a dependency on `go-mod-bootstrap` which will likely create a circular dependency.\\nRefactoring the existing implementation in `go-mod-bootstrap` and have it reside there now seems to be the best choice.\\n##### C Device Service\\nThe C Device SDK will implement the same `SecretProvider` abstraction, InsecureSercets configuration and the underling `SecretStore` client.\\n### Consequences\\n- All service's will have `Writable.InsecureSecrets` section added to their configuration\\n- `InsecureSecrets` definition will be moved from App SDK to go-mod-bootstrap\\n- Go Device SDK will add the SecretProvider to it's bootstrapping\\n- C Device SDK implementation could be big lift?\\n- ` SecretStore`configuration section will be added to all Device Services\\n- edgex-go services will be modified to use the single `SecretProvider` interface from the DIC in place of current usage of the `GetDatabaseCredentials` and `GetCertificateKeyPair` interfaces.\\n- Calls to `GetDatabaseCredentials` and `GetCertificateKeyPair` will be replaced with calls to `GetSecrets` API and appropriate processing of the returned secrets will be added.\\n- App SDK will be modified to use `GetSecrets` API in place of the `GetDatabaseCredentials` API\\n- App SDK will be modified to use the new `SecretProvider` bootstrap handler\\n- app-service-configurable's configuration profiles as well as all the Application Service examples configurations will be updated to remove the `SecretStoreExclusive` configuration and just use the existing `SecretStore` configuration\\n- security-secretstore-setup will be enhanced as described in the [Only Exclusive Secret Stores](#only-exclusive-secret-stores) section above\\n- Adding new services that need static secrets added to their `SecretStore` requires stopping and restarting all the services. The is because security-secretstore-setup has completed but not stopped. If it is rerun without stopping the other services, there tokens and static secrets will have changed. The planned refactor of `security-secretstore-setup` will attempt to resolve this.\\n- Snaps do not yet support setting the environment variable for adding SecretStore. It is planned for Ireland release.\\n","Predictions":"* [Existing Implementations](#existing-implementations)n* [What is a Secret?](#what-is-a-secret)n* [Service Exclusive vs Service Shared Secrets](#service-only-vs-service-shared-secrets)n+ [Known and Unknown Services](#known-and-unknown-services)n+ [Static Secrets and Runtime Secrets](#static-secrets-and-runtime-secrets)n+ [Interfaces and factory methods](#interfaces-and-factory-methods)n* [Bootstrap's current implementation](#bootstraps-current-implementation)n* [Interfaces](#interfaces)n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)n* [App SDK's current implementation](#app-sdks-current-implementation)n* [Interface](#interface)n* [Factory and bootstrap handler methods](#factory-and-bootstrap-handler-methods)n+ [Secret Store for non-secure mode](#secret-store-for-non-secure-mode)n- [InsecureSecrets Configuration](#insecuresecrets-configuration)n- [Decision](#decision)n* [Only Exclusive Secret Stores](#only-exclusive-secret-stores)n* [Abstraction Interface](#abstraction-interface)n* [Implementation](#implementation)n+ [Factory Method and Bootstrap Handler](#factory-method-and-bootstrap-handler)n+ [Caching of Secrets](#caching-of-secrets)n+ [Insecure Secrets](#insecure-secrets)n- [Handling on-the-fly changes to Insecure Secrets](#handling-on-the-fly-changes-to-Insecure Secrets)n- [Handling on-the-fly changes to Insecure Secrets](#insecure-secrets)n- [Handling on-the-fly changes to Insecure Secrets](#insecure-secrets)n- [Handling on-the-fly changes to Insecure Secrets](#insecure-secrets)n- [Handling on-the-fly changes to Insecure Secrets](#insecure-secrets)n- [Handling on-the-fly changes to Insecure Secrets](#insecure-secrets)n- [Handling on-the-fly changes to Insecure Secrets](#insecure-secrets)n- [Handling on-the-fly changes to Insecure Secrets](#insecure-secrets)n- [Handling on-the-fly changes to Insecure Secrets](#insecure-secrets)n- [Handling on-the-fly changes to Insecure Secrets](#insecure-secrets)n- [Handling on-the-fly changes to Insecure Secrets](#insecure-secrets)n- [Handling on-the-fly changes to Insecure Secrets](#insecure-secrets)n- [Handling on-the-fly changes to Insecure Secrets](#insecure-secrets)n- [Handling on-the-fly changes to Insecure Secrets](#insecure-secrets)n- [Handling on-the-fly changes to Insecure Secrets](#insecure-secrets)n- [Handling on-the-fly changes to Insecure Secrets](#insecure-secrets)n- [Handling on-the-fly changes to Insecure Secrets](#in"}
{"File Name":"fundraising-application\/005_URL_Schema.md","Context":"## Context\nWhen we developed the Fundraising application, we did not pay close attention to the URL schema and ended up with three different styles:\n* \"Slashy\", pseudo-[REST][1]-style URLs like `\/donation\/add`\n* \"action-sentences\" like `\/apply-for-membership`\n* a combination of both like `\/contact\/get-in-touch`\nWe don't do search engine optimization (SEO) in the form of \"meaningful, localized and stable URLs\", as the main traffic to the donation page comes from banners and we don't have much relevant content to that search engines can index.\n","Decision":"We will use the \"action-sentence\" style for URLs in the future. They should follow the pattern `verb-noun` or `verb-preposition-noun`.\\nOur reasoning behind the decision:\\n* They convey more information about what the route does, because we can use all verbs of the English language instead of restricting us to `GET` and `POST`.\\n* REST-style URLs are deceiving because our application has no real API and is not explicitly written with a [RESTful][1] architecture.\\n* We can still have a dedicated REST API in the future, by using the `\/api` route.\\n* The sentence style fits better to our use case architecture, which also read more like sentences.\\nWhenever we change a URL, we decide if we need to create a redirect from the old one to the new in the NGinX configuration. GET support is a good indicator for the need for a redirect. If route is more like a functional \"endpoint\" like `donation\/update`, then we don't need a redirect.\\nIf we need to add i18n information to the URL at some point, we will do it with a \"subdirectory prefix\", e.g. `\/de\/apply-for-membership`, `\/en\/apply-for-membership`. The cons listed at https:\/\/support.google.com\/webmasters\/answer\/182192?hl=en do not outweigh the benefits.\\n","tokens":128,"id":1522,"text":"## Context\\nWhen we developed the Fundraising application, we did not pay close attention to the URL schema and ended up with three different styles:\\n* \"Slashy\", pseudo-[REST][1]-style URLs like `\/donation\/add`\\n* \"action-sentences\" like `\/apply-for-membership`\\n* a combination of both like `\/contact\/get-in-touch`\\nWe don't do search engine optimization (SEO) in the form of \"meaningful, localized and stable URLs\", as the main traffic to the donation page comes from banners and we don't have much relevant content to that search engines can index.\\n\n\n##Decision\nWe will use the \"action-sentence\" style for URLs in the future. They should follow the pattern `verb-noun` or `verb-preposition-noun`.\\nOur reasoning behind the decision:\\n* They convey more information about what the route does, because we can use all verbs of the English language instead of restricting us to `GET` and `POST`.\\n* REST-style URLs are deceiving because our application has no real API and is not explicitly written with a [RESTful][1] architecture.\\n* We can still have a dedicated REST API in the future, by using the `\/api` route.\\n* The sentence style fits better to our use case architecture, which also read more like sentences.\\nWhenever we change a URL, we decide if we need to create a redirect from the old one to the new in the NGinX configuration. GET support is a good indicator for the need for a redirect. If route is more like a functional \"endpoint\" like `donation\/update`, then we don't need a redirect.\\nIf we need to add i18n information to the URL at some point, we will do it with a \"subdirectory prefix\", e.g. `\/de\/apply-for-membership`, `\/en\/apply-for-membership`. The cons listed at https:\/\/support.google.com\/webmasters\/answer\/182192?hl=en do not outweigh the benefits.\\n","Predictions":"We will use the \"action-sentence\" style for URLs in the future. They should follow the pattern verb-noun or verb-preposition-noun.nOur reasoning behind the decision:n* They convey more information about what the route does, because we can use all verbs of the English language instead of restricting us to GET and POST.n* REST-style URLs are deceiving because our application has no real API and is not explicitly written with a [RESTful][1] architecture.n* We can still have a dedicated REST API in the future, by using the \/api route.n* The sentence style fits better to our use case architecture, which also read more like sentences.nWhenever we change a URL, we decide if we need to create a redirect from the old one to the new in the NGinX configuration. GET support is a good indication for the need for a redirect. If route is more like a functional \"endpoint\" like donation\/update, then we don't need a redirect.nIf we need to add i18n information to the URL at some point, we will do it with a \"subdirectory prefix\", e.g. \/de\/apply-for-membership, \/en\/apply-for-membership. The cons listed at https:\/\/support.google.com\/webmasters\/answer\/18.n"}
{"File Name":"SoundCloudVisualizer\/0004-use-gulp-to-build-source-code.md","Context":"## Context\nThe application's JS code was loaded via `script` tags in `index.html`. As the project was migrated from Vanilla JS to AngularJS the number of source files that needed to be included grew, and this approach became unwieldy.\nI looked into how we could bundle our code. At the time of development Grunt and Gulp were the two main task runners available.\n","Decision":"[Gulp](https:\/\/gulpjs.com\/) will be used to bundle client-side code.\\n","tokens":81,"id":1576,"text":"## Context\\nThe application's JS code was loaded via `script` tags in `index.html`. As the project was migrated from Vanilla JS to AngularJS the number of source files that needed to be included grew, and this approach became unwieldy.\\nI looked into how we could bundle our code. At the time of development Grunt and Gulp were the two main task runners available.\\n\n\n##Decision\n[Gulp](https:\/\/gulpjs.com\/) will be used to bundle client-side code.\\n","Predictions":"We will use a Schema structure to store the data.n"}
{"File Name":"heptaconnect-docs\/2020-04-30-contracts-and-interfaces.md","Context":"## Context\nThere has to be a guide how to structure code to allow extendability.\nWe have to ensure that functionality can be exchanged without interfering with adjacent packages within the package hierarchy.\n","Decision":"The approved approach is using the language feature type hierarchy.\\nUsing interfaces, traits and contracts (abstract classes) is a good way to structure and allow replacements by ensuring certain behaviors.\\nWe use interfaces when multiple implementation will exist and are unknown at any time for the package that it is introduced in.\\nFor example, we do not know what kind of storage is used within the portal-base, but it will need some kind of storage.\\nTo supply some basic logic for commonly used interfaces we can provide traits for others to implement them easier.\\nWe use contracts similar to interfaces but use their advantages to contain any logic beforehand.\\nThis enables us to add additional code later with a reduced level of changes that can be non-breaking without removing the replacing option.\\nContracts are best without dependencies that have to be given in the constructor as this forces other implementations to follow this pattern regardless whether they need it.\\n","tokens":39,"id":3203,"text":"## Context\\nThere has to be a guide how to structure code to allow extendability.\\nWe have to ensure that functionality can be exchanged without interfering with adjacent packages within the package hierarchy.\\n\n\n##Decision\nThe approved approach is using the language feature type hierarchy.\\nUsing interfaces, traits and contracts (abstract classes) is a good way to structure and allow replacements by ensuring certain behaviors.\\nWe use interfaces when multiple implementation will exist and are unknown at any time for the package that it is introduced in.\\nFor example, we do not know what kind of storage is used within the portal-base, but it will need some kind of storage.\\nTo supply some basic logic for commonly used interfaces we can provide traits for others to implement them easier.\\nWe use contracts similar to interfaces but use their advantages to contain any logic beforehand.\\nThis enables us to add additional code later with a reduced level of changes that can be non-breaking without removing the replacing option.\\nContracts are best without dependencies that have to be given in the constructor as this forces other implementations to follow this pattern regardless whether they need it.\\n","Predictions":"We will use the stack class to create a stack class.n"}
{"File Name":"cf-k8s-networking\/0007-maintain-generated-istio.md","Context":"## Context \ud83e\udd14\nCf-k8s-networking was designed to be integrated with\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/). The Istio\ninstallation used to be maintained by\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/), but the networking\nteam needed to be able to easily make changes to [Istio](https:\/\/istio.io\/)\nconfiguration to enable more networking features for [Cloud\nFoundry](https:\/\/www.cloudfoundry.org\/).\n","Decision":"We decided to move the scripts to build Istio configuration, and maintain a\\ngenerated Istio configuration within the cf-k8s-networking repository.\\nThe build scripts and `ytt` overlays for Istio live in this repo (links removed\\nas they are no longer relevant or accurate). **UPDATE** This configuration has\\nmoved as a result of [ADR\\n017](.\/0017-moving-istio-configuration-out-of-this-repo.md).\\n","tokens":133,"id":2236,"text":"## Context \ud83e\udd14\\nCf-k8s-networking was designed to be integrated with\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/). The Istio\\ninstallation used to be maintained by\\n[cf-for-k8s](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/), but the networking\\nteam needed to be able to easily make changes to [Istio](https:\/\/istio.io\/)\\nconfiguration to enable more networking features for [Cloud\\nFoundry](https:\/\/www.cloudfoundry.org\/).\\n\n\n##Decision\nWe decided to move the scripts to build Istio configuration, and maintain a\\ngenerated Istio configuration within the cf-k8s-networking repository.\\nThe build scripts and `ytt` overlays for Istio live in this repo (links removed\\nas they are no longer relevant or accurate). **UPDATE** This configuration has\\nmoved as a result of [ADR\\n017](.\/0017-moving-istio-configuration-out-of-this-repo.md).\\n","Predictions":"We decided to move the scripts to build Istio configuration, and maintain angenerated Istio configuration within the cf-k8s-networking repository.nThe build scripts and ytt overlays for Istio live in this repo (links removednas they are no longer relevant or accurate). **UPDATE** This configuration hasnmoved as a result of [ADRn017](.\/0017-moving-istio-configuration-out-of-this-repo.md).n"}
{"File Name":"architecture\/0008-code-owners.md","Context":"## Context\nWe get contributed a lot of new integrations, new features to integrations and refactors of integrations. The Home Assistant project is honored to receive so many great contributions to our project!\nUnfortunately, as a contributor, adding oneself as (the, or one of the) code owners of the integration contributed or contributed to, doesn't always happen spontaneously.\nNot adding oneself as a code owner has drawbacks for the project:\n- The contributor doesn't \"own\" (in terms of taking responsibility) his code, and thus contribution, in a more formal fashion.\n- Without being listed as a code owner, our GitHub bot will not notify the contributor, when an issue for the integration is reported, quite possibly affecting his contribution.\n- Integrations have ended up or may end up with having a single code owner or no code owners at all.\nAs a result of this:\n- Bugs are less likely to be resolved in a timely fashion (turn-around time).\n- Integrations are more prone to break in the future.\n- Integration with a single code owner:\n- Do not benefit from multiple code owners being familiar with the integration in terms of code review and general turn-around time.\n- Become largely unmaintained when the single listed code owner can no longer contribute to the project.\nWe have quite a few integrations that haven't got multiple code owners or don't have a code owner.\nDuring the design discussion of this ADR, it also became clear, that the term \"code owner\" has different meanings to our members and contributors. Some interpret it as an honorable mention of contribution; others see it as \"taking responsibility\".\n","Decision":"Code ownership for an integration defined:\\nThe willingness of a contributor to try, at best effort, to maintain the integration. Providing the intention for handling issues, providing bug fixes, or other contributions to the integration one is listed on as a code owner.\\n### Rules\\nIn order to support having (multiple) code owners for integration, to raise the quality and interaction on integration in our codebase, we have a set of rules (exceptions are in the next chapter).\\nFor the following cases, adding oneself as a code owner is required:\\n- When contributing a new integration.\\n- When contributing a new platform to an integration.\\n- When contributing a new feature to an integration.\\n- When contributing a significant refactor or rewrite of an integration.\\nContributions to our integrations, in the above-listed scopes, without having the contributor listed or added as the code owner, is no longer accepted.\\n### Exceptions\\nSome exceptions are in place, to prevent contributors to become demotivated to contribute; and are mainly based around smaller, low-impact contributions.\\nIn the following cases, code ownership may be omitted:\\n- Contributions that solely provides a bug fix(es).\\n- Contributions that only provide additional unit test(s).\\n- Contributions to integrations marked as \"internal\". These integrations are code owned by the Home Assistant core team.\\n- Contributions refactoring across multiple integrations, caused by changes to our core codebase. E.g., due to changes to the used platforms.\\n- Small or low impact contributions to an integration. A currently active code owner for the integration or a Home Assistant code reviewer can decide it may be omitted.\\n- The contributor pro-actively rejects to be listed as a code owner; however, a currently active code owner is willing to accept and take code ownership for the contribution provided by the contributor.\\nCode owner(s) and Home Assistant code reviewers are encouraged to ask a contributor to join an integration code ownership, even when the contribution matches one of the exceptions above.\\n### Withdrawing as a code owner\\nWithdrawing code ownership can happen, and it is sad to see an active code owner leaving the project.\\nA code owner may open up a PR to remove oneself as a code owner. However, this should only be accepted after the last contribution to the integration, made by the contributor, is released in a stable Home Assistant release.\\n","tokens":343,"id":1431,"text":"## Context\\nWe get contributed a lot of new integrations, new features to integrations and refactors of integrations. The Home Assistant project is honored to receive so many great contributions to our project!\\nUnfortunately, as a contributor, adding oneself as (the, or one of the) code owners of the integration contributed or contributed to, doesn't always happen spontaneously.\\nNot adding oneself as a code owner has drawbacks for the project:\\n- The contributor doesn't \"own\" (in terms of taking responsibility) his code, and thus contribution, in a more formal fashion.\\n- Without being listed as a code owner, our GitHub bot will not notify the contributor, when an issue for the integration is reported, quite possibly affecting his contribution.\\n- Integrations have ended up or may end up with having a single code owner or no code owners at all.\\nAs a result of this:\\n- Bugs are less likely to be resolved in a timely fashion (turn-around time).\\n- Integrations are more prone to break in the future.\\n- Integration with a single code owner:\\n- Do not benefit from multiple code owners being familiar with the integration in terms of code review and general turn-around time.\\n- Become largely unmaintained when the single listed code owner can no longer contribute to the project.\\nWe have quite a few integrations that haven't got multiple code owners or don't have a code owner.\\nDuring the design discussion of this ADR, it also became clear, that the term \"code owner\" has different meanings to our members and contributors. Some interpret it as an honorable mention of contribution; others see it as \"taking responsibility\".\\n\n\n##Decision\nCode ownership for an integration defined:\\nThe willingness of a contributor to try, at best effort, to maintain the integration. Providing the intention for handling issues, providing bug fixes, or other contributions to the integration one is listed on as a code owner.\\n### Rules\\nIn order to support having (multiple) code owners for integration, to raise the quality and interaction on integration in our codebase, we have a set of rules (exceptions are in the next chapter).\\nFor the following cases, adding oneself as a code owner is required:\\n- When contributing a new integration.\\n- When contributing a new platform to an integration.\\n- When contributing a new feature to an integration.\\n- When contributing a significant refactor or rewrite of an integration.\\nContributions to our integrations, in the above-listed scopes, without having the contributor listed or added as the code owner, is no longer accepted.\\n### Exceptions\\nSome exceptions are in place, to prevent contributors to become demotivated to contribute; and are mainly based around smaller, low-impact contributions.\\nIn the following cases, code ownership may be omitted:\\n- Contributions that solely provides a bug fix(es).\\n- Contributions that only provide additional unit test(s).\\n- Contributions to integrations marked as \"internal\". These integrations are code owned by the Home Assistant core team.\\n- Contributions refactoring across multiple integrations, caused by changes to our core codebase. E.g., due to changes to the used platforms.\\n- Small or low impact contributions to an integration. A currently active code owner for the integration or a Home Assistant code reviewer can decide it may be omitted.\\n- The contributor pro-actively rejects to be listed as a code owner; however, a currently active code owner is willing to accept and take code ownership for the contribution provided by the contributor.\\nCode owner(s) and Home Assistant code reviewers are encouraged to ask a contributor to join an integration code ownership, even when the contribution matches one of the exceptions above.\\n### Withdrawing as a code owner\\nWithdrawing code ownership can happen, and it is sad to see an active code owner leaving the project.\\nA code owner may open up a PR to remove oneself as a code owner. However, this should only be accepted after the last contribution to the integration, made by the contributor, is released in a stable Home Assistant release.\\n","Predictions":"We will use the following rules:n- The willingness of a contributor to try, at best effort, to maintain the integration. Providing the intention for handling issues, providing bug fixes, or other contributions to the integration one is listed on as a code owner.n### RulesnIn order to support having (multiple) code owners for integration, to raise the quality and interaction on integration in our codebase, we have a set of rules (exceptions are in the next chapter).nFor the following cases, adding (for example, adding a single code owner for a single integration) is not allowed.nFor example, if a single code owner is added to a single integration, he is not allowed to contribute to the other integrations.n"}
{"File Name":"amf-core\/0009-source-information-node-storing-source-locations.md","Context":"## Context\nThe custom AMF validator needs to show the location of the file from which each error was generated.\nGiven the current state of the amf model and emission of jsonld, there was no way to obtain the location of a specific node.\n","Decision":"A new node was defined as a field in BaseUnit call BaseUnitSourceInformation, which has the necessary information to obtain the source location of any node.\\nInternally, this node has two fields, one that stores the root location, and another that stores LocationInformation nodes which contain alternative locations with the ids of all the elements parsed from that location.\\nA new render option was included making the emission of this node to jsonld optional and not activated by default.\\nAn alternative solution was to serialize SourceLocation annotation in each node, but this leads to a 25% or more increase in size of the resulting jsonld, as the paths are stored in a redundant fashion.\\n","tokens":53,"id":405,"text":"## Context\\nThe custom AMF validator needs to show the location of the file from which each error was generated.\\nGiven the current state of the amf model and emission of jsonld, there was no way to obtain the location of a specific node.\\n\n\n##Decision\nA new node was defined as a field in BaseUnit call BaseUnitSourceInformation, which has the necessary information to obtain the source location of any node.\\nInternally, this node has two fields, one that stores the root location, and another that stores LocationInformation nodes which contain alternative locations with the ids of all the elements parsed from that location.\\nA new render option was included making the emission of this node to jsonld optional and not activated by default.\\nAn alternative solution was to serialize SourceLocation annotation in each node, but this leads to a 25% or more increase in size of the resulting jsonld, as the paths are stored in a redundant fashion.\\n","Predictions":"The \/var\/var\/stack structure will be a \/var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/stack structure will be a var\/var\/stack structure.nThe var\/var\/"}
{"File Name":"ditto\/DADR-0003-do-not-interrupt-threads.md","Context":"## Context\nSonarQube [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142) complains when we catch\n`InterruptedException` and not call `Thread.currentThread().interrupt()` in the catch block.\nSimply calling `Thread.currentThread().interrupt()` to silence SonarQube is dangerous. Due to the way JUnit reuses\nthreads, a call to `Thread.interrupt()` breaks the build in unpredictable ways, several tests after the actual site of\ninterruption.\n","Decision":"We will ignore [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142).\\n","tokens":114,"id":777,"text":"## Context\\nSonarQube [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142) complains when we catch\\n`InterruptedException` and not call `Thread.currentThread().interrupt()` in the catch block.\\nSimply calling `Thread.currentThread().interrupt()` to silence SonarQube is dangerous. Due to the way JUnit reuses\\nthreads, a call to `Thread.interrupt()` breaks the build in unpredictable ways, several tests after the actual site of\\ninterruption.\\n\n\n##Decision\nWe will ignore [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142).\\n","Predictions":"We will ignore [RSPEC-2142](https:\/\/rules.sonarsource.com\/java\/tag\/multi-threading\/RSPEC-2142)n"}
{"File Name":"xebikart-infra\/002-use-rabbitmq-with-mqtt-plugin-to-make-devices-communicate-with-each-other.md","Context":"## Context and Problem Statement\nWe want a message broker so the devices and other applications can communicate with the backend.\nWhat broker and protocol should we use?\n## Decision Drivers <!-- optional -->\n- Applicability regarding IoT projects : low-resources clients, etc\n- Possibility to use it to stream frames\/images coming from cars cameras\n- Ease of deployment on Kubernetes\n- Existing knowledge of the team\n","Decision":"- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\nChosen option: **[RabbitMQ](TODO) with [MQTT plugin](https:\/\/www.rabbitmq.com\/mqtt.html)**, because:\\n- It is already well-known among the team\\n- It has some [existing \"official\" Helm chart](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/rabbitmq)\\n- It seems like a good fit to iterate fast\\nWe **do not exclude switching to another MQTT broker such as VerneMQ in the future**, depending on our ability to dsitribute it cleanly on Kubernetes.\\nWe also **do not exclude using another broker such as Kafka or NATS for appropriate use cases**.\\n","tokens":84,"id":979,"text":"## Context and Problem Statement\\nWe want a message broker so the devices and other applications can communicate with the backend.\\nWhat broker and protocol should we use?\\n## Decision Drivers <!-- optional -->\\n- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\n\n\n##Decision\n- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames\/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\nChosen option: **[RabbitMQ](TODO) with [MQTT plugin](https:\/\/www.rabbitmq.com\/mqtt.html)**, because:\\n- It is already well-known among the team\\n- It has some [existing \"official\" Helm chart](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/rabbitmq)\\n- It seems like a good fit to iterate fast\\nWe **do not exclude switching to another MQTT broker such as VerneMQ in the future**, depending on our ability to dsitribute it cleanly on Kubernetes.\\nWe also **do not exclude using another broker such as Kafka or NATS for appropriate use cases**.\\n","Predictions":"* Existing knowledge of the teamnChosen option: **[RabbitMQ](TODO) with [MQTT plugin](https:\/\/www.rabbitmq.com\/mqtt.html)**, because:n- It is already well-known among the teamn- It has some [existing \"official\" Helm chart](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/rabbitmq)n- It seems like a good fit to iterate fastnWe **do not exclude switching to another MQTT broker such as VerneMQ in the future**, depending on our ability to dsitribute it cleanly on Kubernetes.nWe also **do not exclude using another broker such as Kafka or NATS for appropriate use cases**.n"}
{"File Name":"react-template\/0005-use-jest.md","Context":"## Context\nWe want a test framework that has good support for React and TypeScript.\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\napps.\n","Decision":"We will use Jest as our testing framework.\\n","tokens":40,"id":3323,"text":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n\n\n##Decision\nWe will use Jest as our testing framework.\\n","Predictions":"We will use Jest as our testing framework.n"}
{"File Name":"interlok\/0006-workflow-callback.md","Context":"## Context and Problem Statement\nWhen you enable [Dead Letter Queues](https:\/\/docs.aws.amazon.com\/AWSSimpleQueueService\/latest\/SQSDeveloperGuide\/sqs-dead-letter-queues.html) in SQS; the contract is that messages read from a source queue are failed over to the dead letter queue once the redrive policy is fired (i.e. after a max number of attempts to deliver the message). From testing, this is predicated on the fact that the message is not deleted. Of course, our sqs-polling-consumer deletes the message after submitting it to the workflow.\nSince we can't rely on the JMS semantics either (since onMessage() traditionally doesn't throw exceptions, and RuntimeExceptions aren't propagated by Interlok) we then have to think about some way of having an asynchronous callback.\n","Decision":"Chosen option: Modify AdaptrisMessageListener to have callbacks.\\n","tokens":171,"id":2343,"text":"## Context and Problem Statement\\nWhen you enable [Dead Letter Queues](https:\/\/docs.aws.amazon.com\/AWSSimpleQueueService\/latest\/SQSDeveloperGuide\/sqs-dead-letter-queues.html) in SQS; the contract is that messages read from a source queue are failed over to the dead letter queue once the redrive policy is fired (i.e. after a max number of attempts to deliver the message). From testing, this is predicated on the fact that the message is not deleted. Of course, our sqs-polling-consumer deletes the message after submitting it to the workflow.\\nSince we can't rely on the JMS semantics either (since onMessage() traditionally doesn't throw exceptions, and RuntimeExceptions aren't propagated by Interlok) we then have to think about some way of having an asynchronous callback.\\n\n\n##Decision\nChosen option: Modify AdaptrisMessageListener to have callbacks.\\n","Predictions":"We will use a stack method to handle the reload of a message.n"}
{"File Name":"my-notes\/prettier.md","Context":"### Context\nPrettier integrates with well with vscode but conflicts with Standard\n### Decision\nReplace Standard with Prettier\n","Decision":"Replace Standard with Prettier\\n","tokens":26,"id":5087,"text":"### Context\\nPrettier integrates with well with vscode but conflicts with Standard\\n### Decision\\nReplace Standard with Prettier\\n\n\n##Decision\nReplace Standard with Prettier\\n","Predictions":"We will use a stack class to store the stack and stack variables.n"}
{"File Name":"holochain-rust\/0010-write-our-own-dht-implementation.md","Context":"## Context\nFor the Go based prototype we used IPFS' Kademlia DHT implementations with edits.\nSince we are switching over to Rust, we can't easily continue using that code base.\nMore importantly, there are too many Holochain specific additions to a vanilla Kademlia DHT, as well as other possible implementations of achieving entry resilience, routing, our forthcoming World Model, CRDT handling, gossip and our graph properties via linking, so it does not make sense to change existing implementations but instead roll our own.\n","Decision":"We will build our own DHT implementation in Rust from scratch.\\n","tokens":110,"id":1494,"text":"## Context\\nFor the Go based prototype we used IPFS' Kademlia DHT implementations with edits.\\nSince we are switching over to Rust, we can't easily continue using that code base.\\nMore importantly, there are too many Holochain specific additions to a vanilla Kademlia DHT, as well as other possible implementations of achieving entry resilience, routing, our forthcoming World Model, CRDT handling, gossip and our graph properties via linking, so it does not make sense to change existing implementations but instead roll our own.\\n\n\n##Decision\nWe will build our own DHT implementation in Rust from scratch.\\n","Predictions":"We will use [Alfa](https:\/\/github.com\/Microsoft\/TypeScript) for implementing all of Alfa. Being a superset of JavaScript, TypeScript has a low learning curve for people already familiar with JavaScript while providing a solid type system. We will enforce API contracts through generation of [declaration files](https:\/\/www.typescriptlang.org\/docs\/handbook\/declaration-files\/introduction.html) which will dictate the API surface that consumers can access. To the extent possible, we will use TypeScript to implement all of Alfa.n"}
{"File Name":"cosmos-sdk\/adr-021-protobuf-query-encoding.md","Context":"## Context\nThis ADR is a continuation of the motivation, design, and context established in\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\n[ADR 020](.\/adr-020-protobuf-transaction-encoding.md), namely, we aim to design the\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\nThis ADR continues from [ADD 020](.\/adr-020-protobuf-transaction-encoding.md)\nto specify the encoding of queries.\n","Decision":"### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```protobuf\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/cosmos\/gogoproto)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the Cosmos SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryService(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryService(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work directly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe Cosmos SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context`will receive a new method`QueryConn`that returns a`ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n","tokens":108,"id":810,"text":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md) and\\n[ADR 020](.\/adr-020-protobuf-transaction-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nThis ADR continues from [ADD 020](.\/adr-020-protobuf-transaction-encoding.md)\\nto specify the encoding of queries.\\n\n\n##Decision\n### Custom Query Definition\\nModules define custom queries through a protocol buffers `service` definition.\\nThese `service` definitions are generally associated with and used by the\\nGRPC protocol. However, the protocol buffers specification indicates that\\nthey can be used more generically by any request\/response protocol that uses\\nprotocol buffer encoding. Thus, we can use `service` definitions for specifying\\ncustom ABCI queries and even reuse a substantial amount of the GRPC infrastructure.\\nEach module with custom queries should define a service canonically named `Query`:\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) { }\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) { }\\n}\\n```\\n#### Handling of Interface Types\\nModules that use interface types and need true polymorphism generally force a\\n`oneof` up to the app-level that provides the set of concrete implementations of\\nthat interface that the app supports. While app's are welcome to do the same for\\nqueries and implement an app-level query service, it is recommended that modules\\nprovide query methods that expose these interfaces via `google.protobuf.Any`.\\nThere is a concern on the transaction level that the overhead of `Any` is too\\nhigh to justify its usage. However for queries this is not a concern, and\\nproviding generic module-level queries that use `Any` does not preclude apps\\nfrom also providing app-level queries that return use the app-level `oneof`s.\\nA hypothetical example for the `gov` module would look something like:\\n```protobuf\\n\/\/ x\/gov\/types\/types.proto\\nimport \"google\/protobuf\/any.proto\";\\nservice Query {\\nrpc GetProposal(GetProposalParams) returns (AnyProposal) { }\\n}\\nmessage AnyProposal {\\nProposalBase base = 1;\\ngoogle.protobuf.Any content = 2;\\n}\\n```\\n### Custom Query Implementation\\nIn order to implement the query service, we can reuse the existing [gogo protobuf](https:\/\/github.com\/cosmos\/gogoproto)\\ngrpc plugin, which for a service named `Query` generates an interface named\\n`QueryServer` as below:\\n```go\\ntype QueryServer interface {\\nQueryBalance(context.Context, *QueryBalanceParams) (*types.Coin, error)\\nQueryAllBalances(context.Context, *QueryAllBalancesParams) (*QueryAllBalancesResponse, error)\\n}\\n```\\nThe custom queries for our module are implemented by implementing this interface.\\nThe first parameter in this generated interface is a generic `context.Context`,\\nwhereas querier methods generally need an instance of `sdk.Context` to read\\nfrom the store. Since arbitrary values can be attached to `context.Context`\\nusing the `WithValue` and `Value` methods, the Cosmos SDK should provide a function\\n`sdk.UnwrapSDKContext` to retrieve the `sdk.Context` from the provided\\n`context.Context`.\\nAn example implementation of `QueryBalance` for the bank module as above would\\nlook something like:\\n```go\\ntype Querier struct {\\nKeeper\\n}\\nfunc (q Querier) QueryBalance(ctx context.Context, params *types.QueryBalanceParams) (*sdk.Coin, error) {\\nbalance := q.GetBalance(sdk.UnwrapSDKContext(ctx), params.Address, params.Denom)\\nreturn &balance, nil\\n}\\n```\\n### Custom Query Registration and Routing\\nQuery server implementations as above would be registered with `AppModule`s using\\na new method `RegisterQueryService(grpc.Server)` which could be implemented simply\\nas below:\\n```go\\n\/\/ x\/bank\/module.go\\nfunc (am AppModule) RegisterQueryService(server grpc.Server) {\\ntypes.RegisterQueryServer(server, keeper.Querier{am.keeper})\\n}\\n```\\nUnderneath the hood, a new method `RegisterService(sd *grpc.ServiceDesc, handler interface{})`\\nwill be added to the existing `baseapp.QueryRouter` to add the queries to the custom\\nquery routing table (with the routing method being described below).\\nThe signature for this method matches the existing\\n`RegisterServer` method on the GRPC `Server` type where `handler` is the custom\\nquery server implementation described above.\\nGRPC-like requests are routed by the service name (ex. `cosmos_sdk.x.bank.v1.Query`)\\nand method name (ex. `QueryBalance`) combined with `\/`s to form a full\\nmethod name (ex. `\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`). This gets translated\\ninto an ABCI query as `custom\/cosmos_sdk.x.bank.v1.Query\/QueryBalance`. Service handlers\\nregistered with `QueryRouter.RegisterService` will be routed this way.\\nBeyond the method name, GRPC requests carry a protobuf encoded payload, which maps naturally\\nto `RequestQuery.Data`, and receive a protobuf encoded response or error. Thus\\nthere is a quite natural mapping of GRPC-like rpc methods to the existing\\n`sdk.Query` and `QueryRouter` infrastructure.\\nThis basic specification allows us to reuse protocol buffer `service` definitions\\nfor ABCI custom queries substantially reducing the need for manual decoding and\\nencoding in query methods.\\n### GRPC Protocol Support\\nIn addition to providing an ABCI query pathway, we can easily provide a GRPC\\nproxy server that routes requests in the GRPC protocol to ABCI query requests\\nunder the hood. In this way, clients could use their host languages' existing\\nGRPC implementations to make direct queries against Cosmos SDK app's using\\nthese `service` definitions. In order for this server to work, the `QueryRouter`\\non `BaseApp` will need to expose the service handlers registered with\\n`QueryRouter.RegisterService` to the proxy server implementation. Nodes could\\nlaunch the proxy server on a separate port in the same process as the ABCI app\\nwith a command-line flag.\\n### REST Queries and Swagger Generation\\n[grpc-gateway](https:\/\/github.com\/grpc-ecosystem\/grpc-gateway) is a project that\\ntranslates REST calls into GRPC calls using special annotations on service\\nmethods. Modules that want to expose REST queries should add `google.api.http`\\nannotations to their `rpc` methods as in this example below.\\n```protobuf\\n\/\/ x\/bank\/types\/types.proto\\nservice Query {\\nrpc QueryBalance(QueryBalanceParams) returns (cosmos_sdk.v1.Coin) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balance\/{address}\/{denom}\"\\n};\\n}\\nrpc QueryAllBalances(QueryAllBalancesParams) returns (QueryAllBalancesResponse) {\\noption (google.api.http) = {\\nget: \"\/x\/bank\/v1\/balances\/{address}\"\\n};\\n}\\n}\\n```\\ngrpc-gateway will work directly against the GRPC proxy described above which will\\ntranslate requests to ABCI queries under the hood. grpc-gateway can also\\ngenerate Swagger definitions automatically.\\nIn the current implementation of REST queries, each module needs to implement\\nREST queries manually in addition to ABCI querier methods. Using the grpc-gateway\\napproach, there will be no need to generate separate REST query handlers, just\\nquery servers as described above as grpc-gateway handles the translation of protobuf\\nto REST as well as Swagger definitions.\\nThe Cosmos SDK should provide CLI commands for apps to start GRPC gateway either in\\na separate process or the same process as the ABCI app, as well as provide a\\ncommand for generating grpc-gateway proxy `.proto` files and the `swagger.json`\\nfile.\\n### Client Usage\\nThe gogo protobuf grpc plugin generates client interfaces in addition to server\\ninterfaces. For the `Query` service defined above we would get a `QueryClient`\\ninterface like:\\n```go\\ntype QueryClient interface {\\nQueryBalance(ctx context.Context, in *QueryBalanceParams, opts ...grpc.CallOption) (*types.Coin, error)\\nQueryAllBalances(ctx context.Context, in *QueryAllBalancesParams, opts ...grpc.CallOption) (*QueryAllBalancesResponse, error)\\n}\\n```\\nVia a small patch to gogo protobuf ([gogo\/protobuf#675](https:\/\/github.com\/gogo\/protobuf\/pull\/675))\\nwe have tweaked the grpc codegen to use an interface rather than concrete type\\nfor the generated client struct. This allows us to also reuse the GRPC infrastructure\\nfor ABCI client queries.\\n1Context`will receive a new method`QueryConn`that returns a`ClientConn`\\nthat routes calls to ABCI queries\\nClients (such as CLI methods) will then be able to call query methods like this:\\n```go\\nclientCtx := client.NewContext()\\nqueryClient := types.NewQueryClient(clientCtx.QueryConn())\\nparams := &types.QueryBalanceParams{addr, denom}\\nresult, err := queryClient.QueryBalance(gocontext.Background(), params)\\n```\\n### Testing\\nTests would be able to create a query client directly from keeper and `sdk.Context`\\nreferences using a `QueryServerTestHelper` as below:\\n```go\\nqueryHelper := baseapp.NewQueryServerTestHelper(ctx)\\ntypes.RegisterQueryServer(queryHelper, keeper.Querier{app.BankKeeper})\\nqueryClient := types.NewQueryClient(queryHelper)\\n```\\n","Predictions":"We will use the [Protocol Buffer migration path](https:\/\/docs.google.com\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/docs\/doc"}
{"File Name":"libelektra\/error_handling.md","Context":"## Problem\nThere are ambiguous cases where the same return value can refer to multiple problems:\n- name modifications which can be either invalid name or locking the key name\n- getting values of (non-)binary keys\n","Decision":"- Update documentation in `doc\/dev\/error-*` and link to them in the documentation\\nfor the module `kdb`\\n- Add second channel for getting information about errors\\n- Return error codes directly from functions where failures are expected, e.g. `kdbGet`, `keySetName`\\n- Harmonize return values from all functions and move error reporting to second channel\\n- Binary metadata vs flag #4194\\n","tokens":44,"id":1292,"text":"## Problem\\nThere are ambiguous cases where the same return value can refer to multiple problems:\\n- name modifications which can be either invalid name or locking the key name\\n- getting values of (non-)binary keys\\n\n\n##Decision\n- Update documentation in `doc\/dev\/error-*` and link to them in the documentation\\nfor the module `kdb`\\n- Add second channel for getting information about errors\\n- Return error codes directly from functions where failures are expected, e.g. `kdbGet`, `keySetName`\\n- Harmonize return values from all functions and move error reporting to second channel\\n- Binary metadata vs flag #4194\\n","Predictions":"- Update documentation in doc\/dev\/error-* and link to them in the documentationnfor the module kdbn- Add second channel for getting information about errorsn- Return error codes directly from functions where failures are expected, e.g. kdbGet, keySetNamen- Harmonize return values from all functions and move error reporting to second channeln- Binary metadata vs flag #4194n"}
{"File Name":"ionic-dummy-repo\/ADR-001.md","Context":"## Context\nGiven the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard?\nDevelopers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?\n### Who Was Involved in This Decision\n- Alex Ward\n- Chris Weight\n### Relates To\n- N\/A\n","Decision":"- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\nThe Hybrid seed template will _not_ express an opinion via pre-determined dependencies on what state management frameworks (if any) should be used. This can be decided on a per-project basis. Though there are positives and negatives either way, it is felt that the ability to rapidly implement changes to approach over the course of time and projects is a powerful plus.\\n","tokens":93,"id":701,"text":"## Context\\nGiven the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard?\\nDevelopers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?\\n### Who Was Involved in This Decision\\n- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\n\n\n##Decision\n- Alex Ward\\n- Chris Weight\\n### Relates To\\n- N\/A\\nThe Hybrid seed template will _not_ express an opinion via pre-determined dependencies on what state management frameworks (if any) should be used. This can be decided on a per-project basis. Though there are positives and negatives either way, it is felt that the ability to rapidly implement changes to approach over the course of time and projects is a powerful plus.\\n","Predictions":"We will use the Separate Data Access Layer as the data access layer.n"}
{"File Name":"CCS-Architecture-Decision-Records\/0004-language-choices-for-new-development.md","Context":"## Context\nWhen developing a new system, there are many possible language choices available.\n-CCS uses a mixture of different languages for existing systems including:\n- PHP for the Supplier Registration Service and Contracts Finder\n- C# for MISO\n- Java for the CCS ID and DigiTS services\n- Python for the Digital Marketplace\nOther services in government use a mixture of PHP, Java, Python, Ruby, Scala and GO.\nPicking one standard language for all CCS services could restrict the pool of potential suppliers we could interact with when building new services. Conversely, allowing an unlimited selection of languages can make future management and support of services more complex and expensive.\nAgreeing a selection of acceptable languages will allow the greatest flexibility in supplier choice, but also helps minimise future ongoing support costs.\nAny decision we make about languages and frameworks should consider:\n- Skills available in the current team(s) - what are the current team(s) comfortable using? What do they have experience in?\n- Skills of a future team - is there a large enough pool of suppliers and contractors who could support and maintain the services in the future?\n- External toolkits and modules - are there useful toolkits and modules which could reduce development effort (eg the GOV.UK Frontend Toolkit)\n- Cost - what is the cost of developing in this way? Are there licence costs?\n- Hosting - would picking a language restrict hosting options?\nWe should also consider the Service Manual guide on [choosing technology](https:\/\/www.gov.uk\/service-manual\/technology\/choosing-technology-an-introduction).\n","Decision":"New services developed by CCS should use the following languages:\\n- Java\\n- Python\\n- Ruby\\n- Javascript\\nDigital services should also use HTML5, CSS3 and JSON as required.\\nSalesforce work may continue to use the Salesforce Apex language, but where possible, we should use external web service components to reduce the volume of Apex in use.\\nNew services which do not use these languages will require an Architectural Decision to explain why.\\nAs the software development industry changes quickly, we will re-evaluate this decision in 6 months, taking account of current market trends and industry moves.\\nThis decision does not apply to services comprised entirely of Software as a Service (SaaS) solutions, and does not affect systems which already exist. It also does not cover the development of native mobile applications.\\n","tokens":329,"id":1000,"text":"## Context\\nWhen developing a new system, there are many possible language choices available.\\n-CCS uses a mixture of different languages for existing systems including:\\n- PHP for the Supplier Registration Service and Contracts Finder\\n- C# for MISO\\n- Java for the CCS ID and DigiTS services\\n- Python for the Digital Marketplace\\nOther services in government use a mixture of PHP, Java, Python, Ruby, Scala and GO.\\nPicking one standard language for all CCS services could restrict the pool of potential suppliers we could interact with when building new services. Conversely, allowing an unlimited selection of languages can make future management and support of services more complex and expensive.\\nAgreeing a selection of acceptable languages will allow the greatest flexibility in supplier choice, but also helps minimise future ongoing support costs.\\nAny decision we make about languages and frameworks should consider:\\n- Skills available in the current team(s) - what are the current team(s) comfortable using? What do they have experience in?\\n- Skills of a future team - is there a large enough pool of suppliers and contractors who could support and maintain the services in the future?\\n- External toolkits and modules - are there useful toolkits and modules which could reduce development effort (eg the GOV.UK Frontend Toolkit)\\n- Cost - what is the cost of developing in this way? Are there licence costs?\\n- Hosting - would picking a language restrict hosting options?\\nWe should also consider the Service Manual guide on [choosing technology](https:\/\/www.gov.uk\/service-manual\/technology\/choosing-technology-an-introduction).\\n\n\n##Decision\nNew services developed by CCS should use the following languages:\\n- Java\\n- Python\\n- Ruby\\n- Javascript\\nDigital services should also use HTML5, CSS3 and JSON as required.\\nSalesforce work may continue to use the Salesforce Apex language, but where possible, we should use external web service components to reduce the volume of Apex in use.\\nNew services which do not use these languages will require an Architectural Decision to explain why.\\nAs the software development industry changes quickly, we will re-evaluate this decision in 6 months, taking account of current market trends and industry moves.\\nThis decision does not apply to services comprised entirely of Software as a Service (SaaS) solutions, and does not affect systems which already exist. It also does not cover the development of native mobile applications.\\n","Predictions":"We will use the following languages:n- Pythonn- Rubyn- Javascriptn- HTML5n- CSS3n- JSONn- HTML5n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n- HTML5n- CSS3n"}
{"File Name":"adr\/ADR-7-standards-repository.md","Context":"# Context\n* Is the [decentraland\/standards](https:\/\/github.com\/decentraland\/standards) repository still useful?\n* Does it make sense to have ADR and standards?\n* How does it play with [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals)?\n# Options\n* Archive both repositories, leaving only [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\n* \u2705 Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards), leaving [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\n* Only use [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\n# Decision\n* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\nBecause:\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\nDate: 2020-10-16\n","Decision":"* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n","tokens":338,"id":4610,"text":"# Context\\n* Is the [decentraland\/standards](https:\/\/github.com\/decentraland\/standards) repository still useful?\\n* Does it make sense to have ADR and standards?\\n* How does it play with [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals)?\\n# Options\\n* Archive both repositories, leaving only [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n* \u2705 Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards), leaving [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\n* Only use [decentraland\/adr](https:\/\/github.com\/decentraland\/adr)\\n# Decision\\n* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n\n\n##Decision\n* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)\\n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.\\nBecause:\\n* `proposals` is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.\\n* `standards` is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.\\nDate: 2020-10-16\\n","Predictions":"* Archive [decentraland\/standards](https:\/\/github.com\/decentraland\/standards)n* Keep: [decentraland\/adr](https:\/\/github.com\/decentraland\/adr) and [decentraland\/proposals](https:\/\/github.com\/decentraland\/proposals) untouched.nBecause:n* proposals is a public repository where the community can contribute with new ideas and improvements. It has a formal review process that can ve leveraged by anyone.n* standards is not actively maintained, it has a strict approval process. Most of the standards can be also defined as ADR, eliminating the need of maintain both repositories.nDate: 2020-10-16n"}
{"File Name":"adr-generation-tool\/rate_limit_adr_0001.md","Context":"## Context\nContext for rate limiting pattern\n","Decision":"Prosa about the decision in the ADD\\n","tokens":9,"id":5142,"text":"## Context\\nContext for rate limiting pattern\\n\n\n##Decision\nProsa about the decision in the ADD\\n","Predictions":"Prosa about the decision in the ADDn"}
{"File Name":"pottery\/0004-store-shards-in-time-stamped-directories-and-files.md","Context":"## Context\nA project history can cover many years, with many small shards being recorded.  If all the shard files are stored in the same directory, the directory will be difficult to work with when many shards have been posted.  Performance might suffer on older file systems.\n","Decision":"Store shard files in subdirectories named after year and then year-and-month, and name the files after the date and time.\\n","tokens":56,"id":2508,"text":"## Context\\nA project history can cover many years, with many small shards being recorded.  If all the shard files are stored in the same directory, the directory will be difficult to work with when many shards have been posted.  Performance might suffer on older file systems.\\n\n\n##Decision\nStore shard files in subdirectories named after year and then year-and-month, and name the files after the date and time.\\n","Predictions":"The build pipeline will be split into two separate pipelines:n1. The build pipeline will create a new build.json file, and a new app.yaml file will be created.n2. The deploy pipeline will create a new build.json file, and a new app.yaml file will be created.n3. The deploy pipeline will create a new build.json file, and a new app.yaml file will be created.n"}
{"File Name":"search-ui-extensions\/ShowViewedByCustomer.md","Context":"## Context and Problem Statement\n<!-- Quick 2-3 sentence background of the User Story -->\nThe `ViewedByCustomer` component needs to be added on each result template, and therefore could be accidentally missed on one or more, especially if a new template is added at a later time. This would create an inconsistent view of what content the customer has viewed. - From JIRA\n---\n## Decision Drivers <!-- optional -->\n### Context\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\n<!-- Number these so that they are easier to reference in the following section -->\n### Decisions\n1. Need to choose when to edit the results (i.e. need an event)\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\n1. There should be an option whether or not to perform that aforementioned actions with the component\n---\n","Decision":"### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n-   [Option 1] - Leverage the `newResultsDisplayed` event, and loop over every result, performing further action.\\n-   [Option 2] - Leverage the `newResultDisplayed` event, and perform further action.\\n**Decision 2** - Properly adding the ViewedByDocument Component\\n-   [Option 1] - Add the component using `<div class=\"CoveoViewedByCustomer\">`.\\n-   [Option 2] - Add the component using the `ViewedByCustomer` constructor.\\n**Decision 3** - Ensure we don't add the template a second time\\n-   [Option 1] - Query the results `HTMLElement` using the `getElementsByClassName` method.\\n-   [Option 2] - Query the results `HTMLElement` using the `querySelectorAll` method.\\n**Decision 4** - There should be an option whether or not to add the component\\n-   [Option 1] - Have the option be false by default.\\n-   [Option 2] - Have the option be true by default.\\n---\\n#### Decision 1: [Option 2]\\nThere are two reason behind this decision selection: First the `newResultsDisplayed` option wasn't passing back the `args.item`, which would have made editing the dom element harder. Second, using the event trigger instead of a for loop made the methods functionality more simple.\\n#### Decision 2: [Option 2]\\nThe `newResultDisplayed` dom element was firing after the completion of the search-ui, therefore using the `<div>` wasn't possible.\\n#### Decision 3: [Option 1]\\nChoosing to use `getElementsByClassName`, in this context I don't think there is a difference between using `querySelectorAll` and `getElementsByClassName`\\n#### Decision 4: [Option 2]\\nFrom what I understand it makes sense for this feature to be true by default, as it's important for their not to be inconsistencies\\n---\\n","tokens":235,"id":5149,"text":"## Context and Problem Statement\\n<!-- Quick 2-3 sentence background of the User Story -->\\nThe `ViewedByCustomer` component needs to be added on each result template, and therefore could be accidentally missed on one or more, especially if a new template is added at a later time. This would create an inconsistent view of what content the customer has viewed. - From JIRA\\n---\\n## Decision Drivers <!-- optional -->\\n### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n\n\n##Decision\n### Context\\nThe main decision drivers were to be able to add the ViewedByCustomer component to each result, without adding it a second time, when the option in the UserActions component is true.\\n<!-- Number these so that they are easier to reference in the following section -->\\n### Decisions\\n1. Need to choose when to edit the results (i.e. need an event)\\n1. Ensure the `ViewedByCustomer` component is properly added to each result template\\n1. Ensure that if a template already has the `ViewedByCustomer` component that it won't add a second component\\n1. There should be an option whether or not to perform that aforementioned actions with the component\\n---\\n-   [Option 1] - Leverage the `newResultsDisplayed` event, and loop over every result, performing further action.\\n-   [Option 2] - Leverage the `newResultDisplayed` event, and perform further action.\\n**Decision 2** - Properly adding the ViewedByDocument Component\\n-   [Option 1] - Add the component using `<div class=\"CoveoViewedByCustomer\">`.\\n-   [Option 2] - Add the component using the `ViewedByCustomer` constructor.\\n**Decision 3** - Ensure we don't add the template a second time\\n-   [Option 1] - Query the results `HTMLElement` using the `getElementsByClassName` method.\\n-   [Option 2] - Query the results `HTMLElement` using the `querySelectorAll` method.\\n**Decision 4** - There should be an option whether or not to add the component\\n-   [Option 1] - Have the option be false by default.\\n-   [Option 2] - Have the option be true by default.\\n---\\n#### Decision 1: [Option 2]\\nThere are two reason behind this decision selection: First the `newResultsDisplayed` option wasn't passing back the `args.item`, which would have made editing the dom element harder. Second, using the event trigger instead of a for loop made the methods functionality more simple.\\n#### Decision 2: [Option 2]\\nThe `newResultDisplayed` dom element was firing after the completion of the search-ui, therefore using the `<div>` wasn't possible.\\n#### Decision 3: [Option 1]\\nChoosing to use `getElementsByClassName`, in this context I don't think there is a difference between using `querySelectorAll` and `getElementsByClassName`\\n#### Decision 4: [Option 2]\\nFrom what I understand it makes sense for this feature to be true by default, as it's important for their not to be inconsistencies\\n---\\n","Predictions":"1. Need to choose when to edit the results (i.e. need an event)n1. Ensure the ViewedByCustomer component is properly added to each result templaten1. Ensure that if a template already has the ViewedByCustomer component that it won't add a second componentn1. There should be an option whether or not to perform that aforementioned actions with the componentn---n"}
{"File Name":"ditto\/DADR-0006-merge-payload.md","Context":"## Context\nWe want to allow partial or merge updates of things with a single request.\n","Decision":"A merge request\\n* uses HTTP `PATCH` method.\\n* has payload in _JSON merge patch_ format defined in [RFC-7396](https:\/\/tools.ietf.org\/html\/rfc7396).\\n* has the request header `content-type` set to `application\/merge-patch+json`.\\n","tokens":19,"id":779,"text":"## Context\\nWe want to allow partial or merge updates of things with a single request.\\n\n\n##Decision\nA merge request\\n* uses HTTP `PATCH` method.\\n* has payload in _JSON merge patch_ format defined in [RFC-7396](https:\/\/tools.ietf.org\/html\/rfc7396).\\n* has the request header `content-type` set to `application\/merge-patch+json`.\\n","Predictions":"* We will use a LogEnd method to write log entries.n* We will use a LogEnd method to write log entries to a file.n"}
{"File Name":"ftd-scratch3-offline\/0013-communicate-with-the-scratch-app-via-a-local-web-server.md","Context":"## Context\nThe scratch app has to invoke the arduino-cli program to compile the converted programs.\nThe scratch app has to invoke the scratch-to-c program to convert the scratch blocks to a runnable arduino program.\nIn both cases the scratch app would have to invoke a native program.\nThis is not possible in current web browsers.\nThis would be possible using e.g. Electron, but the authors don't know Electron and Electron would add around 100 MB size overhead and also mean that without Electron the app can't be used.\nUsing a local web-server all that is needed is any browser, the scratch ftduino app and the local web-server.\n","Decision":"We will use a local web-server that will be used by the scratch app to invoke certain native programs.\\nThe local web-server may also be used to host the scratch app itself for example when there is no Internet connectivity available.\\n","tokens":135,"id":2634,"text":"## Context\\nThe scratch app has to invoke the arduino-cli program to compile the converted programs.\\nThe scratch app has to invoke the scratch-to-c program to convert the scratch blocks to a runnable arduino program.\\nIn both cases the scratch app would have to invoke a native program.\\nThis is not possible in current web browsers.\\nThis would be possible using e.g. Electron, but the authors don't know Electron and Electron would add around 100 MB size overhead and also mean that without Electron the app can't be used.\\nUsing a local web-server all that is needed is any browser, the scratch ftduino app and the local web-server.\\n\n\n##Decision\nWe will use a local web-server that will be used by the scratch app to invoke certain native programs.\\nThe local web-server may also be used to host the scratch app itself for example when there is no Internet connectivity available.\\n","Predictions":"We have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-project> and paas-project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.n"}
{"File Name":"unit-e-project\/ADR-0002.md","Context":"## Context\nWe have to maintain a consistent naming in the codebase for roles in the Esperanza protocol.\nAt the moment we have plenty of examples where there is confusion especially when mixing terms like `staking`, `miner`, `proposer`.\n","Decision":"We outline two clear actors in the protocol:\\n- The `PROPOSER`: which solves the function of proposing new blocks through his stake. At the moment words like `staking`, `miner`, `mining` are associated with the same function.\\n- The `VALIDATOR`: which takes active part in the finalisation process with the voting.\\nWe therefore MUST replace:\\n- all the instances of `miner` with `proposer`.\\n- all the instances of `mining` with `proposing`.\\n- all the instances of `staking` with `proposing`.\\nBut we MUST NOT replace the instances of the word `stake` since this indicates the actual deposit made in order to propose.\\nThe only exception to this is `stakethread.h` that MUST be renamed to `proposerthread.h`.\\n","tokens":52,"id":3768,"text":"## Context\\nWe have to maintain a consistent naming in the codebase for roles in the Esperanza protocol.\\nAt the moment we have plenty of examples where there is confusion especially when mixing terms like `staking`, `miner`, `proposer`.\\n\n\n##Decision\nWe outline two clear actors in the protocol:\\n- The `PROPOSER`: which solves the function of proposing new blocks through his stake. At the moment words like `staking`, `miner`, `mining` are associated with the same function.\\n- The `VALIDATOR`: which takes active part in the finalisation process with the voting.\\nWe therefore MUST replace:\\n- all the instances of `miner` with `proposer`.\\n- all the instances of `mining` with `proposing`.\\n- all the instances of `staking` with `proposing`.\\nBut we MUST NOT replace the instances of the word `stake` since this indicates the actual deposit made in order to propose.\\nThe only exception to this is `stakethread.h` that MUST be renamed to `proposerthread.h`.\\n","Predictions":"We outline two clear actors in the protocol:n- The PROPOSER: which solves the function of proposing new blocks through his stake. At the moment words like miner, miner, mining are associated with the same function.n- The VALIDATOR: which takes active part in the finalisation process with the voting.nWe therefore MUST replace:n- all the instances of miner with proposer.n- all the instances of mining with proposing.nBut we MUST NOT replace the instances of the word staking since this indicates the actual deposit made in order to propose.nThe only exception to this is stakethread.h that MUST be renamed to proposerthread.h.n"}
{"File Name":"terraform\/AMIs.md","Context":"## Context\nWe have many operating systems in play at DXC. In moving to AWS the number of potential variants does not decrease, as we could choose from AWS images, marketplace images, or we could build\/maintain our own images.\n","Decision":"We will use AWS images only, as this frees us from the operational overheads of patching and testing. AWS have been the fastest company in the cloud space to patch their operating systems in response to threats, so we can benefit from their good practices. This decision also allows us to take the easiest routes for automation and gives us access to the widest range of public cloud software.\\n","tokens":50,"id":923,"text":"## Context\\nWe have many operating systems in play at DXC. In moving to AWS the number of potential variants does not decrease, as we could choose from AWS images, marketplace images, or we could build\/maintain our own images.\\n\n\n##Decision\nWe will use AWS images only, as this frees us from the operational overheads of patching and testing. AWS have been the fastest company in the cloud space to patch their operating systems in response to threats, so we can benefit from their good practices. This decision also allows us to take the easiest routes for automation and gives us access to the widest range of public cloud software.\\n","Predictions":"We will use AWS images only, as this frees us from the operational overheads of patching and testing. AWS have been the fastest company in the cloud space to patch their operating systems in response to threats, so we can benefit from their good practices. This decision also allows us to take the easiest routes for automation and gives us access to the widest range of public cloud software.n"}
{"File Name":"mat-process-utils\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3763,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"lobiani\/0004-accept-coupling-with-spring-boot-in-backend.md","Context":"## Context\nAlmost every software project needs to interact with the 3rd party libraries and frameworks to some extent.\nGenerally, the less coupled the code is with the 3rd party libraries and frameworks (usually thanks to additional abstraction\nlayers on top of them), more flexible the software becomes in regard to technology replacements in the future.\nHowever, sometimes it's quite viable to compromise this flexibility in favor of practical benefits that particular\ntechnology brings, in this case Spring Boot\/Framework. Especially that it is designed with the abstraction and\nextensibility in mind, so introducing another layer of abstraction is senseless.\n","Decision":"We accept direct coupling with Spring Boot\/Framework code without any abstraction layers on top of it.\\n","tokens":127,"id":1035,"text":"## Context\\nAlmost every software project needs to interact with the 3rd party libraries and frameworks to some extent.\\nGenerally, the less coupled the code is with the 3rd party libraries and frameworks (usually thanks to additional abstraction\\nlayers on top of them), more flexible the software becomes in regard to technology replacements in the future.\\nHowever, sometimes it's quite viable to compromise this flexibility in favor of practical benefits that particular\\ntechnology brings, in this case Spring Boot\/Framework. Especially that it is designed with the abstraction and\\nextensibility in mind, so introducing another layer of abstraction is senseless.\\n\n\n##Decision\nWe accept direct coupling with Spring Boot\/Framework code without any abstraction layers on top of it.\\n","Predictions":"We accept direct coupling with Spring Boot\/Framework code without any abstraction layers on top of it.n"}
{"File Name":"octagon\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4597,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"alfa\/adr-002.md","Context":"## Context\nLike so many other accessibility tools, such as the [Accessibility Developer Tools by Google](https:\/\/github.com\/GoogleChrome\/accessibility-developer-tools), [aXe by Deque](https:\/\/github.com\/dequelabs\/axe-core), and [HTML_CodeSniffer by Squiz](https:\/\/github.com\/squizlabs\/HTML_CodeSniffer) to name a few, our proprietary accessibility conformance testing engine at Siteimprove runs within the context of a browser. The reason why this seems to be the de facto way of implementing an accessibility tool is obvious: The browser is the tool used to consume your website, so why not test directly within that very tool? Through the APIs exposed by the browser, we get access to all the information needed in order to assess the accessibility of a website; the structure we can access and inspect through the DOM, information about styling can be gained through the CSSOM, and soon we also get our hands on a standardised accessibility tree through the [AOM](https:\/\/wicg.github.io\/aom\/).\nHowever, not all is good in the land of browsers. Rendering a website is an inherently non-deterministic process and the timing of network requests, script execution, the content of request headers, and much more, all play a role in what the final result will look like. In most cases, this will directly affect the assessment of a tool that runs within the browser and will become very apparent at scale. At Siteimprove, we feel the effect of this on a daily basis; a customer asking us why we came up with a certain result and us having little to no clue because we cannot replicate the exact circumstances that led to that result. This is a frustrating experience for both our customers and ourselves as it makes it difficult to reason about our tool.\nWe want to fix this and we want to fix it for good. To do so, we must ensure that we have the ability to exactly replicate the results of a given accessibility assessment. Ideally, as many unknown browser variables as possible should be taken out of the equation and the browser only be used for what is absolutely necessary.\n","Decision":"We will abandon any sort of dynamic analysis within the context of a browser. The input to Alfa will be static data and any assessment must be made based on that data alone. A browser may or may not be involved in the construction of the data, but the browser will not be required for any further assessment thereof.\\nIf additional data is needed by a given accessibility rule, we will adjust the data format to meet the needs of the rule. We will also carefully consider the extent of the data format as to not bloat it with information that could otherwise be inferred from existing data. Ideally, the size of the data when serialised and stored on disk will not be much larger than the size of the original source code on which the data is based.\\n","tokens":437,"id":3156,"text":"## Context\\nLike so many other accessibility tools, such as the [Accessibility Developer Tools by Google](https:\/\/github.com\/GoogleChrome\/accessibility-developer-tools), [aXe by Deque](https:\/\/github.com\/dequelabs\/axe-core), and [HTML_CodeSniffer by Squiz](https:\/\/github.com\/squizlabs\/HTML_CodeSniffer) to name a few, our proprietary accessibility conformance testing engine at Siteimprove runs within the context of a browser. The reason why this seems to be the de facto way of implementing an accessibility tool is obvious: The browser is the tool used to consume your website, so why not test directly within that very tool? Through the APIs exposed by the browser, we get access to all the information needed in order to assess the accessibility of a website; the structure we can access and inspect through the DOM, information about styling can be gained through the CSSOM, and soon we also get our hands on a standardised accessibility tree through the [AOM](https:\/\/wicg.github.io\/aom\/).\\nHowever, not all is good in the land of browsers. Rendering a website is an inherently non-deterministic process and the timing of network requests, script execution, the content of request headers, and much more, all play a role in what the final result will look like. In most cases, this will directly affect the assessment of a tool that runs within the browser and will become very apparent at scale. At Siteimprove, we feel the effect of this on a daily basis; a customer asking us why we came up with a certain result and us having little to no clue because we cannot replicate the exact circumstances that led to that result. This is a frustrating experience for both our customers and ourselves as it makes it difficult to reason about our tool.\\nWe want to fix this and we want to fix it for good. To do so, we must ensure that we have the ability to exactly replicate the results of a given accessibility assessment. Ideally, as many unknown browser variables as possible should be taken out of the equation and the browser only be used for what is absolutely necessary.\\n\n\n##Decision\nWe will abandon any sort of dynamic analysis within the context of a browser. The input to Alfa will be static data and any assessment must be made based on that data alone. A browser may or may not be involved in the construction of the data, but the browser will not be required for any further assessment thereof.\\nIf additional data is needed by a given accessibility rule, we will adjust the data format to meet the needs of the rule. We will also carefully consider the extent of the data format as to not bloat it with information that could otherwise be inferred from existing data. Ideally, the size of the data when serialised and stored on disk will not be much larger than the size of the original source code on which the data is based.\\n","Predictions":"We will use the [AOM](https:\/\/wicg.github.io\/aom\/) to provide a single, unified view of the accessibility tool.n"}
{"File Name":"buy-for-your-school\/0005-use-bullet-to-catch-nplus1-queries.md","Context":"## Context\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\n","Decision":"Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","tokens":37,"id":1246,"text":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n\n\n##Decision\nAdd an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","Predictions":"Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.n"}
{"File Name":"my-notes\/styled-components.md","Context":"### Context\n- easier to maintain than traditional CSS\n- I was not able to get my tests working with it properly\n### Decision\nDon't start using Styled Components\n","Decision":"Don't start using Styled Components\\n","tokens":35,"id":5090,"text":"### Context\\n- easier to maintain than traditional CSS\\n- I was not able to get my tests working with it properly\\n### Decision\\nDon't start using Styled Components\\n\n\n##Decision\nDon't start using Styled Components\\n","Predictions":"Don't start using Styled Componentsn"}
{"File Name":"radiant-mlhub\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use a modified version of Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). ADR documents that are specific to this project will be stored in the `docs\/adr` directory within this repository. Instead of using the \"deprecated\" and \"superseded\" status value, we will move ADRs that are no longer applicable into the `docs\/adr\/archive` directory in this repository.\\n","tokens":16,"id":704,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use a modified version of Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions). ADR documents that are specific to this project will be stored in the `docs\/adr` directory within this repository. Instead of using the \"deprecated\" and \"superseded\" status value, we will move ADRs that are no longer applicable into the `docs\/adr\/archive` directory in this repository.\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"cosmos-sdk\/adr-030-authz-module.md","Context":"## Context\nThe concrete use cases which motivated this module include:\n* the desire to delegate the ability to vote on proposals to other accounts besides the account which one has\ndelegated stake\n* \"sub-keys\" functionality, as originally proposed in [\\#4480](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4480) which\nis a term used to describe the functionality provided by this module together with\nthe `fee_grant` module from [ADR 029](.\/adr-029-fee-grant-module.md) and the [group module](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).\nThe \"sub-keys\" functionality roughly refers to the ability for one account to grant some subset of its capabilities to\nother accounts with possibly less robust, but easier to use security measures. For instance, a master account representing\nan organization could grant the ability to spend small amounts of the organization's funds to individual employee accounts.\nOr an individual (or group) with a multisig wallet could grant the ability to vote on proposals to any one of the member\nkeys.\nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos-gaians\/cosmos-sdk\/tree\/hackatom\/x\/delegation).\n","Decision":"We will create a module named `authz` which provides functionality for\\ngranting arbitrary privileges from one account (the _granter_) to another account (the _grantee_). Authorizations\\nmust be granted for a particular `Msg` service methods one by one using an implementation\\nof `Authorization` interface.\\n### Types\\nAuthorizations determine exactly what privileges are granted. They are extensible\\nand can be defined for any `Msg` service method even outside of the module where\\nthe `Msg` method is defined. `Authorization`s reference `Msg`s using their TypeURL.\\n#### Authorization\\n```go\\ntype Authorization interface {\\nproto.Message\\n\/\/ MsgTypeURL returns the fully-qualified Msg TypeURL (as described in ADR 020),\\n\/\/ which will process and accept or reject a request.\\nMsgTypeURL() string\\n\/\/ Accept determines whether this grant permits the provided sdk.Msg to be performed, and if\\n\/\/ so provides an upgraded authorization instance.\\nAccept(ctx sdk.Context, msg sdk.Msg) (AcceptResponse, error)\\n\/\/ ValidateBasic does a simple validation check that\\n\/\/ doesn't require access to any other information.\\nValidateBasic() error\\n}\\n\/\/ AcceptResponse instruments the controller of an authz message if the request is accepted\\n\/\/ and if it should be updated or deleted.\\ntype AcceptResponse struct {\\n\/\/ If Accept=true, the controller can accept and authorization and handle the update.\\nAccept bool\\n\/\/ If Delete=true, the controller must delete the authorization object and release\\n\/\/ storage resources.\\nDelete bool\\n\/\/ Controller, who is calling Authorization.Accept must check if `Updated != nil`. If yes,\\n\/\/ it must use the updated version and handle the update on the storage level.\\nUpdated Authorization\\n}\\n```\\nFor example a `SendAuthorization` like this is defined for `MsgSend` that takes\\na `SpendLimit` and updates it down to zero:\\n```go\\ntype SendAuthorization struct {\\n\/\/ SpendLimit specifies the maximum amount of tokens that can be spent\\n\/\/ by this authorization and will be updated as tokens are spent. This field is required. (Generic authorization\\n\/\/ can be used with bank msg type url to create limit less bank authorization).\\nSpendLimit sdk.Coins\\n}\\nfunc (a SendAuthorization) MsgTypeURL() string {\\nreturn sdk.MsgTypeURL(&MsgSend{})\\n}\\nfunc (a SendAuthorization) Accept(ctx sdk.Context, msg sdk.Msg) (authz.AcceptResponse, error) {\\nmSend, ok := msg.(*MsgSend)\\nif !ok {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInvalidType.Wrap(\"type mismatch\")\\n}\\nlimitLeft, isNegative := a.SpendLimit.SafeSub(mSend.Amount)\\nif isNegative {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInsufficientFunds.Wrapf(\"requested amount is more than spend limit\")\\n}\\nif limitLeft.IsZero() {\\nreturn authz.AcceptResponse{Accept: true, Delete: true}, nil\\n}\\nreturn authz.AcceptResponse{Accept: true, Delete: false, Updated: &SendAuthorization{SpendLimit: limitLeft}}, nil\\n}\\n```\\nA different type of capability for `MsgSend` could be implemented\\nusing the `Authorization` interface with no need to change the underlying\\n`bank` module.\\n##### Small notes on `AcceptResponse`\\n* The `AcceptResponse.Accept` field will be set to `true` if the authorization is accepted.\\nHowever, if it is rejected, the function `Accept` will raise an error (without setting `AcceptResponse.Accept` to `false`).\\n* The `AcceptResponse.Updated` field will be set to a non-nil value only if there is a real change to the authorization.\\nIf authorization remains the same (as is, for instance, always the case for a [`GenericAuthorization`](#genericauthorization)),\\nthe field will be `nil`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\n\/\/ Grant grants the provided authorization to the grantee on the granter's\\n\/\/ account with the provided expiration time.\\nrpc Grant(MsgGrant) returns (MsgGrantResponse);\\n\/\/ Exec attempts to execute the provided messages using\\n\/\/ authorizations granted to the grantee. Each message should have only\\n\/\/ one signer corresponding to the granter of the authorization.\\nrpc Exec(MsgExec) returns (MsgExecResponse);\\n\/\/ Revoke revokes any authorization corresponding to the provided method name on the\\n\/\/ granter's account that has been granted to the grantee.\\nrpc Revoke(MsgRevoke) returns (MsgRevokeResponse);\\n}\\n\/\/ Grant gives permissions to execute\\n\/\/ the provided method with expiration time.\\nmessage Grant {\\ngoogle.protobuf.Any       authorization = 1 [(cosmos_proto.accepts_interface) = \"cosmos.authz.v1beta1.Authorization\"];\\ngoogle.protobuf.Timestamp expiration    = 2 [(gogoproto.stdtime) = true, (gogoproto.nullable) = false];\\n}\\nmessage MsgGrant {\\nstring granter = 1;\\nstring grantee = 2;\\nGrant grant = 3 [(gogoproto.nullable) = false];\\n}\\nmessage MsgExecResponse {\\ncosmos.base.abci.v1beta1.Result result = 1;\\n}\\nmessage MsgExec {\\nstring   grantee                  = 1;\\n\/\/ Authorization Msg requests to execute. Each msg must implement Authorization interface\\nrepeated google.protobuf.Any msgs = 2 [(cosmos_proto.accepts_interface) = \"cosmos.base.v1beta1.Msg\"];;\\n}\\n```\\n### Router Middleware\\nThe `authz` `Keeper` will expose a `DispatchActions` method which allows other modules to send `Msg`s\\nto the router based on `Authorization` grants:\\n```go\\ntype Keeper interface {\\n\/\/ DispatchActions routes the provided msgs to their respective handlers if the grantee was granted an authorization\\n\/\/ to send those messages by the first (and only) signer of each msg.\\nDispatchActions(ctx sdk.Context, grantee sdk.AccAddress, msgs []sdk.Msg) sdk.Result`\\n}\\n```\\n### CLI\\n#### `tx exec` Method\\nWhen a CLI user wants to run a transaction on behalf of another account using `MsgExec`, they\\ncan use the `exec` method. For instance `gaiacli tx gov vote 1 yes --from <grantee> --generate-only | gaiacli tx authz exec --send-as <granter> --from <grantee>`\\nwould send a transaction like this:\\n```go\\nMsgExec {\\nGrantee: mykey,\\nMsgs: []sdk.Msg{\\nMsgVote {\\nProposalID: 1,\\nVoter: cosmos3thsdgh983egh823\\nOption: Yes\\n}\\n}\\n}\\n```\\n#### `tx grant <grantee> <authorization> --from <granter>`\\nThis CLI command will send a `MsgGrant` transaction. `authorization` should be encoded as\\nJSON on the CLI.\\n#### `tx revoke <grantee> <method-name> --from <granter>`\\nThis CLI command will send a `MsgRevoke` transaction.\\n### Built-in Authorizations\\n#### `SendAuthorization`\\n```protobuf\\n\/\/ SendAuthorization allows the grantee to spend up to spend_limit coins from\\n\/\/ the granter's account.\\nmessage SendAuthorization {\\nrepeated cosmos.base.v1beta1.Coin spend_limit = 1;\\n}\\n```\\n#### `GenericAuthorization`\\n```protobuf\\n\/\/ GenericAuthorization gives the grantee unrestricted permissions to execute\\n\/\/ the provided method on behalf of the granter's account.\\nmessage GenericAuthorization {\\noption (cosmos_proto.implements_interface) = \"Authorization\";\\n\/\/ Msg, identified by it's type URL, to grant unrestricted permissions to execute\\nstring msg = 1;\\n}\\n```\\n","tokens":276,"id":839,"text":"## Context\\nThe concrete use cases which motivated this module include:\\n* the desire to delegate the ability to vote on proposals to other accounts besides the account which one has\\ndelegated stake\\n* \"sub-keys\" functionality, as originally proposed in [\\#4480](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4480) which\\nis a term used to describe the functionality provided by this module together with\\nthe `fee_grant` module from [ADR 029](.\/adr-029-fee-grant-module.md) and the [group module](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/x\/group).\\nThe \"sub-keys\" functionality roughly refers to the ability for one account to grant some subset of its capabilities to\\nother accounts with possibly less robust, but easier to use security measures. For instance, a master account representing\\nan organization could grant the ability to spend small amounts of the organization's funds to individual employee accounts.\\nOr an individual (or group) with a multisig wallet could grant the ability to vote on proposals to any one of the member\\nkeys.\\nThe current implementation is based on work done by the [Gaian's team at Hackatom Berlin 2019](https:\/\/github.com\/cosmos-gaians\/cosmos-sdk\/tree\/hackatom\/x\/delegation).\\n\n\n##Decision\nWe will create a module named `authz` which provides functionality for\\ngranting arbitrary privileges from one account (the _granter_) to another account (the _grantee_). Authorizations\\nmust be granted for a particular `Msg` service methods one by one using an implementation\\nof `Authorization` interface.\\n### Types\\nAuthorizations determine exactly what privileges are granted. They are extensible\\nand can be defined for any `Msg` service method even outside of the module where\\nthe `Msg` method is defined. `Authorization`s reference `Msg`s using their TypeURL.\\n#### Authorization\\n```go\\ntype Authorization interface {\\nproto.Message\\n\/\/ MsgTypeURL returns the fully-qualified Msg TypeURL (as described in ADR 020),\\n\/\/ which will process and accept or reject a request.\\nMsgTypeURL() string\\n\/\/ Accept determines whether this grant permits the provided sdk.Msg to be performed, and if\\n\/\/ so provides an upgraded authorization instance.\\nAccept(ctx sdk.Context, msg sdk.Msg) (AcceptResponse, error)\\n\/\/ ValidateBasic does a simple validation check that\\n\/\/ doesn't require access to any other information.\\nValidateBasic() error\\n}\\n\/\/ AcceptResponse instruments the controller of an authz message if the request is accepted\\n\/\/ and if it should be updated or deleted.\\ntype AcceptResponse struct {\\n\/\/ If Accept=true, the controller can accept and authorization and handle the update.\\nAccept bool\\n\/\/ If Delete=true, the controller must delete the authorization object and release\\n\/\/ storage resources.\\nDelete bool\\n\/\/ Controller, who is calling Authorization.Accept must check if `Updated != nil`. If yes,\\n\/\/ it must use the updated version and handle the update on the storage level.\\nUpdated Authorization\\n}\\n```\\nFor example a `SendAuthorization` like this is defined for `MsgSend` that takes\\na `SpendLimit` and updates it down to zero:\\n```go\\ntype SendAuthorization struct {\\n\/\/ SpendLimit specifies the maximum amount of tokens that can be spent\\n\/\/ by this authorization and will be updated as tokens are spent. This field is required. (Generic authorization\\n\/\/ can be used with bank msg type url to create limit less bank authorization).\\nSpendLimit sdk.Coins\\n}\\nfunc (a SendAuthorization) MsgTypeURL() string {\\nreturn sdk.MsgTypeURL(&MsgSend{})\\n}\\nfunc (a SendAuthorization) Accept(ctx sdk.Context, msg sdk.Msg) (authz.AcceptResponse, error) {\\nmSend, ok := msg.(*MsgSend)\\nif !ok {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInvalidType.Wrap(\"type mismatch\")\\n}\\nlimitLeft, isNegative := a.SpendLimit.SafeSub(mSend.Amount)\\nif isNegative {\\nreturn authz.AcceptResponse{}, sdkerrors.ErrInsufficientFunds.Wrapf(\"requested amount is more than spend limit\")\\n}\\nif limitLeft.IsZero() {\\nreturn authz.AcceptResponse{Accept: true, Delete: true}, nil\\n}\\nreturn authz.AcceptResponse{Accept: true, Delete: false, Updated: &SendAuthorization{SpendLimit: limitLeft}}, nil\\n}\\n```\\nA different type of capability for `MsgSend` could be implemented\\nusing the `Authorization` interface with no need to change the underlying\\n`bank` module.\\n##### Small notes on `AcceptResponse`\\n* The `AcceptResponse.Accept` field will be set to `true` if the authorization is accepted.\\nHowever, if it is rejected, the function `Accept` will raise an error (without setting `AcceptResponse.Accept` to `false`).\\n* The `AcceptResponse.Updated` field will be set to a non-nil value only if there is a real change to the authorization.\\nIf authorization remains the same (as is, for instance, always the case for a [`GenericAuthorization`](#genericauthorization)),\\nthe field will be `nil`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\n\/\/ Grant grants the provided authorization to the grantee on the granter's\\n\/\/ account with the provided expiration time.\\nrpc Grant(MsgGrant) returns (MsgGrantResponse);\\n\/\/ Exec attempts to execute the provided messages using\\n\/\/ authorizations granted to the grantee. Each message should have only\\n\/\/ one signer corresponding to the granter of the authorization.\\nrpc Exec(MsgExec) returns (MsgExecResponse);\\n\/\/ Revoke revokes any authorization corresponding to the provided method name on the\\n\/\/ granter's account that has been granted to the grantee.\\nrpc Revoke(MsgRevoke) returns (MsgRevokeResponse);\\n}\\n\/\/ Grant gives permissions to execute\\n\/\/ the provided method with expiration time.\\nmessage Grant {\\ngoogle.protobuf.Any       authorization = 1 [(cosmos_proto.accepts_interface) = \"cosmos.authz.v1beta1.Authorization\"];\\ngoogle.protobuf.Timestamp expiration    = 2 [(gogoproto.stdtime) = true, (gogoproto.nullable) = false];\\n}\\nmessage MsgGrant {\\nstring granter = 1;\\nstring grantee = 2;\\nGrant grant = 3 [(gogoproto.nullable) = false];\\n}\\nmessage MsgExecResponse {\\ncosmos.base.abci.v1beta1.Result result = 1;\\n}\\nmessage MsgExec {\\nstring   grantee                  = 1;\\n\/\/ Authorization Msg requests to execute. Each msg must implement Authorization interface\\nrepeated google.protobuf.Any msgs = 2 [(cosmos_proto.accepts_interface) = \"cosmos.base.v1beta1.Msg\"];;\\n}\\n```\\n### Router Middleware\\nThe `authz` `Keeper` will expose a `DispatchActions` method which allows other modules to send `Msg`s\\nto the router based on `Authorization` grants:\\n```go\\ntype Keeper interface {\\n\/\/ DispatchActions routes the provided msgs to their respective handlers if the grantee was granted an authorization\\n\/\/ to send those messages by the first (and only) signer of each msg.\\nDispatchActions(ctx sdk.Context, grantee sdk.AccAddress, msgs []sdk.Msg) sdk.Result`\\n}\\n```\\n### CLI\\n#### `tx exec` Method\\nWhen a CLI user wants to run a transaction on behalf of another account using `MsgExec`, they\\ncan use the `exec` method. For instance `gaiacli tx gov vote 1 yes --from <grantee> --generate-only | gaiacli tx authz exec --send-as <granter> --from <grantee>`\\nwould send a transaction like this:\\n```go\\nMsgExec {\\nGrantee: mykey,\\nMsgs: []sdk.Msg{\\nMsgVote {\\nProposalID: 1,\\nVoter: cosmos3thsdgh983egh823\\nOption: Yes\\n}\\n}\\n}\\n```\\n#### `tx grant <grantee> <authorization> --from <granter>`\\nThis CLI command will send a `MsgGrant` transaction. `authorization` should be encoded as\\nJSON on the CLI.\\n#### `tx revoke <grantee> <method-name> --from <granter>`\\nThis CLI command will send a `MsgRevoke` transaction.\\n### Built-in Authorizations\\n#### `SendAuthorization`\\n```protobuf\\n\/\/ SendAuthorization allows the grantee to spend up to spend_limit coins from\\n\/\/ the granter's account.\\nmessage SendAuthorization {\\nrepeated cosmos.base.v1beta1.Coin spend_limit = 1;\\n}\\n```\\n#### `GenericAuthorization`\\n```protobuf\\n\/\/ GenericAuthorization gives the grantee unrestricted permissions to execute\\n\/\/ the provided method on behalf of the granter's account.\\nmessage GenericAuthorization {\\noption (cosmos_proto.implements_interface) = \"Authorization\";\\n\/\/ Msg, identified by it's type URL, to grant unrestricted permissions to execute\\nstring msg = 1;\\n}\\n```\\n","Predictions":"We will create a module named authz which provides functionality forngranting arbitrary privileges from one account (the _granter_) to another account (the _grantee_). Authorizationsnmust be granted for a particular Msg service methods one by one using an implementationnof Authorization interface.n### TypesnAuthorizations determine exactly what privileges are granted. They are extensiblenand can be defined for any Msg service methods even outside of the module wherenthe Msg method is defined.n"}
{"File Name":"WorkoutLog\/0009_one_time_events_livedata.md","Context":"## Context\nLiveData events will be pushed again after certain LifeCycle events. Ie. when rotating the device or\nafter returning to a fragment from another navigation path. Errors or special events like automatic\nnavigation should only be executed once.\n","Decision":"A new event will be introduced for the ViewModels which has the clear purpose of only occur once.\\n","tokens":49,"id":1231,"text":"## Context\\nLiveData events will be pushed again after certain LifeCycle events. Ie. when rotating the device or\\nafter returning to a fragment from another navigation path. Errors or special events like automatic\\nnavigation should only be executed once.\\n\n\n##Decision\nA new event will be introduced for the ViewModels which has the clear purpose of only occur once.\\n","Predictions":"We will use a Security service to store the URI of the file.n"}
{"File Name":"FlowKit\/0009-asciidoctor-pdf-for-notebook-conversion.md","Context":"## Context\nWe want AutoFlow to support creation of PDF reports from Jupyter notebooks. [nbconvert](https:\/\/nbconvert.readthedocs.io\/en\/latest\/) provides a method for converting Jupyter notebooks to PDF format. However, PDF conversion with nbconvert requires a full LaTeX installation, which is ~4GB in size. To keep the size of the AutoFlow container smaller, it is desirable to find an alternative that doesn't require a LaTeX installation.\n[Asciidoctor PDF](https:\/\/asciidoctor.org\/docs\/asciidoctor-pdf\/) is a tool for converting ASCIIDoc documents to PDF format without generating an interim format such as LaTeX. Since nbconvert can convert notebooks to ASCIIDoc format, we can use Asciidoctor PDF as the second half of a two-step process to convert Jupyter notebooks to PDF via ASCIIDoc.\n","Decision":"We will use a two-step process to convert Jupyter notebooks to PDF reports in AutoFlow: convert the notebook to ASCIIDoc format using nbconvert, and then convert the resulting ASCIIDoc document to PDF using Asciidoc PDF.\\n","tokens":178,"id":5059,"text":"## Context\\nWe want AutoFlow to support creation of PDF reports from Jupyter notebooks. [nbconvert](https:\/\/nbconvert.readthedocs.io\/en\/latest\/) provides a method for converting Jupyter notebooks to PDF format. However, PDF conversion with nbconvert requires a full LaTeX installation, which is ~4GB in size. To keep the size of the AutoFlow container smaller, it is desirable to find an alternative that doesn't require a LaTeX installation.\\n[Asciidoctor PDF](https:\/\/asciidoctor.org\/docs\/asciidoctor-pdf\/) is a tool for converting ASCIIDoc documents to PDF format without generating an interim format such as LaTeX. Since nbconvert can convert notebooks to ASCIIDoc format, we can use Asciidoctor PDF as the second half of a two-step process to convert Jupyter notebooks to PDF via ASCIIDoc.\\n\n\n##Decision\nWe will use a two-step process to convert Jupyter notebooks to PDF reports in AutoFlow: convert the notebook to ASCIIDoc format using nbconvert, and then convert the resulting ASCIIDoc document to PDF using Asciidoc PDF.\\n","Predictions":"We will use a two-step process to convert Jupyter notebooks to PDF reports in AutoFlow: convert the notebook to ASCIIDoc format using nbconvert, and then convert the resulting ASCIIDoc document to PDF using Asciidoc PDF.n"}
{"File Name":"heptaconnect-docs\/2021-10-29-flow-components-are-not-crud.md","Context":"## Context\nAt the time of writing we have explorers, emitters and receivers as three main flow components.\nThey resemble CR and U from the well-known [CRUD](https:\/\/en.wikipedia.org\/wiki\/Create,_read,_update_and_delete).\nMost APIs are CRUD or [BREAD](http:\/\/paul-m-jones.com\/post\/2008\/08\/20\/bread-not-crud\/) based and therefore match the three named flow components.\nFor now, emitting and receiving entities can be also used differently as this \"just\" sends data from one portal node and is received by another portal node.\nEmitters and receivers could send commands instead of entities.\nAs previously mentioned we do not have a deletion flow component.\nA receiver could receive an entity with a custom deletion command with any previous version of HEPTAconnect.\nThis is discouraged but possible.\nWe have already seen implementations, that receive data but don't write anything to the API the portal resembles.\nThis is a misuse that is similar to described scenario above.\nLooking at the other existing flow components we also have webhooks and status reporters.\nThese are not related to CRUD at all, so we are not limited to CRUD.\n","Decision":"Receivers are not meant to do everything, when it is about receiving a command.\\nReceivers are meant to be used for entities only.\\nGrouping explorers, emitters, receivers and \"deleters\" into a single CRUD flow component enforces structures that probably don't benefit APIs, that do not fall into this pattern.\\nGrouping flow components is not helpful when we do not know the possible groups in beforehand and therefore can't be done right.\\nEvery other transfer needs a new flow component.\\nAs routes connect emitters and receivers they need to learn how to decide which flow components to use on a route.\\nThis is described in a [different ADR](.\/2021-10-30-route-capabilities.md).\\n","tokens":250,"id":3205,"text":"## Context\\nAt the time of writing we have explorers, emitters and receivers as three main flow components.\\nThey resemble CR and U from the well-known [CRUD](https:\/\/en.wikipedia.org\/wiki\/Create,_read,_update_and_delete).\\nMost APIs are CRUD or [BREAD](http:\/\/paul-m-jones.com\/post\/2008\/08\/20\/bread-not-crud\/) based and therefore match the three named flow components.\\nFor now, emitting and receiving entities can be also used differently as this \"just\" sends data from one portal node and is received by another portal node.\\nEmitters and receivers could send commands instead of entities.\\nAs previously mentioned we do not have a deletion flow component.\\nA receiver could receive an entity with a custom deletion command with any previous version of HEPTAconnect.\\nThis is discouraged but possible.\\nWe have already seen implementations, that receive data but don't write anything to the API the portal resembles.\\nThis is a misuse that is similar to described scenario above.\\nLooking at the other existing flow components we also have webhooks and status reporters.\\nThese are not related to CRUD at all, so we are not limited to CRUD.\\n\n\n##Decision\nReceivers are not meant to do everything, when it is about receiving a command.\\nReceivers are meant to be used for entities only.\\nGrouping explorers, emitters, receivers and \"deleters\" into a single CRUD flow component enforces structures that probably don't benefit APIs, that do not fall into this pattern.\\nGrouping flow components is not helpful when we do not know the possible groups in beforehand and therefore can't be done right.\\nEvery other transfer needs a new flow component.\\nAs routes connect emitters and receivers they need to learn how to decide which flow components to use on a route.\\nThis is described in a [different ADR](.\/2021-10-30-route-capabilities.md).\\n","Predictions":"We will use the [Emitter](https:\/\/github.com\/emitter\/emitter) as the default CRUD component.n"}
{"File Name":"launcher\/2018-03-29_add_crc_and_uuid_to_log_lines.md","Context":"## Context\n**TL;DR**\nA lack of unique info per-log line makes it hard to uniq osquery data downstream, especially in cases when distinct log lines with identical data can be generated. To improve visibility, we should add a UUID to each emitted line. Additionally, adding a hash of the data lets you uniq log line data without inspecting all the fields of these log lines.\n**Detail**\nWhile a unique ID is generated for every batched log event emitted from launcher, individual log lines within that log cannot be determined to be unique. This is both true for distinct logs which are identical in their contents, and in the instance that the downstream data pipeline cannot guarantee \"exactly once\" delivery of messages.\nBy including a UUID for each log line, distinct but semantically identical log lines of data can be determined not to be duplicates.\nAdditionally, by including a CRC of the data inside each log line, downstream consumers of the log can identify log lines with identical data without inspecting the entirety of log line's data.\nNote that the crc must be calculated before adding the uuid, or else it won't be useful\n","Decision":"Launcher should unpack logs from osquery and decorate each line with both a CRC of the data, and a UUID.\\n","tokens":236,"id":2549,"text":"## Context\\n**TL;DR**\\nA lack of unique info per-log line makes it hard to uniq osquery data downstream, especially in cases when distinct log lines with identical data can be generated. To improve visibility, we should add a UUID to each emitted line. Additionally, adding a hash of the data lets you uniq log line data without inspecting all the fields of these log lines.\\n**Detail**\\nWhile a unique ID is generated for every batched log event emitted from launcher, individual log lines within that log cannot be determined to be unique. This is both true for distinct logs which are identical in their contents, and in the instance that the downstream data pipeline cannot guarantee \"exactly once\" delivery of messages.\\nBy including a UUID for each log line, distinct but semantically identical log lines of data can be determined not to be duplicates.\\nAdditionally, by including a CRC of the data inside each log line, downstream consumers of the log can identify log lines with identical data without inspecting the entirety of log line's data.\\nNote that the crc must be calculated before adding the uuid, or else it won't be useful\\n\n\n##Decision\nLauncher should unpack logs from osquery and decorate each line with both a CRC of the data, and a UUID.\\n","Predictions":"* Adding a unique ID to each log line allows us to uniq osquery data downstream, without inspecting all the fields of these log lines.n* Adding a CRC of the data inside each log line allows downstream consumers of the log to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line's data.n* Adding a CRC of the data inside each log line allows us to identify log lines with identical data without inspecting the entirety of log line'"}
{"File Name":"docs\/0022-function-to-component-mapping.md","Context":"## Context and Problem Statement\nTo implement the EAI patterns we use a combination of a generic component\nwhich handles the communication with Kafka and a FaaS solution. The business logic of the EAI patterns (message splitting\/aggregation or transformation) is provided via functions which are hosted on the FaaS solution. The generic component communicates with Kafka and de\/serializes the messages. We need a means to wire the instances of the generic component with the functions. E.g. A user wants to insert a message splitter between two message-based components. To realize this an instance of the generic component in combination with a splitting FaaS function will be used. The generic component needs the address of the FaaS gateway and the function name (e.g. http:\/\/address:8080\/function\/msg-payload-splitter) to call the function. To provide the necessary information to instances of the generic component we considered the following techniques.\n## Decision Drivers <!-- optional -->\n* MUST be supported by the language\/technology which is used to implement the generic component\n* MUST be easy to integrate into MICO\n* SHOULD be a well known and proven solution\n","Decision":"* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\nChosen option: \"Environment variables\", because MICO already supports this and it is easy to implement in the generic component.\\n","tokens":231,"id":4694,"text":"## Context and Problem Statement\\nTo implement the EAI patterns we use a combination of a generic component\\nwhich handles the communication with Kafka and a FaaS solution. The business logic of the EAI patterns (message splitting\/aggregation or transformation) is provided via functions which are hosted on the FaaS solution. The generic component communicates with Kafka and de\/serializes the messages. We need a means to wire the instances of the generic component with the functions. E.g. A user wants to insert a message splitter between two message-based components. To realize this an instance of the generic component in combination with a splitting FaaS function will be used. The generic component needs the address of the FaaS gateway and the function name (e.g. http:\/\/address:8080\/function\/msg-payload-splitter) to call the function. To provide the necessary information to instances of the generic component we considered the following techniques.\\n## Decision Drivers <!-- optional -->\\n* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\n\n\n##Decision\n* MUST be supported by the language\/technology which is used to implement the generic component\\n* MUST be easy to integrate into MICO\\n* SHOULD be a well known and proven solution\\nChosen option: \"Environment variables\", because MICO already supports this and it is easy to implement in the generic component.\\n","Predictions":"* MUST be supported by the language\/technology which is used to implement the generic componentn* MUST be easy to integrate into MICOn* SHOULD be a well known and proven solutionnChosen option: \"Environment variables\", because MICO already supports this and it is easy to implement in the generic component.n"}
{"File Name":"openchs-adr\/0009-dos-and-donts-for-building-health-module-api-responses.md","Context":"## Context\nSome rules regarding usage of health-module apis.\n","Decision":"Input for health-modules can be anything from openchs-models\\nOutput of health-modules is loose right now. There is no reason at present to fix this.\\nIf a method returns an array of decisions, it has to return the same array everytime. For example, if it has [{\"highRisk\": ['overweight']}], even if there are no high risks detected, decisions have to return ['highRisk': []], and not an empty array.\\n","tokens":13,"id":2823,"text":"## Context\\nSome rules regarding usage of health-module apis.\\n\n\n##Decision\nInput for health-modules can be anything from openchs-models\\nOutput of health-modules is loose right now. There is no reason at present to fix this.\\nIf a method returns an array of decisions, it has to return the same array everytime. For example, if it has [{\"highRisk\": ['overweight']}], even if there are no high risks detected, decisions have to return ['highRisk': []], and not an empty array.\\n","Predictions":"We allow Arg sections to have an ENUM data type and add a possibleValues property to define the possible valuesnfor this arg. The ordinal number of the enum is the same as its position within the possibleValues list starting fromn0.nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that defaultnvalues match one of the possible values (if applicable).nOn code generation, an appropriate representation of this enum will be generated in the target language. The name ofnthe generated enum will be derived from the name of the arg.n### ExamplenkotlinnArg(ENUM, \"padMode\")npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")ndescription = \"padding mode\"nnnThe generated enum will be derived from the name of the arg.n### ExamplenkotlinnArg(ENUM, \"padMode\")npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")ndescription = \"padding mode\"nnnThe generated enum will be generated in the target language. The name ofnthe generated enum will be derived from the name of the arg.n### ExamplenkotlinnArg(ENUM, \"padMode\")npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")ndescription = \"padding mode\"nnnThe generated enum will be generated in the target language. The name ofnthe generated enum will be derived from the name of the arg.n### ExamplenkotlinnArg(ENUM, \"padMode\")npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")ndescription = \"padding mode\"nnnThe generated enum will be generated in the target language. The name ofnthe generated enum will be derived from the name of the arg.n### ExamplenkotlinnArg(ENUM, \"padMode\")npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")ndescription = \"padding mode\"nnnThe generated enum will be generated in the target language. The name ofnthe generated enum will be derived from the name of the arg.n### ExamplenkotlinnArg(ENUM, \"padMode\")npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")ndescription = \"padding mode\"nn"}
{"File Name":"operational-data-hub\/0017-ddd-defines-ubiquitous-language.md","Context":"## Context\nDomain-driven design (DDD) is the concept that the structure and language of software code (class names, class methods, class variables) should match the business domain. For example, if a software processes loan applications, it might have classes such as LoanApplication and Customer, and methods such as AcceptOffer and Withdraw.\nDDD connects the implementation to an evolving model.\nDomain-driven design is predicated on the following goals:\n- placing the project's primary focus on the core domain and domain logic;\n- basing complex designs on a model of the domain;\n- initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems.\nConcepts of the model include:\n### Context\nThe setting in which a word or statement appears that determines its meaning;\n### Domain\nA sphere of knowledge (ontology), influence, or activity. The subject area to which the user applies a program is the domain of the software;\n### Model\nA system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain;\n### Ubiquitous Language\nA language structured around the domain model and used by all team members to connect all the activities of the team with the software.\n","Decision":"We In the context of the ODH we will use the pricniples of Domain Driven Design and use the language which is related to the business where it is operated.\\n","tokens":261,"id":2696,"text":"## Context\\nDomain-driven design (DDD) is the concept that the structure and language of software code (class names, class methods, class variables) should match the business domain. For example, if a software processes loan applications, it might have classes such as LoanApplication and Customer, and methods such as AcceptOffer and Withdraw.\\nDDD connects the implementation to an evolving model.\\nDomain-driven design is predicated on the following goals:\\n- placing the project's primary focus on the core domain and domain logic;\\n- basing complex designs on a model of the domain;\\n- initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems.\\nConcepts of the model include:\\n### Context\\nThe setting in which a word or statement appears that determines its meaning;\\n### Domain\\nA sphere of knowledge (ontology), influence, or activity. The subject area to which the user applies a program is the domain of the software;\\n### Model\\nA system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain;\\n### Ubiquitous Language\\nA language structured around the domain model and used by all team members to connect all the activities of the team with the software.\\n\n\n##Decision\nWe In the context of the ODH we will use the pricniples of Domain Driven Design and use the language which is related to the business where it is operated.\\n","Predictions":"We In the context of the ODH we will use the pricniples of Domain Driven Design and use the language which is related to the business where it is operated.n### Positive Consequencesn* The codebase is easy to navigate and intuitive when searching for certain components. Organization should adhere to accepted practices in the.NET community.n### Negative Consequencesn* The application components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.n"}
{"File Name":"meadow\/0009-tailwind-css-framework.md","Context":"## Context\nWe have used opinionated CSS frameworks such as Twitter Bootstrap in the past, and have found that we spend too much effort and time working around those opinions. Tailwind CSS offers an alternative approach that allows us to iterate quickly with minimal interference from the framework by allowing us to add layout and styles directly in our HTML rather than CSS.\n","Decision":"Use the Tailwind CSS framework for design and layout.\\n","tokens":70,"id":3829,"text":"## Context\\nWe have used opinionated CSS frameworks such as Twitter Bootstrap in the past, and have found that we spend too much effort and time working around those opinions. Tailwind CSS offers an alternative approach that allows us to iterate quickly with minimal interference from the framework by allowing us to add layout and styles directly in our HTML rather than CSS.\\n\n\n##Decision\nUse the Tailwind CSS framework for design and layout.\\n","Predictions":"We will use [TypeScript](https:\/\/github.com\/Microsoft\/TypeScript) for implementing all of Alfa. Being a superset of JavaScript, TypeScript has a low learning curve for people already familiar with JavaScript while providing a solid type system. We will enforce API contracts through generation of [declaration files](https:\/\/www.typescriptlang.org\/docs\/handbook\/declaration-files\/introduction.html) which will dictate the API surface that consumers can access. To the extent possible, we will keep a strict TypeScript configuration in order to catch as many issues as possible at compile time. In particular, this entails strict undefined and null checking in order to get rid of a previously common type of bug.n"}
{"File Name":"cosmos-sdk\/adr-016-validator-consensus-key-rotation.md","Context":"## Context\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos SDK.\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos SDK.\n","Decision":"### Pseudo procedure for consensus key rotation\\n* create new random consensus key.\\n* create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\\n* old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\\n* start validating with new consensus key.\\n* validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\\n### Considerations\\n* consensus key mapping information management strategy\\n* store history of each key mapping changes in the kvstore.\\n* the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\\n* the state machine does not need any historical mapping information which is past more than unbonding period.\\n* key rotation costs related to LCD and IBC\\n* LCD and IBC will have traffic\/computation burden when there exists frequent power changes\\n* In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\\n* Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\\n* limits\\n* rotations are limited to 1 time in an unbonding window. In future rewrites of the staking module it could be made to happen more times than 1\\n* parameters can be decided by governance and stored in genesis file.\\n* key rotation fee\\n* a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\\n* `KeyRotationFee` = (max(`VotingPowerPercentage`, 1)* `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\\n* evidence module\\n* evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\\n* abci.ValidatorUpdate\\n* tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\\n* validator consensus key update can be done via creating new + delete old by change the power to zero.\\n* therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\\n* new genesis parameters in `staking` module\\n* `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\\n* `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\\n### Workflow\\n1. The validator generates a new consensus keypair.\\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\\n```go\\ntype MsgRotateConsPubKey struct {\\nValidatorAddress  sdk.ValAddress\\nNewPubKey         crypto.PubKey\\n}\\n```\\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\\n4. `RotateConsPubKey`\\n* checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\\n* checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\\n* checks if the signing account has enough balance to pay `KeyRotationFee`\\n* pays `KeyRotationFee` to community fund\\n* overwrites `NewPubKey` in `validator.ConsPubKey`\\n* deletes old `ValidatorByConsAddr`\\n* `SetValidatorByConsAddr` for `NewPubKey`\\n* Add `ConsPubKeyRotationHistory` for tracking rotation\\n```go\\ntype ConsPubKeyRotationHistory struct {\\nOperatorAddress         sdk.ValAddress\\nOldConsPubKey           crypto.PubKey\\nNewConsPubKey           crypto.PubKey\\nRotatedHeight           int64\\n}\\n```\\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\\n```go\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(OldConsPubKey),\\nPower:  0,\\n}\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(NewConsPubKey),\\nPower:  v.ConsensusPower(),\\n}\\n```\\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\\n* Note : All above features shall be implemented in `staking` module.\\n","tokens":171,"id":841,"text":"## Context\\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos SDK.\\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos SDK.\\n\n\n##Decision\n### Pseudo procedure for consensus key rotation\\n* create new random consensus key.\\n* create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\\n* old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\\n* start validating with new consensus key.\\n* validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\\n### Considerations\\n* consensus key mapping information management strategy\\n* store history of each key mapping changes in the kvstore.\\n* the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\\n* the state machine does not need any historical mapping information which is past more than unbonding period.\\n* key rotation costs related to LCD and IBC\\n* LCD and IBC will have traffic\/computation burden when there exists frequent power changes\\n* In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\\n* Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\\n* limits\\n* rotations are limited to 1 time in an unbonding window. In future rewrites of the staking module it could be made to happen more times than 1\\n* parameters can be decided by governance and stored in genesis file.\\n* key rotation fee\\n* a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\\n* `KeyRotationFee` = (max(`VotingPowerPercentage`, 1)* `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\\n* evidence module\\n* evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\\n* abci.ValidatorUpdate\\n* tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\\n* validator consensus key update can be done via creating new + delete old by change the power to zero.\\n* therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\\n* new genesis parameters in `staking` module\\n* `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\\n* `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\\n### Workflow\\n1. The validator generates a new consensus keypair.\\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\\n```go\\ntype MsgRotateConsPubKey struct {\\nValidatorAddress  sdk.ValAddress\\nNewPubKey         crypto.PubKey\\n}\\n```\\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\\n4. `RotateConsPubKey`\\n* checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\\n* checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\\n* checks if the signing account has enough balance to pay `KeyRotationFee`\\n* pays `KeyRotationFee` to community fund\\n* overwrites `NewPubKey` in `validator.ConsPubKey`\\n* deletes old `ValidatorByConsAddr`\\n* `SetValidatorByConsAddr` for `NewPubKey`\\n* Add `ConsPubKeyRotationHistory` for tracking rotation\\n```go\\ntype ConsPubKeyRotationHistory struct {\\nOperatorAddress         sdk.ValAddress\\nOldConsPubKey           crypto.PubKey\\nNewConsPubKey           crypto.PubKey\\nRotatedHeight           int64\\n}\\n```\\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\\n```go\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(OldConsPubKey),\\nPower:  0,\\n}\\nabci.ValidatorUpdate{\\nPubKey: cmttypes.TM2PB.PubKey(NewConsPubKey),\\nPower:  v.ConsensusPower(),\\n}\\n```\\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\\n* Note : All above features shall be implemented in `staking` module.\\n","Predictions":"We will use the Security api to generate a new validator.n"}
{"File Name":"heroes-board-game\/0002-tests-definition-and-scope.md","Context":"## Context and Problem Statement\nAcceptance tests are embedded into game server application.\nShould they be written one layer above (UI)?\n## Decision Drivers\n* Acceptance tests should be meaningful for end users\n","Decision":"* Acceptance tests should be meaningful for end users\\nRedefine current acceptance tests and treat them like _component_ tests according to [this definition](https:\/\/www.simpleorientedarchitecture.com\/defining-test-boundaries\/).\\n* Good, because it promotes different tests for different audiences\\n* Good, because it sets a clear terminology\\n* Good, because end users don't care about layers below UI\\n","tokens":44,"id":3166,"text":"## Context and Problem Statement\\nAcceptance tests are embedded into game server application.\\nShould they be written one layer above (UI)?\\n## Decision Drivers\\n* Acceptance tests should be meaningful for end users\\n\n\n##Decision\n* Acceptance tests should be meaningful for end users\\nRedefine current acceptance tests and treat them like _component_ tests according to [this definition](https:\/\/www.simpleorientedarchitecture.com\/defining-test-boundaries\/).\\n* Good, because it promotes different tests for different audiences\\n* Good, because it sets a clear terminology\\n* Good, because end users don't care about layers below UI\\n","Predictions":"* Acceptance tests should be meaningful for end usersnRedefine current acceptance tests and treat them like _component_ tests according to [this definition](https:\/\/www.simpleorientedarchitecture.com\/defining-test-boundaries\/).n* Good, because it promotes different tests for different audiencesn* Good, because it sets a clear terminologyn* Good, because end users don't care about layers below UIn"}
{"File Name":"CCS-Architecture-Decision-Records\/0008-cmp-use-shared-store-for-capturing-buyer-needs.md","Context":"## Context\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\nIn the future we will want to more automatically match buyers\u2019 articulations of need so as to work out which products CCS can offer to meet their need.\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\n","Decision":"Use a shared component to store user tasks in a question and answer format during buying journeys\\nThe Q&A definitions should be defined from a common repository defining the agreements\\nAgreed outcomes must be stored in the agreement records\\n","tokens":123,"id":1001,"text":"## Context\\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\\nIn the future we will want to more automatically match buyers\u2019 articulations of need so as to work out which products CCS can offer to meet their need.\\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\\n\n\n##Decision\nUse a shared component to store user tasks in a question and answer format during buying journeys\\nThe Q&A definitions should be defined from a common repository defining the agreements\\nAgreed outcomes must be stored in the agreement records\\n","Predictions":"We put proposed Architecture Decision Records up for feedback by the entire team. ADRs will be proposed as dedicated changes and iterated upon through the tools provided by our code review system. ADR RFCs will have a deadline; it should be no sooner than one working day after initial request.n"}
{"File Name":"structurizr-python\/0005-code-quality-assurance.md","Context":"## Context\nWriting code that adheres to style guides and other best practices can be\nannoying. We want to standardize on some best-in-class tools.\n","Decision":"We will use isort, black, and flake8.\\n","tokens":35,"id":400,"text":"## Context\\nWriting code that adheres to style guides and other best practices can be\\nannoying. We want to standardize on some best-in-class tools.\\n\n\n##Decision\nWe will use isort, black, and flake8.\\n","Predictions":"We will use isort, black, and flake8.n"}
{"File Name":"stamper\/0003-spring-boot-as-web-framework.md","Context":"## Context\nWe need to choose a web framework for the app.\n","Decision":"We use Spring Boot because it allows us starting fast, and concentrating on business logic of the app, rather than working on infrastructure tasks.\\n","tokens":15,"id":1243,"text":"## Context\\nWe need to choose a web framework for the app.\\n\n\n##Decision\nWe use Spring Boot because it allows us starting fast, and concentrating on business logic of the app, rather than working on infrastructure tasks.\\n","Predictions":"We use [Azure](https:\/\/azure.io\/) as the database model for the application.n"}
{"File Name":"snippets-service\/0005-frequency-capping.md","Context":"## Context\nFrequency Capping allows Content Managers to limit the number of\nimpressions or interactions users have with content. Is a widely\navailable tool in Publishing Platforms.\nIt's usually developed on the server side where the system can decide\nhow many times to serve the content to the requesting users which we\ncall \"Global Frequency Capping\". Additionally the system may be able\nto limit the number of impressions per user which we call \"Local\" or\n\"User Frequency Capping\".\nFor example a Content Piece can be set to 1,000,000 Global Impressions\nand 1 Impression per User, thus indirectly driving 1,000,000 different\nusers to this Content.\nThis functionality has been lacking from the Snippet Service due to\ntechnical limitations imposed by the way metrics were collected and\ncontent selection was handled on the client side. The latest\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\nPipeline unblock this capability. [0]\n","Decision":"We decide to implement the Frequency Capping functionality into our\\nplatform to allow Content Managers to limit the number of Impressions,\\nClicks and Blocks per Job.\\nLocal or User Frequency Capping will be handled on the Browser level\\nby the Firefox Messaging Platform. The later supports only Impression\\nFrequency Capping.\\nThe Snippets Service will provide an interface (UI) for the Content\\nManagers to set upper limits on the number of Impressions a Job gets\\nper Hour, Day, Week, Fortnight, Month or for the complete Browser\\nProfile Lifetime. This information is included in the JSON generated\\nfor each Job.\\nFor Global Frequency Capping the Snippets Service will provide an\\ninterface (UI) for the Content Managers to set the limits on total\\nworldwide number of Impressions, Clicks and Blocks per Job.\\nSnippets Service will query Mozilla's Redash for Telemetry data every\\nten minutes and will fetch current impressions, clicks, blocks for\\neach Job with set limits.\\nWhen the reported numbers exceed the set limits then, the Job will be\\nmarked COMPLETE and will be pulled out of the Bundles on the next run\\nof `update_jobs` cron job.\\nThe Frequency Capping functionality is additional to the Date\\nPublishing controls, therefore a Job can end on a specific Date and\\nTime or when its Global Frequency Capping Limits are met.\\n### Monitoring and Handling of Errors\\nSince Global Frequency Capping depends on an external system for\\nMetrics (Redash \/ Telemetry) it is possible that the latest numbers are\\nnot always available to the Snippets Service to make a decision. Such\\ncases include scheduled or unplanned service interruptions or network\\nerrors.\\nIn co-ordination with Snippet Content Owner we decided that for cases\\nwhere the Snippets Service cannot get the latest numbers for more than\\n24 hours, Jobs with Global Frequency Capping will get canceled. The\\ncancellation reason will state that the Jobs where prematurely\\nterminated due to missing metrics.\\nThe cron job responsible for fetching the Data from Telemetry is\\nmonitored by a Dead Man's Snitch.\\n","tokens":208,"id":3229,"text":"## Context\\nFrequency Capping allows Content Managers to limit the number of\\nimpressions or interactions users have with content. Is a widely\\navailable tool in Publishing Platforms.\\nIt's usually developed on the server side where the system can decide\\nhow many times to serve the content to the requesting users which we\\ncall \"Global Frequency Capping\". Additionally the system may be able\\nto limit the number of impressions per user which we call \"Local\" or\\n\"User Frequency Capping\".\\nFor example a Content Piece can be set to 1,000,000 Global Impressions\\nand 1 Impression per User, thus indirectly driving 1,000,000 different\\nusers to this Content.\\nThis functionality has been lacking from the Snippet Service due to\\ntechnical limitations imposed by the way metrics were collected and\\ncontent selection was handled on the client side. The latest\\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\\nPipeline unblock this capability. [0]\\n\n\n##Decision\nWe decide to implement the Frequency Capping functionality into our\\nplatform to allow Content Managers to limit the number of Impressions,\\nClicks and Blocks per Job.\\nLocal or User Frequency Capping will be handled on the Browser level\\nby the Firefox Messaging Platform. The later supports only Impression\\nFrequency Capping.\\nThe Snippets Service will provide an interface (UI) for the Content\\nManagers to set upper limits on the number of Impressions a Job gets\\nper Hour, Day, Week, Fortnight, Month or for the complete Browser\\nProfile Lifetime. This information is included in the JSON generated\\nfor each Job.\\nFor Global Frequency Capping the Snippets Service will provide an\\ninterface (UI) for the Content Managers to set the limits on total\\nworldwide number of Impressions, Clicks and Blocks per Job.\\nSnippets Service will query Mozilla's Redash for Telemetry data every\\nten minutes and will fetch current impressions, clicks, blocks for\\neach Job with set limits.\\nWhen the reported numbers exceed the set limits then, the Job will be\\nmarked COMPLETE and will be pulled out of the Bundles on the next run\\nof `update_jobs` cron job.\\nThe Frequency Capping functionality is additional to the Date\\nPublishing controls, therefore a Job can end on a specific Date and\\nTime or when its Global Frequency Capping Limits are met.\\n### Monitoring and Handling of Errors\\nSince Global Frequency Capping depends on an external system for\\nMetrics (Redash \/ Telemetry) it is possible that the latest numbers are\\nnot always available to the Snippets Service to make a decision. Such\\ncases include scheduled or unplanned service interruptions or network\\nerrors.\\nIn co-ordination with Snippet Content Owner we decided that for cases\\nwhere the Snippets Service cannot get the latest numbers for more than\\n24 hours, Jobs with Global Frequency Capping will get canceled. The\\ncancellation reason will state that the Jobs where prematurely\\nterminated due to missing metrics.\\nThe cron job responsible for fetching the Data from Telemetry is\\nmonitored by a Dead Man's Snitch.\\n","Predictions":"We decide to implement the Frequency Capping functionality into ournplatform to allow Content Managers to limit the number of Impressions,nClicks and Blocks per Job.nLocal or User Frequency Capping will be handled on the Browser levelnby the Firefox Messaging Platform. The later supports only ImpressionnFrequency Capping.nThe Snippets Service will provide an interface (UI) for the ContentnManagers to set upper limits on the number of Impressions a Job getsnper Hour, Day, Week, Fortnight, Month or for the complete BrowsernProfile Lifetime. This information is included in the JSON generatednfor each Job.nFor Global Frequency Capping the Snippets Service will provide anninterface (UI) for the Content Managers to set the limits on totalnworldwide number of Impressions, Clicks and Blocks per Job.nSnippets Service will query Mozilla's Redash for Telemetry data everynten minutes and will fetch current impressions, clicks, blocks forneach Job with set limits.n"}
{"File Name":"adr\/ADR-1-documenting-architecture-decisions.md","Context":"## Context\nArchitecture for agile projects has to be described and defined differently. Not all decisions will be made at once, nor will all of them be done when the project begins.\nAgile methods are not opposed to documentation, only to valueless documentation. Documents that assist the team itself can have value, but only if they are kept up to date. Large documents are never kept up to date. Small, modular documents have at least a chance at being updated.\nNobody ever reads large documents, either. Most developers have been on at least one project where the specification document was larger (in bytes) than the total source code size. Those documents are too large to open, read, or update. Bite sized pieces are easier for for all stakeholders to consume.\nOne of the hardest things to track during the life of a project is the motivation behind certain decisions. A new person coming on to a project may be perplexed, baffled, delighted, or infuriated by some past decision. Without understanding the rationale or consequences, this person has only two choices:\n1. **Blindly accept the decision.**\nThis response may be OK, if the decision is still valid. It may not be good, however, if the context has changed and the decision should really be revisited. If the project accumulates too many decisions accepted without understanding, then the development team becomes afraid to change anything and the project collapses under its own weight.\n2. **Blindly change it.**\nAgain, this may be OK if the decision needs to be reversed. On the other hand, changing the decision without understanding its motivation or consequences could mean damaging the project's overall value without realizing it. (E.g., the decision supported a non-functional requirement that hasn't been tested yet.)\nIt's better to avoid either blind acceptance or blind reversal.\n","Decision":"We will keep a collection of records for \"architecturally significant\" decisions: those that affect the structure, non-functional characteristics, dependencies, interfaces, or construction techniques.\\nAn architecture decision record is a short text file in a format similar to an Alexandrian pattern. (Though the decisions themselves are not necessarily patterns, they share the characteristic balancing of forces.) Each record describes a set of forces and a single decision in response to those forces. Note that the decision is the central piece here, so specific forces may appear in multiple ADRs.\\nWe will keep ADRs in the project repository under `docs\/ADR-####-title.md`\\nWe should use a lightweight text formatting language like Markdown or Textile.\\nADRs will be numbered sequentially and monotonically. Numbers will not be reused.\\nIf a decision is reversed, we will keep the old one around, but mark it as superseded. (It's still relevant to know that it _was_ the decision, but is _no longer_ the decision.)\\nWe will use a format with just a few parts, so each document is easy to digest. The format has just a few parts.\\n**Title** These documents have names that are short noun phrases. For example, \"ADR 1: Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\n**Context** This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n**Decision** This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will \u2026\"\\n**Status** A decision may be \"proposed\" if the project stakeholders haven't agreed with it yet, or \"accepted\" once it is agreed. If a later ADR changes or reverses a decision, it may be marked as \"deprecated\" or \"superseded\" with a reference to its replacement.\\n**Consequences** This section describes the resulting context, after applying the decision. All consequences should be listed here, not just the \"positive\" ones. A particular decision may have positive, negative, and neutral consequences, but all of them affect the team and project in the future.\\nThe whole document should be one or two pages long. We will write each ADR as if it is a conversation with a future developer. This requires good writing style, with full sentences organized into paragraphs. Bullets are acceptable only for visual style, not as an excuse for writing sentence fragments. (Bullets kill people, even PowerPoint bullets.)\\n# Status\\nAccepted.\\n# Consequences\\nOne ADR describes one significant decision for a specific project. It should be something that has an effect on how the rest of the project will run.\\nThe consequences of one ADR are very likely to become the context for subsequent ADRs. This is also similar to Alexander's idea of a pattern language: the large-scale responses create spaces for the smaller scale to fit into.\\nDevelopers and project stakeholders can see the ADRs, even as the team composition changes over time.\\nThe motivation behind previous decisions is visible for everyone, present and future. Nobody is left scratching their heads to understand, \"What were they thinking?\" and the time to change old decisions will be clear from changes in the project's context.\\n---\\nOriginal document: [Documenting Architecture Decisions](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) by Michael Nygard\\nMore reading:\\n1. [Importance of architecture decisions](http:\/\/www.computer.org\/portal\/web\/csdl\/doi\/10.1109\/MS.2009.52)\\n2. [Documenting software architectures](http:\/\/www.sei.cmu.edu\/library\/abstracts\/books\/0321552687.cfm)\\n","tokens":380,"id":4620,"text":"## Context\\nArchitecture for agile projects has to be described and defined differently. Not all decisions will be made at once, nor will all of them be done when the project begins.\\nAgile methods are not opposed to documentation, only to valueless documentation. Documents that assist the team itself can have value, but only if they are kept up to date. Large documents are never kept up to date. Small, modular documents have at least a chance at being updated.\\nNobody ever reads large documents, either. Most developers have been on at least one project where the specification document was larger (in bytes) than the total source code size. Those documents are too large to open, read, or update. Bite sized pieces are easier for for all stakeholders to consume.\\nOne of the hardest things to track during the life of a project is the motivation behind certain decisions. A new person coming on to a project may be perplexed, baffled, delighted, or infuriated by some past decision. Without understanding the rationale or consequences, this person has only two choices:\\n1. **Blindly accept the decision.**\\nThis response may be OK, if the decision is still valid. It may not be good, however, if the context has changed and the decision should really be revisited. If the project accumulates too many decisions accepted without understanding, then the development team becomes afraid to change anything and the project collapses under its own weight.\\n2. **Blindly change it.**\\nAgain, this may be OK if the decision needs to be reversed. On the other hand, changing the decision without understanding its motivation or consequences could mean damaging the project's overall value without realizing it. (E.g., the decision supported a non-functional requirement that hasn't been tested yet.)\\nIt's better to avoid either blind acceptance or blind reversal.\\n\n\n##Decision\nWe will keep a collection of records for \"architecturally significant\" decisions: those that affect the structure, non-functional characteristics, dependencies, interfaces, or construction techniques.\\nAn architecture decision record is a short text file in a format similar to an Alexandrian pattern. (Though the decisions themselves are not necessarily patterns, they share the characteristic balancing of forces.) Each record describes a set of forces and a single decision in response to those forces. Note that the decision is the central piece here, so specific forces may appear in multiple ADRs.\\nWe will keep ADRs in the project repository under `docs\/ADR-####-title.md`\\nWe should use a lightweight text formatting language like Markdown or Textile.\\nADRs will be numbered sequentially and monotonically. Numbers will not be reused.\\nIf a decision is reversed, we will keep the old one around, but mark it as superseded. (It's still relevant to know that it _was_ the decision, but is _no longer_ the decision.)\\nWe will use a format with just a few parts, so each document is easy to digest. The format has just a few parts.\\n**Title** These documents have names that are short noun phrases. For example, \"ADR 1: Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\n**Context** This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n**Decision** This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will \u2026\"\\n**Status** A decision may be \"proposed\" if the project stakeholders haven't agreed with it yet, or \"accepted\" once it is agreed. If a later ADR changes or reverses a decision, it may be marked as \"deprecated\" or \"superseded\" with a reference to its replacement.\\n**Consequences** This section describes the resulting context, after applying the decision. All consequences should be listed here, not just the \"positive\" ones. A particular decision may have positive, negative, and neutral consequences, but all of them affect the team and project in the future.\\nThe whole document should be one or two pages long. We will write each ADR as if it is a conversation with a future developer. This requires good writing style, with full sentences organized into paragraphs. Bullets are acceptable only for visual style, not as an excuse for writing sentence fragments. (Bullets kill people, even PowerPoint bullets.)\\n# Status\\nAccepted.\\n# Consequences\\nOne ADR describes one significant decision for a specific project. It should be something that has an effect on how the rest of the project will run.\\nThe consequences of one ADR are very likely to become the context for subsequent ADRs. This is also similar to Alexander's idea of a pattern language: the large-scale responses create spaces for the smaller scale to fit into.\\nDevelopers and project stakeholders can see the ADRs, even as the team composition changes over time.\\nThe motivation behind previous decisions is visible for everyone, present and future. Nobody is left scratching their heads to understand, \"What were they thinking?\" and the time to change old decisions will be clear from changes in the project's context.\\n---\\nOriginal document: [Documenting Architecture Decisions](https:\/\/cognitect.com\/blog\/2011\/11\/15\/documenting-architecture-decisions) by Michael Nygard\\nMore reading:\\n1. [Importance of architecture decisions](http:\/\/www.computer.org\/portal\/web\/csdl\/doi\/10.1109\/MS.2009.52)\\n2. [Documenting software architectures](http:\/\/www.sei.cmu.edu\/library\/abstracts\/books\/0321552687.cfm)\\n","Predictions":"We will keep a collection of records for \"architecturally significant\" decisions: those that affect the structure, non-functional characteristics, dependencies, interfaces, or construction techniques.nAn architecture decision record is a short text file in a format similar to an Alexandrian pattern. (Though the decisions themselves are not necessarily patterns, they share the characteristic balancing of structure, non-functional characteristics, dependencies, interfaces, or construction techniques.)nAn architecture decision record is a short text file in a format similar to an Alexandrian pattern. (Though the decisions themselves are not necessarily patterns, they share the characteristic balancing of structure, non-functional characteristics, dependencies, interfaces, or construction techniques.)n"}
{"File Name":"TANF-app\/015-terraform-service-provisioning.md","Context":"## Context\nIn [this issue](https:\/\/github.com\/raft-tech\/TANF-app\/issues\/609) we propose tasks to automate the provisioning of Cloud.gov-brokered services via our CI pipeline.\nThis project plans to use Cloud.gov-brokered S3 buckets for TANF data file storage (sensitive PII data) and for Django Admin static assets (public), and a Cloud.gov-brokered RDS PostgreSQL service. Currently, there are no scripts to provision and secure these S3 buckets and RDS instance, and no dev documentation on how to deploy them, secure them, and verify that they are configured correctly. Additionally, this same initial provisioning must be done again for all additional target environments.\n","Decision":"[Terraform](https:\/\/www.terraform.io\/) is a tool for building, changing, and versioning infrastructure safely and efficiently, and was proposed as a solution for managing our persistent Cloud.gov-brokered infrastructure. We can closely model what is done in [another HHS project](https:\/\/github.com\/HHS\/Head-Start-TTADP) and create per-environment infrastructure configurations which are leveraged by Terraform in CircleCI, with environment-specific settings read directly from Cloud.gov during our CI process. Note that this workflow was a [recommendation from Cloud.gov](https:\/\/www.youtube.com\/watch?v=86wfgNK_0o4), and they themselves use Terraform to provision their own infrastructure.\\n","tokens":148,"id":3794,"text":"## Context\\nIn [this issue](https:\/\/github.com\/raft-tech\/TANF-app\/issues\/609) we propose tasks to automate the provisioning of Cloud.gov-brokered services via our CI pipeline.\\nThis project plans to use Cloud.gov-brokered S3 buckets for TANF data file storage (sensitive PII data) and for Django Admin static assets (public), and a Cloud.gov-brokered RDS PostgreSQL service. Currently, there are no scripts to provision and secure these S3 buckets and RDS instance, and no dev documentation on how to deploy them, secure them, and verify that they are configured correctly. Additionally, this same initial provisioning must be done again for all additional target environments.\\n\n\n##Decision\n[Terraform](https:\/\/www.terraform.io\/) is a tool for building, changing, and versioning infrastructure safely and efficiently, and was proposed as a solution for managing our persistent Cloud.gov-brokered infrastructure. We can closely model what is done in [another HHS project](https:\/\/github.com\/HHS\/Head-Start-TTADP) and create per-environment infrastructure configurations which are leveraged by Terraform in CircleCI, with environment-specific settings read directly from Cloud.gov during our CI process. Note that this workflow was a [recommendation from Cloud.gov](https:\/\/www.youtube.com\/watch?v=86wfgNK_0o4), and they themselves use Terraform to provision their own infrastructure.\\n","Predictions":"We will use the [Security Gateway](https:\/\/github.com\/Security Gateway) as the primary provider of the TANF CI\/CD pipeline.n"}
{"File Name":"moneycount-api\/002-Choose_Persist_Currency_In_Json.md","Context":"## Context\nI have to decide how to persist data related to supported currencies in the project. Choices are a relational database or text files such as txt, xml or json. Even NoSQL databases could be used.\n","Decision":"I decided to use json files to persist supported currencies in the project because this kind of data doesn't change all the time. JSON files are easy to store, to write, and to read in Java Objects. Each time a new currency is added it's just a matter of update the file.\\n","tokens":44,"id":3071,"text":"## Context\\nI have to decide how to persist data related to supported currencies in the project. Choices are a relational database or text files such as txt, xml or json. Even NoSQL databases could be used.\\n\n\n##Decision\nI decided to use json files to persist supported currencies in the project because this kind of data doesn't change all the time. JSON files are easy to store, to write, and to read in Java Objects. Each time a new currency is added it's just a matter of update the file.\\n","Predictions":"We decided to use json files to persist supported currencies in the project because this kind of data doesn't change all the time. JSON files are easy to store, to write, and to read in Java Objects. Each time a new currency is added it's just a matter of update the file.n"}
{"File Name":"edgex-docs\/0018-Service-Registry.md","Context":"- [Context](#context)\n- [Existing Behavior](#existing-behavior)\n* [Device Services](#device-services)\n+ [Registry Client Interface Usage](#registry-client-interface-usage)\n* [Core and Support Services](#core-and-support-services)\n* [Security Proxy Setup](#security-proxy-setup)\n- [History](#history)\n- [Problem Statement](#problem-statement)\n- [Decision](#decision)\n- [References](#references)\n<!--te-->\n","Decision":"- [References](#references)\\n<!--te-->\\nUpdate the core, support, and security-proxy-setup services to use go-mod-registry's ```Client.GetServiceEndpoint``` method (if started with the\\n```--registry``` option) to determine (a) if a service dependency is available and (b) use the returned address information to initialize client endpoints\\n(or setup the correct route in the case of proxy-setup). The same changes also need to be applied to the App Functions SDK and Go Device SDK, with\\nonly minor changes required in the C Device SDK (see previous commments re: the current implementation).\\n**Note** - this design only works if service registration occurs _before_ the service initializes its clients. For instance, Core Data and Core Metadata\\nboth depend on the other, and thus if both defer service registration till after client initialization, neither will be able to successfully lookup\\nthe address of the other service.\\n","tokens":111,"id":946,"text":"- [Context](#context)\\n- [Existing Behavior](#existing-behavior)\\n* [Device Services](#device-services)\\n+ [Registry Client Interface Usage](#registry-client-interface-usage)\\n* [Core and Support Services](#core-and-support-services)\\n* [Security Proxy Setup](#security-proxy-setup)\\n- [History](#history)\\n- [Problem Statement](#problem-statement)\\n- [Decision](#decision)\\n- [References](#references)\\n<!--te-->\\n\n\n##Decision\n- [References](#references)\\n<!--te-->\\nUpdate the core, support, and security-proxy-setup services to use go-mod-registry's ```Client.GetServiceEndpoint``` method (if started with the\\n```--registry``` option) to determine (a) if a service dependency is available and (b) use the returned address information to initialize client endpoints\\n(or setup the correct route in the case of proxy-setup). The same changes also need to be applied to the App Functions SDK and Go Device SDK, with\\nonly minor changes required in the C Device SDK (see previous commments re: the current implementation).\\n**Note** - this design only works if service registration occurs _before_ the service initializes its clients. For instance, Core Data and Core Metadata\\nboth depend on the other, and thus if both defer service registration till after client initialization, neither will be able to successfully lookup\\nthe address of the other service.\\n","Predictions":"* [Existing Behavior](#existing-behavior)n* [Device Services](#device-services)n+ [Registry Client Interface Usage](#registry-client-interface-usage)n* [Core and Support Services](#core-and-support-services)n* [Security Proxy Setup](#security-proxy-setup)n- [History](#history)n- [Problem Statement](#problem-statement)n- [Decision](#decision)n* [References](#references)n!--te-->nUpdate the core, support, and security-proxy-setup services to use go-mod-registry's Client.GetServiceEndpoint method (if started with then--registry option) to determine (a) if a service dependency is available and (b) use the returned address information to initialize client endpointsn(or setup the correct route in the case of proxy-setup). The same changes also need to be applied to the App Functions SDK and Go Device SDK, withnonly minor changes required in the C Device SDK (see previous comments re: the current implementation).n**Note** - this design only works if service registration occurs _before_ the service initializes its clients. For instance, Core Data and Core Metadatanboth depend on the other, and thus if both defer service registration till after client initialization, neither will be able to successfully lookup the address of the other service.n"}
{"File Name":"knot-documentation\/ARC-002-fog-cloud-sync.md","Context":"## Context\nOne of the main questions that appear when discussing the new fog service development\/integration was how the device contextual representation and data should be synchronized between fog and cloud. Before, when we were using the Meshblu we decided to send all the messages (device registration, schema, and data) to the cloud independently of the message type, data relevance at time or cloud connectivity state. This solution was simple for that moment but was increasing the internal message queues size and obviously isn't optimal mainly when dealing with a constrained environment. Therefore, our goal is to start with a simple but optimal approach in the first stage of the new fog service integration.\n","Decision":"* Retry to register and update the device's schema on the cloud five times and if the error persists retry with ten minutes interval.\\n* Retry to publish only the last device's data.\\n* Retry to execute only the last commands sent to the device, avoiding to store a lot of pending commands on the message queue (which could lead to inconsistencies).\\n","tokens":132,"id":3337,"text":"## Context\\nOne of the main questions that appear when discussing the new fog service development\/integration was how the device contextual representation and data should be synchronized between fog and cloud. Before, when we were using the Meshblu we decided to send all the messages (device registration, schema, and data) to the cloud independently of the message type, data relevance at time or cloud connectivity state. This solution was simple for that moment but was increasing the internal message queues size and obviously isn't optimal mainly when dealing with a constrained environment. Therefore, our goal is to start with a simple but optimal approach in the first stage of the new fog service integration.\\n\n\n##Decision\n* Retry to register and update the device's schema on the cloud five times and if the error persists retry with ten minutes interval.\\n* Retry to publish only the last device's data.\\n* Retry to execute only the last commands sent to the device, avoiding to store a lot of pending commands on the message queue (which could lead to inconsistencies).\\n","Predictions":"* Retry to register and update the device's schema on the cloud five times and if the error persists retry with ten minutes interval.n* Retry to publish only the last device's data.n* Retry to execute only the last commands sent to the device, avoiding to store a lot of pending commands on the message queue (which could lead to inconsistencies).n"}
{"File Name":"Marain.Instance\/0001-marain-service-list-and-instance-manifests.md","Context":"## Context\nThe desire to define formally and unambiguously what are the component parts of a Marain instance. (An 'instance' is one deployed set of services operating in isolation from any other instance. We maintain a dev instance separate from any production use. Where customers use Marain services but are not using endjin's hosted production instance, they have their own instance.)\n","Decision":"The `Marain.Instance` repo (this repo) includes a master service list, `Solutions\/MarainServices.jsonc`. This JSON (with comments) file contains an entry for each service that can be part of a Marain instance. This gives a name to the service (e.g. `Marain.Tenancy`) and identifies the GitHub project in which the service is defined. It also defines an API prefix, for use in scenarios where all services are made available behind a single API management layer\u2014the API prefix indicates what the first part of the URL should be at the API gateway for accessing the relevant service.\\nWhereas `MarainServices.jsonc` is common to all instances, each instance also defines a manifest. This determines whether particular services are deployed to a particular instance, and if so which version.\\n","tokens":79,"id":357,"text":"## Context\\nThe desire to define formally and unambiguously what are the component parts of a Marain instance. (An 'instance' is one deployed set of services operating in isolation from any other instance. We maintain a dev instance separate from any production use. Where customers use Marain services but are not using endjin's hosted production instance, they have their own instance.)\\n\n\n##Decision\nThe `Marain.Instance` repo (this repo) includes a master service list, `Solutions\/MarainServices.jsonc`. This JSON (with comments) file contains an entry for each service that can be part of a Marain instance. This gives a name to the service (e.g. `Marain.Tenancy`) and identifies the GitHub project in which the service is defined. It also defines an API prefix, for use in scenarios where all services are made available behind a single API management layer\u2014the API prefix indicates what the first part of the URL should be at the API gateway for accessing the relevant service.\\nWhereas `MarainServices.jsonc` is common to all instances, each instance also defines a manifest. This determines whether particular services are deployed to a particular instance, and if so which version.\\n","Predictions":"The Marain.Instance repo (this repo) includes a master service list, Solutions\/MarainServices.jsonc. This JSON (with comments) file contains an entry for each service that can be part of a Marain instance. This gives a name to the service (e.g. Marain.Tenancy) and identifies the GitHub project in which the service is defined. It also defines an API prefix, for use in scenarios where all services are made available behind a single API management layer\u2014the API prefix indicates what the first part of the URL should be at the API gateway for accessing the relevant service.nWhereas MarainServices.jsonc is common to all instances, each instance also defines a manifest. This determines whether particular services are deployed to a particular instance, and if so which version.n"}
{"File Name":"gsp\/ADR028-container-tools.md","Context":"## Context\nWe are currently using Docker as our container runtime.\nThere are needs for public docker images:\n* so that master builds can be easily pulled and tested in the local development environments\n* so that images can be easily shared between different teams\nThere are needs for digitally verifying the provenance of images:\n* so that production systems can trust that an image has not been tampered with\n* so that production systems can authenticate the origin of a build\nThere are needs for vulnerability scanning:\n* so that production systems can warn or prevent exploitable software running in production\nThe docker ecosystem provides tooling that can help us meet these needs:\n* [Docker Content Trust] (Notary) can be used to sign images and prove provenance\n* [Docker Registries][Docker Registry] can expose images publicly\n* Scanning tools like [Clair] can periodically or at pull\/push time perform CVE scanning.\nUnfortunately AWS [ECR] does not currently support public images or [Docker Content Trust], and there is no managed solution to image scanning from AWS as yet.\n### Option 1: Wrap AWS ECR\nWe could write\/manage a proxy to allow exposing [ECR] publicly and integrate the missing features.\n* Potentially fragile implementation tied to the underlying AWS services\n* Another thing to have to maintain\n* Might offer ability to remove functionality as AWS support more features in future\n### Option 2: Use an external SaaS offering\nUse a SaaS service like [Quay] which offers most of these features.\n* Additional configuration for cluster to pull from external source\n* Reduces ability to automate provisioning (requires additional credential management)\n* Procurement\n### Option 3: Self-hosted Docker Tools in cluster\nWe could deploy Docker Distribution, Notary & Clair into the cluster backed by a managed storage backend like S3\n* Well integrated with the platform\n* Would work for local GSP instance\n","Decision":"We will run a self hosted set of Docker tools\\n","tokens":406,"id":3917,"text":"## Context\\nWe are currently using Docker as our container runtime.\\nThere are needs for public docker images:\\n* so that master builds can be easily pulled and tested in the local development environments\\n* so that images can be easily shared between different teams\\nThere are needs for digitally verifying the provenance of images:\\n* so that production systems can trust that an image has not been tampered with\\n* so that production systems can authenticate the origin of a build\\nThere are needs for vulnerability scanning:\\n* so that production systems can warn or prevent exploitable software running in production\\nThe docker ecosystem provides tooling that can help us meet these needs:\\n* [Docker Content Trust] (Notary) can be used to sign images and prove provenance\\n* [Docker Registries][Docker Registry] can expose images publicly\\n* Scanning tools like [Clair] can periodically or at pull\/push time perform CVE scanning.\\nUnfortunately AWS [ECR] does not currently support public images or [Docker Content Trust], and there is no managed solution to image scanning from AWS as yet.\\n### Option 1: Wrap AWS ECR\\nWe could write\/manage a proxy to allow exposing [ECR] publicly and integrate the missing features.\\n* Potentially fragile implementation tied to the underlying AWS services\\n* Another thing to have to maintain\\n* Might offer ability to remove functionality as AWS support more features in future\\n### Option 2: Use an external SaaS offering\\nUse a SaaS service like [Quay] which offers most of these features.\\n* Additional configuration for cluster to pull from external source\\n* Reduces ability to automate provisioning (requires additional credential management)\\n* Procurement\\n### Option 3: Self-hosted Docker Tools in cluster\\nWe could deploy Docker Distribution, Notary & Clair into the cluster backed by a managed storage backend like S3\\n* Well integrated with the platform\\n* Would work for local GSP instance\\n\n\n##Decision\nWe will run a self hosted set of Docker tools\\n","Predictions":"We will use Docker to run our tests.n"}
{"File Name":"beis-report-official-development-assistance\/0009-use-pundit-to-manage-permissions.md","Context":"## Context\nThe service will be used by a variety of different types of users, and we need\nto ensure that those users are only able to access the parts of the service that\nwe want them to.\nOur current chosen authentication provider, Auth0, has support for assigning\nroles to users, but this couples the service tightly to their service, so we\nshould avoid this.\n","Decision":"We will use the 'Pundit' gem to manage users' access to the service. The\\npermissions will be grouped into roles that can then be assigned to users\\nrequiring a particular level of access.\\n","tokens":81,"id":2397,"text":"## Context\\nThe service will be used by a variety of different types of users, and we need\\nto ensure that those users are only able to access the parts of the service that\\nwe want them to.\\nOur current chosen authentication provider, Auth0, has support for assigning\\nroles to users, but this couples the service tightly to their service, so we\\nshould avoid this.\\n\n\n##Decision\nWe will use the 'Pundit' gem to manage users' access to the service. The\\npermissions will be grouped into roles that can then be assigned to users\\nrequiring a particular level of access.\\n","Predictions":"1. Every client using a Marain instance will have a Security account.n2. Every Marain instance will have a Security account.n3. Every Marain instance will have a Security account.n4. Every Marain instance will have a Security account.n5. Every Marain instance will have a Security account.n"}
{"File Name":"Wikibase\/0009-refactor-hooks-for-testability.md","Context":"## Context\nCurrently, the `RepoHooks` class remains largely untested due to a combination of two factors:\n1. The methods in this class are static, and we do not own the contract under which they should be called, as they are\ndefined as hooks in `extension.json` or as global variables in the entrypoints e.g. [extensions\/Wikibase\/repo\/Wikibase.php:1020](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/7b20d22b3c0bbc37ad23f63e38fadc9b1f2ca057\/repo\/Wikibase.php#L1020), which means we cannot easily refactor the methods to increase testability\n2. Methods rely heavily on the `WikibaseRepo` singleton and it's store, which make it harder to test, as there is no\nway to to inject a mock of `WikibaseRepo` without dependency injection.\nA [RFC for enabling dependency injection](https:\/\/phabricator.wikimedia.org\/T240307) in hooks is currently under way.\nHowever, an interim solution is needed in order to mitigate the amount of untested logic that exists in that file\nand other places in the codebase.\nWhile reviewing this issue, two initial solutions were considered:\n- Refactor `RepoHooks` into a singleton itself, so that when instantiated, we can inject a Mock of `WikibaseRepo`\ninstead of using the real deal.\n- Adopt a pattern used in `WikibaseClient` Which enables us to mock several parts of it (namely the store), and replace\nthe real store by creating an `overrideStore` method. See in following:\n- [`client\/tests\/phpunit\/includes\/MockClientStore.php`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/MockClientStore.php)\n- [`client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php:42`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php#L42)\nHowever, after a discussion, it was decided to implement a middle ground, that would enable us to gradually refactor\nhooks, rather than a one time big change.\n","Decision":"It was decided to adopt an existing pattern in Wikibase repo where each hook handler gets its own singleton class and\\nprovides at least four methods:\\n- A constructor to make dependency injection easier.\\n- A public static method to bootstrap an instance from global settings. For consistency, this would typically be named\\n`newFromGlobalSettings`.\\n- (optional) A public static method to get a cached instance of the handler object (rather than instantiate it each time):\\nThis is useful for hooks handlers which are called several times.\\n- A public static method to wire up functionality into the hooks system and should contain little to no logic (as it is\\nhard to test without complete integration or e2e tests).\\n- A public method to perform the actual handler logic in a testable way.\\nThis class should be placed under the `includes\/Hooks` directory. An example of this pattern can be seen in:\\nhttps:\/\/gerrit.wikimedia.org\/r\/c\/mediawiki\/extensions\/Wikibase\/+\/574495\\n","tokens":496,"id":1339,"text":"## Context\\nCurrently, the `RepoHooks` class remains largely untested due to a combination of two factors:\\n1. The methods in this class are static, and we do not own the contract under which they should be called, as they are\\ndefined as hooks in `extension.json` or as global variables in the entrypoints e.g. [extensions\/Wikibase\/repo\/Wikibase.php:1020](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/7b20d22b3c0bbc37ad23f63e38fadc9b1f2ca057\/repo\/Wikibase.php#L1020), which means we cannot easily refactor the methods to increase testability\\n2. Methods rely heavily on the `WikibaseRepo` singleton and it's store, which make it harder to test, as there is no\\nway to to inject a mock of `WikibaseRepo` without dependency injection.\\nA [RFC for enabling dependency injection](https:\/\/phabricator.wikimedia.org\/T240307) in hooks is currently under way.\\nHowever, an interim solution is needed in order to mitigate the amount of untested logic that exists in that file\\nand other places in the codebase.\\nWhile reviewing this issue, two initial solutions were considered:\\n- Refactor `RepoHooks` into a singleton itself, so that when instantiated, we can inject a Mock of `WikibaseRepo`\\ninstead of using the real deal.\\n- Adopt a pattern used in `WikibaseClient` Which enables us to mock several parts of it (namely the store), and replace\\nthe real store by creating an `overrideStore` method. See in following:\\n- [`client\/tests\/phpunit\/includes\/MockClientStore.php`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/MockClientStore.php)\\n- [`client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php:42`](https:\/\/github.com\/wikimedia\/mediawiki-extensions-Wikibase\/blob\/master\/client\/tests\/phpunit\/includes\/DataAccess\/ParserFunctions\/PropertyParserFunctionIntegrationTest.php#L42)\\nHowever, after a discussion, it was decided to implement a middle ground, that would enable us to gradually refactor\\nhooks, rather than a one time big change.\\n\n\n##Decision\nIt was decided to adopt an existing pattern in Wikibase repo where each hook handler gets its own singleton class and\\nprovides at least four methods:\\n- A constructor to make dependency injection easier.\\n- A public static method to bootstrap an instance from global settings. For consistency, this would typically be named\\n`newFromGlobalSettings`.\\n- (optional) A public static method to get a cached instance of the handler object (rather than instantiate it each time):\\nThis is useful for hooks handlers which are called several times.\\n- A public static method to wire up functionality into the hooks system and should contain little to no logic (as it is\\nhard to test without complete integration or e2e tests).\\n- A public method to perform the actual handler logic in a testable way.\\nThis class should be placed under the `includes\/Hooks` directory. An example of this pattern can be seen in:\\nhttps:\/\/gerrit.wikimedia.org\/r\/c\/mediawiki\/extensions\/Wikibase\/+\/574495\\n","Predictions":"We will use a repo-store method to store the repo-store in the extension.json file.n"}
{"File Name":"qc-atlas\/0008-testcontainers.md","Context":"## Context and Problem Statement\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\n","Decision":"Chosen option: [Testcontainers](https:\/\/www.testcontainers.org\/), because it simplifies the test execution process\\n### Positive Consequences <!-- optional -->\\n* It is not mandatory to ensure postgres is running before starting the tests\\n","tokens":34,"id":699,"text":"## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n\n\n##Decision\nChosen option: [Testcontainers](https:\/\/www.testcontainers.org\/), because it simplifies the test execution process\\n### Positive Consequences <!-- optional -->\\n* It is not mandatory to ensure postgres is running before starting the tests\\n","Predictions":"Chosen option: [Testcontainers](https:\/\/www.testcontainers.org\/), because it simplifies the test execution processn### Positive Consequences!-- optional -->n* It is not mandatory to ensure postgres is running before starting the testsn"}
{"File Name":"govuk-aws\/0015-dns-infrastructure.md","Context":"## Context\n- All our instances will need to be able to resolve internal infrastructure services, such\nus Puppet, Graphite or Logstash\n- Some services and application endpoints will need to be exposed to the Internet and\nbe resolved by public DNS. For instance alerts.integration, deploy.integration, www-origin, etc\n- We want to be able to create new pieces of infrastructure alongside the current piece of infrastructure\nwith the ability to test direct access to each piece using DNS endpoints\n- We want to control which stack is running the active version of a piece of infrastructure, and control\nhow applications connect using DNS\n- We want to ensure the site and all links works correctly when users browse using the\npublishing (publishing.service.gov.uk) domain.\n","Decision":"![DNS](.\/0015-govuk-aws-dns.jpg?raw=true \"DNS Infrastructure\")\\n#### Stack domains\\nEach stack has an internal and external DNS domain. All Terraform projects in that stack add records\\nto Route53 zones to expose the service internally and\/or externally.\\nFor instance, a 'green' stack has its own `green.<internalrootdomain>` and `green.<externalrootdomain>`\\ndomain. Puppet and Icinga services in this stack will add `puppet.green.<internalrootdomain>` and\\n`alerts.green.<externalrootdomain>` to Route53.\\nThis is for an infrastructure level view only. Applications will not work correctly across independent stacks,\\nand will only correctly work using the Publishing domain.\\n### Root domain service records\\nAll services will need an entry with the root domain that points to a stack record. This entry\\ncan be updated to select the active version of each service.\\nFor instance, machines are using the Puppet service `puppet.<internalrootdomain>` that is a CNAME\\nof `puppet.green.<internalrootdomain>`. At some point, a new Puppet stack 'blue' is provisioned to\\ntest a new version, and when it has been tested we switch the CNAME to the new stack, so\\n`puppet.<internalrootdomain>` resolves to `puppet.blue.<internalrootdomain>`.\\n#### External Route53 zones\\nThere is a public (external) Route53 zone to manage the external root domain. Each stack has also its own\\nexternal Route53 zone where we delegate the stack subdomain.\\nFor instance, if we are setting up a new environment with a public root domain `test.govuk.digital`,\\nand create a new stack 'green', we'll have a zone for `test.govuk.digital` and a zone for the stack\\nsubdomain `green.test.govuk.digital`, that stores the stack records.\\n#### Internal Route53 zones\\nFor internal domains we cannot replicate the external configuration because it's not possible to do DNS\\ndelegation with internal zones in the same VPC when the domains overlap.\\nFor instance, for the previous scenario we will have an internal zone for the root domain\\n`test.govuk-internal.digital`, but we cannot create and delegate a subdomain for a zone\\n`green.test.govuk-internal.digital` in the same VPC because it overlaps the root domain.\\nIn this case, for internal domains we can only have a single zone, but we'll still keep the same\\ndomain schema and each stack will generate records appending the stack subdomain.\\n","tokens":154,"id":4056,"text":"## Context\\n- All our instances will need to be able to resolve internal infrastructure services, such\\nus Puppet, Graphite or Logstash\\n- Some services and application endpoints will need to be exposed to the Internet and\\nbe resolved by public DNS. For instance alerts.integration, deploy.integration, www-origin, etc\\n- We want to be able to create new pieces of infrastructure alongside the current piece of infrastructure\\nwith the ability to test direct access to each piece using DNS endpoints\\n- We want to control which stack is running the active version of a piece of infrastructure, and control\\nhow applications connect using DNS\\n- We want to ensure the site and all links works correctly when users browse using the\\npublishing (publishing.service.gov.uk) domain.\\n\n\n##Decision\n![DNS](.\/0015-govuk-aws-dns.jpg?raw=true \"DNS Infrastructure\")\\n#### Stack domains\\nEach stack has an internal and external DNS domain. All Terraform projects in that stack add records\\nto Route53 zones to expose the service internally and\/or externally.\\nFor instance, a 'green' stack has its own `green.<internalrootdomain>` and `green.<externalrootdomain>`\\ndomain. Puppet and Icinga services in this stack will add `puppet.green.<internalrootdomain>` and\\n`alerts.green.<externalrootdomain>` to Route53.\\nThis is for an infrastructure level view only. Applications will not work correctly across independent stacks,\\nand will only correctly work using the Publishing domain.\\n### Root domain service records\\nAll services will need an entry with the root domain that points to a stack record. This entry\\ncan be updated to select the active version of each service.\\nFor instance, machines are using the Puppet service `puppet.<internalrootdomain>` that is a CNAME\\nof `puppet.green.<internalrootdomain>`. At some point, a new Puppet stack 'blue' is provisioned to\\ntest a new version, and when it has been tested we switch the CNAME to the new stack, so\\n`puppet.<internalrootdomain>` resolves to `puppet.blue.<internalrootdomain>`.\\n#### External Route53 zones\\nThere is a public (external) Route53 zone to manage the external root domain. Each stack has also its own\\nexternal Route53 zone where we delegate the stack subdomain.\\nFor instance, if we are setting up a new environment with a public root domain `test.govuk.digital`,\\nand create a new stack 'green', we'll have a zone for `test.govuk.digital` and a zone for the stack\\nsubdomain `green.test.govuk.digital`, that stores the stack records.\\n#### Internal Route53 zones\\nFor internal domains we cannot replicate the external configuration because it's not possible to do DNS\\ndelegation with internal zones in the same VPC when the domains overlap.\\nFor instance, for the previous scenario we will have an internal zone for the root domain\\n`test.govuk-internal.digital`, but we cannot create and delegate a subdomain for a zone\\n`green.test.govuk-internal.digital` in the same VPC because it overlaps the root domain.\\nIn this case, for internal domains we can only have a single zone, but we'll still keep the same\\ndomain schema and each stack will generate records appending the stack subdomain.\\n","Predictions":"![DNS](.\/0015-govuk-aws-dns.jpg?raw=true \"DNS Infrastructure\")n"}
{"File Name":"elm-spec\/003_loading_harness.md","Context":"## Context\nRight now, in order to run an elm-spec program, we need to do a few things in a definite order.\nAt some point, compile the Elm code using the `Compiler` from elm-spec-core. This will\nwrap the compiled code so that the parts of the program that interact with the outside world\ncan be easily faked out.\nThen, to get the program running: First, create an `ElmContext` object. This creates\nall the fake objects on the `window`\nobject that the compiled elm code will attempt to reference. Second, evaluate the compiled\nelm code. It doesn't matter when we compile the elm code, of course, just that it is\nevaluated in the browser environment *after* we have created a new `ElmContext` in that\nenvironment.\nSo, it's a little wild, I guess, that simply instantiating an `ElmContext` modifies the `window`\nobject and so on.\nPart of the need for this comes from the fact that the compiled Elm code is wrapped in an IFFE.\nBut there's no reason why we actually have to do that ...\nWe've been able to deal with this problem so far because the only things that need to go\nthrough this process are elm-spec-runner and karma-elm-spec-framework. But with the harness,\nwe are now asking a test writer to follow this process as well. For that reason, we need to\nsimplify it so it's not a source of errors.\n","Decision":"We should change this flow so that we don't need to create an `ElmContext` and evaluate the\\ncompiled Elm code in a particular order.\\nFirst, we will wrap the compiled Elm code in a function that takes an `ElmContext`. Evaluating\\nthis code will still attach the `Elm` object to the window (since we're providing it with a\\nproxy object). But by using a normal function here, we have more control over when the `Elm`\\nobject is loaded.\\nThen, we need to have `ElmContext` store references to all the fakes inside itself -- there's no\\nreal need to store these on the `window` object. So the only things we need to store on the `window`\\nare (1) the function to load Elm -- because this is how the compiled Elm code provides the function;\\nI don't think we can reference it any other way. (2) Data on what the compiler actually\\ntried to do, like what files it tried to compile, the path and all that; we store this on the window\\nso it's available as soon as the compiled code is evaluated -- there may be better ways to do this.\\nAnd (3) the function for loading files, which has to be a function accessible on the window\\nanyway (since that's how Playwright and JSDOM allow us to register a function to be executed in Node).\\nOnce we do this, then it turns out that the test writer doesn't need to create an `ElmContext`\\nat all ... we can have `HarnessController` do that when `prepareHarness` is called. In fact,\\nwe don't even need to bundle anything extra. This will be included in the bundle of tests that\\nget executed in the browser.\\nWe could also change SuiteRunner to create it's own `ElmContext` as well, but it's not necessary.\\n","tokens":318,"id":4592,"text":"## Context\\nRight now, in order to run an elm-spec program, we need to do a few things in a definite order.\\nAt some point, compile the Elm code using the `Compiler` from elm-spec-core. This will\\nwrap the compiled code so that the parts of the program that interact with the outside world\\ncan be easily faked out.\\nThen, to get the program running: First, create an `ElmContext` object. This creates\\nall the fake objects on the `window`\\nobject that the compiled elm code will attempt to reference. Second, evaluate the compiled\\nelm code. It doesn't matter when we compile the elm code, of course, just that it is\\nevaluated in the browser environment *after* we have created a new `ElmContext` in that\\nenvironment.\\nSo, it's a little wild, I guess, that simply instantiating an `ElmContext` modifies the `window`\\nobject and so on.\\nPart of the need for this comes from the fact that the compiled Elm code is wrapped in an IFFE.\\nBut there's no reason why we actually have to do that ...\\nWe've been able to deal with this problem so far because the only things that need to go\\nthrough this process are elm-spec-runner and karma-elm-spec-framework. But with the harness,\\nwe are now asking a test writer to follow this process as well. For that reason, we need to\\nsimplify it so it's not a source of errors.\\n\n\n##Decision\nWe should change this flow so that we don't need to create an `ElmContext` and evaluate the\\ncompiled Elm code in a particular order.\\nFirst, we will wrap the compiled Elm code in a function that takes an `ElmContext`. Evaluating\\nthis code will still attach the `Elm` object to the window (since we're providing it with a\\nproxy object). But by using a normal function here, we have more control over when the `Elm`\\nobject is loaded.\\nThen, we need to have `ElmContext` store references to all the fakes inside itself -- there's no\\nreal need to store these on the `window` object. So the only things we need to store on the `window`\\nare (1) the function to load Elm -- because this is how the compiled Elm code provides the function;\\nI don't think we can reference it any other way. (2) Data on what the compiler actually\\ntried to do, like what files it tried to compile, the path and all that; we store this on the window\\nso it's available as soon as the compiled code is evaluated -- there may be better ways to do this.\\nAnd (3) the function for loading files, which has to be a function accessible on the window\\nanyway (since that's how Playwright and JSDOM allow us to register a function to be executed in Node).\\nOnce we do this, then it turns out that the test writer doesn't need to create an `ElmContext`\\nat all ... we can have `HarnessController` do that when `prepareHarness` is called. In fact,\\nwe don't even need to bundle anything extra. This will be included in the bundle of tests that\\nget executed in the browser.\\nWe could also change SuiteRunner to create it's own `ElmContext` as well, but it's not necessary.\\n","Predictions":"We should change this flow so that we don't need to create an ElmContext and evaluate thencompiled Elm code in a particular order.nFirst, we will wrap the compiled Elm code in a function that takes an ElmContext. Evaluatingnthis code will still attach the Elm object to the window (since we're providing it with anElmContext). Evaluatingnthis code will still attach the Elm object to the window (since we're providing it with anElmContext).n"}
{"File Name":"devops-challenge\/0010-feat-ci-cd-with-circleci.md","Context":"## Context\nTime to work on the CI\/CD solution.\nI need a job manager to build, test and deploy the apps to the kubernetes\ncluster.\nI know there is a lot of documentation about jenkins, but I have been working\nin the last years with other solutions like:\n* SolanoCI (now closed)\n* CircleCI\nSo I need to see if there is a way to use my knowledge in CircleCI or not.\nBesides, CircleCI has a free plan very useful for testing.\nI know there is a jenkins-x product, but I don't have a clear idea about\nit.  I installed it and spent couple of hours to make it work (jx) but I got\nan error downloading kops, and I couldn't go further.\n","Decision":"Try first with CircleCI to see if I can deploy the apps to kubernetes.\\nAs I'm using a monorepo, all the apps are within the same repository, so we\\ncannot separate the building process of each microservice.  All them will be\\nbuild and deployed as one.\\nBut if nothing has change in the app, then the building process will be faster.\\nTesting will be peformed on all apps secuentially, and some tests could be\\nperformed on all services without implementing mockups.\\nThis way, when a event is received by the CI, a script will be executed to\\nexecute the actions on each app.\\n","tokens":168,"id":3482,"text":"## Context\\nTime to work on the CI\/CD solution.\\nI need a job manager to build, test and deploy the apps to the kubernetes\\ncluster.\\nI know there is a lot of documentation about jenkins, but I have been working\\nin the last years with other solutions like:\\n* SolanoCI (now closed)\\n* CircleCI\\nSo I need to see if there is a way to use my knowledge in CircleCI or not.\\nBesides, CircleCI has a free plan very useful for testing.\\nI know there is a jenkins-x product, but I don't have a clear idea about\\nit.  I installed it and spent couple of hours to make it work (jx) but I got\\nan error downloading kops, and I couldn't go further.\\n\n\n##Decision\nTry first with CircleCI to see if I can deploy the apps to kubernetes.\\nAs I'm using a monorepo, all the apps are within the same repository, so we\\ncannot separate the building process of each microservice.  All them will be\\nbuild and deployed as one.\\nBut if nothing has change in the app, then the building process will be faster.\\nTesting will be peformed on all apps secuentially, and some tests could be\\nperformed on all services without implementing mockups.\\nThis way, when a event is received by the CI, a script will be executed to\\nexecute the actions on each app.\\n","Predictions":"I will use CircleCI to build the apps to kubernetes.nAs I'm using a monorepo, all the apps are within the same repository, so we can not separate the building process of each microservice. All them will be build and deployed as one.nBut if nothing has change in the app, then the building process will be faster.nTesting will be peformed on all apps sequentially, and some tests could benperformed on all services without implementing mockups.nThis way, when a event is received by the CI, a script will be executed tonexecute the actions on each app.n"}
{"File Name":"lbh-frontend-react\/0005-use-eslint.md","Context":"## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\nJavaScript, and has good support for TypeScript though plugins.\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":69,"id":1404,"text":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n\n\n##Decision\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","Predictions":"We will use serialize to serialize models.nWe will use serialize to serialize models.n"}
{"File Name":"titania-os\/0002-docker-for-dapps.md","Context":"## Context\nTitania is supposed to run multiple dApps. There are multiple techologies and concepts on how to define and isolate a dApp.\n","Decision":"We will use LXC technology and represent a dApp as a container. We will use Docker to build, run and manage said containers.\\n","tokens":31,"id":2573,"text":"## Context\\nTitania is supposed to run multiple dApps. There are multiple techologies and concepts on how to define and isolate a dApp.\\n\n\n##Decision\nWe will use LXC technology and represent a dApp as a container. We will use Docker to build, run and manage said containers.\\n","Predictions":"We will use Docker to build, run and manage the dApps.n"}
{"File Name":"ibc-go\/adr-015-ibc-packet-receiver.md","Context":"## Context\n[ICS 26 - Routing Module](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module) defines a function [`handlePacketRecv`](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module#packet-relay).\nIn ICS 26, the routing module is defined as a layer above each application module\nwhich verifies and routes messages to the destination modules. It is possible to\nimplement it as a separate module, however, we already have functionality to route\nmessages upon the destination identifiers in the baseapp. This ADR suggests\nto utilize existing `baseapp.router` to route packets to application modules.\nGenerally, routing module callbacks have two separate steps in them,\nverification and execution. This corresponds to the `AnteHandler`-`Handler`\nmodel inside the SDK. We can do the verification inside the `AnteHandler`\nin order to increase developer ergonomics by reducing boilerplate\nverification code.\nFor atomic multi-message transaction, we want to keep the IBC related\nstate modification to be preserved even the application side state change\nreverts. One of the example might be IBC token sending message following with\nstake delegation which uses the tokens received by the previous packet message.\nIf the token receiving fails for any reason, we might not want to keep\nexecuting the transaction, but we also don't want to abort the transaction\nor the sequence and commitment will be reverted and the channel will be stuck.\nThis ADR suggests new `CodeType`, `CodeTxBreak`, to fix this problem.\n","Decision":"`PortKeeper` will have the capability key that is able to access only the\\nchannels bound to the port. Entities that hold a `PortKeeper` will be\\nable to call the methods on it which are corresponding with the methods with\\nthe same names on the `ChannelKeeper`, but only with the\\nallowed port. `ChannelKeeper.Port(string, ChannelChecker)` will be defined to\\neasily construct a capability-safe `PortKeeper`. This will be addressed in\\nanother ADR and we will use insecure `ChannelKeeper` for now.\\n`baseapp.runMsgs` will break the loop over the messages if one of the handlers\\nreturns `!Result.IsOK()`. However, the outer logic will write the cached\\nstore if `Result.IsOK() || Result.Code.IsBreak()`. `Result.Code.IsBreak()` if\\n`Result.Code == CodeTxBreak`.\\n```go\\nfunc (app *BaseApp) runTx(tx Tx) (result Result) {\\nmsgs := tx.GetMsgs()\\n\/\/ AnteHandler\\nif app.anteHandler != nil {\\nanteCtx, msCache := app.cacheTxContext(ctx)\\nnewCtx, err := app.anteHandler(anteCtx, tx)\\nif !newCtx.IsZero() {\\nctx = newCtx.WithMultiStore(ms)\\n}\\nif err != nil {\\n\/\/ error handling logic\\nreturn res\\n}\\nmsCache.Write()\\n}\\n\/\/ Main Handler\\nrunMsgCtx, msCache := app.cacheTxContext(ctx)\\nresult = app.runMsgs(runMsgCtx, msgs)\\n\/\/ BEGIN modification made in this ADR\\nif result.IsOK() || result.IsBreak() {\\n\/\/ END\\nmsCache.Write()\\n}\\nreturn result\\n}\\n```\\nThe Cosmos SDK will define an `AnteDecorator` for IBC packet receiving. The\\n`AnteDecorator` will iterate over the messages included in the transaction, type\\n`switch` to check whether the message contains an incoming IBC packet, and if so\\nverify the Merkle proof.\\n```go\\ntype ProofVerificationDecorator struct {\\nclientKeeper ClientKeeper\\nchannelKeeper ChannelKeeper\\n}\\nfunc (pvr ProofVerificationDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (Context, error) {\\nfor _, msg := range tx.GetMsgs() {\\nvar err error\\nswitch msg := msg.(type) {\\ncase client.MsgUpdateClient:\\nerr = pvr.clientKeeper.UpdateClient(msg.ClientID, msg.Header)\\ncase channel.MsgPacket:\\nerr = pvr.channelKeeper.RecvPacket(msg.Packet, msg.Proofs, msg.ProofHeight)\\ncase channel.MsgAcknowledgement:\\nerr = pvr.channelKeeper.AcknowledgementPacket(msg.Acknowledgement, msg.Proof, msg.ProofHeight)\\ncase channel.MsgTimeoutPacket:\\nerr = pvr.channelKeeper.TimeoutPacket(msg.Packet, msg.Proof, msg.ProofHeight, msg.NextSequenceRecv)\\ncase channel.MsgChannelOpenInit;\\nerr = pvr.channelKeeper.CheckOpen(msg.PortID, msg.ChannelID, msg.Channel)\\ndefault:\\ncontinue\\n}\\nif err != nil {\\nreturn ctx, err\\n}\\n}\\nreturn next(ctx, tx, simulate)\\n}\\n```\\nWhere `MsgUpdateClient`, `MsgPacket`, `MsgAcknowledgement`, `MsgTimeoutPacket`\\nare `sdk.Msg` types correspond to `handleUpdateClient`, `handleRecvPacket`,\\n`handleAcknowledgementPacket`, `handleTimeoutPacket` of the routing module,\\nrespectively.\\nThe side effects of `RecvPacket`, `VerifyAcknowledgement`,\\n`VerifyTimeout` will be extracted out into separated functions,\\n`WriteAcknowledgement`, `DeleteCommitment`, `DeleteCommitmentTimeout`, respectively,\\nwhich will be called by the application handlers after the execution.\\n`WriteAcknowledgement` writes the acknowledgement to the state that can be\\nverified by the counter-party chain and increments the sequence to prevent\\ndouble execution. `DeleteCommitment` will delete the commitment stored,\\n`DeleteCommitmentTimeout` will delete the commitment and close channel in case\\nof ordered channel.\\n```go\\nfunc (keeper ChannelKeeper) WriteAcknowledgement(ctx Context, packet Packet, ack []byte) {\\nkeeper.SetPacketAcknowledgement(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence(), ack)\\nkeeper.SetNextSequenceRecv(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitment(ctx Context, packet Packet) {\\nkeeper.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitmentTimeout(ctx Context, packet Packet) {\\nk.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\nif channel.Ordering == types.ORDERED [\\nchannel.State = types.CLOSED\\nk.SetChannel(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), channel)\\n}\\n}\\n```\\nEach application handler should call respective finalization methods on the `PortKeeper`\\nin order to increase sequence (in case of packet) or remove the commitment\\n(in case of acknowledgement and timeout).\\nCalling those functions implies that the application logic has successfully executed.\\nHowever, the handlers can return `Result` with `CodeTxBreak` after calling those methods\\nwhich will persist the state changes that has been already done but prevent any further\\nmessages to be executed in case of semantically invalid packet. This will keep the sequence\\nincreased in the previous IBC packets(thus preventing double execution) without\\nproceeding to the following messages.\\nIn any case the application modules should never return state reverting result,\\nwhich will make the channel unable to proceed.\\n`ChannelKeeper.CheckOpen` method will be introduced. This will replace `onChanOpen*` defined\\nunder the routing module specification. Instead of define each channel handshake callback\\nfunctions, application modules can provide `ChannelChecker` function with the `AppModule`\\nwhich will be injected to `ChannelKeeper.Port()` at the top level application.\\n`CheckOpen` will find the correct `ChennelChecker` using the\\n`PortID` and call it, which will return an error if it is unacceptable by the application.\\nThe `ProofVerificationDecorator` will be inserted to the top level application.\\nIt is not safe to make each module responsible to call proof verification\\nlogic, whereas application can misbehave(in terms of IBC protocol) by\\nmistake.\\nThe `ProofVerificationDecorator` should come right after the default sybil attack\\nresistant layer from the current `auth.NewAnteHandler`:\\n```go\\n\/\/ add IBC ProofVerificationDecorator to the Chain of\\nfunc NewAnteHandler(\\nak keeper.AccountKeeper, supplyKeeper types.SupplyKeeper, ibcKeeper ibc.Keeper,\\nsigGasConsumer SignatureVerificationGasConsumer) sdk.AnteHandler {\\nreturn sdk.ChainAnteDecorators(\\nNewSetUpContextDecorator(), \/\/ outermost AnteDecorator. SetUpContext must be called first\\n...\\nNewIncrementSequenceDecorator(ak),\\nibcante.ProofVerificationDecorator(ibcKeeper.ClientKeeper, ibcKeeper.ChannelKeeper), \/\/ innermost AnteDecorator\\n)\\n}\\n```\\nThe implementation of this ADR will also create a `Data` field of the `Packet` of type `[]byte`, which can be deserialised by the receiving module into its own private type. It is up to the application modules to do this according to their own interpretation, not by the IBC keeper.  This is crucial for dynamic IBC.\\nExample application-side usage:\\n```go\\ntype AppModule struct {}\\n\/\/ CheckChannel will be provided to the ChannelKeeper as ChannelKeeper.Port(module.CheckChannel)\\nfunc (module AppModule) CheckChannel(portID, channelID string, channel Channel) error {\\nif channel.Ordering != UNORDERED {\\nreturn ErrUncompatibleOrdering()\\n}\\nif channel.CounterpartyPort != \"bank\" {\\nreturn ErrUncompatiblePort()\\n}\\nif channel.Version != \"\" {\\nreturn ErrUncompatibleVersion()\\n}\\nreturn nil\\n}\\nfunc NewHandler(k Keeper) Handler {\\nreturn func(ctx Context, msg Msg) Result {\\nswitch msg := msg.(type) {\\ncase MsgTransfer:\\nreturn handleMsgTransfer(ctx, k, msg)\\ncase ibc.MsgPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handlePacketDataTransfer(ctx, k, msg, data)\\ncase ibc.MsgTimeoutPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handleTimeoutPacketDataTransfer(ctx, k, packet)\\n\/\/ interface { PortID() string; ChannelID() string; Channel() ibc.Channel }\\n\/\/ MsgChanInit, MsgChanTry implements ibc.MsgChannelOpen\\ncase ibc.MsgChannelOpen:\\nreturn handleMsgChannelOpen(ctx, k, msg)\\n}\\n}\\n}\\nfunc handleMsgTransfer(ctx Context, k Keeper, msg MsgTransfer) Result {\\nerr := k.SendTransfer(ctx,msg.PortID, msg.ChannelID, msg.Amount, msg.Sender, msg.Receiver)\\nif err != nil {\\nreturn sdk.ResultFromError(err)\\n}\\nreturn sdk.Result{}\\n}\\nfunc handlePacketDataTransfer(ctx Context, k Keeper, packet Packet, data PacketDataTransfer) Result {\\nerr := k.ReceiveTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ TODO: Source chain sent invalid packet, shutdown channel\\n}\\nk.ChannelKeeper.WriteAcknowledgement([]byte{0x00}) \/\/ WriteAcknowledgement increases the sequence, preventing double spending\\nreturn sdk.Result{}\\n}\\nfunc handleCustomTimeoutPacket(ctx Context, k Keeper, packet CustomPacket) Result {\\nerr := k.RecoverTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ This chain sent invalid packet or cannot recover the funds\\npanic(err)\\n}\\nk.ChannelKeeper.DeleteCommitmentTimeout(ctx, packet)\\n\/\/ packet timeout should not fail\\nreturn sdk.Result{}\\n}\\nfunc handleMsgChannelOpen(sdk.Context, k Keeper, msg MsgOpenChannel) Result {\\nk.AllocateEscrowAddress(ctx, msg.ChannelID())\\nreturn sdk.Result{}\\n}\\n```\\n","tokens":341,"id":1442,"text":"## Context\\n[ICS 26 - Routing Module](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module) defines a function [`handlePacketRecv`](https:\/\/github.com\/cosmos\/ibc\/tree\/master\/spec\/core\/ics-026-routing-module#packet-relay).\\nIn ICS 26, the routing module is defined as a layer above each application module\\nwhich verifies and routes messages to the destination modules. It is possible to\\nimplement it as a separate module, however, we already have functionality to route\\nmessages upon the destination identifiers in the baseapp. This ADR suggests\\nto utilize existing `baseapp.router` to route packets to application modules.\\nGenerally, routing module callbacks have two separate steps in them,\\nverification and execution. This corresponds to the `AnteHandler`-`Handler`\\nmodel inside the SDK. We can do the verification inside the `AnteHandler`\\nin order to increase developer ergonomics by reducing boilerplate\\nverification code.\\nFor atomic multi-message transaction, we want to keep the IBC related\\nstate modification to be preserved even the application side state change\\nreverts. One of the example might be IBC token sending message following with\\nstake delegation which uses the tokens received by the previous packet message.\\nIf the token receiving fails for any reason, we might not want to keep\\nexecuting the transaction, but we also don't want to abort the transaction\\nor the sequence and commitment will be reverted and the channel will be stuck.\\nThis ADR suggests new `CodeType`, `CodeTxBreak`, to fix this problem.\\n\n\n##Decision\n`PortKeeper` will have the capability key that is able to access only the\\nchannels bound to the port. Entities that hold a `PortKeeper` will be\\nable to call the methods on it which are corresponding with the methods with\\nthe same names on the `ChannelKeeper`, but only with the\\nallowed port. `ChannelKeeper.Port(string, ChannelChecker)` will be defined to\\neasily construct a capability-safe `PortKeeper`. This will be addressed in\\nanother ADR and we will use insecure `ChannelKeeper` for now.\\n`baseapp.runMsgs` will break the loop over the messages if one of the handlers\\nreturns `!Result.IsOK()`. However, the outer logic will write the cached\\nstore if `Result.IsOK() || Result.Code.IsBreak()`. `Result.Code.IsBreak()` if\\n`Result.Code == CodeTxBreak`.\\n```go\\nfunc (app *BaseApp) runTx(tx Tx) (result Result) {\\nmsgs := tx.GetMsgs()\\n\/\/ AnteHandler\\nif app.anteHandler != nil {\\nanteCtx, msCache := app.cacheTxContext(ctx)\\nnewCtx, err := app.anteHandler(anteCtx, tx)\\nif !newCtx.IsZero() {\\nctx = newCtx.WithMultiStore(ms)\\n}\\nif err != nil {\\n\/\/ error handling logic\\nreturn res\\n}\\nmsCache.Write()\\n}\\n\/\/ Main Handler\\nrunMsgCtx, msCache := app.cacheTxContext(ctx)\\nresult = app.runMsgs(runMsgCtx, msgs)\\n\/\/ BEGIN modification made in this ADR\\nif result.IsOK() || result.IsBreak() {\\n\/\/ END\\nmsCache.Write()\\n}\\nreturn result\\n}\\n```\\nThe Cosmos SDK will define an `AnteDecorator` for IBC packet receiving. The\\n`AnteDecorator` will iterate over the messages included in the transaction, type\\n`switch` to check whether the message contains an incoming IBC packet, and if so\\nverify the Merkle proof.\\n```go\\ntype ProofVerificationDecorator struct {\\nclientKeeper ClientKeeper\\nchannelKeeper ChannelKeeper\\n}\\nfunc (pvr ProofVerificationDecorator) AnteHandle(ctx Context, tx Tx, simulate bool, next AnteHandler) (Context, error) {\\nfor _, msg := range tx.GetMsgs() {\\nvar err error\\nswitch msg := msg.(type) {\\ncase client.MsgUpdateClient:\\nerr = pvr.clientKeeper.UpdateClient(msg.ClientID, msg.Header)\\ncase channel.MsgPacket:\\nerr = pvr.channelKeeper.RecvPacket(msg.Packet, msg.Proofs, msg.ProofHeight)\\ncase channel.MsgAcknowledgement:\\nerr = pvr.channelKeeper.AcknowledgementPacket(msg.Acknowledgement, msg.Proof, msg.ProofHeight)\\ncase channel.MsgTimeoutPacket:\\nerr = pvr.channelKeeper.TimeoutPacket(msg.Packet, msg.Proof, msg.ProofHeight, msg.NextSequenceRecv)\\ncase channel.MsgChannelOpenInit;\\nerr = pvr.channelKeeper.CheckOpen(msg.PortID, msg.ChannelID, msg.Channel)\\ndefault:\\ncontinue\\n}\\nif err != nil {\\nreturn ctx, err\\n}\\n}\\nreturn next(ctx, tx, simulate)\\n}\\n```\\nWhere `MsgUpdateClient`, `MsgPacket`, `MsgAcknowledgement`, `MsgTimeoutPacket`\\nare `sdk.Msg` types correspond to `handleUpdateClient`, `handleRecvPacket`,\\n`handleAcknowledgementPacket`, `handleTimeoutPacket` of the routing module,\\nrespectively.\\nThe side effects of `RecvPacket`, `VerifyAcknowledgement`,\\n`VerifyTimeout` will be extracted out into separated functions,\\n`WriteAcknowledgement`, `DeleteCommitment`, `DeleteCommitmentTimeout`, respectively,\\nwhich will be called by the application handlers after the execution.\\n`WriteAcknowledgement` writes the acknowledgement to the state that can be\\nverified by the counter-party chain and increments the sequence to prevent\\ndouble execution. `DeleteCommitment` will delete the commitment stored,\\n`DeleteCommitmentTimeout` will delete the commitment and close channel in case\\nof ordered channel.\\n```go\\nfunc (keeper ChannelKeeper) WriteAcknowledgement(ctx Context, packet Packet, ack []byte) {\\nkeeper.SetPacketAcknowledgement(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence(), ack)\\nkeeper.SetNextSequenceRecv(ctx, packet.GetDestPort(), packet.GetDestChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitment(ctx Context, packet Packet) {\\nkeeper.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\n}\\nfunc (keeper ChannelKeeper) DeleteCommitmentTimeout(ctx Context, packet Packet) {\\nk.deletePacketCommitment(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetSequence())\\nif channel.Ordering == types.ORDERED [\\nchannel.State = types.CLOSED\\nk.SetChannel(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), channel)\\n}\\n}\\n```\\nEach application handler should call respective finalization methods on the `PortKeeper`\\nin order to increase sequence (in case of packet) or remove the commitment\\n(in case of acknowledgement and timeout).\\nCalling those functions implies that the application logic has successfully executed.\\nHowever, the handlers can return `Result` with `CodeTxBreak` after calling those methods\\nwhich will persist the state changes that has been already done but prevent any further\\nmessages to be executed in case of semantically invalid packet. This will keep the sequence\\nincreased in the previous IBC packets(thus preventing double execution) without\\nproceeding to the following messages.\\nIn any case the application modules should never return state reverting result,\\nwhich will make the channel unable to proceed.\\n`ChannelKeeper.CheckOpen` method will be introduced. This will replace `onChanOpen*` defined\\nunder the routing module specification. Instead of define each channel handshake callback\\nfunctions, application modules can provide `ChannelChecker` function with the `AppModule`\\nwhich will be injected to `ChannelKeeper.Port()` at the top level application.\\n`CheckOpen` will find the correct `ChennelChecker` using the\\n`PortID` and call it, which will return an error if it is unacceptable by the application.\\nThe `ProofVerificationDecorator` will be inserted to the top level application.\\nIt is not safe to make each module responsible to call proof verification\\nlogic, whereas application can misbehave(in terms of IBC protocol) by\\nmistake.\\nThe `ProofVerificationDecorator` should come right after the default sybil attack\\nresistant layer from the current `auth.NewAnteHandler`:\\n```go\\n\/\/ add IBC ProofVerificationDecorator to the Chain of\\nfunc NewAnteHandler(\\nak keeper.AccountKeeper, supplyKeeper types.SupplyKeeper, ibcKeeper ibc.Keeper,\\nsigGasConsumer SignatureVerificationGasConsumer) sdk.AnteHandler {\\nreturn sdk.ChainAnteDecorators(\\nNewSetUpContextDecorator(), \/\/ outermost AnteDecorator. SetUpContext must be called first\\n...\\nNewIncrementSequenceDecorator(ak),\\nibcante.ProofVerificationDecorator(ibcKeeper.ClientKeeper, ibcKeeper.ChannelKeeper), \/\/ innermost AnteDecorator\\n)\\n}\\n```\\nThe implementation of this ADR will also create a `Data` field of the `Packet` of type `[]byte`, which can be deserialised by the receiving module into its own private type. It is up to the application modules to do this according to their own interpretation, not by the IBC keeper.  This is crucial for dynamic IBC.\\nExample application-side usage:\\n```go\\ntype AppModule struct {}\\n\/\/ CheckChannel will be provided to the ChannelKeeper as ChannelKeeper.Port(module.CheckChannel)\\nfunc (module AppModule) CheckChannel(portID, channelID string, channel Channel) error {\\nif channel.Ordering != UNORDERED {\\nreturn ErrUncompatibleOrdering()\\n}\\nif channel.CounterpartyPort != \"bank\" {\\nreturn ErrUncompatiblePort()\\n}\\nif channel.Version != \"\" {\\nreturn ErrUncompatibleVersion()\\n}\\nreturn nil\\n}\\nfunc NewHandler(k Keeper) Handler {\\nreturn func(ctx Context, msg Msg) Result {\\nswitch msg := msg.(type) {\\ncase MsgTransfer:\\nreturn handleMsgTransfer(ctx, k, msg)\\ncase ibc.MsgPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handlePacketDataTransfer(ctx, k, msg, data)\\ncase ibc.MsgTimeoutPacket:\\nvar data PacketDataTransfer\\nif err := types.ModuleCodec.UnmarshalBinaryBare(msg.GetData(), &data); err != nil {\\nreturn err\\n}\\nreturn handleTimeoutPacketDataTransfer(ctx, k, packet)\\n\/\/ interface { PortID() string; ChannelID() string; Channel() ibc.Channel }\\n\/\/ MsgChanInit, MsgChanTry implements ibc.MsgChannelOpen\\ncase ibc.MsgChannelOpen:\\nreturn handleMsgChannelOpen(ctx, k, msg)\\n}\\n}\\n}\\nfunc handleMsgTransfer(ctx Context, k Keeper, msg MsgTransfer) Result {\\nerr := k.SendTransfer(ctx,msg.PortID, msg.ChannelID, msg.Amount, msg.Sender, msg.Receiver)\\nif err != nil {\\nreturn sdk.ResultFromError(err)\\n}\\nreturn sdk.Result{}\\n}\\nfunc handlePacketDataTransfer(ctx Context, k Keeper, packet Packet, data PacketDataTransfer) Result {\\nerr := k.ReceiveTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ TODO: Source chain sent invalid packet, shutdown channel\\n}\\nk.ChannelKeeper.WriteAcknowledgement([]byte{0x00}) \/\/ WriteAcknowledgement increases the sequence, preventing double spending\\nreturn sdk.Result{}\\n}\\nfunc handleCustomTimeoutPacket(ctx Context, k Keeper, packet CustomPacket) Result {\\nerr := k.RecoverTransfer(ctx, packet.GetSourcePort(), packet.GetSourceChannel(), packet.GetDestinationPort(), packet.GetDestinationChannel(), data)\\nif err != nil {\\n\/\/ This chain sent invalid packet or cannot recover the funds\\npanic(err)\\n}\\nk.ChannelKeeper.DeleteCommitmentTimeout(ctx, packet)\\n\/\/ packet timeout should not fail\\nreturn sdk.Result{}\\n}\\nfunc handleMsgChannelOpen(sdk.Context, k Keeper, msg MsgOpenChannel) Result {\\nk.AllocateEscrowAddress(ctx, msg.ChannelID())\\nreturn sdk.Result{}\\n}\\n```\\n","Predictions":"We will use the Security method to store the Security tokens.n"}
{"File Name":"james\/0040-quality-levels-definitions.md","Context":"## Context\nWe hereby define as an artifact compiled artifact that external people consumes. This includes:\n- libraries\n- Mail servers\n- Extensions for James Mail Servers\n- Command line tools\nWe designate as a feature an optional, opt-in behaviour of a James server that can be configured by\nuser willing to rely on it.\nJames as a project delivers several artifacts, and features. In order for project users to better\nunderstand the underlying quality of the artifact they use, as well as the level of risk associated,\nwe need to better define some quality levels.\n","Decision":"For a given artifact or feature, by **mature** we mean that:\\n- *interfaces* in components need a contract test suite\\n- *interfaces* have several implementations\\n- *implementation* of these interfaces need to pass this contract test suite which provides unit tests\\n- Decent integration tests coverage is needed\\n- Performance tests need to be conducted out\\n- Quality Assurance with external clients needs to be conducted out\\n- known existing production deployments\/usages\\n- usable documentation\\nThis is the maximum quality level delivered by the James project. Users should feel confident using these\\nartifacts or features.\\nBy **experimental** we designate an artifact or feature not matching yet the above requirements. However some\\nactive contributors are willing to raise the quality level of this component, and eventually make it\\nmature. Or at least are willing to support users.\\nUsers should have low expectations regarding experimental artifacts or features. They are encouraged to contribute to them\\nin order to raise its quality.\\nBy **unsupported** we mean that an artifact or feature do not match most of the *mature* quality conditions. Active\\ncontributors do not feel confident delivering support for it. This artifact or feature might be deprecated and\\nremoved from future James releases. Users are strongly encouraged to contribute to the artifact development.\\n","tokens":118,"id":2150,"text":"## Context\\nWe hereby define as an artifact compiled artifact that external people consumes. This includes:\\n- libraries\\n- Mail servers\\n- Extensions for James Mail Servers\\n- Command line tools\\nWe designate as a feature an optional, opt-in behaviour of a James server that can be configured by\\nuser willing to rely on it.\\nJames as a project delivers several artifacts, and features. In order for project users to better\\nunderstand the underlying quality of the artifact they use, as well as the level of risk associated,\\nwe need to better define some quality levels.\\n\n\n##Decision\nFor a given artifact or feature, by **mature** we mean that:\\n- *interfaces* in components need a contract test suite\\n- *interfaces* have several implementations\\n- *implementation* of these interfaces need to pass this contract test suite which provides unit tests\\n- Decent integration tests coverage is needed\\n- Performance tests need to be conducted out\\n- Quality Assurance with external clients needs to be conducted out\\n- known existing production deployments\/usages\\n- usable documentation\\nThis is the maximum quality level delivered by the James project. Users should feel confident using these\\nartifacts or features.\\nBy **experimental** we designate an artifact or feature not matching yet the above requirements. However some\\nactive contributors are willing to raise the quality level of this component, and eventually make it\\nmature. Or at least are willing to support users.\\nUsers should have low expectations regarding experimental artifacts or features. They are encouraged to contribute to them\\nin order to raise its quality.\\nBy **unsupported** we mean that an artifact or feature do not match most of the *mature* quality conditions. Active\\ncontributors do not feel confident delivering support for it. This artifact or feature might be deprecated and\\nremoved from future James releases. Users are strongly encouraged to contribute to the artifact development.\\n","Predictions":"We will use the following methods to define the tenancy service:n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service**: service is a service that is used by the client to provide services to the user.n- **service"}
{"File Name":"functionaut\/0002-use-google-closure-compiler.md","Context":"## Context\nWe must allow developers to use new JavaScript syntax and features without excluding older execution environments. Code must be automatically checked against common development mistakes and optimised for download and execution.\n","Decision":"We acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.\\n","tokens":38,"id":2687,"text":"## Context\\nWe must allow developers to use new JavaScript syntax and features without excluding older execution environments. Code must be automatically checked against common development mistakes and optimised for download and execution.\\n\n\n##Decision\nWe acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.\\n","Predictions":"We acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.n"}
{"File Name":"unfinished-design-system\/002-unit-testing-tools.md","Context":"* [Context](#context)\n* [Decision](#decision)\n* [Status](#status)\n* [Consequences](#consequences)\n* [More reading](#more-reading)\n* [Updates](#updates)\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use Jest with Enzyme.\\nJest is an incredible unit testing tool for Javascript. It is easy to use, has a great syntax, and our team is familiar with it.\\nEnzyme, on the other hand, is a unit testing tool for React components. It has some libraries that help us debugging and testing those resources.\\nWe've decided on this stack because our team has already used it in the past, and it can improve our development speed.\\n","tokens":53,"id":4547,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use Jest with Enzyme.\\nJest is an incredible unit testing tool for Javascript. It is easy to use, has a great syntax, and our team is familiar with it.\\nEnzyme, on the other hand, is a unit testing tool for React components. It has some libraries that help us debugging and testing those resources.\\nWe've decided on this stack because our team has already used it in the past, and it can improve our development speed.\\n","Predictions":"* [Status](#status)n* [Consequences](#consequences)n* [More reading](#more-reading)n* [Updates](#updates)nWe are going to use Behavior-Driven Development to help us reduce that gap. BDD is a technique where we use User Stories to create acceptance tests based on user behaviors.nInstead of writing complex test statements, we use a single-choice approach to test the application.n"}
{"File Name":"libelektra\/warning_array.md","Context":"## Problem\nCurrently multiple warnings are saved in an elektra non-conforming array\nnotation which is limited to 100 entries. The notation of `#00` is against\nthe design [decision made](array.md).\n","Decision":"The format should be aligned with the correct array notation,\\nstarting with `#0`. The maximum number of warnings will stay at\\n100 entries (`#0` - `#_99`).\\n- [Array](array.md)\\n","tokens":46,"id":1303,"text":"## Problem\\nCurrently multiple warnings are saved in an elektra non-conforming array\\nnotation which is limited to 100 entries. The notation of `#00` is against\\nthe design [decision made](array.md).\\n\n\n##Decision\nThe format should be aligned with the correct array notation,\\nstarting with `#0`. The maximum number of warnings will stay at\\n100 entries (`#0` - `#_99`).\\n- [Array](array.md)\\n","Predictions":"The format should be aligned with the correct array notation,nstarting with #0. The maximum number of warnings will stay atn100 entries (#0 - #_99).n- [Array](array.md)n"}
{"File Name":"amf\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nSome other useful links:\\n[https:\/\/github.blog\/2020-08-13-why-write-adrs\/](https:\/\/github.blog\/2020-08-13-why-write-adrs\/)\\n[https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/](https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/)\\n","tokens":16,"id":1397,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nSome other useful links:\\n[https:\/\/github.blog\/2020-08-13-why-write-adrs\/](https:\/\/github.blog\/2020-08-13-why-write-adrs\/)\\n[https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/](https:\/\/understandlegacycode.com\/blog\/earn-maintainers-esteem-with-adrs\/)\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"dotfiles\/0003-switch-to-use-zsh-in-place-of-bash.md","Context":"## Context\n[Starting with macOS Catalina, zsh will be the default shell](https:\/\/support.apple.com\/en-us\/HT208050).\n","Decision":"zsh should be used in place of bash.\\n","tokens":31,"id":115,"text":"## Context\\n[Starting with macOS Catalina, zsh will be the default shell](https:\/\/support.apple.com\/en-us\/HT208050).\\n\n\n##Decision\nzsh should be used in place of bash.\\n","Predictions":"We remove all complexity at the beginning using bash.n"}
{"File Name":"up-fiscal-data\/003-selnium.md","Context":"## Context and Problem Statement\nDecision required on the framework to be used to scrape and store data from the Koshvani platform in machine readable format.\n","Decision":"- Structure of the Koshvani platform\\n- Platfrom links do not reflect selection criteria\\n- Automation job requirements for data scraping\\n- Periodical jobs and access to new data\\nUse [Selenium](https:\/\/www.selenium.dev\/) to create robust, browser-based automation.\\n","tokens":32,"id":445,"text":"## Context and Problem Statement\\nDecision required on the framework to be used to scrape and store data from the Koshvani platform in machine readable format.\\n\n\n##Decision\n- Structure of the Koshvani platform\\n- Platfrom links do not reflect selection criteria\\n- Automation job requirements for data scraping\\n- Periodical jobs and access to new data\\nUse [Selenium](https:\/\/www.selenium.dev\/) to create robust, browser-based automation.\\n","Predictions":"* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availability"}
{"File Name":"macondo\/0003-remote-repositories-management.md","Context":"## Context\nRight now this cli tool comes with a builtin set of commands that is hardcoded\nto a gist I own somewhere. This is not ideal.\nWe want this tool to easily access \"repositories of commands\" that are either\nremote or local; as well as a way to easily add\/remove repositories.\nSome ideas of repositories:\n- HTTP urls pointing to yaml manifests with commands definitions\n- Github repositories\n- Local files or directories\n","Decision":"- We won't have built-in commands or repositories\\n- We will support three types of repositories:\\n- Remote http resources pointing to yaml files\\n- Local manifest yaml files\\n- Local directories containing .mcd files\\nBecause http resources are now supported, we could make it so that the\\nrepositories with commands has CI\/CD via Jenkins, which generates a repo and\\npublishes it to artifactory.\\nThe macondo file will be simplified to only include, for now, a top-level\\n`repositories` array, which would point to the repositories.\\nA macondo update command will be added that refreshes the list of commands\\nprovided by the repositories. This is only done for http repos, whose\\ncontents could change anytime.\\nA new set of commands to be implemented:\\n- `macondo repo add` to add repositories\\n- `macondo repo remove` to remove them\\n- `macondo repo update` to update them\\n- `macondo repo list` to list them\\n","tokens":92,"id":2191,"text":"## Context\\nRight now this cli tool comes with a builtin set of commands that is hardcoded\\nto a gist I own somewhere. This is not ideal.\\nWe want this tool to easily access \"repositories of commands\" that are either\\nremote or local; as well as a way to easily add\/remove repositories.\\nSome ideas of repositories:\\n- HTTP urls pointing to yaml manifests with commands definitions\\n- Github repositories\\n- Local files or directories\\n\n\n##Decision\n- We won't have built-in commands or repositories\\n- We will support three types of repositories:\\n- Remote http resources pointing to yaml files\\n- Local manifest yaml files\\n- Local directories containing .mcd files\\nBecause http resources are now supported, we could make it so that the\\nrepositories with commands has CI\/CD via Jenkins, which generates a repo and\\npublishes it to artifactory.\\nThe macondo file will be simplified to only include, for now, a top-level\\n`repositories` array, which would point to the repositories.\\nA macondo update command will be added that refreshes the list of commands\\nprovided by the repositories. This is only done for http repos, whose\\ncontents could change anytime.\\nA new set of commands to be implemented:\\n- `macondo repo add` to add repositories\\n- `macondo repo remove` to remove them\\n- `macondo repo update` to update them\\n- `macondo repo list` to list them\\n","Predictions":"* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-sdk-docs).n* All Cosmos SDK documentation is in a github repository (github.com\/cosmos-s"}
{"File Name":"cloud-platform\/020-Environments-and-Pipeline.md","Context":"## Context\nThe key proposition of Cloud Platform is to do the \"hosting\" of services, and we choose [Kubernetes for container management](004-use-kubernetes-for-container-management.md).\nIn agreeing a good interface for service teams, there several concerns:\n* Definitions - teams should be able to specify the workloads and infrastructure they want running.\n* Control - teams should be able to use a default hosting configuration, getting things running as simply as with a PaaS. However teams should also have full control over their Kubernetes resources, including pod configuration, lifecycle, network connectivity, etc.\n* Multi-tenancy - Service teams' workloads need isolation between their dev and prod environments, and from other service teams' workloads.\n","Decision":"1. Teams are offered 'namespaces'. A namespace is the concept of an isolated environment for workloads\/resources.\\n2. A CP namespace is implemented as a Kubernetes namespace and AWS resources (e.g. RDS instance, S3 bucket).\\n3. Isolation in Kubernetes namespaces is implemented using RBAC and NetworkPolicy:\\n* RBAC - teams can only administer k8s resources in their own namespaces\\n* NetworkPolicy - containers can only receive traffic from its ingresses and other containers in the same namespace (implemented with a NetworkPolicy, which teams can edit if needed)\\n4. Isolation between AWS resources is achieved using access control.\\nEach ECR repo, or S3 bucket, RDS bucket is made accessible to an IAM User, and the team are provided access key credentials for it.\\n5. A user defines a namespace in files: YAML (Kubernetes) and Terraform (AWS resources).\\nThe YAML includes by default: a Namespace and various default limits on resources, pods and networking.\\nFor deploying a simple workload, teams can include a YAML Deployment etc, so that these get applied automatically by CP's pipeline. Alternatively teams get more control by managing app resources using their namespace credentials - see below.\\nThe Terraform can specify any AWS resources like S3 buckets, RDS databases, Elasticache. Typically teams specify an ECR repo, so they have somewhere to deploy their images to.\\n6. The namespace definition is held in GitHub.\\nGitHub provides a mechanism for peer-review, automated checks and versioning.\\nOther options considered for configuring a namespace do not come with these advantages, for example:\\n* a console \/ web form, implemented as a custom web app (click ops)\\n* commands via a CLI or API\\nNamespace definitions are stored in the [environments repo](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments)\\n7. Namespace changes are checked by both a bot and a human from the CP team\\nIn Kubernetes, cluster-wide privileges are required to apply changes to a Kubernetes Namespace, as well as associated resources: LimitRange, NetworkPolicy and ServiceAccount. These privileges mean that the blast radius is large when applying changes.\\nIn terms of AWS resources, for common ones like S3 and RDS we provide terraform modules - to abstract away detail and promote best practice (for example, setting default encryption for S3 buckets). However Terraform can specify a huge range of AWS resources, each with multitude options. There are likely ways that one team can disrupt or get access to other teams' AWS services, that we can't anticipate, which is a risk to manage.\\nTo mitigate these concerns:\\n* [automated checks](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments\/tree\/main\/.github\/workflows) are used to validate against common problems\\n* Human review (by an engineer on the CP team) is also required on PRs, to check against unanticipated problems\\n8. Pipeline to deploy namespace automatically.\\nThe \"deploy pipeline\" is a CI\/CD pipeline that applies teams' namespace definitions in the clusters and AWS account. It triggers when the reviewed PR is merged to master.\\n9. Teams have full control within their Kubernetes namespace\\nUsers are given access to Kubernetes user credentials (kubecfg) with admin rights to their namespace. This gives them full control over their pods etc. They can deploy with 'kubectl apply' or Helm. They can debug problems with pod starting up, see logs etc.\\nUsers are also invited to create a ServiceAccount (using their environment YAML), and provide the creds to their CI\/CD, for deploying their app.\\n","tokens":152,"id":631,"text":"## Context\\nThe key proposition of Cloud Platform is to do the \"hosting\" of services, and we choose [Kubernetes for container management](004-use-kubernetes-for-container-management.md).\\nIn agreeing a good interface for service teams, there several concerns:\\n* Definitions - teams should be able to specify the workloads and infrastructure they want running.\\n* Control - teams should be able to use a default hosting configuration, getting things running as simply as with a PaaS. However teams should also have full control over their Kubernetes resources, including pod configuration, lifecycle, network connectivity, etc.\\n* Multi-tenancy - Service teams' workloads need isolation between their dev and prod environments, and from other service teams' workloads.\\n\n\n##Decision\n1. Teams are offered 'namespaces'. A namespace is the concept of an isolated environment for workloads\/resources.\\n2. A CP namespace is implemented as a Kubernetes namespace and AWS resources (e.g. RDS instance, S3 bucket).\\n3. Isolation in Kubernetes namespaces is implemented using RBAC and NetworkPolicy:\\n* RBAC - teams can only administer k8s resources in their own namespaces\\n* NetworkPolicy - containers can only receive traffic from its ingresses and other containers in the same namespace (implemented with a NetworkPolicy, which teams can edit if needed)\\n4. Isolation between AWS resources is achieved using access control.\\nEach ECR repo, or S3 bucket, RDS bucket is made accessible to an IAM User, and the team are provided access key credentials for it.\\n5. A user defines a namespace in files: YAML (Kubernetes) and Terraform (AWS resources).\\nThe YAML includes by default: a Namespace and various default limits on resources, pods and networking.\\nFor deploying a simple workload, teams can include a YAML Deployment etc, so that these get applied automatically by CP's pipeline. Alternatively teams get more control by managing app resources using their namespace credentials - see below.\\nThe Terraform can specify any AWS resources like S3 buckets, RDS databases, Elasticache. Typically teams specify an ECR repo, so they have somewhere to deploy their images to.\\n6. The namespace definition is held in GitHub.\\nGitHub provides a mechanism for peer-review, automated checks and versioning.\\nOther options considered for configuring a namespace do not come with these advantages, for example:\\n* a console \/ web form, implemented as a custom web app (click ops)\\n* commands via a CLI or API\\nNamespace definitions are stored in the [environments repo](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments)\\n7. Namespace changes are checked by both a bot and a human from the CP team\\nIn Kubernetes, cluster-wide privileges are required to apply changes to a Kubernetes Namespace, as well as associated resources: LimitRange, NetworkPolicy and ServiceAccount. These privileges mean that the blast radius is large when applying changes.\\nIn terms of AWS resources, for common ones like S3 and RDS we provide terraform modules - to abstract away detail and promote best practice (for example, setting default encryption for S3 buckets). However Terraform can specify a huge range of AWS resources, each with multitude options. There are likely ways that one team can disrupt or get access to other teams' AWS services, that we can't anticipate, which is a risk to manage.\\nTo mitigate these concerns:\\n* [automated checks](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments\/tree\/main\/.github\/workflows) are used to validate against common problems\\n* Human review (by an engineer on the CP team) is also required on PRs, to check against unanticipated problems\\n8. Pipeline to deploy namespace automatically.\\nThe \"deploy pipeline\" is a CI\/CD pipeline that applies teams' namespace definitions in the clusters and AWS account. It triggers when the reviewed PR is merged to master.\\n9. Teams have full control within their Kubernetes namespace\\nUsers are given access to Kubernetes user credentials (kubecfg) with admin rights to their namespace. This gives them full control over their pods etc. They can deploy with 'kubectl apply' or Helm. They can debug problems with pod starting up, see logs etc.\\nUsers are also invited to create a ServiceAccount (using their environment YAML), and provide the creds to their CI\/CD, for deploying their app.\\n","Predictions":"We will use [AWS Lambda](004-use-kubernetes-for-container-management.md) to manage the workloads and infrastructure.n"}
{"File Name":"cdh-adrs\/0002-cdh-etl-logic-boundaries.md","Context":"## Context\nTo ensure a clear separation as regards what logic resides within CDH and ETL related services where there exists feed specific\nrequirements during onboarding and processing of feed files, the following issues where considered:\n- Will such logic require specialized implementation across feeds?\n- Are such logic generic and require one time implementation or will require continous change\/update?\n- What are the benefits of moving such logic into ETL instead of CDH.\n","Decision":"The most important point agreed on was that CDH will remain focused on defined object and data models as possible and CDH will\\nrun with the expectation that all inputs received are completed. This means CDH should not have domain specific knowledge in regards\\nspecific intricacies about how specific feeds are reconcilied into complete Positions, Transactions and Ticker data.\\nSuch specificity will reside within the ETL service and be housed based on each feed parsing logic within the ETL service.\\nThe benefits of such a system is that only ETL needs to change to accomodate new and changing requirements of old and new feeds\\nensuring the final result is always consistent to march what the CDH service requires.\\n","tokens":92,"id":8,"text":"## Context\\nTo ensure a clear separation as regards what logic resides within CDH and ETL related services where there exists feed specific\\nrequirements during onboarding and processing of feed files, the following issues where considered:\\n- Will such logic require specialized implementation across feeds?\\n- Are such logic generic and require one time implementation or will require continous change\/update?\\n- What are the benefits of moving such logic into ETL instead of CDH.\\n\n\n##Decision\nThe most important point agreed on was that CDH will remain focused on defined object and data models as possible and CDH will\\nrun with the expectation that all inputs received are completed. This means CDH should not have domain specific knowledge in regards\\nspecific intricacies about how specific feeds are reconcilied into complete Positions, Transactions and Ticker data.\\nSuch specificity will reside within the ETL service and be housed based on each feed parsing logic within the ETL service.\\nThe benefits of such a system is that only ETL needs to change to accomodate new and changing requirements of old and new feeds\\nensuring the final result is always consistent to march what the CDH service requires.\\n","Predictions":"We will use the Security model to define the underlying logic for the Security service.n"}
{"File Name":"celestia-core\/adr-056-light-client-amnesia-attacks.md","Context":"## Context\nWhilst most created evidence of malicious behavior is self evident such that any individual can verify them independently there are types of evidence, known collectively as global evidence, that require further collaboration from the network in order to accumulate enough information to create evidence that is individually verifiable and can therefore be processed through consensus. [Fork Accountability](https:\/\/github.com\/tendermint\/spec\/blob\/master\/spec\/consensus\/light-client\/accountability.md) has been coined to describe the entire process of detection, proving and punishing of malicious behavior. This ADR addresses specifically what a light client amnesia attack is and how it can be proven and the current decision around handling light client amnesia attacks. For information on evidence handling by the light client, it is recommended to read [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md).\n### Amnesia Attack\nThe schematic below explains a scenario where an amnesia attack can occur such that two sets of honest nodes, C1 and C2, commit different blocks.\n![](..\/imgs\/tm-amnesia-attack.png)\n1. C1 and F send PREVOTE messages for block A.\n2. C1 sends PRECOMMIT for round 1 for block A.\n3. A new round is started, C2 and F send PREVOTE messages for a different block B.\n4. C2 and F then send PRECOMMIT messages for block B.\n5. F later on creates PRECOMMITS for block A and combines it with those from C1 to form a block\nThis forged block can then be used to fool light clients trying to verify it. It must be stressed that there are a few more hurdles or dimensions to the attack to consider.For a more detailed walkthrough refer to Appendix A.\n","Decision":"The decision surrounding amnesia attacks has both a short term and long term component. In the long term, a more sturdy protocol will need to be fleshed out and implemented. There is already draft documents outlining what such a protocol would look like and the resources it would require (see references). Prior revisions however outlined a protocol which had been implemented (See Appendix B). It was agreed that it still required greater consideration and review given it's importance. It was therefore discussed, with the limited time frame set before 0.34, whether the protocol should be completely removed or if there should remain some logic in handling the aforementioned scenarios.\\nThe latter of the two options meant storing a record of all votes in any height with which there was more than one round. This information would then be accessible for applications if they wanted to perform some off-chain verification and punishment.\\nIn summary, this seemed like too much to ask of the application to implement only on a temporary basis, whilst not having the domain specific knowledge and considering such a difficult and unlikely attack. Therefore the short term decision is to identify when the attack has occurred and implement the detector algorithm highlighted in [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md) but to not implement any accountability protocol that would identify malicious validators and allow applications to punish them. This will hopefully change in the long term with the focus on eventually reaching a concrete and secure protocol with identifying and dealing with these attacks.\\n","tokens":379,"id":4113,"text":"## Context\\nWhilst most created evidence of malicious behavior is self evident such that any individual can verify them independently there are types of evidence, known collectively as global evidence, that require further collaboration from the network in order to accumulate enough information to create evidence that is individually verifiable and can therefore be processed through consensus. [Fork Accountability](https:\/\/github.com\/tendermint\/spec\/blob\/master\/spec\/consensus\/light-client\/accountability.md) has been coined to describe the entire process of detection, proving and punishing of malicious behavior. This ADR addresses specifically what a light client amnesia attack is and how it can be proven and the current decision around handling light client amnesia attacks. For information on evidence handling by the light client, it is recommended to read [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md).\\n### Amnesia Attack\\nThe schematic below explains a scenario where an amnesia attack can occur such that two sets of honest nodes, C1 and C2, commit different blocks.\\n![](..\/imgs\/tm-amnesia-attack.png)\\n1. C1 and F send PREVOTE messages for block A.\\n2. C1 sends PRECOMMIT for round 1 for block A.\\n3. A new round is started, C2 and F send PREVOTE messages for a different block B.\\n4. C2 and F then send PRECOMMIT messages for block B.\\n5. F later on creates PRECOMMITS for block A and combines it with those from C1 to form a block\\nThis forged block can then be used to fool light clients trying to verify it. It must be stressed that there are a few more hurdles or dimensions to the attack to consider.For a more detailed walkthrough refer to Appendix A.\\n\n\n##Decision\nThe decision surrounding amnesia attacks has both a short term and long term component. In the long term, a more sturdy protocol will need to be fleshed out and implemented. There is already draft documents outlining what such a protocol would look like and the resources it would require (see references). Prior revisions however outlined a protocol which had been implemented (See Appendix B). It was agreed that it still required greater consideration and review given it's importance. It was therefore discussed, with the limited time frame set before 0.34, whether the protocol should be completely removed or if there should remain some logic in handling the aforementioned scenarios.\\nThe latter of the two options meant storing a record of all votes in any height with which there was more than one round. This information would then be accessible for applications if they wanted to perform some off-chain verification and punishment.\\nIn summary, this seemed like too much to ask of the application to implement only on a temporary basis, whilst not having the domain specific knowledge and considering such a difficult and unlikely attack. Therefore the short term decision is to identify when the attack has occurred and implement the detector algorithm highlighted in [ADR 47](https:\/\/github.com\/tendermint\/tendermint\/blob\/master\/docs\/architecture\/adr-047-handling-evidence-from-light-client.md) but to not implement any accountability protocol that would identify malicious validators and allow applications to punish them. This will hopefully change in the long term with the focus on eventually reaching a concrete and secure protocol with identifying and dealing with these attacks.\\n","Predictions":"We will use the [Security Layer](https:\/\/github.com\/Security Layer) to provide a mechanism for storing and distributing sensitive data.n"}
{"File Name":"elife-xpub\/0004-code-structure.md","Context":"## Context\nAs the code rapidly grows we need a common structure and approach that will keep the code tidy and coherent.\nThere are various aspects, the initial concern was the data access separation. The issue #140 initially talked about how this could be separated in terms of an ORM (Object-Relational Mapping) or a DAL (Data Access Layer).\nHowever, this discussion also soon covered the structure of the code in general and how to structure it for ease of development and best practise.\n### Rejected Options\nWe considered the following options:\n* The Pubsweet Way - This was to define a base class \"Model\" and extend this for each entity. This was rejected as its another self made ORM but and has validations that we don't want.\n* Other ORMs : Waterline, sequelize, bookshelf - General purpose ORMs were rejected based on the experience that the time spent going up the learning curve and sifting through the docs when things go wrong is greater then simply rolling your own.\n* Query Builders : Knex.js -These were not entirely ruled out, and were deemed compatible with a good code structure that would allow the database access to be encapsulated in a way that would allow a query builder to be used where necessary.\n","Decision":"In summary the server-side code will be structured below the `server` folder as follows:\\n* **entities** - This folder will contain subfolders relating to named entities in the system. See [\"Data Model Specification\"](https:\/\/docs.google.com\/document\/d\/1KU-DLMNhPxjQF2j8HVlJvenvttPLxgtbTHo8Sy_PNRc\/).\\n* **entities\/\\<entity\\>** - The example below shows `Mansuscript` as an example entity, this folder will contain a common set of files that describe the entity's behaviour. Each of these have a particular purpose explained below:\\n* index.js - The main business logic, e.g. for `Manuscript` this could contain `getAuthor()`\\n* typedefs.js - Contains the GraphQL types pertinent to this entity.\\n* resolvers.js - Contains the GraphQL interface specified in terms of `Query` and `Mutation` 's for this entity. These in general should map onto the exposed functions from index.js. For example, `Manuscript` may contain a `Query` for `allManuscripts` and a `Mutation` for `createManuscript`.\\n* data-access.js - Contains the functions for saving to and loading from the database. All references to the database and SQL that is written should remain private to this file.\\n* **manuscript-state-machine** - It is anticipated that this is where the most orchestration of the entites will take place. This has been created as a separate folder so that it can expand to implement the state transitions currently defined for the manuscript.\\n### Example\\n```\\nserver\\n|\\n\u251c\u2500\u2500 entities\\n|   |\\n\u2502   \u251c\u2500\u2500 manuscript\\n\u2502   \u2502   \u251c\u2500\u2500 typedefs.js\\n\u2502   \u2502   \u251c\u2500\u2500 resolvers.js\\n\u2502   \u2502   \u251c\u2500\u2500 data-access.js\\n\u2502   \u2502   \u2514\u2500\u2500 index.js\\n|   |\\n\u2502   \u2514\u2500\u2500 <entity>\\n\u2502       \u251c\u2500\u2500 typedefs.js\\n\u2502       \u251c\u2500\u2500 resolvers.js\\n\u2502       \u251c\u2500\u2500 data-access.js\\n\u2502       \u2514\u2500\u2500 index.js\\n|\\n\u2514\u2500\u2500 manuscript-state-machine\\n\u2514\u2500\u2500 index.js\\n```\\n","tokens":257,"id":3776,"text":"## Context\\nAs the code rapidly grows we need a common structure and approach that will keep the code tidy and coherent.\\nThere are various aspects, the initial concern was the data access separation. The issue #140 initially talked about how this could be separated in terms of an ORM (Object-Relational Mapping) or a DAL (Data Access Layer).\\nHowever, this discussion also soon covered the structure of the code in general and how to structure it for ease of development and best practise.\\n### Rejected Options\\nWe considered the following options:\\n* The Pubsweet Way - This was to define a base class \"Model\" and extend this for each entity. This was rejected as its another self made ORM but and has validations that we don't want.\\n* Other ORMs : Waterline, sequelize, bookshelf - General purpose ORMs were rejected based on the experience that the time spent going up the learning curve and sifting through the docs when things go wrong is greater then simply rolling your own.\\n* Query Builders : Knex.js -These were not entirely ruled out, and were deemed compatible with a good code structure that would allow the database access to be encapsulated in a way that would allow a query builder to be used where necessary.\\n\n\n##Decision\nIn summary the server-side code will be structured below the `server` folder as follows:\\n* **entities** - This folder will contain subfolders relating to named entities in the system. See [\"Data Model Specification\"](https:\/\/docs.google.com\/document\/d\/1KU-DLMNhPxjQF2j8HVlJvenvttPLxgtbTHo8Sy_PNRc\/).\\n* **entities\/\\<entity\\>** - The example below shows `Mansuscript` as an example entity, this folder will contain a common set of files that describe the entity's behaviour. Each of these have a particular purpose explained below:\\n* index.js - The main business logic, e.g. for `Manuscript` this could contain `getAuthor()`\\n* typedefs.js - Contains the GraphQL types pertinent to this entity.\\n* resolvers.js - Contains the GraphQL interface specified in terms of `Query` and `Mutation` 's for this entity. These in general should map onto the exposed functions from index.js. For example, `Manuscript` may contain a `Query` for `allManuscripts` and a `Mutation` for `createManuscript`.\\n* data-access.js - Contains the functions for saving to and loading from the database. All references to the database and SQL that is written should remain private to this file.\\n* **manuscript-state-machine** - It is anticipated that this is where the most orchestration of the entites will take place. This has been created as a separate folder so that it can expand to implement the state transitions currently defined for the manuscript.\\n### Example\\n```\\nserver\\n|\\n\u251c\u2500\u2500 entities\\n|   |\\n\u2502   \u251c\u2500\u2500 manuscript\\n\u2502   \u2502   \u251c\u2500\u2500 typedefs.js\\n\u2502   \u2502   \u251c\u2500\u2500 resolvers.js\\n\u2502   \u2502   \u251c\u2500\u2500 data-access.js\\n\u2502   \u2502   \u2514\u2500\u2500 index.js\\n|   |\\n\u2502   \u2514\u2500\u2500 <entity>\\n\u2502       \u251c\u2500\u2500 typedefs.js\\n\u2502       \u251c\u2500\u2500 resolvers.js\\n\u2502       \u251c\u2500\u2500 data-access.js\\n\u2502       \u2514\u2500\u2500 index.js\\n|\\n\u2514\u2500\u2500 manuscript-state-machine\\n\u2514\u2500\u2500 index.js\\n```\\n","Predictions":"We will use a Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate"}
{"File Name":"up-fiscal-data\/005-phases.md","Context":"## Context and Problem Statement\nNumerous sub-sections from Expenditure were selected to be scraped and analysed. A phased approach will be employed to make sustainable jobs for each section.\n## Decision Drivers\n* Use the data from differnt sections to impute accounting head information\n* The information from various sections will be used to create data checks\n* Long term project with 3 year pipeline to create a sustainable archirtecture\n","Decision":"* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\nSet a priority list and weekly targets foe the data that needs to be scraped from Koshvani.\\n","tokens":88,"id":443,"text":"## Context and Problem Statement\\nNumerous sub-sections from Expenditure were selected to be scraped and analysed. A phased approach will be employed to make sustainable jobs for each section.\\n## Decision Drivers\\n* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\n\n\n##Decision\n* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\nSet a priority list and weekly targets foe the data that needs to be scraped from Koshvani.\\n","Predictions":"* Use the data from differnt sections to impute accounting head informationn* The information from various sections will be used to create data checksn* Long term project with 3 year pipeline to create a sustainable archirtecturenSet a priority list and weekly targets foe the data that needs to be scraped from Koshvani.n"}
{"File Name":"infrastructure-adrs\/0010-message-broker.md","Context":"## Context and Problem Statement\nSDR is evolving into an (increasing) set of (largely) single-purpose services, with communication between these services occurring via synchronous HTTP. This architecture is fragile as it scales under load and as the number of services increases because:\n* Each service must have knowledge about many other services.\n* Each service depends on many other services to perform its job; any of these services being unavailable can bring down the service or hamper performance.\n* Synchronous HTTP can have a performance penalty for operations in which a response is not required (e.g., reporting events).\n* The Stanford network is notoriously unreliable (especially at night); significant coding effort is required to account for this.\nIn addition, there are already places within SDR were we have re-invented a message broker and\/or jerry-rigged around not having a message broker (e.g., polling via HTTP to determine if a long-running job has completed).\nIntroducing a message broker to support asynchronous communication where appropriate will allow decoupling of services, making SDR more resilient and scalable.\n","Decision":"TBD.\\n### Positive Consequences\\n* Increased ability to scale number of services.\\n* Increased ability to scale under load.\\n* Increased resilience to service failures.\\n* Provide mechanism for notification of the completion of long-running jobs.\\n### Negative Consequences\\n* Message broker must be supported by Ops.\\n* Services become dependent on the Message Broker. (However, Message Brokers are typically designed to be depended on.)\\n* The evolution work to begin using the Message Broker.\\n","tokens":224,"id":786,"text":"## Context and Problem Statement\\nSDR is evolving into an (increasing) set of (largely) single-purpose services, with communication between these services occurring via synchronous HTTP. This architecture is fragile as it scales under load and as the number of services increases because:\\n* Each service must have knowledge about many other services.\\n* Each service depends on many other services to perform its job; any of these services being unavailable can bring down the service or hamper performance.\\n* Synchronous HTTP can have a performance penalty for operations in which a response is not required (e.g., reporting events).\\n* The Stanford network is notoriously unreliable (especially at night); significant coding effort is required to account for this.\\nIn addition, there are already places within SDR were we have re-invented a message broker and\/or jerry-rigged around not having a message broker (e.g., polling via HTTP to determine if a long-running job has completed).\\nIntroducing a message broker to support asynchronous communication where appropriate will allow decoupling of services, making SDR more resilient and scalable.\\n\n\n##Decision\nTBD.\\n### Positive Consequences\\n* Increased ability to scale number of services.\\n* Increased ability to scale under load.\\n* Increased resilience to service failures.\\n* Provide mechanism for notification of the completion of long-running jobs.\\n### Negative Consequences\\n* Message broker must be supported by Ops.\\n* Services become dependent on the Message Broker. (However, Message Brokers are typically designed to be depended on.)\\n* The evolution work to begin using the Message Broker.\\n","Predictions":"* Increased ability to scale number of services.n* Increased ability to scale under load.n* Increased resilience to service failures.n* Provide mechanism for notification of the completion of long-running jobs.n### Negative Consequencesn* Message broker must be supported by Ops.n* Services become dependent on the Message Broker. (However, Message Brokers are typically designed to be depended on.)n* The evolution work to begin using the Message Broker.n"}
{"File Name":"form-design-system\/relative_spacing.md","Context":"## Context and Problem Statement\nWe'd like to change the spacing system to something more generic.\n## Decision Drivers <!-- optional -->\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\n","Decision":"The current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\nGeneric tshirt system was chosen over the other options.\\n- Coding a direct \"arbitrary value\" into a CSS class (color, number representing a unit) is generally considered an anti-pattern for reasons that are too long to cover in this ADR.\\n- We like the numbering system but we use t-shirt sizes everywhere else in our system so it makes sense to skew towards alignment with the larger system.\\n","tokens":153,"id":801,"text":"## Context and Problem Statement\\nWe'd like to change the spacing system to something more generic.\\n## Decision Drivers <!-- optional -->\\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\n\n\n##Decision\nThe current pattern of relative spacing (half, double) that's based on a default, is deficient in a couple of ways:\\n- We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.\\n- The current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.\\nGeneric tshirt system was chosen over the other options.\\n- Coding a direct \"arbitrary value\" into a CSS class (color, number representing a unit) is generally considered an anti-pattern for reasons that are too long to cover in this ADR.\\n- We like the numbering system but we use t-shirt sizes everywhere else in our system so it makes sense to skew towards alignment with the larger system.\\n","Predictions":"We'd like to add additional spacing values that don't map cleanly to these words. If default spacing is 16, there's no clean word to describe the jump to 24.nThe current model makes it impossible to ever cleanly change the default value. Given the model of default, half, and double, if we decided to make double the new default, the current default would have to be renamed to half, and half would need to be renamed to half\/half.nGeneric tshirt system was chosen over the other options.nWe like the numbering system but we use t-shirt sizes everywhere else in our system so it makes sense to skew towards alignment with the larger system.n"}
{"File Name":"Corvus.Extensions.Newtonsoft.Json\/0001-json-framework-independence.md","Context":"## Context\nWe frequently need to deal with information that will be serialized as JSON, either because it needs to be used in a web API, or because it is serialized in some JSON-based storage system. For many years, Json.NET has been the de facto API for working with JSON in .NET applications. However, now that .NET Core has `System.Text.Json` built in, there are good reasons to want to move to that. This means that libraries that depend on Json.NET become problematic.\nSo we need ways for our libraries to work with JSON data without forcing the decision of whether to use Json.NET or `System.Text.Json`.\n","Decision":"We have moved types previously in `Corvus.Extensions.Newtonsoft.Json` into `Corvus.Json.Abstractions`, having modified them to remove any direct dependency on Json.NET.\\nCurrently, only the property bag type has been moved. (And it is now an `IPropertyBag` interface.) A great deal of what's in `Corvus.Extensions.Newtonsoft.Json` is already in the right place, because it deals directly with Json.NET-specific concerns (e.g., custom type converters).\\n","tokens":132,"id":2585,"text":"## Context\\nWe frequently need to deal with information that will be serialized as JSON, either because it needs to be used in a web API, or because it is serialized in some JSON-based storage system. For many years, Json.NET has been the de facto API for working with JSON in .NET applications. However, now that .NET Core has `System.Text.Json` built in, there are good reasons to want to move to that. This means that libraries that depend on Json.NET become problematic.\\nSo we need ways for our libraries to work with JSON data without forcing the decision of whether to use Json.NET or `System.Text.Json`.\\n\n\n##Decision\nWe have moved types previously in `Corvus.Extensions.Newtonsoft.Json` into `Corvus.Json.Abstractions`, having modified them to remove any direct dependency on Json.NET.\\nCurrently, only the property bag type has been moved. (And it is now an `IPropertyBag` interface.) A great deal of what's in `Corvus.Extensions.Newtonsoft.Json` is already in the right place, because it deals directly with Json.NET-specific concerns (e.g., custom type converters).\\n","Predictions":"We have moved types previously in Corvus.Extensions.Newtonsoft.Json into Corvus.Json.Abstractions, having modified them to remove any direct dependency on Json.NET.nCurrently, only the property bag type has been moved. (And it is now an IPropertyBag interface.) A great deal of what's in Corvus.Extensions.Newtonsoft.Json is already in the right place, because it deals directly with Json.NET-specific concerns (e.g., custom type converters).n"}
{"File Name":"paas-team-manual\/ADR044-remove-ipsec.html.md","Context":"## Context\nGOV.UK PaaS uses mutual TLS via routing-release between gorouter and apps.\nThis is a form of end-to-end encryption.\nGOV.UK PaaS uses IPSec between gorouter and diego cells (which run apps).\nThis is a form of end-to-end encryption.\nThe [cf-dev mailing list](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/message\/9143) alleges\nthat the IPSec release is no longer maintained.\n","Decision":"We will not run IPSec in our BOSH deployments.\\n","tokens":103,"id":194,"text":"## Context\\nGOV.UK PaaS uses mutual TLS via routing-release between gorouter and apps.\\nThis is a form of end-to-end encryption.\\nGOV.UK PaaS uses IPSec between gorouter and diego cells (which run apps).\\nThis is a form of end-to-end encryption.\\nThe [cf-dev mailing list](https:\/\/lists.cloudfoundry.org\/g\/cf-dev\/message\/9143) alleges\\nthat the IPSec release is no longer maintained.\\n\n\n##Decision\nWe will not run IPSec in our BOSH deployments.\\n","Predictions":"We will not run IPSec in our BOSH deployments.n"}
{"File Name":"thing\/0002-authentication-via-touchstone-saml.md","Context":"## Context\nA mechanism for providing end-user authentication of MIT users is required for\ncore aspects of this application.\nMIT officially only supports Shibboleth \/ Touchstone which traditionally is enabled via an Apache httpd module.\nThe [MIT OpenID Pilot](https:\/\/mitlibraries.github.io\/oauth.html) was determined\nunacceptable for this project.\nIn order to use Shibboleth we intended to containerize the application. However,\nwhile that process ended in a successful proof-of-concept, it used SAML and not\nmod_shib. Once we went down the path of not using the officially MIT supported\nmod_shib solution, we felt using SAML in-app should be explored.\n","Decision":"We will use an in-app Touchstone SP using ruby-saml as documented in our\\n[Developer Documentation](https:\/\/mitlibraries.github.io\/touchstone_saml.html).\\n","tokens":144,"id":2582,"text":"## Context\\nA mechanism for providing end-user authentication of MIT users is required for\\ncore aspects of this application.\\nMIT officially only supports Shibboleth \/ Touchstone which traditionally is enabled via an Apache httpd module.\\nThe [MIT OpenID Pilot](https:\/\/mitlibraries.github.io\/oauth.html) was determined\\nunacceptable for this project.\\nIn order to use Shibboleth we intended to containerize the application. However,\\nwhile that process ended in a successful proof-of-concept, it used SAML and not\\nmod_shib. Once we went down the path of not using the officially MIT supported\\nmod_shib solution, we felt using SAML in-app should be explored.\\n\n\n##Decision\nWe will use an in-app Touchstone SP using ruby-saml as documented in our\\n[Developer Documentation](https:\/\/mitlibraries.github.io\/touchstone_saml.html).\\n","Predictions":"We will use an in-app Touchstone SP using ruby-saml as documented in ourn[Developer Documentation](https:\/\/mitlibraries.github.io\/touchstone_saml.html).n"}
{"File Name":"experimenter\/0007-doc-hub-url.md","Context":"## Context and Problem Statement\nWe'd like to have a permanent URL for the experimenter docs hub that is:\n- Memorable\n- Stable\n- Ideally, part of the rest of our data information architecture\n","Decision":"We will use a custom domain for now (experimenter.info) until a decision is made on the general organization of data.mozilla.org in the future, at which time we will update the URL to fit in with the rest of our data documentation.\\n","tokens":45,"id":1823,"text":"## Context and Problem Statement\\nWe'd like to have a permanent URL for the experimenter docs hub that is:\\n- Memorable\\n- Stable\\n- Ideally, part of the rest of our data information architecture\\n\n\n##Decision\nWe will use a custom domain for now (experimenter.info) until a decision is made on the general organization of data.mozilla.org in the future, at which time we will update the URL to fit in with the rest of our data documentation.\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).nADRs from here on out will be written in the first-person **singular** tense, since it's mainly just me building the app.n"}
{"File Name":"profiles-etl-combiner\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":871,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"Wikibase\/0000-use-cache-instead-wb_terms-for-data-needed-for-display.md","Context":"## Context\nWhen an item or a property is displayed in the short form, either as a link, or as a simple text reference, data needed to render this short form are currently loaded from the SQL table (wb_terms). wb_terms is causing several significant issues, and is not possible to be maintained any more in the long run.\nDecision to use wb_terms, initially introduced as a SQL search index, has been tracked down to [change 176650](https:\/\/gerrit.wikimedia.org\/r\/#\/c\/mediawiki\/extensions\/Wikibase\/+\/176650\/). As discussed there in the code review, and also in https:\/\/phabricator.wikimedia.org\/T74309#798908, it seems there had been no dramatic performance improvements expected, neither noticed after switching to use wb_terms instead of loading the data of the entire item or property.\nWikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\n","Decision":"Wikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\nAs long as using SQL table as a storage of the data used for displaying entities in the short form does not bring significant performance gains, we decide to stop using wb_terms as a data source for this use case.\\nInstead, data of the whole entity is going to be retrieved from storage layer (from the database, or from cached storage that are already in place).\\nIf not efficient enough (e.g. in case of huge-size Wikibase instances like Wikidata ), data needed for display will also be stored in cache, e.g. label of an item in a particular language. That should reduce the amount of computation needed, especially when language fallback needs to be applied, etc.\\n","tokens":330,"id":1341,"text":"## Context\\nWhen an item or a property is displayed in the short form, either as a link, or as a simple text reference, data needed to render this short form are currently loaded from the SQL table (wb_terms). wb_terms is causing several significant issues, and is not possible to be maintained any more in the long run.\\nDecision to use wb_terms, initially introduced as a SQL search index, has been tracked down to [change 176650](https:\/\/gerrit.wikimedia.org\/r\/#\/c\/mediawiki\/extensions\/Wikibase\/+\/176650\/). As discussed there in the code review, and also in https:\/\/phabricator.wikimedia.org\/T74309#798908, it seems there had been no dramatic performance improvements expected, neither noticed after switching to use wb_terms instead of loading the data of the entire item or property.\\nWikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\n\n\n##Decision\nWikibase already uses MediaWiki's caching mechanisms (in production Wikidata environment being based on memcached) to reduce loading of full entity data.\\nIn case of lexemes or forms, entity types provided by WikibaseLexeme extension, that have different internal structure than items and properties, wb_terms has not been used as a source of data for short form display. Full lexeme data has been loaded instead. Early tests didn't show significant performance issues (see https:\/\/phabricator.wikimedia.org\/T188108). Also, due to different internal structure of lexemes, or forms, and the way how their \"short form\" displayed is built, the possible use of wb_terms has not even seem feasible without changing the semantics of the table.\\nAs long as using SQL table as a storage of the data used for displaying entities in the short form does not bring significant performance gains, we decide to stop using wb_terms as a data source for this use case.\\nInstead, data of the whole entity is going to be retrieved from storage layer (from the database, or from cached storage that are already in place).\\nIf not efficient enough (e.g. in case of huge-size Wikibase instances like Wikidata ), data needed for display will also be stored in cache, e.g. label of an item in a particular language. That should reduce the amount of computation needed, especially when language fallback needs to be applied, etc.\\n","Predictions":"We will use a stack method to load the data from the stack table.n"}
{"File Name":"tamr-client\/0006-type-checking.md","Context":"## Context\nStatic type-checking is available for Python, making us of the type annotations already in the codebase.\n","Decision":"Type-check via [mypy](http:\/\/mypy-lang.org\/).\\n","tokens":25,"id":536,"text":"## Context\\nStatic type-checking is available for Python, making us of the type annotations already in the codebase.\\n\n\n##Decision\nType-check via [mypy](http:\/\/mypy-lang.org\/).\\n","Predictions":"* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is a src file that contains the src file.n* src is "}
{"File Name":"FindMeFoodTrucks\/Architecture Style.md","Context":"## :dart: Context\nFollowing Architecture styles were considered for the application\n1. n-tier\n1. Web-Queue-Worker\n1. Microservices\n1. Big data\/compute\nChoosing the right Architecture style will impact the functional and non-functional efficiencies of the project.\n","Decision":"A simple Microservices based architecture style was implementing CQS pattern will be used for this application. More about CQS pattern can be found here :https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/architecture-styles\/\\n","tokens":58,"id":1094,"text":"## :dart: Context\\nFollowing Architecture styles were considered for the application\\n1. n-tier\\n1. Web-Queue-Worker\\n1. Microservices\\n1. Big data\/compute\\nChoosing the right Architecture style will impact the functional and non-functional efficiencies of the project.\\n\n\n##Decision\nA simple Microservices based architecture style was implementing CQS pattern will be used for this application. More about CQS pattern can be found here :https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/docs.microsoft.com\/en-us\/azure\/architecture\/guide\/architecture-styles\/\\n","Predictions":"A simple Microservices based architecture style was implementing CQS pattern will be used for this application. More about CQS pattern can be found here:https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.htmln"}
{"File Name":"iampeterbanjo.com\/0011-artist-images-from-spotify.md","Context":"## Context\nKorin would look much better with artist images. Unfortunately LastFm [removed artist images][lastfm-images-gone] from their API and have replaced it with a placeholder. There are some options to get artist images which are:\n- [Music story][music-story-api]: The documentation is not easy to understand and the SDK is designed for web browsers. It also uses a custom ID to associate artists, not IMDB. One unique feature is that it has lyrics too.\n- [Music brainz][music-brainz-api]: Creative commons but has everything except artist images.\n- [Spotify][spotify-api]: Has artist information including images but uses a custom ID, not IMDB.\n","Decision":"Spotify is the only viable alternative.\\n","tokens":148,"id":1598,"text":"## Context\\nKorin would look much better with artist images. Unfortunately LastFm [removed artist images][lastfm-images-gone] from their API and have replaced it with a placeholder. There are some options to get artist images which are:\\n- [Music story][music-story-api]: The documentation is not easy to understand and the SDK is designed for web browsers. It also uses a custom ID to associate artists, not IMDB. One unique feature is that it has lyrics too.\\n- [Music brainz][music-brainz-api]: Creative commons but has everything except artist images.\\n- [Spotify][spotify-api]: Has artist information including images but uses a custom ID, not IMDB.\\n\n\n##Decision\nSpotify is the only viable alternative.\\n","Predictions":"* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (blobstore-s3).n* Replace current blob-store-objectstorage with swift-s3.n"}
{"File Name":"buildit-all\/0002-version-api-via-uri-path.md","Context":"## Context\nVersioning APIs can be controversial.  Ideally, APIs don't need to change, or be additive so as not to break backwards compatibility.  Occasionally, some changes require bigger changes.\nWe will do our best to be open w\/ what we accept and strict w\/ what we return\nThere are lots of ways an API can change:\n* Breaking changes across the entire API (all endpoints change from REST to GraphQL for example)\n* Breaking changes at a resource level (\/v1\/booking) needs to change\n* Breaking changes in behavior\nThis decision currently applies to the 1st bullet.\n","Decision":"All API endpoints will start with the version of the API (e.g. \/v1\/resource).  We don't anticipate this to change.\\n","tokens":127,"id":4424,"text":"## Context\\nVersioning APIs can be controversial.  Ideally, APIs don't need to change, or be additive so as not to break backwards compatibility.  Occasionally, some changes require bigger changes.\\nWe will do our best to be open w\/ what we accept and strict w\/ what we return\\nThere are lots of ways an API can change:\\n* Breaking changes across the entire API (all endpoints change from REST to GraphQL for example)\\n* Breaking changes at a resource level (\/v1\/booking) needs to change\\n* Breaking changes in behavior\\nThis decision currently applies to the 1st bullet.\\n\n\n##Decision\nAll API endpoints will start with the version of the API (e.g. \/v1\/resource).  We don't anticipate this to change.\\n","Predictions":"We will use a version property to indicate the version of the API.n"}
{"File Name":"admin-react-components\/0003-component-bundling.md","Context":"## Context\nWhen bundling components for usage in consuming applications, should we require consuming applications to have the same dependencies as this repository? Or should exported components be packaged assuming no dependencies?\n","Decision":"We'll package components with their own inline styles. No external dependencies needed.\\n","tokens":38,"id":1631,"text":"## Context\\nWhen bundling components for usage in consuming applications, should we require consuming applications to have the same dependencies as this repository? Or should exported components be packaged assuming no dependencies?\\n\n\n##Decision\nWe'll package components with their own inline styles. No external dependencies needed.\\n","Predictions":"We'll use the Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security"}
{"File Name":"event-routing-backends\/0005-PII-leakage-prevention.rst","Context":"Context\n-------\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\nDecision\n--------\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\nBenefits\n---------\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\n","Decision":"--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n","tokens":307,"id":4477,"text":"Context\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n\n\n##Decision\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https:\/\/github.com\/openedx\/event-routing-backends\/blob\/f430d4cf58bdab01e42fcc944241898606873d82\/event_routing_backends\/processors\/mixins\/base_transformer.py#L139\\n","Predictions":"--------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------nn-------n"}
{"File Name":"TruBudget\/0004-ressource-level-lifetime.md","Context":"## Context\nResources have semantically a certain lifetime. That means, after a resources is created it will experience state-transition until it reaches the end of its lifetime (e.g. archival). The lifetimes and state-transitions need to be defined in an abstract way, in order to allow the platform to excel for different use-cases.\n","Decision":"### Lifetime of a single resource\\n```mermaid\\ngraph TD\\nA[Create resource] -->|Auto-assign to creator| B\\nB[State: Open] --> |Do work|B\\nB --> |Assign to somebody who can close| C\\nC[State: Open & Ready to close] --> |Close| D\\nD[Resource closed]\\n```\\nRe-Opening is not supported; it could be discussed as long as the parent is still open (hierarchy must hold). Different usage-scenarios are acommodated via _manual_ steps.\\n#### Example: 4-Eyes Principle\\nScenario: A subproject must only be closed when 2 people A & B (e.g. different departments) checked it.\\nPrereqs: Closing right is given to an administrative person and not to the people signing off\\nApproach:\\n1. Subproject is executed as usual, then for sign-off is assigned to Person A\\n2. Sign-Off Person A enters a Workflow-Item validating the state and assigns Person B\\n3. Sign-Off Person B enters a Workflow-Item validating the state and assigns to administrative personell that has the close-rights\\n4. Administrative person checks whether all required sign-offs are there and then finally closes; otherwise assigns back\\n### Impact to child resources\\nA resource hierarchically describes a node in a tree-like structure. Therefore we need to look at impact of a closed resource to other nodes in the tree.\\nIf a resource has no children, there is no impact on other resources.\\nIf a resource has children (e.g. subprojects \/ workflowitems), **the resource cannot be closed**. All children need to be in state closed before a parent can be closed.\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n","tokens":69,"id":1216,"text":"## Context\\nResources have semantically a certain lifetime. That means, after a resources is created it will experience state-transition until it reaches the end of its lifetime (e.g. archival). The lifetimes and state-transitions need to be defined in an abstract way, in order to allow the platform to excel for different use-cases.\\n\n\n##Decision\n### Lifetime of a single resource\\n```mermaid\\ngraph TD\\nA[Create resource] -->|Auto-assign to creator| B\\nB[State: Open] --> |Do work|B\\nB --> |Assign to somebody who can close| C\\nC[State: Open & Ready to close] --> |Close| D\\nD[Resource closed]\\n```\\nRe-Opening is not supported; it could be discussed as long as the parent is still open (hierarchy must hold). Different usage-scenarios are acommodated via _manual_ steps.\\n#### Example: 4-Eyes Principle\\nScenario: A subproject must only be closed when 2 people A & B (e.g. different departments) checked it.\\nPrereqs: Closing right is given to an administrative person and not to the people signing off\\nApproach:\\n1. Subproject is executed as usual, then for sign-off is assigned to Person A\\n2. Sign-Off Person A enters a Workflow-Item validating the state and assigns Person B\\n3. Sign-Off Person B enters a Workflow-Item validating the state and assigns to administrative personell that has the close-rights\\n4. Administrative person checks whether all required sign-offs are there and then finally closes; otherwise assigns back\\n### Impact to child resources\\nA resource hierarchically describes a node in a tree-like structure. Therefore we need to look at impact of a closed resource to other nodes in the tree.\\nIf a resource has no children, there is no impact on other resources.\\nIf a resource has children (e.g. subprojects \/ workflowitems), **the resource cannot be closed**. All children need to be in state closed before a parent can be closed.\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n","Predictions":"### Lifetime of a single resourcenmermaidngraph TDnA [Create resource] -->|Auto-assign to creator| BnB[State: Open] --> |Do work|BnB --> |Assign to somebody who can close| CnC[State: Open & Ready to close] --> |Close| DnD[Resource closed]nRe-Opening is not supported; it could be discussed as long as the parent is still open (hierarchy must hold). Different usage-scenarios are acommodated via _manual_ steps.n#### Example: 4 Eyes PrinciplenScenario: A subproject must only be closed when 2 people A & B (e.g. different departments) checked it.nPrereqs: Closing right is given to an administrative person and not to the people signing offnApproach:n1. Subproject is executed as usual, then for sign-off is assigned to Person An2. Sign-off Person A enters a Workflow-Item validating the state and assigns Person Bn3. Sign-off Person B enters a Workflow-Item validating the state and assigns to administrative personell that has the close-rightsn4. Administrative person checks whether all required sign-offs are there and then finally closes; otherwise assigns backn### Impact to child resourcesnA resource hierarchically describes a node in a tree-like structure. Therefore we need to look at impact of a closed resource to other nodes in the tree.nIf a resource has no children, there is no impact on other resources.nIf a resource has children (e.g. subprojects \/ workflowi)nIf a resource has no children (e.g. subprojects \/ workflowi)nIf a resource has children (e.g. subprojects \/ workflowi)nIf a resource has no children (e.g. subprojects \/ workflowi)nIf a resource has children (e.g. subprojects \/ workflowi)nIf a resource has no children (e.g. subprojects \/ workflowi)nIf a resource has no children (e.g"}
{"File Name":"we-be-best\/1-use-both-pointer-and-mouse-events.md","Context":"## Context\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\n- MovementX\/Y is a clean browser only method for determining distance without having to track previous coordinates.\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\n","Decision":"All pointer event interactions have a mouse event fallback.\\n","tokens":102,"id":873,"text":"## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX\/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n\n\n##Decision\nAll pointer event interactions have a mouse event fallback.\\n","Predictions":"We will use a stack class to store the stack data.n"}
{"File Name":"Horace\/0012-use-network-storage-for-large-datafiles.md","Context":"## Context\nHorace and Herbert will require access to large `sqw` and sets of `nxspe` data files as source data and \"expected\" results for unit and system testing.\nThese data files are too large to store in GitHub along side the test code, but will not change frequently.\nSimilar data files are also required for Euphonic testing.\n","Decision":"The data will be stored in STFC hosted SAN (storage area network).\\nTests will read the data from this network storage location, either by copying the files locally or reading the remote file.\\n","tokens":78,"id":4234,"text":"## Context\\nHorace and Herbert will require access to large `sqw` and sets of `nxspe` data files as source data and \"expected\" results for unit and system testing.\\nThese data files are too large to store in GitHub along side the test code, but will not change frequently.\\nSimilar data files are also required for Euphonic testing.\\n\n\n##Decision\nThe data will be stored in STFC hosted SAN (storage area network).\\nTests will read the data from this network storage location, either by copying the files locally or reading the remote file.\\n","Predictions":"We will use a Status folder in the github.com\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/status\/tests\/tests\/status\/tests\/tests\/status\/tests\/tests\/status\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/tests\/"}
{"File Name":"kafkarator\/0003-kafkarator-api-is-focused-around-dedicated-crds.md","Context":"## Context\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\nWe need to make a decision, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\nFrom various discussions, we have a few findings that guide our decision:\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\n- We have two separate concerns that needs two separate configurations\n","Decision":"- We will define one new CRD object to configure topics and access to this\\n- App developers will create this in the cluster when deploying their application\\n- Kafkarator will watch this CRD and take needed actions\\n- App developers will add configuration to their Application resource listing kafka pools they need access to\\n","tokens":151,"id":2834,"text":"## Context\\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\\nWe need to make a decision, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\\nFrom various discussions, we have a few findings that guide our decision:\\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\\n- We have two separate concerns that needs two separate configurations\\n\n\n##Decision\n- We will define one new CRD object to configure topics and access to this\\n- App developers will create this in the cluster when deploying their application\\n- Kafkarator will watch this CRD and take needed actions\\n- App developers will add configuration to their Application resource listing kafka pools they need access to\\n","Predictions":"We will define one new CRD object to configure topics and access to thisn- App developers will create this in the cluster when deploying their applicationn- Kafkarator will watch this CRD and take needed actionsn- App developers will add configuration to their Application resource listing kafka pools they need access ton"}
{"File Name":"terraform-aws-s3-static-website-module\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3068,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"paas-csls-splunk-broker\/ADR001-syslog-http-to-csls-adapter.md","Context":"## Context\nWe want to provide a reliable method of shipping logs from applications (on\nGOV.UK Paas) to Splunk so they can take advantage of the log storage, analytics\nand protective monitoring provided by the GDS Cyber Security team.\nGDS Cyber Security maintain an [AWS Kinesis][kinesis] based log shipping stream\nthat accepts log events in the [AWS CloudWatch Logs][cloudwatch] format and\nqueues them for delivery to Splunk.\nGOV.UK PaaS supports forwarding log events from an application's stdout and\nstderr streams in [syslog format][syslog] via [syslog drains][drains].\nComponents such as [Fluentd][fluentd] are available that process and forwarding\nlogs from various sources to various targets, but their configuration can\nunweildly and hard to test.\nGOV.UK PaaS Tenants can run [sidecar][sidecar] containers to handle custom log\nshipping or instument their applications with [logging libraries that support\nmultiple transports][winston], but this brings extra complexity and doesn't\nmake use of supported logging infrastructure already provided by GOV.UK PaaS.\n","Decision":"We will build an \"adapter\" application to deploy alongside the CSLS logging\\npipeline that accepts requests in the \"syslog over http\" format exported by the\\nGOV.UK PaaS syslog drain system, translates them into the CloudWatch Logs\\nformat and forwards them on to the CSLS Kinesis stream.\\n","tokens":248,"id":3290,"text":"## Context\\nWe want to provide a reliable method of shipping logs from applications (on\\nGOV.UK Paas) to Splunk so they can take advantage of the log storage, analytics\\nand protective monitoring provided by the GDS Cyber Security team.\\nGDS Cyber Security maintain an [AWS Kinesis][kinesis] based log shipping stream\\nthat accepts log events in the [AWS CloudWatch Logs][cloudwatch] format and\\nqueues them for delivery to Splunk.\\nGOV.UK PaaS supports forwarding log events from an application's stdout and\\nstderr streams in [syslog format][syslog] via [syslog drains][drains].\\nComponents such as [Fluentd][fluentd] are available that process and forwarding\\nlogs from various sources to various targets, but their configuration can\\nunweildly and hard to test.\\nGOV.UK PaaS Tenants can run [sidecar][sidecar] containers to handle custom log\\nshipping or instument their applications with [logging libraries that support\\nmultiple transports][winston], but this brings extra complexity and doesn't\\nmake use of supported logging infrastructure already provided by GOV.UK PaaS.\\n\n\n##Decision\nWe will build an \"adapter\" application to deploy alongside the CSLS logging\\npipeline that accepts requests in the \"syslog over http\" format exported by the\\nGOV.UK PaaS syslog drain system, translates them into the CloudWatch Logs\\nformat and forwards them on to the CSLS Kinesis stream.\\n","Predictions":"We will build an \"adapter\" application to deploy alongside the CSLS loggingnpipeline that accepts requests in the [AWS Kinesis][kinesis] format exported by thenGOV.UK PaaS syslog drain system, translates them into the CloudWatch Logsnformat and forwards them on to the CSLS Kinesis stream.n"}
{"File Name":"meadow\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3854,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"dapr\/CLI-002-self-hosted-init-and-uninstall-behaviors.md","Context":"## Context\nChanges in behavior of `init` and `uninstall` on Self Hosted mode for. Discussed in this [issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\n","Decision":"* Calling `dapr init` will\\n* Install `daprd` binary in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Set up the `dapr_placement`, `dapr_redis` and `dapr_zipkin` containers.\\n* Create the default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Create the default components configurations for `pubsub.yaml`, `statestore.yaml` and `zipkin.yaml` in the default `components` folder.\\n* Create a default configuration file in `$HOME\/.dapr\/config.yaml` for Linx\/MacOS and `%USERPROFILE%\\.dapr\\config.yaml` for Windows for enabling tracing by default.\\n* Calling `dapr init --slim` will\\n* Install the binaries `daprd` and `placement` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Create an empty default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Calling `dapr uninstall` will\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker dapr_placement if Docker is installed.\\n* Calling `dapr uninstall --all`\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker containers dapr_placement, dapr_redis and dapr_zipkin if Docker is installed.\\n* Remove the default folder `$HOME\/.dapr` in Linux\/MacOS and `%USERPROFILE%\\.dapr` in Windows.\\n* CLI on the init command will fail if a prior installtion exists in the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* **There will no longer be an option for `--install-path` during init or during uninstall.**\\n* The `dapr` CLI by default will expect the `daprd` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows. The command `dapr run` will not expect the `daprd` binary to be in the `PATH` variable, it will launch the binary from the default path.\\n","tokens":41,"id":85,"text":"## Context\\nChanges in behavior of `init` and `uninstall` on Self Hosted mode for. Discussed in this [issue](https:\/\/github.com\/dapr\/cli\/issues\/411).\\n\n\n##Decision\n* Calling `dapr init` will\\n* Install `daprd` binary in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Set up the `dapr_placement`, `dapr_redis` and `dapr_zipkin` containers.\\n* Create the default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Create the default components configurations for `pubsub.yaml`, `statestore.yaml` and `zipkin.yaml` in the default `components` folder.\\n* Create a default configuration file in `$HOME\/.dapr\/config.yaml` for Linx\/MacOS and `%USERPROFILE%\\.dapr\\config.yaml` for Windows for enabling tracing by default.\\n* Calling `dapr init --slim` will\\n* Install the binaries `daprd` and `placement` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows.\\n* Create an empty default `components` folder in `$HOME\/.dapr\/bin` for Linux\/MacOS or `%USERPROFILE\\.dapr\\bin` for Windows.\\n* Calling `dapr uninstall` will\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker dapr_placement if Docker is installed.\\n* Calling `dapr uninstall --all`\\n* Remove the folder containing binaries `bin` the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* Remove the docker containers dapr_placement, dapr_redis and dapr_zipkin if Docker is installed.\\n* Remove the default folder `$HOME\/.dapr` in Linux\/MacOS and `%USERPROFILE%\\.dapr` in Windows.\\n* CLI on the init command will fail if a prior installtion exists in the default path `$HOME\/.dapr` for Linux\/MacOS and `%USERPROFILE%\\.dapr` for Windows.\\n* **There will no longer be an option for `--install-path` during init or during uninstall.**\\n* The `dapr` CLI by default will expect the `daprd` in `$HOME\/.dapr\/bin` for Linux\/MacOS and `%USERPROFILE%\\.dapr\\bin` for Windows. The command `dapr run` will not expect the `daprd` binary to be in the `PATH` variable, it will launch the binary from the default path.\\n","Predictions":"* Calling dapr init willn* Install daprd binary in $HOME\/.dapr\/bin for Linux\/MacOS and %USERPROFILE%.daprbin for Windows.n* Set up the dapr_placement, dapr_redis and dapr_zipkin containers.n* Create the default components folder in $HOME\/.dapr\/bin for Linux\/MacOS or %USERPROFILE%.daprbin for Windows.n* Create the default components configurations for pubsub.yaml, statestore.yaml and zipkin.yaml in the default components folder.n* Create an empty default components configuration file in $HOME\/.dapr\/bin for Linux\/MacOS or %USERPROFILE%.daprbin for Windows.n* Create a default configuration file in $HOME\/.dapr\/config.yaml for Linx\/MacOS and %USERPROFILE%.daprconfig.yaml for Windows for enabling tracing by default.n* Calling dapr init --slim willn* Install the binaries daprd and placement in $HOME\/.dapr\/bin for Linux\/MacOS and %USERPROFILE%.daprbin for Windows.n* Create an empty default components folder in $HOME\/.dapr\/bin for Linux\/MacOS or %USERPROFILE%.daprbin for Windows.n* Create an empty default components folder in $HOME\/.dapr\/bin for Linux\/MacOS or %USERPROFILE.daprbin for Windows.n* Calling dapr uninstall willn* Remove the folder containing binaries bin the default"}
{"File Name":"operational-data-hub\/0002-use-serverless-infra-components.md","Context":"## Context\nServerless computing offers a number of advantages over traditional cloud-based or server-centric infrastructure. For many developers, serverless architectures offer greater scalability, more flexibility, and quicker time to release, all at a reduced cost. With serverless architectures, developers do not need to worry about purchasing, provisioning, and managing backend servers.\n","Decision":"We will use serverless infrastructure components where possible.\\n","tokens":68,"id":2723,"text":"## Context\\nServerless computing offers a number of advantages over traditional cloud-based or server-centric infrastructure. For many developers, serverless architectures offer greater scalability, more flexibility, and quicker time to release, all at a reduced cost. With serverless architectures, developers do not need to worry about purchasing, provisioning, and managing backend servers.\\n\n\n##Decision\nWe will use serverless infrastructure components where possible.\\n","Predictions":"We will use a Scala approach to deploy the application.n"}
{"File Name":"libelektra\/highlevel_help_message.md","Context":"## Problem\nWe want to allow to print the help message no matter what errors happened in `kdbOpen` or `kdbGet`.\n","Decision":"Ignore missing `require`d keys (in help mode), but fail for every other error.\\n","tokens":31,"id":1274,"text":"## Problem\\nWe want to allow to print the help message no matter what errors happened in `kdbOpen` or `kdbGet`.\\n\n\n##Decision\nIgnore missing `require`d keys (in help mode), but fail for every other error.\\n","Predictions":"We will use stack to handle the stack_name and stack_name commands.n"}
{"File Name":"govuk-aws\/0012-security-groups-in-terraform.md","Context":"## Context\nThere are two methods of defining security groups for AWS in Terraform and they are distinguished by how you add rules: in-line and separate. Using in-line rules keeps the definition close to the resource but when ever a rule is changed Terraform will re-generate the entire resource. Using separate rules Terraform will only make the single rule change but there is greater boilerplate and separation between the group resource and the rule resource.\n","Decision":"Security groups will be defined separate to their rules.\\nAdditionally each security group will be defined, in its entirety, in a single file.\\n","tokens":89,"id":4032,"text":"## Context\\nThere are two methods of defining security groups for AWS in Terraform and they are distinguished by how you add rules: in-line and separate. Using in-line rules keeps the definition close to the resource but when ever a rule is changed Terraform will re-generate the entire resource. Using separate rules Terraform will only make the single rule change but there is greater boilerplate and separation between the group resource and the rule resource.\\n\n\n##Decision\nSecurity groups will be defined separate to their rules.\\nAdditionally each security group will be defined, in its entirety, in a single file.\\n","Predictions":"For now instead of an Event Number as the sort key we will introduce a UUID EventId. And use StreamName as the HASH key. StreamNames will need to be unique anyway.n"}
{"File Name":"educational-platform\/0002-integration-events-implementation.md","Context":"## Context\nFor implementing event-driven application, the platform should be available for communication with integration events.\n","Decision":"We will start from standard Spring Events classes: ApplicationListener, ApplicationEvent without dependency to external middleware component. We can add custom features to Spring functionality when it's needed.\\ntodo: For now, all events will be stored in integration-events module. But this solution should be reviewed. Integration events should be published after successful transaction.\\n- https:\/\/devblogs.microsoft.com\/cesardelatorre\/domain-events-vs-integration-events-in-domain-driven-design-and-microservices-architectures\/\\n","tokens":21,"id":2499,"text":"## Context\\nFor implementing event-driven application, the platform should be available for communication with integration events.\\n\n\n##Decision\nWe will start from standard Spring Events classes: ApplicationListener, ApplicationEvent without dependency to external middleware component. We can add custom features to Spring functionality when it's needed.\\ntodo: For now, all events will be stored in integration-events module. But this solution should be reviewed. Integration events should be published after successful transaction.\\n- https:\/\/devblogs.microsoft.com\/cesardelatorre\/domain-events-vs-integration-events-in-domain-driven-design-and-microservices-architectures\/\\n","Predictions":"We will start from standard Spring Events classes: ApplicationListener, ApplicationEvent without dependency to external middleware component. We can add custom features to Spring functionality when it's needed.ntodo: For now, all events will be stored in integration-events module. But this solution should be reviewed. Integration events should be published after successful transaction.n- https:\/\/devblogs.microsoft.com\/cesardelatorre\/domain-events-vs-integration-events-in-domain-driven-design-and-microservices-architectures\/n"}
{"File Name":"registraties\/001-use-existing-apis.md","Context":"## Context\nThe Registraties project will need to contain a page that will show data per address (or verblijfsobject) from existing APIs, like [Basisregistraties adressen en gebouwen (BAG)](https:\/\/api.data.amsterdam.nl\/bag\/), [Basisregistratie kadaster (BRK)](https:\/\/api.data.amsterdam.nl\/brk\/) and [Handelsregister (HR)](https:\/\/api.data.amsterdam.nl\/handelsregister). All required data is readily available.\n","Decision":"The project's MVP, which will contain a simple search field with auto-suggest functionality, get its data from existing API endpoints and therefore doesn't need its own back-end setup with API. In the future it might, but for the initial phase, the front-end code is capable of retrieving, formatting and showing the data from [api.data.amsterdam.nl](https:\/\/api.data.amsterdam.nl).\\nThe application will have container components that have sagas injected into them. Each saga is responsible for retrieving data from a single endpoint. If the situation calls for it, a saga can retrieve data from more than one endpoint.\\n","tokens":112,"id":1122,"text":"## Context\\nThe Registraties project will need to contain a page that will show data per address (or verblijfsobject) from existing APIs, like [Basisregistraties adressen en gebouwen (BAG)](https:\/\/api.data.amsterdam.nl\/bag\/), [Basisregistratie kadaster (BRK)](https:\/\/api.data.amsterdam.nl\/brk\/) and [Handelsregister (HR)](https:\/\/api.data.amsterdam.nl\/handelsregister). All required data is readily available.\\n\n\n##Decision\nThe project's MVP, which will contain a simple search field with auto-suggest functionality, get its data from existing API endpoints and therefore doesn't need its own back-end setup with API. In the future it might, but for the initial phase, the front-end code is capable of retrieving, formatting and showing the data from [api.data.amsterdam.nl](https:\/\/api.data.amsterdam.nl).\\nThe application will have container components that have sagas injected into them. Each saga is responsible for retrieving data from a single endpoint. If the situation calls for it, a saga can retrieve data from more than one endpoint.\\n","Predictions":"The decision is to use the [Security Manager](https:\/\/docs.aws.amazon.com\/systems-manager\/latest\/userguide\/systems-manager-parameter-store.html) for the storage of secrets.n"}
{"File Name":"bosh-bootloader\/0002-replace-go-bindata-with-packr2.md","Context":"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\nThe original author of go-bindata delete their account and then the repo was\nrecreated under a different owner. The dependency has shifted around enough that\nwe have lost faith in the intention of the maintainers\n[more details here](https:\/\/twitter.com\/francesc\/status\/961249107020001280?lang=en)\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\nduring a test run or final build) made it hard to reason about.\n","Decision":"Use [Packr2](https:\/\/github.com\/gobuffalo\/packr\/tree\/master\/v2) instead.\\n","tokens":126,"id":3873,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThe original author of go-bindata delete their account and then the repo was\\nrecreated under a different owner. The dependency has shifted around enough that\\nwe have lost faith in the intention of the maintainers\\n[more details here](https:\/\/twitter.com\/francesc\/status\/961249107020001280?lang=en)\\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\\nduring a test run or final build) made it hard to reason about.\\n\n\n##Decision\nUse [Packr2](https:\/\/github.com\/gobuffalo\/packr\/tree\/master\/v2) instead.\\n","Predictions":"We will use [Packr2](https:\/\/github.com\/gobuffalo\/packr\/tree\/master\/v2) instead.n"}
{"File Name":"cloud-platform\/005-ECR-monitoring-and-alerting.md","Context":"## Context\nWe use ECR as the Docker container registry that makes it easy for users to store, manage, and deploy Docker container images.\nDue to some applications having a constant rate of images being pushed to their ECR repo, we found that the AWS limit of 1000 images was being hit by some teams. To avoid this we had implemented a lifecycle policy of *100 images* per ECR repo. ECR repositories created for use in the Cloud Platform will have this default lifecycle policy applied.\nAs lifecycle policy will only keep 100 most recent versions of an image and silently delete images, application users raised an [issue][user-issue] on imposing any limit on number of images is potentially dangerous, unless teams have their own clean-up mechanism.\n","Decision":"After discussing with application teams and consideration of possible options, the decision has been made to remove the lifecycle policy altogether, but adding monitoring and alerting such that we can take action before an ECR runs out of space.\\nAs it is to do with metrics & alerts, since prometheus is our monitoring solution we decided to use prometheus for metrics & alerts.\\n","tokens":155,"id":629,"text":"## Context\\nWe use ECR as the Docker container registry that makes it easy for users to store, manage, and deploy Docker container images.\\nDue to some applications having a constant rate of images being pushed to their ECR repo, we found that the AWS limit of 1000 images was being hit by some teams. To avoid this we had implemented a lifecycle policy of *100 images* per ECR repo. ECR repositories created for use in the Cloud Platform will have this default lifecycle policy applied.\\nAs lifecycle policy will only keep 100 most recent versions of an image and silently delete images, application users raised an [issue][user-issue] on imposing any limit on number of images is potentially dangerous, unless teams have their own clean-up mechanism.\\n\n\n##Decision\nAfter discussing with application teams and consideration of possible options, the decision has been made to remove the lifecycle policy altogether, but adding monitoring and alerting such that we can take action before an ECR runs out of space.\\nAs it is to do with metrics & alerts, since prometheus is our monitoring solution we decided to use prometheus for metrics & alerts.\\n","Predictions":"We have decided to use [Azure Cloud Platform](https:\/\/azurecloudplatform.com\/) as the email service for the application.n"}
{"File Name":"community\/dr-013-Allowed_UI_technology_in_the_Console_UI.md","Context":"## Context\nThe Console UI is the administration view of Kyma and is a core part of the product.\nTo simplify the development process in Kyma, the number of used programming languages and frameworks is limited. It makes the collaboration easier and allows the developers to specialize in a chosen technology.\nFollowing this principle, the Go language is used as the only programming language for the back-end functionality of the core Kyma features.\nBy design, the Console UI can be extended using plugins written in any framework, developed either internally or by customers or partners. The interface between the Console UI and the plugins is based on HTML and JavaScript but the approach is to provide libraries for the most popular and well-adapted UI frameworks, and gather knowledge and experience in using them.\n","Decision":"The decision is to allow the [React](https:\/\/reactjs.org) and [Angular](https:\/\/angular.io) frameworks in combination with the JavaScript and TypeScript programming languages to build the core views in the Kyma Console UI.\\n","tokens":159,"id":3460,"text":"## Context\\nThe Console UI is the administration view of Kyma and is a core part of the product.\\nTo simplify the development process in Kyma, the number of used programming languages and frameworks is limited. It makes the collaboration easier and allows the developers to specialize in a chosen technology.\\nFollowing this principle, the Go language is used as the only programming language for the back-end functionality of the core Kyma features.\\nBy design, the Console UI can be extended using plugins written in any framework, developed either internally or by customers or partners. The interface between the Console UI and the plugins is based on HTML and JavaScript but the approach is to provide libraries for the most popular and well-adapted UI frameworks, and gather knowledge and experience in using them.\\n\n\n##Decision\nThe decision is to allow the [React](https:\/\/reactjs.org) and [Angular](https:\/\/angular.io) frameworks in combination with the JavaScript and TypeScript programming languages to build the core views in the Kyma Console UI.\\n","Predictions":"The decision is to allow the [React](https:\/\/reactjs.org) and [Angular](https:\/\/angular.io) frameworks in combination with the JavaScript and TypeScript programming languages to build the core views in the Kyma Console UI.n"}
{"File Name":"architectural-decision-log\/0005-convention-based-logging-classes.md","Context":"## Context and Problem Statement\nHamlet Deploy as a product is made up from several \"components\". They are not written in the same coding\/scripting language and so their outputs - particularly when it comes to log behaviour and error messages - are quite different from each other. When an error or debug message is received, how can the end-user and the Hamlet Deploy developer both gain greater insight from the message that is received?\n## Decision Drivers <!-- optional -->\n* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\n* It must be simple to maintain\n","Decision":"* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\nChosen option:  _\"Use Logging Codes Following a Numerical Convention\"_ with _\"Per-Component Code Range\"_\\nIt was deemed easier to implement and there seemed little benefit and greater complexity to implementing an inheritance-based approach.\\nA code range allows each component to implement the definitions in the way that most makes sense to it.\\n","tokens":122,"id":2975,"text":"## Context and Problem Statement\\nHamlet Deploy as a product is made up from several \"components\". They are not written in the same coding\/scripting language and so their outputs - particularly when it comes to log behaviour and error messages - are quite different from each other. When an error or debug message is received, how can the end-user and the Hamlet Deploy developer both gain greater insight from the message that is received?\\n## Decision Drivers <!-- optional -->\\n* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\n\n\n##Decision\n* A solution to this issue must be possible in Bash, Python and Freemarker (Java)\\n* It must be simple to maintain\\nChosen option:  _\"Use Logging Codes Following a Numerical Convention\"_ with _\"Per-Component Code Range\"_\\nIt was deemed easier to implement and there seemed little benefit and greater complexity to implementing an inheritance-based approach.\\nA code range allows each component to implement the definitions in the way that most makes sense to it.\\n","Predictions":"* A solution to this issue must be possible in Bash, Python and Freemarker (Java)n* It must be simple to maintainnChosen option: _\"Use Logging Codes Following a Numerical Convention\"_ with _\"Per-Component Code Range\"__nIt was deemed easier to implement and there seemed little benefit and greater complexity to implementing an inheritance-based approach.nA code range allows each component to implement the definitions in the way that most makes sense to it.n"}
{"File Name":"opg-use-an-lpa\/0008-session-storage-using-an-encrypted-cookie.md","Context":"## Context\n* We will be storing a very small amount of data in the session.\n* Whilst the above holds true we can avoid additional infrastructure by using client side storage.\n* The session _may_ hold somewhat sensitive details (e.g. an LPA Share code), thus its content is secret.\n* As the cookie is client side, we also need authentication to ensure the message isn't tempered with.\n","Decision":"To use a cookie who's payload is encrypted with AES GCM. This provides secrecy and authentication.\\nNot to use JWT, because:\\n* To ensure message secrecy, additional libraries are needed.\\n* The resulting cookie value is significantly larger.\\n* Concerns over the general suitability around using JWT for client side sessions.\\n","tokens":86,"id":4843,"text":"## Context\\n* We will be storing a very small amount of data in the session.\\n* Whilst the above holds true we can avoid additional infrastructure by using client side storage.\\n* The session _may_ hold somewhat sensitive details (e.g. an LPA Share code), thus its content is secret.\\n* As the cookie is client side, we also need authentication to ensure the message isn't tempered with.\\n\n\n##Decision\nTo use a cookie who's payload is encrypted with AES GCM. This provides secrecy and authentication.\\nNot to use JWT, because:\\n* To ensure message secrecy, additional libraries are needed.\\n* The resulting cookie value is significantly larger.\\n* Concerns over the general suitability around using JWT for client side sessions.\\n","Predictions":"To use a cookie who's payload is encrypted with AES GCM. This provides secrecy and authentication.nNot to use JWT, because:n* To ensure message secrecy, additional libraries are needed.n* The resulting cookie value is significantly larger.nChosen option: **Option A**, because:n* To ensure message secrecy, additional libraries are needed.n* The resulting cookie value is significantly larger.nChosen option: **Option B**, because:n* To ensure message secrecy, additional libraries are needed.n* The resulting cookie value is significantly larger.nChosen option: **Option C**, because:n* To ensure message secrecy, additional libraries are needed.n* The resulting cookie value is significantly larger.nChosen option: **Option D**, because:n* To ensure message secrecy, additional libraries are needed.n* The resulting cookie value is significantly larger.nChosen option: **Option D**, because:n* To ensure message secrecy, additional libraries are needed.n* The resulting cookie value is significantly larger.nChosen option: **Option D**, because:n* To ensure message secrecy, additional libraries are needed.n* The resulting cookie value is significantly larger.nChosen option: **Option D**, because:n* To ensure message secrecy, additional libraries are needed.n* The resulting cookie value is significantly larger.nChosen option: **Option D**, because:n* To ensure message secrecy, additional libraries are needed.n* The resulting cookie value is significantly larger.nChosen option: **Option D**, because:n* To ensure message secrecy, additional libraries are needed.n* The resulting cookie value is significantly larger.nChosen option: **Option D**, because:n* To ensure message secrecy, additional libraries are needed.n* The resulting cookie value is significantly larger.nChosen option: **Option D**, because:n* To ensure message secrecy, additional libraries are needed.n* The resulting cookie value is significantly larger.nChosen option: **Option D**, because:n* To ensure message secrecy, additional libraries are needed.n"}
{"File Name":"meadow\/0011-yarn.md","Context":"## Context\nWe discussed the relative merits of different JS package managers.\n","Decision":"Use `yarn` instead of `npm` in all dev, test, and build environments.\\n","tokens":15,"id":3827,"text":"## Context\\nWe discussed the relative merits of different JS package managers.\\n\n\n##Decision\nUse `yarn` instead of `npm` in all dev, test, and build environments.\\n","Predictions":"We will use Protobuf, because it comes out best (see below).n### Positive Consequencesn- We need to start defining interfaces outside of their implementations.n- We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example, use POST \/get_projects BODY  \"index\": \"name\"  instead of GET \/projects?index=name.n### Negative Consequencesn- The team needs to learn a new language.n- Follow the following steps:n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](https:\/\/github.com\/protobuf\/protobuf)n- [Protobuf](http"}
{"File Name":"runbooks-mkdocs\/0003-use-plantuml-for-diagramming.md","Context":"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":1145,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","Predictions":"The change that we're proposing or have agreed to implement.n"}
{"File Name":"apply-for-teacher-training\/0010-cookie-sessions.md","Context":"## Context\nThis application needs user sessions to allow users (candidates, providers, referees, support staff) to sign in. Rails offers a number of options for this. Each have a trade-off.\n### Cookie sessions\nAll session information is saved in a cookie. The cookie is encrypted to avoid the user changing or reading the data.\nPro:\n- It does not interact with other infrastructure\n- It's the Rails default, well understood by developers\nCon:\n- When the user signs out, we do not invalidate the session. This means that if the user has made a copy of the cookie, they (or an attacker) can sign themselves back in.\n- Users cannot sign out sessions on other devices (remote sign out)\n### Storage based cookies\nThis mechanism relies on a session ID being saved in a cookie. The session ID corresponds to a record either in a traditional database (PostgreSQL in our case) or in a caching service (Memcached, Redis).\nPro:\n- On sign out, the session is deleted and cannot be revived\n- Sessions can be invalidated \"remotely\", to allow sign out of other devices\nCon:\n- Uses other infrastructure - slight performance overhead, risk of services being unavailable\n- Sensitive data is stored in a database\n","Decision":"Use session cookies.\\n","tokens":264,"id":4011,"text":"## Context\\nThis application needs user sessions to allow users (candidates, providers, referees, support staff) to sign in. Rails offers a number of options for this. Each have a trade-off.\\n### Cookie sessions\\nAll session information is saved in a cookie. The cookie is encrypted to avoid the user changing or reading the data.\\nPro:\\n- It does not interact with other infrastructure\\n- It's the Rails default, well understood by developers\\nCon:\\n- When the user signs out, we do not invalidate the session. This means that if the user has made a copy of the cookie, they (or an attacker) can sign themselves back in.\\n- Users cannot sign out sessions on other devices (remote sign out)\\n### Storage based cookies\\nThis mechanism relies on a session ID being saved in a cookie. The session ID corresponds to a record either in a traditional database (PostgreSQL in our case) or in a caching service (Memcached, Redis).\\nPro:\\n- On sign out, the session is deleted and cannot be revived\\n- Sessions can be invalidated \"remotely\", to allow sign out of other devices\\nCon:\\n- Uses other infrastructure - slight performance overhead, risk of services being unavailable\\n- Sensitive data is stored in a database\\n\n\n##Decision\nUse session cookies.\\n","Predictions":"We will use a [JSON Web Token](https:\/\/jwt.io) to authenticate users.n"}
{"File Name":"Head-Start-TTADP\/0006-database.md","Context":"## Context\nData storage and management is a key component of the tta smarthub platform. At a fundamental level, a relational as well as NoSQL systems were considered. Because the platform's data is mainly structured  and with a need for frequent queries and reports, a relational database management system was viewed as more suitable. With that in mind we looked at MySQL and PostreSQL (Postgres) both open source and popular choices.\n","Decision":"While both databases provide adequate storage and management, especially with updates provided by version 8 of MySQL, in the end Postgres was chosen. The main advantages of Postgres are implementations leading to better concurrency, specifically the MVCC (Multiversion Concurrency Control) without the need to use read locks, transactional ACID (Atomicity, Consistency, Isolation, Durability) support making the system less vulnerable to data corruption. Postgres also supports partial indexes, can create indexes in a non-blocking way, multiple cores, GIN\/GIST indexing accelerating full-text searches.\\n","tokens":90,"id":1167,"text":"## Context\\nData storage and management is a key component of the tta smarthub platform. At a fundamental level, a relational as well as NoSQL systems were considered. Because the platform's data is mainly structured  and with a need for frequent queries and reports, a relational database management system was viewed as more suitable. With that in mind we looked at MySQL and PostreSQL (Postgres) both open source and popular choices.\\n\n\n##Decision\nWhile both databases provide adequate storage and management, especially with updates provided by version 8 of MySQL, in the end Postgres was chosen. The main advantages of Postgres are implementations leading to better concurrency, specifically the MVCC (Multiversion Concurrency Control) without the need to use read locks, transactional ACID (Atomicity, Consistency, Isolation, Durability) support making the system less vulnerable to data corruption. Postgres also supports partial indexes, can create indexes in a non-blocking way, multiple cores, GIN\/GIST indexing accelerating full-text searches.\\n","Predictions":"While both databases provide adequate storage and management, especially with updates provided by version 8 of MySQL, in the end Postgres was chosen. The main advantages of Postgres are implementations leading to better concurrency, specifically the MVCC (Multiversion Concurrency Control) without the need to use read locks, transactional ACID (Atomicity, Consistency, Isolation, Durability) support making the system less vulnerable to data corruption. Postgres also supports partial indexes, can create indexes in a non-blocking way, multiple cores, GIN\/GIST indexing accelerating full-text searches.n"}
{"File Name":"cafebabel.com\/0005-flask-architecture.md","Context":"## Context\nFlask allows you to do whatever you want on your app\/folders\/templates\/etc\nand it can quickly become messy.\nPlus, to deal with configurations within extensions is harder if you do not\ndeclare the Flask app once and for all then load the configuration.\n","Decision":"An architecture based on https:\/\/github.com\/stephane\/flask-skeleton\\n","tokens":58,"id":3237,"text":"## Context\\nFlask allows you to do whatever you want on your app\/folders\/templates\/etc\\nand it can quickly become messy.\\nPlus, to deal with configurations within extensions is harder if you do not\\ndeclare the Flask app once and for all then load the configuration.\\n\n\n##Decision\nAn architecture based on https:\/\/github.com\/stephane\/flask-skeleton\\n","Predictions":"* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.n* The feature configuration from Feature::getAll() is converted to a SCSS map inside ShopwareStorefrontThemeThemeCompiler::getFeatureConfigScssMap.n* This SCSS map is always added to the SCSS string which gets processed by ShopwareStorefrontThemeThemeCompiler::compileTheme.n* For webpack hot-proxy the var\/config_js_features.json is used instead.n* The SCSS map looks like this: $sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);n* See https:\/\/sass-lang.com\/documentation\/values\/mapsn* A globally available function feature() is used to read inside the SCSS map if a desired feature is active.nExample:nscssnbody nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"nada-kafkarator\/0002-use-python.md","Context":"## Context\nMany k8s operators use Golang, but none of us working on this project are proficient with Golang.\nAnother option was Rust, which could be a useful learning exercise.\nPython is used in FIAAS, so we have some experience using it for an operator.\n","Decision":"We will use Python for Kafkarator.\\n","tokens":61,"id":861,"text":"## Context\\nMany k8s operators use Golang, but none of us working on this project are proficient with Golang.\\nAnother option was Rust, which could be a useful learning exercise.\\nPython is used in FIAAS, so we have some experience using it for an operator.\\n\n\n##Decision\nWe will use Python for Kafkarator.\\n","Predictions":"I decided to use React and Redux as the base for this project.n"}
{"File Name":"winery\/0027-use-dasherization-for-filenames.md","Context":"## Context and Problem Statement\nGraphics files have to take a consistent file name\n## Decision Drivers <!-- optional -->\n* Easy to process by Jekyll\n* No WTFs at the creators\n","Decision":"* Easy to process by Jekyll\\n* No WTFs at the creators\\nChosen option: \"Dasherization\", because\\n* clear separation of parts of the name\\n* consistent to other URLs (which are typically lowercase)\\n","tokens":40,"id":4308,"text":"## Context and Problem Statement\\nGraphics files have to take a consistent file name\\n## Decision Drivers <!-- optional -->\\n* Easy to process by Jekyll\\n* No WTFs at the creators\\n\n\n##Decision\n* Easy to process by Jekyll\\n* No WTFs at the creators\\nChosen option: \"Dasherization\", because\\n* clear separation of parts of the name\\n* consistent to other URLs (which are typically lowercase)\\n","Predictions":"* Easy to usen* Easy to usenChosen option: \"[RabbitMQ](TODO) with [MQTT plugin](https:\/\/www.rabbitmq.com\/mqtt.html)**, because:n- It is already well-known among the teamn- It has some [existing \"official\" Helm chart](https:\/\/github.com\/helm\/charts\/tree\/master\/stable\/rabbitmq)n- It seems like a good fit to iterate fastnWe **do not exclude switching to another MQTT broker such as VerneMQ in the future**, depending on our ability to dsitribute it cleanly on Kubernetes.nWe also **do not exclude using another broker such as Kafka or NATS for appropriate use cases**.n"}
{"File Name":"libelektra\/null_pointer_checks.md","Context":"## Problem\nCurrently all functions do proper argument checking which might degrade\nperformance.\n","Decision":"Rejected (keep checks) due to time constraints\\n","tokens":17,"id":1317,"text":"## Problem\\nCurrently all functions do proper argument checking which might degrade\\nperformance.\\n\n\n##Decision\nRejected (keep checks) due to time constraints\\n","Predictions":"Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.n"}
{"File Name":"dos-server\/adr-1-metadata.md","Context":"## Context\nDigital objects need to have associated metadata for various use cases (refer to\nthe requirements documentation for details). Metadata of these objects can be descriptive, administrative, and structural.\nTo avoid \"duplication\" of descriptive metadata, it is desired that DOS not store descriptive metadata.\n","Decision":"Descriptive metadata will not be stored by DOS.\\n","tokens":59,"id":3372,"text":"## Context\\nDigital objects need to have associated metadata for various use cases (refer to\\nthe requirements documentation for details). Metadata of these objects can be descriptive, administrative, and structural.\\nTo avoid \"duplication\" of descriptive metadata, it is desired that DOS not store descriptive metadata.\\n\n\n##Decision\nDescriptive metadata will not be stored by DOS.\\n","Predictions":"We will use a [semantic URI](https:\/\/tools.ietf.org\/html\/rfc4151) to define a Uniform Resource Identifier (URI) for all data types on the ODH platform.n"}
{"File Name":"embvm-core\/0021-eliminate-name-variables-from-core-base-classes.md","Context":"## Context\nUnder the initial design of the `embvm-core`, the following types stored a name in the base class:\n- Driver\n- Processor\n- Hardware Platform\n- Platform\nIn some cases, like drivers, this data was duplicated (through the driver registry) or unnecessary (because you use the default name and provide the system name in the driver registry). We don't need to store two different instances of the same name!\nIn other cases, like the processor and platforms, these names were never used.\n","Decision":"These names will no longer be required by the base classes.\\n","tokens":107,"id":3023,"text":"## Context\\nUnder the initial design of the `embvm-core`, the following types stored a name in the base class:\\n- Driver\\n- Processor\\n- Hardware Platform\\n- Platform\\nIn some cases, like drivers, this data was duplicated (through the driver registry) or unnecessary (because you use the default name and provide the system name in the driver registry). We don't need to store two different instances of the same name!\\nIn other cases, like the processor and platforms, these names were never used.\\n\n\n##Decision\nThese names will no longer be required by the base classes.\\n","Predictions":"We will keep the Processor as a Domain Service, because the three different kinds of execution we want to support really only differ for how data is handled in memory, and the domain doesn't concern itself with what is contained in memory (it doesn't even mention memory, since it's a detail of the interpreter, and we might as well use interpreters that don't use any memory):n- in the classic virtual machine, the Program is the entire program, that is executed from start to endn- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor: this is necessary because we might have defined procedures that we want to call in the future; the state of memory is saved between different executions (this is the case for the Processor and Processor):n- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor: this is necessary because we might have defined procedures that we want to call in the future; the state of memory is saved between different executions (this is the case for the Processor and Processor):n- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor: this is necessary because we might have defined procedures that we want to call in the future; the state of memory is saved between different executions (this is the case for the Processor and Processor):n- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor: this is necessary because we might have defined procedures that we want to call in the future; the state of memory is saved between different executions (this is the case for the Processor and Processor):n- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor: this is necessary because we might have defined procedures that we want to call in the future; the state of memory is saved between different executions (this is the case for the Processor and Processor):n- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor: this is necessary because we might have defined procedures that we want to call in the future; the state of memory is saved between different executions (this is the case for the Processor and Processor):n- in the"}
{"File Name":"connaisseur\/ADR-1_bootstrap-sentinel.md","Context":"## Context\nConnaisseur's main components are a MutatingWebhookConfiguration and the Connaisseur Pods. The MutatingWebhookConfiguration intercepts requests to create or update Kubernetes resources and forwards them to the Connaisseur Pods tasked, on a high level, with verifying trust data. The order of deploying both components matters, since a blocking MutatingWebhookConfiguration without the Connaisseur Pods to answer its requests would also block the deployment of said Pods.\nIn [#3](https:\/\/github.com\/sse-secure-systems\/connaisseur\/issues\/3) it was noted that prior to version 1.1.5 of Connaisseur when looking at the `Ready` status of Connaisseur Pods, they could report `Ready` while being non-functional due to the MutatingWebhookConfiguration missing. However, as stated above the MutatingWebhookConfiguration can only be deployed _after_ the Connaisseur Pods, which was solved by checking the `Ready` state of said Pods. If one were to add a dependency to this `Ready` state, such that it only shows `Ready` when the MutatingWebhookConfiguration exists, we run into a deadlock, where the MutatingWebhookConfiguration waits for the Pods and the Pods wait for the MutatingWebhookConfiguration.\n","Decision":"We chose option 1 over option 2, because it was important to us that a brief glance at Connaisseur's Namespace allows one to judge whether it is running properly. Option 3 was not chosen as the readiness status of Pods can be easily seen from the Service, whereas the health status would require querying every single Pod individually. We deemed that to be a very ugly, non-kubernetes-y solution and hence decided against it.\\n### Positive consequences\\nIf the Connaisseur Pods report `Ready` during the `connaisseur-bootstrap-sentinel`'s runtime, the MutatingWebhookConfiguration will be deployed by Helm. Otherwise, the Helm deployment will fail after its timeout period (default: 5min), since there won't be a running `connaisseur-bootstrap-sentinel` Pod anymore that resolves the installation deadlock. The Connaisseur Pods will never reach the `Ready` state and the MutatingWebhookConfiguration never gets deployed. This means, we get consistent deployment failures after the inital waiting period if something did not work out. Additionally, if the MutatingWebhookConfiguration gets removed for whatever reason during operation, Connaisseur Pods will be failing, indicating their failed dependency. Hence, monitoring the Connaisseur Pods is sufficient to ensure their working.\\n### Negative consequences\\nOn the other hand, if an adversary can deploy a Pod named `connaisseur-bootstrap-sentinel` to Connaisseur's Namespace, the Connaisseur Pods will always show `Ready` regardless of the MutatingWebhookConfiguration. However, if an adversary can deploy to Connaisseur's Namespace, chances are Connaisseur can be compromised anyways. More importantly, if not a single Connaisseur Pod is successfully deployed or if the notary healthcheck fails during the sentinel's lifetime, then the deployment will fail regardless of possible recovery at a later time. Another issue would be the `connaisseur-bootstrap-sentinel` Pod being left behind, however since it has a very limited use case we can also clean it up during the deployment, so apart from the minimal additional complexity of the deployment this is a non-issue.\\n","tokens":275,"id":4395,"text":"## Context\\nConnaisseur's main components are a MutatingWebhookConfiguration and the Connaisseur Pods. The MutatingWebhookConfiguration intercepts requests to create or update Kubernetes resources and forwards them to the Connaisseur Pods tasked, on a high level, with verifying trust data. The order of deploying both components matters, since a blocking MutatingWebhookConfiguration without the Connaisseur Pods to answer its requests would also block the deployment of said Pods.\\nIn [#3](https:\/\/github.com\/sse-secure-systems\/connaisseur\/issues\/3) it was noted that prior to version 1.1.5 of Connaisseur when looking at the `Ready` status of Connaisseur Pods, they could report `Ready` while being non-functional due to the MutatingWebhookConfiguration missing. However, as stated above the MutatingWebhookConfiguration can only be deployed _after_ the Connaisseur Pods, which was solved by checking the `Ready` state of said Pods. If one were to add a dependency to this `Ready` state, such that it only shows `Ready` when the MutatingWebhookConfiguration exists, we run into a deadlock, where the MutatingWebhookConfiguration waits for the Pods and the Pods wait for the MutatingWebhookConfiguration.\\n\n\n##Decision\nWe chose option 1 over option 2, because it was important to us that a brief glance at Connaisseur's Namespace allows one to judge whether it is running properly. Option 3 was not chosen as the readiness status of Pods can be easily seen from the Service, whereas the health status would require querying every single Pod individually. We deemed that to be a very ugly, non-kubernetes-y solution and hence decided against it.\\n### Positive consequences\\nIf the Connaisseur Pods report `Ready` during the `connaisseur-bootstrap-sentinel`'s runtime, the MutatingWebhookConfiguration will be deployed by Helm. Otherwise, the Helm deployment will fail after its timeout period (default: 5min), since there won't be a running `connaisseur-bootstrap-sentinel` Pod anymore that resolves the installation deadlock. The Connaisseur Pods will never reach the `Ready` state and the MutatingWebhookConfiguration never gets deployed. This means, we get consistent deployment failures after the inital waiting period if something did not work out. Additionally, if the MutatingWebhookConfiguration gets removed for whatever reason during operation, Connaisseur Pods will be failing, indicating their failed dependency. Hence, monitoring the Connaisseur Pods is sufficient to ensure their working.\\n### Negative consequences\\nOn the other hand, if an adversary can deploy a Pod named `connaisseur-bootstrap-sentinel` to Connaisseur's Namespace, the Connaisseur Pods will always show `Ready` regardless of the MutatingWebhookConfiguration. However, if an adversary can deploy to Connaisseur's Namespace, chances are Connaisseur can be compromised anyways. More importantly, if not a single Connaisseur Pod is successfully deployed or if the notary healthcheck fails during the sentinel's lifetime, then the deployment will fail regardless of possible recovery at a later time. Another issue would be the `connaisseur-bootstrap-sentinel` Pod being left behind, however since it has a very limited use case we can also clean it up during the deployment, so apart from the minimal additional complexity of the deployment this is a non-issue.\\n","Predictions":"We chose option 1 over option 2, because it was important to us that a brief glance at Connaisseur's Namespace allows one to judge whether it is running properly. Option 3 was not chosen as the readiness status of Pods can be easily seen from the Service, whereas the health status would require querying every single Pod individually. We deemed that to be a very ugly, non-kubernetes-y solution and hence decided against it.n### Positive consequencesnIf the Connaisseur Pods report Ready during the connaisseur-bootstrap-sentinel's runtime, the MutatingWebhookConfiguration will be deployed by Helm. Otherwise, the Helm deployment will fail after its timeout period (timeout).n### Negative consequencesnIf the Connaisseur Pods report Ready during the connaisseur-bootstrap-sentinel's runtime, the MutatingWebhookConfiguration will be deployed by Helm. Otherwise, the Helm deployment will fail after its timeout period (timeout).n"}
{"File Name":"GoStudy\/0004-use-ports-adapters-as-application-architecture-style.md","Context":"## Contextadjecent\nWe need to adopt application architecture style adjecent to our architectural drivers\n","Decision":"We will use ports&adapters architecture style\\n","tokens":21,"id":429,"text":"## Contextadjecent\\nWe need to adopt application architecture style adjecent to our architectural drivers\\n\n\n##Decision\nWe will use ports&adapters architecture style\\n","Predictions":"We will use ports&adapters architecture stylen"}
{"File Name":"smjs\/2018120802-implement-clean-architecture.md","Context":"## Context\nThis project is meant to be a playground to experiment with different technologies and design solutions.\n","Decision":"To standardize the project structure, now that it's getting bigger, we will implement the principles suggested by [clean architecture](https:\/\/github.com\/michelezamuner\/notes\/tree\/master\/software-architecture\/clean-architecture\/clean-architecture.martin), enhanced by concepts from Domain Driven Design, and plugin architectures.\\n### Domain\\nThe core domain is the \"virtual machine framework\", which only defines how programs should be executed, but doesn't specify any architecture, meaning that how programs are interpreted is not known.\\nA Program is a sequence of Data Units. Data Units are the kind of data with the smallest possible size, which we set at a single Byte. Data is always represented as a sequence of Data Units. Since we work with sequences, we also define the concepts of Size, which is the number of Data Units in a specific Data, and Address of a Data inside the Program, with the Address of the first Data Unit being 0. Both Size and Address are Integers, which is a generic integral type defined to be independent from the runtime environment implementation. A Program has the ability of fetching blocks of Data given their Address and Size.\\nA Program is run by a Processor, which uses an Interpreter, whose implementation is provided by the specific Architecture selected, to first define which sets of Data Units should be regarded as Instructions, and then to execute these Instructions. When running an Instruction, the Interpreter returns a Status object knowing if the execution should jump, or be terminated. The execution of a Program by a Processor always returns an Exit Status, which is Architecture-dependent. The termination of a Program must always be requested explicitly, via a dedicated instruction, otherwise an error is raised.\\nAn Interpreter must use the System to perform I\/O operations, and ultimately to allow a Program to communicate with the users; however, the implementation of the System depends on the actual application where the Processor and Interpreter are running, so it's left to be specified.\\nAdditional domains are defined for each architecture, so that a virtual machine can support many different architectures.\\nAssemblers and compilers also define their own domains.\\nThe following domains could thus be defined:\\n- `smf`: the core virtual machine framework domain\\n- `sma`: definitions for the SMA architecture domain\\n- `basm`: definitions for the BASM assembler domain\\n- `php`: definitions for the PHP compiler domain\\n### Application\\nThe application layer may define the following primary ports:\\n- the `vm` port allows to execute programs, according to the configured architecture\\n- the `repl` port allows to execute programs interactively, and uses the functionality of `vm`\\n- the `dbg` port allows to execute programs step by step for debugging, and uses the functionality of `vm`\\n- the `asm` port allows to run an assembler on some assembly code to produce executable object code\\n- the `cmp` port allows to run a compiler on some high level language to produce assembly code\\nAs far as secondary ports, we need the following:\\n- a `arcl` port allows the application to load an architecture definition\\n- a `pl` port allows the application to load a program\\n- a `asml` port allows the application to load assembly code, to be assembled\\n- a `cl` port allows the application to load high level code, to be compiled\\n- a `sys` port allows the application to interact with the underlying operating system\\n### Adapters\\nPrimary adapters might be defined to create command line applications, or Web applications. Secondary adapters might be defined to read data from files or from memory. See below for more concrete examples.\\n### Plugin architecture\\nWe want to support building different types of applications by composing together sets of different available plugins. For example:\\n**sloth machine (CLI)**\\nAD_sm + AD_larcl + AD_fpl + AD_ossys + AP_vm + AP_arcl + AP_pl + AP_sys + D_smf + D_sma (or others)\\n**sloth machine assembler (CLI)**\\nAD_asm + AD_fasml + AP_asm + AP_asml + D_basm (or others)\\n**sloth machine compiler (CLI)**\\nAD_cmp + AD_fcl + AD_masml + AP_cmp + AP_asm + AP_cl + AP_asml + D_basm (or others) + D_php (or others)\\n**sloth machine runner (CLI)**\\nAD_run + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_vm + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine REPL (CLI)**\\nAD_repl + AD_larcl + AD_mcl + AD_masml + AD_mpl + AD_ossys + AP_repl + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine debugger (CLI)**\\nAD_dbg + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_dbg + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine Web (Web)**\\nAD_web + AD_warcl + AD_wcl + AD_masml + AD_mpl + AD_wsys + AP_vm + AP_cmp + AP_asm + AP_repl + AP_dbg + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n- `D_smf`: Domain Sloth Machine Framework\\n- `D_sma`: Domain Sloth Machine Architecture\\n- `D_basm`: Domain Basic Assembly for Sloth Machine\\n- `D_php`: Domain PHP\\n- `AP_vm`: Application Virtual Machine (primary port)\\n- `AP_cmp`: Application Compiler (primary port)\\n- `AP_asm`: Application Assembler (primary port)\\n- `AP_repl`: Application REPL (primary port)\\n- `AP_dbg`: Application Debugger (primary port)\\n- `AP_arcl`: Application Architecture Loader (secondary port)\\n- `AP_pl`: Application Program Loader (secondary port)\\n- `AP_asml`: Application Assembly Loader (secondary port)\\n- `AP_cl`: Application Code Loader (secondary port)\\n- `AP_sys`: Application System (secondary port)\\n- `AD_sm`: Adapter Sloth Machine (primary adapter)\\n- `AD_cmp`: Adapter Compiler (primary adapter)\\n- `AD_run`: Adapter Runner (primary adapter)\\n- `AD_repl`: Adapter REPL (primary adapter)\\n- `AD_dbg`: Adapter Debugger (primary adapter)\\n- `AD_web`: Adapter Web (primary adapter)\\n- `AD_larcl`: Adapter Local Architecture Loader (secondary adapter)\\n- `AD_warcl`: Adapter Web Architecture Loader (secondary adapter)\\n- `AD_fpl`: Adapter File Program Loader (secondary adapter)\\n- `AD_mpl`: Adapter Memory Program Loader (secondary adapter)\\n- `AD_fasml`: Adapter File Assembly Loader (secondary adapter)\\n- `AD_masml`: Adapter Memory Assembly Loader (secondary adapter)\\n- `AD_fcl`: Adapter File Code Loader (secondary adapter)\\n- `AD_mcl`: Adapter Memory Code Loader (secondary adapter)\\n- `AD_wcl`: Adapter Web Code Loader (secondary adapter)\\n- `AD_ossys`: Adapter OS System (secondary adapter)\\n- `AD_wsys`: Adapter Web System (secondary adapter)\\n### Example modules\\n```\\ndomain\\nsmf\\ndata\\nDataUnit: (Byte)\\nData: DataUnit[]\\nSize: (Integer)\\nAddress: (Integer)\\nprogram [data]\\nProgram\\nProgram(data.Data)\\nread(data.Address, data.Size): data.Data\\ninterpreter [data]\\nOpcode: data.Data\\nOperands: data.Data\\nExitStatus: (Integer)\\nInstruction\\nInstruction(Address, Opcode, Operands)\\ngetAddress(): Address\\ngetOpcode(): Opcode\\ngetOperands(): Operands\\nStatus\\nshouldJump(): (Boolean)\\ngetJumpAddress(): data.Address\\nshouldExit(): (Boolean)\\ngetExitStatus(): ExitStatus\\n<Interpreter>\\ngetOpcodeSize(): data.Size\\ngetOperandsSize(Opcode): data.Size\\nexec(Instruction): Status\\nprocessor [program, interpreter]\\nProcessor\\nProcessor(interpreter.<Interpreter>)\\nrun(program.Program): interpreter.ExitStatus\\narchitecture [data, interpreter]\\n<System>\\nread(data.Integer fd, data.Size size): data.Data\\nwrite(data.Integer fd, data.Data data, data.Size size): data.Size\\n<Architecture>\\ngetInterpreter(<System>): interpreter.<Interpreter>\\nsma [smf]\\nInterpreter: smf.interpreter.<Interpreter>\\nInterpreter(smf.architecture.<System>)\\napplication\\nvm\\nrun_program [domain.smf, application.arcl, application.pl, application.sys]\\n<Request>\\ngetArchitectureName(): String\\ngetProgramReference(): String\\nResponse\\ngetExitStatus(): domain.smf.interpreter.ExitStatus\\n<Presenter>\\npresent(Response)\\nRunProgram\\nRunProgram(ProcessorFactory, <Presenter>, application.arcl.<ArchitectureLoader>, application.pl.<ProgramLoader>, application.sys.<System>)\\nexec(<Request>)\\nProcessorFactory\\ncreate(domain.smf.interpreter.<Interpreter>): domain.smf.processor.Processor\\narcl [domain.smf]\\n<ArchitectureLoader>\\nload(architectureName: String): domain.smf.architecture.<Architecture>\\npl [domain.smf]\\n<ProgramLoader>\\nload(programReference: String): domain.smf.program.Program\\nsys [domain.smf]\\n<System>: domain.smf.architecture.<System>\\nadapters\\nsm [application.vm, domain.smf]\\nrun_program [application.vm, domain.smf]\\nRequest: application.vm.run_program.<Request>\\nController\\nController(application.vm.run_program.RunProgram)\\nrunProgram(Request)\\nViewModel\\nViewModel(domain.smf.interpreter.<ExitStatus>)\\ngetExitStatus(): <native-integer>\\n<View>\\nrender(ViewModel)\\nExitStatusView: <View>\\nrender(ViewModel)\\ngetExitStatus(): <native-integer>\\nPresenter: application.vm.run_program.<Presenter>\\nPresenter(<View>)\\npresent(application.vm.run_program.Response)\\nlarcl [application.arcl]\\nLocalArchitectureLoader: application.arcl.<ArchitectureLoader>\\nfpl [application.pl]\\nFileProgramLoader: application.pl.<ProgramLoader>\\nossys [application.sys]\\nOSSystem: application.sys.<System>\\n```\\n### Examples of main implementations\\n```\\nmodule domain.smf.processor\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Operands\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.interpreter.Status\\nclass Processor\\nProcessor(<Interpreter> interpreter)\\nthis.interpreter = interpreter\\nrun(Program program): ExitStatus\\nSize opcodeSize = interpreter.getOpcodeSize()\\nAddress address = 0\\nwhile (true)\\nOpcode opcode = program.read(address, opcodeSize)\\nSize operandsSize = interpreter.getOperandsSize(opcode)\\nAddress operandsAddress = address + opcodeSize\\nOperands operands = program.read(operandsAddress, operandsSize)\\nInstruction instruction = new Instruction(address, opcode, operands)\\nStatus status = interpreter.exec(instruction)\\nif (status.shouldExit())\\nreturn status.getExitStatus()\\naddress = status.shouldJump() ? status.getJumpAddress() : operandsAddress + operandsSize\\n\/\/ @todo: handle missing exit\\n```\\n```\\nmodule domain.sma.interpreter\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.interpreter.Status\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.architecture.<System>\\nimport domain.sma.InstructionSet\\nimport domain.sma.Result\\nimport domain.sma.JumpResult\\nimport domain.sma.ExitResult\\nimport domain.sma.InstructionDefinition\\nclass Interpreter: <Interpreter>\\nInterpreter(InstructionSet instructionSet, <System> system)\\nthis.instructionSet = instructionSet\\nthis.system = system\\ngetOpcodeSize(): Size\\nreturn new Integer(1)\\ngetOperandsSize(Opcode opcode): Size\\nreturn instructionSet.getInstructionDefinition(opcode).getOperandsSize()\\nexec(Instruction instruction): Status\\nAddress jumpAddress = null\\nAddress exitStatus = null\\nInstructionDefinition definition = instructionSet.getInstructionDefinition(instruction.getOpcode())\\nResult result = definition.exec(instruction.getOperands())\\nif (result instanceof JumpResult)\\njumpAddress = result.getJumpAddress()\\nif (result instanceof ExitResult)\\nexitStatus = result.getExitStatus()\\nreturn new Status(jumpAddress, exitStatus)\\n```\\n```\\nmodule application.vm.run_program\\nimport application.vm.run_program.ProcessorFactory\\nimport application.vm.run_program.<Presenter>\\nimport application.arcl.<ArchitectureLoader>\\nimport application.pl.<ProgramLoader>\\nimport application.sys.<System>\\nimport application.vm.run_program.<Request>\\nimport domain.smf.architecture.<Architecture>\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.processor.Processor\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport application.vm.run_program.Response\\nclass RunProgram\\nRunProgram(\\nProcessorFactory processorFactory,\\n<Presenter> presenter,\\n<ArchitectureLoader> architectureLoader,\\n<ProgramLoader> programLoader,\\n<System> system\\n)\\nthis.processorFactory = processorFactory\\nthis.presenter = presenter\\nthis.architectureLoader = architectureLoader\\nthis.programLoader = programLoader\\nthis.system = system\\nexec(<Request> request)\\n<Architecture> architecture = architectureLoader.load(request.getArchitectureName())\\n<Interpreter> interpreter = architecture.getInterpreter(this.system)\\nProcessor processor = processorFactory.create(interpreter)\\nProgram program = programLoader.load(request.getProgramReference())\\nExitStatus exitStatus = processor.run(program)\\npresenter.present(new Response(exitStatus))\\n```\\n```\\nmodule adapters.sm.run_program\\nimport application.vm.run_program.RunProgram\\nimport application.vm.run_program.<Request>\\nclass Controller\\nController(RunProgram service)\\nthis.service = service\\nrunProgram(<Request> request)\\nservice.run(request)\\n```\\n```\\nmodule adapters.sm.run_program\\nimport adapters.sm.run_program.<View>\\nimport application.vm.run_program.Response\\nimport adapters.sm.run_program.ViewModel\\nclass Presenter\\nPresenter(<View> view)\\nthis.view = view\\npresent(Response response)\\nViewModel viewModel = new ViewModel(response.getExitStatus())\\nview.render(viewModel)\\n```\\n","tokens":21,"id":3982,"text":"## Context\\nThis project is meant to be a playground to experiment with different technologies and design solutions.\\n\n\n##Decision\nTo standardize the project structure, now that it's getting bigger, we will implement the principles suggested by [clean architecture](https:\/\/github.com\/michelezamuner\/notes\/tree\/master\/software-architecture\/clean-architecture\/clean-architecture.martin), enhanced by concepts from Domain Driven Design, and plugin architectures.\\n### Domain\\nThe core domain is the \"virtual machine framework\", which only defines how programs should be executed, but doesn't specify any architecture, meaning that how programs are interpreted is not known.\\nA Program is a sequence of Data Units. Data Units are the kind of data with the smallest possible size, which we set at a single Byte. Data is always represented as a sequence of Data Units. Since we work with sequences, we also define the concepts of Size, which is the number of Data Units in a specific Data, and Address of a Data inside the Program, with the Address of the first Data Unit being 0. Both Size and Address are Integers, which is a generic integral type defined to be independent from the runtime environment implementation. A Program has the ability of fetching blocks of Data given their Address and Size.\\nA Program is run by a Processor, which uses an Interpreter, whose implementation is provided by the specific Architecture selected, to first define which sets of Data Units should be regarded as Instructions, and then to execute these Instructions. When running an Instruction, the Interpreter returns a Status object knowing if the execution should jump, or be terminated. The execution of a Program by a Processor always returns an Exit Status, which is Architecture-dependent. The termination of a Program must always be requested explicitly, via a dedicated instruction, otherwise an error is raised.\\nAn Interpreter must use the System to perform I\/O operations, and ultimately to allow a Program to communicate with the users; however, the implementation of the System depends on the actual application where the Processor and Interpreter are running, so it's left to be specified.\\nAdditional domains are defined for each architecture, so that a virtual machine can support many different architectures.\\nAssemblers and compilers also define their own domains.\\nThe following domains could thus be defined:\\n- `smf`: the core virtual machine framework domain\\n- `sma`: definitions for the SMA architecture domain\\n- `basm`: definitions for the BASM assembler domain\\n- `php`: definitions for the PHP compiler domain\\n### Application\\nThe application layer may define the following primary ports:\\n- the `vm` port allows to execute programs, according to the configured architecture\\n- the `repl` port allows to execute programs interactively, and uses the functionality of `vm`\\n- the `dbg` port allows to execute programs step by step for debugging, and uses the functionality of `vm`\\n- the `asm` port allows to run an assembler on some assembly code to produce executable object code\\n- the `cmp` port allows to run a compiler on some high level language to produce assembly code\\nAs far as secondary ports, we need the following:\\n- a `arcl` port allows the application to load an architecture definition\\n- a `pl` port allows the application to load a program\\n- a `asml` port allows the application to load assembly code, to be assembled\\n- a `cl` port allows the application to load high level code, to be compiled\\n- a `sys` port allows the application to interact with the underlying operating system\\n### Adapters\\nPrimary adapters might be defined to create command line applications, or Web applications. Secondary adapters might be defined to read data from files or from memory. See below for more concrete examples.\\n### Plugin architecture\\nWe want to support building different types of applications by composing together sets of different available plugins. For example:\\n**sloth machine (CLI)**\\nAD_sm + AD_larcl + AD_fpl + AD_ossys + AP_vm + AP_arcl + AP_pl + AP_sys + D_smf + D_sma (or others)\\n**sloth machine assembler (CLI)**\\nAD_asm + AD_fasml + AP_asm + AP_asml + D_basm (or others)\\n**sloth machine compiler (CLI)**\\nAD_cmp + AD_fcl + AD_masml + AP_cmp + AP_asm + AP_cl + AP_asml + D_basm (or others) + D_php (or others)\\n**sloth machine runner (CLI)**\\nAD_run + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_vm + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine REPL (CLI)**\\nAD_repl + AD_larcl + AD_mcl + AD_masml + AD_mpl + AD_ossys + AP_repl + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine debugger (CLI)**\\nAD_dbg + AD_larcl + AD_fcl + AD_masml + AD_mpl + AD_ossys + AP_dbg + AP_cmp + AP_asm + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n**sloth machine Web (Web)**\\nAD_web + AD_warcl + AD_wcl + AD_masml + AD_mpl + AD_wsys + AP_vm + AP_cmp + AP_asm + AP_repl + AP_dbg + AP_arcl + AP_cl + AP_asml + AP_pl + AP_sys + D_smf + D_sma (or others) + D_basm (or others) + D_php (or others)\\n- `D_smf`: Domain Sloth Machine Framework\\n- `D_sma`: Domain Sloth Machine Architecture\\n- `D_basm`: Domain Basic Assembly for Sloth Machine\\n- `D_php`: Domain PHP\\n- `AP_vm`: Application Virtual Machine (primary port)\\n- `AP_cmp`: Application Compiler (primary port)\\n- `AP_asm`: Application Assembler (primary port)\\n- `AP_repl`: Application REPL (primary port)\\n- `AP_dbg`: Application Debugger (primary port)\\n- `AP_arcl`: Application Architecture Loader (secondary port)\\n- `AP_pl`: Application Program Loader (secondary port)\\n- `AP_asml`: Application Assembly Loader (secondary port)\\n- `AP_cl`: Application Code Loader (secondary port)\\n- `AP_sys`: Application System (secondary port)\\n- `AD_sm`: Adapter Sloth Machine (primary adapter)\\n- `AD_cmp`: Adapter Compiler (primary adapter)\\n- `AD_run`: Adapter Runner (primary adapter)\\n- `AD_repl`: Adapter REPL (primary adapter)\\n- `AD_dbg`: Adapter Debugger (primary adapter)\\n- `AD_web`: Adapter Web (primary adapter)\\n- `AD_larcl`: Adapter Local Architecture Loader (secondary adapter)\\n- `AD_warcl`: Adapter Web Architecture Loader (secondary adapter)\\n- `AD_fpl`: Adapter File Program Loader (secondary adapter)\\n- `AD_mpl`: Adapter Memory Program Loader (secondary adapter)\\n- `AD_fasml`: Adapter File Assembly Loader (secondary adapter)\\n- `AD_masml`: Adapter Memory Assembly Loader (secondary adapter)\\n- `AD_fcl`: Adapter File Code Loader (secondary adapter)\\n- `AD_mcl`: Adapter Memory Code Loader (secondary adapter)\\n- `AD_wcl`: Adapter Web Code Loader (secondary adapter)\\n- `AD_ossys`: Adapter OS System (secondary adapter)\\n- `AD_wsys`: Adapter Web System (secondary adapter)\\n### Example modules\\n```\\ndomain\\nsmf\\ndata\\nDataUnit: (Byte)\\nData: DataUnit[]\\nSize: (Integer)\\nAddress: (Integer)\\nprogram [data]\\nProgram\\nProgram(data.Data)\\nread(data.Address, data.Size): data.Data\\ninterpreter [data]\\nOpcode: data.Data\\nOperands: data.Data\\nExitStatus: (Integer)\\nInstruction\\nInstruction(Address, Opcode, Operands)\\ngetAddress(): Address\\ngetOpcode(): Opcode\\ngetOperands(): Operands\\nStatus\\nshouldJump(): (Boolean)\\ngetJumpAddress(): data.Address\\nshouldExit(): (Boolean)\\ngetExitStatus(): ExitStatus\\n<Interpreter>\\ngetOpcodeSize(): data.Size\\ngetOperandsSize(Opcode): data.Size\\nexec(Instruction): Status\\nprocessor [program, interpreter]\\nProcessor\\nProcessor(interpreter.<Interpreter>)\\nrun(program.Program): interpreter.ExitStatus\\narchitecture [data, interpreter]\\n<System>\\nread(data.Integer fd, data.Size size): data.Data\\nwrite(data.Integer fd, data.Data data, data.Size size): data.Size\\n<Architecture>\\ngetInterpreter(<System>): interpreter.<Interpreter>\\nsma [smf]\\nInterpreter: smf.interpreter.<Interpreter>\\nInterpreter(smf.architecture.<System>)\\napplication\\nvm\\nrun_program [domain.smf, application.arcl, application.pl, application.sys]\\n<Request>\\ngetArchitectureName(): String\\ngetProgramReference(): String\\nResponse\\ngetExitStatus(): domain.smf.interpreter.ExitStatus\\n<Presenter>\\npresent(Response)\\nRunProgram\\nRunProgram(ProcessorFactory, <Presenter>, application.arcl.<ArchitectureLoader>, application.pl.<ProgramLoader>, application.sys.<System>)\\nexec(<Request>)\\nProcessorFactory\\ncreate(domain.smf.interpreter.<Interpreter>): domain.smf.processor.Processor\\narcl [domain.smf]\\n<ArchitectureLoader>\\nload(architectureName: String): domain.smf.architecture.<Architecture>\\npl [domain.smf]\\n<ProgramLoader>\\nload(programReference: String): domain.smf.program.Program\\nsys [domain.smf]\\n<System>: domain.smf.architecture.<System>\\nadapters\\nsm [application.vm, domain.smf]\\nrun_program [application.vm, domain.smf]\\nRequest: application.vm.run_program.<Request>\\nController\\nController(application.vm.run_program.RunProgram)\\nrunProgram(Request)\\nViewModel\\nViewModel(domain.smf.interpreter.<ExitStatus>)\\ngetExitStatus(): <native-integer>\\n<View>\\nrender(ViewModel)\\nExitStatusView: <View>\\nrender(ViewModel)\\ngetExitStatus(): <native-integer>\\nPresenter: application.vm.run_program.<Presenter>\\nPresenter(<View>)\\npresent(application.vm.run_program.Response)\\nlarcl [application.arcl]\\nLocalArchitectureLoader: application.arcl.<ArchitectureLoader>\\nfpl [application.pl]\\nFileProgramLoader: application.pl.<ProgramLoader>\\nossys [application.sys]\\nOSSystem: application.sys.<System>\\n```\\n### Examples of main implementations\\n```\\nmodule domain.smf.processor\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Operands\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.interpreter.Status\\nclass Processor\\nProcessor(<Interpreter> interpreter)\\nthis.interpreter = interpreter\\nrun(Program program): ExitStatus\\nSize opcodeSize = interpreter.getOpcodeSize()\\nAddress address = 0\\nwhile (true)\\nOpcode opcode = program.read(address, opcodeSize)\\nSize operandsSize = interpreter.getOperandsSize(opcode)\\nAddress operandsAddress = address + opcodeSize\\nOperands operands = program.read(operandsAddress, operandsSize)\\nInstruction instruction = new Instruction(address, opcode, operands)\\nStatus status = interpreter.exec(instruction)\\nif (status.shouldExit())\\nreturn status.getExitStatus()\\naddress = status.shouldJump() ? status.getJumpAddress() : operandsAddress + operandsSize\\n\/\/ @todo: handle missing exit\\n```\\n```\\nmodule domain.sma.interpreter\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.interpreter.ExitStatus\\nimport domain.smf.interpreter.Status\\nimport domain.smf.data.Size\\nimport domain.smf.data.Address\\nimport domain.smf.interpreter.Opcode\\nimport domain.smf.interpreter.Instruction\\nimport domain.smf.architecture.<System>\\nimport domain.sma.InstructionSet\\nimport domain.sma.Result\\nimport domain.sma.JumpResult\\nimport domain.sma.ExitResult\\nimport domain.sma.InstructionDefinition\\nclass Interpreter: <Interpreter>\\nInterpreter(InstructionSet instructionSet, <System> system)\\nthis.instructionSet = instructionSet\\nthis.system = system\\ngetOpcodeSize(): Size\\nreturn new Integer(1)\\ngetOperandsSize(Opcode opcode): Size\\nreturn instructionSet.getInstructionDefinition(opcode).getOperandsSize()\\nexec(Instruction instruction): Status\\nAddress jumpAddress = null\\nAddress exitStatus = null\\nInstructionDefinition definition = instructionSet.getInstructionDefinition(instruction.getOpcode())\\nResult result = definition.exec(instruction.getOperands())\\nif (result instanceof JumpResult)\\njumpAddress = result.getJumpAddress()\\nif (result instanceof ExitResult)\\nexitStatus = result.getExitStatus()\\nreturn new Status(jumpAddress, exitStatus)\\n```\\n```\\nmodule application.vm.run_program\\nimport application.vm.run_program.ProcessorFactory\\nimport application.vm.run_program.<Presenter>\\nimport application.arcl.<ArchitectureLoader>\\nimport application.pl.<ProgramLoader>\\nimport application.sys.<System>\\nimport application.vm.run_program.<Request>\\nimport domain.smf.architecture.<Architecture>\\nimport domain.smf.interpreter.<Interpreter>\\nimport domain.smf.processor.Processor\\nimport domain.smf.program.Program\\nimport domain.smf.interpreter.ExitStatus\\nimport application.vm.run_program.Response\\nclass RunProgram\\nRunProgram(\\nProcessorFactory processorFactory,\\n<Presenter> presenter,\\n<ArchitectureLoader> architectureLoader,\\n<ProgramLoader> programLoader,\\n<System> system\\n)\\nthis.processorFactory = processorFactory\\nthis.presenter = presenter\\nthis.architectureLoader = architectureLoader\\nthis.programLoader = programLoader\\nthis.system = system\\nexec(<Request> request)\\n<Architecture> architecture = architectureLoader.load(request.getArchitectureName())\\n<Interpreter> interpreter = architecture.getInterpreter(this.system)\\nProcessor processor = processorFactory.create(interpreter)\\nProgram program = programLoader.load(request.getProgramReference())\\nExitStatus exitStatus = processor.run(program)\\npresenter.present(new Response(exitStatus))\\n```\\n```\\nmodule adapters.sm.run_program\\nimport application.vm.run_program.RunProgram\\nimport application.vm.run_program.<Request>\\nclass Controller\\nController(RunProgram service)\\nthis.service = service\\nrunProgram(<Request> request)\\nservice.run(request)\\n```\\n```\\nmodule adapters.sm.run_program\\nimport adapters.sm.run_program.<View>\\nimport application.vm.run_program.Response\\nimport adapters.sm.run_program.ViewModel\\nclass Presenter\\nPresenter(<View> view)\\nthis.view = view\\npresent(Response response)\\nViewModel viewModel = new ViewModel(response.getExitStatus())\\nview.render(viewModel)\\n```\\n","Predictions":"To standardize the project structure, now that it's getting bigger, we will implement the principles suggested by [clean architecture](https:\/\/github.com\/michelezamuner\/notes\/tree\/master\/software-architecture\/clean-architecture\/martin), enhanced by concepts from Domain Driven Design, and plugin architectures.n### DomainnThe core domain is the \"virtual machine framework\", which only defines how programs should be executed, but doesn't specify any architecture, meaning that how programs are interpreted is not known.nA Program is a sequence of Data Units. Data Units are the kind of data with the smallest possible size, which we set at a single Byte. Data is always represented as a sequence of Data Units. Data Units are the kind of data with the smallest possible size, which we set at a single Byte. Data is always represented as a Byte. Data Units are always represented as a sequence of Data Units. Since we work with sequences, we also define the concepts of Size, which is the number of Data Units in a specific Data, and Address of a Data inside the Program, with the Address of the first Data Unit being 0. Both Size and Address are Integers, which is a generic integral type defined to be independent from the runtime environment implementation. A Program has the ability of fetching blocks of Data given their Address and Size.nA Program is run by a Processor, which uses an Interpreter, whose implementation is provided by the specific Architecture selected, to first define which sets of Data Units should be regarded as Instructions, and then to execute these Instructions. When running an Instruction, the Interpreter returns a Status object knowing if the execution should jump, or be terminated. The execution of a Program by a Processor always returns an Exit Status, which is Architecture-dependent. The termination of a Program must always be requested explicitly, via a dedicated instruction, otherwise an error is raised.nAn Interpreter must use the System to perform I\/O operations, and ultimately to allow a Program to communicate with the users; however, the implementation of the System depends on the actual application where the Processor and Interpreter are running, so it's left to be specified.nAdditional domains are defined for each architecture, so that a virtual machine framework can be used to execute programs.n"}
{"File Name":"dilay\/0003-use-compile-api-to-transfter-class.md","Context":"## Context\nhttps:\/\/github.com\/microsoft\/TypeScript\/wiki\/Using-the-Compiler-API\nhttps:\/\/github.com\/dsherret\/ts-morph#readme\n> TypeScript Compiler API wrapper. Provides an easier way to navigate and manipulate TypeScript and JavaScript code.\n","Decision":"Decision here...\\n","tokens":54,"id":1337,"text":"## Context\\nhttps:\/\/github.com\/microsoft\/TypeScript\/wiki\/Using-the-Compiler-API\\nhttps:\/\/github.com\/dsherret\/ts-morph#readme\\n> TypeScript Compiler API wrapper. Provides an easier way to navigate and manipulate TypeScript and JavaScript code.\\n\n\n##Decision\nDecision here...\\n","Predictions":"* easy to reason aroundn*...!-- numbers of drivers can vary -->n!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves the web app's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application's web application'"}
{"File Name":"meadow\/0012-websockets.md","Context":"## Context\nWe need a way to provide live updates to the front-end for ingest sheet validation,\ningest status, etc.\n","Decision":"We will use the [WebSocket API](https:\/\/www.w3.org\/TR\/websockets\/) via [Phoenix Channels](https:\/\/hexdocs.pm\/phoenix\/channels.html) to enable real-time communication between the client and server.\\n","tokens":27,"id":3845,"text":"## Context\\nWe need a way to provide live updates to the front-end for ingest sheet validation,\\ningest status, etc.\\n\n\n##Decision\nWe will use the [WebSocket API](https:\/\/www.w3.org\/TR\/websockets\/) via [Phoenix Channels](https:\/\/hexdocs.pm\/phoenix\/channels.html) to enable real-time communication between the client and server.\\n","Predictions":"We will use the [WebSocket API](https:\/\/www.w3.org\/TR\/websockets\/) via [Phoenix Channels](https:\/\/hexdocs.pm\/phoenix\/channels.html) to enable real-time communication between the client and server.n"}
{"File Name":"ftd-scratch3-offline\/0015-use-floats-for-storing-numbers-not-doubles-as-scratch-does.md","Context":"## Context\nScratch uses Javascript's numbers type to store all numbers it uses. The Javascript number type is effectively a 64 bit double.\nThe ftduino only supports 32-bit floats.\n64-bit double emulation might be possible but slow and likely exhaust the available memory and program space.\n","Decision":"We will use 32-bit floats to store any numbers.\\n","tokens":60,"id":2631,"text":"## Context\\nScratch uses Javascript's numbers type to store all numbers it uses. The Javascript number type is effectively a 64 bit double.\\nThe ftduino only supports 32-bit floats.\\n64-bit double emulation might be possible but slow and likely exhaust the available memory and program space.\\n\n\n##Decision\nWe will use 32-bit floats to store any numbers.\\n","Predictions":"We will use a string field to store port names.n"}
{"File Name":"android-guidelines\/0007-use-deprecated-annotation-with-description.md","Context":"## Context and Problem Statement\nDeprecated annotation usages without any comment or suggestion makes no sense when using existing classes.\n","Decision":"When we decide to use deprecated annotation in classes, we need to comment reason and alternative to that implementation.\\n","tokens":24,"id":3863,"text":"## Context and Problem Statement\\nDeprecated annotation usages without any comment or suggestion makes no sense when using existing classes.\\n\n\n##Decision\nWhen we decide to use deprecated annotation in classes, we need to comment reason and alternative to that implementation.\\n","Predictions":"* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.n* The feature configuration from Feature::getAll() is converted to a SCSS map inside ShopwareStorefrontThemeThemeCompiler::getFeatureConfigScssMap.n* This SCSS map is always added to the SCSS string which gets processed by ShopwareStorefrontThemeThemeCompiler::compileTheme.n* For webpack hot-proxy the var\/config_js_features.json is used instead.n* The SCSS map looks like this: $sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);n* See https:\/\/sass-lang.com\/documentation\/values\/mapsn* A globally available function feature() is used to read inside the SCSS map if a desired feature is active.nExample:nscssnbody n@if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n @else nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n if feature('"}
{"File Name":"umbrella\/0003-component-configuration-via-context.md","Context":"## Context\nAn alternative configuration procedure to ADR-0002, possibly better\nsuited for dynamic theming, theme changes and separating the component\nconfiguration between behavioral and stylistic aspects. This new\napproach utilizes the hdom context object to retrieve theme attributes,\nwhereas the previous solution ignored the context object entirely.\nA live demo of the code discussed here is available at:\n[demo.thi.ng\/umbrella\/hdom-theme-adr-0003](https:\/\/demo.thi.ng\/umbrella\/hdom-theme-adr-0003)\n","Decision":"### Split component configuration\\n#### Behavioral aspects\\nComponent pre-configuration options SHOULD purely consist of behavioral\\nsettings and NOT include any aesthetic \/ theme oriented options. To\\nbetter express this intention, it's recommended to suffix these\\ninterface names with `Behavior`, e.g. `ButtonBehavior`.\\n```ts\\ninterface ButtonBehavior {\\n\/**\\n* Element name to use for enabled buttons.\\n* Default: \"a\"\\n*\/\\ntag: string;\\n\/**\\n* Element name to use for disabled buttons.\\n* Default: \"span\"\\n*\/\\ntagDisabled: string;\\n\/**\\n* Default attribs, always injected for active button states\\n* and overridable at runtime.\\n* Default: `{ href: \"#\", role: \"button\" }`\\n*\/\\nattribs: IObjectOf<any>;\\n}\\n```\\n#### Theme stored in hdom context\\nEven though there's work underway to develop a flexble theming system\\nfor hdom components, the components themselves SHOULD be agnostic to\\nthis and only expect to somehow obtain styling attributes from the hdom\\ncontext object passed to each component function. How is shown further\\nbelow.\\nIn this example we define a `theme` key in the context object, under\\nwhich theme options for all participating components are stored.\\n```ts\\nconst ctx = {\\n...\\ntheme: {\\nprimaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\nsecondaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\n...\\n}\\n};\\n```\\n### Component definition\\n```ts\\nimport { getIn, Path } from \"@thi.ng\/paths\";\\n\/**\\n* Instance specific runtime args. All optional.\\n*\/\\ninterface ButtonArgs {\\n\/**\\n* Click event handler to be wrapped with preventDefault() call\\n*\/\\nonclick: EventListener;\\n\/**\\n* Disabled flag. Used to determine themed version.\\n*\/\\ndisabled: boolean;\\n\/**\\n* Selected flag. Used to determine themed version.\\n*\/\\nselected: boolean;\\n\/**\\n* Link target.\\n*\/\\nhref: string;\\n}\\nconst button = (themeCtxPath: Path, behavior?: Partial<ButtonBehavior>) => {\\n\/\/ init with defaults\\nbehavior = {\\ntag: \"a\",\\ntagDisabled: \"span\",\\n...behavior\\n};\\nbehavior.attribs = { href: \"#\", role: \"button\", ...behavior.attribs };\\n\/\/ return component function as closure\\nreturn (ctx: any, args: Partial<ButtonArgs>, ...body: any[]) => {\\n\/\/ lookup component theme config in context\\nconst theme = getIn(ctx, themeCtxPath);\\nif (args.disabled) {\\nreturn [behavior.tagDisabled, {\\n...behavior.attribs,\\n...theme.disabled,\\n...args,\\n}, ...body];\\n} else {\\nconst attribs = {\\n...behavior.attribs,\\n...theme[args.selected ? \"selected\" : \"default\"],\\n...args\\n};\\nif (args && args.onclick && (args.href == null || args.href === \"#\")) {\\nattribs.onclick = (e) => (e.preventDefault(), args.onclick(e));\\n}\\nreturn [behavior.tag, attribs, ...body];\\n}\\n};\\n};\\n```\\n### Component usage\\n```ts\\nconst darkTheme = {\\nid: \"dark\",\\nbody: {\\nclass: \"vh-100 bg-black moon-gray pa3 sans-serif\"\\n},\\nlink: {\\nclass: \"link dim b light-silver\"\\n},\\nbutton: {\\ndefault: {\\nclass: \"dib link mr2 ph3 pv2 blue hover-lightest-blue hover-b--current br3 ba b--blue\"\\n},\\nselected: {\\nclass: \"dib link mr2 ph3 pv2 red hover-gold hover-b--current br3 ba b--red\"\\n},\\ndisabled: {\\nclass: \"dib mr2 ph3 pv2 mid-gray br3 ba b--mid-gray\"\\n}\\n}\\n};\\nconst bt = button(\"theme.button\");\\nconst btFixed = button(\"theme.button\", { attribs: { style: { width: \"8rem\" } } });\\nconst app = (ctx) =>\\n[\"div\", ctx.theme.body,\\n[bt, { onclick: () => alert(\"toggle\") }, \"Toggle\"],\\n[bt, { href: \"https:\/\/github.com\/thi-ng\/umbrella\" }, \"External\"],\\n[btFixed, { onclick: () => alert(\"hi\"), selected: true }, \"Selected\"],\\n[btFixed, { disabled: true }, \"Disabled\"] ];\\n\/\/ start app with theme in context\\nstart(\"app\", app, { theme: darkTheme })\\n```\\n","tokens":117,"id":5185,"text":"## Context\\nAn alternative configuration procedure to ADR-0002, possibly better\\nsuited for dynamic theming, theme changes and separating the component\\nconfiguration between behavioral and stylistic aspects. This new\\napproach utilizes the hdom context object to retrieve theme attributes,\\nwhereas the previous solution ignored the context object entirely.\\nA live demo of the code discussed here is available at:\\n[demo.thi.ng\/umbrella\/hdom-theme-adr-0003](https:\/\/demo.thi.ng\/umbrella\/hdom-theme-adr-0003)\\n\n\n##Decision\n### Split component configuration\\n#### Behavioral aspects\\nComponent pre-configuration options SHOULD purely consist of behavioral\\nsettings and NOT include any aesthetic \/ theme oriented options. To\\nbetter express this intention, it's recommended to suffix these\\ninterface names with `Behavior`, e.g. `ButtonBehavior`.\\n```ts\\ninterface ButtonBehavior {\\n\/**\\n* Element name to use for enabled buttons.\\n* Default: \"a\"\\n*\/\\ntag: string;\\n\/**\\n* Element name to use for disabled buttons.\\n* Default: \"span\"\\n*\/\\ntagDisabled: string;\\n\/**\\n* Default attribs, always injected for active button states\\n* and overridable at runtime.\\n* Default: `{ href: \"#\", role: \"button\" }`\\n*\/\\nattribs: IObjectOf<any>;\\n}\\n```\\n#### Theme stored in hdom context\\nEven though there's work underway to develop a flexble theming system\\nfor hdom components, the components themselves SHOULD be agnostic to\\nthis and only expect to somehow obtain styling attributes from the hdom\\ncontext object passed to each component function. How is shown further\\nbelow.\\nIn this example we define a `theme` key in the context object, under\\nwhich theme options for all participating components are stored.\\n```ts\\nconst ctx = {\\n...\\ntheme: {\\nprimaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\nsecondaryButton: {\\ndefault: { class: ... },\\ndisabled: { class: ... },\\nselected: { class: ... },\\n},\\n...\\n}\\n};\\n```\\n### Component definition\\n```ts\\nimport { getIn, Path } from \"@thi.ng\/paths\";\\n\/**\\n* Instance specific runtime args. All optional.\\n*\/\\ninterface ButtonArgs {\\n\/**\\n* Click event handler to be wrapped with preventDefault() call\\n*\/\\nonclick: EventListener;\\n\/**\\n* Disabled flag. Used to determine themed version.\\n*\/\\ndisabled: boolean;\\n\/**\\n* Selected flag. Used to determine themed version.\\n*\/\\nselected: boolean;\\n\/**\\n* Link target.\\n*\/\\nhref: string;\\n}\\nconst button = (themeCtxPath: Path, behavior?: Partial<ButtonBehavior>) => {\\n\/\/ init with defaults\\nbehavior = {\\ntag: \"a\",\\ntagDisabled: \"span\",\\n...behavior\\n};\\nbehavior.attribs = { href: \"#\", role: \"button\", ...behavior.attribs };\\n\/\/ return component function as closure\\nreturn (ctx: any, args: Partial<ButtonArgs>, ...body: any[]) => {\\n\/\/ lookup component theme config in context\\nconst theme = getIn(ctx, themeCtxPath);\\nif (args.disabled) {\\nreturn [behavior.tagDisabled, {\\n...behavior.attribs,\\n...theme.disabled,\\n...args,\\n}, ...body];\\n} else {\\nconst attribs = {\\n...behavior.attribs,\\n...theme[args.selected ? \"selected\" : \"default\"],\\n...args\\n};\\nif (args && args.onclick && (args.href == null || args.href === \"#\")) {\\nattribs.onclick = (e) => (e.preventDefault(), args.onclick(e));\\n}\\nreturn [behavior.tag, attribs, ...body];\\n}\\n};\\n};\\n```\\n### Component usage\\n```ts\\nconst darkTheme = {\\nid: \"dark\",\\nbody: {\\nclass: \"vh-100 bg-black moon-gray pa3 sans-serif\"\\n},\\nlink: {\\nclass: \"link dim b light-silver\"\\n},\\nbutton: {\\ndefault: {\\nclass: \"dib link mr2 ph3 pv2 blue hover-lightest-blue hover-b--current br3 ba b--blue\"\\n},\\nselected: {\\nclass: \"dib link mr2 ph3 pv2 red hover-gold hover-b--current br3 ba b--red\"\\n},\\ndisabled: {\\nclass: \"dib mr2 ph3 pv2 mid-gray br3 ba b--mid-gray\"\\n}\\n}\\n};\\nconst bt = button(\"theme.button\");\\nconst btFixed = button(\"theme.button\", { attribs: { style: { width: \"8rem\" } } });\\nconst app = (ctx) =>\\n[\"div\", ctx.theme.body,\\n[bt, { onclick: () => alert(\"toggle\") }, \"Toggle\"],\\n[bt, { href: \"https:\/\/github.com\/thi-ng\/umbrella\" }, \"External\"],\\n[btFixed, { onclick: () => alert(\"hi\"), selected: true }, \"Selected\"],\\n[btFixed, { disabled: true }, \"Disabled\"] ];\\n\/\/ start app with theme in context\\nstart(\"app\", app, { theme: darkTheme })\\n```\\n","Predictions":"* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a class of the theme.n* Theme is a "}
{"File Name":"buy-for-your-school\/0013-use-dfe-sign-in-as-auth-provider.md","Context":"## Context\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\nThere is currently no formal recommendation for a tool of choice in the technical guidance https:\/\/github.com\/DFE-Digital\/technical-guidance.\nWe want a tool that provides an open and modern security standard.\n","Decision":"We are going to use DSI as our single sign-on provider using the OIDC standard.\\n","tokens":126,"id":1252,"text":"## Context\\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\\nThere is currently no formal recommendation for a tool of choice in the technical guidance https:\/\/github.com\/DFE-Digital\/technical-guidance.\\nWe want a tool that provides an open and modern security standard.\\n\n\n##Decision\nWe are going to use DSI as our single sign-on provider using the OIDC standard.\\n","Predictions":"We will use the Sign-In service to sign in to the service.n"}
{"File Name":"application-services\/0004-early-startup-experiments.md","Context":"## Context and Problem Statement\nAs an experimenter, I would like to run experiments early on a user's first run of the application. However, the experiment data is only available on the second run. We would like to have that experiment data available before the user's first run.\nFor more information: https:\/\/docs.google.com\/document\/d\/1Qw36_7G6XyHvJZdM-Hxh4nqYZyCsYajG0L5mO33Yd5M\/edit\n## Decision Drivers\n* Availability of experiments early on the first run\n* No impact on experimentation data analysis\n* Flexibility in creating experiments\n* Ability to quickly disable experiments\n* Simplicity of releases\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\n","Decision":"* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\nNone of the options were feasible, so for now we are sticking with option **(A) Do Nothing** until there are experiments planned that are expected to run on early startup on the first run, then we will revaluate our options.\\nThe **(B) Bundle Experiment data with app on release** option was rejected mainly due to difficulty in disabling experiments and pausing enrollments. This can create a negative user experience as it prevents us from disabling any problematic experiments. Additionally, it ties experiment creation with application release cycles.\\nThe **(C) Retrieve Experiment data on first run, and deal with delay** option was rejected due to the fact it changes the Nimbus SDK will no longer be idempotent,and the possibility of introducing undesirable UI.\\n","tokens":170,"id":2067,"text":"## Context and Problem Statement\\nAs an experimenter, I would like to run experiments early on a user's first run of the application. However, the experiment data is only available on the second run. We would like to have that experiment data available before the user's first run.\\nFor more information: https:\/\/docs.google.com\/document\/d\/1Qw36_7G6XyHvJZdM-Hxh4nqYZyCsYajG0L5mO33Yd5M\/edit\\n## Decision Drivers\\n* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\n\n\n##Decision\n* Availability of experiments early on the first run\\n* No impact on experimentation data analysis\\n* Flexibility in creating experiments\\n* Ability to quickly disable experiments\\n* Simplicity of releases\\n* Mobile's expectations of Nimbus (The SDK should be idempotent)\\nNone of the options were feasible, so for now we are sticking with option **(A) Do Nothing** until there are experiments planned that are expected to run on early startup on the first run, then we will revaluate our options.\\nThe **(B) Bundle Experiment data with app on release** option was rejected mainly due to difficulty in disabling experiments and pausing enrollments. This can create a negative user experience as it prevents us from disabling any problematic experiments. Additionally, it ties experiment creation with application release cycles.\\nThe **(C) Retrieve Experiment data on first run, and deal with delay** option was rejected due to the fact it changes the Nimbus SDK will no longer be idempotent,and the possibility of introducing undesirable UI.\\n","Predictions":"* Availability of experiments early on the first runn* No impact on experimentation data analysisn* Flexibility in creating experimentsn* Ability to quickly disable experimentsn* Simplicity of releasesn* Mobile's expectations of Nimbus (The SDK should be idempotent)nNone of the options were feasible, so for now we are sticking with option **(A) Do Nothing** until there are experiments planned that are expected to run on early startup on the first run, then we will revaluate our options.nThe **(B) Bundle Experiment data with app on release** option was rejected mainly due to difficulty in disabling experiments and pausing enrollments. This can create a negative user experience as it prevents us from disabling any problematic experiments. Additionally, it ties experiment creation with application release cycles.nThe **(C) Retrieve Experiment data on first run, and deal with delay** option was rejected due to the fact it changes the Nimbus SDK will no longer be idempotent,and the possibility of introducing undesirable UI.n"}
{"File Name":"adr-viewer\/0006-accessibility-as-a-first-class-concern.md","Context":"## Context\nThis tool had, up until this point, made assumptions about how its users might interpret the information it presents (for example, using colour as the main mechanism of distinguishing record types)\n","Decision":"Accessibility will now be a first-class concern of this project. All future design decisions should bear this in mind.\\n","tokens":40,"id":3815,"text":"## Context\\nThis tool had, up until this point, made assumptions about how its users might interpret the information it presents (for example, using colour as the main mechanism of distinguishing record types)\\n\n\n##Decision\nAccessibility will now be a first-class concern of this project. All future design decisions should bear this in mind.\\n","Predictions":"The MVVM (Model-View-View Model) approach is widely used by the WPF (Windows Presentation Foundation) community. It's difficult to search the web for insights into building WPF applications without running into information about MVVM architecture. MVVM appears to be the standard.nApplication components should be divided into a Model, View, and View Model folder, with each folder containing the necessary items for displaying the UI and interpreting data on the app.n### StatusnAcceptedn### ConsequencesnIt will be easier to find solutions to coding problems online by accepting a widely-used design pattern. However, this doesn't necessarily solve the problem of how other items should be organized (lib, images, fonts, etc.).n_In Retrospect:_ The MVVM pattern was overall beneficial, but I was often confused as to how strictly I should adhere to the MVVM pattern. MVVM says each view should have an accompanying view model and model. However, with an app containing so many design elements, it often felt unnecessary to have a data model tied to each view. What would the model be for a modal and how would that be different from the view model?n"}
{"File Name":"documents\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the future architectural decisions made.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":14,"id":138,"text":"## Context\\nWe need to record the future architectural decisions made.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"containers\/2-.md","Context":"## Context\n======\nFedora has minimal base image too, and it will typically have more current versions of any software we need compared to the Ubuntu images.\n","Decision":"======\\nChange all Dockerfiles to use the latest `fedora-minimal` image as the base and update any necessary commands to work with Fedora system tools like `dnf`.\\n","tokens":34,"id":2619,"text":"## Context\\n======\\nFedora has minimal base image too, and it will typically have more current versions of any software we need compared to the Ubuntu images.\\n\n\n##Decision\n======\\nChange all Dockerfiles to use the latest `fedora-minimal` image as the base and update any necessary commands to work with Fedora system tools like `dnf`.\\n","Predictions":"* We will use the latest version of the Serializer library.n* We will use the latest version of the Serializer library.n* We will use the latest version of the Serializer library.n"}
{"File Name":"TANF-app\/001-ADR-template.md","Context":"## Context\nWe need to record the architectural decisions made on this project. This context section should include the history and driving reason(s) on why a decision needed to be made.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":37,"id":3784,"text":"## Context\\nWe need to record the architectural decisions made on this project. This context section should include the history and driving reason(s) on why a decision needed to be made.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).nADRs from here on out will be written in the first-person **singular** tense, since it's mainly just me building the app.n"}
{"File Name":"ontrack\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5122,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"Horace\/0017-separate-absolute-and-relative-indexing-APIs-in-pixel-array.md","Context":"## Context\nThe pixel array within an SQW object can be too large to fit into memory.\nTo avoid running out of memory, the object holding the pixel array can be\nfile-backed.\nThis means that only a \"page\" of the pixel array is loaded into memory at any\none time.\nTherefore two possible ways to index into the pixel array exist:\n1. **Absolute index**:\n_The position of the pixel in the full, file-backed, pixel array_.\n2. **Relative index**:\n_The position of the pixel in the currently loaded page of pixel data._\n","Decision":"There will be two separate APIs for accessing data,\\nthese APIs distinguish between the two types of indexing.\\n1. **Perform absolute indexing  using `get_` methods:**\\nObtaining a subset of pixels or pixel data by absolute index will be\\npossible using a `get_` method.\\nFor example, the following will retrieve pixels 100-200 by absolute index:\\n```matlab\\npixels.get_pixels(100:200)\\n```\\nSimilarly, to retrieve a range of data from particular pixel array fields:\\n```matlab\\npixels.get_data({'signal', 'variance'}, 100:200)\\n```\\nAt time of writing, there are no plans to implement similar `set_`\\nmethods with absolute indexing.\\nIf there becomes a requirement for setters with absolute indexing,\\nthese should follow a similar syntax.\\nFor example, the following should set signal and variance from absolute\\nindex 100 to 200, to zero:\\n```matlab\\nset_data({'signal', 'variance'}, 100:200, zeros(2, 101));\\n```\\n2. **Perform relative indexing using attribute:**\\nObtaining pixel data using an attribute will return just the data for the\\ncurrently cached page.\\nHence, indexing into these attributes will be relative.\\nFor example, the following will retrieve the signal values of pixels 10-20\\nin the currently cached page:\\n```matlab\\npixels.signal(10:20)\\n```\\nSetting of pixel data will be possible using attributes and relative\\nindexing.\\nFor example the following will set pixels 1 to 20 on the current page to\\nzero:\\n```matlab\\npixels.signal(1:20) = 0\\n```\\n","tokens":128,"id":4236,"text":"## Context\\nThe pixel array within an SQW object can be too large to fit into memory.\\nTo avoid running out of memory, the object holding the pixel array can be\\nfile-backed.\\nThis means that only a \"page\" of the pixel array is loaded into memory at any\\none time.\\nTherefore two possible ways to index into the pixel array exist:\\n1. **Absolute index**:\\n_The position of the pixel in the full, file-backed, pixel array_.\\n2. **Relative index**:\\n_The position of the pixel in the currently loaded page of pixel data._\\n\n\n##Decision\nThere will be two separate APIs for accessing data,\\nthese APIs distinguish between the two types of indexing.\\n1. **Perform absolute indexing  using `get_` methods:**\\nObtaining a subset of pixels or pixel data by absolute index will be\\npossible using a `get_` method.\\nFor example, the following will retrieve pixels 100-200 by absolute index:\\n```matlab\\npixels.get_pixels(100:200)\\n```\\nSimilarly, to retrieve a range of data from particular pixel array fields:\\n```matlab\\npixels.get_data({'signal', 'variance'}, 100:200)\\n```\\nAt time of writing, there are no plans to implement similar `set_`\\nmethods with absolute indexing.\\nIf there becomes a requirement for setters with absolute indexing,\\nthese should follow a similar syntax.\\nFor example, the following should set signal and variance from absolute\\nindex 100 to 200, to zero:\\n```matlab\\nset_data({'signal', 'variance'}, 100:200, zeros(2, 101));\\n```\\n2. **Perform relative indexing using attribute:**\\nObtaining pixel data using an attribute will return just the data for the\\ncurrently cached page.\\nHence, indexing into these attributes will be relative.\\nFor example, the following will retrieve the signal values of pixels 10-20\\nin the currently cached page:\\n```matlab\\npixels.signal(10:20)\\n```\\nSetting of pixel data will be possible using attributes and relative\\nindexing.\\nFor example the following will set pixels 1 to 20 on the current page to\\nzero:\\n```matlab\\npixels.signal(1:20) = 0\\n```\\n","Predictions":"The SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a separate API for accessing data.nThe SQW API will be a"}
{"File Name":"sre-challenge\/0004-using-aws-ecs-as-platform.md","Context":"## Context\nI want to use docker at AWS but I don't want to provision a docker\ninfrastructure myself.  So I'm going to use AWS ECS managed service and create\na cluster.\n","Decision":"Done.\\n","tokens":42,"id":2997,"text":"## Context\\nI want to use docker at AWS but I don't want to provision a docker\\ninfrastructure myself.  So I'm going to use AWS ECS managed service and create\\na cluster.\\n\n\n##Decision\nDone.\\n","Predictions":"I will use a stack folder to store the code.n"}
{"File Name":"lobiani\/0005-use-cqrs-architectural-style.md","Context":"## Context\nIn non-trivial projects with complex business rules, it is often a challenge to come up with the model that scales well\nfor reads and writes at the same time in regard to performance and maintainability.\n","Decision":"We will adopt Command Query Responsibility Segregation architectural style where there may exist 2 models for same\\ndomain entities each respectively on the Command (write) and Query (read) sides\\n","tokens":45,"id":1031,"text":"## Context\\nIn non-trivial projects with complex business rules, it is often a challenge to come up with the model that scales well\\nfor reads and writes at the same time in regard to performance and maintainability.\\n\n\n##Decision\nWe will adopt Command Query Responsibility Segregation architectural style where there may exist 2 models for same\\ndomain entities each respectively on the Command (write) and Query (read) sides\\n","Predictions":"We will use a docker-compose.yml as the primary entry point of the projectn"}
{"File Name":"testy\/0007-support-node-versions-with-at-least-security-updates.md","Context":"## Context\nMaking clear which versions are supported and how this will be updated as time passes.\n","Decision":"Only support Node versions with active and security support. Do not support newer, unstable versions.\\nWe can use [endoflife.date](https:\/\/endoflife.date\/nodejs) as a reference. Example: at the moment\\nof this decision, only Node 12, 14 and 16 should be supported.\\n","tokens":20,"id":1048,"text":"## Context\\nMaking clear which versions are supported and how this will be updated as time passes.\\n\n\n##Decision\nOnly support Node versions with active and security support. Do not support newer, unstable versions.\\nWe can use [endoflife.date](https:\/\/endoflife.date\/nodejs) as a reference. Example: at the moment\\nof this decision, only Node 12, 14 and 16 should be supported.\\n","Predictions":"We will use [SecuritySettings](https:\/\/securitysettings.com\/) to provide a way to set up a custom iFrame for each iFrame.n"}
{"File Name":"openlobby-server\/0003-just-api-without-frontend.md","Context":"## Context\nOpen Lobby must have open stable API.\n","Decision":"Open Lobby will be server just with an API based on API First design. Frontend\\nwill be written as separate application (web based, mobile, ...). This will\\nensure that there is a working API for anything that frontend application(s)\\nwill do.\\n","tokens":12,"id":476,"text":"## Context\\nOpen Lobby must have open stable API.\\n\n\n##Decision\nOpen Lobby will be server just with an API based on API First design. Frontend\\nwill be written as separate application (web based, mobile, ...). This will\\nensure that there is a working API for anything that frontend application(s)\\nwill do.\\n","Predictions":"We will use swift-api to create a web application.n"}
{"File Name":"platform\/2021-01-21-deprecation-strategy.md","Context":"## Context\nDefine a strategy for deprecations.\n","Decision":"### Dogma\\n* Don't do changes without feature-flags (only exception are bugfixes)\\n* Don't break things without an alternative\\n* Don't break things in a minor release\\n* Annotate upcoming breaks as soon as possible\\n* Test all new implementations and changes\\n* Be expressive and very verbose on instructions in your inline feature flag comments\\n* There is a world outside with developers that use our public code\\n### Synopsys\\nAs we decided to work in the trunk based development from now on, there are different kinds of cases we need to consider while implementing changes to not cause any breaks while developing for future features.\\nThe main difference we have to take in account, is if we break currently behaviour with our changes or not.\\nFor this difference we have 4 different cases:\\n* Minor Changes which don't cause any breaks or deprecations\\n* Minor Changes which cause deprecations\\n* Minor Changes as part of a major feature which don't cause any breaks\\n* Major changes which cause breaks\\nFor a quick overview this is how you have to deal with the different cases.\\nConcrete Examples and further explanation follow below.\\n#### Only Minor Changes (no breaks)\\nFeature and changes tend to be released in a minor release. Don't cause breaks. Simple additions, refactorings, etc\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Only Minor Changes (with deprecating code)\\nFeature and Changes tend to be released in a minor release and are developed in a backward compatible manner, but deprecate old code. For example a class is replaced by a new one.\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Major Changes (Breaks)\\nParts of a major feature or refactoring which breaks current behaviour. Removal of classes, methods or properties, change of signatures, business logic changes...\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* only difference between the case above is, that you have to take care about the fact, that the whole old behaviour needs to be fully functional until the next major.\\n* Write specific tests for the major flag which tests the new behaviour.\\n* Detailed description here [Detailed Rules](DetailedRules)\\n","tokens":12,"id":4497,"text":"## Context\\nDefine a strategy for deprecations.\\n\n\n##Decision\n### Dogma\\n* Don't do changes without feature-flags (only exception are bugfixes)\\n* Don't break things without an alternative\\n* Don't break things in a minor release\\n* Annotate upcoming breaks as soon as possible\\n* Test all new implementations and changes\\n* Be expressive and very verbose on instructions in your inline feature flag comments\\n* There is a world outside with developers that use our public code\\n### Synopsys\\nAs we decided to work in the trunk based development from now on, there are different kinds of cases we need to consider while implementing changes to not cause any breaks while developing for future features.\\nThe main difference we have to take in account, is if we break currently behaviour with our changes or not.\\nFor this difference we have 4 different cases:\\n* Minor Changes which don't cause any breaks or deprecations\\n* Minor Changes which cause deprecations\\n* Minor Changes as part of a major feature which don't cause any breaks\\n* Major changes which cause breaks\\nFor a quick overview this is how you have to deal with the different cases.\\nConcrete Examples and further explanation follow below.\\n#### Only Minor Changes (no breaks)\\nFeature and changes tend to be released in a minor release. Don't cause breaks. Simple additions, refactorings, etc\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Only Minor Changes (with deprecating code)\\nFeature and Changes tend to be released in a minor release and are developed in a backward compatible manner, but deprecate old code. For example a class is replaced by a new one.\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* Detailed description here [Detailed Rules](DetailedRules)\\n#### Major Changes (Breaks)\\nParts of a major feature or refactoring which breaks current behaviour. Removal of classes, methods or properties, change of signatures, business logic changes...\\n* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.\\n* When Development is completed, remove the feature flag and all the old code that is not used anymore\\n* Mark old code as deprecated and make sure it is not called anywhere else\\n* Make sure everything you removed has a working alternative implemented.\\n* Annotate everything in a manner that the removal of the deprecated code will be a no-brainer on the next major release\\n* only difference between the case above is, that you have to take care about the fact, that the whole old behaviour needs to be fully functional until the next major.\\n* Write specific tests for the major flag which tests the new behaviour.\\n* Detailed description here [Detailed Rules](DetailedRules)\\n","Predictions":"* Don't do changes without feature-flags (only exception are bugfixes)n* Don't break things without an alternativen* Don't break things in a minor releasen* Annotate upcoming breaks as soon as possiblen* Test all new implementations and changesn* Be expressive and very verbose on instructions in your inline feature flag commentsn* There is a world outside with developers that use our public coden### SynopsysnAs we decided to work in the trunk based development from now on, there are different kinds of cases we need to consider while implementing changes to not cause any breaks while developing for future features.nFor this difference we have 4 different cases:n* Minor Changes which don't cause any breaks or deprecationsn* Minor Changes which cause deprecationsn* Minor Changes as part of a major feature which don't cause any breaksn* Major changes which cause breaksnFor a quick overview this is how you have to deal with the different cases.nConcrete Examples and further explanation follow below.n#### Only Minor Changes (no breaks)nFeature and changes tend to be released in a minor release. Don't cause breaks. Simple additions, refactorings, etcn* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.n* When Development is completed, remove the feature flag and all the old code that is not used anymoren* Detailed description here [Detailed Rules](DetailedRules)n### Only Minor Changes (with deprecating code)nFeature and Changes tend to be released in a minor release. Don't cause breaks. Simple additions, refactorings, etcn* Put all your changes behind a feature flag, to be sure that nothing you have changed is called while developing is in progress.n* When Development is completed, remove the feature flag and all the old code that is not used anymoren* Detailed description here [Detailed Rules](Detailed Rules)n"}
{"File Name":"api-catalogue\/0001-remove-tech-docs-gem-dependency.md","Context":"## Context\nThe main data source of the API catalogue website was originally a collection of\nmarkdown files. The build process uses the Middleman static site generator\nconfigured by the [Tech Docs Gem](https:\/\/github.com\/alphagov\/tech-docs-gem)\n('TDG').\nThe TDG provides additional functionality including search, sidebar\nnavigation ('Table of Contents'), the layout, and styling.\nThe TDG is not necessarily a good fit for the API catalogue because the project\nisn't purely documentation, and our data source is now a CSV.\nIn particular it is difficult to override templates inherited from the gem, to\nadjust the layout on a particular page or add page-specific JavaScript for\nexample.\nUsing TDG to render the Table of Contents is slow for our site because\nby design every page is re-rendered multiple times to pull out the headings\n(adding over a minute to build times).\nThe TDG also requires specific dependency versions. These version\nrestrictions prevent us being in control of version upgrades which are necessary\nto remain on support versions and receive security patches.\n","Decision":"Remove the TDG as a dependency by vendoring the code relevant to\\nthe API catalogue directly into the project itself.\\n","tokens":228,"id":3586,"text":"## Context\\nThe main data source of the API catalogue website was originally a collection of\\nmarkdown files. The build process uses the Middleman static site generator\\nconfigured by the [Tech Docs Gem](https:\/\/github.com\/alphagov\/tech-docs-gem)\\n('TDG').\\nThe TDG provides additional functionality including search, sidebar\\nnavigation ('Table of Contents'), the layout, and styling.\\nThe TDG is not necessarily a good fit for the API catalogue because the project\\nisn't purely documentation, and our data source is now a CSV.\\nIn particular it is difficult to override templates inherited from the gem, to\\nadjust the layout on a particular page or add page-specific JavaScript for\\nexample.\\nUsing TDG to render the Table of Contents is slow for our site because\\nby design every page is re-rendered multiple times to pull out the headings\\n(adding over a minute to build times).\\nThe TDG also requires specific dependency versions. These version\\nrestrictions prevent us being in control of version upgrades which are necessary\\nto remain on support versions and receive security patches.\\n\n\n##Decision\nRemove the TDG as a dependency by vendoring the code relevant to\\nthe API catalogue directly into the project itself.\\n","Predictions":"We will use the [Teltr](https:\/\/github.com\/alphagov\/tech-docs-gem) as the project's static site generator.n"}
{"File Name":"deeplearning4j\/0009 - Import node pre processing.md","Context":"## Context\nNd4j's model import framework supports different protobuf based frameworks\nfor importing and executing models. This was introduced in [0003-Import_IR.md](0003-Import_IR.md)\nOne problem with importing models is compatibility between different versions of frameworks.\nOften,migrations are needed to handle compatibility between versions. A node pre processor is proposed\nthat: when combined with the model import framework allows for\nannotation based automatic upgrades of graphs.\n","Decision":"In order to handle preprocessing a node to handle things like upgrades.\\nAn end user can specify a pre processor via a combination of 2 interfaces:\\n1. An annotation for specifying a class that implements a relevant rule\\nfor processing. This will automatically be discoverable via annotation scanning\\nsimilar to other frameworks. This annotation looks as follows:\\n```kotlin\\nannotation class NodePreProcessor(val nodeTypes: Array<String>, val frameworkName: String)\\n```\\nThe information include the nodeTypes which are the operation types to scan for when doing upgrades on a graph.\\nThe framework name: relevant if multiple import modules are on the classpath. Filters rules\\nby their intended framework for import.\\n2. The necessary pre processing hook that will handle processing the node\\nand may modify the graph. Graph modification maybe necessary if we need to add new nodes to compensate\\nfor modification of a node such as an attribute moving to being an input.\\n```kotlin\\ninterface NodePreProcessorHook<NODE_TYPE : GeneratedMessageV3,\\nTENSOR_TYPE : GeneratedMessageV3,\\nATTRIBUTE_TYPE : GeneratedMessageV3,\\nATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_TYPE>\\nwhere  DATA_TYPE: ProtocolMessageEnum {\\nfun modifyNode(\\nnode: IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>,\\ngraph: IRGraph<GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, ProtocolMessageEnum>\\n): IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>\\n}\\n```\\n","tokens":94,"id":2945,"text":"## Context\\nNd4j's model import framework supports different protobuf based frameworks\\nfor importing and executing models. This was introduced in [0003-Import_IR.md](0003-Import_IR.md)\\nOne problem with importing models is compatibility between different versions of frameworks.\\nOften,migrations are needed to handle compatibility between versions. A node pre processor is proposed\\nthat: when combined with the model import framework allows for\\nannotation based automatic upgrades of graphs.\\n\n\n##Decision\nIn order to handle preprocessing a node to handle things like upgrades.\\nAn end user can specify a pre processor via a combination of 2 interfaces:\\n1. An annotation for specifying a class that implements a relevant rule\\nfor processing. This will automatically be discoverable via annotation scanning\\nsimilar to other frameworks. This annotation looks as follows:\\n```kotlin\\nannotation class NodePreProcessor(val nodeTypes: Array<String>, val frameworkName: String)\\n```\\nThe information include the nodeTypes which are the operation types to scan for when doing upgrades on a graph.\\nThe framework name: relevant if multiple import modules are on the classpath. Filters rules\\nby their intended framework for import.\\n2. The necessary pre processing hook that will handle processing the node\\nand may modify the graph. Graph modification maybe necessary if we need to add new nodes to compensate\\nfor modification of a node such as an attribute moving to being an input.\\n```kotlin\\ninterface NodePreProcessorHook<NODE_TYPE : GeneratedMessageV3,\\nTENSOR_TYPE : GeneratedMessageV3,\\nATTRIBUTE_TYPE : GeneratedMessageV3,\\nATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_TYPE>\\nwhere  DATA_TYPE: ProtocolMessageEnum {\\nfun modifyNode(\\nnode: IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>,\\ngraph: IRGraph<GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, GeneratedMessageV3, ProtocolMessageEnum>\\n): IRNode<NODE_TYPE, TENSOR_TYPE, ATTRIBUTE_TYPE, ATTRIBUTE_VALUE_TYPE, DATA_TYPE>\\n}\\n```\\n","Predictions":"In order to handle preprocessing a node to handle things like upgrades.nAn end user can specify a pre processor via a combination of 2 interfaces:n1. An annotation for specifying a class that implements a relevant rulenfor processing. This will automatically be discoverable via annotation scanningnsimilar to other frameworks. This annotation looks as follows:nkotlinninterface NodePreProcessorHookNODE_TYPE : GeneratedMessageV3,nTENSOR_TYPE : GeneratedMessageV3,nATTRIBUTE_TYPE : GeneratedMessageV3,nATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_TYPE>nwhere DATA_TYPE: ProtocolMessageEnum nfun modifyNode(nnode: IRNodeNODE_TYPE, TENSOR_TYPE, ATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_VALUE_TYPE>nwhere DATA_TYPE: ProtocolMessageEnum nfun modifyNode(nnode: IRNodeNODE_TYPE, TENSOR_TYPE, ATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_VALUE_TYPE>nwhere DATA_TYPE: ProtocolMessageEnum nfun modifyNode(nnode: IRNodeNODE_TYPE, TENSOR_TYPE, ATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_VALUE_TYPE>nwhere DATA_TYPE: ProtocolMessageEnum nfun modifyNode(nnode: IRNodeNODE_TYPE, TENSOR_TYPE, ATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_VALUE_TYPE>nwhere DATA_TYPE: ProtocolMessageEnum nfun modifyNode(nnode: IRNodeNODE_TYPE, TENSOR_TYPE, ATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_VALUE_TYPE>nwhere DATA_VALUE_TYPE: ProtocolMessageEnum nfun modifyNode(nnode: IRNodeNODE_TYPE, TENSOR_TYPE, ATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_VALUE_TYPE>nwhere DATA_VALUE_TYPE: ProtocolMessageEnum nfun modifyNode(nnode: IRNodeNODE_TYPE, TENSOR_TYPE, ATTRIBUTE_VALUE_TYPE : GeneratedMessageV3, DATA_VALUE_TYPE>nwhere DATA_VALUE_TYPE:"}
{"File Name":"read-more-api\/0007-deploy-to-heroku.md","Context":"## Context\nThe application needs to be deployed somewhere.\nThere are many container orchestration platforms for configuring a cluster of machines running Docker containers and managing the deployment of images on to these containers. Setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines.\nHeroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.\nHeroku has several pricing tiers for machines that the application will run on, including a free tier.\nHeroku provides a free hosted PostgreSQL option. It will handle setting a \"DATABASE_URL\" environment variable, containing the information required to connect to this database. The free tier database is limited to 10,000 rows.\nWe want the setup process to be as simple as possible.\n","Decision":"We will host the application on Heroku, using their Container Registry solution with a hosted PostgreSQL database.\\n","tokens":193,"id":718,"text":"## Context\\nThe application needs to be deployed somewhere.\\nThere are many container orchestration platforms for configuring a cluster of machines running Docker containers and managing the deployment of images on to these containers. Setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines.\\nHeroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https:\/\/devcenter.heroku.com\/articles\/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.\\nHeroku has several pricing tiers for machines that the application will run on, including a free tier.\\nHeroku provides a free hosted PostgreSQL option. It will handle setting a \"DATABASE_URL\" environment variable, containing the information required to connect to this database. The free tier database is limited to 10,000 rows.\\nWe want the setup process to be as simple as possible.\\n\n\n##Decision\nWe will host the application on Heroku, using their Container Registry solution with a hosted PostgreSQL database.\\n","Predictions":"We will use [Semver](https:\/\/semver.io\/) to create a single SPA for each application.n"}
{"File Name":"tendermint\/adr-003-abci-app-rpc.md","Context":"## Context\nABCI-server could expose its own RPC-server and act as a proxy to Tendermint.\nThe idea was for the Tendermint RPC to just be a transparent proxy to the app.\nClients need to talk to Tendermint for proofs, unless we burden all app devs\nwith exposing Tendermint proof stuff. Also seems less complex to lock down one\nserver than two, but granted it makes querying a bit more kludgy since it needs\nto be passed as a `Query`. Also, **having a very standard rpc interface means\nthe light-client can work with all apps and handle proofs**. The only\napp-specific logic is decoding the binary data to a more readable form (eg.\njson). This is a huge advantage for code-reuse and standardization.\n","Decision":"We dont expose an RPC server on any of our ABCI-apps.\\n","tokens":161,"id":1985,"text":"## Context\\nABCI-server could expose its own RPC-server and act as a proxy to Tendermint.\\nThe idea was for the Tendermint RPC to just be a transparent proxy to the app.\\nClients need to talk to Tendermint for proofs, unless we burden all app devs\\nwith exposing Tendermint proof stuff. Also seems less complex to lock down one\\nserver than two, but granted it makes querying a bit more kludgy since it needs\\nto be passed as a `Query`. Also, **having a very standard rpc interface means\\nthe light-client can work with all apps and handle proofs**. The only\\napp-specific logic is decoding the binary data to a more readable form (eg.\\njson). This is a huge advantage for code-reuse and standardization.\\n\n\n##Decision\nWe dont expose an RPC server on any of our ABCI-apps.\\n","Predictions":"* Use AWS S3 v1 to build the appn"}
{"File Name":"dp\/0009.md","Context":"## Context\nDatasets are made up of observations and dimensions. Each of the options for a\ndimension have wider definitions and metadata which apply to that option in all\nscenarios, across multiple datasets. These code definitions live in code-lists.\nWhen storing both datasets and code-lists in a graph database, there is some\noverlap between the information stored on a code and on a dimension option. A\ndesign decision was needed as to whether the dimension option and code nodes\nin the graph database should be collapsed into a single node, with the relevant\nrelationships for each coming off that single node.\n","Decision":"Dimension options and codes should not be collapsed into a single node. Dimension\\noptions have relationships to every observation within an imported dataset, and\\nadding those relationships to the code node would result in relationships to\\nevery observation across all datasets - a significant and unnecessary burden.\\n","tokens":126,"id":5118,"text":"## Context\\nDatasets are made up of observations and dimensions. Each of the options for a\\ndimension have wider definitions and metadata which apply to that option in all\\nscenarios, across multiple datasets. These code definitions live in code-lists.\\nWhen storing both datasets and code-lists in a graph database, there is some\\noverlap between the information stored on a code and on a dimension option. A\\ndesign decision was needed as to whether the dimension option and code nodes\\nin the graph database should be collapsed into a single node, with the relevant\\nrelationships for each coming off that single node.\\n\n\n##Decision\nDimension options and codes should not be collapsed into a single node. Dimension\\noptions have relationships to every observation within an imported dataset, and\\nadding those relationships to the code node would result in relationships to\\nevery observation across all datasets - a significant and unnecessary burden.\\n","Predictions":"We decided to use a serialize method to store the data in a single node.n"}
{"File Name":"git-en-boite\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3505,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"planet4-docs\/adr-0001-use-gitbook-for-technical-documentation.md","Context":"### Context and Problem Statement\nCurrently Technical Documentation lives in the Handbook, with an [entry point](https:\/\/planet4.greenpeace.org\/create\/tech\/) and everything else organized under the [Development tag](https:\/\/planet4.greenpeace.org\/tag\/development\/). On the other hand Gitbook is a platform specifically for maintaining documentation, based on Markdown. We created a [Planet 4 space](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/) in GP Gitbook account to test how a potential structure of our documentation would look.\n### Decision Drivers\n* Community Management Team should be consulted, since this affects the internal development community too.\n### Considered Options\n* Move to Gitbook\n* Stay in the Handbook\n### Decision Outcome\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\n### Pros and Cons of the Options\n#### Move to Gitbook\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\n#### Stay in the Handbook\n* Good, because everything regarding P4 is in one place.\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\n* Bad, because we sometimes need to do custom development just for the Handbook.\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\n### Links\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\n","Decision":"* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n","tokens":492,"id":4087,"text":"### Context and Problem Statement\\nCurrently Technical Documentation lives in the Handbook, with an [entry point](https:\/\/planet4.greenpeace.org\/create\/tech\/) and everything else organized under the [Development tag](https:\/\/planet4.greenpeace.org\/tag\/development\/). On the other hand Gitbook is a platform specifically for maintaining documentation, based on Markdown. We created a [Planet 4 space](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/) in GP Gitbook account to test how a potential structure of our documentation would look.\\n### Decision Drivers\\n* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n\n\n##Decision\n* Community Management Team should be consulted, since this affects the internal development community too.\\n### Considered Options\\n* Move to Gitbook\\n* Stay in the Handbook\\n### Decision Outcome\\nChosen option: **Decided to move to Gitbook all Technical documentation \\(Development & Infrastructure\\)**\\n### Pros and Cons of the Options\\n#### Move to Gitbook\\n* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.\\n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.\\n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.\\n* Good, because the documentation on the repo is just Markdown so we are not locked into Gitbook, if we ever choose to leave.\\n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org \\(eg. Global Support\\), so people can discover it easier but they are also used to it.\\n#### Stay in the Handbook\\n* Good, because everything regarding P4 is in one place.\\n* Bad, because the perception is that the Handbook is for internal audience. That\u2019s not always the case for the technical documentation.\\n* Bad, because we sometimes need to do custom development just for the Handbook.\\nBad, because it demands manual styling customization for technical documentation \\(e.g scripts\/coding pieces\\) or instructions.\\n### Links\\n* [P4 space on Gitbook](https:\/\/app.gitbook.com\/@greenpeace\/s\/planet4\/)\\n* [Git Repo](https:\/\/github.com\/greenpeace\/planet4-docs\/)\\n","Predictions":"* Community Management Team should be consulted, since this affects the internal development community too.n### Considered Optionsn* Move to Gitbookn* Stay in the Handbookn### Decision OutcomenChosen option: **Decided to move to Gitbook all Technical documentation (Development & Infrastructure)**n### Pros and Cons of the Optionsn* Good, because Gitbook is meant to be used as a documentation platform providing better UI\/UX for that task.n* Good, because docs can be grouped in categories, or browsed as a series of a guide, creating a user journey.n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.n* Good, because it\u2019s connected and synced with a public Github repo, so it\u2019s easy for an external contributor to update the documentation by opening a Pull Request.n* Good, because it\u2019s the choice of [other teams](https:\/\/app.gitbook.com\/@greenpeace\/spaces) in the Org (eg. Global Support), so people can discover it easier but they are also used to it.n### Stay in the Handbookn* Good, because everything regarding P4 is in one place.n* Bad, because the perception is that the Handbook is for internal audience.n"}
{"File Name":"apply-for-teacher-training\/0003-initial-datamodel.md","Context":"## Context\nIn the coming months we will be building this application. The first order of business is to come up with a sensible data model and application structure.\n","Decision":"We discussed a database model that is based around a small number of tables with a lot of columns. In particular:\\n- `Candidate`, primarily used for authentication\\n- `ApplicationForm`, which belongs to a Candidate, and holds the\\ninformation that is common to all of the applications that the user\\nmakes (in the \"Apply 1\" phase, they can apply for up to three courses).\\n- `ApplicationChoice`, which holds the information that is course specific,\\nlike the status of the application and the personal statement. It holds a reference to the \"course choice\", which is the combination of course, provider and the training location.\\nThere will be some other models that hold references, work histories and qualifications.\\nThe eventual table structure will look something like this:\\n![data model](0003-initial-datamodel.png)\\n","tokens":33,"id":4021,"text":"## Context\\nIn the coming months we will be building this application. The first order of business is to come up with a sensible data model and application structure.\\n\n\n##Decision\nWe discussed a database model that is based around a small number of tables with a lot of columns. In particular:\\n- `Candidate`, primarily used for authentication\\n- `ApplicationForm`, which belongs to a Candidate, and holds the\\ninformation that is common to all of the applications that the user\\nmakes (in the \"Apply 1\" phase, they can apply for up to three courses).\\n- `ApplicationChoice`, which holds the information that is course specific,\\nlike the status of the application and the personal statement. It holds a reference to the \"course choice\", which is the combination of course, provider and the training location.\\nThere will be some other models that hold references, work histories and qualifications.\\nThe eventual table structure will look something like this:\\n![data model](0003-initial-datamodel.png)\\n","Predictions":"We discussed a database model that is based around a small number of tables with a lot of columns. In particular:n- Candidate, primarily used for authenticationn- ApplicationForm, which belongs to a Candidate, and holds theninformation that is common to all of the applications that the usernmakes (in the \"Apply 1\" phase, they can apply for up to three courses).n- ApplicationChoice, which holds the information that is course specific,nlike the status of the application and the personal statement. It holds a reference to the \"course choice\", which is the combination of course, provider and the training location.nThere will be some other models that hold references, work histories and qualifications.nThe eventual table structure will look something like this:n![data model](0003-initial-datamodel.png)n"}
{"File Name":"architecture-decision-log\/0013-microservices-overview.md","Context":"* [Context](#context)\n* [Decision](#decision)\n* [Status](#status)\n* [Consequences](#consequences)\n* [More reading](#more-reading)\n* [Updates](#updates)\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWithin this ADR, we're going to keep an overview of our current microservices architecture. Each application has a pre-defined boundary, as well as a well-defined technical definition.\\nYou can take a look at our [live architecture](https:\/\/app.mural.co\/t\/d4c6342\/m\/d4c6342\/1602612797113\/5836de51b220fa5ebc8cefd7ad0f161125610524) or check the diagram below:\\n![Diagram showing our microservices architecture](..\/assets\/0013-microservices-overview\/diagram.png)\\n","tokens":53,"id":4910,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWithin this ADR, we're going to keep an overview of our current microservices architecture. Each application has a pre-defined boundary, as well as a well-defined technical definition.\\nYou can take a look at our [live architecture](https:\/\/app.mural.co\/t\/d4c6342\/m\/d4c6342\/1602612797113\/5836de51b220fa5ebc8cefd7ad0f161125610524) or check the diagram below:\\n![Diagram showing our microservices architecture](..\/assets\/0013-microservices-overview\/diagram.png)\\n","Predictions":"* [Status](#status)n* [Consequences](#consequences)n* [More reading](#more-reading)n* [Updates](#updates)nWe are going to use Behavior-Driven Development to help us reduce that gap. BDD is a technique where we use User Stories to create acceptance tests based on user behaviors.nInstead of writing complex test statements, we use a single-choice approach to test the application.n"}
{"File Name":"volley-management\/0007-follow-optimistic-concurrency.md","Context":"## Context and Problem Statement\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\n## Decision Drivers <!-- optional -->\n* Performance - decision should support high throughput scenarios\n* Maintainability - amount of code needed to write should be minimized\n","Decision":"* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\nChosen option: \"Optimistic concurrency\", because it is a better from the perfromance perspective and it will support our choice to [use Azure Table Storage](0006-use-azure-table-storage-for-persistence.md).\\n### Positive Consequences <!-- optional -->\\n* Concurrency check will fail very rarely so it will have almost no overhead\\n* It is aligned with Azure Table storage Optimistic concurrency\\n### Negative Consequences <!-- optional -->\\n* Whole application will have a responsibility to manage versions properly\\n* If we change a datastore we might need to have a larger change\\n","tokens":86,"id":2526,"text":"## Context and Problem Statement\\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\\n## Decision Drivers <!-- optional -->\\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\n\n\n##Decision\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\nChosen option: \"Optimistic concurrency\", because it is a better from the perfromance perspective and it will support our choice to [use Azure Table Storage](0006-use-azure-table-storage-for-persistence.md).\\n### Positive Consequences <!-- optional -->\\n* Concurrency check will fail very rarely so it will have almost no overhead\\n* It is aligned with Azure Table storage Optimistic concurrency\\n### Negative Consequences <!-- optional -->\\n* Whole application will have a responsibility to manage versions properly\\n* If we change a datastore we might need to have a larger change\\n","Predictions":"* Performance - decision should support high throughput scenariosn* Maintainability - amount of code needed to write should be minimizednChosen option: \"Optimistic concurrency\", because it is a better from the perfromance perspective and it will support our choice to [use Azure Table Storage](0006-use-azure-table-storage-for-persistence.md).n### Positive Consequences!-- optional -->n* Concurrency check will fail very rarely so it will have almost no overheadn* It is aligned with Azure Table storage Optimistic concurrencyn### Negative Consequences!-- optional -->n* Whole application will have a responsibility to manage versions properlyn* If we change a datastore we might need to have a larger changen"}
{"File Name":"platform\/2020-08-28-import-acl-privileges-from-other-roles.md","Context":"## Context\nSome modules have components which require many acl privileges. Examples\nare the rule builder or the media manager. Therefore, you need all privileges\nin each module which have these components. Also you do not want to add the\nmodule to the dependency section because then the user has full access to module\nin the administration.\n","Decision":"To avoid duplication of these privileges we use a helper function. These\\nfunction returns all privileges from the other module dynamically. You can\\nuse it directly in the privileges:\\n```js\\nShopware.Service('privileges')\\n.addPrivilegeMappingEntry({\\ncategory: 'permissions',\\nparent: null,\\nkey: 'promotion',\\nroles: {\\nviewer: {\\nprivileges: ['promotion:read',],\\ndependencies: []\\n},\\neditor: {\\nprivileges: [\\n'promotion:update',\\nShopware.Service('privileges').getPrivileges('rule.creator')\\n],\\ndependencies: [\\n'promotion.viewer'\\n]\\n}\\n}\\n});\\n```\\n","tokens":68,"id":4525,"text":"## Context\\nSome modules have components which require many acl privileges. Examples\\nare the rule builder or the media manager. Therefore, you need all privileges\\nin each module which have these components. Also you do not want to add the\\nmodule to the dependency section because then the user has full access to module\\nin the administration.\\n\n\n##Decision\nTo avoid duplication of these privileges we use a helper function. These\\nfunction returns all privileges from the other module dynamically. You can\\nuse it directly in the privileges:\\n```js\\nShopware.Service('privileges')\\n.addPrivilegeMappingEntry({\\ncategory: 'permissions',\\nparent: null,\\nkey: 'promotion',\\nroles: {\\nviewer: {\\nprivileges: ['promotion:read',],\\ndependencies: []\\n},\\neditor: {\\nprivileges: [\\n'promotion:update',\\nShopware.Service('privileges').getPrivileges('rule.creator')\\n],\\ndependencies: [\\n'promotion.viewer'\\n]\\n}\\n}\\n});\\n```\\n","Predictions":"To avoid duplication of these privileges we use a helper function. Thesenfunction returns all privileges from the other module dynamically. You cannuse it directly in the privileges:njsnShopware.Service('privileges')n.addPrivilegeMappingEntry(ncategory: 'permissions',nparent: null,nkey: 'promotion',nroles: nviewer: nprivileges: ['promotion:read',ndependencies: []n,neditor: nprivileges: [n'promotion:update',nShopware.Service('privileges')ndependencies: [n'promotion.viewer'n]nn);n"}
{"File Name":"front-end-monorepo\/adr-38.md","Context":"## Context\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. As part of this, volunteers can browse a subject set, in the classifier, while they decide which subject they wish to work on.\nThe classifier's subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. Subjects were discarded after being classified. Going backwards through the queue, to view previous subjects, was not possible.\n","Decision":"- The subject queue was changed from an ordered map, `subjects.resources`, to an array `subjects.queue`. Each item in `subjects.queue` is a subject ID, pointing to a subject in `subjects.resources`. Subjects are shown to the volunteer in array order.\\n- Existing workflows continue to use the orignal queueing logic: subjects are shifted off the array after being classified. The active subject is always the first entry in `subjects.queue`.\\n- Indexed subject sets, for Engaging Crowds, use a different logic. Subject IDs are never removed from the queue. Volunteers can browse available subjects by moving forwards and backwards through the array, changing the active index.\\n","tokens":119,"id":532,"text":"## Context\\nEngaging Crowds allows volunteers to browse and search subject sets, linked to a workflow, in order to pick which subjects they want to work on. As part of this, volunteers can browse a subject set, in the classifier, while they decide which subject they wish to work on.\\nThe classifier's subject queue was originally built to support random or sequential subject selection, with subjects being shown to a volunteer in the order they were received from the Panoptes API. Subjects were discarded after being classified. Going backwards through the queue, to view previous subjects, was not possible.\\n\n\n##Decision\n- The subject queue was changed from an ordered map, `subjects.resources`, to an array `subjects.queue`. Each item in `subjects.queue` is a subject ID, pointing to a subject in `subjects.resources`. Subjects are shown to the volunteer in array order.\\n- Existing workflows continue to use the orignal queueing logic: subjects are shifted off the array after being classified. The active subject is always the first entry in `subjects.queue`.\\n- Indexed subject sets, for Engaging Crowds, use a different logic. Subject IDs are never removed from the queue. Volunteers can browse available subjects by moving forwards and backwards through the array, changing the active index.\\n","Predictions":"The front-house was a bare metal structure with a single ribosome and a ribosome that was able to hold the user's keys and source chain.nThe back-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe front-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe back-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe front-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe back-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe front-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe back-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe front-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe back-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe front-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe back-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe front-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe back-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe front-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe back-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe front-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe back-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe front-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe back-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe front-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe back-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe front-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe back-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe front-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe back-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe front-house was a bare metal structure with a single ribosome that was able to hold the user's keys and source chain.nThe back-house was a bare metal structure with a single ribosome that"}
{"File Name":"architecture-decision-log\/0016-analytics-foundations.md","Context":"## Context\nOur company is starting to growth fast. With that growth, it is common to see the need of complex data analysis. We've solved that by installing Metabase in a read-replica of our OKR transactional database, but even that structure lacks more complex analytics. Concurrently with the previous statement, our company plans to create an analytics product for our customers, enabling real-time complex analysis of their users.\nWe can't ignore the need to have a proper analytics foundations inside Bud. Also, we can't afford investing a large amount of time building that infrastructure, since everything could change fast. We need to find a way to create a flexible analytics infrastructure that could:\n(a) Provide meaningful data regarding our customers;\n(b) Be flexible enought to integrate with multiple sources;\n(c) Allow the usage from external applications.\nIn a nutshel, that infrastructure will be the primary source of truth of our company. We could allow customers to fetch data from it. Even our applications could use it in their scopes.\n## Decision Drivers\n1. Flexibility\n2. How easy it is to integrate with external sources\n3. Implementation difficulty\n","Decision":"1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\nAfter evaluating all options, we've decided to proceed with Airbyte. It meets almost every specification that we have. It is extremelly easy to implement and follows all the best standards. It isn't an in-house solution, but in the current scenario we're on that would not be a big deal with it. Also, we could learn from it and maybe create a new tool in the future, designed to met our needs.\\n### Positivo Consequences\\nWith this infrastructure, we're going to achieve a robust ELT infrastructure, with little effort. We can easily create an analytics application that is going to serve all our business requirements with minimal effort. Also, Airbyte uses DBT under the hood, that being said, even if we need to change our ELT structure, we would still be able to migrate our DBT project.\\n### Negative Consequences\\nThere are two main negative consequences of this decision:\\n#### 1. Not being able to query real time data\\nAs pointed by Marcelo Travi, with Airbyte we would not be able to query real-time data. In most use-cases that would not be an issue, but considering that our first requirement is serving an evolution graph of a given key-result percentual proggress, that should be an issue. To fix that, we suggest using only D-1 data from our analytics, and using the transactional data to enrich it.\\n#### 2. Large queries could affect our applications\\nAs pointed by Victor Perin, since this is not a common implementation, we should be aware that complex analytics queries could impact our applications, since we're integrating them directly. There are plenty of ways to fix that, the easiest one would be creating a read-replica of our data warehouse, and using that to query the analytics. By doing so, we would separate both analytics and BI, minizing this issue.\\n","tokens":240,"id":4909,"text":"## Context\\nOur company is starting to growth fast. With that growth, it is common to see the need of complex data analysis. We've solved that by installing Metabase in a read-replica of our OKR transactional database, but even that structure lacks more complex analytics. Concurrently with the previous statement, our company plans to create an analytics product for our customers, enabling real-time complex analysis of their users.\\nWe can't ignore the need to have a proper analytics foundations inside Bud. Also, we can't afford investing a large amount of time building that infrastructure, since everything could change fast. We need to find a way to create a flexible analytics infrastructure that could:\\n(a) Provide meaningful data regarding our customers;\\n(b) Be flexible enought to integrate with multiple sources;\\n(c) Allow the usage from external applications.\\nIn a nutshel, that infrastructure will be the primary source of truth of our company. We could allow customers to fetch data from it. Even our applications could use it in their scopes.\\n## Decision Drivers\\n1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\n\n\n##Decision\n1. Flexibility\\n2. How easy it is to integrate with external sources\\n3. Implementation difficulty\\nAfter evaluating all options, we've decided to proceed with Airbyte. It meets almost every specification that we have. It is extremelly easy to implement and follows all the best standards. It isn't an in-house solution, but in the current scenario we're on that would not be a big deal with it. Also, we could learn from it and maybe create a new tool in the future, designed to met our needs.\\n### Positivo Consequences\\nWith this infrastructure, we're going to achieve a robust ELT infrastructure, with little effort. We can easily create an analytics application that is going to serve all our business requirements with minimal effort. Also, Airbyte uses DBT under the hood, that being said, even if we need to change our ELT structure, we would still be able to migrate our DBT project.\\n### Negative Consequences\\nThere are two main negative consequences of this decision:\\n#### 1. Not being able to query real time data\\nAs pointed by Marcelo Travi, with Airbyte we would not be able to query real-time data. In most use-cases that would not be an issue, but considering that our first requirement is serving an evolution graph of a given key-result percentual proggress, that should be an issue. To fix that, we suggest using only D-1 data from our analytics, and using the transactional data to enrich it.\\n#### 2. Large queries could affect our applications\\nAs pointed by Victor Perin, since this is not a common implementation, we should be aware that complex analytics queries could impact our applications, since we're integrating them directly. There are plenty of ways to fix that, the easiest one would be creating a read-replica of our data warehouse, and using that to query the analytics. By doing so, we would separate both analytics and BI, minizing this issue.\\n","Predictions":"1. Flexibilityn2. How easy it is to integrate with external sourcesn3. Implementation difficultynIn a nutshel, that infrastructure will be the primary source of truth of our company. We could allow customers to fetch data from it. Even our applications could use it in their scopes.n### Positivo ConsequencesnWith this infrastructure, we're going to achieve a robust ELT infrastructure, with little effort. We can easily create an analytics application that is going to serve all our business requirements with minimal effort. Also, Airbyte uses DBT under the hood, that being said, even if we need to change our ELT structure, we would still be able to migrate our DBT project.n### Negative ConsequencesnThere are two main negative consequences of this decision:n1. Flexibilityn2. How easy it is to integrate with external sourcesn3. Implementation difficultynIn a nutshel, that infrastructure will be the primary source of truth of our company. We could allow customers to fetch data from it. Even our applications could use it in their scopes.n### Positive Consequencesn1. We can easily create an analytics application for our customers, enabling real-time complex analysis of their users.n### Negative Consequencesn1. We can't ignore the need to have a proper analytics foundations inside Bud. Also, we can't afford to invest a large amount of time building that infrastructure, since everything could change fast. We need to find a way to create a flexible analytics infrastructure that could:n(a) Provide meaningful data regarding our customers;n(b) Be flexible enought to integrate with multiple sources;n(c) Allow the usage from external applications.nIn a nutshel, that infrastructure will be the primary source of truth of our company. We could allow customers to fetch data from it. Even our applications could use it in their scopes.n### Negative Consequencesn1. We can't ignore the need to have a proper analytics foundations inside Bud. Also, we can't afford to invest a large amount of time building that infrastructure, since everything could change fast. We need to find a way to create a flexible analytics infrastructure that could:n(a) Provide meaningful data regarding our customers;n(b) Be flexible enought to integrate with multiple sources;n(c) Allow the usage from external applications.n"}
{"File Name":"eq-author-app\/0002-routing-mk2.md","Context":"## Context\nRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.\nAs these conditions can be any boolean expression we need a more complex routing structure to handle\n","Decision":"Implement a new version of the routing in the API that can handle nested expressions.\\nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2\\n### Terminology\\n- Routing - all rules etc for a page with a fallback destination when no rules match.\\n- Rule - a series of expressions that when evaluated to true will go to the destination specified.\\n- Expression - Either a binary expression or expression group\\n- Expression Group - A group of BinaryExpressions combined as either `and` or `or`.\\n- Binary Expression - The base item with a left hand side, condtion and right hand side.\\n- Left hand side - The item being compared against (e.g. a question)\\n- Condition - A way of comparing left and right e.g. `=`, `>`, `>=`, `includes`, `not_includes`\\n- Right hand side - The value being compared against a left.\\n### Example\\nOn page 1, when the user enters 5 go to page 2, when they select red or white go to page 3, otherwise go to page 4.\\n```\\npage: {\\ntitle: \"Page 1\",\\nanswers: [\\n{ id: \"answer1\", type: NUMBER, label: \"Answer\" }\\n],\\nrouting: {\\nrules: [\\n{\\nexpressionGroup: {\\noperator: \"AND\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer1\",\\ntype: CURRENCY\\n}\\n},\\ncondition: \"Equals\",\\nright: {\\nnumber: 5\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page2\"\\n}\\n}\\n},\\n{\\nexpressionGroup: {\\noperator: \"OR\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer2\",\\ntype: RADIO\\n}\\n},\\ncondition: \"OneOf\",\\nright: {\\noptions: [\\n{\\nvalue: \"red\"\\n},\\n{\\nvalue: \"white\"\\n}\\n]\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page3\"\\n}\\n}\\n}\\n],\\nelse: {\\npage: {\\nid: \"Page4\"\\n}\\n}\\n}\\n}\\n```\\n","tokens":51,"id":2111,"text":"## Context\\nRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.\\nAs these conditions can be any boolean expression we need a more complex routing structure to handle\\n\n\n##Decision\nImplement a new version of the routing in the API that can handle nested expressions.\\nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2\\n### Terminology\\n- Routing - all rules etc for a page with a fallback destination when no rules match.\\n- Rule - a series of expressions that when evaluated to true will go to the destination specified.\\n- Expression - Either a binary expression or expression group\\n- Expression Group - A group of BinaryExpressions combined as either `and` or `or`.\\n- Binary Expression - The base item with a left hand side, condtion and right hand side.\\n- Left hand side - The item being compared against (e.g. a question)\\n- Condition - A way of comparing left and right e.g. `=`, `>`, `>=`, `includes`, `not_includes`\\n- Right hand side - The value being compared against a left.\\n### Example\\nOn page 1, when the user enters 5 go to page 2, when they select red or white go to page 3, otherwise go to page 4.\\n```\\npage: {\\ntitle: \"Page 1\",\\nanswers: [\\n{ id: \"answer1\", type: NUMBER, label: \"Answer\" }\\n],\\nrouting: {\\nrules: [\\n{\\nexpressionGroup: {\\noperator: \"AND\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer1\",\\ntype: CURRENCY\\n}\\n},\\ncondition: \"Equals\",\\nright: {\\nnumber: 5\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page2\"\\n}\\n}\\n},\\n{\\nexpressionGroup: {\\noperator: \"OR\",\\nexpressions: [\\n{\\nleft: {\\nanswer: {\\nid: \"Answer2\",\\ntype: RADIO\\n}\\n},\\ncondition: \"OneOf\",\\nright: {\\noptions: [\\n{\\nvalue: \"red\"\\n},\\n{\\nvalue: \"white\"\\n}\\n]\\n}\\n}\\n]\\n},\\ndestination: {\\npage: {\\nid: \"Page3\"\\n}\\n}\\n}\\n],\\nelse: {\\npage: {\\nid: \"Page4\"\\n}\\n}\\n}\\n}\\n```\\n","Predictions":"We will implement a new version of the routing in the API that can handle nested expressions.nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2n### TerminologynRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2n### TerminologynRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2n### TerminologynRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2n### TerminologynRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2n### TerminologynRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2n### TerminologynRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2n### TerminologynRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2n### TerminologynRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2n### TerminologynRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2n### TerminologynRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2n### TerminologynRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the answers on a question page.nThis is heavily inspied by https:\/\/github.com\/ONSdigital\/eq-author-app\/wiki\/Routing,-MK2n### TerminologynRouting is a system to allow a user to describe conditions as to where the survey respondent should go after they complete the"}
{"File Name":"Maud\/0011-drains.md","Context":"## Context\nDrains are an important aspect of any model, they are an essential boundary\ncondition for cases such as biomass drains. Curently rate laws are only specified using\nthe Generalised MWC format with the catalytic aspect using the modular rate law.\nIntroducing drains into Maud requires implementing this into the ODE system which\nrelies on enzymes as the defining feature, which are attributes of reactions in Maud.\nTo specify drains we create a new class independent of reactions, despite the fact that\nthey occur in the system of ODEs. This benefits post-processing as some techniques rely\non varying enzyme concentrations, which we've established that drains do not have.\nDrains are considered as priors to ensure that prior predictive checks are informative\nof the experimental condition.\n","Decision":"Drains will not be considered as a reaction class.\\n","tokens":164,"id":233,"text":"## Context\\nDrains are an important aspect of any model, they are an essential boundary\\ncondition for cases such as biomass drains. Curently rate laws are only specified using\\nthe Generalised MWC format with the catalytic aspect using the modular rate law.\\nIntroducing drains into Maud requires implementing this into the ODE system which\\nrelies on enzymes as the defining feature, which are attributes of reactions in Maud.\\nTo specify drains we create a new class independent of reactions, despite the fact that\\nthey occur in the system of ODEs. This benefits post-processing as some techniques rely\\non varying enzyme concentrations, which we've established that drains do not have.\\nDrains are considered as priors to ensure that prior predictive checks are informative\\nof the experimental condition.\\n\n\n##Decision\nDrains will not be considered as a reaction class.\\n","Predictions":"We will use PostgreSQL as the storage for all the CI\/CD state.n"}
{"File Name":"platform\/2020-08-14-implement-individual-sorting.md","Context":"## Context\nShop owners should be able to define custom sorting options for product listings and search result pages out of the administration.\nIt should be possible to define a system default sorting option for product listings.\n`Top Results` will be the default on search pages and suggest route, which sorts products by `_score`.\nCurrently, to define a custom sorting option, you need to define it as a service and tag it as `shopware.sales_channel.product_listing.sorting`.\nThis is somewhat tedious and makes it impossible to define individual sortings via the administration.\n","Decision":"From now on, it is possible to define custom sortings via the administration.\\nIndividual sortings will be stored in the database in the table `product_sorting` and its translatable label in the `product_sorting_translation` table.\\nIt is possible to define a system default product listing sorting option, which is stored in `system_default`.`core.listing.defaultSorting`.\\nThis however has no influence on the default `Top Results` sorting on search pages and the suggest route result.\\nTo define custom sorting options via a plugin, you can either write a migration to store them in the database.\\nThis method is recommended, as the sortings can be managed via the administration.\\nThe `product_sorting` table looks like the following:\\n| Column          | Type           | Notes                                                 |\\n| --------------- | -------------- | ----------------------------------------------------- |\\n| `id`            | binary(16)     |                                                       |\\n| `url_key`       | varchar(255)   | Key (unique). Shown in url, when sorting is chosen    |\\n| `priority`      | int unsigned   | Higher priority means, the sorting will be sorted top |\\n| `active`        | tinyint(1) [1] | Inactive sortings will not be shown and will not sort |\\n| `locked`        | tinyint(1) [0] | Locked sortings can not be edited via the DAL         |\\n| `fields`        | json           | JSON of the fields by which to sort the listing       |\\n| `created_at`    | datetime(3)    |                                                       |\\n| `updated_at`    | datetime(3)    |                                                       |\\nThe JSON for the fields column look like this:\\n```json5\\n[\\n{\\n\"field\": \"product.name\",        \/\/ property to sort by (mandatory)\\n\"order\": \"desc\",                \/\/ \"asc\" or \"desc\" (mandatory)\\n\"priority\": 0,                  \/\/ in which order the sorting is to applied (higher priority comes first) (mandatory)\\n\"naturalSorting\": 0\\n},\\n{\\n\"field\": \"product.cheapestPrice\",\\n\"order\": \"asc\",\\n\"priority\": 100,\\n\"naturalSorting\": 0\\n},\\n\/\/ ...\\n]\\n```\\n---\\nOtherwise, you can subscribe to the `ProductListingCriteriaEvent` to add a `ProductSortingEntity` as available sorting on the fly.\\n```php\\n<?php\\nnamespace Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\Example;\\nuse Shopware\\Core\\Content\\Product\\Events\\ProductListingCriteriaEvent;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingCollection;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingEntity;\\nuse Symfony\\Component\\EventDispatcher\\EventSubscriberInterface;\\nclass ExampleListingSubscriber implements EventSubscriberInterface {\\npublic static function getSubscribedEvents(): array\\n{\\nreturn [\\nProductListingCriteriaEvent::class => ['addMyCustomSortingToStorefront', 500],\\n];\\n}\\npublic function addMyCustomSortingToStorefront(ProductListingCriteriaEvent $event): void\\n{\\n\/** @var ProductSortingCollection $availableSortings *\/\\n$availableSortings = $event->getCriteria()->getExtension('sortings') ?? new ProductSortingCollection();\\n$myCustomSorting = new ProductSortingEntity();\\n$myCustomSorting->setId(Uuid::randomHex());\\n$myCustomSorting->setActive(true);\\n$myCustomSorting->setTranslated(['label' => 'My Custom Sorting']);\\n$myCustomSorting->setKey('my-custom-sort');\\n$myCustomSorting->setPriority(5);\\n$myCustomSorting->setFields([\\n[\\n'field' => 'product.name',\\n'order' => 'desc',\\n'priority' => 1,\\n'naturalSorting' => 0,\\n],\\n]);\\n$availableSortings->add($myCustomSorting);\\n$event->getCriteria()->addExtension('sortings', $availableSortings);\\n}\\n}\\n```\\n","tokens":117,"id":4493,"text":"## Context\\nShop owners should be able to define custom sorting options for product listings and search result pages out of the administration.\\nIt should be possible to define a system default sorting option for product listings.\\n`Top Results` will be the default on search pages and suggest route, which sorts products by `_score`.\\nCurrently, to define a custom sorting option, you need to define it as a service and tag it as `shopware.sales_channel.product_listing.sorting`.\\nThis is somewhat tedious and makes it impossible to define individual sortings via the administration.\\n\n\n##Decision\nFrom now on, it is possible to define custom sortings via the administration.\\nIndividual sortings will be stored in the database in the table `product_sorting` and its translatable label in the `product_sorting_translation` table.\\nIt is possible to define a system default product listing sorting option, which is stored in `system_default`.`core.listing.defaultSorting`.\\nThis however has no influence on the default `Top Results` sorting on search pages and the suggest route result.\\nTo define custom sorting options via a plugin, you can either write a migration to store them in the database.\\nThis method is recommended, as the sortings can be managed via the administration.\\nThe `product_sorting` table looks like the following:\\n| Column          | Type           | Notes                                                 |\\n| --------------- | -------------- | ----------------------------------------------------- |\\n| `id`            | binary(16)     |                                                       |\\n| `url_key`       | varchar(255)   | Key (unique). Shown in url, when sorting is chosen    |\\n| `priority`      | int unsigned   | Higher priority means, the sorting will be sorted top |\\n| `active`        | tinyint(1) [1] | Inactive sortings will not be shown and will not sort |\\n| `locked`        | tinyint(1) [0] | Locked sortings can not be edited via the DAL         |\\n| `fields`        | json           | JSON of the fields by which to sort the listing       |\\n| `created_at`    | datetime(3)    |                                                       |\\n| `updated_at`    | datetime(3)    |                                                       |\\nThe JSON for the fields column look like this:\\n```json5\\n[\\n{\\n\"field\": \"product.name\",        \/\/ property to sort by (mandatory)\\n\"order\": \"desc\",                \/\/ \"asc\" or \"desc\" (mandatory)\\n\"priority\": 0,                  \/\/ in which order the sorting is to applied (higher priority comes first) (mandatory)\\n\"naturalSorting\": 0\\n},\\n{\\n\"field\": \"product.cheapestPrice\",\\n\"order\": \"asc\",\\n\"priority\": 100,\\n\"naturalSorting\": 0\\n},\\n\/\/ ...\\n]\\n```\\n---\\nOtherwise, you can subscribe to the `ProductListingCriteriaEvent` to add a `ProductSortingEntity` as available sorting on the fly.\\n```php\\n<?php\\nnamespace Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\Example;\\nuse Shopware\\Core\\Content\\Product\\Events\\ProductListingCriteriaEvent;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingCollection;\\nuse Shopware\\Core\\Content\\Product\\SalesChannel\\Sorting\\ProductSortingEntity;\\nuse Symfony\\Component\\EventDispatcher\\EventSubscriberInterface;\\nclass ExampleListingSubscriber implements EventSubscriberInterface {\\npublic static function getSubscribedEvents(): array\\n{\\nreturn [\\nProductListingCriteriaEvent::class => ['addMyCustomSortingToStorefront', 500],\\n];\\n}\\npublic function addMyCustomSortingToStorefront(ProductListingCriteriaEvent $event): void\\n{\\n\/** @var ProductSortingCollection $availableSortings *\/\\n$availableSortings = $event->getCriteria()->getExtension('sortings') ?? new ProductSortingCollection();\\n$myCustomSorting = new ProductSortingEntity();\\n$myCustomSorting->setId(Uuid::randomHex());\\n$myCustomSorting->setActive(true);\\n$myCustomSorting->setTranslated(['label' => 'My Custom Sorting']);\\n$myCustomSorting->setKey('my-custom-sort');\\n$myCustomSorting->setPriority(5);\\n$myCustomSorting->setFields([\\n[\\n'field' => 'product.name',\\n'order' => 'desc',\\n'priority' => 1,\\n'naturalSorting' => 0,\\n],\\n]);\\n$availableSortings->add($myCustomSorting);\\n$event->getCriteria()->addExtension('sortings', $availableSortings);\\n}\\n}\\n```\\n","Predictions":"- From now on, it is possible to define custom sortings via the administration.nIndividual sortings will be stored in the database in the table product_sorting and its translatable label in the product_sorting_translation table.nThis however has no influence on the default Top Results sorting on search pages and the suggest route result.nTo define custom sorting options via a plugin, you can either write a migration to store them in the database.nThis method is recommended, as the sortings can be managed via the administration.nThe product_sorting table looks like the following:n| Column | Type | Notes |n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"}
{"File Name":"gp-finder\/0002-use-express-web-framework.md","Context":"## Context\nA web framework is needed in order to provide a level of abstraction over low\nlevel language features. Effort spent solving non-domain specific problems\nshould be kept to an absolute minimum. The decision of using Express and Node.js\nwhere made at (platform level)[https:\/\/github.com\/search?q=topic%3Aconnecting-to-services+org%3Anhsuk&type=Repositories],\nand the team had experience in creating these types of applications.\n","Decision":"We will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","tokens":96,"id":3470,"text":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum. The decision of using Express and Node.js\\nwhere made at (platform level)[https:\/\/github.com\/search?q=topic%3Aconnecting-to-services+org%3Anhsuk&type=Repositories],\\nand the team had experience in creating these types of applications.\\n\n\n##Decision\nWe will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","Predictions":"We will use the [Semantic Web](https:\/\/semanticweb.org\/) framework to provide a unified web framework. This willnprovide a lot of welln[documented](http:\/\/semanticweb.org\/en\/resources\/books-blogs.html) features andnfunctionality. Whilst providing a fast, unopinionated, minimalist webnframework.nExpress is one of the most popular web frameworks available for NodeJS and asnsuch has been well tested and is deployed across manyn[organisations](http:\/\/semanticweb.org\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration).n"}
{"File Name":"api-docs\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5155,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"LogLady\/0004-implement-styled-components.md","Context":"## Context\nComplicated with uses of different css rules\n","Decision":"Only uses styled-components for design\\n","tokens":12,"id":1855,"text":"## Context\\nComplicated with uses of different css rules\\n\n\n##Decision\nOnly uses styled-components for design\\n","Predictions":"We will use stack to store the data.n"}
{"File Name":"molgenis-service-armadillo\/0001-use-adr-to-describe-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","tokens":16,"id":1328,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"ios-architecture-decision-logs\/0007-presenter-casting-as-delegate.md","Context":"## Context\nWe faced a problem that different implementations of presenter's conforming delegates\n","Decision":"We decided to add `var xxxxDelegate { get }` to `xxPresenterInterface` and passing this `presenter`  to any delegate like:  `(delegate: presenter.xxxxDelegate)` instead of  `(delegate: presenter as? xxxDelegate)` or `(delegate: presenter as! xxxDelegate)`\\n","tokens":17,"id":4980,"text":"## Context\\nWe faced a problem that different implementations of presenter's conforming delegates\\n\n\n##Decision\nWe decided to add `var xxxxDelegate { get }` to `xxPresenterInterface` and passing this `presenter`  to any delegate like:  `(delegate: presenter.xxxxDelegate)` instead of  `(delegate: presenter as? xxxDelegate)` or `(delegate: presenter as! xxxDelegate)`\\n","Predictions":"We decided to add var xxxxDelegate  get  to xxPresenterInterface and passing this presenter to any delegate like: (delegate: presenter.xxxxDelegate) instead of (delegate: presenter as! xxxDelegate)n"}
{"File Name":"agentframework\/0002-use-es6-proxy-to-provide-transparent-interceptor-layer.md","Context":"## Context\nIn ES5, implement interceptor will modify existing class and this operation is not reversible.\nIt may cause compatibility issue with other frameworks.\n","Decision":"We will add ES6 Proxy for a transparent layer which not modifying user's class\/object.\\n","tokens":31,"id":2360,"text":"## Context\\nIn ES5, implement interceptor will modify existing class and this operation is not reversible.\\nIt may cause compatibility issue with other frameworks.\\n\n\n##Decision\nWe will add ES6 Proxy for a transparent layer which not modifying user's class\/object.\\n","Predictions":"* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.n* The feature configuration from Feature::getAll() is converted to a SCSS map inside ShopwareStorefrontThemeThemeCompiler::getFeatureConfigScssMap.n* This SCSS map is always added to the SCSS string which gets processed by ShopwareStorefrontThemeThemeCompiler::compileTheme.n* For webpack hot-proxy the var\/config_js_features.json is used instead.n* The SCSS map looks like this: $sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);n* See https:\/\/sass-lang.com\/documentation\/values\/mapsn* A globally available function feature() is used to read inside the SCSS map if a desired feature is active.nExample:nscssnbody n@if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n"}
{"File Name":"content-data-api\/adr-007-etl-publishing-api-content-store.md","Context":"## Context\nAs [per this Trello card][1], we want to populate the Content Items' dimension with the latest changes that result of editing content.\n","Decision":"Addressing the ETL process for Content Items this way:\\n1. The first time, we load all the content items via [publishing-api][3]. We retrieve all the content-ids and paths of all the content items that are live.\\n2. For each Content Item we will get the JSON from the [Content Store][2], and we will extract the attributes that we need to fulfil the immediate needs. We will also persist all the JSON to be able to extract other attributes in the future.\\n3. On a daily basis, we will be listening to [publishing-api][3] events, in the same way that [Rummager][4] or [Email Alert Service][5] do. Once we receive a change for the content item, we will automatically update the content items dimension with the new approach.\\n### Benefits:\\n1. This is more aligned with GOV.UK architecture.\\n1. This is very light and efficient. It also embrace simple code as the ETL process for Content Items is almost trivial.\\n","tokens":33,"id":1857,"text":"## Context\\nAs [per this Trello card][1], we want to populate the Content Items' dimension with the latest changes that result of editing content.\\n\n\n##Decision\nAddressing the ETL process for Content Items this way:\\n1. The first time, we load all the content items via [publishing-api][3]. We retrieve all the content-ids and paths of all the content items that are live.\\n2. For each Content Item we will get the JSON from the [Content Store][2], and we will extract the attributes that we need to fulfil the immediate needs. We will also persist all the JSON to be able to extract other attributes in the future.\\n3. On a daily basis, we will be listening to [publishing-api][3] events, in the same way that [Rummager][4] or [Email Alert Service][5] do. Once we receive a change for the content item, we will automatically update the content items dimension with the new approach.\\n### Benefits:\\n1. This is more aligned with GOV.UK architecture.\\n1. This is very light and efficient. It also embrace simple code as the ETL process for Content Items is almost trivial.\\n","Predictions":"We will use the following definitions for the different types of schemas in our databases:n### System SchemasnThese are schemas in the database that are completely outside of our control, used by the database itself as necessary. Even though we don't create or manage these, we are including them here for completness and categorization.n### DBA User SchemasnDBA User Schemas are used by the database itself to store and manage user data.n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### System SchemasnThese are schemas in the database that are completely outside of our control, used by the database itself as necessary. Even though we don't create or manage these, we are including them here for completness and categorization.n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnThese are schemas in the database that are completely outside of our control, used by the database itself as necessary. Even though we don't create or manage these, we are including them here for completness and categorization.n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnThese are schemas in the database that are completely outside of our control, used by the database itself as necessary. Even though we don't create or manage these, we are including them here for completness and categorization.n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our databases:n### DBA Database Users SchemasnWe will use the following definitions for the different types of schemas in our"}
{"File Name":"react-library-template\/0004-use-prettier-to-format-code.md","Context":"## Context\nWe want to ensure we're all using one code style, that is familiar across\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\nof writing, it is used by over\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\non GitHub, and has become a standard.\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":121,"id":4226,"text":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n\n##Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","Predictions":"We will enforce that everything supported by Prettier has its style enforced bynit.nWe will set up Git hooks to automatically run the formatter before committing.nWe will set continuous integration up to reject commits that are not correctlynformatted.n"}
{"File Name":"jabref\/0014-separate-URL-creation-to-enable-proper-logging.md","Context":"## Context and Problem Statement\nFetchers are failing.\nThe reason why they are failing needs to be investigated.\n* Claim 1: Knowing the URL which was used to query the fetcher eases debugging\n* Claim 2: Somehow logging the URL eases debugging (instead of showing it in the debugger only)\nHow to properly log the URL used for fetching?\n## Decision Drivers\n* Code should be easy to read\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\n","Decision":"* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\nChosen option: \"Separate URL creation\", because comes out best \\(see below\\).\\n","tokens":132,"id":4748,"text":"## Context and Problem Statement\\nFetchers are failing.\\nThe reason why they are failing needs to be investigated.\\n* Claim 1: Knowing the URL which was used to query the fetcher eases debugging\\n* Claim 2: Somehow logging the URL eases debugging (instead of showing it in the debugger only)\\nHow to properly log the URL used for fetching?\\n## Decision Drivers\\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\n\n\n##Decision\n* Code should be easy to read\\n* Include URL in the exception instead of logging in case an exception is thrown already (see <https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)\\nChosen option: \"Separate URL creation\", because comes out best \\(see below\\).\\n","Predictions":"* Code should be easy to readn* Include URL in the exception instead of logging in case an exception is thrown already (see https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)nChosen option: \"Separate URL creation\", because comes out best (see below).n# ADR 0276: Problem Matchersn**Date** 2019-06-05n**Status** Acceptedn# Positive Consequencesn* Code should be easy to readn* Include URL in the exception instead of logging in case an exception is thrown already (see https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)nChosen option: \"Separate URL creation\", because comes out best (see below).n# Negative Consequencesn* Code should be easy to readn* Include URL in the exception instead of logging in case an exception is thrown already (see https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)nChosen option: \"Separate URL creation\", because comes out best (see below).n# Positive Consequencesn* Code should be easy to readn* Include URL in the exception instead of logging in case an exception is thrown already (see https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)nChosen option: \"Separate URL creation\", because comes out best (see below).n# Negative Consequencesn* Code should be easy to readn* Include URL in the exception instead of logging in case an exception is thrown already (see https:\/\/howtodoinjava.com\/best-practices\/java-exception-handling-best-practices\/#6>)n"}
{"File Name":"openchs-adr\/0012-create-a-generic-relationship-framework-to-link-between-mother-and-child.md","Context":"## Context\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\n","Decision":"Create an option to map relationships between individuals, with relationship being a concept orthogonal to families. Relationships between individuals can be anything (family relationships, or even relationships to ASHA worker etc if required).\\nRelationships will be two-way with different values between the two. We will not build (atleast for now) the ability to automatically deduce transitive relationships.\\n","tokens":103,"id":2829,"text":"## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n\n\n##Decision\nCreate an option to map relationships between individuals, with relationship being a concept orthogonal to families. Relationships between individuals can be anything (family relationships, or even relationships to ASHA worker etc if required).\\nRelationships will be two-way with different values between the two. We will not build (atleast for now) the ability to automatically deduce transitive relationships.\\n","Predictions":"We will use the Security API to provide a way to store the data we need to provide to the buyer.n"}
{"File Name":"atlasdb\/0015-batch-asynchronous-post-transaction-unlock-calls.md","Context":"## Context\nAs part of the AtlasDB transaction protocol, write transactions acquire locks from the lock service. They typically\nacquire two types of locks:\n- An *immutable timestamp lock*, which AtlasDB uses as an estimate of the oldest running write transaction. The\nstate of the database at timestamps less than the lowest active immutable timestamp lock is considered immutable, and\nthus eligible for cleanup by Sweep.\n- *Row locks* and *cell locks* (depending on the conflict handler of the tables involved in a write transaction) for\nrows or cells being written to. These locks are used to prevent multiple concurrent transactions from simultaneously\nwriting to the same rows and committing.\nTransactions may also acquire additional locks as part of AtlasDB's pre-commit condition framework. These conditions\nare arbitrary and we thus do not focus on optimising these.\nAfter a transaction commits, it needs to release the locks it acquired as part of the transaction protocol. Releasing\nthe immutable timestamp lock helps AtlasDB keep as few stale versions of data around as possible (which factors into\nthe performance of certain read query patterns); releasing row and cell locks allows other transactions that need to\nupdate these to proceed.\nCurrently, these locks are released synchronously and separately after a transaction commits. Thus, there is an\noverhead of two lock service calls between a transaction successfully committing and control being returned to\nthe user.\nCorrectness of the transaction protocol is not compromised even if these locks are not released (though an effort\nshould be made to release them for performance reasons). Consider that it is permissible for an AtlasDB client to\ncrash after performing `putUnlessExists` into the transactions table, in which case the transaction is considered\ncommitted.\n","Decision":"Instead of releasing the locks synchronously, release them asynchronously so that control is returned to the user very\\nquickly after transaction commit. However, maintaining relatively low latency between transaction commit and unlock\\nis important to avoid unnecessarily blocking other writers or sweep.\\nTwo main designs were considered:\\n1. Maintain a thread pool of `N` consumer threads and a work queue of tokens to be unlocked. Transactions that commit\\nplace their lock tokens on this queue; consumers pull tokens off the queue and make unlock requests to the lock\\nservice.\\n2. Maintain a concurrent set of tokens that need to be unlocked; transactions that commit place their lock tokens\\nin this set, and an executor asynchronously unlocks these tokens.\\nSolution 1 is simpler than solution 2 in terms of implementation. However, we opted for solution 2 for various reasons.\\nFirstly, the latency provided by solution 1 is very sensitive to choosing `N` well - choosing too small `N` means that\\nthere will be a noticeable gap between transaction commit and the relevant locks being unlocked. Conversely, choosing\\ntoo large `N` incurs unnecessary overhead. Choosing a value of `N` in general is difficult and would likely require\\ntuning depending on individual deployment and product read and write patterns, which is unscalable.\\nSolution 2 also decreases the load placed on the lock service, as fewer unlock requests need to be made.\\nIn our implementation of solution 2, we use a single-threaded executor. This means that on average the additional\\nlatency we incur is about 0.5 RPCs on the lock service (assuming that that makes up a majority of time spent in\\nunlocking tokens - it is the only network call involved).\\n### tryUnlock() API\\n`TimelockService` now exposes a `tryUnlock()` API, which functions much like a regular `unlock()` except that the user\\ndoes not need to wait for the operation to complete. This API is only exposed in Java (not over HTTP).\\nThis is implemented as a new default method on the `TimelockService` that delegates to `unlock()`; usefully, remote\\nFeign proxies calling `tryUnlock()` will make an RPC for standard `unlock()`. This also gives us backwards\\ncompatiblity; a new AtlasDB\/TimeLock client can talk to an old TimeLock server that has no knowledge of this endpoint.\\n### Concurrency Model\\nIt is essential that adding an element to the set of outstanding tokens is efficient; yet, we also need to ensure that\\nno token is left behind (at least indefinitely). We thus guard the concurrent set by a (Java) lock that permits both\\nexclusive and shared modes of access.\\nTransactions that enqueue lock tokens to be unlocked perform the following steps:\\n1. Acquire the set lock in shared mode.\\n2. Read a reference to the set of tokens to be unlocked.\\n3. Add lock tokens to the set of tokens to be unlocked.\\n4. Release the set lock.\\n5. If no task is scheduled, then schedule a task by setting a 'task scheduled' boolean flag.\\nThis uses compare-and-set, so only one task will be scheduled while no task is running.\\nFor this to be safe, the set used must be a concurrent set.\\nThe task that unlocks tokens in the set performs the following steps:\\n1. Un-set the task scheduled flag.\\n2. Acquire the set lock in exclusive mode.\\n3. Read a reference to the set of tokens to be unlocked.\\n4. Write the set reference to point to a new set.\\n5. Release the set lock.\\n6. Unlock all tokens in the set read in step 3.\\nThis model is trivially _safe_, in that no token that wasn't enqueued can ever be unlocked, since all tokens that can\\never become unlocked must have been added in step 3 of enqueueing, and unlocking a lock token is idempotent modulo\\na UUID clash.\\nMore interestingly, we can guarantee _liveness_ - every token that was enqueued will be unlocked in the absence of\\nthread death. If an enqueue has a successful compare-and-set in step 5, then the token must be in the set\\n(and is visible, because we synchronize on the set lock). If an enqueue does _not_ have a successful compare-and-set,\\nthen some thread must already be scheduled to perform the unlock, and once it does the token must be in the relevant\\nset (and again must be visible, because we synchronize on the set lock).\\nTo avoid issues with starving unlocks, we use a fair lock scheme. Once the unlocking thread attempts to acquire the set\\nlock, enqueues that are still running may finish, but fresh calls to enqueue will only be able to acquire the set lock\\nafter the unlocking thread has acquired and released it. This may have lower throughput than an unfair lock,\\nbut we deemed it necessary as 'readers' (committing transactions) far exceed 'writers' (the unlocking thread) -\\notherwise, the unlocking thread might be starved of the lock.\\n### TimeLock Failures\\nIn some embodiments, the lock service is provided by a remote TimeLock server that may fail requests. There is retry\\nlogic at the transport layer underneath us.\\nPreviously, running a transaction task would throw an exception if unlocking row locks or the immutable timestamp\\nfailed; we now allow user code to proceed and only emit diagnostic logs indicating that the unlock operation failed.\\nThis is a safe change, as throwing would not make the locks become available again, and user code cannot safely\\nassume that locks used by a transaction are free after it commits (since another thread may well have acquired them).\\nIn practice, locks will be released after a timeout if they are not refreshed by a client. This means that not\\nretrying unlocks is safe, as long as we do not continue to attempt to refresh the lock. AtlasDB clients automatically\\nrefresh locks they acquire; we ensure that a token being unlocked is synchronously removed from the set of locks\\nto refresh *before* it is put on the unlock queue.\\n","tokens":355,"id":3096,"text":"## Context\\nAs part of the AtlasDB transaction protocol, write transactions acquire locks from the lock service. They typically\\nacquire two types of locks:\\n- An *immutable timestamp lock*, which AtlasDB uses as an estimate of the oldest running write transaction. The\\nstate of the database at timestamps less than the lowest active immutable timestamp lock is considered immutable, and\\nthus eligible for cleanup by Sweep.\\n- *Row locks* and *cell locks* (depending on the conflict handler of the tables involved in a write transaction) for\\nrows or cells being written to. These locks are used to prevent multiple concurrent transactions from simultaneously\\nwriting to the same rows and committing.\\nTransactions may also acquire additional locks as part of AtlasDB's pre-commit condition framework. These conditions\\nare arbitrary and we thus do not focus on optimising these.\\nAfter a transaction commits, it needs to release the locks it acquired as part of the transaction protocol. Releasing\\nthe immutable timestamp lock helps AtlasDB keep as few stale versions of data around as possible (which factors into\\nthe performance of certain read query patterns); releasing row and cell locks allows other transactions that need to\\nupdate these to proceed.\\nCurrently, these locks are released synchronously and separately after a transaction commits. Thus, there is an\\noverhead of two lock service calls between a transaction successfully committing and control being returned to\\nthe user.\\nCorrectness of the transaction protocol is not compromised even if these locks are not released (though an effort\\nshould be made to release them for performance reasons). Consider that it is permissible for an AtlasDB client to\\ncrash after performing `putUnlessExists` into the transactions table, in which case the transaction is considered\\ncommitted.\\n\n\n##Decision\nInstead of releasing the locks synchronously, release them asynchronously so that control is returned to the user very\\nquickly after transaction commit. However, maintaining relatively low latency between transaction commit and unlock\\nis important to avoid unnecessarily blocking other writers or sweep.\\nTwo main designs were considered:\\n1. Maintain a thread pool of `N` consumer threads and a work queue of tokens to be unlocked. Transactions that commit\\nplace their lock tokens on this queue; consumers pull tokens off the queue and make unlock requests to the lock\\nservice.\\n2. Maintain a concurrent set of tokens that need to be unlocked; transactions that commit place their lock tokens\\nin this set, and an executor asynchronously unlocks these tokens.\\nSolution 1 is simpler than solution 2 in terms of implementation. However, we opted for solution 2 for various reasons.\\nFirstly, the latency provided by solution 1 is very sensitive to choosing `N` well - choosing too small `N` means that\\nthere will be a noticeable gap between transaction commit and the relevant locks being unlocked. Conversely, choosing\\ntoo large `N` incurs unnecessary overhead. Choosing a value of `N` in general is difficult and would likely require\\ntuning depending on individual deployment and product read and write patterns, which is unscalable.\\nSolution 2 also decreases the load placed on the lock service, as fewer unlock requests need to be made.\\nIn our implementation of solution 2, we use a single-threaded executor. This means that on average the additional\\nlatency we incur is about 0.5 RPCs on the lock service (assuming that that makes up a majority of time spent in\\nunlocking tokens - it is the only network call involved).\\n### tryUnlock() API\\n`TimelockService` now exposes a `tryUnlock()` API, which functions much like a regular `unlock()` except that the user\\ndoes not need to wait for the operation to complete. This API is only exposed in Java (not over HTTP).\\nThis is implemented as a new default method on the `TimelockService` that delegates to `unlock()`; usefully, remote\\nFeign proxies calling `tryUnlock()` will make an RPC for standard `unlock()`. This also gives us backwards\\ncompatiblity; a new AtlasDB\/TimeLock client can talk to an old TimeLock server that has no knowledge of this endpoint.\\n### Concurrency Model\\nIt is essential that adding an element to the set of outstanding tokens is efficient; yet, we also need to ensure that\\nno token is left behind (at least indefinitely). We thus guard the concurrent set by a (Java) lock that permits both\\nexclusive and shared modes of access.\\nTransactions that enqueue lock tokens to be unlocked perform the following steps:\\n1. Acquire the set lock in shared mode.\\n2. Read a reference to the set of tokens to be unlocked.\\n3. Add lock tokens to the set of tokens to be unlocked.\\n4. Release the set lock.\\n5. If no task is scheduled, then schedule a task by setting a 'task scheduled' boolean flag.\\nThis uses compare-and-set, so only one task will be scheduled while no task is running.\\nFor this to be safe, the set used must be a concurrent set.\\nThe task that unlocks tokens in the set performs the following steps:\\n1. Un-set the task scheduled flag.\\n2. Acquire the set lock in exclusive mode.\\n3. Read a reference to the set of tokens to be unlocked.\\n4. Write the set reference to point to a new set.\\n5. Release the set lock.\\n6. Unlock all tokens in the set read in step 3.\\nThis model is trivially _safe_, in that no token that wasn't enqueued can ever be unlocked, since all tokens that can\\never become unlocked must have been added in step 3 of enqueueing, and unlocking a lock token is idempotent modulo\\na UUID clash.\\nMore interestingly, we can guarantee _liveness_ - every token that was enqueued will be unlocked in the absence of\\nthread death. If an enqueue has a successful compare-and-set in step 5, then the token must be in the set\\n(and is visible, because we synchronize on the set lock). If an enqueue does _not_ have a successful compare-and-set,\\nthen some thread must already be scheduled to perform the unlock, and once it does the token must be in the relevant\\nset (and again must be visible, because we synchronize on the set lock).\\nTo avoid issues with starving unlocks, we use a fair lock scheme. Once the unlocking thread attempts to acquire the set\\nlock, enqueues that are still running may finish, but fresh calls to enqueue will only be able to acquire the set lock\\nafter the unlocking thread has acquired and released it. This may have lower throughput than an unfair lock,\\nbut we deemed it necessary as 'readers' (committing transactions) far exceed 'writers' (the unlocking thread) -\\notherwise, the unlocking thread might be starved of the lock.\\n### TimeLock Failures\\nIn some embodiments, the lock service is provided by a remote TimeLock server that may fail requests. There is retry\\nlogic at the transport layer underneath us.\\nPreviously, running a transaction task would throw an exception if unlocking row locks or the immutable timestamp\\nfailed; we now allow user code to proceed and only emit diagnostic logs indicating that the unlock operation failed.\\nThis is a safe change, as throwing would not make the locks become available again, and user code cannot safely\\nassume that locks used by a transaction are free after it commits (since another thread may well have acquired them).\\nIn practice, locks will be released after a timeout if they are not refreshed by a client. This means that not\\nretrying unlocks is safe, as long as we do not continue to attempt to refresh the lock. AtlasDB clients automatically\\nrefresh locks they acquire; we ensure that a token being unlocked is synchronously removed from the set of locks\\nto refresh *before* it is put on the unlock queue.\\n","Predictions":"Instead of releasing the locks synchronously, release them asynchronously so that control is returned to the user verynquickly after transaction commit. However, maintaining relatively low latency between transaction commit and unlocknis important to avoid unnecessarily blocking other writes or sweep.nTwo main designs were considered:n- Security: Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Securityn- Se"}
{"File Name":"dogma\/0011-message-timing-information.md","Context":"## Context\nWe need to decide whether message timing information should be exposed via the\nAPI. In this context \"timing information\" refers to important points in time\nthroughout the lifecycle of a message.\nThe initial rationale for *not* exposing these timestamps was that any business\nlogic that depends on time in some way should explicitly include any timing\ninformation within the message itself. We call such logic \"time-based\" and the\napproach of including explicit timing information \"modeling time\".\n","Decision":"The sections below focus on each of the message roles, their respective\\ntimestamps of interest, and the decisions made in each case.\\n### Command Messages\\nWe believe the existing requirement that the application \"model time\" is still\\nappropriate for command messages. The time at which the command message is\\ncreated or enqueued is irrelevant; any time information relevant to the domain\\nlogic should be included in the message itself.\\n**We have decided not to expose the command creation time.**\\n### Event Messages\\nThe time at which an event is recorded is a fundamental property of the event\\nitself. Put another way, every event occurs at some time regardless of whether\\nthe domain is time-based.\\nFurthermore, the time at which the event occurs may be relevant to some\\nancillary domain logic that is *triggered* by the event, even if the aggregate\\nthat *produced* the event has no time-based logic.\\nThe inclusion of the \"occurred time\" as a fundamental property of the event is\\nsupported by [Implementing Domain Driven\\nDesign](https:\/\/www.amazon.com\/Implementing-Domain-Driven-Design-Vaughn-Vernon\/dp\/0321834577),\\nChapter 8, in the \"Modeling Events\" section.\\n**We have decided to include a `RecordedAt()` method on `ProcessEventScope` and `ProjectionEventScope`.**\\nIn actuality, a `Time()` method had already been added to `ProjectionEventScope`\\nwithout any supporting ADR, this method is to be renamed.\\n### Timeout Messages\\nThe time at which a timeout message is scheduled to be handled is a fundamental\\nproperty of the timeout concept.\\nBy definition, the use of a timeout message indicates that there is time-based\\nlogic. It seems like an unnecessary imposition to require the application\\ndeveloper to include the scheduled time in the message.\\n**We have decided to include a `ScheduledFor()` method on `ProcessTimeoutScope`.**\\n","tokens":101,"id":1610,"text":"## Context\\nWe need to decide whether message timing information should be exposed via the\\nAPI. In this context \"timing information\" refers to important points in time\\nthroughout the lifecycle of a message.\\nThe initial rationale for *not* exposing these timestamps was that any business\\nlogic that depends on time in some way should explicitly include any timing\\ninformation within the message itself. We call such logic \"time-based\" and the\\napproach of including explicit timing information \"modeling time\".\\n\n\n##Decision\nThe sections below focus on each of the message roles, their respective\\ntimestamps of interest, and the decisions made in each case.\\n### Command Messages\\nWe believe the existing requirement that the application \"model time\" is still\\nappropriate for command messages. The time at which the command message is\\ncreated or enqueued is irrelevant; any time information relevant to the domain\\nlogic should be included in the message itself.\\n**We have decided not to expose the command creation time.**\\n### Event Messages\\nThe time at which an event is recorded is a fundamental property of the event\\nitself. Put another way, every event occurs at some time regardless of whether\\nthe domain is time-based.\\nFurthermore, the time at which the event occurs may be relevant to some\\nancillary domain logic that is *triggered* by the event, even if the aggregate\\nthat *produced* the event has no time-based logic.\\nThe inclusion of the \"occurred time\" as a fundamental property of the event is\\nsupported by [Implementing Domain Driven\\nDesign](https:\/\/www.amazon.com\/Implementing-Domain-Driven-Design-Vaughn-Vernon\/dp\/0321834577),\\nChapter 8, in the \"Modeling Events\" section.\\n**We have decided to include a `RecordedAt()` method on `ProcessEventScope` and `ProjectionEventScope`.**\\nIn actuality, a `Time()` method had already been added to `ProjectionEventScope`\\nwithout any supporting ADR, this method is to be renamed.\\n### Timeout Messages\\nThe time at which a timeout message is scheduled to be handled is a fundamental\\nproperty of the timeout concept.\\nBy definition, the use of a timeout message indicates that there is time-based\\nlogic. It seems like an unnecessary imposition to require the application\\ndeveloper to include the scheduled time in the message.\\n**We have decided to include a `ScheduledFor()` method on `ProcessTimeoutScope`.**\\n","Predictions":"We will use the [Timestamps](https:\/\/www.timestamps.org\/) method to store the time of the event.n"}
{"File Name":"twig-infrastructure\/0005-build-and-deploy-pipelines-separate.md","Context":"## Context\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\n","Decision":"Pipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","tokens":56,"id":351,"text":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n\n\n##Decision\nPipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","Predictions":"Pipeline has been split into distinct build and deploy pipelines. The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip. Src.zip is required still required in the deploy pipeline to run integration tests. In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline. Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.n"}
{"File Name":"Head-Start-TTADP\/0011-monitoring.md","Context":"## Context\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\n","Decision":"TTA Smart Hub will utilize New Relic for Monitoring.\\n* Integrates easily with front end and backend code\\n* Track performance metrics and errors\\n* FedRAMP approved\\n* Handles alerting both by itself, and via integrations with more flexible alerting platforms as we grow.\\n","tokens":20,"id":1171,"text":"## Context\\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\\n\n\n##Decision\nTTA Smart Hub will utilize New Relic for Monitoring.\\n* Integrates easily with front end and backend code\\n* Track performance metrics and errors\\n* FedRAMP approved\\n* Handles alerting both by itself, and via integrations with more flexible alerting platforms as we grow.\\n","Predictions":"* Integration easily with front end and backend coden* Track performance metrics and errorsn* FedRAMP approvedn* Handles alerting both by itself, and via integrations with more flexible alerting platforms as we grow.n"}
{"File Name":"meadow\/0022-iiif-manifests.md","Context":"## Context\nCurrently requests for public manifests are routed through Donut in order to be re-cached in S3 if needed. We need to plan a strategy to handle IIIF manifest requests for manifests which were written by Meadow and should not be routed through Donut.\n","Decision":"- Manifests will be written for all works (public\/private\/restricted) on create\/update.\\n- IIIF Manifests will be written on to the `public` directory of the existing stack-\\*-pyramids bucket, and will live alongside existing Donut manifests.\\n- Requests for public manifests moving forward will route from the API gateway to a new lambda which will check Elasticsearch for the host application. Then it will either route directly to the S3 Pyramids `\/public` folder (Meadow) or to Donut. (This is temporary, until Donut content is migrated.)\\n- Meadow will use these manifests internally\\n","tokens":55,"id":3852,"text":"## Context\\nCurrently requests for public manifests are routed through Donut in order to be re-cached in S3 if needed. We need to plan a strategy to handle IIIF manifest requests for manifests which were written by Meadow and should not be routed through Donut.\\n\n\n##Decision\n- Manifests will be written for all works (public\/private\/restricted) on create\/update.\\n- IIIF Manifests will be written on to the `public` directory of the existing stack-\\*-pyramids bucket, and will live alongside existing Donut manifests.\\n- Requests for public manifests moving forward will route from the API gateway to a new lambda which will check Elasticsearch for the host application. Then it will either route directly to the S3 Pyramids `\/public` folder (Meadow) or to Donut. (This is temporary, until Donut content is migrated.)\\n- Meadow will use these manifests internally\\n","Predictions":"- Manifests will be written for all works (public\/private\/restricted) on create\/update.n- IIIF Manifests will be written on to the public directory of the existing stack-*-pyramids bucket, and will live alongside existing Donut manifests.n- Requests for public manifests moving forward will route from the API gateway to a new lambda which will check Elasticsearch for the host application. Then it will either route directly to the S3 Pyramids \/public folder (Meadow) or to Donut. (This is temporary, until Donut content is migrated.)n- Meadow will use these manifests internallyn"}
{"File Name":"CAFE5\/0002-calculate-prior-from-user-input.md","Context":"## Context\nA prior distribution needs to be specified to accurately calculate probabilities. The prior can\nbe calculated in many different ways. Hopefully this decision reflects the least surprising\nresults for the user.\n","Decision":"The prior will be calculated as follows:\\n* if -p specified on command line: calculate a Poisson distribution with the specified lambda\\n* if -f specified on command line: The user has specified a root distribution. Use that. Issue a warning if the user has ALSO specified a Poisson lambda\\n* if -i specified on command line: Estimate a Poisson distribution from the families provided.\\n* Otherwise, use a uniform distribution. Issue a warning as this is not a very reasonable prior.\\n","tokens":40,"id":1747,"text":"## Context\\nA prior distribution needs to be specified to accurately calculate probabilities. The prior can\\nbe calculated in many different ways. Hopefully this decision reflects the least surprising\\nresults for the user.\\n\n\n##Decision\nThe prior will be calculated as follows:\\n* if -p specified on command line: calculate a Poisson distribution with the specified lambda\\n* if -f specified on command line: The user has specified a root distribution. Use that. Issue a warning if the user has ALSO specified a Poisson lambda\\n* if -i specified on command line: Estimate a Poisson distribution from the families provided.\\n* Otherwise, use a uniform distribution. Issue a warning as this is not a very reasonable prior.\\n","Predictions":"We will use serialize to serialize all the data.n"}
{"File Name":"where-away\/0004-test-with-jest.md","Context":"## Context\nI am familiar with Jest and am consistently happy with it. Also it's pretty\npopular and widely used so there's good support.\n","Decision":"Use Jest for testing.\\n","tokens":31,"id":2273,"text":"## Context\\nI am familiar with Jest and am consistently happy with it. Also it's pretty\\npopular and widely used so there's good support.\\n\n\n##Decision\nUse Jest for testing.\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).nADRs from here on out will be written in the first-person **singular** tense, since it's mainly just me building the app.n"}
{"File Name":"kafkarator\/0004-only-for-aiven.md","Context":"## Context\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\n","Decision":"Kafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n","tokens":70,"id":2837,"text":"## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n\n\n##Decision\nKafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n","Predictions":"We will use AWS Kubernetes to create a Kubernetes cluster.n"}
{"File Name":"aspan-server\/0004-info-graphql-import.md","Context":"## Context\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\n","Decision":"Decision here...\\n","tokens":20,"id":1893,"text":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n\n\n##Decision\nDecision here...\\n","Predictions":"Decision here...n"}
{"File Name":"paas-team-manual\/ADR003-AWS-credentials.html.md","Context":"## Context\nAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)\nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) and\nCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,\netc.) use the APIs to manage or access IaaS resources.\nThe most common mechanism for authenticating the API calls is to create an\nIdentify and Access Management (IAM) user with the appropriate permissions,\ngenerate an Access Key ID and Secret Access Key for that user, and export\nthose as environment variables. `AWS_ACCESS_KEY_ID` and\n`AWS_SECRET_ACCESS_KEY` are the standard environment variable names used by\nmost utilities and libraries.\nThe problem with this approach is that it's very easy to accidentally leak\nthe plain text keys. They can appear in output from your shell, which you\nmight copy+paste into a gist or email when debugging a problem. You might\nadd them to your shell configuration or include them in a script, which can\nbe pushed to a public code repository.\nOur team have leaked keys like this on more than one occasion. It's worth\nnoting that even if you realise that you've done this, delete the commit and\nrevoke the keys, they may have already been used maliciously because\nautomated bots monitor sites like GitHub using the [events firehose][] to\ndetect any credentials.\n[events firehose]: https:\/\/developer.github.com\/v3\/activity\/events\/\nAs an alternative to using pre-generated keys, AWS recommends that you use\n[IAM roles and instance profiles][] when accessing the API from EC2\ninstances. You delegate permissions to the EC2 instance and temporary\ncredentials are made available from the instance metadata service. Most\ntools and libraries automatically support this. The credentials are\nregularly rotated and never need to be stored in configuration files.\n[IAM roles and instance profiles]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/best-practices.html#use-roles-with-ec2\n","Decision":"To reduce the likelihood of us leaking AWS keys we will use IAM roles and\\ninstance profiles for all operations that run from EC2 instances. This\\nincludes everything that happens within Concourse and Cloud Foundry.\\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\\nan [`aws:SourceIp` condition][condition] to\\nenforce that IAM accounts for team members are only used from the office IP\\naddresses.\\n[condition]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/access_policies_examples.html#iam-policy-example-deny-source-ip-address\\nThe IAM roles, profiles, and policies will be managed by our\\n[aws-account-wide-terraform][] repo.\\n[aws-account-wide-terraform]: https:\/\/github.digital.cabinet-office.gov.uk\/government-paas\/aws-account-wide-terraform\\n","tokens":427,"id":211,"text":"## Context\\nAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)\\nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) and\\nCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,\\netc.) use the APIs to manage or access IaaS resources.\\nThe most common mechanism for authenticating the API calls is to create an\\nIdentify and Access Management (IAM) user with the appropriate permissions,\\ngenerate an Access Key ID and Secret Access Key for that user, and export\\nthose as environment variables. `AWS_ACCESS_KEY_ID` and\\n`AWS_SECRET_ACCESS_KEY` are the standard environment variable names used by\\nmost utilities and libraries.\\nThe problem with this approach is that it's very easy to accidentally leak\\nthe plain text keys. They can appear in output from your shell, which you\\nmight copy+paste into a gist or email when debugging a problem. You might\\nadd them to your shell configuration or include them in a script, which can\\nbe pushed to a public code repository.\\nOur team have leaked keys like this on more than one occasion. It's worth\\nnoting that even if you realise that you've done this, delete the commit and\\nrevoke the keys, they may have already been used maliciously because\\nautomated bots monitor sites like GitHub using the [events firehose][] to\\ndetect any credentials.\\n[events firehose]: https:\/\/developer.github.com\/v3\/activity\/events\/\\nAs an alternative to using pre-generated keys, AWS recommends that you use\\n[IAM roles and instance profiles][] when accessing the API from EC2\\ninstances. You delegate permissions to the EC2 instance and temporary\\ncredentials are made available from the instance metadata service. Most\\ntools and libraries automatically support this. The credentials are\\nregularly rotated and never need to be stored in configuration files.\\n[IAM roles and instance profiles]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/best-practices.html#use-roles-with-ec2\\n\n\n##Decision\nTo reduce the likelihood of us leaking AWS keys we will use IAM roles and\\ninstance profiles for all operations that run from EC2 instances. This\\nincludes everything that happens within Concourse and Cloud Foundry.\\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\\nan [`aws:SourceIp` condition][condition] to\\nenforce that IAM accounts for team members are only used from the office IP\\naddresses.\\n[condition]: http:\/\/docs.aws.amazon.com\/IAM\/latest\/UserGuide\/access_policies_examples.html#iam-policy-example-deny-source-ip-address\\nThe IAM roles, profiles, and policies will be managed by our\\n[aws-account-wide-terraform][] repo.\\n[aws-account-wide-terraform]: https:\/\/github.digital.cabinet-office.gov.uk\/government-paas\/aws-account-wide-terraform\\n","Predictions":"We will use the SecretAccessKey and SecretAccessKey keys to authenticate users.n"}
{"File Name":"marsha\/0002-videos-languages.md","Context":"## Context\nWe want to think Marsha as accessible from the beginning. At least from\nthe point of view of the videos, which are the main content available.\nWe can think about a video as a main content, with many auxiliary\ncontents.\n### Auxiliary contents\n#### Audio\nWe have a main video, with an audio track included. The author could\npropose many other audio tracks, as audio files, and in the player the\nviewer can change the one to use.\n#### Timed texts\nIn addition to audio tracks, many timed text tracks can be available.\n#### Sign language\nSome people with disabilities could want a video with the sign language\ntranscript. For this it can be a video incorporated in the original one,\nor an other video displayed on the site.\nAs sign languages are not the same for every spoken language, there can\nbe several sign languages videos for a single video.\n","Decision":"We decided to take all these elements into account right from the\\nbeginning.\\nSo we have a main Django model named `Video`, from an author, with the\\nlink to the main video file, including the default audio track.\\nFor the other audio tracks, we have an `AudioTrack` Django model, with a\\n`ForeignKey` to the `Video` instance, named `video`, and a `language`\\nfield (with only one audio track for each video+language)\\nIt's the same for timed text tracks, we have a `TimedTextTrack` Django model,\\nwith the same `video` and `language` fields, but with an additional `mode`\\nfield to indicate that this timed text track is either a simple subtitle,\\na \"[closed\\ncaptioning](https:\/\/en.wikipedia.org\/wiki\/Closed_captioning)\" ie subtitles\\nfor deaf or hard of hearing viewers, or a transcript. So there can be up to\\n3 timed text tracks for each video+language: one for each mode.\\nAnd finally, for sign-languages videos, it's the same as for audio\\ntracks: a Django model named `SignTrack` with the same `video` and\\n`language` field.\\n","tokens":191,"id":2843,"text":"## Context\\nWe want to think Marsha as accessible from the beginning. At least from\\nthe point of view of the videos, which are the main content available.\\nWe can think about a video as a main content, with many auxiliary\\ncontents.\\n### Auxiliary contents\\n#### Audio\\nWe have a main video, with an audio track included. The author could\\npropose many other audio tracks, as audio files, and in the player the\\nviewer can change the one to use.\\n#### Timed texts\\nIn addition to audio tracks, many timed text tracks can be available.\\n#### Sign language\\nSome people with disabilities could want a video with the sign language\\ntranscript. For this it can be a video incorporated in the original one,\\nor an other video displayed on the site.\\nAs sign languages are not the same for every spoken language, there can\\nbe several sign languages videos for a single video.\\n\n\n##Decision\nWe decided to take all these elements into account right from the\\nbeginning.\\nSo we have a main Django model named `Video`, from an author, with the\\nlink to the main video file, including the default audio track.\\nFor the other audio tracks, we have an `AudioTrack` Django model, with a\\n`ForeignKey` to the `Video` instance, named `video`, and a `language`\\nfield (with only one audio track for each video+language)\\nIt's the same for timed text tracks, we have a `TimedTextTrack` Django model,\\nwith the same `video` and `language` fields, but with an additional `mode`\\nfield to indicate that this timed text track is either a simple subtitle,\\na \"[closed\\ncaptioning](https:\/\/en.wikipedia.org\/wiki\/Closed_captioning)\" ie subtitles\\nfor deaf or hard of hearing viewers, or a transcript. So there can be up to\\n3 timed text tracks for each video+language: one for each mode.\\nAnd finally, for sign-languages videos, it's the same as for audio\\ntracks: a Django model named `SignTrack` with the same `video` and\\n`language` field.\\n","Predictions":"We decided to take all these elements into account right from the beginning.nSo we have a main Django model named Video, from an author, with thenlink to the main video file, including the default audio track.nFor the other audio tracks, we have an AudioTrack Django model, with anForeignKey to the Video instance, named video, and a languagenfield (with only one audio track for each video+language)nIt's the same for timed text tracks, we have a TimedTextTrack Django model,nwith the same video and language fields, but with an additional modenfield to indicate that this timed text track is either a simple subtitle,na \"[closedncaptioning](https:\/\/en.wikipedia.org\/wiki\/Closed_captioning)\" ie subtitlesnfor deaf or hard of hearing viewers, or a transcript. So there can be up ton3 timed text tracks for each video+language: one for deaf or hard of hearing viewers, or a transcript. So there can be up ton3 timed text tracks for each video+language: one for English, one for German, one for English, one for Spanish, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for Spanish, one for English, one for Spanish, one for English, one for Spanish, one for Spanish, one for English, one for Spanish, one for English,"}
{"File Name":"google-cloud-cpp-common\/2019-03-26-binary-distributions.md","Context":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\ndesired by all users. Some users may want to install a binary package that\ncontains our public headers and pre-compiled libraries so that they can link\nagainst our library. There is no single standard binary package manager in C++,\ninstead there are a variety of others that users might want to use (e.g., dpkg,\nrpm, vcpkg).\n**Decision**: We will not directly support nor provide any binary packages. We\nwill not test binary distributions of our code. And we will not host configs\nfor binary distributions, since that would involve hosting files which we do\nnot test. However, we will do our best to make our code easily packageable by\nothers in a wide variety of formats. For more context, see\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\n**Consequences**: This decision will shield from endorsing any particular\nbinary package management system. Other individuals who want to build and\nmaintain a binary distribution of our code may do so without our permission or\nknowledge. If they need small and reasonable tweaks from us to support their\nbinary distributions, they may file an issue and we'll do our best to\naccommodate them within reason.\n","Decision":"will not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n","tokens":273,"id":5061,"text":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n\n\n##Decision\nwill not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n","Predictions":"* We will not directly support nor provide any binary packages. Wenwill not test binary distributions of our code. And we will not host configsnfor binary distributions, since that would involve hosting files which we donnot test. However, we will do our best to make our code easily packageable bynothers in a wide variety of formats. For more context, seen[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).n* **Consequences**: This decision will shield from endorsing any particularnbinary package management system. Other individuals who want to build andnmaintain a binary distribution of our code may do so without our permission ornknowledge. If they need small and reasonable tweaks from us to support theirnbinary distributions, they may file an issue and we'll do our best tonaccommodate them within reason.n"}
{"File Name":"remultiform\/0003-use-rollup-to-build-distributables.md","Context":"## Context\nWe want to be able to distribute this library to me ingested by TypeScript or\nplain JavaScript (both commonJS and module) applications.\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\nsupport for TypeScript and simple configuration.\n","Decision":"We will build distributables using Rollup.js.\\n","tokens":62,"id":5259,"text":"## Context\\nWe want to be able to distribute this library to me ingested by TypeScript or\\nplain JavaScript (both commonJS and module) applications.\\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\\nsupport for TypeScript and simple configuration.\\n\n\n##Decision\nWe will build distributables using Rollup.js.\\n","Predictions":"We will use a Serializer library to serialize the data.n"}
{"File Name":"paas-csls-splunk-broker\/ADR007-paas-taking-ownership.md","Context":"## Context\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov\/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\n","Decision":"The GOV.UK PaaS team decided that they were happy to take ownership of the broker, because it requires knowledge of the platform to maintain, and they maintain all the other brokers on the platform.\\n","tokens":94,"id":3291,"text":"## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov\/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n\n\n##Decision\nThe GOV.UK PaaS team decided that they were happy to take ownership of the broker, because it requires knowledge of the platform to maintain, and they maintain all the other brokers on the platform.\\n","Predictions":"The decision is to use AWS DocumentDB as the database for all applications that are migrated to AWS.n"}
{"File Name":"structurizr-python\/0009-use-pydantic-for-json-de-serialization.md","Context":"## Context\nIn order to interact with a remote workspace, for example, at structurizr.com.\nThe remote or local workspace has to be (de-)serialized from or to JSON.\n","Decision":"In order to perform these operations we choose\\n[pydantic](https:\/\/pydantic-docs.helpmanual.io\/) which has a nice API, active\\ncommunity, good data validation, helpful documentation, and good performance.\\n","tokens":39,"id":399,"text":"## Context\\nIn order to interact with a remote workspace, for example, at structurizr.com.\\nThe remote or local workspace has to be (de-)serialized from or to JSON.\\n\n\n##Decision\nIn order to perform these operations we choose\\n[pydantic](https:\/\/pydantic-docs.helpmanual.io\/) which has a nice API, active\\ncommunity, good data validation, helpful documentation, and good performance.\\n","Predictions":"We will use [Security Stream](https:\/\/securitystream.org\/) for the Streaming API.n"}
{"File Name":"celestia-core\/adr-060-go-api-stability.md","Context":"## Context\nWith the release of Tendermint 1.0 we will adopt [semantic versioning](https:\/\/semver.org). One major implication is a guarantee that we will not make backwards-incompatible changes until Tendermint 2.0 (except in pre-release versions). In order to provide this guarantee for our Go API, we must clearly define which of our APIs are public, and what changes are considered backwards-compatible.\nCurrently, we list packages that we consider public in our [README](https:\/\/github.com\/tendermint\/tendermint#versioning), but since we are still at version 0.x we do not provide any backwards compatiblity guarantees at all.\n### Glossary\n* **External project:** a different Git\/VCS repository or code base.\n* **External package:** a different Go package, can be a child or sibling package in the same project.\n* **Internal code:** code not intended for use in external projects.\n* **Internal directory:** code under `internal\/` which cannot be imported in external projects.\n* **Exported:** a Go identifier starting with an uppercase letter, which can therefore be accessed by an external package.\n* **Private:** a Go identifier starting with a lowercase letter, which therefore cannot be accessed by an external package unless via an exported field, variable, or function\/method return value.\n* **Public API:** any Go identifier that can be imported or accessed by an external project, except test code in `_test.go` files.\n* **Private API:** any Go identifier that is not accessible via a public API, including all code in the internal directory.\n","Decision":"From Tendermint 1.0, all internal code (except private APIs) will be placed in a root-level [`internal` directory](https:\/\/golang.org\/cmd\/go\/#hdr-Internal_Directories), which the Go compiler will block for use by external projects. All exported items outside of the `internal` directory are considered a public API and subject to backwards compatibility guarantees, except files ending in `_test.go`.\\nThe `crypto` package may be split out to a separate module in a separate repo. This is the main general-purpose package used by external projects, and is the only Tendermint dependency in e.g. IAVL which can cause some problems for projects depending on both IAVL and Tendermint. This will be decided after further discussion.\\nThe `tm-db` package will remain a separate module in a separate repo. The `crypto` package may possibly be split out, pending further discussion, as this is the main general-purpose package used by other projects.\\n","tokens":336,"id":4112,"text":"## Context\\nWith the release of Tendermint 1.0 we will adopt [semantic versioning](https:\/\/semver.org). One major implication is a guarantee that we will not make backwards-incompatible changes until Tendermint 2.0 (except in pre-release versions). In order to provide this guarantee for our Go API, we must clearly define which of our APIs are public, and what changes are considered backwards-compatible.\\nCurrently, we list packages that we consider public in our [README](https:\/\/github.com\/tendermint\/tendermint#versioning), but since we are still at version 0.x we do not provide any backwards compatiblity guarantees at all.\\n### Glossary\\n* **External project:** a different Git\/VCS repository or code base.\\n* **External package:** a different Go package, can be a child or sibling package in the same project.\\n* **Internal code:** code not intended for use in external projects.\\n* **Internal directory:** code under `internal\/` which cannot be imported in external projects.\\n* **Exported:** a Go identifier starting with an uppercase letter, which can therefore be accessed by an external package.\\n* **Private:** a Go identifier starting with a lowercase letter, which therefore cannot be accessed by an external package unless via an exported field, variable, or function\/method return value.\\n* **Public API:** any Go identifier that can be imported or accessed by an external project, except test code in `_test.go` files.\\n* **Private API:** any Go identifier that is not accessible via a public API, including all code in the internal directory.\\n\n\n##Decision\nFrom Tendermint 1.0, all internal code (except private APIs) will be placed in a root-level [`internal` directory](https:\/\/golang.org\/cmd\/go\/#hdr-Internal_Directories), which the Go compiler will block for use by external projects. All exported items outside of the `internal` directory are considered a public API and subject to backwards compatibility guarantees, except files ending in `_test.go`.\\nThe `crypto` package may be split out to a separate module in a separate repo. This is the main general-purpose package used by external projects, and is the only Tendermint dependency in e.g. IAVL which can cause some problems for projects depending on both IAVL and Tendermint. This will be decided after further discussion.\\nThe `tm-db` package will remain a separate module in a separate repo. The `crypto` package may possibly be split out, pending further discussion, as this is the main general-purpose package used by other projects.\\n","Predictions":"* **Semantic versioning**: a version of the project that is not a public version.n* **External package:** a different Go package, can be a child or sibling package in the same project.n* **Internal code:** code not intended for use in external projects.n* **Internal directory:** code under internal\/ which cannot be imported in external projects.n* **Exported:** a Go identifier starting with an uppercase letter, which can therefore be accessed by an external package.n* **Private:** a Go identifier starting with a lowercase letter, which therefore cannot be accessed by an external package unless via an exported field, variable, or function\/method return value.n* **Public API:** any Go identifier that can be imported or accessed by an external project, except test code in _test.go files.n* **Private API:** any Go identifier that is not accessible via a public API, including all code in the internal directory.n"}
{"File Name":"operational-data-hub\/0059-runtime-dependency-only-on-gcp.md","Context":"## Context\nAvailability of systems can be improved by reducing the number of dependencies. Each additional dependency comes with the risk of that service breaking, causing issues to our system. On the other hand, 3rd party services can be leveraged to quickly build and apply managed services at low cost, compared to creating and running these services by ourselves. Therefore, the right balance between dependency on and benefitting from external services is important.\nDuring run the changes in 3rd party dependencies do not change. Therefore, caching or copying the functionality of these dependencies is possible in many cases. In build and development environments, the dependencies on 3rd party components and services change more often. Therefore, it is oftentimes not opportune to invest in becoming independent on these components and services.\n","Decision":"During runtime, we will only depend on resources services from the Google Cloud Platform.\\n","tokens":158,"id":2701,"text":"## Context\\nAvailability of systems can be improved by reducing the number of dependencies. Each additional dependency comes with the risk of that service breaking, causing issues to our system. On the other hand, 3rd party services can be leveraged to quickly build and apply managed services at low cost, compared to creating and running these services by ourselves. Therefore, the right balance between dependency on and benefitting from external services is important.\\nDuring run the changes in 3rd party dependencies do not change. Therefore, caching or copying the functionality of these dependencies is possible in many cases. In build and development environments, the dependencies on 3rd party components and services change more often. Therefore, it is oftentimes not opportune to invest in becoming independent on these components and services.\\n\n\n##Decision\nDuring runtime, we will only depend on resources services from the Google Cloud Platform.\\n","Predictions":"TBD.n### Positive Consequencesn* Increased ability to scale number of services.n* Increased ability to scale under load.n* Increased resilience to service failures.n### Negative Consequencesn* Increased complexity of the architecture.n* Increased complexity of the infrastructure.n"}
{"File Name":"govuk-aws\/0038-mongo_replacement_by_documentdb.md","Context":"## Context\nGOV.UK uses MongoDB as the backing database for a number of apps. In a generic\nGOV.UK environment, there are 5 MongoDB clusters: one for Licensify (a legacy\nservice), one for performance monitoring, one for router and another per cloud\nprovider (i.e. AWS and Carrenza) for all other apps (e.g. assets, imminence,\nmaslow, content, publisher and short url manager).\nOne architectural strategy principle of TechOps is to make use as much as\npossible AWS managed services in order to reduce the burden on GOV.UK teams to\nmanage Commercial off-the-shelf (COTS) functionality\/services.\n","Decision":"The approaches taken for migrating from MongoDB to AWS DocumentDB are:\\n1. new apps that are being migrated from legacy cloud providers (e.g UKCloud and\\nCarrenza) to AWS and use MongoDB should be evaluated for compatibility with\\nAWS DocumentDB during the migration phase. Based on this evaluation, a\\ndecision should be made based on cost benefit whether a particular migrating\\napp should be using DocumentDB or MongoDB.\\n2. there are some apps (e.g. content store and imminence) that have already been\\nmigrated to AWS and uses the MongoDB cluster there. These apps should be\\nevaluated for compatibility with AWS DocumentDB and prioritised accordingly\\nfor migration to AWS DocumentDB.\\n","tokens":145,"id":4047,"text":"## Context\\nGOV.UK uses MongoDB as the backing database for a number of apps. In a generic\\nGOV.UK environment, there are 5 MongoDB clusters: one for Licensify (a legacy\\nservice), one for performance monitoring, one for router and another per cloud\\nprovider (i.e. AWS and Carrenza) for all other apps (e.g. assets, imminence,\\nmaslow, content, publisher and short url manager).\\nOne architectural strategy principle of TechOps is to make use as much as\\npossible AWS managed services in order to reduce the burden on GOV.UK teams to\\nmanage Commercial off-the-shelf (COTS) functionality\/services.\\n\n\n##Decision\nThe approaches taken for migrating from MongoDB to AWS DocumentDB are:\\n1. new apps that are being migrated from legacy cloud providers (e.g UKCloud and\\nCarrenza) to AWS and use MongoDB should be evaluated for compatibility with\\nAWS DocumentDB during the migration phase. Based on this evaluation, a\\ndecision should be made based on cost benefit whether a particular migrating\\napp should be using DocumentDB or MongoDB.\\n2. there are some apps (e.g. content store and imminence) that have already been\\nmigrated to AWS and uses the MongoDB cluster there. These apps should be\\nevaluated for compatibility with AWS DocumentDB and prioritised accordingly\\nfor migration to AWS DocumentDB.\\n","Predictions":"The approach taken for migrating from MongoDB to AWS DocumentDB is:n1. new apps that are being migrated from legacy cloud providers (e.g. UKCloud andnCarrenza) to AWS and use MongoDB during the migration phase. Based this evaluation, andecision should be made based on cost benefit whether a particular migratingnapp should be using DocumentDB or MongoDB.n2. there are some apps (e.g. content store and imminence) that have already beennmigrated to AWS and uses the MongoDB cluster there. These apps should benevaluated for compatibility with AWS DocumentDB and prioritised accordinglynfor migration to AWS DocumentDB.n"}
{"File Name":"oasis-core\/0001-tm-multi-root-apphash.md","Context":"## Context\nCurrently the Tendermint ABCI application hash is equal to the consensus state\nroot for a specific height. In order to allow additional uses, like proving to\nlight clients that specific events have been emitted in a block, we should make\nthe application hash be derivable from potentially different kinds of roots.\n","Decision":"The proposed design is to derive the Tendermint ABCI application hash by hashing\\nall the different roots as follows:\\n```\\nAppHash := H(Context || Root_0 || ... || Root_n)\\n```\\nWhere:\\n- `H` is the SHA-512\/256 hash function.\\n- `Context` is the string `oasis-core\/tendermint: roots`.\\n- `Root_i` is the fixed-size SHA-512\/256 root hash of the specified root.\\nCurrently, the only root would be the existing consensus state root at index 0.\\nTo implement this change the following modifications would be required:\\n- Update the ABCI multiplexer's `Commit` method to calculate and return the\\napplication hash using the scheme specified above.\\n- Update the consensus API `SignedHeader` response to include the\\n`UntrustedStateRoot` (the untrusted prefix denotes that the user must verify\\nthat the state root corresponds to `AppHash` provided in the signed header in\\n`Meta`).\\nWhen new roots will be added in the future, both `Block` and `SignedHeader`\\nwill need to include them all.\\n","tokens":65,"id":4354,"text":"## Context\\nCurrently the Tendermint ABCI application hash is equal to the consensus state\\nroot for a specific height. In order to allow additional uses, like proving to\\nlight clients that specific events have been emitted in a block, we should make\\nthe application hash be derivable from potentially different kinds of roots.\\n\n\n##Decision\nThe proposed design is to derive the Tendermint ABCI application hash by hashing\\nall the different roots as follows:\\n```\\nAppHash := H(Context || Root_0 || ... || Root_n)\\n```\\nWhere:\\n- `H` is the SHA-512\/256 hash function.\\n- `Context` is the string `oasis-core\/tendermint: roots`.\\n- `Root_i` is the fixed-size SHA-512\/256 root hash of the specified root.\\nCurrently, the only root would be the existing consensus state root at index 0.\\nTo implement this change the following modifications would be required:\\n- Update the ABCI multiplexer's `Commit` method to calculate and return the\\napplication hash using the scheme specified above.\\n- Update the consensus API `SignedHeader` response to include the\\n`UntrustedStateRoot` (the untrusted prefix denotes that the user must verify\\nthat the state root corresponds to `AppHash` provided in the signed header in\\n`Meta`).\\nWhen new roots will be added in the future, both `Block` and `SignedHeader`\\nwill need to include them all.\\n","Predictions":"The proposed design is to derive the Tendermint ABCI application hash by hashingnall the different root as follows:nH is the SHA-512\/256 hash function.nContext is the string oasis-core\/tendermint: root.nRoot_i is the fixed-size SHA-512\/256 root hash of the specified root.nCurrently, the only root would be the existing consensus state root at index 0.nTo implement this change the following modifications would be required:n- Update the ABCI multiplexer's Commit method to calculate and return thenapplication hash using the scheme specified above.n- Update the consensus API SignedHeader response to include thenUntrustedStateRoot (the untrusted prefix denotes that the user must verifynthat the state root corresponds to AppHash provided in the signed header innMeta).nWhen new roots will be added in the future, both Block and SignedHeadernwill need to include them all.n"}
{"File Name":"structurizr-python\/0002-version-control-our-code.md","Context":"## Context\nWe need to version control our code in order to avoid disasters and maintain\nsanity. We also want to collaborate online with a wider community.\n","Decision":"We use git for version control and GitHub for collaboration.\\n","tokens":33,"id":398,"text":"## Context\\nWe need to version control our code in order to avoid disasters and maintain\\nsanity. We also want to collaborate online with a wider community.\\n\n\n##Decision\nWe use git for version control and GitHub for collaboration.\\n","Predictions":"We will use a classify command to create a new class for each of the following classes:n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---n!---"}
{"File Name":"adr-tools\/0005-help-comments.md","Context":"## Context\nThe tool will have a `help` subcommand to provide documentation\nfor users.\nIt's nice to have usage documentation in the script files\nthemselves, in comments.  When reading the code, that's the first\nplace to look for information about how to run a script.\n","Decision":"Write usage documentation in comments in the source file.\\nDistinguish between documentation comments and normal comments.\\nDocumentation comments have two hash characters at the start of\\nthe line.\\nThe `adr help` command can parse comments out from the script\\nusing the standard Unix tools `grep` and `cut`.\\n","tokens":64,"id":3566,"text":"## Context\\nThe tool will have a `help` subcommand to provide documentation\\nfor users.\\nIt's nice to have usage documentation in the script files\\nthemselves, in comments.  When reading the code, that's the first\\nplace to look for information about how to run a script.\\n\n\n##Decision\nWrite usage documentation in comments in the source file.\\nDistinguish between documentation comments and normal comments.\\nDocumentation comments have two hash characters at the start of\\nthe line.\\nThe `adr help` command can parse comments out from the script\\nusing the standard Unix tools `grep` and `cut`.\\n","Predictions":"The codestyleguide is a set of conventions on how to write the source code. It covers many areas, including the use of camelCase vs PascalCase for variable identifiers, whether or not to use a semicolon as a statement termination, or the use of tabs or spaces for indentation.nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and money.n"}
{"File Name":"connecting-to-services\/0003-use-postcodes-io-for-postcode-lookup.md","Context":"## Context\nThe application is about finding services closest to the search point. All\nservices have a co-ordinate in lat\/lon format. Currently the application\nrequests either an out-code or a postcode as the search point. The submitted\npostcode needs to be resolved to a lat\/lon that can be used to query against.\n","Decision":"[Postcodes.io](https:\/\/postcodes.io\/) provides both an out-code and a postcode\\nlookup. The results of which return latitude and longitude. The service is\\nfree, [supported](https:\/\/postcodes.io\/about),\\n[monitored](https:\/\/status.ideal-postcodes.co.uk\/) and contains the full set of\\nactive postcodes for the UK. Supplied by the\\n[ONS](https:\/\/data.gov.uk\/dataset\/national-statistics-postcode-lookup-uk).\\n","tokens":68,"id":2204,"text":"## Context\\nThe application is about finding services closest to the search point. All\\nservices have a co-ordinate in lat\/lon format. Currently the application\\nrequests either an out-code or a postcode as the search point. The submitted\\npostcode needs to be resolved to a lat\/lon that can be used to query against.\\n\n\n##Decision\n[Postcodes.io](https:\/\/postcodes.io\/) provides both an out-code and a postcode\\nlookup. The results of which return latitude and longitude. The service is\\nfree, [supported](https:\/\/postcodes.io\/about),\\n[monitored](https:\/\/status.ideal-postcodes.co.uk\/) and contains the full set of\\nactive postcodes for the UK. Supplied by the\\n[ONS](https:\/\/data.gov.uk\/dataset\/national-statistics-postcode-lookup-uk).\\n","Predictions":"The --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used to determine the require state of the application.nThe --require parameter will be used"}
{"File Name":"connaisseur\/ADR-2_release-management.md","Context":"## Context\nDuring its initial development Connaisseur was more or less maintained by a single person and not released frequently. Hence, the easiest option was to just have the maintainer build and push at certain stages of development. With the influx of more team members, the number of contributions and hence the number of needed\/reasonable releases went up. Also since publication, it is more important that the uploaded Connaisseur image corresponds to the most recent version referenced in the Helm chart.\nA single person having to build, sign and push the images whenever a new pull request is accepted is hence unpractical for both development and agility.\n","Decision":"For choice 1, we decided to go for two branches. On the one hand, `master` being the branch that contains the code of the latest release and will be tagged with release versions. On the other hand, there will be a `develop` branch that hosts the current state of development and will be merged to `master` whenever we want to create a new release.\\nThis way we get rid of the current pain of releasing with every pull request at the cost a some overhead during release.\\nIn the process of automating most of the release process, we will run an integration test with locally built images for pull requests to `master`. Regarding choice 2, whenever a pull request is merged, whoever merged the PR has to tag this commit on the `master` branch with the most recent version. Right after the merge, whoever merged the PR builds, signs and pushes the new Connaisseur release and creates a tag on the `master` branch referencing the new release version.\\nAfter the image is pushed and the new commit tagged, the pipeline will run the integration test with the image pulled from Docker Hub to ensure that the released version is working.\\nWe decided for this option as it does not expose credentials to GitHub Actions, which we wanted to avoid especially in light of the [recent GitHub Actions injection attacks](https:\/\/bugs.chromium.org\/p\/project-zero\/issues\/detail?id=2070) and as it would also prevent us from opening up the repository to Pull Requests. To alleviate the work required for doing the steps outside the pipeline we use a shell script that will automate these steps given suitable environment, i.e. Docker context and DCT keys.\\n### Positive consequences\\n- We can develop without having to ship changes immediatly.\\n- Release process does not expose credentials to GitHub Actions.\\n- Code gets Git tags.\\n### Negative consequences\\n- Process from code to release for a single change is more cumbersome than right now.\\n- Release still requires human intervention.\\n","tokens":131,"id":4397,"text":"## Context\\nDuring its initial development Connaisseur was more or less maintained by a single person and not released frequently. Hence, the easiest option was to just have the maintainer build and push at certain stages of development. With the influx of more team members, the number of contributions and hence the number of needed\/reasonable releases went up. Also since publication, it is more important that the uploaded Connaisseur image corresponds to the most recent version referenced in the Helm chart.\\nA single person having to build, sign and push the images whenever a new pull request is accepted is hence unpractical for both development and agility.\\n\n\n##Decision\nFor choice 1, we decided to go for two branches. On the one hand, `master` being the branch that contains the code of the latest release and will be tagged with release versions. On the other hand, there will be a `develop` branch that hosts the current state of development and will be merged to `master` whenever we want to create a new release.\\nThis way we get rid of the current pain of releasing with every pull request at the cost a some overhead during release.\\nIn the process of automating most of the release process, we will run an integration test with locally built images for pull requests to `master`. Regarding choice 2, whenever a pull request is merged, whoever merged the PR has to tag this commit on the `master` branch with the most recent version. Right after the merge, whoever merged the PR builds, signs and pushes the new Connaisseur release and creates a tag on the `master` branch referencing the new release version.\\nAfter the image is pushed and the new commit tagged, the pipeline will run the integration test with the image pulled from Docker Hub to ensure that the released version is working.\\nWe decided for this option as it does not expose credentials to GitHub Actions, which we wanted to avoid especially in light of the [recent GitHub Actions injection attacks](https:\/\/bugs.chromium.org\/p\/project-zero\/issues\/detail?id=2070) and as it would also prevent us from opening up the repository to Pull Requests. To alleviate the work required for doing the steps outside the pipeline we use a shell script that will automate these steps given suitable environment, i.e. Docker context and DCT keys.\\n### Positive consequences\\n- We can develop without having to ship changes immediatly.\\n- Release process does not expose credentials to GitHub Actions.\\n- Code gets Git tags.\\n### Negative consequences\\n- Process from code to release for a single change is more cumbersome than right now.\\n- Release still requires human intervention.\\n","Predictions":"We will use a master branch to maintain the code of the latest release and be tagged with release versions.n"}
{"File Name":"opensmarthouse-core\/adr0000.md","Context":"## Context\nOpenHAB uses OSGi as a runtime.\nThis runtime promotes clear separation between implementation and contract (API\/implementation\/SPI) packages.\nOnce program gets launched OSGi framework such Apache Felix or Eclipse Equinox makes sure that \"implementation\" packages stay hidden.\nOn the build tool side we do not have such strong separation because many parts of project are co-developed.\nInternal packages and API are in the same source root, and often functionally different elements of code are included in the same bundle.\nFor example, this means that the `org.openhab.core.items` package is in the same module as `org.openhab.core.items.internal`.\nAs a result, during compile time we have all of the dependencies together - ones which are required by `core.items` and ones used by `core.items.internal` package.\nWhile it might not cause major issues for this module, it might have devastating influence over callers who depend on public parts of the API.\nDuring compilation phase they will get polluted by internal package dependencies and quite often use them.\nSuch approach promotes tight coupling between contract and implementation.\nMore over, it also promotes exposure of specific implementation classes via public API.\nThe natural way to deal with such things is to address them with a build tool that includes an appropriate includes\/excludes mechanism for dependencies.\nIt would work properly, but openHAB core is a single jar which makes things even harder.\nThis means that quite many dependencies get unnecessarily propagated to all callers of public APIs.\nopenHAB utilizes Apache Karaf for provisioning of the application.\nKaraf provisioning itself is capable of verifying its \"features\" based on declared modules, bundles, JAR files, etc.\nCurrently, most of the project features depend on one of two root features, `openhab-core-base` or `openhab-runtime-base`, making no distinction on how particular parts of the framework interact with each other.\nA tiny extension (SPI) bundle that is targeted at a specific framework feature would need to resolve one of the above features, which would then lead to the activation of the entire openhab framework.\nPresent structure of modules \/ bundles is as follows:\n```\n[openhab thing core] <--- [openhab rest core]\n```\n","Decision":"Since openHAB core is a framework, we need to keep strong and clear separation of API and implementation packages.\\nThis will clarify module responsibilities and streamline development of extensions through smaller and easier to maintain modules.\\nThere are also significant opportunities to speed up the build processes and make better use of resolution and graph analysis between framework parts.\\nWe decided that a clear separation between API and implementation packages should be made.\\nThis means that each service intended to be interacted with through an OSGi services API should form its own contract bundle.\\nNew framework modules will separate `internal` package under a `core.<area>.core` bundle and have a distinct interface in a separate module.\\nAdditionally, functionally separate components should be placed in separate bundles to ensure that unnecessary dependencies are not pulled in to a build.\\nThis is a big change which involves a lot of code relocations and also cleans up the project dependencies which were built up over time.\\nBy preserving public package names we will be able to keep backward compatibility at an affordable level.\\n```\\n[openhab thing api] <--- [openhab rest thing]\\n^\\n|\\n[openhab thing core]\\n```\\nIdeally, such a pattern should evolve further to include a common structure used for keeping contracts and implementations separate.\\n","tokens":471,"id":660,"text":"## Context\\nOpenHAB uses OSGi as a runtime.\\nThis runtime promotes clear separation between implementation and contract (API\/implementation\/SPI) packages.\\nOnce program gets launched OSGi framework such Apache Felix or Eclipse Equinox makes sure that \"implementation\" packages stay hidden.\\nOn the build tool side we do not have such strong separation because many parts of project are co-developed.\\nInternal packages and API are in the same source root, and often functionally different elements of code are included in the same bundle.\\nFor example, this means that the `org.openhab.core.items` package is in the same module as `org.openhab.core.items.internal`.\\nAs a result, during compile time we have all of the dependencies together - ones which are required by `core.items` and ones used by `core.items.internal` package.\\nWhile it might not cause major issues for this module, it might have devastating influence over callers who depend on public parts of the API.\\nDuring compilation phase they will get polluted by internal package dependencies and quite often use them.\\nSuch approach promotes tight coupling between contract and implementation.\\nMore over, it also promotes exposure of specific implementation classes via public API.\\nThe natural way to deal with such things is to address them with a build tool that includes an appropriate includes\/excludes mechanism for dependencies.\\nIt would work properly, but openHAB core is a single jar which makes things even harder.\\nThis means that quite many dependencies get unnecessarily propagated to all callers of public APIs.\\nopenHAB utilizes Apache Karaf for provisioning of the application.\\nKaraf provisioning itself is capable of verifying its \"features\" based on declared modules, bundles, JAR files, etc.\\nCurrently, most of the project features depend on one of two root features, `openhab-core-base` or `openhab-runtime-base`, making no distinction on how particular parts of the framework interact with each other.\\nA tiny extension (SPI) bundle that is targeted at a specific framework feature would need to resolve one of the above features, which would then lead to the activation of the entire openhab framework.\\nPresent structure of modules \/ bundles is as follows:\\n```\\n[openhab thing core] <--- [openhab rest core]\\n```\\n\n\n##Decision\nSince openHAB core is a framework, we need to keep strong and clear separation of API and implementation packages.\\nThis will clarify module responsibilities and streamline development of extensions through smaller and easier to maintain modules.\\nThere are also significant opportunities to speed up the build processes and make better use of resolution and graph analysis between framework parts.\\nWe decided that a clear separation between API and implementation packages should be made.\\nThis means that each service intended to be interacted with through an OSGi services API should form its own contract bundle.\\nNew framework modules will separate `internal` package under a `core.<area>.core` bundle and have a distinct interface in a separate module.\\nAdditionally, functionally separate components should be placed in separate bundles to ensure that unnecessary dependencies are not pulled in to a build.\\nThis is a big change which involves a lot of code relocations and also cleans up the project dependencies which were built up over time.\\nBy preserving public package names we will be able to keep backward compatibility at an affordable level.\\n```\\n[openhab thing api] <--- [openhab rest thing]\\n^\\n|\\n[openhab thing core]\\n```\\nIdeally, such a pattern should evolve further to include a common structure used for keeping contracts and implementations separate.\\n","Predictions":"- OpenHAB is a openhab-runtime that is used by the application.n- OpenHAB is a openhab-runtime that is used by the application.n- OpenHAB is a openhab-runtime that is used by the application.n- OpenHAB is a openhab-runtime that is used by the application.n- OpenHAB is a openhab-runtime that is used by the application.n- OpenHAB is a openhab-runtime that is used by the application.n- OpenHAB is a openhab-runtime that is used by the application.n- OpenHAB is a openhab-runtime that is used by the application.n- OpenHAB is a openhab-runtime that is used by the application.n- OpenHAB is a openhab-runtime that is used by the application.n- OpenHAB is a openhab-runtime that is used by the application.n- OpenHAB is a openhab-runtime that is used by the application.n- OpenHAB is a openhab-runtime that is used by the application.n-  OpenHAB is a openhab-runtime that is used by the application.n-  OpenHAB is a openhab-runtime that is used by the application.n-  OpenHAB is a openhab-runtime that is used by the application.n-  OpenHAB is a openhab-runtime that is used by the application.n-  OpenHAB is a openhab-runtime that is used by the application.n-  OpenHAB is a openhab-runtime that is used by the application.n-  OpenHAB is a openhab-runtime that is used by the application.n-  OpenHAB is a openhab-runtime that is used by the application.n-  OpenHAB is a openhab-runtime that is used by the application.n-  OpenHAB is a openhab-runtime that is used by the application.n-  OpenHAB is a "}
{"File Name":"drt-v2\/0006-physical-deployment-one-jvm-per-port.md","Context":"## Context\nDRTv1 tried to normalise all ports into a single model. We frequently had requests for port specific features\nor structures. Also it\n","Decision":"Use a jvm per airport\\n","tokens":33,"id":1912,"text":"## Context\\nDRTv1 tried to normalise all ports into a single model. We frequently had requests for port specific features\\nor structures. Also it\\n\n\n##Decision\nUse a jvm per airport\\n","Predictions":"We will use a stack to store our data.n"}
{"File Name":"atlasdb\/0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md","Context":"## Context\nCassandra [has an issue](https:\/\/issues.apache.org\/jira\/browse\/CASSANDRA-10699) which can cause data loss in the situation:\n1. Node A and node B concurrently create table \"some.table\"\n2. Both table creations succeed, creating tables with column family ids \"123\" and \"456\"\n3. Cassandra picks \"123\" to be the correct table for \"some.table\"\n4. Cassandra is restarted\n5. After restart Cassandra gossips and decides that \"456\" is the correct table for \"some.table\"\n6. All data that was written to \"123\" is now lost\nTo fix this we must prevent tables from being created at the same time.\n","Decision":"All schema mutations will globally synchronise via a check-and-set operation on a specific cell in a _lock table.\\n","tokens":151,"id":3094,"text":"## Context\\nCassandra [has an issue](https:\/\/issues.apache.org\/jira\/browse\/CASSANDRA-10699) which can cause data loss in the situation:\\n1. Node A and node B concurrently create table \"some.table\"\\n2. Both table creations succeed, creating tables with column family ids \"123\" and \"456\"\\n3. Cassandra picks \"123\" to be the correct table for \"some.table\"\\n4. Cassandra is restarted\\n5. After restart Cassandra gossips and decides that \"456\" is the correct table for \"some.table\"\\n6. All data that was written to \"123\" is now lost\\nTo fix this we must prevent tables from being created at the same time.\\n\n\n##Decision\nAll schema mutations will globally synchronise via a check-and-set operation on a specific cell in a _lock table.\\n","Predictions":"We will use the [Security Stream](https:\/\/securitystream.org\/) to store the data we want to store.n"}
{"File Name":"saas-platform-frontend\/0006-use-antdesign-as-the-ui-framework.md","Context":"## Context\nTo speed up the UI development we need to select a UI Framework that has a good community as well as good functionality.\n","Decision":"We use Ant Design as the UI Framework.\\n","tokens":28,"id":3718,"text":"## Context\\nTo speed up the UI development we need to select a UI Framework that has a good community as well as good functionality.\\n\n\n##Decision\nWe use Ant Design as the UI Framework.\\n","Predictions":"We use Ant Design as the UI Framework.n"}
{"File Name":"docspell\/0012_periodic_tasks.md","Context":"# Context and Problem Statement\nCurrently there is a `Scheduler` that consumes tasks off a queue in\nthe database. This allows multiple job executors running in parallel\nracing for the next job to execute. This is for executing tasks\nimmediately \u2013 as long as there are enough resource.\nWhat is missing, is a component that maintains periodic tasks. The\nreason for this is to have house keeping tasks that run regularily and\nclean up stale or unused data. Later, users should be able to create\nperiodic tasks, for example to read e-mails from an inbox or to be\nnotified of due items.\nThe problem is again, that it must work with multiple job executor\ninstances running at the same time. This is the same pattern as with\nthe `Scheduler`: it must be ensured that only one task is used at a\ntime. Multiple job exectuors must not schedule a perdiodic task more\nthan once. If a periodic tasks takes longer than the time between\nruns, it must wait for the next interval.\n# Considered Options\n1. Adding a `timer` and `nextrun` field to the current `job` table\n2. Creating a separate table for periodic tasks\n","Decision":"The 2. option.\\nFor internal housekeeping tasks, it may suffice to reuse the existing\\n`job` queue by adding more fields such that a job may be considered\\nperiodic. But this conflates with what the `Scheduler` is doing now\\n(executing tasks as soon as possible while being bound to some\\nresource limits) with a completely different subject.\\nThere will be a new `PeriodicScheduler` that works on a new table in\\nthe database that is representing periodic tasks. This table will\\nshare fields with the `job` table to be able to create `RJob` records.\\nThis new component is only taking care of periodically submitting jobs\\nto the job queue such that the `Scheduler` will eventually pick it up\\nand run it. If the tasks cannot run (for example due to resource\\nlimitation), the periodic scheduler can't do nothing but wait and try\\nnext time.\\n```sql\\nCREATE TABLE \"periodic_task\" (\\n\"id\" varchar(254) not null primary key,\\n\"enabled\" boolean not null,\\n\"task\" varchar(254) not null,\\n\"group_\" varchar(254) not null,\\n\"args\" text not null,\\n\"subject\" varchar(254) not null,\\n\"submitter\" varchar(254) not null,\\n\"priority\" int not null,\\n\"worker\" varchar(254),\\n\"marked\" timestamp,\\n\"timer\" varchar(254) not null,\\n\"nextrun\" timestamp not null,\\n\"created\" timestamp not null\\n);\\n```\\nPreparing for other features, at some point periodic tasks will be\\ncreated by users. It should be possible to disable\/enable them. The\\nnext 6 properties are needed to insert jobs into the `job` table. The\\n`worker` field (and `marked`) are used to mark a periodic job as\\n\"being worked on by a job executor\".\\nThe `timer` is the schedule, which is a\\n[systemd-like](https:\/\/man7.org\/linux\/man-pages\/man7\/systemd.time.7.html#CALENDAR_EVENTS)\\ncalendar event string. This is parsed by [this\\nlibrary](https:\/\/github.com\/eikek\/calev). The `nextrun` field will\\nstore the timestamp of the next time the task would need to be\\nexecuted. This is needed to query this table for the newest task.\\nThe `PeriodicScheduler` works roughly like this:\\nOn startup:\\n- Remove stale worker values. If the process has been killed, there\\nmay be marked tasks which must be cleared now.\\nMain-Loop:\\n0. Cancel current scheduled notify (see 4. below)\\n1. get next (= earliest & enabled) periodic job\\n2. if none: stop\\n3. if triggered (= `nextrun <= 'now'`):\\n- Mark periodic task. On fail: goto 1.\\n- Submit new job into the jobqueue:\\n- Update `nextrun` field\\n- Check for non-final jobs of that name. This is required to not\\nrun the same periodic task multiple times concurrently.\\n- if exist: goto 4.\\n- if not exist: submit job\\n- Unmark periodic task\\n4. if future\\n- schedule notify: notify self to run again next time the task\\nschedule triggers\\n","tokens":257,"id":279,"text":"# Context and Problem Statement\\nCurrently there is a `Scheduler` that consumes tasks off a queue in\\nthe database. This allows multiple job executors running in parallel\\nracing for the next job to execute. This is for executing tasks\\nimmediately \u2013 as long as there are enough resource.\\nWhat is missing, is a component that maintains periodic tasks. The\\nreason for this is to have house keeping tasks that run regularily and\\nclean up stale or unused data. Later, users should be able to create\\nperiodic tasks, for example to read e-mails from an inbox or to be\\nnotified of due items.\\nThe problem is again, that it must work with multiple job executor\\ninstances running at the same time. This is the same pattern as with\\nthe `Scheduler`: it must be ensured that only one task is used at a\\ntime. Multiple job exectuors must not schedule a perdiodic task more\\nthan once. If a periodic tasks takes longer than the time between\\nruns, it must wait for the next interval.\\n# Considered Options\\n1. Adding a `timer` and `nextrun` field to the current `job` table\\n2. Creating a separate table for periodic tasks\\n\n\n##Decision\nThe 2. option.\\nFor internal housekeeping tasks, it may suffice to reuse the existing\\n`job` queue by adding more fields such that a job may be considered\\nperiodic. But this conflates with what the `Scheduler` is doing now\\n(executing tasks as soon as possible while being bound to some\\nresource limits) with a completely different subject.\\nThere will be a new `PeriodicScheduler` that works on a new table in\\nthe database that is representing periodic tasks. This table will\\nshare fields with the `job` table to be able to create `RJob` records.\\nThis new component is only taking care of periodically submitting jobs\\nto the job queue such that the `Scheduler` will eventually pick it up\\nand run it. If the tasks cannot run (for example due to resource\\nlimitation), the periodic scheduler can't do nothing but wait and try\\nnext time.\\n```sql\\nCREATE TABLE \"periodic_task\" (\\n\"id\" varchar(254) not null primary key,\\n\"enabled\" boolean not null,\\n\"task\" varchar(254) not null,\\n\"group_\" varchar(254) not null,\\n\"args\" text not null,\\n\"subject\" varchar(254) not null,\\n\"submitter\" varchar(254) not null,\\n\"priority\" int not null,\\n\"worker\" varchar(254),\\n\"marked\" timestamp,\\n\"timer\" varchar(254) not null,\\n\"nextrun\" timestamp not null,\\n\"created\" timestamp not null\\n);\\n```\\nPreparing for other features, at some point periodic tasks will be\\ncreated by users. It should be possible to disable\/enable them. The\\nnext 6 properties are needed to insert jobs into the `job` table. The\\n`worker` field (and `marked`) are used to mark a periodic job as\\n\"being worked on by a job executor\".\\nThe `timer` is the schedule, which is a\\n[systemd-like](https:\/\/man7.org\/linux\/man-pages\/man7\/systemd.time.7.html#CALENDAR_EVENTS)\\ncalendar event string. This is parsed by [this\\nlibrary](https:\/\/github.com\/eikek\/calev). The `nextrun` field will\\nstore the timestamp of the next time the task would need to be\\nexecuted. This is needed to query this table for the newest task.\\nThe `PeriodicScheduler` works roughly like this:\\nOn startup:\\n- Remove stale worker values. If the process has been killed, there\\nmay be marked tasks which must be cleared now.\\nMain-Loop:\\n0. Cancel current scheduled notify (see 4. below)\\n1. get next (= earliest & enabled) periodic job\\n2. if none: stop\\n3. if triggered (= `nextrun <= 'now'`):\\n- Mark periodic task. On fail: goto 1.\\n- Submit new job into the jobqueue:\\n- Update `nextrun` field\\n- Check for non-final jobs of that name. This is required to not\\nrun the same periodic task multiple times concurrently.\\n- if exist: goto 4.\\n- if not exist: submit job\\n- Unmark periodic task\\n4. if future\\n- schedule notify: notify self to run again next time the task\\nschedule triggers\\n","Predictions":"1. Adding a timer and nextrun field to the current job tablen2. Creating a separate table for periodic tasksn3. Creating a separate table for periodic tasksn4. Adding a timer and nextrun field to the current job tablen5. Adding a new PeriodicScheduler that works on a new table innthe database that is representing periodic tasks. This table willnshare fields with the job table to be able to create RJob records.nThis new component is only taking care of periodically submitting jobsnto the job queue such that the Scheduler is able to create RJob records.nThis new component is only taking care of periodically submitting jobsnto the job queue such that the Scheduler is able to create RJob records.n1. Adding a timer and nextrun field to the current job tablen2. Creating a separate table for periodic tasksn3. Creating a separate table for periodic tasksn4. Adding a timer and nextrun field to the current job tablen5. Adding a new PeriodicScheduler that works on a new table innthe database that is representing periodic tasks. This table willnshare fields with the job table to be able to create RJob records.nThis new component is only taking care of periodically submitting jobsnto the job queue such that the Scheduler is able to create RJob records.n1. Adding a new PeriodicScheduler that works on a new table innthe database that is representing periodic tasks. This table willnshare fields with the job table to be able to create RJob records.n1. Adding a new PeriodicScheduler that works on a new table innthe database that is representing periodic tasks. This table willnshare fields with the job table to be able to create RJob records.n1. Adding a new PeriodicScheduler that works on a new table innthe database that is representing periodic tasks. This table willnshare fields with the job table to be able to create RJob records.n1. Adding a new PeriodicScheduler that works on a new table innthe database that is representing periodic tasks. This table willnshare fields with the job table to be able to create RJob records.n1. Adding a new PeriodicScheduler that works on a new table innthe database that is representing periodic tasks. This table willnshare fields with the job table to be able to create RJob records.n1. Adding a new PeriodicScheduler that works on a new table innthe database that is representing periodic tasks. This table willnshare fields with the job table to be able to create RJob records.n1. Adding a new PeriodicScheduler that works on a new table"}
{"File Name":"buildit-all\/0005-use-id-token-from-microsoft-as-bearer-token.md","Context":"## Context\nIn the interest of time and getting something to work, we are going to break up the steps further\n","Decision":"* Instead of exchanging id_token for opaque access_token, the client will always send the id_token as the Bearer token\\n* Proper validation of the id_token will still occur\\n","tokens":24,"id":4416,"text":"## Context\\nIn the interest of time and getting something to work, we are going to break up the steps further\\n\n\n##Decision\n* Instead of exchanging id_token for opaque access_token, the client will always send the id_token as the Bearer token\\n* Proper validation of the id_token will still occur\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).nADRs from here on out will be written in the first-person **singular** tense, since it's mainly just me building the app.n"}
{"File Name":"openfido-app-service\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2472,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"FindMeFoodTrucks\/Messaging choice.md","Context":"## :dart: Context\nThe Web APIs can be implemented as Synchronous or Asynchronous.\n","Decision":"Considering the fact that the APIs does a datastore look up which can take some time, making the services asynchronous is the recommendation.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/azure.microsoft.com\/mediahandler\/files\/resourcefiles\/api-design\/Azure_API-Design_Guide_eBook.pdf\\n","tokens":20,"id":1091,"text":"## :dart: Context\\nThe Web APIs can be implemented as Synchronous or Asynchronous.\\n\n\n##Decision\nConsidering the fact that the APIs does a datastore look up which can take some time, making the services asynchronous is the recommendation.\\nThe decision is based on the guidance provided by Microsoft here: https:\/\/azure.microsoft.com\/mediahandler\/files\/resourcefiles\/api-design\/Azure_API-Design_Guide_eBook.pdf\\n","Predictions":"We will use Log as the Log API.n"}
{"File Name":"datalab\/0002-ansible-for-provisioning-tool.md","Context":"## Context\nWe need a tool to provision servers and software for the datalabs project.\n","Decision":"We will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessella\\nteam have experience using it.\\n","tokens":20,"id":771,"text":"## Context\\nWe need a tool to provision servers and software for the datalabs project.\\n\n\n##Decision\nWe will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessella\\nteam have experience using it.\\n","Predictions":"We will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessellanteam have experience using it.n"}
{"File Name":"govuk-aws\/0008-postgres-on-puppetmaster.md","Context":"## Context\nWe need to decide whether to run PostgreSQL on the Puppetmaster itself or use\nRDS from the off.\nCurrently our Puppetmaster uses a local PostgreSQL instance as its database. In moving to AWS we have the opportunity to replace this with an RDS instance.\nThis may remove a reasonable amount of puppet code used to configure the database and associated tasks (e.g. backups). It would also require an amount of Terraform work to provision the RDS instance and some updates to the Puppetmaster code to use that instance.\n","Decision":"The Puppetmaster will continue to use a local instance of PostgreSQL.\\n","tokens":110,"id":4061,"text":"## Context\\nWe need to decide whether to run PostgreSQL on the Puppetmaster itself or use\\nRDS from the off.\\nCurrently our Puppetmaster uses a local PostgreSQL instance as its database. In moving to AWS we have the opportunity to replace this with an RDS instance.\\nThis may remove a reasonable amount of puppet code used to configure the database and associated tasks (e.g. backups). It would also require an amount of Terraform work to provision the RDS instance and some updates to the Puppetmaster code to use that instance.\\n\n\n##Decision\nThe Puppetmaster will continue to use a local instance of PostgreSQL.\\n","Predictions":"Use Rollbar to collect and manage our application errors.n"}
{"File Name":"saas-platform-frontend\/0002-use-the-stack-from-react-the-complete-guide-to-keep-up-to-date-with-react.md","Context":"## Context\nI need to decided with which tools I build my react app.\n","Decision":"I build the react app using [`React - The Complete Guide (incl Hooks, React Router, Redux)`](https:\/\/www.udemy.com\/react-the-complete-guide-incl-redux\/).\\n","tokens":17,"id":3721,"text":"## Context\\nI need to decided with which tools I build my react app.\\n\n\n##Decision\nI build the react app using [`React - The Complete Guide (incl Hooks, React Router, Redux)`](https:\/\/www.udemy.com\/react-the-complete-guide-incl-redux\/).\\n","Predictions":"We will use [Status](https:\/\/github.com\/status\/status) to build the application.n"}
{"File Name":"ansible-docker\/choose_webserver.md","Context":"### Context: A webserver is a necessary requirment to complete the assignment\n* A webserver is needed to host the web application.\n* There is a PHP-apache bundle offered by docker (see Dockerfile in php directory), thus apache was chosen purely out of convenience.\n* I do not have much experience with Web Servers and there is a lot of documentation on apache. Again, convenience was a major factor in making the decision to use apache.\n### Decision: The change proposed to the current implementation is to add an apache web server to host the php web application created in the php directory\n### Consequences: No forseen consequences or drawbacks in using apache.\n","Decision":"### Consequences: No forseen consequences or drawbacks in using apache.\\n","tokens":139,"id":4833,"text":"### Context: A webserver is a necessary requirment to complete the assignment\\n* A webserver is needed to host the web application.\\n* There is a PHP-apache bundle offered by docker (see Dockerfile in php directory), thus apache was chosen purely out of convenience.\\n* I do not have much experience with Web Servers and there is a lot of documentation on apache. Again, convenience was a major factor in making the decision to use apache.\\n### Decision: The change proposed to the current implementation is to add an apache web server to host the php web application created in the php directory\\n### Consequences: No forseen consequences or drawbacks in using apache.\\n\n\n##Decision\n### Consequences: No forseen consequences or drawbacks in using apache.\\n","Predictions":"We decided to use [Security Object Validator](https:\/\/github.com\/SecurityObjectValidator) as our validation library.n"}
{"File Name":"celestia-core\/adr-005-decouple-blockid-and-partsetheader.md","Context":"## Context\nCelestia has multiple commits to the block data via the `DataHash` and the `PartSetHeader` in the `BlockID`. As stated in the [#184](https:\/\/github.com\/celestiaorg\/lazyledger-core\/issues\/184), we no longer need the `PartSetHeader` for this additional commitment to the block's data. However, we are still planning to use the `PartSetHeader` for block propagation during consensus in the short-medium term. This means that we will remove the `PartSetHeader` from as many places as possible, but keep it in the `Proposal` struct.\n","Decision":"While we build other better designs to experiment with, we will continue to implement the design specified here as it is not orthogonal. https:\/\/github.com\/celestiaorg\/lazyledger-core\/pull\/434#issuecomment-869158788\\n","tokens":132,"id":4152,"text":"## Context\\nCelestia has multiple commits to the block data via the `DataHash` and the `PartSetHeader` in the `BlockID`. As stated in the [#184](https:\/\/github.com\/celestiaorg\/lazyledger-core\/issues\/184), we no longer need the `PartSetHeader` for this additional commitment to the block's data. However, we are still planning to use the `PartSetHeader` for block propagation during consensus in the short-medium term. This means that we will remove the `PartSetHeader` from as many places as possible, but keep it in the `Proposal` struct.\\n\n\n##Decision\nWhile we build other better designs to experiment with, we will continue to implement the design specified here as it is not orthogonal. https:\/\/github.com\/celestiaorg\/lazyledger-core\/pull\/434#issuecomment-869158788\\n","Predictions":"We will use the Proposal struct for the Proposal struct.n"}
{"File Name":"adr\/0002-SYS-rdbms.md","Context":"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4616,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","Predictions":"The change that we're proposing or have agreed to implement.n"}
{"File Name":"dogma\/0012-identifier-comparison.md","Context":"## Context\nIdentifiers (the names and keys used to identify applications and handlers) must\nbe compared by engines to determine if two such entities are to be considered\nequivalent.\nThe documentation specifies that such keys must be non-empty UTF-8 strings\nconsisting of printable characters without whitespace, but it did not previously\nspecify how such strings would be compared.\nThese identifiers are either mostly or entirely immutable and generated as part\nof the source code. They do not need to be parsed and validated from user input.\n","Decision":"In keeping with current behavior, we've decided to specify byte-wise comparison\\nsemantics for identifiers.\\n","tokens":109,"id":1618,"text":"## Context\\nIdentifiers (the names and keys used to identify applications and handlers) must\\nbe compared by engines to determine if two such entities are to be considered\\nequivalent.\\nThe documentation specifies that such keys must be non-empty UTF-8 strings\\nconsisting of printable characters without whitespace, but it did not previously\\nspecify how such strings would be compared.\\nThese identifiers are either mostly or entirely immutable and generated as part\\nof the source code. They do not need to be parsed and validated from user input.\\n\n\n##Decision\nIn keeping with current behavior, we've decided to specify byte-wise comparison\\nsemantics for identifiers.\\n","Predictions":"We will use a type field to identify the type of a service.n"}
{"File Name":"road-registry\/006-provisional-changes.md","Context":"## Context\nAs a result of running the feature-compare process a bunch of files are produced. These files are essentially a _diff_ between what the operator started out with, that is the dump \/ extract he \/ she started working from, and what they ended up with, after having edited shapes and associated data in their editor. Some of those files have a `_ALL.DBF|_ALL.SHP` suffix. These files are the files we base our logic on to validate and translate to an _internal_ change request. Most notably, these files have an extra column called `RECORDTYPE` which, as the name suggests, classifies a record as either being `IDENTICAL`, `ADDED`, `MODIFIED` or `REMOVED`. Next to that most of the files contain, per record, the equivalent of a primary key that identifies a row. This primary key is used in other files as a foreign key to reference a record sitting in another file. Records with a `RECORDTYPE` of `ADDED` will use a temporary identifier (a really big number that we assume is free to use) to make this work (see [004-temporary-and-permanent-identifiers.md](004-temporary-and-permanent-identifiers.md) for how that works). One gotcha is that the primary keys are not always unique, that is, they can appear multiple times in the `*_ALL.DBF|*_ALL.SHP` files, once for each record type. A common scenario is a modification represented as a removal and an addition record.\nFor the `WEGSEGMENT_ALL.DBF` file, things are more complicated ... next to having a `WS_OIDN` column act as primary key it has a `EVENTIDN` column acting as an alternative primary key in some cases. In case the `RECORDTYPE` is `ADDED` and the `EVENTIDN` has a value differing from `0`, the `WS_OIDN` column refers to an existing road segment and the `EVENTIDN` column refers to its new representation. In such a case, other files refer to a road segment by the value found in the `EVENTIDN`, not by the value in `WS_OIDN`. Alas, such is life ...\n","Decision":"Modifying a road segment involves data from `WEGSEGMENT_ALL.DBF`, `WEGSEGMENT_ALL.SHP` and `ATTRIJSTROKEN_ALL.DBF`, `ATTWEGBREEDTE_ALL.DBF`, `ATTWEGVERHARDING_ALL.DBF` - that is, when it is represented as an _internal_ change request command. Each of those `.DBF` files contains a `RECORDTYPE` column. As such, a road segment could be marked as identical in `WEGSEGMENT_ALL.DB`, yet it's lanes, width and \/ or surfaces could be marked as a mixture of modified, removed, added, identical. This is the reason why a road segment that is identical is appended as a `provisional` change to the list of translated changes. Because we're not sure, just yet, that it is an actual change. The order in which these files are translated causes a `provisional` change, if warranted, to be promoted to an actual change.\\nWhy go thru all this trouble? Well, lanes, widths, and surfaces are tightly coupled to the geometry of a road segment such that it makes sense to capture them as a holistic change rather than as individual fragmented changes. There's still a bigger debate to be had about which pieces of data change together ...\\n","tokens":467,"id":4931,"text":"## Context\\nAs a result of running the feature-compare process a bunch of files are produced. These files are essentially a _diff_ between what the operator started out with, that is the dump \/ extract he \/ she started working from, and what they ended up with, after having edited shapes and associated data in their editor. Some of those files have a `_ALL.DBF|_ALL.SHP` suffix. These files are the files we base our logic on to validate and translate to an _internal_ change request. Most notably, these files have an extra column called `RECORDTYPE` which, as the name suggests, classifies a record as either being `IDENTICAL`, `ADDED`, `MODIFIED` or `REMOVED`. Next to that most of the files contain, per record, the equivalent of a primary key that identifies a row. This primary key is used in other files as a foreign key to reference a record sitting in another file. Records with a `RECORDTYPE` of `ADDED` will use a temporary identifier (a really big number that we assume is free to use) to make this work (see [004-temporary-and-permanent-identifiers.md](004-temporary-and-permanent-identifiers.md) for how that works). One gotcha is that the primary keys are not always unique, that is, they can appear multiple times in the `*_ALL.DBF|*_ALL.SHP` files, once for each record type. A common scenario is a modification represented as a removal and an addition record.\\nFor the `WEGSEGMENT_ALL.DBF` file, things are more complicated ... next to having a `WS_OIDN` column act as primary key it has a `EVENTIDN` column acting as an alternative primary key in some cases. In case the `RECORDTYPE` is `ADDED` and the `EVENTIDN` has a value differing from `0`, the `WS_OIDN` column refers to an existing road segment and the `EVENTIDN` column refers to its new representation. In such a case, other files refer to a road segment by the value found in the `EVENTIDN`, not by the value in `WS_OIDN`. Alas, such is life ...\\n\n\n##Decision\nModifying a road segment involves data from `WEGSEGMENT_ALL.DBF`, `WEGSEGMENT_ALL.SHP` and `ATTRIJSTROKEN_ALL.DBF`, `ATTWEGBREEDTE_ALL.DBF`, `ATTWEGVERHARDING_ALL.DBF` - that is, when it is represented as an _internal_ change request command. Each of those `.DBF` files contains a `RECORDTYPE` column. As such, a road segment could be marked as identical in `WEGSEGMENT_ALL.DB`, yet it's lanes, width and \/ or surfaces could be marked as a mixture of modified, removed, added, identical. This is the reason why a road segment that is identical is appended as a `provisional` change to the list of translated changes. Because we're not sure, just yet, that it is an actual change. The order in which these files are translated causes a `provisional` change, if warranted, to be promoted to an actual change.\\nWhy go thru all this trouble? Well, lanes, widths, and surfaces are tightly coupled to the geometry of a road segment such that it makes sense to capture them as a holistic change rather than as individual fragmented changes. There's still a bigger debate to be had about which pieces of data change together ...\\n","Predictions":"We will use the SecurityType field to determine the _type_ of the file. This will be used to determine the _type_ of the file.n"}
{"File Name":"sepa-customer-platform\/0004-authenticate-backstage-users-azure-ad.md","Context":"## Context\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage\/SEPA users).\n## Decision Drivers\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\n","Decision":"SEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n[Option 1] SEPA should look to integrate Azure Active Directory with Dynamics to provide a shared identity 'federation', where backstage (SEPA) users are able to access Dynamics data via the WebAPI & perform CRUD operations using their existing single sign on login.\\nDynamics users would therefore match the users within the Active Directory, and SEPA would require a Dynamics license for each SEPA user.\\nIf SEPA's Active Directory is managed on premises, Azure AD connect can be used to auto sync existing Active Directory accounts to the Azure setup.\\n### Positive Consequences\\n* Reuse of existing SEPA logins\\n* Better backstage user experience\\n### Negative Consequences\\n* If Azure Active Directory connect is used to sync accounts, this would need to be monitored & managed by SEPA.\\n","tokens":90,"id":5157,"text":"## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage\/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n\n\n##Decision\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n[Option 1] SEPA should look to integrate Azure Active Directory with Dynamics to provide a shared identity 'federation', where backstage (SEPA) users are able to access Dynamics data via the WebAPI & perform CRUD operations using their existing single sign on login.\\nDynamics users would therefore match the users within the Active Directory, and SEPA would require a Dynamics license for each SEPA user.\\nIf SEPA's Active Directory is managed on premises, Azure AD connect can be used to auto sync existing Active Directory accounts to the Azure setup.\\n### Positive Consequences\\n* Reuse of existing SEPA logins\\n* Better backstage user experience\\n### Negative Consequences\\n* If Azure Active Directory connect is used to sync accounts, this would need to be monitored & managed by SEPA.\\n","Predictions":"* The Azure Blob storage system should be able to display links to Azure files for administrators.n* The Azure Blob storage system should be able to display links to Azure files for administrators.n* The Azure Blob storage system should be able to display links to Azure files for administrators.n* The Azure Blob storage system should be able to display links to Azure files for administrators.n"}
{"File Name":"pomi-data-etl\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":2487,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"claim-additional-payments-for-teaching\/0003-use-rubocop-for-linting.md","Context":"## Context\nWe need to lint our Ruby code, the team has previous experience working with\nRuboCop.\n","Decision":"We will use RuboCop for linting our Ruby code\\n","tokens":24,"id":2101,"text":"## Context\\nWe need to lint our Ruby code, the team has previous experience working with\\nRuboCop.\\n\n\n##Decision\nWe will use RuboCop for linting our Ruby code\\n","Predictions":"We will use state to store the current validator set.n"}
{"File Name":"opg-metrics\/0002-aws-infrastructure.md","Context":"## Context\nBased on ADRs and developer need, we need to create a system that is light weight, fully managed and to integrate into.\n","Decision":"To run in a fully managed AWS cloud environment using Terraform to manage it.\\n","tokens":31,"id":1545,"text":"## Context\\nBased on ADRs and developer need, we need to create a system that is light weight, fully managed and to integrate into.\\n\n\n##Decision\nTo run in a fully managed AWS cloud environment using Terraform to manage it.\\n","Predictions":"We will use Terraform to manage the system.n"}
{"File Name":"mediawiki-extensions-Wikispeech\/0001-store-synthesized-audio-in-mediawiki-extension.md","Context":"## Context\nThe original implementation of Wikispeech stored the synthesized audio\nas files in a folder within the Speechoid service (in the\nwikispeech-server sub-service). The paths to these files, together\nwith the related metadata were then passed on as a response to the\nMediaWiki extension.\nThis implementation had a few identified drawbacks: Wikimedia\ninfrastructure expects files to be stored in [Swift] rather than as\nfiles on disk, supporting this would require implementing Swift\nstorage in the Speechoid service.  There is a desire to keep the\nSpeechoid service stateless, persistent storage of synthesized files\nwithin the service runs counter to this.  The utterance metadata was\nnot stored, requiring that each sentence always be re-synthesized\nunless cached together with the file path.\nWhile Wikimedia requires Swift many other MediaWiki installations\nmight not be interested in that. It is therefore important with a\nsolution where the file storage backend can be changed as desired\nthrough the configs.\nDue to [RevisionDelete] none of the content (words) of any segment\nanywhere should be stored anywhere, e.g. in a table, since these must\nthen not be publicly queryable, and to include mechanisms preventing\nnon-public segments from being synthesized.\nWe have an interest in storing the utterance audio for a long time to\navoid the expensive operation of synthesizing segments on demand, but\nwe still want a mechanism that flush stored utterances after a given\nperiod of time. If a user makes a change to a text segment, it is\nunlikely that the previous revision of that segment is used in another\narticle and could thus be instantly flushed. There is also the case\nwhere we want to flush to trigger re-synthesizing segments when a word\nis added to or updated in the phonetic lexicon, as that would improve\nthe resulting synthesized speech.\nRe-use of utterance audio across a site (or many sites) is desirable,\nbut likely to be rare (largely limited to headings and shorter\nphrases). What will likely be more common is re-use of utterance audio\nacross multiple revisions of the same page. If a single segment is\nedited then all other segments, and their corresponding audio, remain\nvalid. For this reason utterance audio should not be tied to a given\npage or revision.\n","Decision":"Files are only temporarily stored within Speechoid.\\nWhen a segment is synthesized, or when the audio is retrieved, a check\\nmust be performed to ensure it corresponds to a page revision which\\nhas not been suppressed through RevisionDelete. A segment is\\nrepresented by a hash to satisfy RevisionDelete requirements on public\\ntables. The segment hash should only be constructed from its\\ncontents. For the sake of RevisionDelete the link to the synthesised\\naudio should never be exposed to the end user.\\nThe MediaWiki extension parses the response from Speechoid, fetches\\nthe synthesized audio and stores this as a file using the provided\\n[FileBackend] functionality. The corresponding utterance metadata is\\nstored as a JSON file. Both files share the same base filename.\\nAn expiry date is attached to each stored Speechoid response to allow\\nlexicon updates to propagate and for the flushing of outdated\\nsegments.\\n","tokens":488,"id":3351,"text":"## Context\\nThe original implementation of Wikispeech stored the synthesized audio\\nas files in a folder within the Speechoid service (in the\\nwikispeech-server sub-service). The paths to these files, together\\nwith the related metadata were then passed on as a response to the\\nMediaWiki extension.\\nThis implementation had a few identified drawbacks: Wikimedia\\ninfrastructure expects files to be stored in [Swift] rather than as\\nfiles on disk, supporting this would require implementing Swift\\nstorage in the Speechoid service.  There is a desire to keep the\\nSpeechoid service stateless, persistent storage of synthesized files\\nwithin the service runs counter to this.  The utterance metadata was\\nnot stored, requiring that each sentence always be re-synthesized\\nunless cached together with the file path.\\nWhile Wikimedia requires Swift many other MediaWiki installations\\nmight not be interested in that. It is therefore important with a\\nsolution where the file storage backend can be changed as desired\\nthrough the configs.\\nDue to [RevisionDelete] none of the content (words) of any segment\\nanywhere should be stored anywhere, e.g. in a table, since these must\\nthen not be publicly queryable, and to include mechanisms preventing\\nnon-public segments from being synthesized.\\nWe have an interest in storing the utterance audio for a long time to\\navoid the expensive operation of synthesizing segments on demand, but\\nwe still want a mechanism that flush stored utterances after a given\\nperiod of time. If a user makes a change to a text segment, it is\\nunlikely that the previous revision of that segment is used in another\\narticle and could thus be instantly flushed. There is also the case\\nwhere we want to flush to trigger re-synthesizing segments when a word\\nis added to or updated in the phonetic lexicon, as that would improve\\nthe resulting synthesized speech.\\nRe-use of utterance audio across a site (or many sites) is desirable,\\nbut likely to be rare (largely limited to headings and shorter\\nphrases). What will likely be more common is re-use of utterance audio\\nacross multiple revisions of the same page. If a single segment is\\nedited then all other segments, and their corresponding audio, remain\\nvalid. For this reason utterance audio should not be tied to a given\\npage or revision.\\n\n\n##Decision\nFiles are only temporarily stored within Speechoid.\\nWhen a segment is synthesized, or when the audio is retrieved, a check\\nmust be performed to ensure it corresponds to a page revision which\\nhas not been suppressed through RevisionDelete. A segment is\\nrepresented by a hash to satisfy RevisionDelete requirements on public\\ntables. The segment hash should only be constructed from its\\ncontents. For the sake of RevisionDelete the link to the synthesised\\naudio should never be exposed to the end user.\\nThe MediaWiki extension parses the response from Speechoid, fetches\\nthe synthesized audio and stores this as a file using the provided\\n[FileBackend] functionality. The corresponding utterance metadata is\\nstored as a JSON file. Both files share the same base filename.\\nAn expiry date is attached to each stored Speechoid response to allow\\nlexicon updates to propagate and for the flushing of outdated\\nsegments.\\n","Predictions":"We have decided to use [Swift](https:\/\/swift.io\/) as the backend for thensynthesized audio files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files.nWe have decided to use [RevisionDelete](https:\/\/swift.io\/) as the backend for thensynthesized files"}
{"File Name":"event-routing-backends\/0006-versioning-of-event-transformers.rst","Context":"Context\n-------\nEvent transformers may undergo modification in future in response to consumer request, change in specification, bug fixes etc.\nDecision\n--------\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\n#. Transformer version number will be the semantic version of the event-routing-backends package.\n#. The event-routing-backends major version will be incremented when:\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\n#. A key is removed from or renamed in the existing transformer.\n#. Value of a key is updated in the existing transformer.\n#. The event-routing-backends minor version will be incremented when:\n#. A key is added to an existing transformer.\n#. A new event is added for transformation.\n#. Minor version (Z) will be incremented when:\n#. A bug is fixed.\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\nChangelog\n---------\n- Updated 2023-02-28 to change the format of the transformer version.\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\n","Decision":"--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n","tokens":398,"id":4483,"text":"Context\\n-------\\nEvent transformers may undergo modification in future in response to consumer request, change in specification, bug fixes etc.\\nDecision\\n--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n\n\n##Decision\n--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n","Predictions":"--------nEvent transformers will be maintained and emitted as part of xAPI and Caliper events.n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in context [ extensions [ https:\/\/w3id.org\/xapi\/openedx\/extension\/transformer-version ] ] for xAPI statement and in extensions [ transformerVersion ] for Caliper event.n#. The transformer version number will be the semantic version of the event-routing-backends package.n#. The \"transformer version\" will be incremented when:n#. Transformer is changed due to update in original specification (xAPI or Caliper).n#. A key is removed from or renamed in the existing transformer.n#. Value of a key is updated in the existing transformer.n#. The event-routing-backends minor version will be incremented when:n#. A key is added to an existing transformer.n#. A new event is added for transformation.n#. Minor version (Z) will be incremented when:n#. A bug is fixed.n#. Change logs of transformers will be maintained for both xAPI and Caliper.nChangelogn---------n- Updated 2023-02-28 to change the format of the transformer version.n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event, the event-routing-backends event will be maintained for both xAPI and Caliper.nChangelogn---------n- Updated 2023-02-28 to change the format of the transformer version.n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event, the event-routing-backends event will be maintained for both xAPI and Caliper.nChangelogn---------n- Updated 2023-02-28 to change the format of the transformer version.n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event, the event-routing-backends event will be maintained for both xAPI and Caliper.nChangelogn---------n- Updated 2023-02-28 to change the format of the transformer version.n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event, the event"}
{"File Name":"runner\/0276-problem-matchers.md","Context":"# ADR 0276: Problem Matchers\n**Date** 2019-06-05\n**Status** Accepted\n## Context\nCompilation failures during a CI build should surface good error messages.\nFor example, the actual compile errors from the typescript compiler should bubble as issues in the UI. And not simply \"tsc exited with exit code 1\".\nVSCode has an extensible model for solving this type of problem. VSCode allows users to configure which [problems matchers](https:\/\/code.visualstudio.com\/docs\/editor\/tasks#_defining-a-problem-matcher) to use, when scanning output. For example, a user can apply the `tsc` problem matcher to receive a rich error output experience in VSCode, when compiling their typescript project.\nThe problem-matcher concept fits well with \"setup\" actions. For example, the `setup-nodejs` action will download node.js, add it to the PATH, and register the `tsc` problem matcher. For the duration of the job, the `tsc` problem matcher will be applied against the output.\n","Decision":"### Registration\\n#### Using `::` command\\n`::add-matcher::path-to-problem-matcher-config.json`\\nUsing a `::` command allows for flexibility:\\n- Ad hoc scripts can register problem matchers\\n- Allows problem matchers to be conditionally registered\\nNote, if a matcher with the same name is registered a second time, it will clobber the first instance.\\n","tokens":227,"id":3734,"text":"# ADR 0276: Problem Matchers\\n**Date** 2019-06-05\\n**Status** Accepted\\n## Context\\nCompilation failures during a CI build should surface good error messages.\\nFor example, the actual compile errors from the typescript compiler should bubble as issues in the UI. And not simply \"tsc exited with exit code 1\".\\nVSCode has an extensible model for solving this type of problem. VSCode allows users to configure which [problems matchers](https:\/\/code.visualstudio.com\/docs\/editor\/tasks#_defining-a-problem-matcher) to use, when scanning output. For example, a user can apply the `tsc` problem matcher to receive a rich error output experience in VSCode, when compiling their typescript project.\\nThe problem-matcher concept fits well with \"setup\" actions. For example, the `setup-nodejs` action will download node.js, add it to the PATH, and register the `tsc` problem matcher. For the duration of the job, the `tsc` problem matcher will be applied against the output.\\n\n\n##Decision\n### Registration\\n#### Using `::` command\\n`::add-matcher::path-to-problem-matcher-config.json`\\nUsing a `::` command allows for flexibility:\\n- Ad hoc scripts can register problem matchers\\n- Allows problem matchers to be conditionally registered\\nNote, if a matcher with the same name is registered a second time, it will clobber the first instance.\\n","Predictions":"* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Mapsn* ES6 Maps"}
{"File Name":"infection\/0004-PHPUnit-expect-exception-over-try-catch.md","Context":"### Context\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\n```php\nfunction test_something(): void {\n\/\/ ...\ntry {\n\/\/ the statement that fail\n$this->fail();\n} catch (Exception $e) {\n\/\/ ...\n}\n}\n```\nOr:\n```php\nfunction test_something(): void {\n\/\/ ...\n$this->expectException($exception)\n\/\/ the statement that fail\n}\n```\n### Decision\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\nA pull request to fix this practice in the whole codebase may be done but has not been made\nmandatory. New pull requests though should stick to this practice.\n### Status\nAccepted ([#1090][1090])\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\n","Decision":"As recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","tokens":285,"id":68,"text":"### Context\\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\ntry {\\n\/\/ the statement that fail\\n$this->fail();\\n} catch (Exception $e) {\\n\/\/ ...\\n}\\n}\\n```\\nOr:\\n```php\\nfunction test_something(): void {\\n\/\/ ...\\n$this->expectException($exception)\\n\/\/ the statement that fail\\n}\\n```\\n### Decision\\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n\n\n##Decision\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","Predictions":"* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_something()n* Exceptions are void in the test_something() and test_"}
{"File Name":"paas-team-manual\/ADR025-service-plan-naming-conventions.html.md","Context":"## Context\nOur service plans have evolved incrementally over the last few years and are in\nneed of some attention. Names are inconsistent, potentially confusing and\nin many cases contain irrelevant redundant information that is of no practical\nuse to the platform operators or to tenants consuming the service.\nAdding additional versions of services has the potential to compound the\nproblem by multiplying plans of different characteristics with different\nversions.\n","Decision":"We have decided to use the following naming convention for naming plans going forward:\\n```\\nSIZE[-HA][-LABEL,-LABEL,...]-VERSION\\n```\\nWhere:\\n* `SIZE` is a string describing the scale one of the plan, it should be one of: `xlarge` `large` `medium` `small` `tiny`.\\n* `HA` is the string `ha` to indicate highly available if relevent.\\n* `LABEL` is a string describing some specific variant of the service if relvent.\\n* `VERSION` is the version number of the service plan.\\n### For example:\\nA large multi-az postgres plan for version 9.6 would be `large-ha-9.6`.\\nA small not multi-az, unclustered, redis 3.2 plan would be `redis-unclustered-3.2`.\\n### Example migrations of some existing plans:\\n```\\nL-HA-enc-dedicated-9.5 => large-ha-9.5\\nM-HA-enc-dedicated-9.5 => medium-ha-9.5\\ntiny-clustered => tiny-clustered-3.2\\ntiny-unclustered => tiny-unclustered-3.2\\n```\\n### Additionally:\\n* We will avoid use of the word \"free\" in names.\\n* We will avoid using redundent words (like 'dedicated') in names to reduce noise.\\n* We will avoid use of uppercase characters in names.\\n* We will avoid abbriviations where possible (\"medium\" instead of \"M\", \"large\" instead of \"L\").\\n* We will avoid offering unencrypted plans where an encrypted version is available (legacy unencrypted plans will be explicitly labelled with `-unencrypted` and hidden from the marketplace).\\n","tokens":83,"id":200,"text":"## Context\\nOur service plans have evolved incrementally over the last few years and are in\\nneed of some attention. Names are inconsistent, potentially confusing and\\nin many cases contain irrelevant redundant information that is of no practical\\nuse to the platform operators or to tenants consuming the service.\\nAdding additional versions of services has the potential to compound the\\nproblem by multiplying plans of different characteristics with different\\nversions.\\n\n\n##Decision\nWe have decided to use the following naming convention for naming plans going forward:\\n```\\nSIZE[-HA][-LABEL,-LABEL,...]-VERSION\\n```\\nWhere:\\n* `SIZE` is a string describing the scale one of the plan, it should be one of: `xlarge` `large` `medium` `small` `tiny`.\\n* `HA` is the string `ha` to indicate highly available if relevent.\\n* `LABEL` is a string describing some specific variant of the service if relvent.\\n* `VERSION` is the version number of the service plan.\\n### For example:\\nA large multi-az postgres plan for version 9.6 would be `large-ha-9.6`.\\nA small not multi-az, unclustered, redis 3.2 plan would be `redis-unclustered-3.2`.\\n### Example migrations of some existing plans:\\n```\\nL-HA-enc-dedicated-9.5 => large-ha-9.5\\nM-HA-enc-dedicated-9.5 => medium-ha-9.5\\ntiny-clustered => tiny-clustered-3.2\\ntiny-unclustered => tiny-unclustered-3.2\\n```\\n### Additionally:\\n* We will avoid use of the word \"free\" in names.\\n* We will avoid using redundent words (like 'dedicated') in names to reduce noise.\\n* We will avoid use of uppercase characters in names.\\n* We will avoid abbriviations where possible (\"medium\" instead of \"M\", \"large\" instead of \"L\").\\n* We will avoid offering unencrypted plans where an encrypted version is available (legacy unencrypted plans will be explicitly labelled with `-unencrypted` and hidden from the marketplace).\\n","Predictions":"We have decided to use the following naming convention for naming plans going forward:nSIZE[-HA][-LABEL,-LAB,...]-VERSIONnnWhere:n* SIZE is a string describing the scale one of the plan, it should be one of: xlarge large medium small tiny.n* HA is the string ha to indicate highly available if pertinent.n* LABEL is a string describing some specific variant of the service if relvent.n* VERSION is the version number of the service plan.n### Additionally:n* We will avoid use of the word \"free\" in names.n* We will avoid using redundent words (like 'dedicated') in names to reduce noise.n* We will avoid use of uppercase characters in names.n* We will avoid abbriviations where possible (\"medium\" instead of \"free\")n* We will avoid use of uppercase characters in names.n* We will avoid abbriviations where possible (\"medium\" instead of \"free\")n"}
{"File Name":"beis-report-official-development-assistance\/0022-use-govuk-paas-for-hosting.md","Context":"## Context\nAs a government entity, BEIS want to host their service on [GOVUK\nPaaS](https:\/\/www.cloud.service.gov.uk\/). This was always the service teams\nintention.\n","Decision":"Host all environments excpet local development on BEIS own GOVUK PaaS account.\\n","tokens":44,"id":2410,"text":"## Context\\nAs a government entity, BEIS want to host their service on [GOVUK\\nPaaS](https:\/\/www.cloud.service.gov.uk\/). This was always the service teams\\nintention.\\n\n\n##Decision\nHost all environments excpet local development on BEIS own GOVUK PaaS account.\\n","Predictions":"We will use the [Security Store](https:\/\/docs.aws.amazon.com\/systems-manager\/latest\/userguide\/systems-manager-security-store.html) to store secrets securely.n"}
{"File Name":"site-stitcher\/001-generate-static-pages.md","Context":"## Context\nI see so many thing broken on the web today, it is not only website speed, it is also\nhow many tools we have accepted to use to solve a simple problem. Especially in the JS\nworld I feel the healthy \"How much do I need?\" question is asked rarely. We also teach\nnewcomers that it is ok to load 1GB of npm modules before we can actually build an empty page.\nIn this repo I try to do the simplest thing possible to generate my website.\nI am looking for dependencies that are as tiny as possible, some I just need to not use and\nwrite the needed code itself, this also makes me aware how much of a dependency I would have needed\nonly.\nAlso the site that I want to ship shall have all a11y, lighthouse, etc. scores as high as possible\ntherefore the site shall degrade gracefully be accessible and very light weight.\nI also want the simplest hosting setup and infrastructure possible, due to using github it might\nbe github-pages, which is just static pages.\n","Decision":"With static pages the above can be achieved. Easily.\\n","tokens":222,"id":1931,"text":"## Context\\nI see so many thing broken on the web today, it is not only website speed, it is also\\nhow many tools we have accepted to use to solve a simple problem. Especially in the JS\\nworld I feel the healthy \"How much do I need?\" question is asked rarely. We also teach\\nnewcomers that it is ok to load 1GB of npm modules before we can actually build an empty page.\\nIn this repo I try to do the simplest thing possible to generate my website.\\nI am looking for dependencies that are as tiny as possible, some I just need to not use and\\nwrite the needed code itself, this also makes me aware how much of a dependency I would have needed\\nonly.\\nAlso the site that I want to ship shall have all a11y, lighthouse, etc. scores as high as possible\\ntherefore the site shall degrade gracefully be accessible and very light weight.\\nI also want the simplest hosting setup and infrastructure possible, due to using github it might\\nbe github-pages, which is just static pages.\\n\n\n##Decision\nWith static pages the above can be achieved. Easily.\\n","Predictions":"We will use a resources folder in the apps directory.n"}
{"File Name":"digital-paper-edit-client\/2019-04-29-SQL-database-schema.md","Context":"## Context and Problem Statement\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\nEg have 3 version of the schema for the 3 main milestones:\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\n## Decision Drivers <!-- optional -->\n* easy to reason around\n* \u2026 <!-- numbers of drivers can vary -->\n","Decision":"* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","tokens":248,"id":4167,"text":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","Predictions":"* easy to reason aroundn*...!-- numbers of drivers can vary -->n!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force |... | comes out best (see below)]. -->n_TBC_n### Positive Consequences!-- optional -->n!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required,...] -->n*...n_TBC_n### Negative consequences !-- optional -->n!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force |... | comes out best (see below)]. -->n_TBC_n### Negative consequences !-- optional -->n!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required,...] -->n*...n_TBC_n### Positive consequences!-- optional -->n!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required,...] -->n_TBC_n### Negative consequences !--"}
{"File Name":"ReportMI-service-manual\/0006-use-terraform-to-create-and-document-infrastructure.md","Context":"## Context\nRunning a service like the Data Submission Service requires infrastructure of\nvarious kinds.\nIt's good practice to manage this infrastructure using code (Infrastructure as\nCode), as this allows the infrastructure to be version-controlled and managed\nlike the rest of the application code.\nThere are various mechanisms for doing this. Each of the main cloud providers\nhave their own solutions to manage infrastructure in code, for example Amazon\nWeb Services (AWS) has CloudFormation, Microsoft Azure has Resource Manager etc.\nHowever each of these are specific to the individual cloud provider.\nIt would also be possible to do this manually by running scripts against the\ncloud provider API. However, this would take a significant amount of time and\nwould take effort to make it work across more than one cloud provider.\nThere are tools available, like Terraform, which allow you to define\ninfrastructure as code, in a standard way which can then be applied against more\nthan one cloud provider. The tool handles the differences between providers.\n","Decision":"We will use Terraform to create the infrastructure for the Data Submission\\nService.\\n","tokens":208,"id":2062,"text":"## Context\\nRunning a service like the Data Submission Service requires infrastructure of\\nvarious kinds.\\nIt's good practice to manage this infrastructure using code (Infrastructure as\\nCode), as this allows the infrastructure to be version-controlled and managed\\nlike the rest of the application code.\\nThere are various mechanisms for doing this. Each of the main cloud providers\\nhave their own solutions to manage infrastructure in code, for example Amazon\\nWeb Services (AWS) has CloudFormation, Microsoft Azure has Resource Manager etc.\\nHowever each of these are specific to the individual cloud provider.\\nIt would also be possible to do this manually by running scripts against the\\ncloud provider API. However, this would take a significant amount of time and\\nwould take effort to make it work across more than one cloud provider.\\nThere are tools available, like Terraform, which allow you to define\\ninfrastructure as code, in a standard way which can then be applied against more\\nthan one cloud provider. The tool handles the differences between providers.\\n\n\n##Decision\nWe will use Terraform to create the infrastructure for the Data Submission\\nService.\\n","Predictions":"We will use Terraform to create the infrastructure for the Data SubmissionnService.n"}
{"File Name":"CAFE5\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1751,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"opg-data\/0005-content-structure.md","Context":"## Context\nA consistent and well-defined document specification is required so that we may develop an API contract\n","Decision":"Our structure closely follows the [JSON-API](https:\/\/jsonapi.org\/format\/#document-structure) document structure\\n### Document Root Level\\nAt the root level is always a JSON object.\\nThis **MUST** contain at least one of the following root-level members:\\n* data: the document's \"primary data\" resource object\\n* A single resource object is represented by a JSON object\\n* A collection or resource objects is represented by an array of objects\\n* errors: an array of error objects\\n#### Single resource object\\n```json\\n{\\n\"data\": {},\\n\"meta\": {}\\n}\\n```\\n#### Collection of resource objects\\n```json\\n{\\n\"data\": [\\n{...},\\n{...}\\n],\\n\"errors\": [],\\n\"meta\": {},\\n\"links\": {}\\n}\\n```\\nJSON-API states\\n> The members data and errors MUST NOT coexist in the same document.\\n`opg-data` standard is that if a data resource OBJECT is returned, then there **MUST be no** error member.\\nHowever there ARE certain circumstances where an array of errors **MAY** be returned alongside a data resource COLLECTION. See [0007-error-handling-and-status-codes.md#errors-in-20x](0007-error-handling-and-status-codes.md#errors-in-20x)\\nThe root JSON object **MAY** also contain the following root-level members:\\n* meta: a meta object that contains non-standard meta-information\\n* links: a links object related to the primary data (typically used for pagination links if the data returned is a collection)\\n### The Resource Object\\nSee [JSON-AP](https:\/\/jsonapi.org\/format\/#document-resource-objects)\\nNamely:\\n* As a minimum, every resource object **MUST** contain:\\n* an id member\\n* a type member. The values of the id and type members **MUST** be strings. For consistency and the avoidance of confusion, types **MUST** use PLURAL. eg \"articles\", \"people\"\\n* an array of attributes (even if empty)\\n* IF the \"data\" top level member is present, then there **MUST** be a `links` array, containing as it's minimum, a `self` member with a URL which **MUST** be callable at the API\\n* A resource object's data attributes is presented in an array named \"attributes\"\\n* A resource object's links is presented in an object named \"links\"\\n* A resource object's relationships is presented in an array named \"relationships\"\\n* A resource object's attributes and its relationships are collectively called its \"fields\"\\n```json\\n{\\n\"data\": [\\n{\\n\"type\": \"articles\",\\n\"id\": \"1\",\\n\"attributes\": {\\n\"title\": \"My First Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/1\",\\n\"next\": \"https:\/\/api.example.com\/articles\/2\"\\n},\\n\"relationships\": {\\n...\\n}\\n},\\n{\\n\"type\": \"articles\",\\n\"id\": \"2\",\\n\"attributes\": {\\n\"title\": \"Second Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/2\",\\n\"prev\": \"https:\/\/api.example.com\/articles\/1\"\\n},\\n\"relationships\": {\\n...\\n}\\n}\\n],\\n\"links\": {},\\n\"meta\": {}\\n}\\n```\\n#### Relationships\\nRelationships will be defined in a separate ADR\\n### Meta\\nSee [JSON-API](https:\/\/jsonapi.org\/format\/#document-meta)\\n### Errors\\nErrors responses will be defined in a separate ADR\\n","tokens":21,"id":2181,"text":"## Context\\nA consistent and well-defined document specification is required so that we may develop an API contract\\n\n\n##Decision\nOur structure closely follows the [JSON-API](https:\/\/jsonapi.org\/format\/#document-structure) document structure\\n### Document Root Level\\nAt the root level is always a JSON object.\\nThis **MUST** contain at least one of the following root-level members:\\n* data: the document's \"primary data\" resource object\\n* A single resource object is represented by a JSON object\\n* A collection or resource objects is represented by an array of objects\\n* errors: an array of error objects\\n#### Single resource object\\n```json\\n{\\n\"data\": {},\\n\"meta\": {}\\n}\\n```\\n#### Collection of resource objects\\n```json\\n{\\n\"data\": [\\n{...},\\n{...}\\n],\\n\"errors\": [],\\n\"meta\": {},\\n\"links\": {}\\n}\\n```\\nJSON-API states\\n> The members data and errors MUST NOT coexist in the same document.\\n`opg-data` standard is that if a data resource OBJECT is returned, then there **MUST be no** error member.\\nHowever there ARE certain circumstances where an array of errors **MAY** be returned alongside a data resource COLLECTION. See [0007-error-handling-and-status-codes.md#errors-in-20x](0007-error-handling-and-status-codes.md#errors-in-20x)\\nThe root JSON object **MAY** also contain the following root-level members:\\n* meta: a meta object that contains non-standard meta-information\\n* links: a links object related to the primary data (typically used for pagination links if the data returned is a collection)\\n### The Resource Object\\nSee [JSON-AP](https:\/\/jsonapi.org\/format\/#document-resource-objects)\\nNamely:\\n* As a minimum, every resource object **MUST** contain:\\n* an id member\\n* a type member. The values of the id and type members **MUST** be strings. For consistency and the avoidance of confusion, types **MUST** use PLURAL. eg \"articles\", \"people\"\\n* an array of attributes (even if empty)\\n* IF the \"data\" top level member is present, then there **MUST** be a `links` array, containing as it's minimum, a `self` member with a URL which **MUST** be callable at the API\\n* A resource object's data attributes is presented in an array named \"attributes\"\\n* A resource object's links is presented in an object named \"links\"\\n* A resource object's relationships is presented in an array named \"relationships\"\\n* A resource object's attributes and its relationships are collectively called its \"fields\"\\n```json\\n{\\n\"data\": [\\n{\\n\"type\": \"articles\",\\n\"id\": \"1\",\\n\"attributes\": {\\n\"title\": \"My First Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/1\",\\n\"next\": \"https:\/\/api.example.com\/articles\/2\"\\n},\\n\"relationships\": {\\n...\\n}\\n},\\n{\\n\"type\": \"articles\",\\n\"id\": \"2\",\\n\"attributes\": {\\n\"title\": \"Second Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https:\/\/api.example.com\/articles\/2\",\\n\"prev\": \"https:\/\/api.example.com\/articles\/1\"\\n},\\n\"relationships\": {\\n...\\n}\\n}\\n],\\n\"links\": {},\\n\"meta\": {}\\n}\\n```\\n#### Relationships\\nRelationships will be defined in a separate ADR\\n### Meta\\nSee [JSON-API](https:\/\/jsonapi.org\/format\/#document-meta)\\n### Errors\\nErrors responses will be defined in a separate ADR\\n","Predictions":"We will use the [Semantic Data Model](https:\/\/semantic-datamodel.org\/) model to model the data model.n"}
{"File Name":"otm-docs\/api_framework.md","Context":"## Context\nUsing pure PHP is kinda an uphill battle. Frameworks deliver ready-made solutions. Choosing proper framework ensures smooth workflow.\n","Decision":"Laravel 7\\n","tokens":27,"id":3062,"text":"## Context\\nUsing pure PHP is kinda an uphill battle. Frameworks deliver ready-made solutions. Choosing proper framework ensures smooth workflow.\\n\n\n##Decision\nLaravel 7\\n","Predictions":"We will use stack to create a new stack component.n"}
{"File Name":"clone_difitalcitizenship\/0002-use-openapi-to-defined-the-api-specs.md","Context":"## Context\nWe need to define the API specifications of the services we're going to implement.\n","Decision":"We use the [OpenAPI 2.0](https:\/\/swagger.io\/specification\/) specification (aka Swagger spec) as standard for our REST API definitions.\\n","tokens":20,"id":1180,"text":"## Context\\nWe need to define the API specifications of the services we're going to implement.\\n\n\n##Decision\nWe use the [OpenAPI 2.0](https:\/\/swagger.io\/specification\/) specification (aka Swagger spec) as standard for our REST API definitions.\\n","Predictions":"We will use the [OpenAPI 2.0](https:\/\/swagger.io\/specification\/) specification (aka Swagger spec) as standard for our REST API definitions.n"}
{"File Name":"easyvista_toolkit\/0001_Using_globalVariable.md","Context":"## Context\nWe want to be able to have autocompletion in the cmdlet to make them more user friendly.\nBecause the easyvista rest API relies on GUID for some parameters we'll need to be able to query their friendly name within in the cmdlet parameters.\nWe tried using environment variables but they don't seem to work with hashtable (we did not check extensively).\n","Decision":"We will use global variables (named *$Global:EZVvariablename*) set by a dedicated cmdlet (*set-EZVcontext*). That cmdlet will define an execution context for all other cmdlet in the project.\\n","tokens":79,"id":2329,"text":"## Context\\nWe want to be able to have autocompletion in the cmdlet to make them more user friendly.\\nBecause the easyvista rest API relies on GUID for some parameters we'll need to be able to query their friendly name within in the cmdlet parameters.\\nWe tried using environment variables but they don't seem to work with hashtable (we did not check extensively).\\n\n\n##Decision\nWe will use global variables (named *$Global:EZVvariablename*) set by a dedicated cmdlet (*set-EZVcontext*). That cmdlet will define an execution context for all other cmdlet in the project.\\n","Predictions":"Use Rollbar to collect and manage our application errors.n"}
{"File Name":"verify-service-provider\/0013-we-will-write-acceptance-tests-against-compliance-tool.md","Context":"## Context\nVerify provide a \"compliance tool\" which allows relying parties to check that their\nimplementations are compatible with the SAML profile of Verify.\nCurrently relying parties must test their service against the compliance tool as part\nof the onboarding process.\nBy writing tests against the compliance tool we can:\n* make sure our own implementation is compliant\n* demonstrate to relying parties how to write tests against the compliance tool\n* learn more about the user experience of using the compliance tool\n","Decision":"We'll write acceptance tests against the compliance tool for the verify service provider\\nand the stub relying parties.\\nTests for the service provider will give us direct feedback on whether it's compliant.\\nTests for the stub relying parties will give us confidence they work end-to-end and should\\nprovide a template for real relying parties to write their own tests.\\n","tokens":101,"id":4442,"text":"## Context\\nVerify provide a \"compliance tool\" which allows relying parties to check that their\\nimplementations are compatible with the SAML profile of Verify.\\nCurrently relying parties must test their service against the compliance tool as part\\nof the onboarding process.\\nBy writing tests against the compliance tool we can:\\n* make sure our own implementation is compliant\\n* demonstrate to relying parties how to write tests against the compliance tool\\n* learn more about the user experience of using the compliance tool\\n\n\n##Decision\nWe'll write acceptance tests against the compliance tool for the verify service provider\\nand the stub relying parties.\\nTests for the service provider will give us direct feedback on whether it's compliant.\\nTests for the stub relying parties will give us confidence they work end-to-end and should\\nprovide a template for real relying parties to write their own tests.\\n","Predictions":"We'll write acceptance tests against the compliance tool for the verify service providernand the stub relying parties.nTests for the service provider will give us direct feedback on whether it's compliant.nTests for the stub relying parties will give us confidence they work end-to-end and shouldnprovide a template for real relying parties to write their own tests.n"}
{"File Name":"bookit-api\/0006-use-okta-as-identity-provider.md","Context":"## Context\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\nLater on, we could add additional social logins (or support AD directly if needed).\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\n","Decision":"* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","tokens":279,"id":3217,"text":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n\n\n##Decision\n* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","Predictions":"* Delay for now - because we think we still need a local user table, we can add this in latern* Use OKta as IdP for Bookitn* Use OKta's React client for client side coden* Use OKta's Spring Boot 1.x starter forn* Use OKta Access Tokens as Bearer tokensn"}
{"File Name":"uniprot-rest-api\/0003-spring-framework.md","Context":"## Context\nProgramming frameworks can promote the productivity of a project; producing smaller code bases, added reliability,\nadditional features (than one would otherwise write themselves), etc.\n","Decision":"We have used the [Spring framework](https:\/\/spring.io\/) within the team for a number of years, and its recent advances in the domain of REST applications makes it an ideal choice.\\n","tokens":35,"id":1477,"text":"## Context\\nProgramming frameworks can promote the productivity of a project; producing smaller code bases, added reliability,\\nadditional features (than one would otherwise write themselves), etc.\\n\n\n##Decision\nWe have used the [Spring framework](https:\/\/spring.io\/) within the team for a number of years, and its recent advances in the domain of REST applications makes it an ideal choice.\\n","Predictions":"We have used the [Spring framework](https:\/\/spring.io\/) within the team for a number of years, and its recent advances in the domain of REST applications makes it an ideal choice.n"}
{"File Name":"beis-report-official-development-assistance\/0025-use-terraform-to-set-up-and-deploy-to-the-paas.md","Context":"## Context\nWe like to control our infrastructure with code. On a previous project we used shell scripts to set up the spaces and services within a PaaS organisation and to also do deploys from Travis.\nWhile shell scripts are fine there is now a useful [terraform provider](https:\/\/github.com\/cloudfoundry-community\/terraform-provider-cf) for cloudfoundry.\nThe cloudfoundry provider also deploys the app.\nTerraform is the default choice for provisioning infrastructure these days.\n","Decision":"- spaces and services should be created with Terraform\\n- the application should be deployed with Terraform\\n- Terraform should be run from Travis to deploy the application\\n","tokens":107,"id":2409,"text":"## Context\\nWe like to control our infrastructure with code. On a previous project we used shell scripts to set up the spaces and services within a PaaS organisation and to also do deploys from Travis.\\nWhile shell scripts are fine there is now a useful [terraform provider](https:\/\/github.com\/cloudfoundry-community\/terraform-provider-cf) for cloudfoundry.\\nThe cloudfoundry provider also deploys the app.\\nTerraform is the default choice for provisioning infrastructure these days.\\n\n\n##Decision\n- spaces and services should be created with Terraform\\n- the application should be deployed with Terraform\\n- Terraform should be run from Travis to deploy the application\\n","Predictions":"We will use [Security Stream](https:\/\/github.com\/SecurityStream\/SecurityStream) to provide a way to deploy Kubernetes clusters.n"}
{"File Name":"cosmos-sdk\/adr-033-protobuf-inter-module-comm.md","Context":"## Context\nIn the current Cosmos SDK documentation on the [Object-Capability Model](https:\/\/docs.cosmos.network\/main\/learn\/advanced\/ocap#ocaps-in-practice), it is stated that:\n> We assume that a thriving ecosystem of Cosmos SDK modules that are easy to compose into a blockchain application will contain faulty or malicious modules.\nThere is currently not a thriving ecosystem of Cosmos SDK modules. We hypothesize that this is in part due to:\n1. lack of a stable v1.0 Cosmos SDK to build modules off of. Module interfaces are changing, sometimes dramatically, from\npoint release to point release, often for good reasons, but this does not create a stable foundation to build on.\n2. lack of a properly implemented object capability or even object-oriented encapsulation system which makes refactors\nof module keeper interfaces inevitable because the current interfaces are poorly constrained.\n### `x\/bank` Case Study\nCurrently the `x\/bank` keeper gives pretty much unrestricted access to any module which references it. For instance, the\n`SetBalance` method allows the caller to set the balance of any account to anything, bypassing even proper tracking of supply.\nThere appears to have been some later attempts to implement some semblance of OCAPs using module-level minting, staking\nand burning permissions. These permissions allow a module to mint, burn or delegate tokens with reference to the module\u2019s\nown account. These permissions are actually stored as a `[]string` array on the `ModuleAccount` type in state.\nHowever, these permissions don\u2019t really do much. They control what modules can be referenced in the `MintCoins`,\n`BurnCoins` and `DelegateCoins***` methods, but for one there is no unique object capability token that controls access \u2014\njust a simple string. So the `x\/upgrade` module could mint tokens for the `x\/staking` module simple by calling\n`MintCoins(\u201cstaking\u201d)`. Furthermore, all modules which have access to these keeper methods, also have access to\n`SetBalance` negating any other attempt at OCAPs and breaking even basic object-oriented encapsulation.\n","Decision":"Based on [ADR-021](.\/adr-021-protobuf-query-encoding.md) and [ADR-031](.\/adr-031-msg-service.md), we introduce the\\nInter-Module Communication framework for secure module authorization and OCAPs.\\nWhen implemented, this could also serve as an alternative to the existing paradigm of passing keepers between\\nmodules. The approach outlined here-in is intended to form the basis of a Cosmos SDK v1.0 that provides the necessary\\nstability and encapsulation guarantees that allow a thriving module ecosystem to emerge.\\nOf particular note \u2014 the decision is to _enable_ this functionality for modules to adopt at their own discretion.\\nProposals to migrate existing modules to this new paradigm will have to be a separate conversation, potentially\\naddressed as amendments to this ADR.\\n### New \"Keeper\" Paradigm\\nIn [ADR 021](.\/adr-021-protobuf-query-encoding.md), a mechanism for using protobuf service definitions to define queriers\\nwas introduced and in [ADR 31](.\/adr-031-msg-service.md), a mechanism for using protobuf service to define `Msg`s was added.\\nProtobuf service definitions generate two golang interfaces representing the client and server sides of a service plus\\nsome helper code. Here is a minimal example for the bank `cosmos.bank.Msg\/Send` message type:\\n```go\\npackage bank\\ntype MsgClient interface {\\nSend(context.Context, *MsgSend, opts ...grpc.CallOption) (*MsgSendResponse, error)\\n}\\ntype MsgServer interface {\\nSend(context.Context, *MsgSend) (*MsgSendResponse, error)\\n}\\n```\\n[ADR 021](.\/adr-021-protobuf-query-encoding.md) and [ADR 31](.\/adr-031-msg-service.md) specifies how modules can implement the generated `QueryServer`\\nand `MsgServer` interfaces as replacements for the legacy queriers and `Msg` handlers respectively.\\nIn this ADR we explain how modules can make queries and send `Msg`s to other modules using the generated `QueryClient`\\nand `MsgClient` interfaces and propose this mechanism as a replacement for the existing `Keeper` paradigm. To be clear,\\nthis ADR does not necessitate the creation of new protobuf definitions or services. Rather, it leverages the same proto\\nbased service interfaces already used by clients for inter-module communication.\\nUsing this `QueryClient`\/`MsgClient` approach has the following key benefits over exposing keepers to external modules:\\n1. Protobuf types are checked for breaking changes using [buf](https:\/\/buf.build\/docs\/breaking-overview) and because of\\nthe way protobuf is designed this will give us strong backwards compatibility guarantees while allowing for forward\\nevolution.\\n2. The separation between the client and server interfaces will allow us to insert permission checking code in between\\nthe two which checks if one module is authorized to send the specified `Msg` to the other module providing a proper\\nobject capability system (see below).\\n3. The router for inter-module communication gives us a convenient place to handle rollback of transactions,\\nenabling atomicy of operations ([currently a problem](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/8030)). Any failure within a module-to-module call would result in a failure of the entire\\ntransaction\\nThis mechanism has the added benefits of:\\n* reducing boilerplate through code generation, and\\n* allowing for modules in other languages either via a VM like CosmWasm or sub-processes using gRPC\\n### Inter-module Communication\\nTo use the `Client` generated by the protobuf compiler we need a `grpc.ClientConn` [interface](https:\/\/github.com\/grpc\/grpc-go\/blob\/v1.49.x\/clientconn.go#L441-L450)\\nimplementation. For this we introduce\\na new type, `ModuleKey`, which implements the `grpc.ClientConn` interface. `ModuleKey` can be thought of as the \"private\\nkey\" corresponding to a module account, where authentication is provided through use of a special `Invoker()` function,\\ndescribed in more detail below.\\nBlockchain users (external clients) use their account's private key to sign transactions containing `Msg`s where they are listed as signers (each\\nmessage specifies required signers with `Msg.GetSigner`). The authentication checks is performed by `AnteHandler`.\\nHere, we extend this process, by allowing modules to be identified in `Msg.GetSigners`. When a module wants to trigger the execution a `Msg` in another module,\\nits `ModuleKey` acts as the sender (through the `ClientConn` interface we describe below) and is set as a sole \"signer\". It's worth to note\\nthat we don't use any cryptographic signature in this case.\\nFor example, module `A` could use its `A.ModuleKey` to create `MsgSend` object for `\/cosmos.bank.Msg\/Send` transaction. `MsgSend` validation\\nwill assure that the `from` account (`A.ModuleKey` in this case) is the signer.\\nHere's an example of a hypothetical module `foo` interacting with `x\/bank`:\\n```go\\npackage foo\\ntype FooMsgServer {\\n\/\/ ...\\nbankQuery bank.QueryClient\\nbankMsg   bank.MsgClient\\n}\\nfunc NewFooMsgServer(moduleKey RootModuleKey, ...) FooMsgServer {\\n\/\/ ...\\nreturn FooMsgServer {\\n\/\/ ...\\nmodouleKey: moduleKey,\\nbankQuery: bank.NewQueryClient(moduleKey),\\nbankMsg: bank.NewMsgClient(moduleKey),\\n}\\n}\\nfunc (foo *FooMsgServer) Bar(ctx context.Context, req *MsgBarRequest) (*MsgBarResponse, error) {\\nbalance, err := foo.bankQuery.Balance(&bank.QueryBalanceRequest{Address: fooMsgServer.moduleKey.Address(), Denom: \"foo\"})\\n...\\nres, err := foo.bankMsg.Send(ctx, &bank.MsgSendRequest{FromAddress: fooMsgServer.moduleKey.Address(), ...})\\n...\\n}\\n```\\nThis design is also intended to be extensible to cover use cases of more fine grained permissioning like minting by\\ndenom prefix being restricted to certain modules (as discussed in\\n[#7459](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#discussion_r529545528)).\\n### `ModuleKey`s and `ModuleID`s\\nA `ModuleKey` can be thought of as a \"private key\" for a module account and a `ModuleID` can be thought of as the\\ncorresponding \"public key\". From the [ADR 028](.\/adr-028-public-key-addresses.md), modules can have both a root module account and any number of sub-accounts\\nor derived accounts that can be used for different pools (ex. staking pools) or managed accounts (ex. group\\naccounts). We can also think of module sub-accounts as similar to derived keys - there is a root key and then some\\nderivation path. `ModuleID` is a simple struct which contains the module name and optional \"derivation\" path,\\nand forms its address based on the `AddressHash` method from [the ADR-028](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-028-public-key-addresses.md):\\n```go\\ntype ModuleID struct {\\nModuleName string\\nPath []byte\\n}\\nfunc (key ModuleID) Address() []byte {\\nreturn AddressHash(key.ModuleName, key.Path)\\n}\\n```\\nIn addition to being able to generate a `ModuleID` and address, a `ModuleKey` contains a special function called\\n`Invoker` which is the key to safe inter-module access. The `Invoker` creates an `InvokeFn` closure which is used as an `Invoke` method in\\nthe `grpc.ClientConn` interface and under the hood is able to route messages to the appropriate `Msg` and `Query` handlers\\nperforming appropriate security checks on `Msg`s. This allows for even safer inter-module access than keeper's whose\\nprivate member variables could be manipulated through reflection. Golang does not support reflection on a function\\nclosure's captured variables and direct manipulation of memory would be needed for a truly malicious module to bypass\\nthe `ModuleKey` security.\\nThe two `ModuleKey` types are `RootModuleKey` and `DerivedModuleKey`:\\n```go\\ntype Invoker func(callInfo CallInfo) func(ctx context.Context, request, response interface{}, opts ...interface{}) error\\ntype CallInfo {\\nMethod string\\nCaller ModuleID\\n}\\ntype RootModuleKey struct {\\nmoduleName string\\ninvoker Invoker\\n}\\nfunc (rm RootModuleKey) Derive(path []byte) DerivedModuleKey { \/* ... *\/}\\ntype DerivedModuleKey struct {\\nmoduleName string\\npath []byte\\ninvoker Invoker\\n}\\n```\\nA module can get access to a `DerivedModuleKey`, using the `Derive(path []byte)` method on `RootModuleKey` and then\\nwould use this key to authenticate `Msg`s from a sub-account. Ex:\\n```go\\npackage foo\\nfunc (fooMsgServer *MsgServer) Bar(ctx context.Context, req *MsgBar) (*MsgBarResponse, error) {\\nderivedKey := fooMsgServer.moduleKey.Derive(req.SomePath)\\nbankMsgClient := bank.NewMsgClient(derivedKey)\\nres, err := bankMsgClient.Balance(ctx, &bank.MsgSend{FromAddress: derivedKey.Address(), ...})\\n...\\n}\\n```\\nIn this way, a module can gain permissioned access to a root account and any number of sub-accounts and send\\nauthenticated `Msg`s from these accounts. The `Invoker` `callInfo.Caller` parameter is used under the hood to\\ndistinguish between different module accounts, but either way the function returned by `Invoker` only allows `Msg`s\\nfrom either the root or a derived module account to pass through.\\nNote that `Invoker` itself returns a function closure based on the `CallInfo` passed in. This will allow client implementations\\nin the future that cache the invoke function for each method type avoiding the overhead of hash table lookup.\\nThis would reduce the performance overhead of this inter-module communication method to the bare minimum required for\\nchecking permissions.\\nTo re-iterate, the closure only allows access to authorized calls. There is no access to anything else regardless of any\\nname impersonation.\\nBelow is a rough sketch of the implementation of `grpc.ClientConn.Invoke` for `RootModuleKey`:\\n```go\\nfunc (key RootModuleKey) Invoke(ctx context.Context, method string, args, reply interface{}, opts ...grpc.CallOption) error {\\nf := key.invoker(CallInfo {Method: method, Caller: ModuleID {ModuleName: key.moduleName}})\\nreturn f(ctx, args, reply)\\n}\\n```\\n### `AppModule` Wiring and Requirements\\nIn [ADR 031](.\/adr-031-msg-service.md), the `AppModule.RegisterService(Configurator)` method was introduced. To support\\ninter-module communication, we extend the `Configurator` interface to pass in the `ModuleKey` and to allow modules to\\nspecify their dependencies on other modules using `RequireServer()`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nModuleKey() ModuleKey\\nRequireServer(msgServer interface{})\\n}\\n```\\nThe `ModuleKey` is passed to modules in the `RegisterService` method itself so that `RegisterServices` serves as a single\\nentry point for configuring module services. This is intended to also have the side-effect of greatly reducing boilerplate in\\n`app.go`. For now, `ModuleKey`s will be created based on `AppModule.Name()`, but a more flexible system may be\\nintroduced in the future. The `ModuleManager` will handle creation of module accounts behind the scenes.\\nBecause modules do not get direct access to each other anymore, modules may have unfulfilled dependencies. To make sure\\nthat module dependencies are resolved at startup, the `Configurator.RequireServer` method should be added. The `ModuleManager`\\nwill make sure that all dependencies declared with `RequireServer` can be resolved before the app starts. An example\\nmodule `foo` could declare it's dependency on `x\/bank` like this:\\n```go\\npackage foo\\nfunc (am AppModule) RegisterServices(cfg Configurator) {\\ncfg.RequireServer((*bank.QueryServer)(nil))\\ncfg.RequireServer((*bank.MsgServer)(nil))\\n}\\n```\\n### Security Considerations\\nIn addition to checking for `ModuleKey` permissions, a few additional security precautions will need to be taken by\\nthe underlying router infrastructure.\\n#### Recursion and Re-entry\\nRecursive or re-entrant method invocations pose a potential security threat. This can be a problem if Module A\\ncalls Module B and Module B calls module A again in the same call.\\nOne basic way for the router system to deal with this is to maintain a call stack which prevents a module from\\nbeing referenced more than once in the call stack so that there is no re-entry. A `map[string]interface{}` table\\nin the router could be used to perform this security check.\\n#### Queries\\nQueries in Cosmos SDK are generally un-permissioned so allowing one module to query another module should not pose\\nany major security threats assuming basic precautions are taken. The basic precaution that the router system will\\nneed to take is making sure that the `sdk.Context` passed to query methods does not allow writing to the store. This\\ncan be done for now with a `CacheMultiStore` as is currently done for `BaseApp` queries.\\n### Internal Methods\\nIn many cases, we may wish for modules to call methods on other modules which are not exposed to clients at all. For this\\npurpose, we add the `InternalServer` method to `Configurator`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nInternalServer() grpc.Server\\n}\\n```\\nAs an example, x\/slashing's Slash must call x\/staking's Slash, but we don't want to expose x\/staking's Slash to end users\\nand clients.\\nInternal protobuf services will be defined in a corresponding `internal.proto` file in the given module's\\nproto package.\\nServices registered against `InternalServer` will be callable from other modules but not by external clients.\\nAn alternative solution to internal-only methods could involve hooks \/ plugins as discussed [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#issuecomment-733807753).\\nA more detailed evaluation of a hooks \/ plugin system will be addressed later in follow-ups to this ADR or as a separate\\nADR.\\n### Authorization\\nBy default, the inter-module router requires that messages are sent by the first signer returned by `GetSigners`. The\\ninter-module router should also accept authorization middleware such as that provided by [ADR 030](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-030-authz-module.md).\\nThis middleware will allow accounts to otherwise specific module accounts to perform actions on their behalf.\\nAuthorization middleware should take into account the need to grant certain modules effectively \"admin\" privileges to\\nother modules. This will be addressed in separate ADRs or updates to this ADR.\\n### Future Work\\nOther future improvements may include:\\n* custom code generation that:\\n* simplifies interfaces (ex. generates code with `sdk.Context` instead of `context.Context`)\\n* optimizes inter-module calls - for instance caching resolved methods after first invocation\\n* combining `StoreKey`s and `ModuleKey`s into a single interface so that modules have a single OCAPs handle\\n* code generation which makes inter-module communication more performant\\n* decoupling `ModuleKey` creation from `AppModule.Name()` so that app's can override root module account names\\n* inter-module hooks and plugins\\n","tokens":451,"id":812,"text":"## Context\\nIn the current Cosmos SDK documentation on the [Object-Capability Model](https:\/\/docs.cosmos.network\/main\/learn\/advanced\/ocap#ocaps-in-practice), it is stated that:\\n> We assume that a thriving ecosystem of Cosmos SDK modules that are easy to compose into a blockchain application will contain faulty or malicious modules.\\nThere is currently not a thriving ecosystem of Cosmos SDK modules. We hypothesize that this is in part due to:\\n1. lack of a stable v1.0 Cosmos SDK to build modules off of. Module interfaces are changing, sometimes dramatically, from\\npoint release to point release, often for good reasons, but this does not create a stable foundation to build on.\\n2. lack of a properly implemented object capability or even object-oriented encapsulation system which makes refactors\\nof module keeper interfaces inevitable because the current interfaces are poorly constrained.\\n### `x\/bank` Case Study\\nCurrently the `x\/bank` keeper gives pretty much unrestricted access to any module which references it. For instance, the\\n`SetBalance` method allows the caller to set the balance of any account to anything, bypassing even proper tracking of supply.\\nThere appears to have been some later attempts to implement some semblance of OCAPs using module-level minting, staking\\nand burning permissions. These permissions allow a module to mint, burn or delegate tokens with reference to the module\u2019s\\nown account. These permissions are actually stored as a `[]string` array on the `ModuleAccount` type in state.\\nHowever, these permissions don\u2019t really do much. They control what modules can be referenced in the `MintCoins`,\\n`BurnCoins` and `DelegateCoins***` methods, but for one there is no unique object capability token that controls access \u2014\\njust a simple string. So the `x\/upgrade` module could mint tokens for the `x\/staking` module simple by calling\\n`MintCoins(\u201cstaking\u201d)`. Furthermore, all modules which have access to these keeper methods, also have access to\\n`SetBalance` negating any other attempt at OCAPs and breaking even basic object-oriented encapsulation.\\n\n\n##Decision\nBased on [ADR-021](.\/adr-021-protobuf-query-encoding.md) and [ADR-031](.\/adr-031-msg-service.md), we introduce the\\nInter-Module Communication framework for secure module authorization and OCAPs.\\nWhen implemented, this could also serve as an alternative to the existing paradigm of passing keepers between\\nmodules. The approach outlined here-in is intended to form the basis of a Cosmos SDK v1.0 that provides the necessary\\nstability and encapsulation guarantees that allow a thriving module ecosystem to emerge.\\nOf particular note \u2014 the decision is to _enable_ this functionality for modules to adopt at their own discretion.\\nProposals to migrate existing modules to this new paradigm will have to be a separate conversation, potentially\\naddressed as amendments to this ADR.\\n### New \"Keeper\" Paradigm\\nIn [ADR 021](.\/adr-021-protobuf-query-encoding.md), a mechanism for using protobuf service definitions to define queriers\\nwas introduced and in [ADR 31](.\/adr-031-msg-service.md), a mechanism for using protobuf service to define `Msg`s was added.\\nProtobuf service definitions generate two golang interfaces representing the client and server sides of a service plus\\nsome helper code. Here is a minimal example for the bank `cosmos.bank.Msg\/Send` message type:\\n```go\\npackage bank\\ntype MsgClient interface {\\nSend(context.Context, *MsgSend, opts ...grpc.CallOption) (*MsgSendResponse, error)\\n}\\ntype MsgServer interface {\\nSend(context.Context, *MsgSend) (*MsgSendResponse, error)\\n}\\n```\\n[ADR 021](.\/adr-021-protobuf-query-encoding.md) and [ADR 31](.\/adr-031-msg-service.md) specifies how modules can implement the generated `QueryServer`\\nand `MsgServer` interfaces as replacements for the legacy queriers and `Msg` handlers respectively.\\nIn this ADR we explain how modules can make queries and send `Msg`s to other modules using the generated `QueryClient`\\nand `MsgClient` interfaces and propose this mechanism as a replacement for the existing `Keeper` paradigm. To be clear,\\nthis ADR does not necessitate the creation of new protobuf definitions or services. Rather, it leverages the same proto\\nbased service interfaces already used by clients for inter-module communication.\\nUsing this `QueryClient`\/`MsgClient` approach has the following key benefits over exposing keepers to external modules:\\n1. Protobuf types are checked for breaking changes using [buf](https:\/\/buf.build\/docs\/breaking-overview) and because of\\nthe way protobuf is designed this will give us strong backwards compatibility guarantees while allowing for forward\\nevolution.\\n2. The separation between the client and server interfaces will allow us to insert permission checking code in between\\nthe two which checks if one module is authorized to send the specified `Msg` to the other module providing a proper\\nobject capability system (see below).\\n3. The router for inter-module communication gives us a convenient place to handle rollback of transactions,\\nenabling atomicy of operations ([currently a problem](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/8030)). Any failure within a module-to-module call would result in a failure of the entire\\ntransaction\\nThis mechanism has the added benefits of:\\n* reducing boilerplate through code generation, and\\n* allowing for modules in other languages either via a VM like CosmWasm or sub-processes using gRPC\\n### Inter-module Communication\\nTo use the `Client` generated by the protobuf compiler we need a `grpc.ClientConn` [interface](https:\/\/github.com\/grpc\/grpc-go\/blob\/v1.49.x\/clientconn.go#L441-L450)\\nimplementation. For this we introduce\\na new type, `ModuleKey`, which implements the `grpc.ClientConn` interface. `ModuleKey` can be thought of as the \"private\\nkey\" corresponding to a module account, where authentication is provided through use of a special `Invoker()` function,\\ndescribed in more detail below.\\nBlockchain users (external clients) use their account's private key to sign transactions containing `Msg`s where they are listed as signers (each\\nmessage specifies required signers with `Msg.GetSigner`). The authentication checks is performed by `AnteHandler`.\\nHere, we extend this process, by allowing modules to be identified in `Msg.GetSigners`. When a module wants to trigger the execution a `Msg` in another module,\\nits `ModuleKey` acts as the sender (through the `ClientConn` interface we describe below) and is set as a sole \"signer\". It's worth to note\\nthat we don't use any cryptographic signature in this case.\\nFor example, module `A` could use its `A.ModuleKey` to create `MsgSend` object for `\/cosmos.bank.Msg\/Send` transaction. `MsgSend` validation\\nwill assure that the `from` account (`A.ModuleKey` in this case) is the signer.\\nHere's an example of a hypothetical module `foo` interacting with `x\/bank`:\\n```go\\npackage foo\\ntype FooMsgServer {\\n\/\/ ...\\nbankQuery bank.QueryClient\\nbankMsg   bank.MsgClient\\n}\\nfunc NewFooMsgServer(moduleKey RootModuleKey, ...) FooMsgServer {\\n\/\/ ...\\nreturn FooMsgServer {\\n\/\/ ...\\nmodouleKey: moduleKey,\\nbankQuery: bank.NewQueryClient(moduleKey),\\nbankMsg: bank.NewMsgClient(moduleKey),\\n}\\n}\\nfunc (foo *FooMsgServer) Bar(ctx context.Context, req *MsgBarRequest) (*MsgBarResponse, error) {\\nbalance, err := foo.bankQuery.Balance(&bank.QueryBalanceRequest{Address: fooMsgServer.moduleKey.Address(), Denom: \"foo\"})\\n...\\nres, err := foo.bankMsg.Send(ctx, &bank.MsgSendRequest{FromAddress: fooMsgServer.moduleKey.Address(), ...})\\n...\\n}\\n```\\nThis design is also intended to be extensible to cover use cases of more fine grained permissioning like minting by\\ndenom prefix being restricted to certain modules (as discussed in\\n[#7459](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#discussion_r529545528)).\\n### `ModuleKey`s and `ModuleID`s\\nA `ModuleKey` can be thought of as a \"private key\" for a module account and a `ModuleID` can be thought of as the\\ncorresponding \"public key\". From the [ADR 028](.\/adr-028-public-key-addresses.md), modules can have both a root module account and any number of sub-accounts\\nor derived accounts that can be used for different pools (ex. staking pools) or managed accounts (ex. group\\naccounts). We can also think of module sub-accounts as similar to derived keys - there is a root key and then some\\nderivation path. `ModuleID` is a simple struct which contains the module name and optional \"derivation\" path,\\nand forms its address based on the `AddressHash` method from [the ADR-028](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-028-public-key-addresses.md):\\n```go\\ntype ModuleID struct {\\nModuleName string\\nPath []byte\\n}\\nfunc (key ModuleID) Address() []byte {\\nreturn AddressHash(key.ModuleName, key.Path)\\n}\\n```\\nIn addition to being able to generate a `ModuleID` and address, a `ModuleKey` contains a special function called\\n`Invoker` which is the key to safe inter-module access. The `Invoker` creates an `InvokeFn` closure which is used as an `Invoke` method in\\nthe `grpc.ClientConn` interface and under the hood is able to route messages to the appropriate `Msg` and `Query` handlers\\nperforming appropriate security checks on `Msg`s. This allows for even safer inter-module access than keeper's whose\\nprivate member variables could be manipulated through reflection. Golang does not support reflection on a function\\nclosure's captured variables and direct manipulation of memory would be needed for a truly malicious module to bypass\\nthe `ModuleKey` security.\\nThe two `ModuleKey` types are `RootModuleKey` and `DerivedModuleKey`:\\n```go\\ntype Invoker func(callInfo CallInfo) func(ctx context.Context, request, response interface{}, opts ...interface{}) error\\ntype CallInfo {\\nMethod string\\nCaller ModuleID\\n}\\ntype RootModuleKey struct {\\nmoduleName string\\ninvoker Invoker\\n}\\nfunc (rm RootModuleKey) Derive(path []byte) DerivedModuleKey { \/* ... *\/}\\ntype DerivedModuleKey struct {\\nmoduleName string\\npath []byte\\ninvoker Invoker\\n}\\n```\\nA module can get access to a `DerivedModuleKey`, using the `Derive(path []byte)` method on `RootModuleKey` and then\\nwould use this key to authenticate `Msg`s from a sub-account. Ex:\\n```go\\npackage foo\\nfunc (fooMsgServer *MsgServer) Bar(ctx context.Context, req *MsgBar) (*MsgBarResponse, error) {\\nderivedKey := fooMsgServer.moduleKey.Derive(req.SomePath)\\nbankMsgClient := bank.NewMsgClient(derivedKey)\\nres, err := bankMsgClient.Balance(ctx, &bank.MsgSend{FromAddress: derivedKey.Address(), ...})\\n...\\n}\\n```\\nIn this way, a module can gain permissioned access to a root account and any number of sub-accounts and send\\nauthenticated `Msg`s from these accounts. The `Invoker` `callInfo.Caller` parameter is used under the hood to\\ndistinguish between different module accounts, but either way the function returned by `Invoker` only allows `Msg`s\\nfrom either the root or a derived module account to pass through.\\nNote that `Invoker` itself returns a function closure based on the `CallInfo` passed in. This will allow client implementations\\nin the future that cache the invoke function for each method type avoiding the overhead of hash table lookup.\\nThis would reduce the performance overhead of this inter-module communication method to the bare minimum required for\\nchecking permissions.\\nTo re-iterate, the closure only allows access to authorized calls. There is no access to anything else regardless of any\\nname impersonation.\\nBelow is a rough sketch of the implementation of `grpc.ClientConn.Invoke` for `RootModuleKey`:\\n```go\\nfunc (key RootModuleKey) Invoke(ctx context.Context, method string, args, reply interface{}, opts ...grpc.CallOption) error {\\nf := key.invoker(CallInfo {Method: method, Caller: ModuleID {ModuleName: key.moduleName}})\\nreturn f(ctx, args, reply)\\n}\\n```\\n### `AppModule` Wiring and Requirements\\nIn [ADR 031](.\/adr-031-msg-service.md), the `AppModule.RegisterService(Configurator)` method was introduced. To support\\ninter-module communication, we extend the `Configurator` interface to pass in the `ModuleKey` and to allow modules to\\nspecify their dependencies on other modules using `RequireServer()`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nModuleKey() ModuleKey\\nRequireServer(msgServer interface{})\\n}\\n```\\nThe `ModuleKey` is passed to modules in the `RegisterService` method itself so that `RegisterServices` serves as a single\\nentry point for configuring module services. This is intended to also have the side-effect of greatly reducing boilerplate in\\n`app.go`. For now, `ModuleKey`s will be created based on `AppModule.Name()`, but a more flexible system may be\\nintroduced in the future. The `ModuleManager` will handle creation of module accounts behind the scenes.\\nBecause modules do not get direct access to each other anymore, modules may have unfulfilled dependencies. To make sure\\nthat module dependencies are resolved at startup, the `Configurator.RequireServer` method should be added. The `ModuleManager`\\nwill make sure that all dependencies declared with `RequireServer` can be resolved before the app starts. An example\\nmodule `foo` could declare it's dependency on `x\/bank` like this:\\n```go\\npackage foo\\nfunc (am AppModule) RegisterServices(cfg Configurator) {\\ncfg.RequireServer((*bank.QueryServer)(nil))\\ncfg.RequireServer((*bank.MsgServer)(nil))\\n}\\n```\\n### Security Considerations\\nIn addition to checking for `ModuleKey` permissions, a few additional security precautions will need to be taken by\\nthe underlying router infrastructure.\\n#### Recursion and Re-entry\\nRecursive or re-entrant method invocations pose a potential security threat. This can be a problem if Module A\\ncalls Module B and Module B calls module A again in the same call.\\nOne basic way for the router system to deal with this is to maintain a call stack which prevents a module from\\nbeing referenced more than once in the call stack so that there is no re-entry. A `map[string]interface{}` table\\nin the router could be used to perform this security check.\\n#### Queries\\nQueries in Cosmos SDK are generally un-permissioned so allowing one module to query another module should not pose\\nany major security threats assuming basic precautions are taken. The basic precaution that the router system will\\nneed to take is making sure that the `sdk.Context` passed to query methods does not allow writing to the store. This\\ncan be done for now with a `CacheMultiStore` as is currently done for `BaseApp` queries.\\n### Internal Methods\\nIn many cases, we may wish for modules to call methods on other modules which are not exposed to clients at all. For this\\npurpose, we add the `InternalServer` method to `Configurator`:\\n```go\\ntype Configurator interface {\\nMsgServer() grpc.Server\\nQueryServer() grpc.Server\\nInternalServer() grpc.Server\\n}\\n```\\nAs an example, x\/slashing's Slash must call x\/staking's Slash, but we don't want to expose x\/staking's Slash to end users\\nand clients.\\nInternal protobuf services will be defined in a corresponding `internal.proto` file in the given module's\\nproto package.\\nServices registered against `InternalServer` will be callable from other modules but not by external clients.\\nAn alternative solution to internal-only methods could involve hooks \/ plugins as discussed [here](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/7459#issuecomment-733807753).\\nA more detailed evaluation of a hooks \/ plugin system will be addressed later in follow-ups to this ADR or as a separate\\nADR.\\n### Authorization\\nBy default, the inter-module router requires that messages are sent by the first signer returned by `GetSigners`. The\\ninter-module router should also accept authorization middleware such as that provided by [ADR 030](https:\/\/github.com\/cosmos\/cosmos-sdk\/blob\/main\/docs\/architecture\/adr-030-authz-module.md).\\nThis middleware will allow accounts to otherwise specific module accounts to perform actions on their behalf.\\nAuthorization middleware should take into account the need to grant certain modules effectively \"admin\" privileges to\\nother modules. This will be addressed in separate ADRs or updates to this ADR.\\n### Future Work\\nOther future improvements may include:\\n* custom code generation that:\\n* simplifies interfaces (ex. generates code with `sdk.Context` instead of `context.Context`)\\n* optimizes inter-module calls - for instance caching resolved methods after first invocation\\n* combining `StoreKey`s and `ModuleKey`s into a single interface so that modules have a single OCAPs handle\\n* code generation which makes inter-module communication more performant\\n* decoupling `ModuleKey` creation from `AppModule.Name()` so that app's can override root module account names\\n* inter-module hooks and plugins\\n","Predictions":"We assume that a thriving ecosystem of Cosmos SDK modules that are easy to compose into a blockchain application will contain faulty or malicious modules.nWe hypothesise that this is in part due to:n1. lack of a stable v1.0 Cosmos SDK to build modules off of. Module interfaces are changing, sometimes dramatically, fromnpoint release to point release, often for good reasons, but this does not create a stable foundation to build on.n2. lack of a properly implemented object capability or even object-oriented encapsulation system which makes refactorsnof module keeper interfaces inevitable because the current interfaces are poorly constrained.n### x\/bank Case StudynCurrently the x\/bank keeper gives pretty much unrestricted access to any module which references it. For instance, thenSetBalance method allows the caller to set the balance of any account to anything, bypassing even proper tracking of supply.nThere appears to have been some later attempts to implement some semblance of OCAPs using module-level minting, stakingnand burning permissions. These permissions allow a module to mint, burn or delegate tokens with reference to the module\u2019snown account. These permissions are actually stored as a []string array on the ModuleAccount type in state.nHowever, these permissions don\u2019t really do much. They control what modules can be referenced in the MintCoins,nBurnCoins and DelegateCoins*** methods, but for one there is no unique object capability token that controls access \u2014njust a simple string. So the x\/upgrade module could mint tokens for the x\/staking module simple by callingnx\/upgrade module could mint tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module could delegate tokens for the x\/staking module simple by callingnx\/upgrade module"}
{"File Name":"publicservice-registry\/0004-sqlstreamstore.md","Context":"## Context\nSince we decided to use event sourcing, we need a way to store events in our database.\nIn `Wegwijs` we stored events in `MSSQL`, which allows easy debugging of events. All sql statements to save\/read events were hand-written.\n**However**, since we decided on async event handlers in a previous ADR, we would benefit a lot from having catch-up subscriptions for our event handlers. Catch-up subscriptions allow event handlers to be in charge of what events they are interested in, and give event handlers more autonomy over their own rebuilds.\nWhile `GetEventStore` supports this, and is most likely a top-notch choice for storing events, this would require us to take care of hosting this. We also have doubts about the support for storing business-critical data outside of `MSSQL` in `AIV`.\nWe currently host no VMs for business-critical concerns, and we feel that hosting `GetEventStore` ourselves, would add a significant burden.\nAs an alternative, `SqlStreamStore` is an OSS library on GitHub which supports storing events into `MSSQL`, and has support for catch-up subscriptions. It has an active community, and has been used in several production systems successfully according to that community.\n","Decision":"We will use the `SqlStreamStore` library as our event store. We will keep an eye on ongoing developments from `SqlStreamStore`.\\n","tokens":264,"id":3123,"text":"## Context\\nSince we decided to use event sourcing, we need a way to store events in our database.\\nIn `Wegwijs` we stored events in `MSSQL`, which allows easy debugging of events. All sql statements to save\/read events were hand-written.\\n**However**, since we decided on async event handlers in a previous ADR, we would benefit a lot from having catch-up subscriptions for our event handlers. Catch-up subscriptions allow event handlers to be in charge of what events they are interested in, and give event handlers more autonomy over their own rebuilds.\\nWhile `GetEventStore` supports this, and is most likely a top-notch choice for storing events, this would require us to take care of hosting this. We also have doubts about the support for storing business-critical data outside of `MSSQL` in `AIV`.\\nWe currently host no VMs for business-critical concerns, and we feel that hosting `GetEventStore` ourselves, would add a significant burden.\\nAs an alternative, `SqlStreamStore` is an OSS library on GitHub which supports storing events into `MSSQL`, and has support for catch-up subscriptions. It has an active community, and has been used in several production systems successfully according to that community.\\n\n\n##Decision\nWe will use the `SqlStreamStore` library as our event store. We will keep an eye on ongoing developments from `SqlStreamStore`.\\n","Predictions":"We will use the GetEventStore library as our event store. We will keep an eye on ongoing developments from GetEventStore.n"}
{"File Name":"ADR\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":440,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"nhsuk-prototype-kit-version-one\/0003-use-npm-scripts-and-gulp-for-running-tasks.md","Context":"## Context\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\n","Decision":"We will use a mixture on NPM scripts and [Gulp](https:\/\/gulpjs.com) to run our tasks. NPM scripts give us the core installation and start tasks as well as the ability to run Gulp tasks. Gulp, written in javascript, is very extensible and will allow us to have complete control over compilation and assembly of the applications assets.\\n","tokens":49,"id":3682,"text":"## Context\\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\\n\n\n##Decision\nWe will use a mixture on NPM scripts and [Gulp](https:\/\/gulpjs.com) to run our tasks. NPM scripts give us the core installation and start tasks as well as the ability to run Gulp tasks. Gulp, written in javascript, is very extensible and will allow us to have complete control over compilation and assembly of the applications assets.\\n","Predictions":"We will use [Azure](https:\/\/azure.io\/) to handle error handling.n"}
{"File Name":"fxa\/0020-application-architecture.md","Context":"## Context and Problem Statement\nThe FxA back-end service stack contains an application architecture that is ad-hoc, not documented, and missing modern features (such as Dependency Injection) which results in the following problems:\n- New developers struggle to get up to speed as they must learn the architecture by reading the code as we have no documentation on the application structure, why they're structured the way they are, or how new components should be added to fit in. Each back-end service may vary in its ad-hoc architecture as well.\n- Adding new objects needed in a route handler can be time-consuming as the object must be plumbed through the entire initialization chain vs. more elegant methods like Dependency Injection (DI).\n- Not clear where\/how to add new components and takes time to study\/understand how things are currently setup in an attempt to mimic the structure for the new component.\n- Time consuming to setup boiler-plate for components, as we have no tooling to work with the current ad-hoc application architectures.\n- Our ad-hoc architecture frequently mixes concerns such as having business logic mixed in with request handling logic, and has other warts from its evolution over time vs. being planned up front.\n- New back-end services evolve differently resulting in more ad-hoc application architectures to learn.\n- Shared components in `fxa-shared` can't rely on basic object lifecycles or setup approaches as they may be used in multiple different ad-hoc application architectures.\nNot choosing an application framework means that we have choosen to make ad-hoc application architectures which will continue to exhibit the problems above.\nIt is assumed that the four newest FxA back-end services (admin-server, support-panel, event-broker, gql-api) will be switched to the chosen approach for consistency.\n## Decision Drivers\n- Documented application architecture.\n- Tooling that reduces boiler-plate and creates consistent code architecture.\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\n- Training materials so that new developers can understand and work on application features easily.\n- Ability to migrate legacy applications to similar conventions\/setup.\n- Handles situations we need such as:\n- Exception handling\n- Validation\n- Service instantiation (DI, etc)\n- Authentication\n- RESTful API's (Swagger\/OpenAPI a bonus)\n- GraphQL\n- Use of components\/services easily in a script\n","Decision":"- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\nChosen Option: Use an existing framework: NestJS\\nNestJS and LoopBack are the two most compelling options, however NestJS has substantially better GraphQL support and a much larger user-base. LoopBack has its user-base divided between two versions with substantial changes between them. We will use NestJS for new projects and update the newest services (admin-server, support-panel, event-broker, gql-api) to NestJS.\\nTo reduce documentation needs and address that auth-server will not be reasonable to migrate, we will backport code organization schemes and directory\/filename conventions from NestJS and use `typedi` as a DI system to ease configuration\/testing in auth-server and possibly other legacy packages. This will allow developers familiar with NestJS projects to still locate and easily work on FxA legacy packages.\\n### Positive Consequences\\n- Less documentation to create, as we only document differences for legacy services.\\n- NestJS GraphQL support matches existing typegraphql paradigm closely for minimal effort switch.\\n### Negative Consequences\\n- There will be some effort involved to migrate the 4 packages to NestJS.\\n- We will need to document the differences between NestJS and other legacy services.\\n","tokens":499,"id":389,"text":"## Context and Problem Statement\\nThe FxA back-end service stack contains an application architecture that is ad-hoc, not documented, and missing modern features (such as Dependency Injection) which results in the following problems:\\n- New developers struggle to get up to speed as they must learn the architecture by reading the code as we have no documentation on the application structure, why they're structured the way they are, or how new components should be added to fit in. Each back-end service may vary in its ad-hoc architecture as well.\\n- Adding new objects needed in a route handler can be time-consuming as the object must be plumbed through the entire initialization chain vs. more elegant methods like Dependency Injection (DI).\\n- Not clear where\/how to add new components and takes time to study\/understand how things are currently setup in an attempt to mimic the structure for the new component.\\n- Time consuming to setup boiler-plate for components, as we have no tooling to work with the current ad-hoc application architectures.\\n- Our ad-hoc architecture frequently mixes concerns such as having business logic mixed in with request handling logic, and has other warts from its evolution over time vs. being planned up front.\\n- New back-end services evolve differently resulting in more ad-hoc application architectures to learn.\\n- Shared components in `fxa-shared` can't rely on basic object lifecycles or setup approaches as they may be used in multiple different ad-hoc application architectures.\\nNot choosing an application framework means that we have choosen to make ad-hoc application architectures which will continue to exhibit the problems above.\\nIt is assumed that the four newest FxA back-end services (admin-server, support-panel, event-broker, gql-api) will be switched to the chosen approach for consistency.\\n## Decision Drivers\\n- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\n\n\n##Decision\n- Documented application architecture.\\n- Tooling that reduces boiler-plate and creates consistent code architecture.\\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\\n- Training materials so that new developers can understand and work on application features easily.\\n- Ability to migrate legacy applications to similar conventions\/setup.\\n- Handles situations we need such as:\\n- Exception handling\\n- Validation\\n- Service instantiation (DI, etc)\\n- Authentication\\n- RESTful API's (Swagger\/OpenAPI a bonus)\\n- GraphQL\\n- Use of components\/services easily in a script\\nChosen Option: Use an existing framework: NestJS\\nNestJS and LoopBack are the two most compelling options, however NestJS has substantially better GraphQL support and a much larger user-base. LoopBack has its user-base divided between two versions with substantial changes between them. We will use NestJS for new projects and update the newest services (admin-server, support-panel, event-broker, gql-api) to NestJS.\\nTo reduce documentation needs and address that auth-server will not be reasonable to migrate, we will backport code organization schemes and directory\/filename conventions from NestJS and use `typedi` as a DI system to ease configuration\/testing in auth-server and possibly other legacy packages. This will allow developers familiar with NestJS projects to still locate and easily work on FxA legacy packages.\\n### Positive Consequences\\n- Less documentation to create, as we only document differences for legacy services.\\n- NestJS GraphQL support matches existing typegraphql paradigm closely for minimal effort switch.\\n### Negative Consequences\\n- There will be some effort involved to migrate the 4 packages to NestJS.\\n- We will need to document the differences between NestJS and other legacy services.\\n","Predictions":"* Documented application architecture.n* Tooling that reduces boiler-plate and creates consistent code architecture.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Training materials so that new developers can understand and work on application features.n* Modern paradigms to ease creation of global objects and their testability, such as DI.n* Modern paradigms to ease creation of global objects and their testability, such as DI"}
{"File Name":"fundraising-application\/003_Validation.md","Context":"## Context\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\n* Use an established library, like [Symfony Validation](https:\/\/symfony.com\/doc\/current\/validation.html).\n* Write our own validation logic.\nThe arguments in favor of writing our own logic were:\n* We don't want to bind our domain layer to a concrete validation library implementation.\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\n* Every developer would have to learn the API of the external library.\nAt the start of the project we did not know where we should put the validation logic:\n* At the framework\/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\n* At the use case layer, making validation part of the use case.\n","Decision":"For each use case we write a validator class that checks the `Request` value object of that use case. The validator class must ensure that the use case can create valid domain objects from the request object. The validator class uses simple `if` checks and no external framework.\\nWe return result data structures from validation classes. The result data structures that have some way of communicating to the framework layer what input caused the validation error. If necessary, one input can have more than one validation error.\\nValidation error names are language-independent unique strings in `snake_case`. When we need to translate those error codes, we put the translations in the file [`validations.js`](https:\/\/github.com\/wmde\/fundraising-frontend-content\/blob\/test\/i18n\/de_DE\/messages\/validations.json) in the [content repository](https:\/\/github.com\/wmde\/fundraising-frontend-content). We don't  map every error to a translation, we can write frontend layer code that summarizes the errors or maps them in a different way.\\n","tokens":225,"id":1532,"text":"## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https:\/\/symfony.com\/doc\/current\/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework\/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n\n\n##Decision\nFor each use case we write a validator class that checks the `Request` value object of that use case. The validator class must ensure that the use case can create valid domain objects from the request object. The validator class uses simple `if` checks and no external framework.\\nWe return result data structures from validation classes. The result data structures that have some way of communicating to the framework layer what input caused the validation error. If necessary, one input can have more than one validation error.\\nValidation error names are language-independent unique strings in `snake_case`. When we need to translate those error codes, we put the translations in the file [`validations.js`](https:\/\/github.com\/wmde\/fundraising-frontend-content\/blob\/test\/i18n\/de_DE\/messages\/validations.json) in the [content repository](https:\/\/github.com\/wmde\/fundraising-frontend-content). We don't  map every error to a translation, we can write frontend layer code that summarizes the errors or maps them in a different way.\\n","Predictions":"We decided to use the [Semantic Validation](https:\/\/github.com\/SemanticValidation) library for the validation of the domain.n"}
{"File Name":"sre-challenge\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3006,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"oasis-core\/0008-standard-account-key-generation.md","Context":"## Context\nCurrently, each application interacting with the [Oasis Network] defines its own\nmethod of generating an account's private\/public key pair.\n[Account]'s public key is in turn used to derive the account's address of the\nform `oasis1 ... 40 characters ...` which is used to for a variety of operations\n(i.e. token transfers, delegations\/undelegations, ...) on the network.\nThe blockchain ecosystem has developed many standards for generating keys which\nimprove key storage and interoperability between different applications.\nAdopting these standards will allow the Oasis ecosystem to:\n- Make key derivation the same across different applications (i.e. wallets).\n- Allow users to hold keys in hardware wallets.\n- Allow users to hold keys in cold storage more reliably (i.e. using the\nfamiliar 24 word mnemonics).\n- Define how users can generate multiple keys from a single seed (i.e.\nthe 24 or 12 word mnemonic).\n","Decision":"### Mnemonic Codes for Master Key Derivation\\nWe use Bitcoin's [BIP-0039]: _Mnemonic code for generating deterministic keys_\\nto derivate a binary seed from a mnemonic code.\\nThe binary seed is in turn used to derive the _master key_, the root key from\\nwhich a hierarchy of deterministic keys is derived, as described in\\n[Hierarchical Key Derivation Scheme][hd-scheme].\\nWe strongly recommend using 24 word mnemonics which correspond to 256 bits of\\nentropy.\\n### Hierarchical Key Derivation Scheme\\nWe use Sathoshi Labs' [SLIP-0010]: _Universal private key derivation from master\\nprivate key_, which is a superset of\\nBitcoin's [BIP-0032]: _Hierarchical Deterministic Wallets_ derivation algorithm,\\nextended to work on other curves.\\nAccount keys use the [edwards25519 curve] from the Ed25519 signature scheme\\nspecified in [RFC 8032].\\n### Key Derivation Paths\\nWe adapt [BIP-0044]: _Multi-Account Hierarchy for Deterministic Wallets_ for\\ngenerating deterministic keys where `coin_type` equals 474, as assigned to the\\nOasis Network by [SLIP-0044].\\nThe following [BIP-0032] path should be used to generate keys:\\n```\\nm\/44'\/474'\/x'\\n```\\nwhere `x` represents the key number.\\nNote that all path levels are _hardened_, e.g. `44'` is `44 | 0x8000000` or\\n`44 + 2^31`.\\nThe key corresponding to key number 0 (i.e. `m\/44'\/474'\/0'`) is called the\\n_primary key_.\\nThe account corresponding to the _primary key_ is called the _primary account_.\\nApplications (i.e. wallets) should use this account as a user's default Oasis\\naccount.\\n","tokens":208,"id":4346,"text":"## Context\\nCurrently, each application interacting with the [Oasis Network] defines its own\\nmethod of generating an account's private\/public key pair.\\n[Account]'s public key is in turn used to derive the account's address of the\\nform `oasis1 ... 40 characters ...` which is used to for a variety of operations\\n(i.e. token transfers, delegations\/undelegations, ...) on the network.\\nThe blockchain ecosystem has developed many standards for generating keys which\\nimprove key storage and interoperability between different applications.\\nAdopting these standards will allow the Oasis ecosystem to:\\n- Make key derivation the same across different applications (i.e. wallets).\\n- Allow users to hold keys in hardware wallets.\\n- Allow users to hold keys in cold storage more reliably (i.e. using the\\nfamiliar 24 word mnemonics).\\n- Define how users can generate multiple keys from a single seed (i.e.\\nthe 24 or 12 word mnemonic).\\n\n\n##Decision\n### Mnemonic Codes for Master Key Derivation\\nWe use Bitcoin's [BIP-0039]: _Mnemonic code for generating deterministic keys_\\nto derivate a binary seed from a mnemonic code.\\nThe binary seed is in turn used to derive the _master key_, the root key from\\nwhich a hierarchy of deterministic keys is derived, as described in\\n[Hierarchical Key Derivation Scheme][hd-scheme].\\nWe strongly recommend using 24 word mnemonics which correspond to 256 bits of\\nentropy.\\n### Hierarchical Key Derivation Scheme\\nWe use Sathoshi Labs' [SLIP-0010]: _Universal private key derivation from master\\nprivate key_, which is a superset of\\nBitcoin's [BIP-0032]: _Hierarchical Deterministic Wallets_ derivation algorithm,\\nextended to work on other curves.\\nAccount keys use the [edwards25519 curve] from the Ed25519 signature scheme\\nspecified in [RFC 8032].\\n### Key Derivation Paths\\nWe adapt [BIP-0044]: _Multi-Account Hierarchy for Deterministic Wallets_ for\\ngenerating deterministic keys where `coin_type` equals 474, as assigned to the\\nOasis Network by [SLIP-0044].\\nThe following [BIP-0032] path should be used to generate keys:\\n```\\nm\/44'\/474'\/x'\\n```\\nwhere `x` represents the key number.\\nNote that all path levels are _hardened_, e.g. `44'` is `44 | 0x8000000` or\\n`44 + 2^31`.\\nThe key corresponding to key number 0 (i.e. `m\/44'\/474'\/0'`) is called the\\n_primary key_.\\nThe account corresponding to the _primary key_ is called the _primary account_.\\nApplications (i.e. wallets) should use this account as a user's default Oasis\\naccount.\\n","Predictions":"We will use the [Security Layer](https:\/\/securitylayer.org\/) protocol to provide a single, unified,nseparate, andnunified mnemonic code for eachnapplication.nThe mnemonic code will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed, and a single mnemonic.nThe mnemonic will be a single seed, a single seed"}
{"File Name":"tech-radar\/adr-4-structure_of_principle.md","Context":"# Context\nWe need consistency in terms of format and structure for our patterns across the customer facing, integration and other architectures.\nWe are also keen to link back to business strategy directly so we can isolate points for consideration in design, implementsation and assessment.\n# Decision\nWe propose the following struture for principle artefacts:\n* Context\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\n* Problems\n* Rationale\n* Examples\n* Discussion Points\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\n# Status\nProposed\n# Consequences\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\n# Example\n* Independently deployable services\n* Context\n* (Link to URI for business strategy)\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\n* Problems\n* Respond to change quickly\n* Availability\n* Scalability\n* Rationale\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\n* Examples\n(TODO: MDTP Link explaining)\n* Discussion Points\nNot Applicable\nTODO: Given scenario X, contact TRAP (email hyperlink).\n","Decision":"We propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n","tokens":382,"id":3925,"text":"# Context\\nWe need consistency in terms of format and structure for our patterns across the customer facing, integration and other architectures.\\nWe are also keen to link back to business strategy directly so we can isolate points for consideration in design, implementsation and assessment.\\n# Decision\\nWe propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n\n\n##Decision\nWe propose the following struture for principle artefacts:\\n* Context\\n* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promote\\n* Problems\\n* Rationale\\n* Examples\\n* Discussion Points\\n* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)\\n# Status\\nProposed\\n# Consequences\\nStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase. The first phase is to get the material into the repository.\\n# Example\\n* Independently deployable services\\n* Context\\n* (Link to URI for business strategy)\\n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]\\n* Problems\\n* Respond to change quickly\\n* Availability\\n* Scalability\\n* Rationale\\nThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services. Loose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.\\nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.\\n* Examples\\n(TODO: MDTP Link explaining)\\n* Discussion Points\\nNot Applicable\\nTODO: Given scenario X, contact TRAP (email hyperlink).\\n","Predictions":"* Where possible link to URI's for business strategy identifying bullet points the principle is designed to promoten* Problemsn* Rationalen* Examplesn* Discussion Pointsn* Case by case topics (e.g. If service uses legacy APIs, contact TRAP: add TRAP\/Arc Triage email hyperlink)n# StatusnProposedn# ConsequencesnStakeholder groups will need to re-factor the format and structure of pattern material in a later editing phase.n# Examplen* Independently deployable servicesn* Contextn* (Link to URI for business strategy)n(see Martin Fowler) [http:\/\/martinfowler.com\/articles\/microservices.html#ComponentizationViaServices]n* Problemsn* Response to change quicklyn* Availabilityn* Scalabilityn* RationalenThis could otherwise be stated as \u201cloose coupling and high cohesion\u201d. When changes must be made, they should be achieved through independently deployable services.nLoose coupling means that a change in one service does not require a change in any other service. High cohesion means that related changes are typically made in a single location.nThis allows teams to deliver at their own speed, without being constrained by any of their consumers. Failing to do this correctly would limit our ability to deliver quickly and incrementally.n* Examplesn(TODO: MDTP Link explaining)n* Discussion PointsnNot ApplicablenTODO: Given scenario X, contact TRAP (email hyperlink).n"}
{"File Name":"paas-team-manual\/ADR010-postgres-bind-behaviour.html.md","Context":"## Context\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\nOnly the owner of a table can alter\/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\nWe attempted to work around the PostgreSQL permissions system in the following ways:\n* Using [`ALTER DEFAULT PRIVILEGES`](https:\/\/www.postgresql.org\/docs\/9.5\/static\/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https:\/\/www.postgresql.org\/docs\/9.5\/static\/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\n","Decision":"We decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\\n","tokens":470,"id":224,"text":"## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter\/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https:\/\/www.postgresql.org\/docs\/9.5\/static\/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https:\/\/www.postgresql.org\/docs\/9.5\/static\/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n\n\n##Decision\nWe decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\\n","Predictions":"We have implemented a new Security module that will allow us to create new Security fields for each user.nThis module will be called Security and will be used to create new fields for all users.n"}
{"File Name":"james\/0015-objectstorage-blobid-list.md","Context":"## Context\nA given mail is often written to the blob store by different components. And mail traffic is heavily duplicated (several recipients receiving similar email, same attachments). This causes a given blob to often be persisted several times.\nCassandra was the first implementation of the blobStore. Cassandra is a heavily write optimized NoSQL database. One can assume writes to be fast on top of Cassandra. Thus we assumed we could always overwrite blobs.\nThis usage pattern was also adopted for BlobStore on top of ObjectStorage.\nHowever writing in Object storage:\n- Takes time\n- Is billed by most cloud providers\nThus choosing a right strategy to avoid writing blob twice is desirable.\nHowever, ObjectStorage (OpenStack Swift) `exist` method was not efficient enough to be a real cost and performance saver.\n","Decision":"Rely on a StoredBlobIdsList API to know which blob is persisted or not in object storage. Provide a Cassandra implementation of it.\\nLocated in blob-api for convenience, this it not a top level API. It is intended to be used by some blobStore implementations\\n(here only ObjectStorage). We will provide a CassandraStoredBlobIdsList in blob-cassandra project so that guice products combining\\nobject storage and Cassandra can define a binding to it.\\n- When saving a blob with precomputed blobId, we can check the existence of the blob in storage, avoiding possibly the expensive \"save\".\\n- When saving a blob too big to precompute its blobId, once the blob had been streamed using a temporary random blobId, copy operation can be avoided and the temporary blob could be directly removed.\\nCassandra is probably faster doing \"write every time\" rather than \"read before write\" so we should not use the stored blob projection for it\\nSome performance tests will be run in order to evaluate the improvements.\\n","tokens":166,"id":2122,"text":"## Context\\nA given mail is often written to the blob store by different components. And mail traffic is heavily duplicated (several recipients receiving similar email, same attachments). This causes a given blob to often be persisted several times.\\nCassandra was the first implementation of the blobStore. Cassandra is a heavily write optimized NoSQL database. One can assume writes to be fast on top of Cassandra. Thus we assumed we could always overwrite blobs.\\nThis usage pattern was also adopted for BlobStore on top of ObjectStorage.\\nHowever writing in Object storage:\\n- Takes time\\n- Is billed by most cloud providers\\nThus choosing a right strategy to avoid writing blob twice is desirable.\\nHowever, ObjectStorage (OpenStack Swift) `exist` method was not efficient enough to be a real cost and performance saver.\\n\n\n##Decision\nRely on a StoredBlobIdsList API to know which blob is persisted or not in object storage. Provide a Cassandra implementation of it.\\nLocated in blob-api for convenience, this it not a top level API. It is intended to be used by some blobStore implementations\\n(here only ObjectStorage). We will provide a CassandraStoredBlobIdsList in blob-cassandra project so that guice products combining\\nobject storage and Cassandra can define a binding to it.\\n- When saving a blob with precomputed blobId, we can check the existence of the blob in storage, avoiding possibly the expensive \"save\".\\n- When saving a blob too big to precompute its blobId, once the blob had been streamed using a temporary random blobId, copy operation can be avoided and the temporary blob could be directly removed.\\nCassandra is probably faster doing \"write every time\" rather than \"read before write\" so we should not use the stored blob projection for it\\nSome performance tests will be run in order to evaluate the improvements.\\n","Predictions":"Rely on a StoredBlobIdsList API to know which blob is persisted or not in object storage. Provide a Cassandra implementation of it.nLocated in blob-api for convenience, this it not a top level API. It is intended to be used by some blobStore implementationsn(here only ObjectStorage). We will provide a CassandraStoredBlobIdsList in blob-cassandra project so that guice products combiningnobject storage and Cassandra can define a binding to it.n- When saving a blob with precomputed blobId, we can check the existence of the blob in storage, avoiding possibly the expensive \"save\".n- When saving a blob too big to precompute its blobId, once the blob had been streamed using a temporary random blobId, copy operation can be avoided and the temporary blob could be directly removed.nCassandra is probably faster doing \"write every time\" rather than \"read before write\" so we should not use the stored blob projection.n"}
{"File Name":"qc-atlas\/0002-monorepo.md","Context":"## Context and Problem Statement\nShould the components in the PlanQK platform be splitted into individual repos?\n","Decision":"Start with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\\n### Positive Consequences <!-- optional -->\\n* Recommended approach by [Martin Fowler](https:\/\/martinfowler.com\/bliki\/MonolithFirst.html)\\n","tokens":22,"id":697,"text":"## Context and Problem Statement\\nShould the components in the PlanQK platform be splitted into individual repos?\\n\n\n##Decision\nStart with a monorepo, split up later if needed. Let FOCUS decide what is best for their semantic knowledge graph component.\\n### Positive Consequences <!-- optional -->\\n* Recommended approach by [Martin Fowler](https:\/\/martinfowler.com\/bliki\/MonolithFirst.html)\\n","Predictions":"We will use the [Semantic Knowledge graph](https:\/\/semantic-knowledge-graph.org\/) library to provide a way to define a custom iFrame.n"}
{"File Name":"ELEN4010\/ADR Login API Choice.md","Context":"## Context\nIt would be convenient to use the Google Login API as an alternative method for users to login. This would provide a template for our own login details stored in the DB, as well as a quick way to get the Sprint 1 User story related to login completed ASAP.\n","Decision":"Using a well known and widely known\/supported login mechanism such as Google's OAuth2 will allow more rapid development of an appropriate security setup for the site. We will apply for an API key and start implementing the login\/registration page through the Google Login API\\n","tokens":58,"id":61,"text":"## Context\\nIt would be convenient to use the Google Login API as an alternative method for users to login. This would provide a template for our own login details stored in the DB, as well as a quick way to get the Sprint 1 User story related to login completed ASAP.\\n\n\n##Decision\nUsing a well known and widely known\/supported login mechanism such as Google's OAuth2 will allow more rapid development of an appropriate security setup for the site. We will apply for an API key and start implementing the login\/registration page through the Google Login API\\n","Predictions":"We will use a monolog\/monolog library to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog\/monolog to provide configurable logging levels throughout the application.nWe will use monolog"}
{"File Name":"react-template\/0014-use-dependabot-to-keep-dependencies-up-to-date.md","Context":"## Context\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\n","Decision":"We will use Dependabot to monitor dependency updates.\\n","tokens":38,"id":3328,"text":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n\n\n##Decision\nWe will use Dependabot to monitor dependency updates.\\n","Predictions":"We will use Dependabot to monitor dependency updates.n"}
{"File Name":"titania-os\/0005-networkmanager-for-networking.md","Context":"## Context\nSystemD provides native tools for managing the network connections. NetworkManager is an alternative service that provides more features and can be controlled and queried with a DBus interface. Wicd is another alternative, but is rarely used in embedded applications. There are several other options, but they are not feature full compared to NetworkManager.\n","Decision":"We will disable systemd network capability and use NetworkManager for managing the networks (duh!).\\n","tokens":69,"id":2574,"text":"## Context\\nSystemD provides native tools for managing the network connections. NetworkManager is an alternative service that provides more features and can be controlled and queried with a DBus interface. Wicd is another alternative, but is rarely used in embedded applications. There are several other options, but they are not feature full compared to NetworkManager.\\n\n\n##Decision\nWe will disable systemd network capability and use NetworkManager for managing the networks (duh!).\\n","Predictions":"We will disable systemd network capability and use NetworkManager for managing the networks (duh!).n"}
{"File Name":"super-eks\/0007-backup.md","Context":"## Context and Problem Statement\nIn the current setup of super eks there is no backup solution installed and configured. A backup solution can be helpful for some resources that are not managed by IaC.\n","Decision":"### Positive Consequences\\nUsers get a state of the art backup technology included with super eks, enabling them to restore a cluster after a\\ncatastropic failure.\\n### Negative Consequences\\nUsers have to think about how the backup needs to be setup and distinguish between resources that have been set up by\\nIaC and manually set up resources (e.g., secrets).\\n","tokens":41,"id":2034,"text":"## Context and Problem Statement\\nIn the current setup of super eks there is no backup solution installed and configured. A backup solution can be helpful for some resources that are not managed by IaC.\\n\n\n##Decision\n### Positive Consequences\\nUsers get a state of the art backup technology included with super eks, enabling them to restore a cluster after a\\ncatastropic failure.\\n### Negative Consequences\\nUsers have to think about how the backup needs to be setup and distinguish between resources that have been set up by\\nIaC and manually set up resources (e.g., secrets).\\n","Predictions":"We will use a stack database to store the settings.n"}
{"File Name":"openchs-adr\/0006-use-row-level-security-in-postgres-to-implement-multitenancy.md","Context":"## Context\nMultitenancy is required because we are now deploying openchs-server on the cloud. This can be achieved by\n1. Multiple databases\n2. Multiple schemas\n3. Same database with a discriminator column\n### Constraints\n1. An organisation should not be able to view or update another organisation's data\n2. There is common metadata for forms, concepts that organisations can override\n### Other concerns\/requirements\n1. The architecture should prevent mistakes in code to leak data from one organisation to another\n2. Maintaining multiple copies of metadata (if using multiple dbs\/schemas) is hard\n3. Data per organisation is low\n","Decision":"- Use discriminators to achieve multitenancy.\\n- Enforce multitenancy using Postgres RLS policies\\n- Create a hierarchical organisation structure where organisations can read metadata from current or parent organisations, and read\/write data for their own organisation\\n- Use OpenCHS as the grandparent organisation. It will hold the default program metadata\\n","tokens":131,"id":2828,"text":"## Context\\nMultitenancy is required because we are now deploying openchs-server on the cloud. This can be achieved by\\n1. Multiple databases\\n2. Multiple schemas\\n3. Same database with a discriminator column\\n### Constraints\\n1. An organisation should not be able to view or update another organisation's data\\n2. There is common metadata for forms, concepts that organisations can override\\n### Other concerns\/requirements\\n1. The architecture should prevent mistakes in code to leak data from one organisation to another\\n2. Maintaining multiple copies of metadata (if using multiple dbs\/schemas) is hard\\n3. Data per organisation is low\\n\n\n##Decision\n- Use discriminators to achieve multitenancy.\\n- Enforce multitenancy using Postgres RLS policies\\n- Create a hierarchical organisation structure where organisations can read metadata from current or parent organisations, and read\/write data for their own organisation\\n- Use OpenCHS as the grandparent organisation. It will hold the default program metadata\\n","Predictions":"* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availabilityn* Availability"}
{"File Name":"slingshot\/0003-use-one-geoserver-instance-with-multiple-workspaces.md","Context":"## Context\nThere are a few problems that need to be solved around handling authentication with GeoServer:\n1. We have some layers which can only be accessed by authenticated users and some that are publicly accessible.\n2. The user never interacts directly with GeoServer. Requests to GeoServer for data come from the user's browser either through `img` tags or through AJAX requests from Leaflet. This means the user needs to be preauthenticated with GeoServer in some way.\n3. GeoServer's authentication is incredibly difficult to work with.\nIn the past, we solved these problems by leveraging the SSO capabilities of Shibboleth and simply running two GeoServer instances--one that was publicly accessible and one that was behind Shibboleth. Touchstone authentication happened for the user through the Rails application. Given the decision to move away from Shibboleth this is no longer an option. Running two GeoServer instances also adds additional burdens to deployment, management and the data publication process.\n","Decision":"Run a single GeoServer instance. GeoServer supports what it calls workspaces, which is just a way of partitioning layers within the system. The authentication for each workspace can be configured separately. Configure one workspace to be readable by anyone and one workspace to require Basic Auth using a predefined username and password.\\nAccess to this GeoServer instance will need to be proxied. The proxy (our Geoblacklight instance) will handle SAML authentication and augment the proxy request to GeoServer with the Basic Auth login if the user has authenticated with Geoblacklight.\\n","tokens":200,"id":2578,"text":"## Context\\nThere are a few problems that need to be solved around handling authentication with GeoServer:\\n1. We have some layers which can only be accessed by authenticated users and some that are publicly accessible.\\n2. The user never interacts directly with GeoServer. Requests to GeoServer for data come from the user's browser either through `img` tags or through AJAX requests from Leaflet. This means the user needs to be preauthenticated with GeoServer in some way.\\n3. GeoServer's authentication is incredibly difficult to work with.\\nIn the past, we solved these problems by leveraging the SSO capabilities of Shibboleth and simply running two GeoServer instances--one that was publicly accessible and one that was behind Shibboleth. Touchstone authentication happened for the user through the Rails application. Given the decision to move away from Shibboleth this is no longer an option. Running two GeoServer instances also adds additional burdens to deployment, management and the data publication process.\\n\n\n##Decision\nRun a single GeoServer instance. GeoServer supports what it calls workspaces, which is just a way of partitioning layers within the system. The authentication for each workspace can be configured separately. Configure one workspace to be readable by anyone and one workspace to require Basic Auth using a predefined username and password.\\nAccess to this GeoServer instance will need to be proxied. The proxy (our Geoblacklight instance) will handle SAML authentication and augment the proxy request to GeoServer with the Basic Auth login if the user has authenticated with Geoblacklight.\\n","Predictions":"We will use a sample env vars to store the env values for the environment variables.n"}
{"File Name":"adr-viewer\/0005-distinguish-amendments-to-records-with-colour.md","Context":"## Context\nArchitecture Decision Records may be `amended` rather than `superseded` if e.g. only a small part of the decision changes.\n","Decision":"Amended records, although not officially supported as a distinct flag in `adr-tools`, should be distinguished from records that are either Accepted or Superseded by.\\n","tokens":33,"id":3813,"text":"## Context\\nArchitecture Decision Records may be `amended` rather than `superseded` if e.g. only a small part of the decision changes.\\n\n\n##Decision\nAmended records, although not officially supported as a distinct flag in `adr-tools`, should be distinguished from records that are either Accepted or Superseded by.\\n","Predictions":"We will use the ADR_007 flag to indicate that the architecture decision is amended.n"}
{"File Name":"react-library-template\/0007-use-dependabot-to-keep-dependencies-up-to-date.md","Context":"## Context\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\n","Decision":"We will use Dependabot to monitor dependency updates.\\n","tokens":38,"id":4225,"text":"## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n\n\n##Decision\nWe will use Dependabot to monitor dependency updates.\\n","Predictions":"We will use Dependabot to monitor dependency updates.n"}
{"File Name":"generator-latex-template\/0005-custom-example-environment.md","Context":"## Context and Problem Statement\nWe want to present LaTeX examples.\nThey should be shown both as LaTeX source and as rendered output.\n","Decision":"* Write once - have two outputs (source and rendered)\\n* Good interaction with all pacakges\\n* Should support floats\\n* Should support compilation without special configuration (such as `--shell-escape`)\\nChosen option: \"custom solution\", because resolves all forces.\\n### Positive Consequences\\n* Works with floats and umlauts\\n### Negative Consequences\\n* A little bit more than minimal code is presented to the user\\n","tokens":28,"id":2337,"text":"## Context and Problem Statement\\nWe want to present LaTeX examples.\\nThey should be shown both as LaTeX source and as rendered output.\\n\n\n##Decision\n* Write once - have two outputs (source and rendered)\\n* Good interaction with all pacakges\\n* Should support floats\\n* Should support compilation without special configuration (such as `--shell-escape`)\\nChosen option: \"custom solution\", because resolves all forces.\\n### Positive Consequences\\n* Works with floats and umlauts\\n### Negative Consequences\\n* A little bit more than minimal code is presented to the user\\n","Predictions":"We will use a stack file to store the diagrams.n"}
{"File Name":"buy-for-your-school\/0006-use-dotenv-for-managing-environment-variables.md","Context":"## Context\nAccessing ENV directly without a wrapper is limited and can introduce problems.\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\n","Decision":"Use DotEnv to load our environment variables.\\n","tokens":324,"id":1254,"text":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n\n\n##Decision\nUse DotEnv to load our environment variables.\\n","Predictions":"Instead of adding nil guards throughout the codebase for required environment variables (eg. ENV.fetch('FOO', 'default'), this should be managed centrally.nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg.  allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. allow(Figaro).to receive(:env).with(:foo).and_return(\"1234\")). We should then consider how we can stub environment variables.n"}
{"File Name":"verify-service-provider\/0023-we-will-report-the-version-in-a-saml-extension.md","Context":"## Context\nThe verify-service-provider will be deployed to a number of relying parties.\nWhich relying party is using which version could potentially become difficult\nto keep track of.\nTo make it as easy as possible for us to manage this we'd like the\nverify-service-provider to report its version in some way.\nBecause the verify-service-provider is not intended to be accessible to the\ninternet we can't simply expose an endpoint that reports the version number.\nAlso, since the SAML messages go via the browser we can't use a custom HTTP\nheader.\nThere's also a concern about the security implications of reporting a version\nnumber in cleartext.\nWe considered a couple of options:\n- Requesting metadata from Verify with a custom user-agent string\n- Sending the version in an unencrypted saml extension\n- Sending the version in an encrypted saml extension\n","Decision":"We decided to send the version number in the SAML AuthnRequests as an encrypted\\nSAML extension. The XML will look roughly like this:\\n```\\n<saml:AuthnRequest>\\n<saml:Issuer>...<\/saml:Issuer>\\n<saml:Signature>...<\/saml:Signature>\\n<saml:Extensions>\\n<saml:EncryptedAttribute>...<\/saml:EncryptedAttribute>\\n<\/saml:Extensions>\\n<\/saml:AuthnRequest>\\n```\\nOnce decrypted, the Attribute in the Extensions will look like:\\n```\\n<saml:Attribute Name=\"Versions\">\\n<saml:AttributeValue xsi:type=\"metrics:VersionsType\">\\n<metrics:ApplicationVersion>3.4.1<\/metrics:ApplicationVersion>\\n<\/saml:AttributeValue>\\n<\/saml:Attribute>\\n```\\n","tokens":182,"id":4436,"text":"## Context\\nThe verify-service-provider will be deployed to a number of relying parties.\\nWhich relying party is using which version could potentially become difficult\\nto keep track of.\\nTo make it as easy as possible for us to manage this we'd like the\\nverify-service-provider to report its version in some way.\\nBecause the verify-service-provider is not intended to be accessible to the\\ninternet we can't simply expose an endpoint that reports the version number.\\nAlso, since the SAML messages go via the browser we can't use a custom HTTP\\nheader.\\nThere's also a concern about the security implications of reporting a version\\nnumber in cleartext.\\nWe considered a couple of options:\\n- Requesting metadata from Verify with a custom user-agent string\\n- Sending the version in an unencrypted saml extension\\n- Sending the version in an encrypted saml extension\\n\n\n##Decision\nWe decided to send the version number in the SAML AuthnRequests as an encrypted\\nSAML extension. The XML will look roughly like this:\\n```\\n<saml:AuthnRequest>\\n<saml:Issuer>...<\/saml:Issuer>\\n<saml:Signature>...<\/saml:Signature>\\n<saml:Extensions>\\n<saml:EncryptedAttribute>...<\/saml:EncryptedAttribute>\\n<\/saml:Extensions>\\n<\/saml:AuthnRequest>\\n```\\nOnce decrypted, the Attribute in the Extensions will look like:\\n```\\n<saml:Attribute Name=\"Versions\">\\n<saml:AttributeValue xsi:type=\"metrics:VersionsType\">\\n<metrics:ApplicationVersion>3.4.1<\/metrics:ApplicationVersion>\\n<\/saml:AttributeValue>\\n<\/saml:Attribute>\\n```\\n","Predictions":"We decided to send the version number in the SAML AuthnRequests as an encryptednSAML extension. The XML will look roughly like this:nsaml:AuthnRequest>nsaml:Issuer>...\/saml:Issuer>nsaml:Signature>...\/saml:Signature>n\/saml:Extensions>n\/saml:AuthnRequest>nOnce decrypted, the Attribute in the Extensions will look like:nsaml:Attribute Name=\"Versions\">nsaml:AttributeValue xsi:type=\"metrics:VersionsType\">nmetrics:ApplicationVersion>3.4.1\/metrics:ApplicationVersion>nOnce decrypted, the Attribute in the Extensions will look like:nnsaml:Attribute Name=\"Versions\">nsaml:AttributeValue xsi:type=\"metrics:VersionsType\">nmetrics:ApplicationVersion>3.4.1\/metrics:ApplicationVersionnnOnce decrypted, the Attribute in the Extensions will look like:nnsaml:Attribute Name=\"Versions\">nsaml:AttributeValue xsi:type=\"metrics:VersionsType\">nmetrics:ApplicationVersion>3.4.1\/metrics:ApplicationVersionnnOnce decrypted, the Attribute in the Extensions will look like:nnsaml:Attribute Name=\"Versions\">nsaml:AttributeValue xsi:type=\"metrics:VersionsType\">nmetrics:ApplicationVersion>3.4.1\/metrics:ApplicationVersionnnOnce decrypted, the Attribute in the Extensions will look like:nnsaml:Attribute Name=\"Versions\">nsaml:AttributeValue xsi:type=\"metrics:VersionsType\">nmetrics:ApplicationVersion>3.4.1\/metrics:ApplicationVersionnnOnce decrypted, the Attribute in the Extensions will look like:nnsaml:Attribute Name=\"Versions\">nsaml:AttributeValue xsi:type=\"metrics:VersionsType\">nmetrics:ApplicationVersion>3.4.1\/metrics:ApplicationVersionnnOnce decrypted, the Attribute in the Extensions will look like:nnsaml:Attribute Name=\"Versions\">nsaml:AttributeValue xsi:type=\"metrics:VersionsType\">nmetrics:ApplicationVersion>3.4.1\/metrics:ApplicationVersionnnOnce decrypted, the Attribute in the Extensions will look like:nsaml:Attribute Name=\"Versions\">nsaml:AttributeValue xsi:type=\"metrics:VersionsType\">nmetrics:ApplicationVersion>3.4.1\/metrics:ApplicationVersionnnOnce decrypted, the Attribute in the Extensions will look like:nsaml:Attribute Name=\"Versions\">nsaml:AttributeValue xsi:type=\"metrics:VersionsType\">nnOnce decrypted, the Attribut"}
{"File Name":"lbh-adrs\/Microfrontend-Composition.md","Context":"## **Context**\nAccording to the [Microfrontend-Architecture ADR](https:\/\/github.com\/LBHackney-IT\/lbh-adrs\/blob\/feature\/create-ADRs\/Platform\/Accepted\/Microfrontend-Architecture.md), we are going to define micro-frontends by using a vertical splitting.\nIn this way each micro-frontends has got its repository and each team manages the development and the deployment of it.\nAbout the client-side micro-frontends compositions, there are two options to implement that:\n1. Writing our own boilerplate code\nEach micro frontend is included in the html page using a \\<script> tag and each of those applications exposes a global function as its entry-point. The container application determines which frontend should be mounted and calls the relevant function to tell a micro frontend when and where to render itself.\n2. Using a framework as Single SPA (https:\/\/single-spa.js.org\/ )\nSingle SPA is a Javascript framework for frontend microservices. In practice, it applies a lifecycle to every application. Each app can respond to url routing events and must know how to bootstrap, mount and unmount itself from the DOM.\n","Decision":"**Using Single SPA framework**\\nThis framework adopts the same principle as we would implement our own boilerplate code, but the advantage is that we don\u2019t have to build and document our own logic but instead we use this light framework supported by an open source community.\\nThe other advantage is that the framework allows to compose also applications written in Angualar and Vue.js\\n","tokens":242,"id":2317,"text":"## **Context**\\nAccording to the [Microfrontend-Architecture ADR](https:\/\/github.com\/LBHackney-IT\/lbh-adrs\/blob\/feature\/create-ADRs\/Platform\/Accepted\/Microfrontend-Architecture.md), we are going to define micro-frontends by using a vertical splitting.\\nIn this way each micro-frontends has got its repository and each team manages the development and the deployment of it.\\nAbout the client-side micro-frontends compositions, there are two options to implement that:\\n1. Writing our own boilerplate code\\nEach micro frontend is included in the html page using a \\<script> tag and each of those applications exposes a global function as its entry-point. The container application determines which frontend should be mounted and calls the relevant function to tell a micro frontend when and where to render itself.\\n2. Using a framework as Single SPA (https:\/\/single-spa.js.org\/ )\\nSingle SPA is a Javascript framework for frontend microservices. In practice, it applies a lifecycle to every application. Each app can respond to url routing events and must know how to bootstrap, mount and unmount itself from the DOM.\\n\n\n##Decision\n**Using Single SPA framework**\\nThis framework adopts the same principle as we would implement our own boilerplate code, but the advantage is that we don\u2019t have to build and document our own logic but instead we use this light framework supported by an open source community.\\nThe other advantage is that the framework allows to compose also applications written in Angualar and Vue.js\\n","Predictions":"We will use the [Semantic Data Structure](https:\/\/semantic-datastructure.org\/) as the data structure for the workflow resource attribute.n"}
{"File Name":"beis-report-official-development-assistance\/0031-replace-publicactivity-with-our-own-hand-rolled-way-of-recording-changes.md","Context":"## Context\nWe adopted the `PublicActivity` gem (as documented in [0019](https:\/\/github.com\/UKGovernmentBEIS\/beis-report-official-development-assistance\/blob\/731b91f5773c4306f919433af81976d29c5feef1\/doc\/architecture\/decisions\/0019-use-public-activity-gem.md)), but the information we record has never been used, or displayed to the user.\nNow we're looking at surfacing information about changes to end users it's becoming increasingly clear that is `PublicActivity` is not fit for our needs, and would be easier to build our own approach to logging what changed were made by what user.\n","Decision":"We have decided to introduce a `HistoricalEvent` model to record changes to models, starting with `Activity`s.\\nOnce we have applied `HistoricalEvents` to all the models we need to track, we can remove all the calls to `PublicActivity` and remove the gem entirely.\\n","tokens":145,"id":2403,"text":"## Context\\nWe adopted the `PublicActivity` gem (as documented in [0019](https:\/\/github.com\/UKGovernmentBEIS\/beis-report-official-development-assistance\/blob\/731b91f5773c4306f919433af81976d29c5feef1\/doc\/architecture\/decisions\/0019-use-public-activity-gem.md)), but the information we record has never been used, or displayed to the user.\\nNow we're looking at surfacing information about changes to end users it's becoming increasingly clear that is `PublicActivity` is not fit for our needs, and would be easier to build our own approach to logging what changed were made by what user.\\n\n\n##Decision\nWe have decided to introduce a `HistoricalEvent` model to record changes to models, starting with `Activity`s.\\nOnce we have applied `HistoricalEvents` to all the models we need to track, we can remove all the calls to `PublicActivity` and remove the gem entirely.\\n","Predictions":"We have decided to use a Security based on the [Security & Security Policy](https:\/\/github.com\/UKGovernmentBEIS\/beis-report-official-development-assistance\/blob\/731b91f5773c4306f919433af81976d29c5feef1\/doc\/architecture\/decisions\/0019-use-security-and-security-policy.md) approach to manage the security of the Cloud.gov organization.n"}
{"File Name":"embvm-core\/0014-refactor-driver-interfaces-to-use-namespaces.md","Context":"## Context\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\n```\nstruct tof\n{\nusing distance_t = uint16_t;\nusing cb_t = stdext::inplace_function<void(distance_t)>;\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\nenum class mode\n{\ndefaultRange = 0,\nshortRange,\nmedRange,\nlongRange,\n};\n}''\n```\nInterface classes would inherit from these structs:\n```\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\n```\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\n```\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\n```\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\n","Decision":"Each driver interface class and any related types are to be stored in a separate namespace per driver type. This keeps related types tied together, improves our generated documentation, and provides improved names for classes and types.\\n","tokens":225,"id":3027,"text":"## Context\\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\\n```\\nstruct tof\\n{\\nusing distance_t = uint16_t;\\nusing cb_t = stdext::inplace_function<void(distance_t)>;\\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\\nenum class mode\\n{\\ndefaultRange = 0,\\nshortRange,\\nmedRange,\\nlongRange,\\n};\\n}''\\n```\\nInterface classes would inherit from these structs:\\n```\\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\\n```\\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\\n```\\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\\n```\\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\\n\n\n##Decision\nEach driver interface class and any related types are to be stored in a separate namespace per driver type. This keeps related types tied together, improves our generated documentation, and provides improved names for classes and types.\\n","Predictions":"We will store driver interface classes and any related types in a separate namespace per driver type. This keeps related types tied together, improves our generated documentation, and provides improved names for classes and types.n"}
{"File Name":"openlobby-server\/0005-python.md","Context":"## Context\nWe need to choose main programming language for this project.\n","Decision":"I'm now fluent in writing Python and JavaScript. Both languages are widely used\\nthese days and may attract developers to join this project.\\nPython is more mature with more stable libs for writing server side code and\\ntesting. JS may have better libraries for writing GraphQL APIs because\\nGraphQL originated in JS ecosystem.\\nI'm doing this in free time and for fun. I want to focus on project's features\\nand not to waste a time with poorly designed language lacking mature and\\nstable libraries. So for obvious reasons I'm choosing Python.\\n","tokens":15,"id":472,"text":"## Context\\nWe need to choose main programming language for this project.\\n\n\n##Decision\nI'm now fluent in writing Python and JavaScript. Both languages are widely used\\nthese days and may attract developers to join this project.\\nPython is more mature with more stable libs for writing server side code and\\ntesting. JS may have better libraries for writing GraphQL APIs because\\nGraphQL originated in JS ecosystem.\\nI'm doing this in free time and for fun. I want to focus on project's features\\nand not to waste a time with poorly designed language lacking mature and\\nstable libraries. So for obvious reasons I'm choosing Python.\\n","Predictions":"We will use stack to build the application.n"}
{"File Name":"reactive-interaction-gateway\/0003-for-config-prefer-prefix-over-nesting-and-don-t-hide-defaults-in-code.md","Context":"## Context\nThere no agreed-upon way of handling application configuration. There are two issues this ADR aims to address.\n### Issue 1: Nested keys\nIn `config.exs`, configuration keys can be nested by using a Keyword as value. Unfortunately, there is no built-in support for this nesting. For example:\n```\nApplication.fetch_env!(:rig, :required_key)\n```\nwill show a nice error, while\n```\nApplication.fetch_env!(:rig, :required_key)[:required_subkey]\n```\nwill simply return nil in case `:required_key` is present but `:required_subkey` is not.\n### Issue 2: Default values\nSome default values are defined where they are needed in the code, which is a problem once a key is used more than once. Also, defaults are no easily inspectable.\n","Decision":"* We prefer not to nest configuration keys; instead, we prefix them where it makes sense. For example, instead of `config :rig, :kafka, client_id: :rig_brod_client` we write `config :rig, kafka_client_id: :rig_brod_client`. This allows us to leverage the built-in methods (e.g., `Application.fetch_env!`), which produce sensible error messages in case required values are not defined.\\n* We always set default values in config.exs (in turn this means that we prefer `Application.fetch_env!` over `Application.get_env`). This way, it is easy to reason about default values, and using a default value in more than one place is not an issue.\\n","tokens":182,"id":5227,"text":"## Context\\nThere no agreed-upon way of handling application configuration. There are two issues this ADR aims to address.\\n### Issue 1: Nested keys\\nIn `config.exs`, configuration keys can be nested by using a Keyword as value. Unfortunately, there is no built-in support for this nesting. For example:\\n```\\nApplication.fetch_env!(:rig, :required_key)\\n```\\nwill show a nice error, while\\n```\\nApplication.fetch_env!(:rig, :required_key)[:required_subkey]\\n```\\nwill simply return nil in case `:required_key` is present but `:required_subkey` is not.\\n### Issue 2: Default values\\nSome default values are defined where they are needed in the code, which is a problem once a key is used more than once. Also, defaults are no easily inspectable.\\n\n\n##Decision\n* We prefer not to nest configuration keys; instead, we prefix them where it makes sense. For example, instead of `config :rig, :kafka, client_id: :rig_brod_client` we write `config :rig, kafka_client_id: :rig_brod_client`. This allows us to leverage the built-in methods (e.g., `Application.fetch_env!`), which produce sensible error messages in case required values are not defined.\\n* We always set default values in config.exs (in turn this means that we prefer `Application.fetch_env!` over `Application.get_env`). This way, it is easy to reason about default values, and using a default value in more than one place is not an issue.\\n","Predictions":"* We prefer not to nest configuration keys; instead, we prefix them where it makes sense. For example, instead of config :rig, :kafka, client_id: :rig_brod_client we write config :rig, kafka_client_id: :rig_brod_client. This allows us to leverage the built-in methods (e.g., Application.fetch_env!), which produce sensible error messages in case required values are not defined.n* We always set default values in config.exs (in turn this means that we prefer Application.fetch_env! over Application.get_env). This way, it is easy to reason about default values, and using a default value in more than one place is not an issue.n"}
{"File Name":"opg-use-an-lpa\/0002-persistent-storage-uses-dynamodb.md","Context":"## Context\n* The application will require persistent storage for storing LPA ownership, granted access, and possibly user credentials\n* Current MoJ strategy is to use managed services where possible\n","Decision":"Use DynamoDB for persistent storage\\n","tokens":37,"id":4841,"text":"## Context\\n* The application will require persistent storage for storing LPA ownership, granted access, and possibly user credentials\\n* Current MoJ strategy is to use managed services where possible\\n\n\n##Decision\nUse DynamoDB for persistent storage\\n","Predictions":"Use DynamoDB for persistent storagen"}
{"File Name":"skunkworks\/state-management.md","Context":"## Context\nTypes of State:\n- Local State\n- e.g. React `useState` Hook\n- Shared State\n- e.g. React Context, Redux\n- Remote State (i.e. API)\n- e.g. React Context, Redux\nShared State Options:\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\n- Drawbacks\n- Boilerplate\n- React Context\n- Drawbacks\n- \"Provider Hell\"\n- Unintentional Re-Renders\n- `constate` \/ `recoil` \/ `react-tracked`\nRemote State Options:\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\n- Drawbacks\n- Boilerplate\n- React Context\n- Drawbacks\n- \"Provider Hell\"\n- Unintentional Re-Renders\n- `constate` \/ `recoil` \/ `react-tracked` \/ `@tanstack\/react-query`\n","Decision":"- Local State: React `useState` Hook\\n- Shared State: `recoil`\\n- Remote State: `@tanstack\/react-query`\\n","tokens":208,"id":2298,"text":"## Context\\nTypes of State:\\n- Local State\\n- e.g. React `useState` Hook\\n- Shared State\\n- e.g. React Context, Redux\\n- Remote State (i.e. API)\\n- e.g. React Context, Redux\\nShared State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked`\\nRemote State Options:\\n- `redux` + `redux-observable` + `rxjs` + `react-redux`\\n- Drawbacks\\n- Boilerplate\\n- React Context\\n- Drawbacks\\n- \"Provider Hell\"\\n- Unintentional Re-Renders\\n- `constate` \/ `recoil` \/ `react-tracked` \/ `@tanstack\/react-query`\\n\n\n##Decision\n- Local State: React `useState` Hook\\n- Shared State: `recoil`\\n- Remote State: `@tanstack\/react-query`\\n","Predictions":"- Local State: React useState Hookn- Shared State: recoiln- Remote State: @tanstack\/react-queryn"}
{"File Name":"buddy-auth.jwt-oidc\/adr-001.md","Context":"# Context #\nThe library currently assumes that the ID tokens are signed by a\npublic key (and the code unconditionally tries to build the actual\npublic key from the data we get from the ID Token issuer).\nAt the same time, the OpenID Connect Core 1.0 specification states\nthat:\n> \"Symmetric signatures MUST NOT be used by public (non-confidential)\n> Clients because of their inability to keep secrets\"\n(see https:\/\/openid.net\/specs\/openid-connect-core-1_0.html#Signing).\nAs we don't expect to support confidential clients, it seems safe to\nignore Symmetric Keys when validating tokens.\n# Decision #\nTokens signed with a Symmetric Key will always be marked as invalid\n(whether they are actually valid or not).\n# Status #\nAccepted.\n# Consequences #\nEven if the ID Token issuer is configured to use Symmetric keys to\nsign its tokens, and the library gets a perfectly valid token signed\nusing one of such keys, the library will always return that the token\nis invalid.\n","Decision":"Tokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n","tokens":229,"id":3872,"text":"# Context #\\nThe library currently assumes that the ID tokens are signed by a\\npublic key (and the code unconditionally tries to build the actual\\npublic key from the data we get from the ID Token issuer).\\nAt the same time, the OpenID Connect Core 1.0 specification states\\nthat:\\n> \"Symmetric signatures MUST NOT be used by public (non-confidential)\\n> Clients because of their inability to keep secrets\"\\n(see https:\/\/openid.net\/specs\/openid-connect-core-1_0.html#Signing).\\nAs we don't expect to support confidential clients, it seems safe to\\nignore Symmetric Keys when validating tokens.\\n# Decision #\\nTokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n\n\n##Decision\nTokens signed with a Symmetric Key will always be marked as invalid\\n(whether they are actually valid or not).\\n# Status #\\nAccepted.\\n# Consequences #\\nEven if the ID Token issuer is configured to use Symmetric keys to\\nsign its tokens, and the library gets a perfectly valid token signed\\nusing one of such keys, the library will always return that the token\\nis invalid.\\n","Predictions":"The Java server will be removed from James.n"}
{"File Name":"GSoC-Braitenberg-Vehicles\/use_javafx_for_animation.md","Context":"Context\n-------\nAfter a while writing the project code, I've realized that it needs constant and probably heavyweight rendering.\nDecision\n--------\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\nRationalization\n---------------\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\nJavaFX seems to be a nice opportunity with relatively little to learn.\n","Decision":"--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n","tokens":101,"id":1400,"text":"Context\\n-------\\nAfter a while writing the project code, I've realized that it needs constant and probably heavyweight rendering.\\nDecision\\n--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n\n\n##Decision\n--------\\nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.\\nRationalization\\n---------------\\nAlthough there are probably better frameworks\/engines (e.g. Unity, OpenGL), configuring and learning to use them would probably take time I don't have.\\nJavaFX seems to be a nice opportunity with relatively little to learn.\\n","Predictions":"--------nLeave with JavaFX\/TornadoFX as constant rendering\/animation framework.nRationalizationn---------------nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.n### StatusnAcceptedn### ConsequencesnFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the app.n"}
{"File Name":"paas-team-manual\/ADR018-rds-broker-restore-last-operation.html.md","Context":"## Context\nWe use a completely stateless implementation for the RDS broker, as described in [ADR006](..\/ADR006-rds-broker).\nSo all the asynchronous operations on RDS instances were relying on executing a unique operation on AWS API, and querying the RDS instance status reported the AWS API.\nBut to implement the feature of restore from snapshot, we must execute several operations sequentially.\nThe broker must:\n1. Start the restore from snapshot, which can take minutes.\n2. Once finish, update several parameters of the instance (security groups, parameters, etc).\n3. Once that is finish, reset the master password of the RDS instance.\n4. Finally reset the passwords of the users previously bind in the original DB.\nAs the create operation is a asynchronous operation, the Cloud Controller API will periodically request the `LastOperation` endpoint to query the state of the restored instance.\nThe rds-broker must respond accordingly.\nThe Cloud Controller API includes logic to ensure the resiliance of a service creation, maintaining the workers that will poll the `LastOperation` until the service is created or there is timeout.\nTo implement this kind logic, some kind of state must be kept to track the changes on the instance.  Options are:\n* run a background house-keeping routine. This house-keeping should be resilient to rds-broker restarts and able to work with multiple rds-broker instances..\n* Use SNS and SQS, by subscribing to the [AWS events from RDS](http:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/UserGuide\/USER_Events.html). This requires a lot of additional work and integration effort.\n* Store the state in some database or k\/v store.\n","Decision":"We decided:\\n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/Using_Tags.html) of the instance.\\nWe will add a list of tags for each pending operations to execute.\\n* We make use of `LastOperation` to check the pending operations and perform them, to finally delete the corresponding tag to mark it as done.\\nWe assume that:\\n* all the required operations are either asynchronous in the AWS API (eg. update instance) or quick to execute (e.g. reset bind user passwords)\\n* that update the tags is atomic and synchronous.\\n","tokens":360,"id":207,"text":"## Context\\nWe use a completely stateless implementation for the RDS broker, as described in [ADR006](..\/ADR006-rds-broker).\\nSo all the asynchronous operations on RDS instances were relying on executing a unique operation on AWS API, and querying the RDS instance status reported the AWS API.\\nBut to implement the feature of restore from snapshot, we must execute several operations sequentially.\\nThe broker must:\\n1. Start the restore from snapshot, which can take minutes.\\n2. Once finish, update several parameters of the instance (security groups, parameters, etc).\\n3. Once that is finish, reset the master password of the RDS instance.\\n4. Finally reset the passwords of the users previously bind in the original DB.\\nAs the create operation is a asynchronous operation, the Cloud Controller API will periodically request the `LastOperation` endpoint to query the state of the restored instance.\\nThe rds-broker must respond accordingly.\\nThe Cloud Controller API includes logic to ensure the resiliance of a service creation, maintaining the workers that will poll the `LastOperation` until the service is created or there is timeout.\\nTo implement this kind logic, some kind of state must be kept to track the changes on the instance.  Options are:\\n* run a background house-keeping routine. This house-keeping should be resilient to rds-broker restarts and able to work with multiple rds-broker instances..\\n* Use SNS and SQS, by subscribing to the [AWS events from RDS](http:\/\/docs.aws.amazon.com\/AmazonRDS\/latest\/UserGuide\/USER_Events.html). This requires a lot of additional work and integration effort.\\n* Store the state in some database or k\/v store.\\n\n\n##Decision\nWe decided:\\n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/Using_Tags.html) of the instance.\\nWe will add a list of tags for each pending operations to execute.\\n* We make use of `LastOperation` to check the pending operations and perform them, to finally delete the corresponding tag to mark it as done.\\nWe assume that:\\n* all the required operations are either asynchronous in the AWS API (eg. update instance) or quick to execute (e.g. reset bind user passwords)\\n* that update the tags is atomic and synchronous.\\n","Predictions":"We decided:n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/UsingUsingAWS tags)n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/UsingUsingAWS tags)n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/UsingUsingAWS tags)n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/UsingUsingAWS tags)n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/UsingUsingAWS tags)n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/UsingUsingAWS tags)n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/UsingUsingAWS tags)n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/UsingUsingAWS tags)n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/UsingUsingAWS tags)n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/UsingUsingAWS tags)n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/UsingUsingAWS tags)n* Implement a state machine using the [AWS tags](http:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/Using"}
{"File Name":"buildit-all\/0007-use-pac4j-to-validate-tokens.md","Context":"## Context\nhttp:\/\/www.pac4j.org\nWhile exporing alternatives to our custom filter and token validator, we came across pac4j which can deal with Azure AD out of the box.\nThe driver for this alternative is to have a library where we can point it to the discovery url (https:\/\/login.microsoftonline.com\/organizations\/v2.0\/.well-known\/openid-configuration) and have it \"just work.\"  Microsoft is a bit off standard where they add {tenantId} to the Issuer URL because this is a multi-tenant endpoint.  While it's not hard, openid connect libraries need to be adjusted to handle this.  Pac4j does this for free.\n","Decision":"* Use Pac4J to validate MS id_tokens instead of our current custom filter code\\n","tokens":144,"id":4422,"text":"## Context\\nhttp:\/\/www.pac4j.org\\nWhile exporing alternatives to our custom filter and token validator, we came across pac4j which can deal with Azure AD out of the box.\\nThe driver for this alternative is to have a library where we can point it to the discovery url (https:\/\/login.microsoftonline.com\/organizations\/v2.0\/.well-known\/openid-configuration) and have it \"just work.\"  Microsoft is a bit off standard where they add {tenantId} to the Issuer URL because this is a multi-tenant endpoint.  While it's not hard, openid connect libraries need to be adjusted to handle this.  Pac4j does this for free.\\n\n\n##Decision\n* Use Pac4J to validate MS id_tokens instead of our current custom filter code\\n","Predictions":"We will use swift-stack to store the data.n"}
{"File Name":"Corvus.Tenancy\/0004-v2-to-v3-transition.md","Context":"## Context\nAs described in [ADR 0004, `Corvus.Tenancy` will not create storage containers automatically](.\/0003-no-automatic-storage-container-creation.md), `Corvus.Tenancy` v3 introduces a change: applications are now responsible for creating all necessary containers when onboarding a client. This creates a challenge for applications that have already been deployed on v2, because the following things may be true:\n* a tenant may exist in which only a subset of its storage containers exist\n* in a no-downtime migration, a compute farm may have a mixture of v2 and v3 components in use\nTo enable applications currently using `Corvus.Tenancy` v2 to migrate to v3 without disruption, we need a clearly defined path of how a system will be upgraded.\n","Decision":"Upgrades from v2 to v3 use a multi-phase approach, in which any single compute node in the application goes through these steps:\\n1. using nothing but v2\\n1. using v3 libraries mostly (see below) in v2 mode\\n1. using v3 libraries, onboarding new clients in v3 style, using v3 config where available, falling back to v2 config and auto-creation of containers when v3 config not available\\n1. using v3 libraries in non-transitional mode\\nWhile in phase 3, we would run a tool to transition all v2 configuration to v3. Once this tool has completed its work, we are then free to move into phase 4. (There's no particular hurry to move into this final phase. Once all tenants that had v2 configuration have been migrated to v3, there's no behavioural difference between phases 3 and 4. The main motivation for moving to phase 4 is that it enables applications to remove transitional code once transition is complete. Phase 4 might not occur until years after the other phases. For example, libraries such as [Marain](https:\/\/github.com\/marain-dotnet) that enable developers to host their own instances of a service might choose to retain transitional code for a very long time to give customers of these libraries time to complete their migration.)\\nTo support zero-downtime upgrades, it's necessary to support a state where all compute nodes using a particular store are in a mixture of two adjacent phases. E.g., when we move from 1 to 2, there will be a period of time in which some nodes are still in phase 1, and some are in phase 2. However, we will avoid ever being in three phases simultaneously. For example, we will wait until all compute nodes have completed their move to state 2 before moving any into state 3.\\nThe following sections describe the behaviour required in each of the v3 states to support transition. (There's nothing to document here for phase 1, because that's how systems already using v2 today behave.)\\n### Phase 2: using v3 libraries, operating in v2 mode\\nA node in this phase has upgraded to v3 libraries, but is using the transition support and is essentially operating in v2 mode. It will never create new v3 configuration. New tenants continue to be onboarded in the same way as with v2 libraries\u2014the application does not pre-create containers, and expects the tenancy library to create them on demand as required. This gives applications a low-impact way in which to upgrade to v3 libraries without changing any behaviour, and also opens the path to migration towards the new style of operation.\\nThe one difference in behaviour (the reason we describe this as \"mostly\" v2 mode above) is that if v3 configuration is present for a particular configuration key, it has the following effects:\\n* the application will use the v3 configuration and will not even look to see if v2 configuration is present\\n* the application will presume that all relevant containers for this configuration have already been created, and will not attempt to create anything on demand\\nThis is necessary to support the case where all nodes have completed their transition to phase 2 (so none is in phase 1), and some have have moved to phase 3. Nodes that are still in phase 2 at this point need to be able to cope with the possibility that some clients have been onboarded by a phase 3 node, and so there will be only v3 configuration available. (We do not expect both v2 and v3 configuration to be present for any particular container at this point, because migration of tenants onboarded the v2 way into v3 configuration does not start until all nodes have reached phase 3.)\\nTo configure a node to run in this mode, use storage through a suitable transitional interface (e.g., `IBlobContainerSourceWithTenantLegacyTransition`). The application must provide two configuration keys: one for v2 configuration and one for v3 configuration. The transitional adapter will never create v3 configuration, but it will look for it, and only looks for v2 configuration when no v3 configuration is present.\\n### Phase 3: v3 libraries, operating in v3 mode, falling back to v2 as necessary\\nA node in this phase is using the v3 libraries. When onboarding new tenants, it pre-creates all necessary containers, and stores v3 config, but it still uses the transition support so that in cases where existing tenants have only v2 configuration available, it can fall back to the old behaviour.\\nThe only difference between phase 2 and phase 3 is how the application onboards new tenants. Both phases use the transitional adapter in exactly the same way.\\n### Configuration migration\\nOnce all nodes are in phase 3, a tool can be run to upgrade all v2 configuration to v3. Some aspects of this tooling are necessarily application-specific: only the application can know how to discover all of its tenants, and only the application can know what configuration it is storing, and under which keys.\\n","tokens":171,"id":714,"text":"## Context\\nAs described in [ADR 0004, `Corvus.Tenancy` will not create storage containers automatically](.\/0003-no-automatic-storage-container-creation.md), `Corvus.Tenancy` v3 introduces a change: applications are now responsible for creating all necessary containers when onboarding a client. This creates a challenge for applications that have already been deployed on v2, because the following things may be true:\\n* a tenant may exist in which only a subset of its storage containers exist\\n* in a no-downtime migration, a compute farm may have a mixture of v2 and v3 components in use\\nTo enable applications currently using `Corvus.Tenancy` v2 to migrate to v3 without disruption, we need a clearly defined path of how a system will be upgraded.\\n\n\n##Decision\nUpgrades from v2 to v3 use a multi-phase approach, in which any single compute node in the application goes through these steps:\\n1. using nothing but v2\\n1. using v3 libraries mostly (see below) in v2 mode\\n1. using v3 libraries, onboarding new clients in v3 style, using v3 config where available, falling back to v2 config and auto-creation of containers when v3 config not available\\n1. using v3 libraries in non-transitional mode\\nWhile in phase 3, we would run a tool to transition all v2 configuration to v3. Once this tool has completed its work, we are then free to move into phase 4. (There's no particular hurry to move into this final phase. Once all tenants that had v2 configuration have been migrated to v3, there's no behavioural difference between phases 3 and 4. The main motivation for moving to phase 4 is that it enables applications to remove transitional code once transition is complete. Phase 4 might not occur until years after the other phases. For example, libraries such as [Marain](https:\/\/github.com\/marain-dotnet) that enable developers to host their own instances of a service might choose to retain transitional code for a very long time to give customers of these libraries time to complete their migration.)\\nTo support zero-downtime upgrades, it's necessary to support a state where all compute nodes using a particular store are in a mixture of two adjacent phases. E.g., when we move from 1 to 2, there will be a period of time in which some nodes are still in phase 1, and some are in phase 2. However, we will avoid ever being in three phases simultaneously. For example, we will wait until all compute nodes have completed their move to state 2 before moving any into state 3.\\nThe following sections describe the behaviour required in each of the v3 states to support transition. (There's nothing to document here for phase 1, because that's how systems already using v2 today behave.)\\n### Phase 2: using v3 libraries, operating in v2 mode\\nA node in this phase has upgraded to v3 libraries, but is using the transition support and is essentially operating in v2 mode. It will never create new v3 configuration. New tenants continue to be onboarded in the same way as with v2 libraries\u2014the application does not pre-create containers, and expects the tenancy library to create them on demand as required. This gives applications a low-impact way in which to upgrade to v3 libraries without changing any behaviour, and also opens the path to migration towards the new style of operation.\\nThe one difference in behaviour (the reason we describe this as \"mostly\" v2 mode above) is that if v3 configuration is present for a particular configuration key, it has the following effects:\\n* the application will use the v3 configuration and will not even look to see if v2 configuration is present\\n* the application will presume that all relevant containers for this configuration have already been created, and will not attempt to create anything on demand\\nThis is necessary to support the case where all nodes have completed their transition to phase 2 (so none is in phase 1), and some have have moved to phase 3. Nodes that are still in phase 2 at this point need to be able to cope with the possibility that some clients have been onboarded by a phase 3 node, and so there will be only v3 configuration available. (We do not expect both v2 and v3 configuration to be present for any particular container at this point, because migration of tenants onboarded the v2 way into v3 configuration does not start until all nodes have reached phase 3.)\\nTo configure a node to run in this mode, use storage through a suitable transitional interface (e.g., `IBlobContainerSourceWithTenantLegacyTransition`). The application must provide two configuration keys: one for v2 configuration and one for v3 configuration. The transitional adapter will never create v3 configuration, but it will look for it, and only looks for v2 configuration when no v3 configuration is present.\\n### Phase 3: v3 libraries, operating in v3 mode, falling back to v2 as necessary\\nA node in this phase is using the v3 libraries. When onboarding new tenants, it pre-creates all necessary containers, and stores v3 config, but it still uses the transition support so that in cases where existing tenants have only v2 configuration available, it can fall back to the old behaviour.\\nThe only difference between phase 2 and phase 3 is how the application onboards new tenants. Both phases use the transitional adapter in exactly the same way.\\n### Configuration migration\\nOnce all nodes are in phase 3, a tool can be run to upgrade all v2 configuration to v3. Some aspects of this tooling are necessarily application-specific: only the application can know how to discover all of its tenants, and only the application can know what configuration it is storing, and under which keys.\\n","Predictions":"- The v3 v2 configuration will be migrated to v3 using a multi-phase approach, in which any single compute node in the application goes through these steps:n1. using nothing but v2n1. using v3 libraries mostly (see below) in v2 moden1. using v3 libraries, onboarding new clients in v3 style, using v3 config where available, falling back to v2 config and auto-creation of containers when v3 config not availablen1. using v3 libraries in non-transitional modenWhile in phase 3, we would run a tool to transition all v2 configuration to v3. Once this tool has completed its work, we are then free to move into phase 4. (There's no particular hurry to move into this final phase. Once all tenants that had v2 configuration have been migrated to v3, there's no behaviour difference between phases 3 and 4. The main motivation for moving to phase 4 is that it enables applications to remove transitional code once transition is complete. Phase 4 might not occur until years after the other phases. For example, libraries such as [Marain](https:\/\/github.com\/marain-dotnet) that enable developers to host their own instances of a service might choose to retain transitional code once transitional code is complete.n- The v3 v2 configuration will be migrated to v3 using a multi-phase approach, in which any single compute node in the application goes through these steps:n1. using nothing but v2n1. using v3 libraries mostly (see below) in v2 moden1. using v3 libraries mostly (see below) in v2 moden1. using v3 libraries, onboarding new clients in v3 style, using v3 config where available, falling back to v2 config and auto-creation of containers when v3 config not availablen1. using v3 libraries in non-transitional modenWhile in phase 3, we would run a tool to transition all v2 configuration to v3. Once this tool has completed its work, we are then free to move into phase 4. (There's no particular hurry to move into this final phase. Once all tenants that had v2 configuration have been migrated to v3, there's no behaviour difference between phases 3 and 4. The main motivation for moving to phase 4 is that it enables applications to remove transitional code once transition is complete. Phase 4 might not occur until years after the other phases. For example, libraries such as [Marain](https:\/\/github.com\/marain-dotnet) that enable developers to host their own instances of a service might choose to retain transitional code once transitional code is complete."}
{"File Name":"registraties\/002-use-styled-components.md","Context":"## Context\nSASS is used in most of the [Amsterdam](https:\/\/github.com\/Amsterdam) projects. The way SASS is compiled by Webpack, however, has all defined style rules affect the global scope. This can lead to unwanted and unexpected side-effect and will increase maintenance, because each developer working on the project needs to be aware of the rules that have been defined and are scattered throughout the code base. Also, using SASS requires a naming convention to prevent specifity problems.\nAn alternative to global scope CSS is [styled components](https:\/\/www.styled-components.com\/). This approach is also used by the [Amsterdam reusable component library](https:\/\/amsterdam.github.io\/amsterdam-styled-components), it makes sense to also apply it to projects (like this one) that make use of components from that library.\n","Decision":"SASS will not be used in the Registraties project. Instead, styled components will be used.\\n","tokens":174,"id":1125,"text":"## Context\\nSASS is used in most of the [Amsterdam](https:\/\/github.com\/Amsterdam) projects. The way SASS is compiled by Webpack, however, has all defined style rules affect the global scope. This can lead to unwanted and unexpected side-effect and will increase maintenance, because each developer working on the project needs to be aware of the rules that have been defined and are scattered throughout the code base. Also, using SASS requires a naming convention to prevent specifity problems.\\nAn alternative to global scope CSS is [styled components](https:\/\/www.styled-components.com\/). This approach is also used by the [Amsterdam reusable component library](https:\/\/amsterdam.github.io\/amsterdam-styled-components), it makes sense to also apply it to projects (like this one) that make use of components from that library.\\n\n\n##Decision\nSASS will not be used in the Registraties project. Instead, styled components will be used.\\n","Predictions":"We will use a [Semantic Query Language](https:\/\/semantic-search.org\/) to query the data.n"}
{"File Name":"taxonomy-manager\/001-graph-database.md","Context":"## Context and Problem Statement\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\n## Decision Drivers <!-- optional -->\n* High availability\/Fault tolerance\n* Learning curve\n* Maintenance overhead\n* Vendor lock-in\n","Decision":"* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\nChosen option: Apache Jena with underlying PostgreSQL persistence store and a custom persistence layer, because it provides a highly available database persistence that is widely available as a managed service. We opted against using Apache Jena's SDB to achieve this as it has been in a state of \"maintenance only\" since June 2013.\\n### Positive Consequences\\n* PostgreSQL can be clustered, making it highly available\\n* PostgreSQL is broadly available as a managed service\\n* We can utilise the same PostgreSQL database for storing additional data (e.g. users, roles, etc)\\n### Negative Consequences\\n* We have to design our own schema and code for interacting with the PostgreSQL database\\n","tokens":95,"id":1510,"text":"## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n\n\n##Decision\n* High availability\/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\nChosen option: Apache Jena with underlying PostgreSQL persistence store and a custom persistence layer, because it provides a highly available database persistence that is widely available as a managed service. We opted against using Apache Jena's SDB to achieve this as it has been in a state of \"maintenance only\" since June 2013.\\n### Positive Consequences\\n* PostgreSQL can be clustered, making it highly available\\n* PostgreSQL is broadly available as a managed service\\n* We can utilise the same PostgreSQL database for storing additional data (e.g. users, roles, etc)\\n### Negative Consequences\\n* We have to design our own schema and code for interacting with the PostgreSQL database\\n","Predictions":"* User prospectiven* Availabilityn* Maintenance overheadn* Vendor lock-innChosen option: Apache Jest, because it provides a high availability and low maintenance overhead.n* Jest is widely available as a managed servicen* We can utilise the same Jest database for storing additional data (e.g. users, roles, etc)n### Positive Consequencesn* Jest can be clustered, making it highly availablen* Jest is broadly available as a managed servicen* We can utilise the same Jest database for storing additional data (e.g. users, roles, etc)n### Negative Consequencesn* We have to design our own schema and code for interacting with the Jest databasen"}
{"File Name":"inner-source_kotlin_spark_template\/20180525-gradle-kotlin.md","Context":"## Context\nGradle builds written in Kotlin are less esoteric comparing to Groovy ones.\nIntellij IDEA has great support for Kotlin based builds.\n","Decision":"We will adopt Kotlin as a Gradle build's definition language.\\n","tokens":32,"id":1463,"text":"## Context\\nGradle builds written in Kotlin are less esoteric comparing to Groovy ones.\\nIntellij IDEA has great support for Kotlin based builds.\\n\n\n##Decision\nWe will adopt Kotlin as a Gradle build's definition language.\\n","Predictions":"We will use Poetry as a virtualenv for Python projects.n"}
{"File Name":"fare-platform\/0002-permission-crud-operations-on-records-config.md","Context":"## Context\nInitially a record could be made by anyone, but the problem is that that record was only some metadata without file attached on it.\n","Decision":"To avoid the situation where a record is created without file, we put `deny_all` permission in fare\/records\/config.py that denies to all the possibilities to create, update and delete this kind of records.\\n","tokens":30,"id":2215,"text":"## Context\\nInitially a record could be made by anyone, but the problem is that that record was only some metadata without file attached on it.\\n\n\n##Decision\nTo avoid the situation where a record is created without file, we put `deny_all` permission in fare\/records\/config.py that denies to all the possibilities to create, update and delete this kind of records.\\n","Predictions":"We will use the [Security Stream](https:\/\/github.com\/Arctic-Stream\/SecurityStream) to store the data in a way that is consistent with the data being stored in the on-chain storage.n"}
{"File Name":"community\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3466,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"iampeterbanjo.com\/0012-parallel-database-tests-mongodb-server.md","Context":"## Context\nJest tests are fast because they can be run in parallel. If we use the same database for every test, it can cause race conditions as multiple operations are performed on models and collections. There are two ways to decouple tests:\n- [define databases in tests][defined-test-database]\n- [randomly create databases for each test][random-test-database]\n","Decision":"In the context of database tests, and facing the concern of race conditions then create random databases for each test. There are different approaches for setting up the [test Mongodb server][test-mongodb-server] with Jest. I prefer using the `beforeAll` and `afterAll` hooks because this is more flexible and is less coupled to Jest's idiosyncracies.\\n","tokens":80,"id":1589,"text":"## Context\\nJest tests are fast because they can be run in parallel. If we use the same database for every test, it can cause race conditions as multiple operations are performed on models and collections. There are two ways to decouple tests:\\n- [define databases in tests][defined-test-database]\\n- [randomly create databases for each test][random-test-database]\\n\n\n##Decision\nIn the context of database tests, and facing the concern of race conditions then create random databases for each test. There are different approaches for setting up the [test Mongodb server][test-mongodb-server] with Jest. I prefer using the `beforeAll` and `afterAll` hooks because this is more flexible and is less coupled to Jest's idiosyncracies.\\n","Predictions":"In the context of database tests, and facing the concern of race conditions then create random databases for each test. There are different approaches for setting up the [test Mongodb server][test-mongodb-server] with Jest. I prefer using the beforeAll and afterAll hooks because this is more flexible and is less coupled to Jest's idiosyncracies.n"}
{"File Name":"compliantkubernetes\/0008-use-hostnetwork-or-loadbalancer-for-ingress.md","Context":"## Context and Problem Statement\nMany regulations require traffic to be encrypted over public Internet. Compliant Kubernetes solves this problem via an [Ingress controller](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/ingress-controllers\/) and [cert-manager](https:\/\/github.com\/jetstack\/cert-manager). As of February 2021, Compliant Kubernetes comes by default with [nginx-ingress](https:\/\/kubernetes.github.io\/ingress-nginx\/), but [Ambassador](https:\/\/www.getambassador.io\/docs\/latest\/topics\/running\/ingress-controller\/) is planned as an alternative. The question is, how does traffic arrive at the Ingress controller?\n## Decision Drivers\n* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\n* We want to keep things simple.\n","Decision":"* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\nChosen options:\\n1. Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker nodes with a manual or Terraform-controlled load-balancer. This includes:\\n* Where load-balancing does not add value, e.g., if a deployment is planned to have only a single-node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead.\\n* Exoscale currently falls in this category, due to its Kubernetes integration being rather recent.\\n* SafeSpring falls in this category, since it is missing load balancers.\\n* If the cloud provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing.\\n2. Use Service Type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud.\\nAdditional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until *after* we apply Compliant Kubernetes Apps. There is a risk for \"the Internet\" -- LetsEncrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.:\\n```\\n*.$BASE_DOMAIN     60s A 203.0.113.123\\n*.ops.$BASE_DOMAIN 60s A 203.0.113.123\\n```\\n203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by [kops](https:\/\/github.com\/kubernetes\/kops\/blob\/d5d08a43034dd4c7242cf1faa020cf9a8c3965e2\/upup\/pkg\/fi\/cloudup\/dns.go#L41) and should not feel astonishing.\\n### Positive Consequences\\n* We make the best of each cloud provider.\\n* Obeys principle of least astonishment.\\n* We do not add a load balancer \"just because\".\\n### Negative Consequences\\n* Complexity is a bit increased, however, this feels like essential complexity.\\n","tokens":254,"id":3104,"text":"## Context and Problem Statement\\nMany regulations require traffic to be encrypted over public Internet. Compliant Kubernetes solves this problem via an [Ingress controller](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/ingress-controllers\/) and [cert-manager](https:\/\/github.com\/jetstack\/cert-manager). As of February 2021, Compliant Kubernetes comes by default with [nginx-ingress](https:\/\/kubernetes.github.io\/ingress-nginx\/), but [Ambassador](https:\/\/www.getambassador.io\/docs\/latest\/topics\/running\/ingress-controller\/) is planned as an alternative. The question is, how does traffic arrive at the Ingress controller?\\n## Decision Drivers\\n* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\n\n\n##Decision\n* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\\n* We want to keep things simple.\\nChosen options:\\n1. Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker nodes with a manual or Terraform-controlled load-balancer. This includes:\\n* Where load-balancing does not add value, e.g., if a deployment is planned to have only a single-node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead.\\n* Exoscale currently falls in this category, due to its Kubernetes integration being rather recent.\\n* SafeSpring falls in this category, since it is missing load balancers.\\n* If the cloud provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing.\\n2. Use Service Type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud.\\nAdditional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until *after* we apply Compliant Kubernetes Apps. There is a risk for \"the Internet\" -- LetsEncrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.:\\n```\\n*.$BASE_DOMAIN     60s A 203.0.113.123\\n*.ops.$BASE_DOMAIN 60s A 203.0.113.123\\n```\\n203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by [kops](https:\/\/github.com\/kubernetes\/kops\/blob\/d5d08a43034dd4c7242cf1faa020cf9a8c3965e2\/upup\/pkg\/fi\/cloudup\/dns.go#L41) and should not feel astonishing.\\n### Positive Consequences\\n* We make the best of each cloud provider.\\n* Obeys principle of least astonishment.\\n* We do not add a load balancer \"just because\".\\n### Negative Consequences\\n* Complexity is a bit increased, however, this feels like essential complexity.\\n","Predictions":"* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.n* We want to keep things simple.n"}
{"File Name":"digital-paper-edit-firebase\/2019-09-28-time-estimation-for-links.md","Context":"## Context and Problem Statement\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\n## Decision Drivers <!-- optional -->\n* Ease of implementation\n* Accuracy of time estimated\n* Extensibility\n","Decision":"* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","tokens":49,"id":5250,"text":"## Context and Problem Statement\\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\\n## Decision Drivers <!-- optional -->\\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\n\n\n##Decision\n* Ease of implementation\\n* Accuracy of time estimated\\n* Extensibility\\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\\n### Positive Consequences <!-- optional -->\\n* A fairly easy implementation.\\n### Negative consequences <!-- optional -->\\n* Less accurate than other options.\\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\\n","Predictions":"* Ease of implementationn* Accuracy of time estimatedn* ExtensibilitynChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.n### Positive Consequences!-- optional -->n* A fairly easy implementation.n### Negative consequences!-- optional -->n* Less accurate than other options.n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))n"}
{"File Name":"CrossyToad\/adr-0010-support-linear-and-discrete-input.md","Context":"## Context\nWe need to support both discrete and continous input.\nDiscrete input is the detection of an individual keypress. We need discrete input to support\nmovement in game menus, state transitions and other \"one keypress one action\" scenarios.\nContinous input is the detection of the state of an input on an ongoing basis. We need continous\nimport to support linear movement in the game, notably the movement of the toad.\nCurrently we are using SDL to retrieve discrete input and we have no mechanism for continous input.\nOne option for implementing continous input would be to use the [SDL_GetKeyboardState](https:\/\/wiki.libsdl.org\/SDL_GetKeyboardState) function from SDL. We could call this function every frame and\nmake the results available to every Scene for interpretation.\nAnother option would be to consume our existing input event stream which is fed from [SDL_PollEvent](https:\/\/www.libsdl.org\/release\/SDL-1.2.15\/docs\/html\/sdlpollevent.html) and transition our own \"InputState\" based on the \"KeyPressed\" and \"KeyReleased\" events.\nAt the moment I prefer maintaining our own state instead of letting SDL do it as we can reduce the\nnumber of possible key combinations we want to support through the type system.\n","Decision":"We will support continous and discrete input using the existing poll events architecture and\\nsome new state machinery.\\n","tokens":270,"id":2482,"text":"## Context\\nWe need to support both discrete and continous input.\\nDiscrete input is the detection of an individual keypress. We need discrete input to support\\nmovement in game menus, state transitions and other \"one keypress one action\" scenarios.\\nContinous input is the detection of the state of an input on an ongoing basis. We need continous\\nimport to support linear movement in the game, notably the movement of the toad.\\nCurrently we are using SDL to retrieve discrete input and we have no mechanism for continous input.\\nOne option for implementing continous input would be to use the [SDL_GetKeyboardState](https:\/\/wiki.libsdl.org\/SDL_GetKeyboardState) function from SDL. We could call this function every frame and\\nmake the results available to every Scene for interpretation.\\nAnother option would be to consume our existing input event stream which is fed from [SDL_PollEvent](https:\/\/www.libsdl.org\/release\/SDL-1.2.15\/docs\/html\/sdlpollevent.html) and transition our own \"InputState\" based on the \"KeyPressed\" and \"KeyReleased\" events.\\nAt the moment I prefer maintaining our own state instead of letting SDL do it as we can reduce the\\nnumber of possible key combinations we want to support through the type system.\\n\n\n##Decision\nWe will support continous and discrete input using the existing poll events architecture and\\nsome new state machinery.\\n","Predictions":"We will support continuous input using the existing poll events architecture andnsome new state machinery.n"}
{"File Name":"iampeterbanjo.com\/0002-blog-engine.md","Context":"## Context\nI wanted to create a website blog using [Markdown][1], a [static site generator][2] and [HapiJS][3] to serve the files. Static site generators I tried, in order, were -\n### [11ty][4]\nPros\n- Easy setup\n- Predictable static file output\n- Relatively small codebase\n- Supporst different template engines\nCons\n- The frontmatter parser [didn't work as I expected][7]\n- Template errors were not reported and would only fail on build\n- Needs some attention to make production ready e.g. SEO, images, favicons etc\n### [Gatsby][5]\nPros\n- Modern [JAMstack][8] project with React\n- Batteries included by default - includes helpers for favicons, image loading, SEO etc.\n- Opportunity to use GraphQL\nCons\n- Doesn't output static files - [totally depends on clientside JavaScript][9]\n- It relies on service workers to cache responses which causes flakey e2e tests and unpredictable behaviour\n- Developing static files based on API was difficult because of caching and unexpected error handling\n- Relatively complex project with separate build, develop steps, config, plugins etc\n### [Hugo][6]\nPros\n- Fast and mature static site generator\n- Small footprint\nCons\n- Written in Go - I prefer fullstack JavaScript for this site\n","Decision":"In the context of using several static site generators each of them had with their own downsides. And facing the concern of having blog that I can easily maintain and customise I decided to build my blog with HapiJS to achieve a smaller blog footprint, predictable behaviour and an opportunity to learn something new. I accept that I'm re-inventing the wheel.\\n","tokens":300,"id":1599,"text":"## Context\\nI wanted to create a website blog using [Markdown][1], a [static site generator][2] and [HapiJS][3] to serve the files. Static site generators I tried, in order, were -\\n### [11ty][4]\\nPros\\n- Easy setup\\n- Predictable static file output\\n- Relatively small codebase\\n- Supporst different template engines\\nCons\\n- The frontmatter parser [didn't work as I expected][7]\\n- Template errors were not reported and would only fail on build\\n- Needs some attention to make production ready e.g. SEO, images, favicons etc\\n### [Gatsby][5]\\nPros\\n- Modern [JAMstack][8] project with React\\n- Batteries included by default - includes helpers for favicons, image loading, SEO etc.\\n- Opportunity to use GraphQL\\nCons\\n- Doesn't output static files - [totally depends on clientside JavaScript][9]\\n- It relies on service workers to cache responses which causes flakey e2e tests and unpredictable behaviour\\n- Developing static files based on API was difficult because of caching and unexpected error handling\\n- Relatively complex project with separate build, develop steps, config, plugins etc\\n### [Hugo][6]\\nPros\\n- Fast and mature static site generator\\n- Small footprint\\nCons\\n- Written in Go - I prefer fullstack JavaScript for this site\\n\n\n##Decision\nIn the context of using several static site generators each of them had with their own downsides. And facing the concern of having blog that I can easily maintain and customise I decided to build my blog with HapiJS to achieve a smaller blog footprint, predictable behaviour and an opportunity to learn something new. I accept that I'm re-inventing the wheel.\\n","Predictions":"- Status: The Status is a Status document that describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Status of a project. It describes the Stat"}
{"File Name":"cljdoc\/0008-use-circleci-as-analysis-sandbox.md","Context":"## Context\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\nsandboxed environment. A Docker image has been created to help with this but this still\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\nto run Bitcoin miners and other compute-stealing stuff.\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\nbe found in the [notes on Isolation](https:\/\/github.com\/martinklepsch\/cljdoc\/blob\/72da65055ab94942f33fb63b29b732e81b559508\/doc\/isolation.md)\n","Decision":"For a first version of cljdoc we will use CircleCI to run analysis for us. The result of\\nthis analysis will be made available as a build artifact which can then be laoded in\\na trusted environment to import data into Grimoire and build HTML (or other) documentation\\nfrontends.\\n","tokens":160,"id":2601,"text":"## Context\\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\\nsandboxed environment. A Docker image has been created to help with this but this still\\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\\nto run Bitcoin miners and other compute-stealing stuff.\\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\\nbe found in the [notes on Isolation](https:\/\/github.com\/martinklepsch\/cljdoc\/blob\/72da65055ab94942f33fb63b29b732e81b559508\/doc\/isolation.md)\\n\n\n##Decision\nFor a first version of cljdoc we will use CircleCI to run analysis for us. The result of\\nthis analysis will be made available as a build artifact which can then be laoded in\\na trusted environment to import data into Grimoire and build HTML (or other) documentation\\nfrontends.\\n","Predictions":"We will use CircleCI to run analysis for us. The result ofnthis analysis will be made available as a build artifact which can then be laoded inna trusted environment to import data into Grimoire and build HTML (or other) documentation.n"}
{"File Name":"stamper\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1244,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"rotc\/0002-use-aws-as-example-cloud-platform.md","Context":"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":3546,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","Predictions":"The change that we're proposing or have agreed to implement.n"}
{"File Name":"architecture-decision-log\/0007-behavior-driven-development.md","Context":"* [Context](#context)\n* [Decision](#decision)\n* [Status](#status)\n* [Consequences](#consequences)\n* [More reading](#more-reading)\n* [Updates](#updates)\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe are going to use Behavior-Driven Development to help us reduce that gap. BDD is a technique where we use User Stories to create acceptance tests based on user behaviors.\\nInstead of writing complex test statements, we use a BDD tool to create a readable phrase, like the following:\\n```txt\\nGiven a new created user\\nWhen that user places a new order\\nThen a notification must be sent\\n```\\nEach line of that phrase relates to a given function that will execute steps and persist the state machine's context. We can use assertions to return errors along the way.\\nBDD is a powerful tool that enhances our TDD cycle.\\n","tokens":53,"id":4918,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe are going to use Behavior-Driven Development to help us reduce that gap. BDD is a technique where we use User Stories to create acceptance tests based on user behaviors.\\nInstead of writing complex test statements, we use a BDD tool to create a readable phrase, like the following:\\n```txt\\nGiven a new created user\\nWhen that user places a new order\\nThen a notification must be sent\\n```\\nEach line of that phrase relates to a given function that will execute steps and persist the state machine's context. We can use assertions to return errors along the way.\\nBDD is a powerful tool that enhances our TDD cycle.\\n","Predictions":"* [Status](#status)n* [Consequences](#consequences)n* [More reading](#more-reading)n* [Updates](#updates)nWe are going to use Behavior-Driven Development to help us reduce that gap. BDD is a technique where we use User Stories to create acceptance tests based on user behaviors.nInstead of writing complex test statements, we use a single-choice approach to test the application.n"}
{"File Name":"operational-data-hub\/0003-repo-directory-structure.md","Context":"## Context\nWe feel the need to standardize the directory structure for github repos.\n","Decision":"### 1. General rules\\n* Directory names must always:\\n* Always use underscores (\u201c\\_\u201d) in names for folders and files to split between words.\\n* use lowercase.\\n### 2. Cloud functions\\n* The root directory for cloud functions is 'functions\/'.\\n* A '.gcloudignore' file is provided to limit the uploaded files.\\n### 3. API\\n* the root directory for the API is 'app\/'.\\n* For OpenApi APIs see the separate ADR for working with OpenApi generated code.\\n### 4. Frontend\\n* the root directory for the APP is 'app\/'.\\n* The directory structure below the app directory is based on the recommendations of the framework.\\n* For the angular applications the recommendations can be found at [Angular Workspace and project file structure](https:\/\/angular.io\/guide\/file-structure).\\n### 5. Tests\\n* Files needed for testing are stored in the 'test' directory.\\n### 6. SAST\\n* Files needed for the SAST scan are stored in the 'sast-config' directory.\\n","tokens":18,"id":2703,"text":"## Context\\nWe feel the need to standardize the directory structure for github repos.\\n\n\n##Decision\n### 1. General rules\\n* Directory names must always:\\n* Always use underscores (\u201c\\_\u201d) in names for folders and files to split between words.\\n* use lowercase.\\n### 2. Cloud functions\\n* The root directory for cloud functions is 'functions\/'.\\n* A '.gcloudignore' file is provided to limit the uploaded files.\\n### 3. API\\n* the root directory for the API is 'app\/'.\\n* For OpenApi APIs see the separate ADR for working with OpenApi generated code.\\n### 4. Frontend\\n* the root directory for the APP is 'app\/'.\\n* The directory structure below the app directory is based on the recommendations of the framework.\\n* For the angular applications the recommendations can be found at [Angular Workspace and project file structure](https:\/\/angular.io\/guide\/file-structure).\\n### 5. Tests\\n* Files needed for testing are stored in the 'test' directory.\\n### 6. SAST\\n* Files needed for the SAST scan are stored in the 'sast-config' directory.\\n","Predictions":"We identify rules for the GitHub organisation, the source repository name format and basic source code guidelines.n### VWT DigitalnThis is the main organisation for all Operational Data Hub (ODH) specific source code. When creating a new repository,nit **has to pass** one of the following applications:n- An ODH-specific solution;n- A solution that transcends multiple solution domains (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+namingconvention#Cloudnamingconvention-Domains)).nIn addition, these repositories **may not** contain:n- Any configuration;n- Any configurationn- Any configurationn"}
{"File Name":"buy-for-your-school\/0009-use-simplecov-to-monitor-code-test-coverage.md","Context":"## Context\nWe want to keep our test coverage as high as possible without having to run\nmanual checks as these take time and are easy to forget.\n","Decision":"Use Simplecov with RSpec to monitor coverage changes on every test run\\n","tokens":32,"id":1258,"text":"## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n\n\n##Decision\nUse Simplecov with RSpec to monitor coverage changes on every test run\\n","Predictions":"We will use Simplecov with RSpec to monitor coverage changes on every test runn"}
{"File Name":"adrflow\/5-Utility_Context_Lazily_Initialized.md","Context":"# 5 Utility Context Lazily Initialized\n## Context\nThe [synchronous utils](..\/..\/src\/adr_util_sync.js) are intended to be used by commands to work on ADRs, with the given context of the project, e.g. the [adr marker file](1-ADR_directory_identified_by_marker_file.md).\nSince the context is mandatory (the ADR directory, existing ADRs), it is initialized at the beginning, in the object's constructor.\nSeveral commands (scripts) already include that script, and initialize the object on the script loading, as part of the `require` statement.\nThis resulted in loading the context whenever a command is loaded.\nThis might have performance implications, but the more immediate functional lacuna is that this then assumes that the context *can* be loaded every time.\nOf course, this assumption breaks - when running the `init` command, where by definition there is no context to load - the `init` command creates that.\nThis resulted in trying to run the `init` command, on an existing directory, in an error.\n","Decision":"The ADR project context will be loaded lazily, only when needed.\\nThis assumes the `init` command doesn't require it, and hence will not invoke any commands that require that context (e.g. the existing files).\\nSo any command that is loaded can still create the context object and create an instance of, but the actual context will be loaded easily.\\nThis assumption seems safe enough since `init` command indeed creates the context (the `.adr` file).\\nAnd this will probably hold for similar commands.\\nCommands that will require the ADR context, will continue to use it as today, and it will be loaded lazily.\\nWe need of course to make sure that the context isn't created\/calculated every time it is called; e.g. searching for the ADR dir every time one of the utility functions is called. In other words - memoize the context-creating functions.\\n","tokens":227,"id":3635,"text":"# 5 Utility Context Lazily Initialized\\n## Context\\nThe [synchronous utils](..\/..\/src\/adr_util_sync.js) are intended to be used by commands to work on ADRs, with the given context of the project, e.g. the [adr marker file](1-ADR_directory_identified_by_marker_file.md).\\nSince the context is mandatory (the ADR directory, existing ADRs), it is initialized at the beginning, in the object's constructor.\\nSeveral commands (scripts) already include that script, and initialize the object on the script loading, as part of the `require` statement.\\nThis resulted in loading the context whenever a command is loaded.\\nThis might have performance implications, but the more immediate functional lacuna is that this then assumes that the context *can* be loaded every time.\\nOf course, this assumption breaks - when running the `init` command, where by definition there is no context to load - the `init` command creates that.\\nThis resulted in trying to run the `init` command, on an existing directory, in an error.\\n\n\n##Decision\nThe ADR project context will be loaded lazily, only when needed.\\nThis assumes the `init` command doesn't require it, and hence will not invoke any commands that require that context (e.g. the existing files).\\nSo any command that is loaded can still create the context object and create an instance of, but the actual context will be loaded easily.\\nThis assumption seems safe enough since `init` command indeed creates the context (the `.adr` file).\\nAnd this will probably hold for similar commands.\\nCommands that will require the ADR context, will continue to use it as today, and it will be loaded lazily.\\nWe need of course to make sure that the context isn't created\/calculated every time it is called; e.g. searching for the ADR dir every time one of the utility functions is called. In other words - memoize the context-creating functions.\\n","Predictions":"The ADR project context will be loaded lazily, only when needed.nThis assumes the init command doesn't require it, and hence will not invoke any commands that require that context (e.g. the existing files).nSo any command that is loaded can still create the context object and create an instance of, but the actual context will be loaded easily.nThis assumption seems safe enough since init command actually creates the context (the.adr file).nAnd this will probably hold for similar commands.nCommands that will require the ADR context, will continue to use it as today, and it will be loaded lazily.nWe need of course to make sure that the context isn't created\/calculated every time it is called; e.g. searching for the ADR dir every time one of the utility functions is called. In other words - memoize the context. In other words, search for the ADR dir every time one of the utility functions is called. In other words, memoize the context.n"}
{"File Name":"testy\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1049,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"openfido-workflow-service\/0002-pipelines.md","Context":"## Context\nA couple of client projects need infrastructure to process GridLabD jobs.\nThe goal would be that other projects could either include this project as its\nown service in their infrastructure, or incorporate it directly into their\nFlask-based project.\nThese kinds of jobs are long running, and produce artifacts that each project\nmay store in different ways (but primarily S3).\n","Decision":"Create a Flask Rest service, coupled with Celery for job processing.\\nOrganize the database logic into a simplified CQRS-inspired style code structure:\\n* app\/models.py contain all sql models.\\n* app\/services.py contain all db commands that modify database state.\\n* app\/queries.py contain all db queries to the database.\\nUse the structure of the [presence-account-service](https:\/\/github.com\/PresencePG\/presence-account-service) project as a reference.\\n","tokens":78,"id":4388,"text":"## Context\\nA couple of client projects need infrastructure to process GridLabD jobs.\\nThe goal would be that other projects could either include this project as its\\nown service in their infrastructure, or incorporate it directly into their\\nFlask-based project.\\nThese kinds of jobs are long running, and produce artifacts that each project\\nmay store in different ways (but primarily S3).\\n\n\n##Decision\nCreate a Flask Rest service, coupled with Celery for job processing.\\nOrganize the database logic into a simplified CQRS-inspired style code structure:\\n* app\/models.py contain all sql models.\\n* app\/services.py contain all db commands that modify database state.\\n* app\/queries.py contain all db queries to the database.\\nUse the structure of the [presence-account-service](https:\/\/github.com\/PresencePG\/presence-account-service) project as a reference.\\n","Predictions":"Create a Flask Rest service, coupled with Celery for job processing.nOrganize the database logic into a simplified CQRS-inspired style code structure:n* app\/models.py contain all sql models.n* app\/services.py contain all db commands that modify database state.n* app\/queries.py contain all db queries to the database.nUse the structure of the [presence-account-service](https:\/\/github.com\/PresencePG\/presence-account-service) project as a reference.n"}
{"File Name":"claim-additional-payments-for-teaching\/0009-capture-teacher-reference-number.md","Context":"## Context\nA claimant\u2019s eligibility is, in part, determined by their qualifications. We\nwant to be able to validate that a claimant\u2019s qualifications match those of the\neligibility criteria.\n","Decision":"To aid DfE in the process of validating a claimant\u2019s qualifications, we will\\ncollect the claimant\u2019s \u2018Teacher Reference Number\u2019 or TRN.\\nWith the TRN, DfE can use the Database of Qualified Teachers\\n([DQT](https:\/\/teacherservices.education.gov.uk\/SelfService\/Login)) to validate\\na claimant\u2019s qualifications.\\n","tokens":42,"id":2098,"text":"## Context\\nA claimant\u2019s eligibility is, in part, determined by their qualifications. We\\nwant to be able to validate that a claimant\u2019s qualifications match those of the\\neligibility criteria.\\n\n\n##Decision\nTo aid DfE in the process of validating a claimant\u2019s qualifications, we will\\ncollect the claimant\u2019s \u2018Teacher Reference Number\u2019 or TRN.\\nWith the TRN, DfE can use the Database of Qualified Teachers\\n([DQT](https:\/\/teacherservices.education.gov.uk\/SelfService\/Login)) to validate\\na claimant\u2019s qualifications.\\n","Predictions":"To aid DfE in the process of validating a claimant\u2019s qualifications, we willncollect the claimant\u2019s \u2018Teacher Reference Number\u2019 or TRN.nWith the TRN, DfE can use the Database of Qualified Teachersn([DQT](https:\/\/teacherservices.education.gov.uk\/SelfService\/Login)) to validatena claimant\u2019s qualifications.n"}
{"File Name":"ftd-scratch3-offline\/0009-use-jackson-to-read-json-files.md","Context":"## Context\nScratch save files are zip files that contain the actual code in a `project.json` file.\nTo read this file we need a deserialization library.\nJackson, GSON and org.json are common libraries.\nJackson seems to offer the most features and the authors already have used it.\nGSON offers many features Jackson also features.\norg.json seems to be only a json parser and the use has to create the resulting object themselves, while the other libraries also feature object mapping.\n","Decision":"We will use [Jackson](https:\/\/github.com\/FasterXML\/jackson-databind\/).\\n","tokens":104,"id":2625,"text":"## Context\\nScratch save files are zip files that contain the actual code in a `project.json` file.\\nTo read this file we need a deserialization library.\\nJackson, GSON and org.json are common libraries.\\nJackson seems to offer the most features and the authors already have used it.\\nGSON offers many features Jackson also features.\\norg.json seems to be only a json parser and the use has to create the resulting object themselves, while the other libraries also feature object mapping.\\n\n\n##Decision\nWe will use [Jackson](https:\/\/github.com\/FasterXML\/jackson-databind\/).\\n","Predictions":"We will use [JSON](https:\/\/github.com\/FasterXML\/jackson-databind\/).n"}
{"File Name":"rfcs\/0000-template.md","Context":"## Context\n[context]: #context\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\n","Decision":"[decision]: #decision\\n> This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","tokens":60,"id":1879,"text":"## Context\\n[context]: #context\\n> This section describes the forces at play, including technological, political, social, and project local. These forces are probably in tension, and should be called out as such. The language in this section is value-neutral. It is simply describing facts.\\n\n\n##Decision\n[decision]: #decision\\n> This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","Predictions":"* This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will...\"n"}
{"File Name":"operational-data-hub\/0012-repository-organisation-rules.md","Context":"## Context\nWe feel the need to define rules to select the correct GitHub organisation for source repositories.\n","Decision":"We identify rules for the GitHub organisation, the source repository name format and basic source code guidelines.\\n### VWT Digital\\nThis is the main organisation for all Operational Data Hub (ODH) specific source code. When creating a new repository,\\nit **has to pass** one of the following applications:\\n- An ODH-specific solution;\\n- A solution that transcends multiple solution domains (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration;\\n- Any solution business logic.\\nWhen creating a name for the repository, it **has to start** with either `odh-<repo_name>` or `dat-<repo_name>`:\\n- `odh-`: When the repository contains source code specifically made for the Operational Data Hub;\\n- `dat-`: When the repository contains source code created for generic usage within the Digital Ambition Team domain.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Solutions\\nThis is the main organisation for all Solution-specific source code. When creating a new repository, it **has to pass** the following application:\\n- A specific application for a domain solution, only used by one domain (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration.\\nWhen creating a name for the repository, it has to start with the abbreviation of the solutions it is a part of.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Configuration\\nThis is the main organisation for all configuration code. When creating a new repository, it **has to pass** the following application:\\n- Google Cloud Platform (GCP) project-specific configuration code that is for private usage only.\\nWhen creating a name for the repository, it has to contain the GCP project they are connected to minus the customer,\\nenvironment and location. Furthermore, it has to end with `-config`.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n","tokens":21,"id":2725,"text":"## Context\\nWe feel the need to define rules to select the correct GitHub organisation for source repositories.\\n\n\n##Decision\nWe identify rules for the GitHub organisation, the source repository name format and basic source code guidelines.\\n### VWT Digital\\nThis is the main organisation for all Operational Data Hub (ODH) specific source code. When creating a new repository,\\nit **has to pass** one of the following applications:\\n- An ODH-specific solution;\\n- A solution that transcends multiple solution domains (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration;\\n- Any solution business logic.\\nWhen creating a name for the repository, it **has to start** with either `odh-<repo_name>` or `dat-<repo_name>`:\\n- `odh-`: When the repository contains source code specifically made for the Operational Data Hub;\\n- `dat-`: When the repository contains source code created for generic usage within the Digital Ambition Team domain.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Solutions\\nThis is the main organisation for all Solution-specific source code. When creating a new repository, it **has to pass** the following application:\\n- A specific application for a domain solution, only used by one domain (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).\\nIn addition, these repositories **may not** contain:\\n- Any configuration.\\nWhen creating a name for the repository, it has to start with the abbreviation of the solutions it is a part of.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n### VWT Digital Configuration\\nThis is the main organisation for all configuration code. When creating a new repository, it **has to pass** the following application:\\n- Google Cloud Platform (GCP) project-specific configuration code that is for private usage only.\\nWhen creating a name for the repository, it has to contain the GCP project they are connected to minus the customer,\\nenvironment and location. Furthermore, it has to end with `-config`.\\nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more elaborate explanation.\\n","Predictions":"We identify rules for the GitHub organisation, the source repository name format and basic source code guidelines.n### VWT DigitalnThis is the main organisation for all Operational Data Hub (ODH) specific source code. When creating a new repository,nit **has to pass** one of the following applications:n- An ODH-specific solution;n- A solution that transcends multiple solution domains (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).nIn addition, these repositories **may not** contain:n- Any configuration;n- Any solution business logic.nWhen creating a name for the repository, it **has to start** with either odh-repo_name> or dat-repo_name>:n- odh-: When the repository contains source code specifically made for the Operational Data Hub;n- dat-: When the repository contains source code created for generic usage within the Digital Ambition Team domain.nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more detailed explanation.n### VWT Digital SolutionsnThis is the main organisation for all Solution-specific source code. When creating a new repository, it **has to pass** the following application:n- A specific application for a domain solution, only used by one domain (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).nIn addition, these repositories **may not** contain:n- Any configuration.nWhen creating a name for the repository, it **has to start** with either odh-repo_name> or dat-repo_name>:n- odh-: When the repository contains source code specifically made for the Operational Data Hub;n- dat-: When the repository contains source code created for generic usage within the Digital Ambition Team domain.nSee our [repo naming convention](0002-repo-naming-conventions.md#4-normal-vwt-dat-repositories) for a more detailed explanation.n### VWT Digital SolutionsnThis is the main organisation for all Solution-specific source code. When creating a new repository, it **has to pass** the following application:n- A specific application for a domain solution, only used by one domain (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnamingconvention-Domains)).nIn addition, these repositories **may not** contain:n- Any configuration.nWhen creating a name for the repository, it **has to start** with either odh-repo_name> or dat-repo_name>:n- odh-: When the repository contains source code specifically made for the Operational Data Hub;n- dat-: When the repository contains source code created for generic usage within the Digital Ambition Team domain.nSee our [repo naming convention](0002-repo-naming-convention-Domains) for a more detailed explanation.n### VWT Digital SolutionsnThis is the main organisation for all Solution-specific source code. When creating a new repository, it **has to pass** the following application:n- A specific application for a domain solution, only used by one domain (see [solution domains](https:\/\/recognize.atlassian.net\/wiki\/spaces\/DAT\/pages\/1304166628\/Cloud+naming+convention#Cloudnaming"}
{"File Name":"architecture\/0003-monitor-condition-and-data-selectors.md","Context":"## Context\nA lot of Home Assistant integrations use config options like `CONF_MONITORED_CONDITIONS` to allow the user to select which data the integration should expose from the data. This means that the user needs to know what the different data points mean while setting up the integration. While configuring its Lovelace UI, the user has the option to include the entity or not. This means that we allow the user to pick twice.\n","Decision":"Integrations should expose all available data to the backend if that data is fetched in a single API request.\\nIntegrations should only include selector logic if it make sense in the context of interface, like if it would require extra requests. User should not have to read the available documentation and API descriptions to find out which data they want have.\\n```\\nLayer model:\\nFetch            Manage           View\\n-------------      ---------      ------------\\n| Interface | ---> | Core  | ---> | Frontend |\\n-------------      ---------      ------------\\n```\\nIntegrations can set the `entity_registry_enabled_default` property on their entity objects to instruct Home Assistant to disable certain entities by default ([docs](https:\/\/developers.home-assistant.io\/docs\/entity_registry_disabled_by\/#integrations-setting-default-value-of-disabled_by-for-new-entity-registry-entries)).\\n","tokens":90,"id":1417,"text":"## Context\\nA lot of Home Assistant integrations use config options like `CONF_MONITORED_CONDITIONS` to allow the user to select which data the integration should expose from the data. This means that the user needs to know what the different data points mean while setting up the integration. While configuring its Lovelace UI, the user has the option to include the entity or not. This means that we allow the user to pick twice.\\n\n\n##Decision\nIntegrations should expose all available data to the backend if that data is fetched in a single API request.\\nIntegrations should only include selector logic if it make sense in the context of interface, like if it would require extra requests. User should not have to read the available documentation and API descriptions to find out which data they want have.\\n```\\nLayer model:\\nFetch            Manage           View\\n-------------      ---------      ------------\\n| Interface | ---> | Core  | ---> | Frontend |\\n-------------      ---------      ------------\\n```\\nIntegrations can set the `entity_registry_enabled_default` property on their entity objects to instruct Home Assistant to disable certain entities by default ([docs](https:\/\/developers.home-assistant.io\/docs\/entity_registry_disabled_by\/#integrations-setting-default-value-of-disabled_by-for-new-entity-registry-entries)).\\n","Predictions":"- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector field should be a default field.n- The selector"}
{"File Name":"celestia-core\/adr-004-historical-validators.md","Context":"## Context\nRight now, we can query the present validator set, but there is no history.\nIf you were offline for a long time, there is no way to reconstruct past validators. This is needed for the light client and we agreed needs enhancement of the API.\n","Decision":"For every block, store a new structure that contains either the latest validator set,\\nor the height of the last block for which the validator set changed. Note this is not\\nthe height of the block which returned the validator set change itself, but the next block,\\nie. the first block it comes into effect for.\\nStoring the validators will be handled by the `state` package.\\nAt some point in the future, we may consider more efficient storage in the case where the validators\\nare updated frequently - for instance by only saving the diffs, rather than the whole set.\\nAn alternative approach suggested keeping the validator set, or diffs of it, in a merkle IAVL tree.\\nWhile it might afford cheaper proofs that a validator set has not changed, it would be more complex,\\nand likely less efficient.\\n","tokens":56,"id":4101,"text":"## Context\\nRight now, we can query the present validator set, but there is no history.\\nIf you were offline for a long time, there is no way to reconstruct past validators. This is needed for the light client and we agreed needs enhancement of the API.\\n\n\n##Decision\nFor every block, store a new structure that contains either the latest validator set,\\nor the height of the last block for which the validator set changed. Note this is not\\nthe height of the block which returned the validator set change itself, but the next block,\\nie. the first block it comes into effect for.\\nStoring the validators will be handled by the `state` package.\\nAt some point in the future, we may consider more efficient storage in the case where the validators\\nare updated frequently - for instance by only saving the diffs, rather than the whole set.\\nAn alternative approach suggested keeping the validator set, or diffs of it, in a merkle IAVL tree.\\nWhile it might afford cheaper proofs that a validator set has not changed, it would be more complex,\\nand likely less efficient.\\n","Predictions":"We will use the state package to store the current validator set.n"}
{"File Name":"ksch-workflows\/0002-using-spring-boot.md","Context":"## Context\nWe need [dependency injection](https:\/\/stackoverflow.com\/questions\/130794\/what-is-dependency-injection) in order to build decoupled software units.\nAlso we need access on a relational database.\n","Decision":"Spring offers a lot of convenience features for those purposes.\\nApache Wicket already provides a Spring integration.\\nWith the help of the [wicket-spring-boot](https:\/\/github.com\/MarcGiffing\/wicket-spring-boot) the convenience feature of Spring Boot can be applied on top of that.\\nThis makes the development faster and easier than using the alternative of dependency injection with [Google Guice](http:\/\/software.danielwatrous.com\/wicket-guice-including-unittests\/).\\nFurther usage of Spring enables potential useage of the various Spring sub-projects for advanced requirements like the integration of other software systems.\\n","tokens":46,"id":1758,"text":"## Context\\nWe need [dependency injection](https:\/\/stackoverflow.com\/questions\/130794\/what-is-dependency-injection) in order to build decoupled software units.\\nAlso we need access on a relational database.\\n\n\n##Decision\nSpring offers a lot of convenience features for those purposes.\\nApache Wicket already provides a Spring integration.\\nWith the help of the [wicket-spring-boot](https:\/\/github.com\/MarcGiffing\/wicket-spring-boot) the convenience feature of Spring Boot can be applied on top of that.\\nThis makes the development faster and easier than using the alternative of dependency injection with [Google Guice](http:\/\/software.danielwatrous.com\/wicket-guice-including-unittests\/).\\nFurther usage of Spring enables potential useage of the various Spring sub-projects for advanced requirements like the integration of other software systems.\\n","Predictions":"We will use a Serializer library to serialize the data.n"}
{"File Name":"deeplearning4j\/0001-SameDiff_File_Format.md","Context":"## Context\nSameDiff models need to be serializable - i.e., something we can save to disk or send over the network.\nAdditionally, we need to be able to save and load model files in C++, and have those be readable in other languages (mainly Java).\nCurrently, we have a FlatBuffers-based format for SameDiff graph serialization, but it has a number of problems, as discussed in this issue: https:\/\/github.com\/eclipse\/deeplearning4j\/issues\/8312\n","Decision":"We will transition from a pure FlatBuffers to a Zip + FlatBuffers model format.\\nFlatBuffers will be used for the graph structure only. Parameters will be stored separately to the graph structure, also within the zip.\\nWe will introduce the ability to support multiple versions of a graph in the model files.\\nThis will enable the model file to support storing\\n* Multiple data types (for example, a FP32 version and a quantized INT8 version)\\n* Multiple different checkpoints (parameters after 1000 iterations, after 5000, and so on)\\n* Multiple versions of a given model (English vs. Chinese, or cased\/uncased, etc)\\nBy default when loading a graph (unless it is otherwise specified) we will load the most recent model tag.\\nTags must be valid file\/folder identifiers, and are not case sensitive.\\nThe structure of the zip file will be as follows:\\n```\\ntags.txt                         \/\/List of graph tags, one per line, in UTF8 format, no duplicates. Oldest first, newest last\\n<tag_name>\/graph.fb              \/\/The graph structure, in FlatBuffers format\\n<tag_name>\/params.txt            \/\/The mapping between variable names and parameter file names\\n<tag_name>\/params\/*.fb           \/\/The set of NDArrays that are the parameters, in FlatBuffers format\\n<tag_name>\/trainingConfig.fb     \/\/The training configuration - updater, learning rate, etc\\n<tag_name>\/updater.txt           \/\/The mapping between variable names and the updater state file names\\n<tag_name>\/updater\/*.fb          \/\/The set of NDArrays that are the updater state\\n```\\nNote that params.txt will allow for parameter sharing via references to other parameters:\\n```\\nmy_normal_param 0\\nshared_param <other_tag_name>\/7\\n```\\nThis means the parameters values for parameter \"my_normal_param\" are present at `<tag_name>\/params\/0.fb` within the zip file, and the parameter values for \"shared_param\" are available at `<other_tag_name>\/params\/7.fb`\\nNote also that the motivation for using the params.txt file (instead of the raw parameter name as the file name) is that some parameters will have invalid or ambiguous file names - \"my\/param\/name\", \"&MyParam*\" etc\\nIn terms of updater state, they will be stored in a similar format. For example, for the Adam updater with the M and V state arrays (each of same shape as the parameter):\\n```\\nmy_param 0 1\\nother_param 2 3\\n```\\nThat means my_param(M) is `<tag_name>\/updater\/0.fb` and my_param(V) is at `<tag_name>\/updater\/1.fb`\\nThis format also allows for updater state sharing, if we need it.\\n**Graph Structure**\\nThe graph structure will be encoded in FlatBuffers format using a schema with 2 parts:\\n1. A list of variables - each with name, datatype, and (for placeholders, constants and parameters) a shape\\n2. A list of operations - each with a name, op name\/type, input variable names, output variable names, and arguments\\nNote that both legacy and custom ops will be encoded in the same way. For legacy ops, we simply need the operation type, and the operation number.\\nOperation argument encoding will be done using named arguments: essentially, a `Map<String,T>` structure, where T is one of `{long, double, boolean, datatype}`.\\nThis allows for improved backward compatibility (no ambiguity as ops are modified after a graph file was written) and improved interpretability compared to using simple arrays of iargs, bargs, targs and dargs.\\nOne consequence\/downside of this is that we need to define mapping between our named arguments and iargs\/bargs\/targs\/dargs. In Java we have essentially done this manually, though clearly don't want to replicate this work in C++ (or any future languages).\\nTo avoid the need to do a significant amount of work (such as moving the name\/arg mapping to code generation) the following is proposed:\\nThe `Map<String,T>` is split up in the FlatBuffers schema into 4 pairs of fields.\\n* `String[] iArgNames`, `long[] iArgs`\\n* `String[] tArgNames`, `double[] dArgs`\\n* `String[] bArgNames`, `boolean[] bArgs`\\n* `String[] dArgNames`, `DataType[] dArgs`\\nClearly the name and value arrays (for each pair) would each be the same length, and name\/value correspondence is by array index.\\nThis is essentially equivalent to the `Map<String,T>` representation, but has the benefit of not needing us to define the mapping for named args to array-style args any time soon in C++, but also allowing us to add it in the future (mainly before we can write graphs from C++, or have better\/proper backward compatibility after op changes)\\n**Extensibility to Other Types**\\nSuppose in the future we want to store other data for a variable, not just an array?\\nExamples include lists and maps (for example, for NLP applications).\\nWhile we will not implement this right now, there are a number of options for adding this without breaking backward compatibility.\\nFirst: we can enhance the params.txt file format, perhaps using something like the following:\\n```\\nmap_param 0 MAP\\n```\\nSecond: We can add a similar text file for other types. For example, a params_maps.txt, same format as params.txt, with content at `<tag_name>\/params_maps\/*.fb`\\n","tokens":104,"id":2944,"text":"## Context\\nSameDiff models need to be serializable - i.e., something we can save to disk or send over the network.\\nAdditionally, we need to be able to save and load model files in C++, and have those be readable in other languages (mainly Java).\\nCurrently, we have a FlatBuffers-based format for SameDiff graph serialization, but it has a number of problems, as discussed in this issue: https:\/\/github.com\/eclipse\/deeplearning4j\/issues\/8312\\n\n\n##Decision\nWe will transition from a pure FlatBuffers to a Zip + FlatBuffers model format.\\nFlatBuffers will be used for the graph structure only. Parameters will be stored separately to the graph structure, also within the zip.\\nWe will introduce the ability to support multiple versions of a graph in the model files.\\nThis will enable the model file to support storing\\n* Multiple data types (for example, a FP32 version and a quantized INT8 version)\\n* Multiple different checkpoints (parameters after 1000 iterations, after 5000, and so on)\\n* Multiple versions of a given model (English vs. Chinese, or cased\/uncased, etc)\\nBy default when loading a graph (unless it is otherwise specified) we will load the most recent model tag.\\nTags must be valid file\/folder identifiers, and are not case sensitive.\\nThe structure of the zip file will be as follows:\\n```\\ntags.txt                         \/\/List of graph tags, one per line, in UTF8 format, no duplicates. Oldest first, newest last\\n<tag_name>\/graph.fb              \/\/The graph structure, in FlatBuffers format\\n<tag_name>\/params.txt            \/\/The mapping between variable names and parameter file names\\n<tag_name>\/params\/*.fb           \/\/The set of NDArrays that are the parameters, in FlatBuffers format\\n<tag_name>\/trainingConfig.fb     \/\/The training configuration - updater, learning rate, etc\\n<tag_name>\/updater.txt           \/\/The mapping between variable names and the updater state file names\\n<tag_name>\/updater\/*.fb          \/\/The set of NDArrays that are the updater state\\n```\\nNote that params.txt will allow for parameter sharing via references to other parameters:\\n```\\nmy_normal_param 0\\nshared_param <other_tag_name>\/7\\n```\\nThis means the parameters values for parameter \"my_normal_param\" are present at `<tag_name>\/params\/0.fb` within the zip file, and the parameter values for \"shared_param\" are available at `<other_tag_name>\/params\/7.fb`\\nNote also that the motivation for using the params.txt file (instead of the raw parameter name as the file name) is that some parameters will have invalid or ambiguous file names - \"my\/param\/name\", \"&MyParam*\" etc\\nIn terms of updater state, they will be stored in a similar format. For example, for the Adam updater with the M and V state arrays (each of same shape as the parameter):\\n```\\nmy_param 0 1\\nother_param 2 3\\n```\\nThat means my_param(M) is `<tag_name>\/updater\/0.fb` and my_param(V) is at `<tag_name>\/updater\/1.fb`\\nThis format also allows for updater state sharing, if we need it.\\n**Graph Structure**\\nThe graph structure will be encoded in FlatBuffers format using a schema with 2 parts:\\n1. A list of variables - each with name, datatype, and (for placeholders, constants and parameters) a shape\\n2. A list of operations - each with a name, op name\/type, input variable names, output variable names, and arguments\\nNote that both legacy and custom ops will be encoded in the same way. For legacy ops, we simply need the operation type, and the operation number.\\nOperation argument encoding will be done using named arguments: essentially, a `Map<String,T>` structure, where T is one of `{long, double, boolean, datatype}`.\\nThis allows for improved backward compatibility (no ambiguity as ops are modified after a graph file was written) and improved interpretability compared to using simple arrays of iargs, bargs, targs and dargs.\\nOne consequence\/downside of this is that we need to define mapping between our named arguments and iargs\/bargs\/targs\/dargs. In Java we have essentially done this manually, though clearly don't want to replicate this work in C++ (or any future languages).\\nTo avoid the need to do a significant amount of work (such as moving the name\/arg mapping to code generation) the following is proposed:\\nThe `Map<String,T>` is split up in the FlatBuffers schema into 4 pairs of fields.\\n* `String[] iArgNames`, `long[] iArgs`\\n* `String[] tArgNames`, `double[] dArgs`\\n* `String[] bArgNames`, `boolean[] bArgs`\\n* `String[] dArgNames`, `DataType[] dArgs`\\nClearly the name and value arrays (for each pair) would each be the same length, and name\/value correspondence is by array index.\\nThis is essentially equivalent to the `Map<String,T>` representation, but has the benefit of not needing us to define the mapping for named args to array-style args any time soon in C++, but also allowing us to add it in the future (mainly before we can write graphs from C++, or have better\/proper backward compatibility after op changes)\\n**Extensibility to Other Types**\\nSuppose in the future we want to store other data for a variable, not just an array?\\nExamples include lists and maps (for example, for NLP applications).\\nWhile we will not implement this right now, there are a number of options for adding this without breaking backward compatibility.\\nFirst: we can enhance the params.txt file format, perhaps using something like the following:\\n```\\nmap_param 0 MAP\\n```\\nSecond: We can add a similar text file for other types. For example, a params_maps.txt, same format as params.txt, with content at `<tag_name>\/params_maps\/*.fb`\\n","Predictions":"We will transition from a pure FlatBuffers to a Zip + FlatBuffers model format.nFlatBuffers will be used for the graph structure only. Parameters will be stored separately to the graph structure, also within the zip.nWe will introduce the ability to support multiple versions of a graph in the model files.nThis will enable the model file to support storingn* Multiple data types (for example, a FP32 version and a quantized INT8 version)n* Multiple different checkpoints (parameters after 1000 iterations, after 5000, and so on)n* Multiple versions of a given model (English vs. Chinese, or cased\/uncased, etc)nBy default when loading a graph (unless it is otherwise specified) we will load the most recent model tag.nTags must be valid file\/folder identifiers, and are not case sensitive.nThe structure of the zip file will be as follows:ntag_name>\/graph.fb \/\/ The graph structure, in FlatBuffers formatntag_name>\/params.txt \/\/ The mapping between variable names and parameter file namesntag_name>\/params\/*.fb \/\/ The set of NDArrays that are the parameters, in FlatBuffers formatntag_name>\/trainingConfig.fb \/\/ The training configuration - updaterntag_name>\/trainingConfig.fb \/\/ The training configuration - updaterntag_name>\/trainingConfig.fb \/\/ The training configuration - updaternnTags must be valid file\/folder identifiers, and are not case sensitive.nThe structure of the zip file will be as follows:ntag_name>\/graph.fb \/\/ The graph structure, in FlatBuffers formatntag_name>\/params.txt \/\/ The mapping between variable names and parameter file namesntag_name>\/params\/*.fb \/\/ The set of NDArrays that are the parameters, in FlatBuffers formatntag_name>\/trainingConfig.fb \/\/ The training configuration - updaterntag_name>\/trainingConfig.fb \/\/ The training configuration - updaterntag_name>\/trainingConfig.fb \/\/ The training configuration - updaternnTags must be valid file\/folder identifiers, and are not case sensitive.nThe structure of the zip file will be as follows:ntag_name>\/graph.fb \/\/ The graph structure, in FlatBuffers formatntag_name>\/params.txt \/\/ The mapping between variable names and parameter file namesntag_name>\/trainingConfig.fb \/\/ The set of NDArrays that are the parameters, in FlatBuffers formatntag_name>\/trainingConfig.fb \/\/ The training configuration - updaterntag_name>\/trainingConfig.fb \/\/ The training configuration - updaternnTags must be valid file\/folder identifiers, and are not case sensitive.nThe structure of the zip file will be as follows:ntag_name>\/graph.fb \/\/ The graph structure, in FlatBuffers formatntag_name>\/params.txt \/\/ The mapping between variable names and parameter file namesntag_name>\/trainingConfig.fb \/\/ The set of NDArrays that are the parameters, in FlatBuffers formatntag_name>\/trainingConfig.fb \/\/ The training configuration - updaterntag_name>\/trainingConfig.fb \/\/ The training configuration - updaternnTags must be valid file\/folder identifiers, and are not case sensitive.nThe structure of the zip file will be as follows:ntag_name>\/graph.fb \/\/ The structure of the graph, in FlatBuffers formatntag_name>\/params.txt \/\/ The mapping between variable names and parameter file namesntag_name>\/trainingConfig.fb \/\/ The set of NDArrays that"}
{"File Name":"google-cloud-cpp\/2018-06-13-storage-always-retries.md","Context":"**Context**: operations that change state in the server may complete with an\nindeterminate status code. For example: an error in the 500 range may be\nproduced by a middle-tier server after the operation was started by the backend.\nRe-issuing the request may not work as expected for the application developer.\n**Decision**: by default the library will retry all requests, including\nnon-idempotent ones. The library will allow application developers to override\nthe definition of what operations are automatically retried.\n**Consequences**: most operations become easier to use for application\ndevelopers. In very rare cases the operation will result in double uploads, or\nin a new generation of the object or metadata being created. In even more rare\ncases the operation may fail, for example, an operation to create an object with\n`IfGenerationMatch(0)` would fail on the second attempt.\n","Decision":"non-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n","tokens":188,"id":2466,"text":"**Context**: operations that change state in the server may complete with an\\nindeterminate status code. For example: an error in the 500 range may be\\nproduced by a middle-tier server after the operation was started by the backend.\\nRe-issuing the request may not work as expected for the application developer.\\n**Decision**: by default the library will retry all requests, including\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n\n\n##Decision\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n","Predictions":"* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n* The library will allow application developers to override the definition of what operations are automatically retried.n"}
{"File Name":"ToPS\/0001-extra-test-class-for-junit-test-with-SWI-Prolog-involved.md","Context":"## Context and Problem Statement\nTests including Prolog queries with SWI Prolog can not be executed with CircleCI because SWI Prolog is not\nrunning at the executing system.\n","Decision":"Chosen option: extra test class to reduce the effort and to still enable testing SWI Prolog locally. For this the extra class is\\nannotated with @Ignore. All other tests run with CircleCI. These tests includes the core functionalities of the developed tool.\\n","tokens":38,"id":2080,"text":"## Context and Problem Statement\\nTests including Prolog queries with SWI Prolog can not be executed with CircleCI because SWI Prolog is not\\nrunning at the executing system.\\n\n\n##Decision\nChosen option: extra test class to reduce the effort and to still enable testing SWI Prolog locally. For this the extra class is\\nannotated with @Ignore. All other tests run with CircleCI. These tests includes the core functionalities of the developed tool.\\n","Predictions":"Chosen option: [Status](https:\/\/github.com\/status-project\/status-project\/) because it is the only option that is compatible with the Status project.n"}
{"File Name":"digital-paper-edit-client\/2019-05-01-security.md","Context":"## Context and Problem Statement\nThese are notes based on the InfoSec meeting held on the 30th of May 2019.\n## Decision Drivers\n* Ease of use for journalists when system authenticates them\n* Security levels (public, protect, or restrict)\n* Data retention based on inactivity\n* Complexity\n","Decision":"* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\nWe are going with Option 2 as this provides us with security, while\\nproviding a simpler way to set up access for journalists.\\nBen Thornton has agreed to this.\\nThe benefits of this approach means that we will be\\nusing existing BBC services to implement security, rather than implementing our\\nown servers. A single VPC is sufficient for **protected** levels of service, and\\nwe can apply SecurityGroups in between to have added security.\\nThe downside of this approach is that it is not using SSO but we can add that\\nlater on.\\nIn terms of data retention, we will go with the recommended 90 days.\\n![Architecture Diagram with Security](.\/dpe-transcript-EC2_security.png)\\n","tokens":67,"id":4166,"text":"## Context and Problem Statement\\nThese are notes based on the InfoSec meeting held on the 30th of May 2019.\\n## Decision Drivers\\n* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\n\n\n##Decision\n* Ease of use for journalists when system authenticates them\\n* Security levels (public, protect, or restrict)\\n* Data retention based on inactivity\\n* Complexity\\nWe are going with Option 2 as this provides us with security, while\\nproviding a simpler way to set up access for journalists.\\nBen Thornton has agreed to this.\\nThe benefits of this approach means that we will be\\nusing existing BBC services to implement security, rather than implementing our\\nown servers. A single VPC is sufficient for **protected** levels of service, and\\nwe can apply SecurityGroups in between to have added security.\\nThe downside of this approach is that it is not using SSO but we can add that\\nlater on.\\nIn terms of data retention, we will go with the recommended 90 days.\\n![Architecture Diagram with Security](.\/dpe-transcript-EC2_security.png)\\n","Predictions":"* Ease of use for journalists when system authenticates themn* Security levels (public, protect, or restrict)n* Data retention based on inactivityn* ComplexitynWe are going with Option 2 as this provides us with security, whilenproviding a simpler way to set up access for journalists.nBen Thornton has agreed to this.nThe benefits of this approach means that we will benusing existing BBC services to implement security, rather than implementing ournown servers. A single VPC is sufficient for **protected** levels of service, andnwe can apply SecurityGroups in between to have added security.n![Architecture Diagram with Security](.\/dpe-transcript-EC2_security.png)n"}
{"File Name":"dapr\/ENG-001-tagging.md","Context":"## Context\nAs we embraced using Docker repositories to store our images, and keeping in mind we support multiple repositories along with versioning of images and different architectures,\nWe needed a way to construct an accepted and constant way of naming our Docker images.\n","Decision":"* An image will conform to the following format: \\<namespace>\/\\<repository>:\\<tag>\\n* A valid tag conforms to the following format: \\<version>-\\<architecture>, or just \\<version>, then arch is assumed Linux\\n","tokens":51,"id":74,"text":"## Context\\nAs we embraced using Docker repositories to store our images, and keeping in mind we support multiple repositories along with versioning of images and different architectures,\\nWe needed a way to construct an accepted and constant way of naming our Docker images.\\n\n\n##Decision\n* An image will conform to the following format: \\<namespace>\/\\<repository>:\\<tag>\\n* A valid tag conforms to the following format: \\<version>-\\<architecture>, or just \\<version>, then arch is assumed Linux\\n","Predictions":"* An image will conform to the following format: namespace>\/repository>:tag>n* A valid tag conforms to the following format: version>-architecture>, or just version>, then Arch is assumed Linuxn"}
{"File Name":"log4brains\/20200925-multi-packages-architecture-in-a-monorepo-with-yarn-and-lerna.md","Context":"## Context and Problem Statement\nWe have to define the initial overall architecture of the project.\nFor now, we are sure that we want to provide these features:\n- Local preview web UI\n- Static Site Generation from the CI\/CD\n- CLI to create a new ADR quickly\nIn the future, we might want to provide these features:\n- Create\/edit ADRs from the local web UI\n- VSCode extension to create and maybe edit an ADR from the IDE\n- Support ADR aggregation from multiple repositories\n","Decision":"Chosen option: \"Multi-packages, monorepo, with Yarn and Lerna\", because\\n- We don't want a monolith because we want the core library\/API to be very well tested and probably developed with DDD and hexagonal architecture. The other packages will just call this core API, they will contain fewer business rules as possible. As we are not so sure about the features we will provide in the future, this is good for extensibility.\\n- Yarn + Lerna seems to be a very good practice used by a lot of other open-source projects to publish npm packages.\\n","tokens":110,"id":1067,"text":"## Context and Problem Statement\\nWe have to define the initial overall architecture of the project.\\nFor now, we are sure that we want to provide these features:\\n- Local preview web UI\\n- Static Site Generation from the CI\/CD\\n- CLI to create a new ADR quickly\\nIn the future, we might want to provide these features:\\n- Create\/edit ADRs from the local web UI\\n- VSCode extension to create and maybe edit an ADR from the IDE\\n- Support ADR aggregation from multiple repositories\\n\n\n##Decision\nChosen option: \"Multi-packages, monorepo, with Yarn and Lerna\", because\\n- We don't want a monolith because we want the core library\/API to be very well tested and probably developed with DDD and hexagonal architecture. The other packages will just call this core API, they will contain fewer business rules as possible. As we are not so sure about the features we will provide in the future, this is good for extensibility.\\n- Yarn + Lerna seems to be a very good practice used by a lot of other open-source projects to publish npm packages.\\n","Predictions":"We will use a RailBoard library to manage the logging of the application.n"}
{"File Name":"platform\/2022-01-05-add-feature-flag-support-for-storefront-scss.md","Context":"## Context\n* There is no possibility to check for feature flags inside the Storefront SCSS.\n* Altering the SCSS depending on a feature flag will require workarounds like e.g. \"making up\" and additional class in the template and use the feature toggle in twig instead.\n* The SCSS of a selector which is hidden behind a feature flag will still be in the compiled CSS.\n* It is not easily possible to make breaking changes inside SCSS functions, mixins or variables backward-compatible with the use of feature flags.\n","Decision":"* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.\\n* The feature configuration from `Feature::getAll()` is converted to a SCSS map inside `\\Shopware\\Storefront\\Theme\\ThemeCompiler::getFeatureConfigScssMap`.\\n* This SCSS map is always added to the SCSS string which gets processed by `\\Shopware\\Storefront\\Theme\\ThemeCompiler::compileTheme`.\\n* For webpack hot-proxy the `var\/config_js_features.json` is used instead.\\n* The SCSS map looks like this: `$sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);`\\n* See https:\/\/sass-lang.com\/documentation\/values\/maps\\n* A globally available function `feature()` is used to read inside the SCSS map if a desired feature is active.\\nExample:\\n```scss\\nbody {\\n@if feature('FEATURE_NEXT_1') {\\nbackground-color: #ff0000;\\n} @else {\\nbackground-color: #ffcc00;\\n}\\n}\\n```\\n","tokens":112,"id":4508,"text":"## Context\\n* There is no possibility to check for feature flags inside the Storefront SCSS.\\n* Altering the SCSS depending on a feature flag will require workarounds like e.g. \"making up\" and additional class in the template and use the feature toggle in twig instead.\\n* The SCSS of a selector which is hidden behind a feature flag will still be in the compiled CSS.\\n* It is not easily possible to make breaking changes inside SCSS functions, mixins or variables backward-compatible with the use of feature flags.\\n\n\n##Decision\n* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.\\n* The feature configuration from `Feature::getAll()` is converted to a SCSS map inside `\\Shopware\\Storefront\\Theme\\ThemeCompiler::getFeatureConfigScssMap`.\\n* This SCSS map is always added to the SCSS string which gets processed by `\\Shopware\\Storefront\\Theme\\ThemeCompiler::compileTheme`.\\n* For webpack hot-proxy the `var\/config_js_features.json` is used instead.\\n* The SCSS map looks like this: `$sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);`\\n* See https:\/\/sass-lang.com\/documentation\/values\/maps\\n* A globally available function `feature()` is used to read inside the SCSS map if a desired feature is active.\\nExample:\\n```scss\\nbody {\\n@if feature('FEATURE_NEXT_1') {\\nbackground-color: #ff0000;\\n} @else {\\nbackground-color: #ffcc00;\\n}\\n}\\n```\\n","Predictions":"* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.n* The feature configuration from Feature::getAll() is converted to a SCSS map inside ShopwareStorefrontThemeThemeCompiler::getFeatureConfigScssMap.n* This SCSS map is always added to the SCSS string which gets processed by ShopwareStorefrontThemeThemeCompiler::compileTheme.n* For webpack hot-proxy the var\/config_js_features.json is used instead.n* The SCSS map looks like this: $sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);n* See https:\/\/sass-lang.com\/documentation\/values\/mapsn* A globally available function feature() is used to read inside the SCSS map if a desired feature is active.nExample:nscssnbody n@if feature('FEATURE_NEXT_1') nbackground-color: #ff0000;n @else nbackground-color: #ffcc00;nnn"}
{"File Name":"react-library-template\/0005-use-eslint.md","Context":"## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\nJavaScript, and has good support for TypeScript though plugins.\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":69,"id":4224,"text":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n\n\n##Decision\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","Predictions":"We will use serialize to serialize models.nWe will use serialize to serialize models.n"}
{"File Name":"tracking-consent-frontend\/0002-use-data-attribute-for-gtm-container.md","Context":"## Context and Problem Statement\nIn order to simplify the tracking consent build and deploy process and\nmake integrating with tracking consent less surprising, should\nthe configuration of the GTM container used by tracking consent be via\ndata attributes rather than separate bundles?\n## Decision Drivers\n* The need to keep things simple for service developers\n* The need to improve the operability of tracking consent\n","Decision":"* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\nChosen option: \"Use a data attribute\", because based on the benefits listed below the team\\nbelieves this is the best way forward.\\n### Positive Consequences\\n* The Javascript bundle creation process is simplified.\\n* The Scala Play routing is simplified\\n* Only one endpoint needs to be managed in production\\n* Future additional containers can be supported more easily\\n* Service developers only have to configure a single URL to tracking consent and use\\na data attribute to configure the container.\\n* The central common configuration repo (owned by a separate team) only requires a single URL to\\ntracking consent defining for each environment, rather than one for each GTM container.\\n### Negative Consequences\\n* Service developers need to add id=\"tracking-consent-script-tag\" to the SCRIPT\\ntag when integrating.\\n","tokens":78,"id":3529,"text":"## Context and Problem Statement\\nIn order to simplify the tracking consent build and deploy process and\\nmake integrating with tracking consent less surprising, should\\nthe configuration of the GTM container used by tracking consent be via\\ndata attributes rather than separate bundles?\\n## Decision Drivers\\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\n\n\n##Decision\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\nChosen option: \"Use a data attribute\", because based on the benefits listed below the team\\nbelieves this is the best way forward.\\n### Positive Consequences\\n* The Javascript bundle creation process is simplified.\\n* The Scala Play routing is simplified\\n* Only one endpoint needs to be managed in production\\n* Future additional containers can be supported more easily\\n* Service developers only have to configure a single URL to tracking consent and use\\na data attribute to configure the container.\\n* The central common configuration repo (owned by a separate team) only requires a single URL to\\ntracking consent defining for each environment, rather than one for each GTM container.\\n### Negative Consequences\\n* Service developers need to add id=\"tracking-consent-script-tag\" to the SCRIPT\\ntag when integrating.\\n","Predictions":"* The need to improve the operational efficiency of tracking consentnChosen option: \"Use a data attribute\", because based on the benefits listed below the teamnbelieves this is the best way forward.n### Positive Consequencesn* The Javascript bundle creation process is simplified.n* The Scala Play routing is simplifiedn* Only one endpoint needs to be managed in productionn* Future additional containers can be supported more easilyn* Service developers only have to configure a single URL to tracking consent and usena data attribute to configure the container.n* The central common configuration repo (owned by a separate team) only requires a single URL to tracking consent defining for each environment, rather than one for each GTM container.n### Negative Consequencesn* Service developers need to add id=\"tracking-consent-script-tag\" to the SCRIPTntag when integrating.n"}
{"File Name":"CrossyToad\/adr-0007-SUPERSEDED-mtl-style-for-affects.md","Context":"## Context\nIn Haskell we need a way to manage side-effects, particularly for games. There are a _bunch_ of\noptions including:\n- MTL Style\n- Using `IO` for everything\n- Freer Monads\nI haven't used mtl style in anger yet yet so I'm keen to give it a go.\nI'm also steaing a lot of ideas from [dino-rush](https:\/\/github.com\/jxv\/dino-rush\/) and it\nseems to be using MTL style.\n","Decision":"We're using MTL style.\\n","tokens":114,"id":2473,"text":"## Context\\nIn Haskell we need a way to manage side-effects, particularly for games. There are a _bunch_ of\\noptions including:\\n- MTL Style\\n- Using `IO` for everything\\n- Freer Monads\\nI haven't used mtl style in anger yet yet so I'm keen to give it a go.\\nI'm also steaing a lot of ideas from [dino-rush](https:\/\/github.com\/jxv\/dino-rush\/) and it\\nseems to be using MTL style.\\n\n\n##Decision\nWe're using MTL style.\\n","Predictions":"We're using stack to handle the stack and stacknrequirements.n"}
{"File Name":"volley-management\/0006-use-azure-table-storage-for-persistence.md","Context":"## Context and Problem Statement\nWe need a persistent state for the system. Previously I've used Azure SQL but it is quite pricey but has it's own set of features including relational model. But given that system is not very complex I can model persistence mechanism for NoSQL model.\n## Decision Drivers\n* Cost\n* Backup capabilities\n","Decision":"* Cost\\n* Backup capabilities\\nChosen option: \"Azure Table Storage\", because cost.\\n### Positive Consequences <!-- optional -->\\n* Persistence bill should be down from ~$5\/month to less than $1\/month\\n### Negative Consequences <!-- optional -->\\n* Students in IT Academy won't be exposed to a relational model, which at this moment dominates work they will have to be doing. We will have to come up with a strategy to get them good experience.\\n","tokens":68,"id":2520,"text":"## Context and Problem Statement\\nWe need a persistent state for the system. Previously I've used Azure SQL but it is quite pricey but has it's own set of features including relational model. But given that system is not very complex I can model persistence mechanism for NoSQL model.\\n## Decision Drivers\\n* Cost\\n* Backup capabilities\\n\n\n##Decision\n* Cost\\n* Backup capabilities\\nChosen option: \"Azure Table Storage\", because cost.\\n### Positive Consequences <!-- optional -->\\n* Persistence bill should be down from ~$5\/month to less than $1\/month\\n### Negative Consequences <!-- optional -->\\n* Students in IT Academy won't be exposed to a relational model, which at this moment dominates work they will have to be doing. We will have to come up with a strategy to get them good experience.\\n","Predictions":"* Costn* Backup capabilitiesnChosen option: \"Azure Table Storage\", because cost.n### Positive Consequences!-- optional -->n* Persistence bill should be down from $5\/month to less than $1\/monthn### Negative Consequences!-- optional -->n* Students in IT Academy won't be exposed to a relational model, which at this moment dominates work they will have to be doing.n"}
{"File Name":"devops-challenge\/0009-doc-about-monorepo.md","Context":"## Context\nMicroservices are complex because they are small and can use different languages\nand different stacks to run: java, spring, jakarta, python, flask, tornado, ruby\nrails, sinatra, mysql, postgresql, mongodb, redis...\nThere are so heterogeneous that sometimes we need to set a common way to work with\nthem.  Each microservice has a different life-cycle, some are update more\nfrequently, others are not.\nUsually, when apps differs in their life-cycle speed, incompatibilities will come\nup.  Those incompatibilities can make the system get down.  And keeping an\nupdated and acurated matrix of compatibilities is a pain.\nA way to minimize this risks is to put all code in the same repository, but not as\na monolithic application, but in separate folders.  This has some advantages\nlike:\n- all code is tested at the same time\n- transversal refactors can be easy\n- compatibility matrices are simplified\n","Decision":"To use a monorepo to keep all code together and deploy it at the same time.\\n","tokens":210,"id":3483,"text":"## Context\\nMicroservices are complex because they are small and can use different languages\\nand different stacks to run: java, spring, jakarta, python, flask, tornado, ruby\\nrails, sinatra, mysql, postgresql, mongodb, redis...\\nThere are so heterogeneous that sometimes we need to set a common way to work with\\nthem.  Each microservice has a different life-cycle, some are update more\\nfrequently, others are not.\\nUsually, when apps differs in their life-cycle speed, incompatibilities will come\\nup.  Those incompatibilities can make the system get down.  And keeping an\\nupdated and acurated matrix of compatibilities is a pain.\\nA way to minimize this risks is to put all code in the same repository, but not as\\na monolithic application, but in separate folders.  This has some advantages\\nlike:\\n- all code is tested at the same time\\n- transversal refactors can be easy\\n- compatibility matrices are simplified\\n\n\n##Decision\nTo use a monorepo to keep all code together and deploy it at the same time.\\n","Predictions":"To use a monorepo to keep all code together and deploy it at the same time.n"}
{"File Name":"occurrent\/0002-mongodb-cloudevent-serialization.md","Context":"## Context\nCurrently, Occurrent is doing \"unnecessary\" work when converting from a `CloudEvent` to `Document` and vice versa\nSee [issue 196](https:\/\/github.com\/cloudevents\/sdk-java\/issues\/196) in the cloud event java sdk project.\n","Decision":"None yet\\n","tokens":59,"id":4382,"text":"## Context\\nCurrently, Occurrent is doing \"unnecessary\" work when converting from a `CloudEvent` to `Document` and vice versa\\nSee [issue 196](https:\/\/github.com\/cloudevents\/sdk-java\/issues\/196) in the cloud event java sdk project.\\n\n\n##Decision\nNone yet\\n","Predictions":"The jest-fetch-mock package will be used for unittests.n"}
{"File Name":"ReportMI-service-manual\/0014-use-gov-uk-notify-for-sending-notifications.md","Context":"## Context\nThe Data Submission Service will need to send notifications to suppliers at\nvarious stages during the submission process.\nThe notifications will, for example, include:\n- Telling a supplier they have a task to complete\n- Telling a supplier that a deadline is approaching\n- Telling a supplier that their task is overdue\n- Providing a receipt for a submission\nInitially, the notifications will be email, but may also need to be SMS based in\nfuture.\nTo reduce the need for each organisation or service team to build it's own\ninfrastructure for sending email and SMS based messages, the Government Digital\nService (GDS) has built [GOV.UK Notify][service-notify].\nGOV.UK Notify can send notifications either via an API call, or by uploading a\nCSV to the website. Sending email is free.\nThe service is already used in various parts of CCS.\n","Decision":"We will use GOV.UK notify to send notifications for Data Submission Service.\\nFor MVP we may use the CSV upload function to send notifications, but this will\\nbe replaced by automated API calls later in development.\\n","tokens":188,"id":2050,"text":"## Context\\nThe Data Submission Service will need to send notifications to suppliers at\\nvarious stages during the submission process.\\nThe notifications will, for example, include:\\n- Telling a supplier they have a task to complete\\n- Telling a supplier that a deadline is approaching\\n- Telling a supplier that their task is overdue\\n- Providing a receipt for a submission\\nInitially, the notifications will be email, but may also need to be SMS based in\\nfuture.\\nTo reduce the need for each organisation or service team to build it's own\\ninfrastructure for sending email and SMS based messages, the Government Digital\\nService (GDS) has built [GOV.UK Notify][service-notify].\\nGOV.UK Notify can send notifications either via an API call, or by uploading a\\nCSV to the website. Sending email is free.\\nThe service is already used in various parts of CCS.\\n\n\n##Decision\nWe will use GOV.UK notify to send notifications for Data Submission Service.\\nFor MVP we may use the CSV upload function to send notifications, but this will\\nbe replaced by automated API calls later in development.\\n","Predictions":"We will use GOV.UK notify to send notifications for Data Submission Service.nFor MVP we may use the CSV upload function to send notifications, but this willnbe replaced by automated API calls later in development.n"}
{"File Name":"operational-data-hub\/0003-create-cloud-native-solutions.md","Context":"## Context\nCloud-native architecture fully takes advantage of the [serverless computing](0002-use-serverless-infra-components.md) to maximise your focus on writing code, creating business value and keeping customers happy.\nAll the developer has to worry about is orchestrating all the infrastructure they need ([sofware defined](0004-create-software-defined-everything.md)) and the application code itself.\n","Decision":"We will build cloud-native solutions.\\n","tokens":83,"id":2752,"text":"## Context\\nCloud-native architecture fully takes advantage of the [serverless computing](0002-use-serverless-infra-components.md) to maximise your focus on writing code, creating business value and keeping customers happy.\\nAll the developer has to worry about is orchestrating all the infrastructure they need ([sofware defined](0004-create-software-defined-everything.md)) and the application code itself.\\n\n\n##Decision\nWe will build cloud-native solutions.\\n","Predictions":"We will build cloud-native solutions.n"}
{"File Name":"ockam\/0001-record-architectural-decisions.md","Context":"## Context\nWe need to record the architectural decisions that we make as we develop Ockam.\n","Decision":"We will keep a collection of records for \"architecturally significant\" decisions: those that\\naffect the structure, non-functional characteristics, dependencies, interfaces, or construction\\ntechniques.\\nWe will use Architecture Decision Records, as [described by Michael Nygard](1).\\n","tokens":21,"id":4539,"text":"## Context\\nWe need to record the architectural decisions that we make as we develop Ockam.\\n\n\n##Decision\nWe will keep a collection of records for \"architecturally significant\" decisions: those that\\naffect the structure, non-functional characteristics, dependencies, interfaces, or construction\\ntechniques.\\nWe will use Architecture Decision Records, as [described by Michael Nygard](1).\\n","Predictions":"We will keep a collection of records for \"architecturally significant\" decisions: those that affect the structure, non-functional characteristics, dependencies, interfaces, or construction techniques.nWe will use Architecture Decision Records, as [described by Michael Nygard](1).n"}
{"File Name":"aspan-server\/0003-replacing-ramda-with-lodash.md","Context":"## Context\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\n","Decision":"Decision here...\\n","tokens":22,"id":1890,"text":"## Context\\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\\n\n\n##Decision\nDecision here...\\n","Predictions":"here...n"}
{"File Name":"beis-report-official-development-assistance\/0011-use-govuk-design-system-form-builder.md","Context":"## Context\nBuilding forms in Rails that are compliant with the GOVUK Design System involve\nmanually declaring the correct HTML structure, class names and ARIA attributes,\nwhich is time-consuming and hard to get right.\nAdditionally, our validation errors currently use Rails' default pattern, rather\nthan the one recommended for use as part of the design system, which is designed\nwith accessibility in mind.\n","Decision":"We will use DfE's `govuk_design_system_formbuilder` to simplify the creation of\\nGOV.UK Design System-compliant forms.\\nAs we are currently using Simple Form rather than Rails' default form builder\\nfor our other forms, the two form builders can co-exist for the time being,\\nwhilst we transition the forms over.\\n","tokens":83,"id":2386,"text":"## Context\\nBuilding forms in Rails that are compliant with the GOVUK Design System involve\\nmanually declaring the correct HTML structure, class names and ARIA attributes,\\nwhich is time-consuming and hard to get right.\\nAdditionally, our validation errors currently use Rails' default pattern, rather\\nthan the one recommended for use as part of the design system, which is designed\\nwith accessibility in mind.\\n\n\n##Decision\nWe will use DfE's `govuk_design_system_formbuilder` to simplify the creation of\\nGOV.UK Design System-compliant forms.\\nAs we are currently using Simple Form rather than Rails' default form builder\\nfor our other forms, the two form builders can co-exist for the time being,\\nwhilst we transition the forms over.\\n","Predictions":"We will use [Semantic Query Language](https:\/\/github.com\/SemanticQuery\/SemanticQuery) for implementing all of Alfa. Being a superset of JavaScript, TypeScript has a low learning curve for people already familiar with JavaScript while providing a solid type system. We will enforce API contracts through generation of [declaration files](https:\/\/www.typescript.org\/) through generation of [declaration files](https:\/\/www.typescript.org\/).n"}
{"File Name":"ios-architecture-decision-logs\/0012-use-networkManager-for-network-requests.md","Context":"## Context\nOur team created NetworkManager for network requests. For consistency we should replace and use NetworkManager instead of Fetchable protocol\n","Decision":"Every new interactors must use NetworkManager. Also every new endpoints should use new networker approach\\n","tokens":27,"id":4974,"text":"## Context\\nOur team created NetworkManager for network requests. For consistency we should replace and use NetworkManager instead of Fetchable protocol\\n\n\n##Decision\nEvery new interactors must use NetworkManager. Also every new endpoints should use new networker approach\\n","Predictions":"We will use REST_API to provide a single API for all services.n"}
{"File Name":"aspan_micro_front_ends\/0004-info-graphql-import.md","Context":"## Context\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\n","Decision":"Decision here...\\n","tokens":20,"id":3194,"text":"## Context\\nImport & export definitions in GraphQL SDL (also refered to as GraphQL modules)\\n\n\n##Decision\nDecision here...\\n","Predictions":"Decision here...n"}
{"File Name":"fxa\/0016-use-graphql-and-apollo-for-settings-redesign.md","Context":"## Context and Problem Statement\nThe [Settings Redesign project](https:\/\/jira.mozilla.com\/browse\/FXA-840) will be created [as a new React application](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) and in turn, has opened the door to assess certain pieces of our technology stack.\n[GraphQL](https:\/\/graphql.org\/), or GQL, is not a database query language, but is instead a query language for APIs. It _describes_ data requirements, and is a powerful alternative to REST. Some benefits can be gained by using it on top of existing REST architecture.\n\"Apollo\" in this document refers to the \"Apollo client\" and \"Apollo server\" pieces of the Apollo platform\u00b9, which can be described as a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\nThis ADR serves to lay out pros and cons of using GraphQL and Apollo in the Settings Redesign project as an alternative to hitting our conventional REST endpoints.\n\u00b9Apollo also offers Apollo Graph Manager and Apollo Federation which are paid services, [read more from their docs](https:\/\/www.apollographql.com\/docs\/intro\/platform\/). We do not need to use these to use GQL with Apollo server or Apollo client.\n## Decision Drivers\n- Performance implications; consideration around the number of network requests and data transferred\n- Ease of setup for client-side API calls\n- Clarity around expected data, React integration, and developer tooling\n- Development speed, both around initial setup and as new features roll out\n","Decision":"- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\nChosen option: \"B - Layer GraphQL on top of our REST architecture\", because:\\n- GQL offers performance optimizations by allowing us to consolidate our network requests by sending data requirements up in a single request, asking for only what is needed on the client, shifting the burden of figuring out how and where to gather this data from the client onto the server. The server then compensates for overfetching by only sending back what is requested.\\n- Allowing developers to query for and expect only exactly what is needed with end-to-end typing and a more \"declarative\" way of thinking towards data requirements, along with keeping data requirement schemas close to where they're consumed, makes it painfully clear what's being sent and received between client and server.\\n- Does not preclude option C, as we can replace or supplement direct calls to the FxA auth-server down the line. This is faster for initial development, and this option over option C will also help mitigate risk around using a relatively novel piece of technology for FxA.\\n- Nice side-effects include the GQL playground, managing only a single API endpoint, and the ability to store both local state and network data in the Apollo cache.\\n","tokens":366,"id":387,"text":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/jira.mozilla.com\/browse\/FXA-840) will be created [as a new React application](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) and in turn, has opened the door to assess certain pieces of our technology stack.\\n[GraphQL](https:\/\/graphql.org\/), or GQL, is not a database query language, but is instead a query language for APIs. It _describes_ data requirements, and is a powerful alternative to REST. Some benefits can be gained by using it on top of existing REST architecture.\\n\"Apollo\" in this document refers to the \"Apollo client\" and \"Apollo server\" pieces of the Apollo platform\u00b9, which can be described as a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\\nThis ADR serves to lay out pros and cons of using GraphQL and Apollo in the Settings Redesign project as an alternative to hitting our conventional REST endpoints.\\n\u00b9Apollo also offers Apollo Graph Manager and Apollo Federation which are paid services, [read more from their docs](https:\/\/www.apollographql.com\/docs\/intro\/platform\/). We do not need to use these to use GQL with Apollo server or Apollo client.\\n## Decision Drivers\\n- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\n\n\n##Decision\n- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\nChosen option: \"B - Layer GraphQL on top of our REST architecture\", because:\\n- GQL offers performance optimizations by allowing us to consolidate our network requests by sending data requirements up in a single request, asking for only what is needed on the client, shifting the burden of figuring out how and where to gather this data from the client onto the server. The server then compensates for overfetching by only sending back what is requested.\\n- Allowing developers to query for and expect only exactly what is needed with end-to-end typing and a more \"declarative\" way of thinking towards data requirements, along with keeping data requirement schemas close to where they're consumed, makes it painfully clear what's being sent and received between client and server.\\n- Does not preclude option C, as we can replace or supplement direct calls to the FxA auth-server down the line. This is faster for initial development, and this option over option C will also help mitigate risk around using a relatively novel piece of technology for FxA.\\n- Nice side-effects include the GQL playground, managing only a single API endpoint, and the ability to store both local state and network data in the Apollo cache.\\n","Predictions":"* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store.n* We will use a Security database to store the data we need to store."}
{"File Name":"Maud\/0018-backwards-differential-solver.md","Context":"## Context\nTo determine the steady state conditions of the model, we require an ODE solver.\nWe switched to the adjoint ODE solver for faster gradient evaluations,\nhowever, recent use suggests that the solver is unstable and fails silently.\n","Decision":"Reverting to the ode_bdf_tol() solver implemented in Stan still solves\\nstiff ODE problems but slightly slower. This is overlooked as the current\\nadjoint solver is too unstable to use and fails on larger systems. The current\\nexample files in the `tests\/data\/` folder work appropriately, however, larger\\nsystems fail with the step size approaching 0.\\nAn attempt was made to make the flux calculations in the ode more stable by\\nusing built in functions and converting products to sums of logs etc.\\nThis did not help the adjoint solver and hence this revertion was made.\\nFuture versions of Stan can easily be tested by reverting to the previous\\nadjoint solver specification and the input files will still accept the tolerances\\nfor the solver.\\n","tokens":51,"id":239,"text":"## Context\\nTo determine the steady state conditions of the model, we require an ODE solver.\\nWe switched to the adjoint ODE solver for faster gradient evaluations,\\nhowever, recent use suggests that the solver is unstable and fails silently.\\n\n\n##Decision\nReverting to the ode_bdf_tol() solver implemented in Stan still solves\\nstiff ODE problems but slightly slower. This is overlooked as the current\\nadjoint solver is too unstable to use and fails on larger systems. The current\\nexample files in the `tests\/data\/` folder work appropriately, however, larger\\nsystems fail with the step size approaching 0.\\nAn attempt was made to make the flux calculations in the ode more stable by\\nusing built in functions and converting products to sums of logs etc.\\nThis did not help the adjoint solver and hence this revertion was made.\\nFuture versions of Stan can easily be tested by reverting to the previous\\nadjoint solver specification and the input files will still accept the tolerances\\nfor the solver.\\n","Predictions":"We will use the stack method to encode the response.n"}
{"File Name":"beis-report-official-development-assistance\/0002-use-bullet-to-catch-nplus1-queries.md","Context":"## Context\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\n","Decision":"Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","tokens":37,"id":2383,"text":"## Context\\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\\n\n\n##Decision\nAdd an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\\n","Predictions":"Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.n"}
{"File Name":"raster-foundry\/adr-0013-spark-execution-environment.md","Context":"## Context\nPart of the Raster Foundry processing workflow requires a source imagery ingest into a GeoTrellis Catalog. GeoTrellis depends on Apache Spark to parallelize work during an ingest, and that work typically occurs on an Apache Spark enabled compute cluster.\nPrior iterations of Raster Foundry attempted ingests with an Apache YARN managed compute cluster via Amazon Elastic MapReduce (EMR) on a per-request basis. Unfortunately, that didn't perform well due to the overhead of bootstrapping an EMR cluster.\nOur goal for the this iteration aims to keep some Spark cluster components active at all times through a combination of spot pricing and usage based cluster auto-scaling. This approach should help minimize cluster bootstrapping durations and keep cost-incurring cluster resources at a minimum.\n","Decision":"Given that we are still early in Raster Foundry's product development, the desire to keep costs low and ship quickly carry significant weight. In order to meet those objectives, but still minimize cluster bootstrapping overhead, the first pass at having Spark enabled resources on standby will consist of a shared (across staging and production) Amazon EMR cluster.\\nThe overall Raster Foundry infrastructure will be split across three Amazon Virtual Private Clouds (VPCs):\\n- Staging (`10.0.0.0\/18`, 16382 IPs)\\n- Spark (`10.0.64.0\/18`, 16382 IPs)\\n- Production (`10.0.128.0\/18`, 16382 IPs)\\nThe `Staging` and `Production` VPCs will be peered with the `Spark` VPC to allow direct private network access, if necessary. Regardless, public facing APIs such as the Amazon EMR's Step API, or the Raster Foundry API can be used to pass information between systems.\\nDespite having thought long and hard about the consequences, I suspect that the section below does not cover all of the issues that lie ahead of us. The hope is that at least these core ideas remain in future iterations of the Spark execution environment:\\n- Shared cluster resources between environments\\n- Isolated environments at the VPC level\\n- Build on top of the Amazon EMR platform\\n- Use spot pricing and auto-scaling aggressively to keep costs low\\n","tokens":162,"id":1681,"text":"## Context\\nPart of the Raster Foundry processing workflow requires a source imagery ingest into a GeoTrellis Catalog. GeoTrellis depends on Apache Spark to parallelize work during an ingest, and that work typically occurs on an Apache Spark enabled compute cluster.\\nPrior iterations of Raster Foundry attempted ingests with an Apache YARN managed compute cluster via Amazon Elastic MapReduce (EMR) on a per-request basis. Unfortunately, that didn't perform well due to the overhead of bootstrapping an EMR cluster.\\nOur goal for the this iteration aims to keep some Spark cluster components active at all times through a combination of spot pricing and usage based cluster auto-scaling. This approach should help minimize cluster bootstrapping durations and keep cost-incurring cluster resources at a minimum.\\n\n\n##Decision\nGiven that we are still early in Raster Foundry's product development, the desire to keep costs low and ship quickly carry significant weight. In order to meet those objectives, but still minimize cluster bootstrapping overhead, the first pass at having Spark enabled resources on standby will consist of a shared (across staging and production) Amazon EMR cluster.\\nThe overall Raster Foundry infrastructure will be split across three Amazon Virtual Private Clouds (VPCs):\\n- Staging (`10.0.0.0\/18`, 16382 IPs)\\n- Spark (`10.0.64.0\/18`, 16382 IPs)\\n- Production (`10.0.128.0\/18`, 16382 IPs)\\nThe `Staging` and `Production` VPCs will be peered with the `Spark` VPC to allow direct private network access, if necessary. Regardless, public facing APIs such as the Amazon EMR's Step API, or the Raster Foundry API can be used to pass information between systems.\\nDespite having thought long and hard about the consequences, I suspect that the section below does not cover all of the issues that lie ahead of us. The hope is that at least these core ideas remain in future iterations of the Spark execution environment:\\n- Shared cluster resources between environments\\n- Isolated environments at the VPC level\\n- Build on top of the Amazon EMR platform\\n- Use spot pricing and auto-scaling aggressively to keep costs low\\n","Predictions":"We will use a Spark cluster to provide a single, isolated, and centralized cluster for the ingest of source imagery.n"}
{"File Name":"gatemint-sdk\/adr-014-proportional-slashing.md","Context":"## Context\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\n","Decision":"### Design\\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\\n```\\nslash_amount = k * power \/\/ power is the faulting validator's voting power and k is some on-chain constant\\n```\\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts, so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\\n```\\nslash_amount = k * (power_1 + power_2 + ... + power_n) \/\/ where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\\n```\\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will still get slashed at the sum 10% amount.\\nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount.  In the case that the validators do fault together, they will get slashed the same amount as if they were one entity.  There is no con to splitting up.  However, if operators are going to split up their stake without actually decorrelating their setups, this also causes a negative externality to the network as it fills up validator slots that could have gone to others or increases the commit size.  In order to disincentivize this, we want it to be the case such that splitting up a validator into multiple validators and they fault together is punished more heavily that keeping it as a single validator that faults.\\nWe can achieve this by not only taking into account the sum of the percentages of the validators that faulted, but also the *number* of validators that faulted in the window.  One general form for an equation that fits this desired property looks like this:\\n```\\nslash_amount = k * ((power_1)^(1\/r) + (power_2)^(1\/r) + ... + (power_n)^(1\/r))^r \/\/ where k and r are both on-chain constants\\n```\\nSo now, for example, assuming k=1 and r=2, if one validator of 10% faults, it gets a 10% slash, while if two validators of 5% each fault together, they both get a 20% slash ((sqrt(0.05)+sqrt(0.05))^2).\\n#### Correlation across non-sybil validators\\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\\n#### Parameterization\\nThe value of k and r can be different for different types of slashable faults.  For example, we may want to punish liveness faults 10% as severely as double signs.\\nThere can also be minimum and maximums put in place in order to bound the size of the slash percent.\\n#### Griefing\\nGriefing, the act of intentionally being slashed to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker could not substantially grief without getting slashed a substantial amount themselves.  The larger the validator is, the more heavily it can impact the slash, it needs to be non-trivial to have a significant impact on the slash percent.  Furthermore, the larger the grief, the griefer loses quadratically more.\\nIt may also be possible to, rather than the k and r factors being constants, perhaps using an inverse gini coefficient may mitigate some griefing attacks, but this an area for future research.\\n### Implementation\\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define \"recent slashes\" as ones that have occured within the last `unbonding period`.  For liveness faults, we will define \"recent slashes\" as ones that have occured withing the last `jail period`.\\n```\\ntype SlashEvent struct {\\nAddress                     sdk.ValAddress\\nSqrtValidatorVotingPercent  sdk.Dec\\nSlashedSoFar                sdk.Dec\\n}\\n```\\nThese slash events will be pruned from the queue once they are older than their respective \"recent slash period\".\\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\\nWe then will iterate over all the SlashEvents in the queue, adding their `SqrtValidatorVotingPercent` and squaring the result to calculate the new percent to slash all the validators in the queue at, using the \"Square of Sum of Roots\" formula introduced above.\\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occured, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\\n","tokens":104,"id":28,"text":"## Context\\nIn Proof of Stake-based chains, centralization of consensus power amongst a small set of validators can cause harm to the network due to increased risk of censorship, liveness failure, fork attacks, etc.  However, while this centralization causes a negative externality to the network, it is not directly felt by the delegators contributing towards delegating towards already large validators.  We would like a way to pass on the negative externality cost of centralization onto those large validators and their delegators.\\n\n\n##Decision\n### Design\\nTo solve this problem, we will implement a procedure called Proportional Slashing.  The desire is that the larger a validator is, the more they should be slashed.  The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.\\n```\\nslash_amount = k * power \/\/ power is the faulting validator's voting power and k is some on-chain constant\\n```\\nHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts, so that if they fault, they all get slashed at a lower percent.  The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.\\n```\\nslash_amount = k * (power_1 + power_2 + ... + power_n) \/\/ where power_i is the voting power of the ith validator faulting in the specified time frame and k is some on-chain constant\\n```\\nNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will still get slashed at the sum 10% amount.\\nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount.  In the case that the validators do fault together, they will get slashed the same amount as if they were one entity.  There is no con to splitting up.  However, if operators are going to split up their stake without actually decorrelating their setups, this also causes a negative externality to the network as it fills up validator slots that could have gone to others or increases the commit size.  In order to disincentivize this, we want it to be the case such that splitting up a validator into multiple validators and they fault together is punished more heavily that keeping it as a single validator that faults.\\nWe can achieve this by not only taking into account the sum of the percentages of the validators that faulted, but also the *number* of validators that faulted in the window.  One general form for an equation that fits this desired property looks like this:\\n```\\nslash_amount = k * ((power_1)^(1\/r) + (power_2)^(1\/r) + ... + (power_n)^(1\/r))^r \/\/ where k and r are both on-chain constants\\n```\\nSo now, for example, assuming k=1 and r=2, if one validator of 10% faults, it gets a 10% slash, while if two validators of 5% each fault together, they both get a 20% slash ((sqrt(0.05)+sqrt(0.05))^2).\\n#### Correlation across non-sybil validators\\nOne will note, that this model doesn't differentiate between multiple validators run by the same operators vs validators run by different operators.  This can be seen as an additional benefit in fact.  It incentivizes validators to differentiate their setups from other validators, to avoid having correlated faults with them or else they risk a higher slash.  So for example, operators should avoid using the same popular cloud hosting platforms or using the same Staking as a Service providers.  This will lead to a more resilient and decentralized network.\\n#### Parameterization\\nThe value of k and r can be different for different types of slashable faults.  For example, we may want to punish liveness faults 10% as severely as double signs.\\nThere can also be minimum and maximums put in place in order to bound the size of the slash percent.\\n#### Griefing\\nGriefing, the act of intentionally being slashed to make another's slash worse, could be a concern here.  However, using the protocol described here, the attacker could not substantially grief without getting slashed a substantial amount themselves.  The larger the validator is, the more heavily it can impact the slash, it needs to be non-trivial to have a significant impact on the slash percent.  Furthermore, the larger the grief, the griefer loses quadratically more.\\nIt may also be possible to, rather than the k and r factors being constants, perhaps using an inverse gini coefficient may mitigate some griefing attacks, but this an area for future research.\\n### Implementation\\nIn the slashing module, we will add two queues that will track all of the recent slash events.  For double sign faults, we will define \"recent slashes\" as ones that have occured within the last `unbonding period`.  For liveness faults, we will define \"recent slashes\" as ones that have occured withing the last `jail period`.\\n```\\ntype SlashEvent struct {\\nAddress                     sdk.ValAddress\\nSqrtValidatorVotingPercent  sdk.Dec\\nSlashedSoFar                sdk.Dec\\n}\\n```\\nThese slash events will be pruned from the queue once they are older than their respective \"recent slash period\".\\nWhenever a new slash occurs, a `SlashEvent` struct is created with the faulting validator's voting percent and a `SlashedSoFar` of 0.  Because recent slash events are pruned before the unbonding period and unjail period expires, it should not be possible for the same validator to have multiple SlashEvents in the same Queue at the same time.\\nWe then will iterate over all the SlashEvents in the queue, adding their `SqrtValidatorVotingPercent` and squaring the result to calculate the new percent to slash all the validators in the queue at, using the \"Square of Sum of Roots\" formula introduced above.\\nOnce we have the `NewSlashPercent`, we then iterate over all the `SlashEvent`s in the queue once again, and if `NewSlashPercent > SlashedSoFar` for that SlashEvent, we call the `staking.Slash(slashEvent.Address, slashEvent.Power, Math.Min(Math.Max(minSlashPercent, NewSlashPercent - SlashedSoFar), maxSlashPercent)` (we pass in the power of the validator before any slashes occured, so that we slash the right amount of tokens).  We then set `SlashEvent.SlashedSoFar` amount to `NewSlashPercent`.\\n","Predictions":"We will implement a procedure called Proportional Slashing. The desire is that the larger a validator is, the more they should be slashed. The first naive attempt is to make a validator's slash percent proportional to their share of consensus voting power.nslash_amount = k * power \/\/ power is the faulting validator's voting power and k is some on-chain constantnnHowever, this will incentivize validators with large amounts of stake to split up their voting power amongst accounts, so that if they fault, they all get slashed at a lower percent. The solution to this is to take into account not just a validator's own voting percentage, but also the voting percentage of all the other validators who get slashed in a specified time frame.nnslash_amount = k * (power_1 + power_2 +... + power_n) \/\/ where power_i is the voting power of the faulting validator faulting in the specified time frame and k is some on-chain constantnnNow, if someone splits a validator of 10% into two validators of 5% each which both fault, then they both fault in the same time frame, they both will still get slashed at the sum 10% amount.nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount. In the case that the validators do fault together, they will get slashed the same amount as a 10% split. In the case that the validators do fault together, they will get slashed the same amount as a 10% split.nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount. In the case that the validators do fault together, they will get slashed the same amount as a 10% split.nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount. In the case that the validators do fault together, they will get slashed the same amount as a 10% split.nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount. In the case that the validators do fault together, they will get slashed the same amount as a 10% split.nHowever, an operator might still choose to split up their stake across multiple accounts with hopes that if any of them fault independently, they will not get slashed at the full amount. In the case that the validators do fault together, they will get slashed the same amount as a 10% split.n"}
{"File Name":"atlasdb\/0007-use-cql-for-column-paging-for-sweep.md","Context":"## Context\nAs of version 0.12.0, our implementation of sweep for Cassandra relied upon the getPageWithRangeCreator method of\nCassandraKeyValueService, which fetches values for all columns and timestamps from Cassandra. In cases where a cell with\na large value was overwritten (with other large values) many times, this caused Cassandra to run out of memory, even\nif only a single row was fetched. (internal issue 44272).\nWe needed a way to run sweep without Cassandra running out of memory in such cases.\n","Decision":"We are unaware of any way to fetch columns and timestamps without also temporarily loading values into memory (within\\nCassandra). Therefore, to avoid running out of memory, we needed to make it possible to fetch only a certain number of\\ncells (rather than rows) from Cassandra at once.\\nWe decided to introduce a more granular batching solution. In particular, we decided to page through the\\ncolumns for each row. Briefly, the algorithm implemented is as follows:\\n1. Fetch a number of rows equal to the row batch size (but only one column per row).\\n2. Use CQL queries with limits to collect the <column, timestamp> pairs.\\nFurther, since performing a number of additional CQL queries for every row will be less efficient, and unnecessary for\\nmost users, we decided to add an optional parameter, timestampsGetterBatchSize, in CassandraKVSConfig, and use the method\\ndescribed above only if this parameter is set.\\nWe chose CQL queries over thrift because CQL queries are simpler to write, and have been empirically shown to have\\nhigher throughput.\\n","tokens":112,"id":3090,"text":"## Context\\nAs of version 0.12.0, our implementation of sweep for Cassandra relied upon the getPageWithRangeCreator method of\\nCassandraKeyValueService, which fetches values for all columns and timestamps from Cassandra. In cases where a cell with\\na large value was overwritten (with other large values) many times, this caused Cassandra to run out of memory, even\\nif only a single row was fetched. (internal issue 44272).\\nWe needed a way to run sweep without Cassandra running out of memory in such cases.\\n\n\n##Decision\nWe are unaware of any way to fetch columns and timestamps without also temporarily loading values into memory (within\\nCassandra). Therefore, to avoid running out of memory, we needed to make it possible to fetch only a certain number of\\ncells (rather than rows) from Cassandra at once.\\nWe decided to introduce a more granular batching solution. In particular, we decided to page through the\\ncolumns for each row. Briefly, the algorithm implemented is as follows:\\n1. Fetch a number of rows equal to the row batch size (but only one column per row).\\n2. Use CQL queries with limits to collect the <column, timestamp> pairs.\\nFurther, since performing a number of additional CQL queries for every row will be less efficient, and unnecessary for\\nmost users, we decided to add an optional parameter, timestampsGetterBatchSize, in CassandraKVSConfig, and use the method\\ndescribed above only if this parameter is set.\\nWe chose CQL queries over thrift because CQL queries are simpler to write, and have been empirically shown to have\\nhigher throughput.\\n","Predictions":"We are unaware of any way to fetch columns and timestamps without also temporarily loading values into memory (withinnCassandra). Therefore, to avoid running out of memory, we needed to make it possible to fetch only a certain number of cells (rather than rows) from Cassandra at once.nWe decided to introduce a more granular batching solution. In particular, we decided to page through thencolumns for each row. Briefly, the algorithm implemented is as follows:n1. Fetch a number of rows equal to the row batch size (but only one column per row).n2. Use CQL queries with limits to collect the column, timestamp> pairs.nFurther, since performing a number of additional CQL queries for every row will be less efficient, and unnecessary fornmost users, we decided to add an optional parameter, timestampsGetterBatchSize, in CassandraKVSConfig, and use the methodndescribed above only if this parameter is set.nWe chose CQL queries over thrift because CQL queries are simpler to write, and have been empirically shown to havenhigher throughput.n"}
{"File Name":"opg-data\/0009-api-domain-structure.md","Context":"## Context\nWe need to establish a domain structure which\n* Is product agnostic\n* Is consistent across the opg-data service\n","Decision":"We will adopt the pattern:\\n[pull-request-id].[account-stage].[microservice-domain].api.opg.service.justive.gov.uk\\nWhere [pull-request-id] is for ephemeral dev environments.\\nWhere [account-stage] is the stage in our path to live\/accounts i.e. dev,pre and empty for prod services.\\nWhere [microservice-domain] is the scoped domain of the microservice\/integration in in question.\\n* hey\\n### examples\\n#### root:\\n* https:\/\/api.opg.service.justice.gov.uk\\n#### integration:\\n* https:\/\/deputy-reporting.api.opg.service.justice.gov.uk\\n#### environments per integration:\\n* https:\/\/pre.deputy-reporting.api.opg.service.justice.gov.uk\\n* https:\/\/dev.deputy-reporting.api.opg.service.justice.gov.uk\\n#### pr raised on an environment per integration:\\n* https:\/\/pr-1234.dev.deputy-reporting.api.opg.service.justice.gov.uk\\n","tokens":28,"id":2183,"text":"## Context\\nWe need to establish a domain structure which\\n* Is product agnostic\\n* Is consistent across the opg-data service\\n\n\n##Decision\nWe will adopt the pattern:\\n[pull-request-id].[account-stage].[microservice-domain].api.opg.service.justive.gov.uk\\nWhere [pull-request-id] is for ephemeral dev environments.\\nWhere [account-stage] is the stage in our path to live\/accounts i.e. dev,pre and empty for prod services.\\nWhere [microservice-domain] is the scoped domain of the microservice\/integration in in question.\\n* hey\\n### examples\\n#### root:\\n* https:\/\/api.opg.service.justice.gov.uk\\n#### integration:\\n* https:\/\/deputy-reporting.api.opg.service.justice.gov.uk\\n#### environments per integration:\\n* https:\/\/pre.deputy-reporting.api.opg.service.justice.gov.uk\\n* https:\/\/dev.deputy-reporting.api.opg.service.justice.gov.uk\\n#### pr raised on an environment per integration:\\n* https:\/\/pr-1234.dev.deputy-reporting.api.opg.service.justice.gov.uk\\n","Predictions":"We will use Python as our base programming language.n"}
{"File Name":"cfo\/0002-use-yarn-install-instead-of-npm-install-for-web.md","Context":"## Context\nI see found both `yarn.lock` and `package-lock.json` in the repository.\n","Decision":"Sticking to `yarn` for `web` for now.\\n","tokens":23,"id":1320,"text":"## Context\\nI see found both `yarn.lock` and `package-lock.json` in the repository.\\n\n\n##Decision\nSticking to `yarn` for `web` for now.\\n","Predictions":"We will use proposer for all roles in the protocol.n"}
{"File Name":"karma-sabarivka-reporter\/0001-documenting-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4002,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"Data-Platform-Playbook\/010-using-pytest-for-verifying-pyspark-transformations.md","Context":"## Context\nThe Data Platform team has been writing Apache Spark jobs using PySpark to transform data within the platform.\nExamples include:\n- Address matching\n- Address cleaning\n- Repairs sheets data cleaning\nThese jobs lack automated tests, which has meant that debugging these scripts has involved slow feedback loops, running against actual data within the platform.\nBy introducing testing practices, frameworks and tools we hope to:\n- Improve the speed at which PySpark scripts can be developed\n- Provide documentation for each script with example data they expect, and what results they output\n- Increase the proportion of defects found before they reach staging environment\n","Decision":"We will:\\n- Use a Python testing framework, [pytest][pytest]\\n- Use the same [Docker container][awsglue-dockerhub] we use for the Jypiter Notebook for running the tests, as it replicates the AWS Glue Spark environment locally.\\n- Integrate that framework into Apache Spark, and provide example test code\\n- Create documentation and guidance around how to productively test PySpark scripts\\n- Run the suite of Python tests as part of the deployment pipeline, and prevent failing tests from being deployed to staging\\n[pytest]: https:\/\/docs.pytest.org\\n[awsglue-dockerhub]: https:\/\/hub.docker.com\/r\/amazon\/aws-glue-libs\\n","tokens":128,"id":1012,"text":"## Context\\nThe Data Platform team has been writing Apache Spark jobs using PySpark to transform data within the platform.\\nExamples include:\\n- Address matching\\n- Address cleaning\\n- Repairs sheets data cleaning\\nThese jobs lack automated tests, which has meant that debugging these scripts has involved slow feedback loops, running against actual data within the platform.\\nBy introducing testing practices, frameworks and tools we hope to:\\n- Improve the speed at which PySpark scripts can be developed\\n- Provide documentation for each script with example data they expect, and what results they output\\n- Increase the proportion of defects found before they reach staging environment\\n\n\n##Decision\nWe will:\\n- Use a Python testing framework, [pytest][pytest]\\n- Use the same [Docker container][awsglue-dockerhub] we use for the Jypiter Notebook for running the tests, as it replicates the AWS Glue Spark environment locally.\\n- Integrate that framework into Apache Spark, and provide example test code\\n- Create documentation and guidance around how to productively test PySpark scripts\\n- Run the suite of Python tests as part of the deployment pipeline, and prevent failing tests from being deployed to staging\\n[pytest]: https:\/\/docs.pytest.org\\n[awsglue-dockerhub]: https:\/\/hub.docker.com\/r\/amazon\/aws-glue-libs\\n","Predictions":"We will:n- Use a Python testing framework, [pytest][pytest]n- Use the same [Docker container][awsglue-dockerhub] we use for the Jypiter Notebook for running the tests, as it replicates the AWS Glue Spark environment locally.n- Integrate that framework into Apache Spark, and provide example test coden- Run the suite of Python tests as part of the deployment pipeline, and prevent failing tests from being deployed to staging environmentn[pytest]: https:\/\/docs.pytest.orgn[awsglue-dockerhub]: https:\/\/hub.docker.com\/r\/amazon\/aws-glue-libsn"}
{"File Name":"pottery\/0003-pottery-shards-are-markdown-text.md","Context":"## Context\nEach event recorded by Pottery (aka \"shard\") is a short piece of text.  Maybe with links, or basic formatting.\nPottery is used on the command line, so shards need to be generated by Unix command line tools, readable on the terminal and edited in a text editor.\n","Decision":"The content of each shard will be in Markdown format.\\n","tokens":65,"id":2510,"text":"## Context\\nEach event recorded by Pottery (aka \"shard\") is a short piece of text.  Maybe with links, or basic formatting.\\nPottery is used on the command line, so shards need to be generated by Unix command line tools, readable on the terminal and edited in a text editor.\\n\n\n##Decision\nThe content of each shard will be in Markdown format.\\n","Predictions":"The shard will be a shard that will be generated by the shard command.n"}
{"File Name":"axiomatic\/0001-enabling-configuration-repo-access.md","Context":"## Context and Problem Statement\nHow should Axiomatic ensure that dir2consul, when run as a Nomad batch job, has access to configuration repos hosted on GitHub?\n## Decision Drivers\n* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\n* There is no appetite for spending money on Service User accounts.\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\n* Option 2 setup is an infrequent activity as would be key rotation\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\n* Option 4 was eliminated due to possible violation of our acceptable use policies\n","Decision":"* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\nChosen option: Option 2, Axiomatic Retrieves Deployment Keys\\n### Positive Consequences\\n* Allows us to provide a repeatable solution for configurationn repos in the short term\\n* We will default to using read-only ssh access\\n### Negative Consequences\\n* Requires codifying a key management process and a potential automation effort\\n","tokens":161,"id":3595,"text":"## Context and Problem Statement\\nHow should Axiomatic ensure that dir2consul, when run as a Nomad batch job, has access to configuration repos hosted on GitHub?\\n## Decision Drivers\\n* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\n\n\n##Decision\n* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.\\n* There is no appetite for spending money on Service User accounts.\\n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)\\n* Option 2 setup is an infrequent activity as would be key rotation\\n* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomad\\n* Option 4 was eliminated due to possible violation of our acceptable use policies\\nChosen option: Option 2, Axiomatic Retrieves Deployment Keys\\n### Positive Consequences\\n* Allows us to provide a repeatable solution for configurationn repos in the short term\\n* We will default to using read-only ssh access\\n### Negative Consequences\\n* Requires codifying a key management process and a potential automation effort\\n","Predictions":"* GitHub prevents deployment key reuse. The SSH key can only be attached to one repo.n* There is no appetite for spending money on Service User accounts.n* Option 2 needs a process for creating the ssh key pair, adding the public key to the repo, and storing the private key in Vault (potentially automated via Terraform)n* Option 2 setup is an infrequent activity as would be key rotationn* Option 3 fundamentally cannot be done due to bugs\/non-support in Nomadn* Option 4 was eliminated due to possible violation of our acceptable use policiesnChosen option: Option 2, Axiomatic Retrieves Deployment Keysn### Positive Consequencesn* Allows us to provide a repeatable solution for configurationn repos in the short termn* We will default to using read-only ssh accessn### Negative Consequencesn* Requires codifying a key management process and a potential automation effortn"}
{"File Name":"arch\/0036-ha-for-mysql.md","Context":"## Context\n1. \u6570\u636e\u5e93\u7248\u672c 5.1\uff0c\u592a\u65e7\uff0c\u6027\u80fd\uff0c\u5b89\u5168\uff0c\u4e3b\u4ece\u590d\u5236\u90fd\u5b58\u5728\u95ee\u9898\uff1b\n2. \u6570\u636e\u5e93\u90e8\u7f72\u5728 ECS \u4e0a\uff0c\u4f46\u78c1\u76d8\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0cIOPS \u5df2\u5230\u9608\u503c\uff08\u4f18\u5148\u7ea7\u6700\u9ad8\uff09\uff1b\n3. \u6570\u636e\u5e93\u4e00\u4e3b\u4e24\u4ece\uff0c\u4f46\u65e0\u9ad8\u53ef\u7528\uff1b\n4. \u4e1a\u52a1\u7aef\u4f7f\u7528 IP \u8fde\u63a5\u4e3b\u6570\u636e\u5e93\u3002\n","Decision":"1. \u63d0\u4ea4 Aliyun \u5de5\u5355\uff0c\u5c1d\u8bd5\u662f\u5426\u80fd\u7533\u8bf7\u4e0b 5.1 \u7248\u672c\u7684 MySQL\uff0c\u8fc1\u79fb\u6570\u636e\u81f3 RDS\uff0c\u89e3\u51b3 2\uff0c3\uff0c4 \u95ee\u9898\uff08\u6c9f\u901a\u540e\uff0c5.1 \u7248\u672c\u5df2\u4e0d\u518d\u63d0\u4f9b\uff0cPASS\uff09\uff1b\\n2. \u5c06\u90e8\u5206\u6570\u636e\u5e93\u8fc1\u79fb\u51fa\uff0c\u7f13\u89e3\u5f53\u524d MySQL \u670d\u52a1\u5668\u538b\u529b\uff0c\u7ef4\u62a4\u591a\u4e2a\u6570\u636e\u5e93\u5b9e\u4f8b\uff08\u5e76\u672a\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\uff0cPASS\uff0c\u5f53\u524d\u538b\u529b\u6700\u7ec8\u786e\u8ba4\u662f\u6162\u67e5\u8be2\u539f\u56e0\uff09\uff1b\\n3. ECS \u4e0a\u81ea\u5efa HA\uff0c\u5e76\u542f\u7528\u65b0\u7684\u5b9e\u4f8b\u78c1\u76d8\u4e3a SSD\uff0c\u5207\u6362\u65b0\u5b9e\u4f8b\u4e3a Master\uff0c\u505c\u6389\u65e7\u5b9e\u4f8b\uff08\u6839\u672c\u95ee\u9898\u672a\u89e3\u51b3\uff0c\u6280\u672f\u503a\u4e00\u76f4\u5b58\u5728\uff0c\u81ea\u884c\u7ef4\u62a4\u4ecd\u7136\u5b58\u5728\u98ce\u9669\u70b9\uff09\uff1b\\n4. \u8c03\u7814 5.5 \u548c 5.1 \u7684\u5dee\u5f02\uff0c\u76f4\u63a5\u8fc1\u79fb\u81ea\u5efa\u6570\u636e\u5e93\u81f3 Aliyun RDS MySQL 5.5\u3002\\n\u9274\u4e8e\u67e5\u770b\u6587\u6863\u540e\uff0c 5.1 \u5230 5.5 \u7684\u5dee\u5f02\u6027\u5f71\u54cd\u4e0d\u5927\uff0cAliyun \u5b98\u65b9\u4e5f\u652f\u6301\u76f4\u63a5 5.1 \u5230 5.5 \u7684\u8fc1\u79fb\uff0c\u6240\u4ee5\u8ba1\u5212\u76f4\u63a5\u8fc1\u79fb\u81f3 RDS \u7684 5.5 \u7248\u672c\u3002\\n\u4e3a\u4e86\u675c\u7edd\u98ce\u9669\uff1a\\n1. \u6309\u4e1a\u52a1\u5206\u6570\u636e\u5e93\u5206\u522b\u8fc1\u79fb\uff1b\\n2. \u6240\u6709\u8fc1\u79fb\u5148\u8d70\u6d4b\u8bd5\u6570\u636e\u5e93\uff0c\u7531 QA \u505a\u5b8c\u6574\u7684\u6d4b\u8bd5\u3002\\nECS self built MySQL 5.1 to RDS 5.5 with DTS \u8fc1\u79fb\u6d41\u7a0b\uff1a\\n1. \u5728 RDS \u4e2d\u521b\u5efa\u539f MySQL \u6570\u636e\u5e93\u5bf9\u5e94\u7684\u8d26\u53f7(\u5404\u4e2a\u9879\u76ee\u8d26\u53f7\u72ec\u7acb)\uff1b\\n2. \u66f4\u65b0\u767d\u540d\u5355\uff1a\u6dfb\u52a0\u9879\u76ee\u6240\u90e8\u7f72\u7684\u670d\u52a1\u5668\uff1b\\n3. \u660e\u786e\u6570\u636e\u89c4\u6a21\uff0c\u5bf9\u540c\u6b65\u65f6\u95f4\u505a\u4e2a\u9884\u671f\uff1b\\n4. \u540c\u6b65\uff08\u5168\u91cf or \u589e\u91cf\uff09\uff0c\u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff1b\\n5. \u66f4\u65b0\u6570\u636e\u5e93\u8fde\u63a5\u914d\u7f6e\u6587\u4ef6\uff1b\\n6. \u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff0c\u505c\u670d\uff1b\\n7. \u786e\u5b9a\u6570\u636e\u91cf\u4e00\u81f4\uff08\u7531\u9884\u5148\u5199\u597d\u7684\u811a\u672c\u5224\u65ad\uff09(1min)\uff1b\\n8. \u5173\u95ed\u8fc1\u79fb\u670d\u52a1(10s)\uff1b\\n9. \u91cd\u542f\u670d\u52a1\u5668\uff0810s\uff09\u3002\\n6 \u81f3 9 \u6b65\u51b3\u5b9a\u6211\u4eec\u7684\u505c\u670d\u65f6\u95f4\u3002\\n\u9274\u4e8e\u6211\u4eec\u4f7f\u7528\u4ece\u5e93\u4f5c\u4e3a\u8fc1\u79fb\u7684\u6570\u636e\u6e90\uff0c\u9700\u66f4\u65b0\u5982\u4e0b\u914d\u7f6e\uff1a\\n* log-slave-updates=1\\n* binlog-format=row\\n","tokens":112,"id":2459,"text":"## Context\\n1. \u6570\u636e\u5e93\u7248\u672c 5.1\uff0c\u592a\u65e7\uff0c\u6027\u80fd\uff0c\u5b89\u5168\uff0c\u4e3b\u4ece\u590d\u5236\u90fd\u5b58\u5728\u95ee\u9898\uff1b\\n2. \u6570\u636e\u5e93\u90e8\u7f72\u5728 ECS \u4e0a\uff0c\u4f46\u78c1\u76d8\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0cIOPS \u5df2\u5230\u9608\u503c\uff08\u4f18\u5148\u7ea7\u6700\u9ad8\uff09\uff1b\\n3. \u6570\u636e\u5e93\u4e00\u4e3b\u4e24\u4ece\uff0c\u4f46\u65e0\u9ad8\u53ef\u7528\uff1b\\n4. \u4e1a\u52a1\u7aef\u4f7f\u7528 IP \u8fde\u63a5\u4e3b\u6570\u636e\u5e93\u3002\\n\n\n##Decision\n1. \u63d0\u4ea4 Aliyun \u5de5\u5355\uff0c\u5c1d\u8bd5\u662f\u5426\u80fd\u7533\u8bf7\u4e0b 5.1 \u7248\u672c\u7684 MySQL\uff0c\u8fc1\u79fb\u6570\u636e\u81f3 RDS\uff0c\u89e3\u51b3 2\uff0c3\uff0c4 \u95ee\u9898\uff08\u6c9f\u901a\u540e\uff0c5.1 \u7248\u672c\u5df2\u4e0d\u518d\u63d0\u4f9b\uff0cPASS\uff09\uff1b\\n2. \u5c06\u90e8\u5206\u6570\u636e\u5e93\u8fc1\u79fb\u51fa\uff0c\u7f13\u89e3\u5f53\u524d MySQL \u670d\u52a1\u5668\u538b\u529b\uff0c\u7ef4\u62a4\u591a\u4e2a\u6570\u636e\u5e93\u5b9e\u4f8b\uff08\u5e76\u672a\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\uff0cPASS\uff0c\u5f53\u524d\u538b\u529b\u6700\u7ec8\u786e\u8ba4\u662f\u6162\u67e5\u8be2\u539f\u56e0\uff09\uff1b\\n3. ECS \u4e0a\u81ea\u5efa HA\uff0c\u5e76\u542f\u7528\u65b0\u7684\u5b9e\u4f8b\u78c1\u76d8\u4e3a SSD\uff0c\u5207\u6362\u65b0\u5b9e\u4f8b\u4e3a Master\uff0c\u505c\u6389\u65e7\u5b9e\u4f8b\uff08\u6839\u672c\u95ee\u9898\u672a\u89e3\u51b3\uff0c\u6280\u672f\u503a\u4e00\u76f4\u5b58\u5728\uff0c\u81ea\u884c\u7ef4\u62a4\u4ecd\u7136\u5b58\u5728\u98ce\u9669\u70b9\uff09\uff1b\\n4. \u8c03\u7814 5.5 \u548c 5.1 \u7684\u5dee\u5f02\uff0c\u76f4\u63a5\u8fc1\u79fb\u81ea\u5efa\u6570\u636e\u5e93\u81f3 Aliyun RDS MySQL 5.5\u3002\\n\u9274\u4e8e\u67e5\u770b\u6587\u6863\u540e\uff0c 5.1 \u5230 5.5 \u7684\u5dee\u5f02\u6027\u5f71\u54cd\u4e0d\u5927\uff0cAliyun \u5b98\u65b9\u4e5f\u652f\u6301\u76f4\u63a5 5.1 \u5230 5.5 \u7684\u8fc1\u79fb\uff0c\u6240\u4ee5\u8ba1\u5212\u76f4\u63a5\u8fc1\u79fb\u81f3 RDS \u7684 5.5 \u7248\u672c\u3002\\n\u4e3a\u4e86\u675c\u7edd\u98ce\u9669\uff1a\\n1. \u6309\u4e1a\u52a1\u5206\u6570\u636e\u5e93\u5206\u522b\u8fc1\u79fb\uff1b\\n2. \u6240\u6709\u8fc1\u79fb\u5148\u8d70\u6d4b\u8bd5\u6570\u636e\u5e93\uff0c\u7531 QA \u505a\u5b8c\u6574\u7684\u6d4b\u8bd5\u3002\\nECS self built MySQL 5.1 to RDS 5.5 with DTS \u8fc1\u79fb\u6d41\u7a0b\uff1a\\n1. \u5728 RDS \u4e2d\u521b\u5efa\u539f MySQL \u6570\u636e\u5e93\u5bf9\u5e94\u7684\u8d26\u53f7(\u5404\u4e2a\u9879\u76ee\u8d26\u53f7\u72ec\u7acb)\uff1b\\n2. \u66f4\u65b0\u767d\u540d\u5355\uff1a\u6dfb\u52a0\u9879\u76ee\u6240\u90e8\u7f72\u7684\u670d\u52a1\u5668\uff1b\\n3. \u660e\u786e\u6570\u636e\u89c4\u6a21\uff0c\u5bf9\u540c\u6b65\u65f6\u95f4\u505a\u4e2a\u9884\u671f\uff1b\\n4. \u540c\u6b65\uff08\u5168\u91cf or \u589e\u91cf\uff09\uff0c\u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff1b\\n5. \u66f4\u65b0\u6570\u636e\u5e93\u8fde\u63a5\u914d\u7f6e\u6587\u4ef6\uff1b\\n6. \u660e\u786e\u65e0\u5ef6\u8fdf\u72b6\u6001\uff0c\u505c\u670d\uff1b\\n7. \u786e\u5b9a\u6570\u636e\u91cf\u4e00\u81f4\uff08\u7531\u9884\u5148\u5199\u597d\u7684\u811a\u672c\u5224\u65ad\uff09(1min)\uff1b\\n8. \u5173\u95ed\u8fc1\u79fb\u670d\u52a1(10s)\uff1b\\n9. \u91cd\u542f\u670d\u52a1\u5668\uff0810s\uff09\u3002\\n6 \u81f3 9 \u6b65\u51b3\u5b9a\u6211\u4eec\u7684\u505c\u670d\u65f6\u95f4\u3002\\n\u9274\u4e8e\u6211\u4eec\u4f7f\u7528\u4ece\u5e93\u4f5c\u4e3a\u8fc1\u79fb\u7684\u6570\u636e\u6e90\uff0c\u9700\u66f4\u65b0\u5982\u4e0b\u914d\u7f6e\uff1a\\n* log-slave-updates=1\\n* binlog-format=row\\n","Predictions":",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}
{"File Name":"dotcom-rendering\/004-emotion.md","Context":"## Context\nUsing a CSS-in-JS library will allow us to scope styles to a component. Popular CSS-in-JS libraries include Styled Components, Emotion and Styletron.\nStyletron generates atomic CSS, guaranteeing the smallest possible CSS output. However, it does not support language features that cannot be atomised, such as nesting and sibling selectors.\nStyled Components and Emotion support more complex features and have a similar API (composing styled components using the `styled()` method). Emotion additionally supports adding styles to the `css` prop on React components. It is also smaller (5.66KB Gzipped vs 16.27KB for styled-components).\n","Decision":"We will use Emotion as our CSS-in-JS library.\\n","tokens":138,"id":2655,"text":"## Context\\nUsing a CSS-in-JS library will allow us to scope styles to a component. Popular CSS-in-JS libraries include Styled Components, Emotion and Styletron.\\nStyletron generates atomic CSS, guaranteeing the smallest possible CSS output. However, it does not support language features that cannot be atomised, such as nesting and sibling selectors.\\nStyled Components and Emotion support more complex features and have a similar API (composing styled components using the `styled()` method). Emotion additionally supports adding styles to the `css` prop on React components. It is also smaller (5.66KB Gzipped vs 16.27KB for styled-components).\\n\n\n##Decision\nWe will use Emotion as our CSS-in-JS library.\\n","Predictions":"We will use TypeScriptn"}
{"File Name":"monocle\/0007-production-ready-in-all-in-one-setup.md","Context":"## Context and Problem Statement\nHow do we serve the API using a production ready server?\n## Decision Drivers\n* use production ready server.\n* good integration with reverse proxy servers like Nginx and Apache.\n* support Flask in a simple and performant way\n","Decision":"* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\nChosen option: \"[option 1]\", because it is well known in the\\nindustry. It is not the most performant but it has good performances\\nto serve WSGI app like Flask thanks to the uwsgi protocol used between\\nNginx\/Apache.\\n### Negative Consequences\\n* complexify the architecture.\\n","tokens":56,"id":4642,"text":"## Context and Problem Statement\\nHow do we serve the API using a production ready server?\\n## Decision Drivers\\n* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\n\n\n##Decision\n* use production ready server.\\n* good integration with reverse proxy servers like Nginx and Apache.\\n* support Flask in a simple and performant way\\nChosen option: \"[option 1]\", because it is well known in the\\nindustry. It is not the most performant but it has good performances\\nto serve WSGI app like Flask thanks to the uwsgi protocol used between\\nNginx\/Apache.\\n### Negative Consequences\\n* complexify the architecture.\\n","Predictions":"* use production ready server.n* good integration with reverse proxy servers like Nginx and Apache.n* support Flask in a simple and performant waynChosen option: \"[option 1]\", because it is well known in thenindustry. It is not the most performant but it has goodnperformancento serve WSGI app like Flask thanks to the uwsgi protocol used betweennNginx\/Apache.n### Negative Consequencesn* complexify the architecture.n"}
{"File Name":"terraform-aws-dynamodb\/0004-terraform-requirements-in-module.md","Context":"## Context\nTerraform enables you to constrain the version of Terraform able to run a\nmodules, as well as the minimium allowed versions of providers. Many of LIC\nexisting modules do not leverage these configurations.\n","Decision":"The Terraform version able to run this module will be set in `main.tf`.\\nThe miniumum allowed versions of providers will be set in `main.tf`\\n","tokens":46,"id":2536,"text":"## Context\\nTerraform enables you to constrain the version of Terraform able to run a\\nmodules, as well as the minimium allowed versions of providers. Many of LIC\\nexisting modules do not leverage these configurations.\\n\n\n##Decision\nThe Terraform version able to run this module will be set in `main.tf`.\\nThe miniumum allowed versions of providers will be set in `main.tf`\\n","Predictions":"We will use the [Security Gateway](https:\/\/github.com\/securitygate\/security-gate) as the default gateway for the application.n"}
{"File Name":"DunkMe\/0003-relational-database.md","Context":"#### Context and Problem Statement\nA relational database is required to persist the applications data.\n#### Considered Options\n- Microsoft SQL\n- MySQL\n- PostgreSQL\n- SQLite\n#### Decision Outcome\nChosen option: \"Microsoft SQL\", because\n- This is in-line with Trade Me's technical stack.\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\n","Decision":"Chosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n","tokens":78,"id":1661,"text":"#### Context and Problem Statement\\nA relational database is required to persist the applications data.\\n#### Considered Options\\n- Microsoft SQL\\n- MySQL\\n- PostgreSQL\\n- SQLite\\n#### Decision Outcome\\nChosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n\n\n##Decision\nChosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n","Predictions":"OutcomenChosen option: \"Api\", becausen- This is a front end, client facing technology stack.n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.n- A repository pattern can be applied facilitating Unit Testing.n([back](README.md))n"}
{"File Name":"Sylius\/2020_05_13_handling_non_crud_operations_in_api.md","Context":"## Context and Problem Statement\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`\/`PUT` HTTP methods),\nRead(`GET` HTTP method), Update(`PUT`\/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\nworkflow in a state machine) or command execution.\n## Decision Drivers\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\ntwo different state machines.\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\n","Decision":"* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\nThe \"Taking advantage of the `Controller` REST archetype\" should be considered as a recommended solution. All Sylius\\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\\nor new schema should be introduced for the JSON-LD structure.\\nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing is\\ndelivered with vanilla Sylius installation.\\n","tokens":225,"id":674,"text":"## Context and Problem Statement\\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`\/`PUT` HTTP methods),\\nRead(`GET` HTTP method), Update(`PUT`\/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\\nworkflow in a state machine) or command execution.\\n## Decision Drivers\\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\n\n\n##Decision\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\nThe \"Taking advantage of the `Controller` REST archetype\" should be considered as a recommended solution. All Sylius\\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\\nor new schema should be introduced for the JSON-LD structure.\\nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing is\\ndelivered with vanilla Sylius installation.\\n","Predictions":"* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to define the state of the application.n* The State API should be used to"}
{"File Name":"island.is\/0001-use-nx.md","Context":"## Context and Problem Statement\nWe want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI\/CD.\n## Decision Drivers\n- Low complexity and overhead in development.\n- Fit for our stack.\n- Optimize CI\/CD with dependency graphs and\/or caching.\n- Flexible.\n","Decision":"- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\nChosen option: \"Nx\", because:\\n- It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).\\n- It's relatively easy to learn with focused documentation.\\n- It has schematics to generate apps, libraries and components that includes all of our tools.\\n- It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.\\n","tokens":87,"id":1117,"text":"## Context and Problem Statement\\nWe want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI\/CD.\\n## Decision Drivers\\n- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\n\n\n##Decision\n- Low complexity and overhead in development.\\n- Fit for our stack.\\n- Optimize CI\/CD with dependency graphs and\/or caching.\\n- Flexible.\\nChosen option: \"Nx\", because:\\n- It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).\\n- It's relatively easy to learn with focused documentation.\\n- It has schematics to generate apps, libraries and components that includes all of our tools.\\n- It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.\\n","Predictions":"- Low complexity and overhead in development.n- Fit for our stack.n- Optimize CI\/CD with dependency graphs and\/or caching.n- Flexible.nChosen option: \"Nx\", because:n- It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).n- It's relatively easy to learn with focused documentation.n- It has schematics to generate apps, libraries and components that includes all of our tools.n- It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.n"}
{"File Name":"pharmacy-data-etl\/0004-make-etl-re-entrant.md","Context":"## Context\nIf the ETL if is interrupted it will need to start over again, i.e. a 6 hour ETL is stopped in the 5th hour, restarting will take another 6 hours to finish.\n","Decision":"The ETL will be re-entrant, storing state on a regular basis.\\nRestarting the ETL will pick up from the last point.\\n","tokens":47,"id":965,"text":"## Context\\nIf the ETL if is interrupted it will need to start over again, i.e. a 6 hour ETL is stopped in the 5th hour, restarting will take another 6 hours to finish.\\n\n\n##Decision\nThe ETL will be re-entrant, storing state on a regular basis.\\nRestarting the ETL will pick up from the last point.\\n","Predictions":"We will continue to use the Heroku scheduler on a ten-minute interval, but will instead use the Heroku CLI to run the management command (.\/manage.py build_static_site) in a separate, detached worker dyno. This worker dyno has a lifespan of 24 hours and is not limited to the interval of the scheduled job.nThe command we will use is heroku run:detached -a app_name>.\/manage.py build_static_site.nProsn------n* Very easy to implement.n* Will allow static site builds that take up to 24 hours (which is greater than any reasonable build length).nConsn------n* We remain unable to responsively schedule static site builds at the moment they are needed and rely on the scheduler to kick off a build.n* We have to expose a Heroku API key and install the build.n"}
{"File Name":"ftd-scratch3-offline\/0017-don-t-support-scratch-images-sprites-and-image-sprite-related-blocks.md","Context":"## Context\nScratch supports images\/sprites.\nThe ftduino has no way to show images\/sprites.\n","Decision":"Scratch images\/sprites and image\/sprite related blocks are not supported.\\n","tokens":23,"id":2637,"text":"## Context\\nScratch supports images\/sprites.\\nThe ftduino has no way to show images\/sprites.\\n\n\n##Decision\nScratch images\/sprites and image\/sprite related blocks are not supported.\\n","Predictions":"The adr_help_ command will be used to generate help text.n"}
{"File Name":"frontend-library\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":784,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"unfinished-design-system\/001-components-documentation.md","Context":"* [Context](#context)\n* [Decision](#decision)\n* [Status](#status)\n* [Consequences](#consequences)\n* [More reading](#more-reading)\n* [Updates](#updates)\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Storybook](https:\/\/storybook.js.org\/). This tool helps us document our components and develop them since we can make a simple canvas to interact with it.\\nSome major companies with mature design systems use this tool, which could be a precious asset.\\n","tokens":53,"id":4544,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Storybook](https:\/\/storybook.js.org\/). This tool helps us document our components and develop them since we can make a simple canvas to interact with it.\\nSome major companies with mature design systems use this tool, which could be a precious asset.\\n","Predictions":"* [Status](#status)n* [Consequences](#consequences)n* [More reading](#more-reading)n* [Updates](#updates)nWe are going to use Behavior-Driven Development to help us reduce that gap. BDD is a technique where we use User Stories to create acceptance tests based on user behaviors.nInstead of writing complex test statements, we use a single-choice approach to test the application.n"}
{"File Name":"pace-developers\/0012-matlab-python-wrapper.md","Context":"## Context\nBoth Euphonic and Brille are PACE projects with primarily a Python user interface (UI).\nPACE, however, aims to provide both a Python and a Matlab UI to users,\nand also to foster inter-operability between projects which are written both in Matlab and Python.\nIn particular, `pyHorace` ([prototype](https:\/\/github.com\/mducle\/hugo)) cannot use the\n[standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for Matlab to run Python code,\nwhere calls to Python from Matlab are prefixed with `py.` followed by the full module specification.\nFor example, `r = py.numpy.random.rand()` uses `numpy` to generate a random number.\nThis is because such a call causes Matlab to\n[automatically spawn](https:\/\/uk.mathworks.com\/help\/matlab\/ref\/pyenv.html) a dependent Python interpreter,\nwhich can be either created within the same process as the Matlab interpreter (`InProcess`)\nor in an external process (`OutOfProcess`).\n`pyHorace` already runs within a Python interpreter and the compiled Matlab library *must* be loaded in-process.\nThus, if Matlab spawns a second Python intepreter with the default `InProcess` execution mode,\nthe two Python interpreters will conflict causing memory errors and a crash.\nWe can force Matlab to launch the dependent Python interpreter `OutOfProcess`\nbut this imposes a significant performance penalty\n(extensive testing was not done but Brille+SpinW runs about 10x slower than with `InProcess`).\n","Decision":"At a meeting on Jan 7 2021, the developers of `pyHorace`, `brillem` and `horace-euphonic-interface` agreed to accept this proposal.\\n`brillem` and `horace-euphonic-interface` will be refactored to use the `light_python_wrapper` proposed here.\\nThe meeting also agreed implementation details which will be described in [ADR #13](0013-light-python-wrapper-implementation-detail.md).\\n","tokens":333,"id":5171,"text":"## Context\\nBoth Euphonic and Brille are PACE projects with primarily a Python user interface (UI).\\nPACE, however, aims to provide both a Python and a Matlab UI to users,\\nand also to foster inter-operability between projects which are written both in Matlab and Python.\\nIn particular, `pyHorace` ([prototype](https:\/\/github.com\/mducle\/hugo)) cannot use the\\n[standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for Matlab to run Python code,\\nwhere calls to Python from Matlab are prefixed with `py.` followed by the full module specification.\\nFor example, `r = py.numpy.random.rand()` uses `numpy` to generate a random number.\\nThis is because such a call causes Matlab to\\n[automatically spawn](https:\/\/uk.mathworks.com\/help\/matlab\/ref\/pyenv.html) a dependent Python interpreter,\\nwhich can be either created within the same process as the Matlab interpreter (`InProcess`)\\nor in an external process (`OutOfProcess`).\\n`pyHorace` already runs within a Python interpreter and the compiled Matlab library *must* be loaded in-process.\\nThus, if Matlab spawns a second Python intepreter with the default `InProcess` execution mode,\\nthe two Python interpreters will conflict causing memory errors and a crash.\\nWe can force Matlab to launch the dependent Python interpreter `OutOfProcess`\\nbut this imposes a significant performance penalty\\n(extensive testing was not done but Brille+SpinW runs about 10x slower than with `InProcess`).\\n\n\n##Decision\nAt a meeting on Jan 7 2021, the developers of `pyHorace`, `brillem` and `horace-euphonic-interface` agreed to accept this proposal.\\n`brillem` and `horace-euphonic-interface` will be refactored to use the `light_python_wrapper` proposed here.\\nThe meeting also agreed implementation details which will be described in [ADR #13](0013-light-python-wrapper-implementation-detail.md).\\n","Predictions":"We will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating Python code.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/ref\/pyenv.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/ref\/pyenv.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating a random number.nWe will use the [standard method](https:\/\/uk.mathworks.com\/help\/matlab\/call-python-libraries.html) for generating a random number."}
{"File Name":"ockam\/0007-rust-error-handling.md","Context":"## Context\nError handling is a very important process that is needed by every crate of the original Ockam rust library, as well as any third-party crate that was designed to supplement Ockam rust library (such as transport and vault implementations).\nThere are multiple requirements to error handling:\n- agile enough to be used in different cases\n- portable to work in different environments with different constraints\n- convenient for both library developer and library user\n","Decision":"In search of balance between above-mentioned requirements it has been decided that errors are handled in native to Rust way of using Result type, Error type for such Result should be either of ockam_core::Error of implement Into<ockam_core::Error>\\n","tokens":91,"id":4536,"text":"## Context\\nError handling is a very important process that is needed by every crate of the original Ockam rust library, as well as any third-party crate that was designed to supplement Ockam rust library (such as transport and vault implementations).\\nThere are multiple requirements to error handling:\\n- agile enough to be used in different cases\\n- portable to work in different environments with different constraints\\n- convenient for both library developer and library user\\n\n\n##Decision\nIn search of balance between above-mentioned requirements it has been decided that errors are handled in native to Rust way of using Result type, Error type for such Result should be either of ockam_core::Error of implement Into<ockam_core::Error>\\n","Predictions":"In search of balance between above-mentioned requirements it has been decided that errors are handled in native to Rust way of using Result type, Error type for such Result should be either of ockam_core::Error of implement Intoockam_core::Error>n"}
{"File Name":"educational-platform\/0010-use-axon-framework.md","Context":"## Context\nCurrently, a lot of custom classes are defined for DDD building blocks. It will be better to use DDD library for these goals.\n","Decision":"Axon Framework will be used as DDD library.\\n","tokens":32,"id":2498,"text":"## Context\\nCurrently, a lot of custom classes are defined for DDD building blocks. It will be better to use DDD library for these goals.\\n\n\n##Decision\nAxon Framework will be used as DDD library.\\n","Predictions":"We will use a Rail library to create a Rail class.n"}
{"File Name":"sdbmss\/0008-rdf-sparql-support.md","Context":"## Context\nAs part of participation in the [Mapping Manuscript Migrations (MMM)][mmm] Linked Data project, the SDBM needed to export its data in RDF format for aggregation in a unified set of data from the project's three contributing organizations.\n[mmm]: http:\/\/mappingmanuscriptmigrations.org \"Mapping Manuscript Migrations project site\"\nAt the time the export was built a single unified data model had not been agreed upon.\n","Decision":"The project decided to build a SPARQL endpoint built on Apache Jena. Since there was no target data model, the SDBM was exported to a custom namespace using a simple, direct mapping from SDBM model attributes to RDF. RabbitMQ messenger queue and a listener interface (`interface.rb`) to push updates from the SDBM to Jena. Also a simple SPARQL query interface was added for SDBM users.\\n","tokens":94,"id":1839,"text":"## Context\\nAs part of participation in the [Mapping Manuscript Migrations (MMM)][mmm] Linked Data project, the SDBM needed to export its data in RDF format for aggregation in a unified set of data from the project's three contributing organizations.\\n[mmm]: http:\/\/mappingmanuscriptmigrations.org \"Mapping Manuscript Migrations project site\"\\nAt the time the export was built a single unified data model had not been agreed upon.\\n\n\n##Decision\nThe project decided to build a SPARQL endpoint built on Apache Jena. Since there was no target data model, the SDBM was exported to a custom namespace using a simple, direct mapping from SDBM model attributes to RDF. RabbitMQ messenger queue and a listener interface (`interface.rb`) to push updates from the SDBM to Jena. Also a simple SPARQL query interface was added for SDBM users.\\n","Predictions":"The project decided to build a SPARQL endpoint built on Apache Jena. Since there was no target data model, the SDBM was exported to a custom namespace using a simple, direct mapping from SDBM model attributes to RDF. RabbitMQ messenger queue and a listener interface (interface.rb) to push updates from the SDBM to Jena. Also a simple SPARQL query interface was added for SDBM users.n"}
{"File Name":"modernisation-platform\/0006-use-a-multi-account-strategy-for-applications.md","Context":"## Context\nIn the Modernisation Platform, we want to reduce the blast radius and increase the scalability of how we create, maintain, and support applications in the context of what AWS account(s) they sit within.\n","Decision":"We've decided to use a multi-account strategy, split by application. We have a complete write-up as part of our [environments concept](https:\/\/user-guide.modernisation-platform.service.justice.gov.uk\/concepts\/environments\/).\\n","tokens":44,"id":3147,"text":"## Context\\nIn the Modernisation Platform, we want to reduce the blast radius and increase the scalability of how we create, maintain, and support applications in the context of what AWS account(s) they sit within.\\n\n\n##Decision\nWe've decided to use a multi-account strategy, split by application. We have a complete write-up as part of our [environments concept](https:\/\/user-guide.modernisation-platform.service.justice.gov.uk\/concepts\/environments\/).\\n","Predictions":"We've decided to use a multi-account strategy, split by application. We have a complete write-up as part of our [environments concept](https:\/\/user-guide.modernisation-platform.service.justice.gov.uk\/concepts\/environments\/).n"}
{"File Name":"cljdoc\/0014-add-support-for-examples.md","Context":"## Context\nExamples are an often suggested feature for cljdoc that could enable library authors and the community to further guide library users.\n","Decision":"Add support for examples to cljdoc. Allow libary users to provide examples through their Git repository but also run a community-maintained examples repository where examples can be maintained outside of a project's own Git repository.\\n","tokens":28,"id":2616,"text":"## Context\\nExamples are an often suggested feature for cljdoc that could enable library authors and the community to further guide library users.\\n\n\n##Decision\nAdd support for examples to cljdoc. Allow libary users to provide examples through their Git repository but also run a community-maintained examples repository where examples can be maintained outside of a project's own Git repository.\\n","Predictions":"We will use the iFrame API to render the iFrame.n"}
{"File Name":"alfresco-anaxes-shipyard\/0002-docker-registry-for-internal-and-protected-images.md","Context":"## Context\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\n","Decision":"We will use [Quay.io](https:\/\/quay.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and\/or enterprise artifacts with trail licenses).\\nWe\u2019d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we'd prefer a SaaS solution.\\nThe REST API of Quay.io allows our organization to potentially automate user provisioning\/invitation and user\/group management which is not available for Docker Cloud at this time.\\nAdditionally, Quay \/ CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product.\\nThe summary [![report of the comparison](https:\/\/img.shields.io\/badge\/report%20of%20the%20comparison-PRIVATE-red.svg)](https:\/\/ts.alfresco.com\/share\/s\/mVAV1sGIReC_iqgMN0GGnQ) also contains reference links to the full investigation.\\n","tokens":130,"id":3598,"text":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\\n\n\n##Decision\nWe will use [Quay.io](https:\/\/quay.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and\/or enterprise artifacts with trail licenses).\\nWe\u2019d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we'd prefer a SaaS solution.\\nThe REST API of Quay.io allows our organization to potentially automate user provisioning\/invitation and user\/group management which is not available for Docker Cloud at this time.\\nAdditionally, Quay \/ CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product.\\nThe summary [![report of the comparison](https:\/\/img.shields.io\/badge\/report%20of%20the%20comparison-PRIVATE-red.svg)](https:\/\/ts.alfresco.com\/share\/s\/mVAV1sGIReC_iqgMN0GGnQ) also contains reference links to the full investigation.\\n","Predictions":"We will use [Quay.io](https:\/\/quay.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and\/or enterprise artifacts with trail licenses).nWe\u2019d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we\u2019d prefer a SaaS solution.nThe REST API of Quay.io allows our organization to potentially automate user provisioning\/invitation and user\/group management which is not available for Docker Cloud at this time.nAdditionally, Quay \/ CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product.nThe summary [![report of the comparison](https:\/\/img.shields.io\/badge\/report%20of%20the%20comparison-PRIVATE-red.svg)](https:\/\/ts.alfresco.com\/share\/s\/mVAV1sGIReC_iqgMN0GGnQ) also contains reference links to the full investigation.n"}
{"File Name":"corona-hackathon\/0010-branching-strategy.md","Context":"## Context\nWe have to make a decision on the branching strategy for development.\n","Decision":"[Git Flow](https:\/\/danielkummer.github.io\/git-flow-cheatsheet\/) it will be.\\n","tokens":17,"id":3653,"text":"## Context\\nWe have to make a decision on the branching strategy for development.\\n\n\n##Decision\n[Git Flow](https:\/\/danielkummer.github.io\/git-flow-cheatsheet\/) it will be.\\n","Predictions":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisionsn"}
{"File Name":"ReportMI-service-manual\/0011-use-auth0-for-temporary-user-authentication.md","Context":"## Context\nOur user facing applications will need to authenticate users.\nIn the long-term, we want to use the standard user sign-on services in CCS for\neach appropriate user group.\nFor suppliers, this is the [Supplier Registration Service (SRS)][service-srs].\nFor CCS staff it is their G-Suite account.\nUsing a single authentication service will reduce administration overheads\nbecause we won't have to manage a JML (Joiners Movers Leavers) process. For\nusers it will also mean they don't have to remember yet another username and\npassword combination.\nHowever, we want to get moving quickly, and integration with SRS\/CCS G-Suite\nwill take time. For now, we want a simple sign-in service that we can integrate\nwith quickly, to enable us to test and iterate the application.\n","Decision":"We will use [Auth0][service-auth0] for authentication on a temporary basis.\\nAuth0 is an authentication service that uses the OAuth protocol. It provides\\nand simple integration path, and is free for several thousand active users.\\nWe acknowledge that we are incurring technical debt by not integrating\\nimmediately with SRS and G-Suite, however we believe that this will let us move\\nmore quickly during development.\\nThis decision will also allow us to gain a better understanding of the user\\nneeds around authentication before a decision around the final authentication\\nsolution is made.\\nWe will replace Auth0 with the proper authentication service later in the\\ndevelopment process.\\n","tokens":179,"id":2041,"text":"## Context\\nOur user facing applications will need to authenticate users.\\nIn the long-term, we want to use the standard user sign-on services in CCS for\\neach appropriate user group.\\nFor suppliers, this is the [Supplier Registration Service (SRS)][service-srs].\\nFor CCS staff it is their G-Suite account.\\nUsing a single authentication service will reduce administration overheads\\nbecause we won't have to manage a JML (Joiners Movers Leavers) process. For\\nusers it will also mean they don't have to remember yet another username and\\npassword combination.\\nHowever, we want to get moving quickly, and integration with SRS\/CCS G-Suite\\nwill take time. For now, we want a simple sign-in service that we can integrate\\nwith quickly, to enable us to test and iterate the application.\\n\n\n##Decision\nWe will use [Auth0][service-auth0] for authentication on a temporary basis.\\nAuth0 is an authentication service that uses the OAuth protocol. It provides\\nand simple integration path, and is free for several thousand active users.\\nWe acknowledge that we are incurring technical debt by not integrating\\nimmediately with SRS and G-Suite, however we believe that this will let us move\\nmore quickly during development.\\nThis decision will also allow us to gain a better understanding of the user\\nneeds around authentication before a decision around the final authentication\\nsolution is made.\\nWe will replace Auth0 with the proper authentication service later in the\\ndevelopment process.\\n","Predictions":"We will use [Auth0][service-auth0] for authentication on a temporary basis.nAuth0 is an authentication service that uses the OAuth protocol. It providesnand simple integration path, and is free for several thousand active users.nWe acknowledge that we are incurring technical debt by not integrating with SRS and G-Suite, however we believe that this will let us movenmore quickly during development.nThis decision will also allow us to gain a better understanding of the user needs around authentication before a decision around the final authentication servicensolution is made.nWe will replace Auth0 with the proper authentication service later in thedevelopment process.n"}
{"File Name":"libelektra\/lookup_every_key.md","Context":"## Problem\nOn structures like maps or [arrays](array.md) there are different\npossibilities which keys are looked up in the KeySet and which\nare simply iterated.\nWithout any guidelines, applications would provide arbitrary inconsistent\nbehavior.\n","Decision":"Every key that an application wants to use, must be looked up\\nwith `ksLookup` using a cascading lookup key.\\n- [Arrays](array.md)\\n","tokens":51,"id":1310,"text":"## Problem\\nOn structures like maps or [arrays](array.md) there are different\\npossibilities which keys are looked up in the KeySet and which\\nare simply iterated.\\nWithout any guidelines, applications would provide arbitrary inconsistent\\nbehavior.\\n\n\n##Decision\nEvery key that an application wants to use, must be looked up\\nwith `ksLookup` using a cascading lookup key.\\n- [Arrays](array.md)\\n","Predictions":"- [Separate key](https:\/\/github.com\/separate-keys\/separate-keys)n- [Separate key](https:\/\/github.com\/separate-keys\/separate-keys\/separate-keys\/)n- [Separate key](https:\/\/github.com\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-keys\/separate-"}
{"File Name":"where-away\/0009-use-xml-for-bookmarks-format.md","Context":"## Context\nOptions:\n- custom format, custom parser\n- that's a lot of work\n- json\n- awkward format for depicting a tree\n- yaml\n- significant whitespace makes this more error-prone, and the bookmarks file\nis likely to be edited frequently\n- xml\n- classic\n- shows trees well\n- less error prone to edit\n- not supported natively in node\n- neither parsing nor schema validation\n- schema is simple enough that I could hand-roll a validator\n- see discussion on parsing -- it needs a library: https:\/\/stackoverflow.com\/questions\/11398419\/trying-to-use-the-domparser-with-node-js\n- top two promising libraries:\n- jsdom, 32 dependencies https:\/\/www.npmjs.com\/package\/jsdom\n- xmldom, 0 dependencies https:\/\/www.npmjs.com\/package\/xmldom\n","Decision":"Use XML for the bookmarks format. Use\\n[xmldom](https:\/\/www.npmjs.com\/package\/xmldom) to parse, and hand-roll the\\nvalidation.\\n","tokens":180,"id":2274,"text":"## Context\\nOptions:\\n- custom format, custom parser\\n- that's a lot of work\\n- json\\n- awkward format for depicting a tree\\n- yaml\\n- significant whitespace makes this more error-prone, and the bookmarks file\\nis likely to be edited frequently\\n- xml\\n- classic\\n- shows trees well\\n- less error prone to edit\\n- not supported natively in node\\n- neither parsing nor schema validation\\n- schema is simple enough that I could hand-roll a validator\\n- see discussion on parsing -- it needs a library: https:\/\/stackoverflow.com\/questions\/11398419\/trying-to-use-the-domparser-with-node-js\\n- top two promising libraries:\\n- jsdom, 32 dependencies https:\/\/www.npmjs.com\/package\/jsdom\\n- xmldom, 0 dependencies https:\/\/www.npmjs.com\/package\/xmldom\\n\n\n##Decision\nUse XML for the bookmarks format. Use\\n[xmldom](https:\/\/www.npmjs.com\/package\/xmldom) to parse, and hand-roll the\\nvalidation.\\n","Predictions":"- [Option 1: n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n- n-"}
{"File Name":"eslint-config-nhsuk\/0002-replace-travis-ci-with-github-actions.md","Context":"## Context\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\ngoing to be generally available on\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\nGitHub Actions will have a long term future. It is likely GitHub Actions\nwill become the default CI mechanism (and possibly more) for projects hosted on\nGitHub. Using them in this repo, which has a basic use case will provide some\nexposure to the service.\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":114,"id":152,"text":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nGitHub Actions will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repo, which has a basic use case will provide some\\nexposure to the service.\\n\n\n##Decision\nThe decision is to replace Travis CI with GitHub Actions.\\n","Predictions":"The decision is to use a version attribute to indicate the version number of a release.n"}
{"File Name":"ODT_OnlineBank\/ARD-0001.md","Context":"## Context\n*This section describes the forces at play, including technological, political, social, and project local.\nThese forces are probably in tension, and should be called out as such. The language in this section is value-neutral.\nIt is simply describing facts.*\nThe Office of Development Transformation (ODT) is focused on helping CAD learn and adopt lean, agile, and devOps techniques and\nprocesses. ODT will be initialy focus on Green Field Developent. The Online Bank is a sample app used in the ODT University classes.  The Purpose of theapplication is to provide a\nsample app as a context for the training courses.  We believe most people are familiar with online\nbanking, and won't need to spend a lot of time learning the problem domain.\nThe purpose of this document is to determine the technology stack for the sample application.\n**NOTE:** While we are initially focused on a Java Web stack, we may elect to develop out an example in the Microsoft Stack.\nIf you would like to help, please contact us.\n","Decision":"*This section describes our response to these forces. It is stated in full sentences, with active voice.\\n\"We will ...\"*\\nWe will be focusing on the modern web java stack.\\n### UI Technology Stack:\\n- Angular 2\\n- Bootstrap\\n- protractor\\n- jasmine\\n### Server Side Technology Stack:\\n- Java 1.8+\\n- SpringBoot 1.4+\\n- Restful\\n- MySQL\\n- Spring Data JPA (Hibernate)\\n- JUnit 5 \/ Spock\\n### Development Technology Stack:\\n- Local Development\\n- SCM: GIT\\n- Intellij\\n- Favorite Editor\\n- Gradle\\n- Angular Quickstart\\n- Concourse CI (CI\/CD)\\n- Jira\\n- Sonar\\n#### Deployment:\\n- Pivotal Cloud Foundry (Java and Static Build Packs)\\n","tokens":220,"id":1665,"text":"## Context\\n*This section describes the forces at play, including technological, political, social, and project local.\\nThese forces are probably in tension, and should be called out as such. The language in this section is value-neutral.\\nIt is simply describing facts.*\\nThe Office of Development Transformation (ODT) is focused on helping CAD learn and adopt lean, agile, and devOps techniques and\\nprocesses. ODT will be initialy focus on Green Field Developent. The Online Bank is a sample app used in the ODT University classes.  The Purpose of theapplication is to provide a\\nsample app as a context for the training courses.  We believe most people are familiar with online\\nbanking, and won't need to spend a lot of time learning the problem domain.\\nThe purpose of this document is to determine the technology stack for the sample application.\\n**NOTE:** While we are initially focused on a Java Web stack, we may elect to develop out an example in the Microsoft Stack.\\nIf you would like to help, please contact us.\\n\n\n##Decision\n*This section describes our response to these forces. It is stated in full sentences, with active voice.\\n\"We will ...\"*\\nWe will be focusing on the modern web java stack.\\n### UI Technology Stack:\\n- Angular 2\\n- Bootstrap\\n- protractor\\n- jasmine\\n### Server Side Technology Stack:\\n- Java 1.8+\\n- SpringBoot 1.4+\\n- Restful\\n- MySQL\\n- Spring Data JPA (Hibernate)\\n- JUnit 5 \/ Spock\\n### Development Technology Stack:\\n- Local Development\\n- SCM: GIT\\n- Intellij\\n- Favorite Editor\\n- Gradle\\n- Angular Quickstart\\n- Concourse CI (CI\/CD)\\n- Jira\\n- Sonar\\n#### Deployment:\\n- Pivotal Cloud Foundry (Java and Static Build Packs)\\n","Predictions":"* This section describes our response to these forces. It is stated in full sentences, with active voice. \"We will...\"nWe will be focusing on the modern web java stack.n### UI Technology Stack:n- Angular 2n- Bootstrapn- protractorn- jasminen### Server Side Technology Stack:n- Java 1.8+n- SpringBoot 1.4+n- Restfuln- MySQLn- Spring Data JPA (Hibernate)n- JUnit 5 \/ Spockn### Development Technology Stack:n- Local Developmentn- SCM: GITn- Intellijn- Favorite Editorn- Gradlen- Angular Quickstartn- Concourse CI (CI\/CD)n- Jiran- Sonarn### Deployment:n- Pivotal Cloud Foundry (Java and Static Build Packs)n"}
{"File Name":"texas-holdem-code-challenge\/0009-use-typescript-3-4-5.md","Context":"## Context\nThe TypeScript plugin for ESLint requires TypeScript below 3.5.0. The next earlier\nversion is 3.4.5 (from ~ 1 month ago)\nWarning from ESLint:\n```\n=============\nWARNING: You are currently running a version of TypeScript which is not officially supported by typescript-estree.\nYou may find that it works just fine, or you may not.\nSUPPORTED TYPESCRIPT VERSIONS: >=3.2.1 <3.5.0\nYOUR TYPESCRIPT VERSION: 3.5.1\nPlease only submit bug reports when using the officially supported version.\n=============\n```\nFurther, there are no cutting edge TypeScript features I need for this.\n","Decision":"Use TypeScript 3.4.5\\n","tokens":158,"id":2296,"text":"## Context\\nThe TypeScript plugin for ESLint requires TypeScript below 3.5.0. The next earlier\\nversion is 3.4.5 (from ~ 1 month ago)\\nWarning from ESLint:\\n```\\n=============\\nWARNING: You are currently running a version of TypeScript which is not officially supported by typescript-estree.\\nYou may find that it works just fine, or you may not.\\nSUPPORTED TYPESCRIPT VERSIONS: >=3.2.1 <3.5.0\\nYOUR TYPESCRIPT VERSION: 3.5.1\\nPlease only submit bug reports when using the officially supported version.\\n=============\\n```\\nFurther, there are no cutting edge TypeScript features I need for this.\\n\n\n##Decision\nUse TypeScript 3.4.5\\n","Predictions":"* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version suffix to indicate the version number of the application.n* We will use a version"}
{"File Name":"govuk-aws\/0009-environment-bootstrapping-process.md","Context":"## Context\nWe need a process to bootstrap a new Amazon environment. This should be both heavily automated\nand include set places where the operator can evaluate the status of the provisioning.\nThis bootstrapping process assumes the following:\n* The required repositories are available\n* An Amazon Web Services admin account is available\n* Backups of our data can be retrieved (for the restore process)\n","Decision":"The chosen process has a few, early bootstrap steps, that differ from normal operations. These\\naim to quickly provide the usual level of self-service to allow teams to independently\\nrestore services without a dependency on a central operations team. An early draft of\\nthis process, which will be updated as changes are required, will be:\\n1. Clone all the relevant repositories\\n1. Build the S3 bucket for Terraform state\\n1. Provision the new DNS zones for this environment\\n1. Build the Puppet master\\n1. Deploy the puppet code and secrets\\n1. Build the deploy Jenkins\\n* Rebuild everything else in the usual deployment ways\\nThis document will be written so that one of the non-infrastructure members of the team\\nwill be able to provision a complete environment within our allowed recovery time.\\n","tokens":80,"id":4025,"text":"## Context\\nWe need a process to bootstrap a new Amazon environment. This should be both heavily automated\\nand include set places where the operator can evaluate the status of the provisioning.\\nThis bootstrapping process assumes the following:\\n* The required repositories are available\\n* An Amazon Web Services admin account is available\\n* Backups of our data can be retrieved (for the restore process)\\n\n\n##Decision\nThe chosen process has a few, early bootstrap steps, that differ from normal operations. These\\naim to quickly provide the usual level of self-service to allow teams to independently\\nrestore services without a dependency on a central operations team. An early draft of\\nthis process, which will be updated as changes are required, will be:\\n1. Clone all the relevant repositories\\n1. Build the S3 bucket for Terraform state\\n1. Provision the new DNS zones for this environment\\n1. Build the Puppet master\\n1. Deploy the puppet code and secrets\\n1. Build the deploy Jenkins\\n* Rebuild everything else in the usual deployment ways\\nThis document will be written so that one of the non-infrastructure members of the team\\nwill be able to provision a complete environment within our allowed recovery time.\\n","Predictions":"We will use [Security Gateway](https:\/\/github.com\/Security Gateway) to provide a single, centralised, and centralized access point to the TANF service.n"}
{"File Name":"winery\/0030-multiple-repositories.md","Context":"## Context and Problem Statement\nTo help developers obtain TOSCA elements in a more practical way, an approach to supporting multiple repositories distributed across different servers is presented here.\nThe result provides the ability to define a set of URLs and load them into the current winery-repository.\n","Decision":"Option 2 was chosen to manage each repository individually. This makes it possible to use existing version control systems in the individual repositories.\\nThe repositories are structured as follows.\\n![Repository Structure](graphics\/repo-structure.png)\\n### Positive Consequences <!-- optional -->\\n* User can search for elements online and load a repository using the url\\n* It's enough to send a URL instead of a CSAR\\n* Manage each repository independently\\n* The origin and the version history of a repository can be checked\\n* The possibility to contribute directly to the development of a repository\\n### Negative consequences <!-- optional -->\\n* User is forced to define namespaces in Namespaces.json.\\n* Additional configuration file (repositories.json)\\n","tokens":57,"id":4305,"text":"## Context and Problem Statement\\nTo help developers obtain TOSCA elements in a more practical way, an approach to supporting multiple repositories distributed across different servers is presented here.\\nThe result provides the ability to define a set of URLs and load them into the current winery-repository.\\n\n\n##Decision\nOption 2 was chosen to manage each repository individually. This makes it possible to use existing version control systems in the individual repositories.\\nThe repositories are structured as follows.\\n![Repository Structure](graphics\/repo-structure.png)\\n### Positive Consequences <!-- optional -->\\n* User can search for elements online and load a repository using the url\\n* It's enough to send a URL instead of a CSAR\\n* Manage each repository independently\\n* The origin and the version history of a repository can be checked\\n* The possibility to contribute directly to the development of a repository\\n### Negative consequences <!-- optional -->\\n* User is forced to define namespaces in Namespaces.json.\\n* Additional configuration file (repositories.json)\\n","Predictions":"Chosen option: \"Protobuf\", because it comes out best (see below).n### Positive Consequencesn- We need to start defining interfaces outside of their implementations.n- We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example, we need to use a simple HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example, we need to use a simple HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example, we need to use a simple HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example, we need to use a simple HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example, we need to use a simple HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example, we need to use a simple HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example, we need to use a simple HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example, we need to use a simple HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example, we need to use a simple HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example, we need to use a simple HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example, we need to use a simple HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example, we need to use a simple HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example"}
{"File Name":"simple-android\/001-screen-controllers.md","Context":"## Context\nWe don\u2019t want to put business logic inside Android framework classes (like an `Activity` or `Fragment`) because those cannot be unit tested. To enable\na fast feedback loop (i.e. tests that run on the JVM and not Android VM), we separate screens and controllers using\nthe [MVI architecture](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1) [pattern](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1)\n.\n","Decision":"Every screen has one controller that consumes user events, performs business logic with the help of data repositories and communicates UI changes back\\nto the screen.\\nUser interactions happening on the screen are abstracted inside data classes of type `UiEvent`. These events flow to the controller in the form of\\nRxJava streams.\\n```kotlin\\n\/\/ Create the UsernameTextChanged event by listening to the EditText\\nRxTextView\\n.textChanges(usernameEditText)\\n.map { text -> UsernameTextChanged(text) }\\n\/\/ Event\\ndata class UsernameTextChanged(text: String) : UiEvent\\n```\\nThe screen sends a single stream of `UiEvent`s to the controller and gets back a transformed stream of UI changes. The flow of data is\\nuni-directional. To merge multiple streams into one, RxJava\u2019s `merge()`  operator is used.\\n```kotlin\\n\/\/ Login screen\\nObservable.merge(usernameChanges(), passwordChanges(), submitClicks())\\n.compose(controller)\\n.takeUntil(screenDestroy)\\n.subscribe { uiChange -> uiChange(this) }\\n```\\nIn the controller, `UiEvent`s are transformed as per the business logic and `UiChange`s are sent back to the screen. The `UiChange` is a simple lambda\\nfunction that takes the screen itself as an argument, which can call a method implemented by the screen interface.\\n```kotlin\\ntypealias Ui = LoginScreen\\ntypealias UiChange = (LoginScreen) -> Unit\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nevents.ofType<UsernameTextChanged>\\n.map { isValidUsername(it.text) }\\n.map { isValid ->\\n{ ui: Ui -> ui.setSubmitButtonEnabled(isValid) } \/\/ UiChange lambda!\\n}\\n}\\n}\\n```\\nWhen the events have to observed across multiple functions in the controller, the stream is shared using `replay()` + `refCount()` so that the UI\\nevents aren't recreated once for every subscription. `replay()` shares a single subscription to the screen by replaying the events to every observer\\nand `refCount()` keeps the subscription alive as long as there is at least one observer.\\n```kotlin\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nval replayedEvents = events.replay().refCount()\\nreturn Observable.merge(\\nenableSubmitButton(replayedEvents),\\nloginUser(replayedEvents))\\n}\\nfun enableSubmitButton(events: Observable<UiEvent>): Observable<UiChange>()\\nfun loginOnSubmit(events: Observable<UiEvent>): Observable<UiChange>()\\n}\\n```\\n![data flow from the Ui to the controller](images\/diagram_screen_controller.png)\\n([diagram source](https:\/\/docs.google.com\/drawings\/d\/1I_VdUM8Pf9O3nOYViqVF6kiyqFaYFD2fHmKRyvwmEl4\/edit?usp=sharing))\\n","tokens":137,"id":1133,"text":"## Context\\nWe don\u2019t want to put business logic inside Android framework classes (like an `Activity` or `Fragment`) because those cannot be unit tested. To enable\\na fast feedback loop (i.e. tests that run on the JVM and not Android VM), we separate screens and controllers using\\nthe [MVI architecture](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1) [pattern](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1)\\n.\\n\n\n##Decision\nEvery screen has one controller that consumes user events, performs business logic with the help of data repositories and communicates UI changes back\\nto the screen.\\nUser interactions happening on the screen are abstracted inside data classes of type `UiEvent`. These events flow to the controller in the form of\\nRxJava streams.\\n```kotlin\\n\/\/ Create the UsernameTextChanged event by listening to the EditText\\nRxTextView\\n.textChanges(usernameEditText)\\n.map { text -> UsernameTextChanged(text) }\\n\/\/ Event\\ndata class UsernameTextChanged(text: String) : UiEvent\\n```\\nThe screen sends a single stream of `UiEvent`s to the controller and gets back a transformed stream of UI changes. The flow of data is\\nuni-directional. To merge multiple streams into one, RxJava\u2019s `merge()`  operator is used.\\n```kotlin\\n\/\/ Login screen\\nObservable.merge(usernameChanges(), passwordChanges(), submitClicks())\\n.compose(controller)\\n.takeUntil(screenDestroy)\\n.subscribe { uiChange -> uiChange(this) }\\n```\\nIn the controller, `UiEvent`s are transformed as per the business logic and `UiChange`s are sent back to the screen. The `UiChange` is a simple lambda\\nfunction that takes the screen itself as an argument, which can call a method implemented by the screen interface.\\n```kotlin\\ntypealias Ui = LoginScreen\\ntypealias UiChange = (LoginScreen) -> Unit\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nevents.ofType<UsernameTextChanged>\\n.map { isValidUsername(it.text) }\\n.map { isValid ->\\n{ ui: Ui -> ui.setSubmitButtonEnabled(isValid) } \/\/ UiChange lambda!\\n}\\n}\\n}\\n```\\nWhen the events have to observed across multiple functions in the controller, the stream is shared using `replay()` + `refCount()` so that the UI\\nevents aren't recreated once for every subscription. `replay()` shares a single subscription to the screen by replaying the events to every observer\\nand `refCount()` keeps the subscription alive as long as there is at least one observer.\\n```kotlin\\nclass LoginScreenController : ObservableTransformer<UiEvent, UiChange>() {\\nfun apply(events: Observable<UiEvent>): Observable<UiChange> {\\nval replayedEvents = events.replay().refCount()\\nreturn Observable.merge(\\nenableSubmitButton(replayedEvents),\\nloginUser(replayedEvents))\\n}\\nfun enableSubmitButton(events: Observable<UiEvent>): Observable<UiChange>()\\nfun loginOnSubmit(events: Observable<UiEvent>): Observable<UiChange>()\\n}\\n```\\n![data flow from the Ui to the controller](images\/diagram_screen_controller.png)\\n([diagram source](https:\/\/docs.google.com\/drawings\/d\/1I_VdUM8Pf9O3nOYViqVF6kiyqFaYFD2fHmKRyvwmEl4\/edit?usp=sharing))\\n","Predictions":"We will use the [MVI architecture](https:\/\/medium.com\/@ragunathjawahar\/mvi-series-a-pragmatic-reactive-architecture-for-android-87ca176387d1) to create a single controller that consumes user events, performs business logic with the help of data repositories and communicates UI changes backnto the screen.nUser interactions happening on the screen are abstracted inside data classes of type UiEvent. These events flow to the controller in the form ofnRxJava streams.nkotlinn\/\/ Create the UsernameTextChanged event by listening to the EditTextnRxTextViewn.textChanges(usernameEditText)n.map  text -> UsernameTextChanged(text) n\/\/ Eventndata class UsernameTextChanged(text: String) : UiEventnnThe screen sends a single stream of UiEvents to the controller and gets back a transformed stream of UI changes. The flow of data isnuni-directional. To merge multiple streams into one, RxJava\u2019s merge() operator is used.nkotlinn\/\/ Login screennObservable.merge(usernameChanges(), passwordChanges(), submitClicks())n.compose(controller)n.takeUntil(screenDestroy)n.subscribe  uiChange nnThe user interface will be a single controller that consumes user events, performs business logic with the help of data repositories and communicates UI changes backnto the screen.nUser interactions happening on the screen are abstracted inside data classes of type UiEvent. These events flow to the controller in the form ofnRxJava streams.nkotlinn\/\/ Create the UsernameTextChanged event by listening to the EditTextnRxTextViewn.textChanges(usernameEditText)n.map  text -> UsernameTextChanged(text) n\/\/ Eventndata class UsernameTextChanged(text: String) : UiEventnnThe screen sends a single stream of UiEvents to the controller and gets back a transformed stream of UI changes. The flow of data isnuni-directional. To merge multiple streams into one, RxJava\u2019s merge() operator is used.nkotlinn\/\/ Login screennObservable.merge(usernameChanges(), passwordChanges(), submitClicks())n.compose(controller)n.takeUntil(screenDestroy)n.subscribe  uiChange nn"}
{"File Name":"adr\/ADR-9-DecentralandInterface-evolution-plan.md","Context":"## Context and Problem Statement\nHow should we proceed to maintain compatibility between the current SDK and the next-gen SDK currently being developed?\nTaking into consideration:\n* All the scenes currently deployed must work as they work today.\n* `DecentralandInterface` should work indefinitely to maintain backwards compatibility.\n* Maintainance costs should be taken into account\n","Decision":"We decided to develop a new and clean interface for the next-gen SDK, then create an adapter for the legacy `DecentralandInterface` (option 2) because:\\n* We should not be limited by the decisions of the past SDK\\n* The new SDK must eventually prove itself by reaching a feature parity with the current interface\\n* Avoid sunk cost fallacy with the old SDK\\n","tokens":72,"id":4618,"text":"## Context and Problem Statement\\nHow should we proceed to maintain compatibility between the current SDK and the next-gen SDK currently being developed?\\nTaking into consideration:\\n* All the scenes currently deployed must work as they work today.\\n* `DecentralandInterface` should work indefinitely to maintain backwards compatibility.\\n* Maintainance costs should be taken into account\\n\n\n##Decision\nWe decided to develop a new and clean interface for the next-gen SDK, then create an adapter for the legacy `DecentralandInterface` (option 2) because:\\n* We should not be limited by the decisions of the past SDK\\n* The new SDK must eventually prove itself by reaching a feature parity with the current interface\\n* Avoid sunk cost fallacy with the old SDK\\n","Predictions":"We decided to develop a new and clean interface for the next-gen SDK, then create an adapter for the legacy DecentralandInterface (option 2) because:n* We should not be limited by the decisions of the past SDKn* The new SDK must eventually prove itself by reaching a feature parity with the current interfacen* Avoid sunk cost fallacy with the old SDKn"}
{"File Name":"adr\/ADR-28-smart-contract-wallets-and-meta-transactions.md","Context":"## Context and Problem Statement\nCurrently we support meta-transactions on Polygon from several of our dApps (Marketplace, Builder, Account) and the Explorer.\nMeta-transactions require the user to sign a function signature that they want us to call on their behalf (so we can subsidize the gas).\nThe smart contracts that support meta-transactions (like the Marketplace contract) recover the account from the signature on chain, and uses that as the message sender.\nSmart contract wallets are smart contracts that can hold assets, and have a set of signers that can command the wallet to perform actions on those assets by providing signed messages authorizing given actions.\nWhen a user of a contract wallet signs a meta transaction, and the public key is recovered on chain by the smart contract that supports meta-transactions, the public key is the one from the signer, and not the contract address of the smart contract wallet (which is the actual owner of the assets). This causes the meta-transactions to fail on chain.\n","Decision":"We choose option 2 since we can never be sure that the smart contract wallet has a matching deployment on Polygon, and is not the case for the most popular smart contract wallets (like Argent or Authereum)\\n","tokens":205,"id":4623,"text":"## Context and Problem Statement\\nCurrently we support meta-transactions on Polygon from several of our dApps (Marketplace, Builder, Account) and the Explorer.\\nMeta-transactions require the user to sign a function signature that they want us to call on their behalf (so we can subsidize the gas).\\nThe smart contracts that support meta-transactions (like the Marketplace contract) recover the account from the signature on chain, and uses that as the message sender.\\nSmart contract wallets are smart contracts that can hold assets, and have a set of signers that can command the wallet to perform actions on those assets by providing signed messages authorizing given actions.\\nWhen a user of a contract wallet signs a meta transaction, and the public key is recovered on chain by the smart contract that supports meta-transactions, the public key is the one from the signer, and not the contract address of the smart contract wallet (which is the actual owner of the assets). This causes the meta-transactions to fail on chain.\\n\n\n##Decision\nWe choose option 2 since we can never be sure that the smart contract wallet has a matching deployment on Polygon, and is not the case for the most popular smart contract wallets (like Argent or Authereum)\\n","Predictions":"We choose option 2 since we can never be sure that the smart contract wallet has a matching deployment on Polygon, and is not the case for the most popular smart contract wallets (like Argent or Authereum)n"}
{"File Name":"libelektra\/cryptograhic_key_handling.md","Context":"## Problem\nThe crypto plugin applies cryptographic operations to Keys and KeySets.\nIn order to do that it needs keys and initialization vectors (IV).\nThe problem is how to retrieve or derivate those keys in a safe way and how\nto pass them on to the underlying crypto libraries (OpenSSL and libgcrypt\nat the time of writing).\n","Decision":"### General Approach\\nThe introduction of a GPG interface enables the user to utilize her existing key-pairs for cryptographic operations in Elektra.\\nThe private key is used for encrypting a random sequence, which serves as seed for a key derivation function (KDF).\\nThis way we can safely derivate cryptographic keys for symmetric value encryption.\\nBoth OpenSSL and libgcrypt have built-in support for the PBKDF2 (see RFC 2898).\\nThe PBKDF2 needs an iteration number and a salt in order to work.\\nThose values will be stored per Key as MetaKey.\\n### Implementation Details\\nDuring the **mount phase** a random master password _r_ is being generated. _r_ is sent to the gpg binary for encryption. The resulting encrypted master password _m_ is stored in the plugin configuration at `config\/masterChallenge`.\\nDuring the **set phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be encrypted. A random salt _s(k)_ is generated. By applying the PBKDF2 (mentioned earlier) with _r_ and _s(k)_, the cryptographic key _e(k)_ and the initialization vector _i(k)_ is being derived. The value of _k_ will be encrypted using _e(k)_ and _i(k)_. The seed _s(k)_ will be encoded as prefix into the encrypted value.\\nDuring the **get phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be decrypted. The salt _s(k)_ is read from the encrypted message. By applying the PBKDF2 with _r_ and _s(k)_ the values of _e(k)_ and _i(k)_ are restored. Then the encrypted message can be decrypted.\\n","tokens":73,"id":1270,"text":"## Problem\\nThe crypto plugin applies cryptographic operations to Keys and KeySets.\\nIn order to do that it needs keys and initialization vectors (IV).\\nThe problem is how to retrieve or derivate those keys in a safe way and how\\nto pass them on to the underlying crypto libraries (OpenSSL and libgcrypt\\nat the time of writing).\\n\n\n##Decision\n### General Approach\\nThe introduction of a GPG interface enables the user to utilize her existing key-pairs for cryptographic operations in Elektra.\\nThe private key is used for encrypting a random sequence, which serves as seed for a key derivation function (KDF).\\nThis way we can safely derivate cryptographic keys for symmetric value encryption.\\nBoth OpenSSL and libgcrypt have built-in support for the PBKDF2 (see RFC 2898).\\nThe PBKDF2 needs an iteration number and a salt in order to work.\\nThose values will be stored per Key as MetaKey.\\n### Implementation Details\\nDuring the **mount phase** a random master password _r_ is being generated. _r_ is sent to the gpg binary for encryption. The resulting encrypted master password _m_ is stored in the plugin configuration at `config\/masterChallenge`.\\nDuring the **set phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be encrypted. A random salt _s(k)_ is generated. By applying the PBKDF2 (mentioned earlier) with _r_ and _s(k)_, the cryptographic key _e(k)_ and the initialization vector _i(k)_ is being derived. The value of _k_ will be encrypted using _e(k)_ and _i(k)_. The seed _s(k)_ will be encoded as prefix into the encrypted value.\\nDuring the **get phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be decrypted. The salt _s(k)_ is read from the encrypted message. By applying the PBKDF2 with _r_ and _s(k)_ the values of _e(k)_ and _i(k)_ are restored. Then the encrypted message can be decrypted.\\n","Predictions":"The github.com\/security-api\/master\/security-api repository will be a github.com\/security-api\/master\/security-api repository.n"}
{"File Name":"jabref\/0010-use-h2-as-internal-database.md","Context":"## Context and Problem Statement\nWe need to store data internally in a structured way to gain performance.\n## Decision Drivers\n* Easy to integrate\n* Easy to use\n* Common technology\n","Decision":"* Easy to integrate\\n* Easy to use\\n* Common technology\\nChosen option: \"H2 Database Engine\", because it was straight-forward to use.\\n","tokens":39,"id":4747,"text":"## Context and Problem Statement\\nWe need to store data internally in a structured way to gain performance.\\n## Decision Drivers\\n* Easy to integrate\\n* Easy to use\\n* Common technology\\n\n\n##Decision\n* Easy to integrate\\n* Easy to use\\n* Common technology\\nChosen option: \"H2 Database Engine\", because it was straight-forward to use.\\n","Predictions":"* Easy to usen* Easy to usenChosen option: \"Standard Rails\", because it is a lightweight Rails application.n### Positive Consequencesn* Rails is easy to use and easy to usen### Negative Consequencesn* Rails is not a good fit for our needsn"}
{"File Name":"digitalrig-metal-aws\/0005-use-aws-rds-aurora-mysql-for-database-persistence.md","Context":"## Context\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL\/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted\/PaaS.\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor\/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny\/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\n","Decision":"Use AWS RDS Aurora MySql implemntation for our database persistence mechansim.  Create via Riglet CloudFormation stacks.  Production will run with 1 primary and 1 replica for HA.  Staging & Integration will run with only 1 Primary to keep costs down.\\n","tokens":238,"id":1740,"text":"## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL\/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted\/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor\/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny\/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n\n\n##Decision\nUse AWS RDS Aurora MySql implemntation for our database persistence mechansim.  Create via Riglet CloudFormation stacks.  Production will run with 1 primary and 1 replica for HA.  Staging & Integration will run with only 1 Primary to keep costs down.\\n","Predictions":"We will use AWS Aurora for our database persistence mechanism. Create via Riglet CloudFormation stacks. Production will run with 1 primary and 1 replica for HA. Staging & Integration will run with only 1 Primary to keep costs down.n"}
{"File Name":"TANF-app\/011-buildpacks.md","Context":"## Context\nCurrently, our frontend and backend apps are running in Docker containers on Cloud.gov. The deployment process involves building the docker containers at [DockerHub](https:\/\/dockerhub.com). Because there is very little security documentation on DockerHub, the path to getting an ATO with this process would be very difficult. There are other options that may be easier to document, but none of them offer the benefits of buildpacks, which have already been Fed Ramped and documented.\n","Decision":"Our recommendation is to move to Cloud.gov buildpacks at this time. They are already Fed Ramped, [shift responsibility to Cloud.gov](https:\/\/cloud.gov\/docs\/technology\/responsibilities\/) and ensure tightened security.\\n","tokens":99,"id":3793,"text":"## Context\\nCurrently, our frontend and backend apps are running in Docker containers on Cloud.gov. The deployment process involves building the docker containers at [DockerHub](https:\/\/dockerhub.com). Because there is very little security documentation on DockerHub, the path to getting an ATO with this process would be very difficult. There are other options that may be easier to document, but none of them offer the benefits of buildpacks, which have already been Fed Ramped and documented.\\n\n\n##Decision\nOur recommendation is to move to Cloud.gov buildpacks at this time. They are already Fed Ramped, [shift responsibility to Cloud.gov](https:\/\/cloud.gov\/docs\/technology\/responsibilities\/) and ensure tightened security.\\n","Predictions":"We will use AWS S3 v2 SDK to implement a Reactive S3 blob-store (blobstore-s3).n* Replace current blob-store-objectstorage with blobstore-s3n* Run load tests to ensure there's no penalty when switching to Swift support at the same time.n"}
{"File Name":"decode-demo\/0003-remove-spectre-css-and-replace-with-bootstrap.md","Context":"## Context\nThe deployed demo site must be designed to render nicely on mobile devices.\nThis is so that participants in the pilot can pretend the demo site provides\nthe same basic user experience as the wallet once this exists.\nSpectre CSS at least in the configuration implemented in the original version\nof this app doesn't render a UI that is usable from a mobile device,\nparticularly when it comes to form inputs (buttons and fields were tiny and\nhard to read).\n","Decision":"We will replace [Spectre.CSS](https:\/\/picturepan2.github.io\/spectre\/) with\\n[Bootstrap](https:\/\/getbootstrap.com\/).\\n","tokens":98,"id":3999,"text":"## Context\\nThe deployed demo site must be designed to render nicely on mobile devices.\\nThis is so that participants in the pilot can pretend the demo site provides\\nthe same basic user experience as the wallet once this exists.\\nSpectre CSS at least in the configuration implemented in the original version\\nof this app doesn't render a UI that is usable from a mobile device,\\nparticularly when it comes to form inputs (buttons and fields were tiny and\\nhard to read).\\n\n\n##Decision\nWe will replace [Spectre.CSS](https:\/\/picturepan2.github.io\/spectre\/) with\\n[Bootstrap](https:\/\/getbootstrap.com\/).\\n","Predictions":"We will replace [Spectre.CSS](https:\/\/picturepan2.github.io\/spectre\/) withn[Bootstrap](https:\/\/getbootstrap.com\/).n"}
{"File Name":"fxa\/0018-use-tailwind-with-custom-scss.md","Context":"## Context and Problem Statement\nThe [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.\nHistorically, the Firefox Accounts codebase has not adhered to a formal CSS structure. This ADR serves to determine how we'll approach our CSS architecture in the Settings Redesign project, evaluating libraries and frameworks to determine which if any will be the best option for the FxA ecosystem. It is part 2 of two [Settings Redesign CSS ADRs](https:\/\/github.com\/mozilla\/fxa\/issues\/5087); part 1, detailing how we'll approach build conventions and variables, [can be found here](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0015-use-css-variables-and-scss.md).\nConsiderations around class naming conventions, color and measurement standards, interoperability across shared components, and custom configuration options offered by each library to meet Settings Redesign design standards are taken into account. Notably, the new design uses space measurements in increments of 8px and [colors](https:\/\/protocol.mozilla.org\/fundamentals\/color.html) are based in Mozilla Protocol's design system, where a hue's brightness scales in increments of 10.\n## Decision Drivers\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\n","Decision":"- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\nChosen options: \"Option B\" with Tailwind CSS for majority styling, and implementation details from \"Option D\" when utility classes don't meet the entire need, because:\\n- Of the options set forth, a utility library provides us with the most flexible yet durable set of tools.\\n- Single-purpose classes are performant and reduce the possibility of overly-complex or convoluted stylesheets.\\n- A utility library is leaner and less opinionated compared to a set of UI components and other options, allowing greater flexibility and reusability across various projects.\\n- Our team has prior experience with Tailwind in particular and newcomers should ramp up quickly with a utility pattern.\\n- Tailwind is highly configurable without being cumbersome, allowing us to modify type and spacing scales, define color ranges, and set up media queries to meet our exact needs.\\n- For cases when we do need to write custom SCSS we will structure our React components to initially rely on utility classes, but allow additional custom styles to be written in an adjacent SCSS file when needed. This is also applicable to components in `fxa-components` where the component can accept a `classes` prop with a list of needed utility classes, and any additional styling can be done in an external SCSS file located where the component was composed as needed (e.g., outside of `fxa-components`). CSS variables can be shared across the Tailwind configuration and in custom SCSS.\\n- Note: class name conventions for the custom SCSS will be declared when the library configuration is setup, as we'd like them to make sense together. Examples will be provided in the `fxa-settings` README at this time as well.\\n","tokens":397,"id":381,"text":"## Context and Problem Statement\\nThe [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.\\nHistorically, the Firefox Accounts codebase has not adhered to a formal CSS structure. This ADR serves to determine how we'll approach our CSS architecture in the Settings Redesign project, evaluating libraries and frameworks to determine which if any will be the best option for the FxA ecosystem. It is part 2 of two [Settings Redesign CSS ADRs](https:\/\/github.com\/mozilla\/fxa\/issues\/5087); part 1, detailing how we'll approach build conventions and variables, [can be found here](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0015-use-css-variables-and-scss.md).\\nConsiderations around class naming conventions, color and measurement standards, interoperability across shared components, and custom configuration options offered by each library to meet Settings Redesign design standards are taken into account. Notably, the new design uses space measurements in increments of 8px and [colors](https:\/\/protocol.mozilla.org\/fundamentals\/color.html) are based in Mozilla Protocol's design system, where a hue's brightness scales in increments of 10.\\n## Decision Drivers\\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\n\n\n##Decision\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\\nChosen options: \"Option B\" with Tailwind CSS for majority styling, and implementation details from \"Option D\" when utility classes don't meet the entire need, because:\\n- Of the options set forth, a utility library provides us with the most flexible yet durable set of tools.\\n- Single-purpose classes are performant and reduce the possibility of overly-complex or convoluted stylesheets.\\n- A utility library is leaner and less opinionated compared to a set of UI components and other options, allowing greater flexibility and reusability across various projects.\\n- Our team has prior experience with Tailwind in particular and newcomers should ramp up quickly with a utility pattern.\\n- Tailwind is highly configurable without being cumbersome, allowing us to modify type and spacing scales, define color ranges, and set up media queries to meet our exact needs.\\n- For cases when we do need to write custom SCSS we will structure our React components to initially rely on utility classes, but allow additional custom styles to be written in an adjacent SCSS file when needed. This is also applicable to components in `fxa-components` where the component can accept a `classes` prop with a list of needed utility classes, and any additional styling can be done in an external SCSS file located where the component was composed as needed (e.g., outside of `fxa-components`). CSS variables can be shared across the Tailwind configuration and in custom SCSS.\\n- Note: class name conventions for the custom SCSS will be declared when the library configuration is setup, as we'd like them to make sense together. Examples will be provided in the `fxa-settings` README at this time as well.\\n","Predictions":"* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will be used to store and retrieve data from the Security component.n* The Security component will be a Security component that will"}
{"File Name":"libmemory\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3590,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"meadow\/0018-preservation-storage-object-naming-scheme.md","Context":"## Context\nThe application needs a way to store objects in preservation buckets that facilitate upload and retrieval and allows for duplicate file names among FileSets associated with a Work object.\n","Decision":"Use a combination of the pairtree of a FileSet ULID plus its sha256 checksum as the key for S3 objects in the preservation bucket. So an FileSet with a ULID `01dpxt2xajvkdsbckqrs8ry677`\\nand sha256 checksum `87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`will be stored in s3 as\\n`s3:\/\/[preservation-bucket]\/01\/dp\/xt\/2x\/01dpxt2xajvkdsbckqrs8ry677\/87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`\\n","tokens":35,"id":3831,"text":"## Context\\nThe application needs a way to store objects in preservation buckets that facilitate upload and retrieval and allows for duplicate file names among FileSets associated with a Work object.\\n\n\n##Decision\nUse a combination of the pairtree of a FileSet ULID plus its sha256 checksum as the key for S3 objects in the preservation bucket. So an FileSet with a ULID `01dpxt2xajvkdsbckqrs8ry677`\\nand sha256 checksum `87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`will be stored in s3 as\\n`s3:\/\/[preservation-bucket]\/01\/dp\/xt\/2x\/01dpxt2xajvkdsbckqrs8ry677\/87d2c7faf7774a8b07c43be06055c67c4bd602b8ec0e9d6b15241967d500d356`\\n","Predictions":"In search of balance between above-mentioned requirements it has been decided that errors are handled in native to Rust way of using Result type, Error type for such Result should be either of ockam_core::Error of implement Intoockam_core::Error>n"}
{"File Name":"cygnus-infra\/0002-use-debian-as-a-host-os.md","Context":"## Context\nI used the following points to make my decision:\n- Ubuntu 20.04 uses a unique auto-installation method, which is generally nice, but completely non-transferable\n- `snapd` remains controversial, but using Ubuntu systems forces that path more nowadays\n- Debian should be relatively \"install and forget\"\n- Centos is more opinionated about some things like how network configuration is done\n- Centos doesn't do automatic security upgrades, at least not in the same way RHEL advertises\n- I've successfully used Debian before for this role, before trying out Ubuntu again\n- Remastering a Debian iso with the preseed configuration can result in a one-button install process\n","Decision":"Use Debian buster as the hypervisor OS, with an automated installation via Debian preseeding.\\n","tokens":145,"id":3868,"text":"## Context\\nI used the following points to make my decision:\\n- Ubuntu 20.04 uses a unique auto-installation method, which is generally nice, but completely non-transferable\\n- `snapd` remains controversial, but using Ubuntu systems forces that path more nowadays\\n- Debian should be relatively \"install and forget\"\\n- Centos is more opinionated about some things like how network configuration is done\\n- Centos doesn't do automatic security upgrades, at least not in the same way RHEL advertises\\n- I've successfully used Debian before for this role, before trying out Ubuntu again\\n- Remastering a Debian iso with the preseed configuration can result in a one-button install process\\n\n\n##Decision\nUse Debian buster as the hypervisor OS, with an automated installation via Debian preseeding.\\n","Predictions":"- Use a src-dst-msg file to define the src-dst-msg file.n- Use a src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the src-dst-msg file to define the"}
{"File Name":"cloud-on-k8s\/0005-configurable-operator.md","Context":"## Context and Problem Statement\nMost operators out there operate in one of these two modes:\n1. Cluster-wide operator. Can manage resources in all namespaces, with cluster-wide admin privilege. A single operator running on the cluster.\n2. Namespaced operator. Can manage resources in the namespace it's deployed in, with admin permissions in that namespace. Several operators can be running in different namespaces.\nThe first option (cluster-wide single operator) has some major drawbacks:\n* it does not scale well with the number of clusters\n* it requires elevated permissions on the cluster\nThe second option (namespace operator) also has some major drawbacks:\n* it does not play well with cross-namespace features (a single enterprise license pool for multiple clusters in multiple namespaces, cross-cluster search and replication on clusters across namespaces)\n* to deploy 5 clusters in 5 different namespaces, it requires 5 operators running. A single one could have been technically enough.\n## Decision Drivers\n* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\n","Decision":"* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\nChosen option: option 2 (configurable operator), because it gives us more flexibility on the deployment strategy, and allows restricting RBAC permissions to a finer-grained level.\\n### Positive Consequences\\n* Much more flexibility to cover various deployment scenarios\\n* a single cluster-wide operator\\n* one operator per namespace\\n* one operator for all production namespaces, another one for all staging namespaces\\n* and so on\\n* We don't have to require cluster-level permissions to handle enterprise licensing\\n* A single operator concept, no namespace\/global\/ecosystem vocabulary madness\\n### Negative Consequences\\n* Too many options can lead to confusion, we need proper documentation\\n* Increased yaml complexity: need to develop a tool to generate yaml specifications\\n* The controller-runtime is not ready yet for multi-namespace watches\\n","tokens":354,"id":4707,"text":"## Context and Problem Statement\\nMost operators out there operate in one of these two modes:\\n1. Cluster-wide operator. Can manage resources in all namespaces, with cluster-wide admin privilege. A single operator running on the cluster.\\n2. Namespaced operator. Can manage resources in the namespace it's deployed in, with admin permissions in that namespace. Several operators can be running in different namespaces.\\nThe first option (cluster-wide single operator) has some major drawbacks:\\n* it does not scale well with the number of clusters\\n* it requires elevated permissions on the cluster\\nThe second option (namespace operator) also has some major drawbacks:\\n* it does not play well with cross-namespace features (a single enterprise license pool for multiple clusters in multiple namespaces, cross-cluster search and replication on clusters across namespaces)\\n* to deploy 5 clusters in 5 different namespaces, it requires 5 operators running. A single one could have been technically enough.\\n## Decision Drivers\\n* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\n\n\n##Decision\n* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\nChosen option: option 2 (configurable operator), because it gives us more flexibility on the deployment strategy, and allows restricting RBAC permissions to a finer-grained level.\\n### Positive Consequences\\n* Much more flexibility to cover various deployment scenarios\\n* a single cluster-wide operator\\n* one operator per namespace\\n* one operator for all production namespaces, another one for all staging namespaces\\n* and so on\\n* We don't have to require cluster-level permissions to handle enterprise licensing\\n* A single operator concept, no namespace\/global\/ecosystem vocabulary madness\\n### Negative Consequences\\n* Too many options can lead to confusion, we need proper documentation\\n* Increased yaml complexity: need to develop a tool to generate yaml specifications\\n* The controller-runtime is not ready yet for multi-namespace watches\\n","Predictions":"* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.n"}
{"File Name":"experimenter\/0009-nimbus-web-sdk-architecture.md","Context":"## Context and Problem Statement\nThis is part of a body of work necessary to support the use of Nimbus within web applications.\nThe current Nimbus SDK is written in such a way that it supports client-oriented experimentation \u2014 experiments are downloaded, evaluated, and stored on the client, and a feature store is exposed with the experiment branches applied.\nIn previous decisions (not in this repository), we've already decided that in order to support web experimentation the Nimbus SDK will need to be updated to be stateless, to support a more statically defined set of helper methods, and to have additional support for Python.\nUltimately, the problem we're trying to solve can be boiled down to one question \u2014 how can we update the Nimbus SDK to support web applications while continuing to support the existing clients?\nAs an example of what the Cirrus API might look like, we can likely expect endpoints to perform the following:\n* Enroll a user into available experiment(s)\n* This would return the enrolled experiments as well as the feature values given the enrollments\n* Fetch the default feature values\n* Fetch the feature manifest\n* Fetch a specific feature value given enrolled experiments\n## Decision Drivers\n* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\n* The SDK needs to support Python via UniFFI\n* The SDK needs to be stateless\n","Decision":"* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\nWe have decided to move forward with option number 2, \"Cargo features, one library\".\\nThis option, like the others mentioned, meets the key decision drivers.\\nWe believe using this option will be the most maintainable long-term, despite the added complexity of using cargo features.\\nIn addition, implementing this option has a similarly short timeline and amount of work necessary as compared to the \"Cargo features, separate library\" option, without the additional overhead of more complex typing.\\n### Positive Consequences\\n* Meets all decision drivers.\\n* Small amount of work necessary with a very limited amount of code churn.\\n### Negative Consequences\\n* It will be difficult to draw boundaries between Cirrus and Nimbus code.\\n* We could run into unexpected issues with UniFFI, as multiple UDL files in a single crate has not yet been tested.\\n2. [Cargo Features](https:\/\/doc.rust-lang.org\/cargo\/reference\/features.html)\\n2. Examples\\n1. [Multi-module typing example](https:\/\/github.com\/jeddai\/application-services\/pull\/1\/files#diff-02305e8e02a7900352e67be1fd2eef0b5a7c7cf91f4cc3e4559668c124d34e88R11-R19)\\n","tokens":307,"id":1819,"text":"## Context and Problem Statement\\nThis is part of a body of work necessary to support the use of Nimbus within web applications.\\nThe current Nimbus SDK is written in such a way that it supports client-oriented experimentation \u2014 experiments are downloaded, evaluated, and stored on the client, and a feature store is exposed with the experiment branches applied.\\nIn previous decisions (not in this repository), we've already decided that in order to support web experimentation the Nimbus SDK will need to be updated to be stateless, to support a more statically defined set of helper methods, and to have additional support for Python.\\nUltimately, the problem we're trying to solve can be boiled down to one question \u2014 how can we update the Nimbus SDK to support web applications while continuing to support the existing clients?\\nAs an example of what the Cirrus API might look like, we can likely expect endpoints to perform the following:\\n* Enroll a user into available experiment(s)\\n* This would return the enrolled experiments as well as the feature values given the enrollments\\n* Fetch the default feature values\\n* Fetch the feature manifest\\n* Fetch a specific feature value given enrolled experiments\\n## Decision Drivers\\n* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\n\n\n##Decision\n* Architecture decisions for Nimbus on the web[<sup>[1.i]<\/sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\nWe have decided to move forward with option number 2, \"Cargo features, one library\".\\nThis option, like the others mentioned, meets the key decision drivers.\\nWe believe using this option will be the most maintainable long-term, despite the added complexity of using cargo features.\\nIn addition, implementing this option has a similarly short timeline and amount of work necessary as compared to the \"Cargo features, separate library\" option, without the additional overhead of more complex typing.\\n### Positive Consequences\\n* Meets all decision drivers.\\n* Small amount of work necessary with a very limited amount of code churn.\\n### Negative Consequences\\n* It will be difficult to draw boundaries between Cirrus and Nimbus code.\\n* We could run into unexpected issues with UniFFI, as multiple UDL files in a single crate has not yet been tested.\\n2. [Cargo Features](https:\/\/doc.rust-lang.org\/cargo\/reference\/features.html)\\n2. Examples\\n1. [Multi-module typing example](https:\/\/github.com\/jeddai\/application-services\/pull\/1\/files#diff-02305e8e02a7900352e67be1fd2eef0b5a7c7cf91f4cc3e4559668c124d34e88R11-R19)\\n","Predictions":"* The SDK needs to support Python via UniFFIn* The SDK needs to be statelessnWe have decided to move forward with option number 2, \"Cargo features, one library\".nThis option, like the others mentioned, meets the key decision drivers.n* The SDK needs to support Python via UniFFIn* The SDK needs to support statelessnWe have decided to move forward with option number 3, \"Cargo features, one library\".nThis option, like the others mentioned, meets the key decision drivers.n* The SDK needs to support Python via UniFFIn* The SDK needs to support Python via UniFFIn* The SDK needs to support Python via UniFFInWe have decided to move forward with option number 4, \"Cargo features, one library\".nThis option, like the others mentioned, meets the key decision drivers.n* The SDK needs to support Python via UniFFIn* The SDK needs to support Python via UniFFIn* The SDK needs to support Python via UniFFInWe have decided to move forward with option number 3, \"Cargo features, one library\".nThis option, like the others mentioned, meets the key decision drivers.n* The SDK needs to support Python via UniFFIn* The SDK needs to support Python via UniFFIn* The SDK needs to support Python via UniFFIn* The SDK needs to support Python via UniFFInWe have decided to move forward with option number 4, \"Cargo features, one library\".nThis option, like the others mentioned, meets the key decision drivers.n* The SDK needs to support Python via UniFFIn* The SDK needs to support Python via UniFFIn* The SDK needs to support Python via UniFFIn* The SDK needs to support Python via UniFFIn* The SDK needs to support Python via UniFFIn"}
{"File Name":"cf-k8s-networking\/0008-implement-workarounds-for-capi-and-log-cache-to-unblock-global-strict-mtls.md","Context":"## Context\nWe need to turn on STRICT mTLS for all components on the mesh. However, some\ncomponents are currently incompatible with this mode.\nCAPI is incompatible because it uses an init container to run migrations. This\ninit container comes up before the sidecar, so it is unable to establish an mTLS\nconnection with the capi database. This causes the init container to fail and\nprevents capi from coming up. See [this\nissue](https:\/\/github.com\/cloudfoundry\/capi-k8s-release\/issues\/12) in capi.\nLog-cache is incompatible because it is configured to establish its own tls\nconnection, which is incompatible with the mTLS the sidecars are attempting to\nestablish.\n","Decision":"We have provided configuration workarounds in the form of Policies, that were\\nplaced in the cf-for-k8s repo to be owned by the respective teams that manage\\nthe troublesome components.\\n[Pull Request](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/pull\/35)\\n","tokens":153,"id":2249,"text":"## Context\\nWe need to turn on STRICT mTLS for all components on the mesh. However, some\\ncomponents are currently incompatible with this mode.\\nCAPI is incompatible because it uses an init container to run migrations. This\\ninit container comes up before the sidecar, so it is unable to establish an mTLS\\nconnection with the capi database. This causes the init container to fail and\\nprevents capi from coming up. See [this\\nissue](https:\/\/github.com\/cloudfoundry\/capi-k8s-release\/issues\/12) in capi.\\nLog-cache is incompatible because it is configured to establish its own tls\\nconnection, which is incompatible with the mTLS the sidecars are attempting to\\nestablish.\\n\n\n##Decision\nWe have provided configuration workarounds in the form of Policies, that were\\nplaced in the cf-for-k8s repo to be owned by the respective teams that manage\\nthe troublesome components.\\n[Pull Request](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/pull\/35)\\n","Predictions":"We have provided configuration workarounds in the form of Policies, that werenplaced in the cf-for-k8s repo to be owned by the respective teams that managenthe troublesome components.n[Pull Request](https:\/\/github.com\/cloudfoundry\/cf-for-k8s\/pull\/35)n"}
{"File Name":"dapr\/ENG-004-signing.md","Context":"## Context\nAuthenticode signing of binaries.\n","Decision":"* Binaries will not be signed with Microsoft keys. In future we can revisit to sign the binaries with dapr.io keys.\\n","tokens":11,"id":72,"text":"## Context\\nAuthenticode signing of binaries.\\n\n\n##Decision\n* Binaries will not be signed with Microsoft keys. In future we can revisit to sign the binaries with dapr.io keys.\\n","Predictions":"* schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that is used to store the data.nThe schema is a schemantype that"}
{"File Name":"register-a-food-business-front-end\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3041,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"link_platform\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5035,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"firefox-tv\/adr-0004-fftv-release-versioning.md","Context":"## Context\nWe build several different versions of Firefox for purposes such as release, beta testing, and staged rollout. It's not obvious how the version numbers are related to these types of builds, and features have sometimes disappeared in rollback versions, so we document our strategy here.\nFirefox TV releases are listed [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/releases](here). (These include LATs, which are not included in the changelog, and the changelog may include additional information, like reasons for re-releasing a version.)\nAs of the time of writing, the current release version is `3.9`.\n","Decision":"Firefox TV versioning is based off of [https:\/\/semver.org\/](semantic versioning) of MAJOR.MINOR.PATCH, but reflects features rather than API compatibility.\\nAdditionally, we also use alphanumeric suffixes to clearly differentiate between early test builds, releases, and re-releases.\\nEach release has a *tag* prefixed by `v`, such as `v3.8` and are listed in the [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/tags](Tags) page of the repo.\\n### Semantic Versioning\\n* MAJOR version changes signal significant changes to UI or functionality\\n* MINOR version changes are released every Sprint, unless they are skipped for release blockers\\n* PATCH version changes are for critical bug-fixes that cannot wait for the next Sprint.\\n* (LETTER-SUFFIX) reflects builds for our additional purposes that are detailed in following sections.\\n### Release\\nAs of 3.8, public releases have no suffix, and are released using the staged rollout capability of the Amazon Developer portal.\\n### Live App Testing (-LAT1)\\nAs part of our early testing, we create Live App Test (LAT) builds to send out candidate builds to our early testing groups before a release.\\nThese have a `-LAT1` suffix, where the number is incremented per test build sent out per version. For example, the second test build for 3.5 would be `3.5-LAT2`.\\nThis is first used in `3.3.0-LAT1`. These are used for testing, not general release.\\n#### Deprecated LAT versioning\\nPreviously, the versioning was much more confusing. We wanted to preserve monotonic order versioning, so a LAT would have an additional number appended at the end of the *previous* version; for example, the second LAT testing the 3.2 release would be versioned `3.1.3.2`, because the last released version before `3.2` was `3.1.3`.\\nThis deprecated LAT versioning was used between `2.1.0.1` and `3.1.3.2`.\\n### GeckoView (-GV)\\nCurrently, there are two distinct web engines that Firefox for Fire TV can be build with: the system WebView or GeckoView. Since a build currently can only use one of these, when we make a build that uses the GeckoView engine, we need a separate suffix to differentiate it.\\nThese GeckoView builds are suffixed with `-GV`.\\nThis is first used in `3.4-GV`, but is used for testing and not released to the general population.\\n### Re-Release (-A)\\nThere are two cases for re-release:\\n1) Rollback to a previous version due to critical bugs (e.g. rollback of 3.4 should be 3.3-A, although this is untested, and the platform may not allow decremented versioning, in which case, we would release the rollback as 3.4-A)\\n1) (deprecated) Release of a tested \"staged rollout\" build to the rest of the devices. (This is no longer used because staged rollout capability has been added to the app store.) This was monotonic because the \"general population\" devices had not been upgraded past this version.\\nThis is a build that has already been released before, either to a portion of the population, or because in a subsequent release we needed to do a version bump in order to push out a release, but did not change the code.\\nThese re-release builds are suffixed with a letter starting with `-A`, which is incremented with each re-release.\\nThis is first used in `3.4-A`.\\n#### Deprecated re-release versioning\\nBefore we started using letters to signify re-releases, we simply bumped the version number, so there are several versions that are simply re-releases of previous versions, but with different version numbers. These are listed below:\\n**3.1.3** is the same version as:\\n* 3.2.5\\n* 3.3\\n* 3.4-A\\n* 3.4-B\\n(and also listed in the changelog)\\n### Deprecated Split-Device Staged Rollout (-RO)\\nThese split-device staged rollout releases were suffixed with a `-RO`, e.g. `3.7-RO`.\\nBefore the staged rollout capability was added to the Amazon Developer portal, we handled staged rollout by releasing the newest version only to a single device, in this case the Firefox TV 4K Pendant (AFTN).\\nThis versioning scheme was used between `3.5-RO` and `3.7-RO`.\\n### Status: Accepted\\n","tokens":131,"id":4364,"text":"## Context\\nWe build several different versions of Firefox for purposes such as release, beta testing, and staged rollout. It's not obvious how the version numbers are related to these types of builds, and features have sometimes disappeared in rollback versions, so we document our strategy here.\\nFirefox TV releases are listed [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/releases](here). (These include LATs, which are not included in the changelog, and the changelog may include additional information, like reasons for re-releasing a version.)\\nAs of the time of writing, the current release version is `3.9`.\\n\n\n##Decision\nFirefox TV versioning is based off of [https:\/\/semver.org\/](semantic versioning) of MAJOR.MINOR.PATCH, but reflects features rather than API compatibility.\\nAdditionally, we also use alphanumeric suffixes to clearly differentiate between early test builds, releases, and re-releases.\\nEach release has a *tag* prefixed by `v`, such as `v3.8` and are listed in the [https:\/\/github.com\/mozilla-mobile\/firefox-tv\/tags](Tags) page of the repo.\\n### Semantic Versioning\\n* MAJOR version changes signal significant changes to UI or functionality\\n* MINOR version changes are released every Sprint, unless they are skipped for release blockers\\n* PATCH version changes are for critical bug-fixes that cannot wait for the next Sprint.\\n* (LETTER-SUFFIX) reflects builds for our additional purposes that are detailed in following sections.\\n### Release\\nAs of 3.8, public releases have no suffix, and are released using the staged rollout capability of the Amazon Developer portal.\\n### Live App Testing (-LAT1)\\nAs part of our early testing, we create Live App Test (LAT) builds to send out candidate builds to our early testing groups before a release.\\nThese have a `-LAT1` suffix, where the number is incremented per test build sent out per version. For example, the second test build for 3.5 would be `3.5-LAT2`.\\nThis is first used in `3.3.0-LAT1`. These are used for testing, not general release.\\n#### Deprecated LAT versioning\\nPreviously, the versioning was much more confusing. We wanted to preserve monotonic order versioning, so a LAT would have an additional number appended at the end of the *previous* version; for example, the second LAT testing the 3.2 release would be versioned `3.1.3.2`, because the last released version before `3.2` was `3.1.3`.\\nThis deprecated LAT versioning was used between `2.1.0.1` and `3.1.3.2`.\\n### GeckoView (-GV)\\nCurrently, there are two distinct web engines that Firefox for Fire TV can be build with: the system WebView or GeckoView. Since a build currently can only use one of these, when we make a build that uses the GeckoView engine, we need a separate suffix to differentiate it.\\nThese GeckoView builds are suffixed with `-GV`.\\nThis is first used in `3.4-GV`, but is used for testing and not released to the general population.\\n### Re-Release (-A)\\nThere are two cases for re-release:\\n1) Rollback to a previous version due to critical bugs (e.g. rollback of 3.4 should be 3.3-A, although this is untested, and the platform may not allow decremented versioning, in which case, we would release the rollback as 3.4-A)\\n1) (deprecated) Release of a tested \"staged rollout\" build to the rest of the devices. (This is no longer used because staged rollout capability has been added to the app store.) This was monotonic because the \"general population\" devices had not been upgraded past this version.\\nThis is a build that has already been released before, either to a portion of the population, or because in a subsequent release we needed to do a version bump in order to push out a release, but did not change the code.\\nThese re-release builds are suffixed with a letter starting with `-A`, which is incremented with each re-release.\\nThis is first used in `3.4-A`.\\n#### Deprecated re-release versioning\\nBefore we started using letters to signify re-releases, we simply bumped the version number, so there are several versions that are simply re-releases of previous versions, but with different version numbers. These are listed below:\\n**3.1.3** is the same version as:\\n* 3.2.5\\n* 3.3\\n* 3.4-A\\n* 3.4-B\\n(and also listed in the changelog)\\n### Deprecated Split-Device Staged Rollout (-RO)\\nThese split-device staged rollout releases were suffixed with a `-RO`, e.g. `3.7-RO`.\\nBefore the staged rollout capability was added to the Amazon Developer portal, we handled staged rollout by releasing the newest version only to a single device, in this case the Firefox TV 4K Pendant (AFTN).\\nThis versioning scheme was used between `3.5-RO` and `3.7-RO`.\\n### Status: Accepted\\n","Predictions":"We will use [Semantic Versioning](https:\/\/semanticversioning.org\/) to indicate that we are releasing a version that is not compatible with the API.nWe will use [LATs](Tags) to indicate that we are releasing a version that is not compatible with the API.n"}
{"File Name":"rtk-companion\/0003-mvp-functionality-in-add-card-window.md","Context":"## Context and Problem Statement\nAfter experimenting with the previous MVP goal defined by [ADR-0002](0002-define-goal-for-an-mvp.md), I've decided it's not a good approach.\nMy main issue with is that in a way, it is re-creating Anki's \"Add Card\" screen, albeit a custom one which adds RTK cards.\nHere's a screenshot of the UI created and filled with placeholder text.\n![](..\/assets\/rtk-browser-screenshot.png)\nWhile I still think it'd be cool to have an RTK browser in Anki and working with QT is ....interesting, I thought\na simpler approach should be sought.\nSo the problem stands, what should be done as an MVP and what should it contain?\n","Decision":"After some experimenting and research, I've decided an MVP should augment the `Add Card`.\\nHere's the vision of how a user interacts with the MVP.\\n1. User opens the `Add Card` window which has a \"RTK\" button, and a \"Stories\" button.\\n1. User types in their desired Heisig Kanji number is the \"Number\" field.\\n1. User clicks the RTK button and the character, and stroke order get auto filled.\\n1. User clicks \"Stories\" button, which opens an embedded browser window to user stories.\\n### Positive Consequences <!-- optional -->\\n- The approach is simpler.\\n- We keep Building UI elements with `QT` to a minimum under this approach.\\n### Negative Consequences <!-- optional -->\\n- We leave out Additional keyword meanings and the radicals for the Kanji\\n","tokens":161,"id":4402,"text":"## Context and Problem Statement\\nAfter experimenting with the previous MVP goal defined by [ADR-0002](0002-define-goal-for-an-mvp.md), I've decided it's not a good approach.\\nMy main issue with is that in a way, it is re-creating Anki's \"Add Card\" screen, albeit a custom one which adds RTK cards.\\nHere's a screenshot of the UI created and filled with placeholder text.\\n![](..\/assets\/rtk-browser-screenshot.png)\\nWhile I still think it'd be cool to have an RTK browser in Anki and working with QT is ....interesting, I thought\\na simpler approach should be sought.\\nSo the problem stands, what should be done as an MVP and what should it contain?\\n\n\n##Decision\nAfter some experimenting and research, I've decided an MVP should augment the `Add Card`.\\nHere's the vision of how a user interacts with the MVP.\\n1. User opens the `Add Card` window which has a \"RTK\" button, and a \"Stories\" button.\\n1. User types in their desired Heisig Kanji number is the \"Number\" field.\\n1. User clicks the RTK button and the character, and stroke order get auto filled.\\n1. User clicks \"Stories\" button, which opens an embedded browser window to user stories.\\n### Positive Consequences <!-- optional -->\\n- The approach is simpler.\\n- We keep Building UI elements with `QT` to a minimum under this approach.\\n### Negative Consequences <!-- optional -->\\n- We leave out Additional keyword meanings and the radicals for the Kanji\\n","Predictions":"We will use the [Microsoft GraphQL](https:\/\/www.microsoft.com\/) approach to generate GraphQL models.n"}
{"File Name":"macondo\/0007-automatic-aligning-of-user-host-user-for-better-interoperability.md","Context":"## Context\nA well constructed macondo command should be easy to run directly (without\nmacondo) by just executing it. This is hard to achieve unless the user in the\ncontainer resembles as much as possible the host running it.\nOne case where this is important is modifying files in the host. This is\nachieved by mounting a volume into the running container. However, the files\nwritten from the docker container are owned by the docker user, i.e. the user id\nand group id of the file is that of whatever docker user happened to write it.\nSo if the container runs with the root user, which is unfortunately common, then\nthe files written to the host also are owned by root, making them innacessible\nto the host user.\nPotential ideas:\n- A flag that enables user alignment and mounts HOME into docker's HOME.\n- Customize home when running in OSX to be \/Users\/bla instead of \/home\/ble\n- Even if mounting the whole home, it should be easy to mount current PWD into\nsomething else \/mnt\/blablabla and use tha as working directory\n","Decision":"Add a mechanism to align the user\/group of the host system with that of the docker container.\\n","tokens":237,"id":2193,"text":"## Context\\nA well constructed macondo command should be easy to run directly (without\\nmacondo) by just executing it. This is hard to achieve unless the user in the\\ncontainer resembles as much as possible the host running it.\\nOne case where this is important is modifying files in the host. This is\\nachieved by mounting a volume into the running container. However, the files\\nwritten from the docker container are owned by the docker user, i.e. the user id\\nand group id of the file is that of whatever docker user happened to write it.\\nSo if the container runs with the root user, which is unfortunately common, then\\nthe files written to the host also are owned by root, making them innacessible\\nto the host user.\\nPotential ideas:\\n- A flag that enables user alignment and mounts HOME into docker's HOME.\\n- Customize home when running in OSX to be \/Users\/bla instead of \/home\/ble\\n- Even if mounting the whole home, it should be easy to mount current PWD into\\nsomething else \/mnt\/blablabla and use tha as working directory\\n\n\n##Decision\nAdd a mechanism to align the user\/group of the host system with that of the docker container.\\n","Predictions":"The config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a config file that contains the config file.nThe config file should be a "}
{"File Name":"scholarsphere\/0008-asset-pipeline.md","Context":"## Context\nWe were unable to address a security vulnerability in jQuery because we couldn't update Bootstrap. This was due to the\nfact that it was present as both a gem and an npm package.\n","Decision":"We removed asset pipeline completely and moved all css and image assets to webpacker. This allowed us to update jQuery\\nvia yarn.\\n","tokens":41,"id":4557,"text":"## Context\\nWe were unable to address a security vulnerability in jQuery because we couldn't update Bootstrap. This was due to the\\nfact that it was present as both a gem and an npm package.\\n\n\n##Decision\nWe removed asset pipeline completely and moved all css and image assets to webpacker. This allowed us to update jQuery\\nvia yarn.\\n","Predictions":"We will use a Security identifier to authenticate users.n"}
{"File Name":"operational-data-hub\/0057-naming-convention-defines-uri-tags.md","Context":"## Context\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme\n","Decision":"We will use [the 'tag' URI Scheme](https:\/\/tools.ietf.org\/html\/rfc4151).\\n","tokens":55,"id":2763,"text":"## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme\\n\n\n##Decision\nWe will use [the 'tag' URI Scheme](https:\/\/tools.ietf.org\/html\/rfc4151).\\n","Predictions":"We will use [the 'tag' URI Scheme](https:\/\/tools.ietf.org\/html\/rfc4151).n"}
{"File Name":"architecture\/0013-home-assistant-container.md","Context":"## Context\nDefine a supported installation method as per [ADR-0012](https:\/\/github.com\/home-assistant\/architecture\/blob\/master\/adr\/0012-define-supported-installation-method.md).\n","Decision":"This is for running just the Home Assistant Core application on native OCI compatible containerization system. It does not provide the Supervisor experience, and thus does not provide the Supervisor panel and add-ons.\\nThis is a general installation method that is recommended as an alternative to the Home Assistant OS installation method. Due to the shared image with the Home Assistant OS installation method, almost all documentation applies to the Home Assistant Container as well.\\nThe only supported way to run the container is on the host network as root with full privileges.\\n### Supported Containerization system and version\\n- Any Open Container Initiative (OCI) compatible containerization system.\\n### Supported boards\/hardware\/machines\\n- Machines of the following architectures: amd64, i386, armhf, aarch64, armv7\\n### Supported Operating Systems and versions\\n- Running Home Assistant Container is only supported on Linux.\\n- Windows and BSD installations (e.g., macOS and FreeBSD) are not supported.\\n### Additional notes\\nThere is a wide variety of containerization software available. From that perspective, Home Assistant will only actively document the use of Docker.\\n### Required Expertise\\n- **Installation**\\nThis requires the user to have an existing system that can run Docker containers. Installation is either done by running a command from the Docker-cli or via a user interface (Synology, Portainer)\\n* **Start when the system is started:** The user is responsible for configuring the system to start the container when the system is started.\\n* **Run with full network access:** Default installation instructions prescribe net=host to be configured.\\n* **Access USB devices:** It is up to the user to ensure that all devices are correctly passed through to the container.\\n* **Maintaining the Home Assistant installation**\\nIf using the Docker-cli the user needs to manually update the run command. If using a UI the user might be notified of an upgrade or automatically update \u2013 automatically applying updates may result in the system not coming back online. There is no rollback in case the instance does not come online after an update.\\n- **Python upgrades:** Included in the Home Assistant container\\n- **Installing Python dependencies:** Included in the Home Assistant container\\n- **Updating Home Assistant:** Included in the Home Assistant container\\n- **Maintaining the Operating System**\\nSince this is just the core container, all OS responsibilities are with the user.\\n- **Security updates for OS:** Responsibility of the user.\\n- **Maintaining the components required for the Supervisor:** No supervisor, so N\/A\\n**Conclusion:** medium expertise required. Some Docker UIs make it easy to run and update containers. Mapping devices and manually updating Home Assistant will be challenging as they depend per platform.\\n","tokens":41,"id":1430,"text":"## Context\\nDefine a supported installation method as per [ADR-0012](https:\/\/github.com\/home-assistant\/architecture\/blob\/master\/adr\/0012-define-supported-installation-method.md).\\n\n\n##Decision\nThis is for running just the Home Assistant Core application on native OCI compatible containerization system. It does not provide the Supervisor experience, and thus does not provide the Supervisor panel and add-ons.\\nThis is a general installation method that is recommended as an alternative to the Home Assistant OS installation method. Due to the shared image with the Home Assistant OS installation method, almost all documentation applies to the Home Assistant Container as well.\\nThe only supported way to run the container is on the host network as root with full privileges.\\n### Supported Containerization system and version\\n- Any Open Container Initiative (OCI) compatible containerization system.\\n### Supported boards\/hardware\/machines\\n- Machines of the following architectures: amd64, i386, armhf, aarch64, armv7\\n### Supported Operating Systems and versions\\n- Running Home Assistant Container is only supported on Linux.\\n- Windows and BSD installations (e.g., macOS and FreeBSD) are not supported.\\n### Additional notes\\nThere is a wide variety of containerization software available. From that perspective, Home Assistant will only actively document the use of Docker.\\n### Required Expertise\\n- **Installation**\\nThis requires the user to have an existing system that can run Docker containers. Installation is either done by running a command from the Docker-cli or via a user interface (Synology, Portainer)\\n* **Start when the system is started:** The user is responsible for configuring the system to start the container when the system is started.\\n* **Run with full network access:** Default installation instructions prescribe net=host to be configured.\\n* **Access USB devices:** It is up to the user to ensure that all devices are correctly passed through to the container.\\n* **Maintaining the Home Assistant installation**\\nIf using the Docker-cli the user needs to manually update the run command. If using a UI the user might be notified of an upgrade or automatically update \u2013 automatically applying updates may result in the system not coming back online. There is no rollback in case the instance does not come online after an update.\\n- **Python upgrades:** Included in the Home Assistant container\\n- **Installing Python dependencies:** Included in the Home Assistant container\\n- **Updating Home Assistant:** Included in the Home Assistant container\\n- **Maintaining the Operating System**\\nSince this is just the core container, all OS responsibilities are with the user.\\n- **Security updates for OS:** Responsibility of the user.\\n- **Maintaining the components required for the Supervisor:** No supervisor, so N\/A\\n**Conclusion:** medium expertise required. Some Docker UIs make it easy to run and update containers. Mapping devices and manually updating Home Assistant will be challenging as they depend per platform.\\n","Predictions":"This is for running just the Home Assistant Core application on native OCI compatible containerization system. It does not provide the Supervisor experience, and thus does not provide the Supervisor panel and add-ons.nThis is a general installation method that is recommended as an alternative to the Home Assistant OS installation method. Due to the shared image with the Home Assistant OS installation method, almost all documentation applies to the Home Assistant Container as well.nThe only supported way to run the container is on the host network as root with full privileges.n### Supported Containerization system and versionn- Any Open Container Initiative (OCI) compatible containerization system.n### Supported boards\/hardware\/machinesn- Machines of the following architectures: amd64, i386, armhf, aarch64, armv7n### Supported Operating Systems and versionsn- Running Home Assistant Container is only supported on Linux.n- Windows and BSD installations (e.g., macOS and FreeBSD) are not supported.n### Additional notesnThere is a wide variety of containerization software available. From that perspective, Home Assistant will only actively document the use of Docker.n### Required Expertisen- **Installation**nThis requires the user to have an existing system that can run Docker containers. Installation is either done by running a command from the Docker-cli or via a user interface (Synology, Portainer)n* **Start when the system is started:** The user is responsible for configuring the system to start the container when the system is started.n* **Run with full network access:** Default installation instructions prescribe net=host to be configured.n* **Access USB devices:** It is up to the user to ensure that all devices are correctly passed through to the container.n* **Maintaining the Home Assistant installation**nIf using the Docker-cli the user must ensure that all devices are correctly passed through to the container.n"}
{"File Name":"tech-events-calendar\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3753,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"digitalrig-metal-aws\/0003-use-aws-codepipeline-and-codebuild-instead-of-travis.md","Context":"## Context\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted\/PaaS CI\/CD solution\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Travis\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":36,"id":1741,"text":"## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted\/PaaS CI\/CD solution\\n\n\n##Decision\n* Use AWS CodePipeline and CodeBuild instead of Travis\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","Predictions":"Use Rollbar to collect and manage our application errors.n"}
{"File Name":"biosamples-v4\/0004-inverse-relationships-sample-update.md","Context":"## Context\nWhen a relation is created between two samples, the sample target of the relationship doesn't change the update date.\nStated in another way, the relationship inversion process doesn't change the update date of the sample.\nThis is associated with BSD-1088 - https:\/\/www.ebi.ac.uk\/panda\/jira\/browse\/BSD-1088\n","Decision":"We decided that is good for the relationship inversion process to not change the update date of the sample as we don't\\nhave any actual usecase for this to happen and it would also create issues for NCBI as they don't care about relationships\\nand no real information is added to the sample\\n","tokens":74,"id":4726,"text":"## Context\\nWhen a relation is created between two samples, the sample target of the relationship doesn't change the update date.\\nStated in another way, the relationship inversion process doesn't change the update date of the sample.\\nThis is associated with BSD-1088 - https:\/\/www.ebi.ac.uk\/panda\/jira\/browse\/BSD-1088\\n\n\n##Decision\nWe decided that is good for the relationship inversion process to not change the update date of the sample as we don't\\nhave any actual usecase for this to happen and it would also create issues for NCBI as they don't care about relationships\\nand no real information is added to the sample\\n","Predictions":"We decided that is good for the relationship inversion process to not change the update date of the sample as we don'tnhave any actual usecase for this to happen and it would also create issues for NCBI as they don't care about relationshipsnand no real information is added to the samplen"}
{"File Name":"octagon\/0002-components-shall-be-stateless-by-default.md","Context":"## Context\nOctagon strives to be a view layer for consumers.  Applications manage their state via their own independent mechanisms.  State embedded within Octagon components may reduce the declaritive ability of said components & violate the top-down, _\"config via props\"_ React mantra.  Consequently, stateful components generally deviate from the pure view layer goals we wish to provide.  Embedded state reduces the ablitity for apps to \"declare\" Octagon components into view state.\nExample, consider if a `<OctoNavItem \/>` maintains state on whether or not that navigation page is active or not.  If a user reloaded the page, an active item may suddenly show as not active, because that component's state is lost on reload.\nInstead, the API should be stateless, and be used like: `<OctoNavItem active \/>`.\nNow, the user of the view layer is in control.\n","Decision":"Make things stateless.  In rare cases, some emphemeral state is OK to maintain internally to components.  These should be highlighted, drawn out, and discussed before merge.  Ideally, these issues should be raised during design, pre-merge if a feature branch is longer running.\\n","tokens":189,"id":4596,"text":"## Context\\nOctagon strives to be a view layer for consumers.  Applications manage their state via their own independent mechanisms.  State embedded within Octagon components may reduce the declaritive ability of said components & violate the top-down, _\"config via props\"_ React mantra.  Consequently, stateful components generally deviate from the pure view layer goals we wish to provide.  Embedded state reduces the ablitity for apps to \"declare\" Octagon components into view state.\\nExample, consider if a `<OctoNavItem \/>` maintains state on whether or not that navigation page is active or not.  If a user reloaded the page, an active item may suddenly show as not active, because that component's state is lost on reload.\\nInstead, the API should be stateless, and be used like: `<OctoNavItem active \/>`.\\nNow, the user of the view layer is in control.\\n\n\n##Decision\nMake things stateless.  In rare cases, some emphemeral state is OK to maintain internally to components.  These should be highlighted, drawn out, and discussed before merge.  Ideally, these issues should be raised during design, pre-merge if a feature branch is longer running.\\n","Predictions":"We will use stateless components. In rare cases, some emphemeral state is OK to maintain internally to components. These should be highlighted, drawn out, and discussed before merge. Ideally, these issues should be raised during design, pre-merge if a feature branch is longer running.n"}
{"File Name":"infection\/0003-PHPUnit-this-over-self.md","Context":"### Context\nPHPUnit assertions are static methods, yet in our code base we call them with `$this` instead of\n`self`.\nWhilst \"incorrect\", this usage does not break anything. Besides:\n- [PHUnit documentation][phpunit-doc] itself uses this by default\n- `$this` is much more widely used than `self` in this context in the community\n- all Infection code uses `$this`\nThere is not much shortcomings from using this other than the \"incorrectness\" of using a static\nmethod as a non-static one.\n### Decision\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\nthe decision is to keep the usage of `$this` over `self` in the codebase.\n### Status\nAccepted ([#1061][1061])\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\n","Decision":"Since there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","tokens":221,"id":66,"text":"### Context\\nPHPUnit assertions are static methods, yet in our code base we call them with `$this` instead of\\n`self`.\\nWhilst \"incorrect\", this usage does not break anything. Besides:\\n- [PHUnit documentation][phpunit-doc] itself uses this by default\\n- `$this` is much more widely used than `self` in this context in the community\\n- all Infection code uses `$this`\\nThere is not much shortcomings from using this other than the \"incorrectness\" of using a static\\nmethod as a non-static one.\\n### Decision\\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n\n\n##Decision\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https:\/\/phpunit.de\/manual\/6.5\/en\/appendixes.assertions.html\\n[1061]: https:\/\/github.com\/infection\/infection\/pull\/1061\\n","Predictions":"* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a self method that is used by all Infection coden* self is a "}
{"File Name":"arch\/0019-server-request-and-upgrade-capacity-evaluation.md","Context":"## Context\n1. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\uff0c\u4f7f\u7528\u7387\u5747\u5728 90% \u5de6\u53f3\uff0c\u53e6\u4e00\u4e9b\u673a\u5668\u5404\u9879\u6307\u6807\u4f7f\u7528\u7387\u5728 1% \u5de6\u53f3\uff1b\n2. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\u642d\u914d\u4e0d\u5408\u7406\uff0cCPU \u4f7f\u7528\u7387 1%\uff0c\u4f46\u5185\u5b58\u4f7f\u7528\u7387\u5728 90% \u5de6\u53f3\uff1b\n3. \u4e00\u4e9b\u5bf9\u78c1\u76d8\u8bfb\u5199\u8981\u6c42\u9ad8\u7684\u670d\u52a1\uff0c\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0c\u6bd4\u5982\uff0c\u6570\u636e\u5e93\uff0cSVN\u7b49\uff1b\n4. \u7533\u8bf7\u673a\u5668\u65f6\uff0c\u65e0\u6cd5\u63d0\u51fa\u914d\u7f6e\u8981\u6c42\uff0c\u57fa\u672c\u9760\u62cd\u8111\u95e8\u51b3\u5b9a\uff1b\n5. \u5bf9\u670d\u52a1\u7684\u53d1\u5c55\u6ca1\u6709\u601d\u8003\uff0c\u914d\u7f6e\u8981\u4e86 12 \u4e2a\u6708\u540e\u624d\u80fd\u4f7f\u7528\u5230\u7684\u914d\u7f6e\u3002\n","Decision":"1. \u538b\u529b\u6d4b\u8bd5\uff1b\\n2. \u5206\u6790\u4e1a\u52a1\u5404\u4e2a\u6307\u6807\u7684\u4f7f\u7528\u60c5\u51b5\uff0cCPU \u5bc6\u96c6\u578b\uff0c\u5185\u5b58\u5bc6\u96c6\u578b\u8fd8\u662f\u6709\u5176\u4ed6\u7684\u7279\u70b9\uff1b\\n3. \u9274\u4e8e Aliyun ECS \u968f\u65f6\u53ef\u4ee5\u6269\u5c55\uff0c\u53ef\u4ee5\u5148\u7528\u4f4e\u914d\u673a\u5668\uff0c\u6839\u636e\u4f7f\u7528\u60c5\u51b5\uff0c\u8fdb\u884c\u5355\u6307\u6807\u5782\u76f4\u5347\u7ea7\uff1b\\n4. \u6c34\u5e73\u6269\u5c55\uff0c\u5373\u63d0\u5347\u4e86\u670d\u52a1\u7684\u5904\u7406\u80fd\u529b\u53c8\u505a\u4e86\u9ad8\u53ef\u7528\uff1b\\n5. \u5bf9\u4e8e\u4e0d\u5408\u7406\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u8981\u5206\u6790\u81ea\u5df1\u7a0b\u5e8f\u4e2d\u662f\u5426\u6709\u5185\u5b58\u6cc4\u6f0f\u6216\u5927\u6570\u636e\u52a0\u8f7d\u3002\\n","tokens":208,"id":2440,"text":"## Context\\n1. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\uff0c\u4f7f\u7528\u7387\u5747\u5728 90% \u5de6\u53f3\uff0c\u53e6\u4e00\u4e9b\u673a\u5668\u5404\u9879\u6307\u6807\u4f7f\u7528\u7387\u5728 1% \u5de6\u53f3\uff1b\\n2. \u90e8\u5206\u673a\u5668\u7684 CPU\uff0c\u5185\u5b58\uff0c\u786c\u76d8\u642d\u914d\u4e0d\u5408\u7406\uff0cCPU \u4f7f\u7528\u7387 1%\uff0c\u4f46\u5185\u5b58\u4f7f\u7528\u7387\u5728 90% \u5de6\u53f3\uff1b\\n3. \u4e00\u4e9b\u5bf9\u78c1\u76d8\u8bfb\u5199\u8981\u6c42\u9ad8\u7684\u670d\u52a1\uff0c\u4f7f\u7528\u7684\u662f\u666e\u901a\u4e91\u76d8\uff0c\u6bd4\u5982\uff0c\u6570\u636e\u5e93\uff0cSVN\u7b49\uff1b\\n4. \u7533\u8bf7\u673a\u5668\u65f6\uff0c\u65e0\u6cd5\u63d0\u51fa\u914d\u7f6e\u8981\u6c42\uff0c\u57fa\u672c\u9760\u62cd\u8111\u95e8\u51b3\u5b9a\uff1b\\n5. \u5bf9\u670d\u52a1\u7684\u53d1\u5c55\u6ca1\u6709\u601d\u8003\uff0c\u914d\u7f6e\u8981\u4e86 12 \u4e2a\u6708\u540e\u624d\u80fd\u4f7f\u7528\u5230\u7684\u914d\u7f6e\u3002\\n\n\n##Decision\n1. \u538b\u529b\u6d4b\u8bd5\uff1b\\n2. \u5206\u6790\u4e1a\u52a1\u5404\u4e2a\u6307\u6807\u7684\u4f7f\u7528\u60c5\u51b5\uff0cCPU \u5bc6\u96c6\u578b\uff0c\u5185\u5b58\u5bc6\u96c6\u578b\u8fd8\u662f\u6709\u5176\u4ed6\u7684\u7279\u70b9\uff1b\\n3. \u9274\u4e8e Aliyun ECS \u968f\u65f6\u53ef\u4ee5\u6269\u5c55\uff0c\u53ef\u4ee5\u5148\u7528\u4f4e\u914d\u673a\u5668\uff0c\u6839\u636e\u4f7f\u7528\u60c5\u51b5\uff0c\u8fdb\u884c\u5355\u6307\u6807\u5782\u76f4\u5347\u7ea7\uff1b\\n4. \u6c34\u5e73\u6269\u5c55\uff0c\u5373\u63d0\u5347\u4e86\u670d\u52a1\u7684\u5904\u7406\u80fd\u529b\u53c8\u505a\u4e86\u9ad8\u53ef\u7528\uff1b\\n5. \u5bf9\u4e8e\u4e0d\u5408\u7406\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u8981\u5206\u6790\u81ea\u5df1\u7a0b\u5e8f\u4e2d\u662f\u5426\u6709\u5185\u5b58\u6cc4\u6f0f\u6216\u5927\u6570\u636e\u52a0\u8f7d\u3002\\n","Predictions":"1.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}
{"File Name":"celestia-core\/adr-024-sign-bytes.md","Context":"## Context\nCurrently, the messages exchanged between tendermint and a (potentially remote) signer\/validator,\nnamely votes, proposals, and heartbeats, are encoded as a JSON string\n(e.g., via `Vote.SignBytes(...)`) and then\nsigned . JSON encoding is sub-optimal for both, hardware wallets\nand for usage in ethereum smart contracts. Both is laid down in detail in [issue#1622].\nAlso, there are currently no differences between sign-request and -replies. Also, there is no possibility\nfor a remote signer to include an error code or message in case something went wrong.\nThe messages exchanged between tendermint and a remote signer currently live in\n[privval\/socket.go] and encapsulate the corresponding types in [types].\n[privval\/socket.go]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/privval\/socket.go#L496-L502\n[issue#1622]: https:\/\/github.com\/tendermint\/tendermint\/issues\/1622\n[types]: https:\/\/github.com\/tendermint\/tendermint\/tree\/master\/types\n","Decision":"- restructure vote, proposal, and heartbeat such that their encoding is easily parseable by\\nhardware devices and smart contracts using a  binary encoding format ([amino] in this case)\\n- split up the messages exchanged between tendermint and remote signers into requests and\\nresponses (see details below)\\n- include an error type in responses\\n### Overview\\n```\\n+--------------+                      +----------------+\\n|              |     SignXRequest     |                |\\n|Remote signer |<---------------------+  tendermint    |\\n| (e.g. KMS)   |                      |                |\\n|              +--------------------->|                |\\n+--------------+    SignedXReply      +----------------+\\nSignXRequest {\\nx: X\\n}\\nSignedXReply {\\nx: X\\nsig: Signature \/\/ []byte\\nerr: Error{\\ncode: int\\ndesc: string\\n}\\n}\\n```\\nTODO: Alternatively, the type `X` might directly include the signature. A lot of places expect a vote with a\\nsignature and do not necessarily deal with \"Replies\".\\nStill exploring what would work best here.\\nThis would look like (exemplified using X = Vote):\\n```\\nVote {\\n\/\/ all fields besides signature\\n}\\nSignedVote {\\nVote Vote\\nSignature []byte\\n}\\nSignVoteRequest {\\nVote Vote\\n}\\nSignedVoteReply {\\nVote SignedVote\\nErr  Error\\n}\\n```\\n**Note:** There was a related discussion around including a fingerprint of, or, the whole public-key\\ninto each sign-request to tell the signer which corresponding private-key to\\nuse to sign the message. This is particularly relevant in the context of the KMS\\nbut is currently not considered in this ADR.\\n[amino]: https:\/\/github.com\/tendermint\/go-amino\/\\n### Vote\\nAs explained in [issue#1622] `Vote` will be changed to contain the following fields\\n(notation in protobuf-like syntax for easy readability):\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Vote {\\nVersion       fixed32\\nHeight        sfixed64\\nRound         sfixed32\\nVoteType      fixed32\\nTimestamp     Timestamp         \/\/ << using protobuf definition\\nBlockID       BlockID           \/\/ << as already defined\\nChainID       string            \/\/ at the end because length could vary a lot\\n}\\n\/\/ this is an amino registered type; like currently privval.SignVoteMsg:\\n\/\/ registered with \"tendermint\/socketpv\/SignVoteRequest\"\\nmessage SignVoteRequest {\\nVote vote\\n}\\n\/\/  amino registered type\\n\/\/ registered with \"tendermint\/socketpv\/SignedVoteReply\"\\nmessage SignedVoteReply {\\nVote      Vote\\nSignature Signature\\nErr       Error\\n}\\n\/\/ we will use this type everywhere below\\nmessage Error {\\nType        uint  \/\/ error code\\nDescription string  \/\/ optional description\\n}\\n```\\nThe `ChainID` gets moved into the vote message directly. Previously, it was injected\\nusing the [Signable] interface method `SignBytes(chainID string) []byte`. Also, the\\nsignature won't be included directly, only in the corresponding `SignedVoteReply` message.\\n[Signable]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/types\/signable.go#L9-L11\\n### Proposal\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Proposal {\\nHeight            sfixed64\\nRound             sfixed32\\nTimestamp         Timestamp         \/\/ << using protobuf definition\\nBlockPartsHeader  PartSetHeader     \/\/ as already defined\\nPOLRound          sfixed32\\nPOLBlockID        BlockID           \/\/ << as already defined\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalRequest\"\\nmessage SignProposalRequest {\\nProposal proposal\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalReply\"\\nmessage SignProposalReply {\\nProp   Proposal\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n### Heartbeat\\n**TODO**: clarify if heartbeat also needs a fixed offset and update the fields accordingly:\\n```proto\\nmessage Heartbeat {\\nValidatorAddress Address\\nValidatorIndex   int\\nHeight           int64\\nRound            int\\nSequence         int\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatRequest\"\\nmessage SignHeartbeatRequest {\\nHb Heartbeat\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatReply\"\\nmessage SignHeartbeatReply {\\nHb     Heartbeat\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n","tokens":249,"id":4097,"text":"## Context\\nCurrently, the messages exchanged between tendermint and a (potentially remote) signer\/validator,\\nnamely votes, proposals, and heartbeats, are encoded as a JSON string\\n(e.g., via `Vote.SignBytes(...)`) and then\\nsigned . JSON encoding is sub-optimal for both, hardware wallets\\nand for usage in ethereum smart contracts. Both is laid down in detail in [issue#1622].\\nAlso, there are currently no differences between sign-request and -replies. Also, there is no possibility\\nfor a remote signer to include an error code or message in case something went wrong.\\nThe messages exchanged between tendermint and a remote signer currently live in\\n[privval\/socket.go] and encapsulate the corresponding types in [types].\\n[privval\/socket.go]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/privval\/socket.go#L496-L502\\n[issue#1622]: https:\/\/github.com\/tendermint\/tendermint\/issues\/1622\\n[types]: https:\/\/github.com\/tendermint\/tendermint\/tree\/master\/types\\n\n\n##Decision\n- restructure vote, proposal, and heartbeat such that their encoding is easily parseable by\\nhardware devices and smart contracts using a  binary encoding format ([amino] in this case)\\n- split up the messages exchanged between tendermint and remote signers into requests and\\nresponses (see details below)\\n- include an error type in responses\\n### Overview\\n```\\n+--------------+                      +----------------+\\n|              |     SignXRequest     |                |\\n|Remote signer |<---------------------+  tendermint    |\\n| (e.g. KMS)   |                      |                |\\n|              +--------------------->|                |\\n+--------------+    SignedXReply      +----------------+\\nSignXRequest {\\nx: X\\n}\\nSignedXReply {\\nx: X\\nsig: Signature \/\/ []byte\\nerr: Error{\\ncode: int\\ndesc: string\\n}\\n}\\n```\\nTODO: Alternatively, the type `X` might directly include the signature. A lot of places expect a vote with a\\nsignature and do not necessarily deal with \"Replies\".\\nStill exploring what would work best here.\\nThis would look like (exemplified using X = Vote):\\n```\\nVote {\\n\/\/ all fields besides signature\\n}\\nSignedVote {\\nVote Vote\\nSignature []byte\\n}\\nSignVoteRequest {\\nVote Vote\\n}\\nSignedVoteReply {\\nVote SignedVote\\nErr  Error\\n}\\n```\\n**Note:** There was a related discussion around including a fingerprint of, or, the whole public-key\\ninto each sign-request to tell the signer which corresponding private-key to\\nuse to sign the message. This is particularly relevant in the context of the KMS\\nbut is currently not considered in this ADR.\\n[amino]: https:\/\/github.com\/tendermint\/go-amino\/\\n### Vote\\nAs explained in [issue#1622] `Vote` will be changed to contain the following fields\\n(notation in protobuf-like syntax for easy readability):\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Vote {\\nVersion       fixed32\\nHeight        sfixed64\\nRound         sfixed32\\nVoteType      fixed32\\nTimestamp     Timestamp         \/\/ << using protobuf definition\\nBlockID       BlockID           \/\/ << as already defined\\nChainID       string            \/\/ at the end because length could vary a lot\\n}\\n\/\/ this is an amino registered type; like currently privval.SignVoteMsg:\\n\/\/ registered with \"tendermint\/socketpv\/SignVoteRequest\"\\nmessage SignVoteRequest {\\nVote vote\\n}\\n\/\/  amino registered type\\n\/\/ registered with \"tendermint\/socketpv\/SignedVoteReply\"\\nmessage SignedVoteReply {\\nVote      Vote\\nSignature Signature\\nErr       Error\\n}\\n\/\/ we will use this type everywhere below\\nmessage Error {\\nType        uint  \/\/ error code\\nDescription string  \/\/ optional description\\n}\\n```\\nThe `ChainID` gets moved into the vote message directly. Previously, it was injected\\nusing the [Signable] interface method `SignBytes(chainID string) []byte`. Also, the\\nsignature won't be included directly, only in the corresponding `SignedVoteReply` message.\\n[Signable]: https:\/\/github.com\/tendermint\/tendermint\/blob\/d419fffe18531317c28c29a292ad7d253f6cafdf\/types\/signable.go#L9-L11\\n### Proposal\\n```proto\\n\/\/ vanilla protobuf \/ amino encoded\\nmessage Proposal {\\nHeight            sfixed64\\nRound             sfixed32\\nTimestamp         Timestamp         \/\/ << using protobuf definition\\nBlockPartsHeader  PartSetHeader     \/\/ as already defined\\nPOLRound          sfixed32\\nPOLBlockID        BlockID           \/\/ << as already defined\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalRequest\"\\nmessage SignProposalRequest {\\nProposal proposal\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignProposalReply\"\\nmessage SignProposalReply {\\nProp   Proposal\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n### Heartbeat\\n**TODO**: clarify if heartbeat also needs a fixed offset and update the fields accordingly:\\n```proto\\nmessage Heartbeat {\\nValidatorAddress Address\\nValidatorIndex   int\\nHeight           int64\\nRound            int\\nSequence         int\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatRequest\"\\nmessage SignHeartbeatRequest {\\nHb Heartbeat\\n}\\n\/\/ amino registered with \"tendermint\/socketpv\/SignHeartbeatReply\"\\nmessage SignHeartbeatReply {\\nHb     Heartbeat\\nSig    Signature\\nErr    Error     \/\/ as defined above\\n}\\n```\\n","Predictions":"- restructure vote, proposal, and heartbeat such that their encoding is easily parseable bynhardware devices and smart contracts using a binary encoding format ([amino] in this case)n- split up the messages exchanged between tendermint and remote signers into requests andnresponses (see details below)n- include an error type in responsesn### Overviewnn+-------------+ +-------------------+n| | SignXRequest | |n| Remote signer |---------n| | SignXRequest | |n| SignXRequest |n------------------n| | SignXRequest | |n| Remote signer |---------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n| Remote signer |------------n| | SignXRequest | |n|"}
{"File Name":"cukeness\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3316,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"tdr-dev-documentation\/0017-govuk-notify-keycloak-message.md","Context":"## Context\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\n","Decision":"It was decided to pass the entire Keycloak message into the GovUK Notify template via the `keycloakMessage` personalisation parameter, rather than having a mix of text and parameters in the Keycloak template.\\nThis is because the Keycloak message is passed in as a complete constructed string. It would be very difficult to extract particular portions of this string, for example a link for use within the GovUK Notify template.\\nThis approach does mean it is harder for non-developers to make changes to the email messages. The decision may be revisited in the future if this proves to be causing issues.\\n","tokens":33,"id":1786,"text":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\n\n\n##Decision\nIt was decided to pass the entire Keycloak message into the GovUK Notify template via the `keycloakMessage` personalisation parameter, rather than having a mix of text and parameters in the Keycloak template.\\nThis is because the Keycloak message is passed in as a complete constructed string. It would be very difficult to extract particular portions of this string, for example a link for use within the GovUK Notify template.\\nThis approach does mean it is harder for non-developers to make changes to the email messages. The decision may be revisited in the future if this proves to be causing issues.\\n","Predictions":"It was decided to pass the entire Keycloak message into the GovUK Notify template via the keycloakMessage personalisation parameter, rather than having a mix of text and parameters in the Keycloak template.nThis is because the Keycloak message is passed in as a complete constructed string. It would be very difficult to extract particular portions of this string, for example a link for use within the GovUK Notify template.nThis approach does mean it is harder for non-developers to make changes to the email messages. The decision may be revisited in the future if this proves to be causing issues.n"}
{"File Name":"apply-for-teacher-training\/0015-carrying-over-applications.md","Context":"## Context\nThe current recruitment cycle ends on 18th September 2020. At that point there\nwill be some candidates who could benefit from their application being carried\nover to the next cycle. Carrying over an application means the candidate can\napply to courses in the new recruitment cycle without having to fill in the\nwhole application form again.\n### Carrying over an application makes sense in the following states\n#### Before the application reaches the provider\nThese applications could be carried over because the provider has not seen them yet.\n- Withdrawn\n- Unsubmitted\n- Ready to send to provider\n#### After the application can\u2019t progress any further\nThese applications could be carried over because they have reached an\nunsuccessful end state. Enabling candidates to turn these into fresh applications\nin the next cycle makes it as easy as possible for them to try again.\n- Conditions not met\n- Offer withdrawn\n- Offer declined\n- Application cancelled\n- Rejected\n### Carrying over an application does not make sense in the following states\n#### While the application is already under consideration by the provider\n- Awaiting provider decision\n#### When the application already has an offer in flight\n- Offer\n- Meeting conditions (i.e. offer accepted)\n- Recruited\n### Copying the Apply again approach\nThe current approach for moving applications into Apply again is to copy the\nentire application (including references) and invite the user to add a new\ncourse choice. This approach seems like it will work here too, with a couple of\nextra things to take into account:\n- applications that are carried over might be in Apply 1 or Apply again as the\ncycle ends. All carried-over applications should start over as Apply 1\napplications applications moving into Apply again all have complete\nreferences because they\u2019ve already completed Apply 1, for which references\nare required.\n- Carried over applications might have no references, references in flight, or\ncompleted references.\nMoving the new application into the next cycle is a question of making sure its\ncourse choices come from that cycle. As long as carrying over is only possible\nonce the current cycle is closed, this should present no problems because the\navailable courses will all come from the new cycle.\n","Decision":"- We will only allow applications to be carried over once the current cycle is\\nover, and we\u2019ll only allow applications in the above states\\n- To carry over an application, we will adopt the Apply again pattern of\\ncloning the ApplicationForm and removing the courses\\n- We will copy references onto the carried-over application, but only if\\nthey\u2019re complete. Referees who had been contacted but had not responded\\nbefore the application was carried over will need to be cancelled.\\n- The applications that were carried over will remain in the database without\\nany further state change\\n- Applications which were not yet sent to the provider at end of cycle and also\\nnot carried over will still be in the database \u2014 we would like to mark these\\nwith a new state equivalent to \u201cincomplete at end of cycle\u201d. This state would\\nnever be visible to providers.\\n- It\u2019s up to the candidate whether to carry over their application, and we\u2019ll\\ngive them a button to do this\\n","tokens":467,"id":4010,"text":"## Context\\nThe current recruitment cycle ends on 18th September 2020. At that point there\\nwill be some candidates who could benefit from their application being carried\\nover to the next cycle. Carrying over an application means the candidate can\\napply to courses in the new recruitment cycle without having to fill in the\\nwhole application form again.\\n### Carrying over an application makes sense in the following states\\n#### Before the application reaches the provider\\nThese applications could be carried over because the provider has not seen them yet.\\n- Withdrawn\\n- Unsubmitted\\n- Ready to send to provider\\n#### After the application can\u2019t progress any further\\nThese applications could be carried over because they have reached an\\nunsuccessful end state. Enabling candidates to turn these into fresh applications\\nin the next cycle makes it as easy as possible for them to try again.\\n- Conditions not met\\n- Offer withdrawn\\n- Offer declined\\n- Application cancelled\\n- Rejected\\n### Carrying over an application does not make sense in the following states\\n#### While the application is already under consideration by the provider\\n- Awaiting provider decision\\n#### When the application already has an offer in flight\\n- Offer\\n- Meeting conditions (i.e. offer accepted)\\n- Recruited\\n### Copying the Apply again approach\\nThe current approach for moving applications into Apply again is to copy the\\nentire application (including references) and invite the user to add a new\\ncourse choice. This approach seems like it will work here too, with a couple of\\nextra things to take into account:\\n- applications that are carried over might be in Apply 1 or Apply again as the\\ncycle ends. All carried-over applications should start over as Apply 1\\napplications applications moving into Apply again all have complete\\nreferences because they\u2019ve already completed Apply 1, for which references\\nare required.\\n- Carried over applications might have no references, references in flight, or\\ncompleted references.\\nMoving the new application into the next cycle is a question of making sure its\\ncourse choices come from that cycle. As long as carrying over is only possible\\nonce the current cycle is closed, this should present no problems because the\\navailable courses will all come from the new cycle.\\n\n\n##Decision\n- We will only allow applications to be carried over once the current cycle is\\nover, and we\u2019ll only allow applications in the above states\\n- To carry over an application, we will adopt the Apply again pattern of\\ncloning the ApplicationForm and removing the courses\\n- We will copy references onto the carried-over application, but only if\\nthey\u2019re complete. Referees who had been contacted but had not responded\\nbefore the application was carried over will need to be cancelled.\\n- The applications that were carried over will remain in the database without\\nany further state change\\n- Applications which were not yet sent to the provider at end of cycle and also\\nnot carried over will still be in the database \u2014 we would like to mark these\\nwith a new state equivalent to \u201cincomplete at end of cycle\u201d. This state would\\nnever be visible to providers.\\n- It\u2019s up to the candidate whether to carry over their application, and we\u2019ll\\ngive them a button to do this\\n","Predictions":"The Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.nThe Approved option is available in the Approved section of the application.n"}
{"File Name":"holochain-rust\/0003-redux-architecture-pattern.md","Context":"## Context\nWe are doing a rewrite.\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\n","Decision":"The new holochain architecture will follow a redux architecture in order for an agent to have one global state.\\nWe will apply nested state objects which represent a state tree, with sub states for each module.\\nWe use reference counting smart pointers for the sub\\nstates such that it is possible for each module's\\nreducer to decide if the sub state is to be mutated or reused.\\n","tokens":92,"id":1495,"text":"## Context\\nWe are doing a rewrite.\\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\\n\n\n##Decision\nThe new holochain architecture will follow a redux architecture in order for an agent to have one global state.\\nWe will apply nested state objects which represent a state tree, with sub states for each module.\\nWe use reference counting smart pointers for the sub\\nstates such that it is possible for each module's\\nreducer to decide if the sub state is to be mutated or reused.\\n","Predictions":"We will use a stack structure to store the commit messages.n"}
{"File Name":"libelektra\/store_name.md","Context":"## Problem\nMemory Consumption in Elektra is quite high as the key names\nare long and stored twice in memory.\n","Decision":"Only store the unescaped key name, suitable for comparing\/searching\/iterating over name, i.e.:\\n- Remove the escaped name from `struct _Key` and use it only when necessary.\\n- Clarify and reduce [terminology](\/doc\/help\/elektra-glossary.md).\\n- API Changes:\\n- `keyNew (const char*, size_t)`\\n- `keyName` returns the unescaped name\\n- remove `keyUnescapedName`, `keyGetUnescapedNameSize`.\\n- reverse terminology: with \"key name\" we will refer to the unescaped (base) name,\\nthe escaped name will be explicitly called \"escaped key name\".\\n- escaped name will be outside the core for tooling\\n- `keyDup(.., int)` with options to filter which parts are copied\\n(to allow copy of keys where only the key name is copied)\\n","tokens":26,"id":1291,"text":"## Problem\\nMemory Consumption in Elektra is quite high as the key names\\nare long and stored twice in memory.\\n\n\n##Decision\nOnly store the unescaped key name, suitable for comparing\/searching\/iterating over name, i.e.:\\n- Remove the escaped name from `struct _Key` and use it only when necessary.\\n- Clarify and reduce [terminology](\/doc\/help\/elektra-glossary.md).\\n- API Changes:\\n- `keyNew (const char*, size_t)`\\n- `keyName` returns the unescaped name\\n- remove `keyUnescapedName`, `keyGetUnescapedNameSize`.\\n- reverse terminology: with \"key name\" we will refer to the unescaped (base) name,\\nthe escaped name will be explicitly called \"escaped key name\".\\n- escaped name will be outside the core for tooling\\n- `keyDup(.., int)` with options to filter which parts are copied\\n(to allow copy of keys where only the key name is copied)\\n","Predictions":"- keyName returns the unescaped namen- remove keyUnescapedName, keyGetUnescapedNameSize.n- reverse terminology: with \"key name\" we will refer to the unescaped (base) name,nthe escaped name will be explicitly called \"escaped key name\".n- escaped name will be outside the core for toolingn- keyDup(.., int) with options to filter which parts are copiedn(to allow copy of keys where only the key name is copied)n"}
{"File Name":"TruBudget\/0003-project-data-model.md","Context":"## Context\nWe need to define how to save and query project, subproject and workflow data efficiently.\n### Definition of terms\n- Project: Is the root element of the hierarchy. It contains metadata and Subprojects\n- Subproject: Are a subset of Projects and collect a number of Workflowitems\n- Workflowitems: Are a number of steps which need to be executed in a certain order\n- Resources: Projects, Subprojects, Workflowitems are resources. This means they contain permissions, a log and metadata.\n### Relations\n```mermaid\ngraph TD;\nProject-->Subproject_A;\nProject-->Subproject_B;\nSubproject_A-->Workflowitem_A;\nSubproject_A-->Workflowitem_B;\nSubproject_B-->Workflowitem_C;\nSubproject_B-->Workflowitem_D;\n```\n### Options\n1. Each resource has its own stream. Each resource has an array which links to the underlying resources (e.g. A Projects has an array of the ids of its Subrojects). That means if we want to query Subprojects of a specific Project, we need to lookup the Subproject ID's inside the projects to afterswards fetch all the underlying Subproject streams. **Pro:** We are only fetching the data which is necessary, changes inside the resources are storage efficent. **Con:** Resources with a high number of relations result in multiple (maybe hundreds) of queries.\n2. Each Project has its own stream. All relational resources of the projects are stored inside a JSON and resolved on the API Layer. **Pro:** Only one query necessary to resolve the whole resource tree for one project. **Con:** Large amount of storage needed, since every change inside a single resource would trigger a whole copy of the project tree. Large datastructures have to be kept in memory\n3. Each project has its own stream. All relational resources are organized with multiple key-items inside the project stream. This allows to specifically query resources with a single query. Updates to resources will only result in the copy of the affected resource. **Pro:** Easy to query, low storage \/ memory footprint **Con:** None\n","Decision":"Since we need to be able to scale out to larger projects, it is important to be able to execute fast queries and having a low storage \/ memory footprint. The goal is to exploit the Multichain API as much as possible, since we can expect it to be more performant than our API. Option 3 is the option, which covers the required capabilities in the best way.\\n","tokens":438,"id":1225,"text":"## Context\\nWe need to define how to save and query project, subproject and workflow data efficiently.\\n### Definition of terms\\n- Project: Is the root element of the hierarchy. It contains metadata and Subprojects\\n- Subproject: Are a subset of Projects and collect a number of Workflowitems\\n- Workflowitems: Are a number of steps which need to be executed in a certain order\\n- Resources: Projects, Subprojects, Workflowitems are resources. This means they contain permissions, a log and metadata.\\n### Relations\\n```mermaid\\ngraph TD;\\nProject-->Subproject_A;\\nProject-->Subproject_B;\\nSubproject_A-->Workflowitem_A;\\nSubproject_A-->Workflowitem_B;\\nSubproject_B-->Workflowitem_C;\\nSubproject_B-->Workflowitem_D;\\n```\\n### Options\\n1. Each resource has its own stream. Each resource has an array which links to the underlying resources (e.g. A Projects has an array of the ids of its Subrojects). That means if we want to query Subprojects of a specific Project, we need to lookup the Subproject ID's inside the projects to afterswards fetch all the underlying Subproject streams. **Pro:** We are only fetching the data which is necessary, changes inside the resources are storage efficent. **Con:** Resources with a high number of relations result in multiple (maybe hundreds) of queries.\\n2. Each Project has its own stream. All relational resources of the projects are stored inside a JSON and resolved on the API Layer. **Pro:** Only one query necessary to resolve the whole resource tree for one project. **Con:** Large amount of storage needed, since every change inside a single resource would trigger a whole copy of the project tree. Large datastructures have to be kept in memory\\n3. Each project has its own stream. All relational resources are organized with multiple key-items inside the project stream. This allows to specifically query resources with a single query. Updates to resources will only result in the copy of the affected resource. **Pro:** Easy to query, low storage \/ memory footprint **Con:** None\\n\n\n##Decision\nSince we need to be able to scale out to larger projects, it is important to be able to execute fast queries and having a low storage \/ memory footprint. The goal is to exploit the Multichain API as much as possible, since we can expect it to be more performant than our API. Option 3 is the option, which covers the required capabilities in the best way.\\n","Predictions":"We need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently.nWe need to define how to store and query project, subproject and workflow data efficiently."}
{"File Name":"tdr-dev-documentation\/0015-govuk-notify-staging-configuration.md","Context":"## Context\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\nGovUK services have a \"trial mode\", and a \"live mode\".\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\n","Decision":"The \"staging\" GovUK Notify service is to be live to allow for user testing on this environment.\\nAlso, it will mirror the TDR production environment as closely as possible.\\n","tokens":126,"id":1784,"text":"## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\\nGovUK services have a \"trial mode\", and a \"live mode\".\\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\\n\n\n##Decision\nThe \"staging\" GovUK Notify service is to be live to allow for user testing on this environment.\\nAlso, it will mirror the TDR production environment as closely as possible.\\n","Predictions":"We will use the [Verify][service-verify] to send notifications for Verify.nFor MVP we may use the CSV upload function to send notifications, but this willnbe replaced by automated API calls later in development.n"}
{"File Name":"sre-challenge\/0007-use-a-dashboard-for-etcd.md","Context":"## Context\nTo visualize the metrics of the etcd cluster, I would like to implement a\ndashboard using the USE and RED methods.\n- USE to measure the performance of the system hosting the etcd cluster\n- RED to measure the performance of the gRPC side of the etcd cluster\nBut I haven't found anything like this and I have no time to waste.  So\nI have found a dashboard on the grafana site that has some metrics.\n","Decision":"As I'm out of time, I'm just going to implement it at the configuration\\nmanagement system, to automatically provision the dashboard and the\\ndatasource.\\n","tokens":97,"id":3004,"text":"## Context\\nTo visualize the metrics of the etcd cluster, I would like to implement a\\ndashboard using the USE and RED methods.\\n- USE to measure the performance of the system hosting the etcd cluster\\n- RED to measure the performance of the gRPC side of the etcd cluster\\nBut I haven't found anything like this and I have no time to waste.  So\\nI have found a dashboard on the grafana site that has some metrics.\\n\n\n##Decision\nAs I'm out of time, I'm just going to implement it at the configuration\\nmanagement system, to automatically provision the dashboard and the\\ndatasource.\\n","Predictions":"We will use the sdk-repository to store the data.n"}
{"File Name":"AYCHAIN.github.io\/adr-005-substeps-list.md","Context":"## Context\nSubstepsList needs to be animated between showing and hiding the substeps of a step.\nAnimating proves convenient on `max-height`\nWe were initially hard coding the `max-height` property of the substeps list to have a maximum to animate too (from 0).\nHowever, hard-coding that value proved not flexible, especially when the substep was also holding an `ErrorMessage`, which could span an undeterminate height (multiple lines, responsiveness, etc).\nThe height of the list all determines the height of the progress bar which at this point is settled and working with little work.\n","Decision":"We chose to follow the second approach which requires less code to work. It is however creating issues with the `html` templating function of `lit-element`, since we change the state of the attribute without informing the library.\\nDoing the latter would mean re-rendering via a state prop which would have to be updated at the `_didRender` moment, and would create complains from Polymer.\\nWe thus decided to trick the component:\\n- we only calculate the `totalHeight` once, since content will not change once rendered.\\n- we only force the `max-height` with the calculated height once, if the component was forced rendered and if we haven't applied the trick already\\nWe discovered that it would then require 3 clicks on the `hide` button to finally close the list. Some research led us to discover that the `html` templating function handles the values of the attributes, and does not trigger a change (cf: https:\/\/github.com\/Polymer\/lit-html\/blob\/master\/src\/core.ts#L538). As such, in the state of the code, the active value is `0` and not `[totalHeight]px` as we forced it to be. Since the closing value is 0 too, the code didn't see the reason for the change.\\nTo circumvent this issue, we are now setting the `hidden` state value to be 1px, which allows for the initial closing.\\n","tokens":124,"id":158,"text":"## Context\\nSubstepsList needs to be animated between showing and hiding the substeps of a step.\\nAnimating proves convenient on `max-height`\\nWe were initially hard coding the `max-height` property of the substeps list to have a maximum to animate too (from 0).\\nHowever, hard-coding that value proved not flexible, especially when the substep was also holding an `ErrorMessage`, which could span an undeterminate height (multiple lines, responsiveness, etc).\\nThe height of the list all determines the height of the progress bar which at this point is settled and working with little work.\\n\n\n##Decision\nWe chose to follow the second approach which requires less code to work. It is however creating issues with the `html` templating function of `lit-element`, since we change the state of the attribute without informing the library.\\nDoing the latter would mean re-rendering via a state prop which would have to be updated at the `_didRender` moment, and would create complains from Polymer.\\nWe thus decided to trick the component:\\n- we only calculate the `totalHeight` once, since content will not change once rendered.\\n- we only force the `max-height` with the calculated height once, if the component was forced rendered and if we haven't applied the trick already\\nWe discovered that it would then require 3 clicks on the `hide` button to finally close the list. Some research led us to discover that the `html` templating function handles the values of the attributes, and does not trigger a change (cf: https:\/\/github.com\/Polymer\/lit-html\/blob\/master\/src\/core.ts#L538). As such, in the state of the code, the active value is `0` and not `[totalHeight]px` as we forced it to be. Since the closing value is 0 too, the code didn't see the reason for the change.\\nTo circumvent this issue, we are now setting the `hidden` state value to be 1px, which allows for the initial closing.\\n","Predictions":"We chose to follow the second approach which requires less code to work. It is however creating issues with the html templating function of lit-element, since we change the state of the attribute without informing the library.nDoing the latter would mean re-rendering via a state prop which would have to be updated at the _didRender moment, and would create complains from Polymer.nWe discovered that it would then require 3 clicks on the hide button to finally close the list. Some research led us to discover that the html templating function handles the values of the attributes, and does not trigger a change (cf: https:\/\/github.com\/Polymer\/lit-html\/blob\/master\/src\/core.ts#L538). As such, in the state of the code, the active value is 0 and not [totalHeight]px as we forced it to be. Since the closing value is 0 too, the code didn't see the reason for the change.nTo circumvent this issue, we are now setting the hidden state value to be 1px, which allows for the hidden state to be 1px, which allows for the hidden state to be 1px, which allows for the hidden state to be 1px, and allows for the hidden state to be 1px, which allows for the hidden state to be 1px, and allows for the hidden state to be 1px, which allows for the hidden state to be 1px, and allows for the hidden state to be 1px, which allows for the hidden state to be 1px, and allows for the hidden state to be 1px, which allows for the hidden state to be 1px, and allows for the hidden state to be 1px, which allows for the hidden state to be 1px, and allows for the hidden state to be 1px, which allows for the hidden state to be 1px, and allows for the hidden state to be 1px, which allows for the hidden state to be 1px, and allows for the hidden state to be 1px, which allows for the hidden state to be 1px, and allows for the hidden state to be 1px, which allows for the hidden state to be 1px, and allows for the hidden state to be 1px, which allows for the hidden state to be 1px, and allows for the hidden state to be 1px, which allows for the hidden state to be 1px, and allows for the hidden state to be 1px, which allows for the hidden state to be 1px, and allows for the hidden state to be 1px, which allows for the hidden state to be 1px, and allows for the hidden state to be 1px, which allows for the hidden state to be 1px, and allows for the hidden state to be 1px, which allows for the hidden state to be 1px, and allows for the hidden state to be 1px,"}
{"File Name":"python-library-project-generator\/0000-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on Opinionated Digital Center.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":18,"id":875,"text":"## Context\\nWe need to record the architectural decisions made on Opinionated Digital Center.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"LogLady\/0003-implement-redux.md","Context":"## Context\nState handling in React becomes very complicated when you have too many states\n","Decision":"We implement Redux to handle state\\n","tokens":17,"id":1854,"text":"## Context\\nState handling in React becomes very complicated when you have too many states\\n\n\n##Decision\nWe implement Redux to handle state\\n","Predictions":"* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.n* The feature configuration from Feature::getAll() is converted to a SCSS map inside ShopwareStorefrontThemeThemeCompiler::getFeatureConfigScssMap.n* This SCSS map is always added to the SCSS string which gets processed by Shopware.n"}
{"File Name":"opg-use-an-lpa\/0016-logging-and-tracing.md","Context":"## Context\nWe want to have an application wide logging infrastructure in place so that we can trace user requests through the\napplication. It should be possible to trace each request through the application layers so that we can see what outgoing\ncalls were made for incoming requests.\n","Decision":"The use of `monolog\/monolog` to provide configurable logging levels throughout the application. Unique tracing\\ninformation is already provided by the amazon loadbalancers so this should be made available to the logging library and\\nattached to the logged information.\\n* Logging of service code to be main source of log information. If needed handlers can also be logged.\\n* Most logging to be done at an _info_ level.\\n* User actions that need monitoring (e.g. authentication failures) at _notice_ level\\n* Personally Identifiable Information (PII) to **not** be logged anywhere but _debug_\\n","tokens":55,"id":4852,"text":"## Context\\nWe want to have an application wide logging infrastructure in place so that we can trace user requests through the\\napplication. It should be possible to trace each request through the application layers so that we can see what outgoing\\ncalls were made for incoming requests.\\n\n\n##Decision\nThe use of `monolog\/monolog` to provide configurable logging levels throughout the application. Unique tracing\\ninformation is already provided by the amazon loadbalancers so this should be made available to the logging library and\\nattached to the logged information.\\n* Logging of service code to be main source of log information. If needed handlers can also be logged.\\n* Most logging to be done at an _info_ level.\\n* User actions that need monitoring (e.g. authentication failures) at _notice_ level\\n* Personally Identifiable Information (PII) to **not** be logged anywhere but _debug_\\n","Predictions":"We will use a stack library to provide a stack directory for the stack directory. This will be used to store the stack directory and the stack directory.n"}
{"File Name":"figgy\/0002-queues.md","Context":"## Context\nWe have background jobs that are processed by Sidekiq workers on several dedicated background processing\nmachines.  The background jobs include a variety of different tasks from different sources:\n* processing user requests (e.g., ingesting files, (re)generating derivatives)\n* cleaning up after user actions (e.g., propagating state and visibility, removing files before deleting\ntheir parent resource)\n* refreshing metadata from Voyager\/PULFA\n* bulk-ingesting content\n* routine cleanup (cleaning out guest users)\n* fixity checking\n","Decision":"We will have three queues for processing background jobs:\\n1. `default` for processing user-initiated requests needed for ingesting and displaying objects\\n2. `low` for bulk processing, validation, cleanup, etc.\\n3. `super_low` for fixity checking and other long-term preservation actions\\n","tokens":116,"id":4821,"text":"## Context\\nWe have background jobs that are processed by Sidekiq workers on several dedicated background processing\\nmachines.  The background jobs include a variety of different tasks from different sources:\\n* processing user requests (e.g., ingesting files, (re)generating derivatives)\\n* cleaning up after user actions (e.g., propagating state and visibility, removing files before deleting\\ntheir parent resource)\\n* refreshing metadata from Voyager\/PULFA\\n* bulk-ingesting content\\n* routine cleanup (cleaning out guest users)\\n* fixity checking\\n\n\n##Decision\nWe will have three queues for processing background jobs:\\n1. `default` for processing user-initiated requests needed for ingesting and displaying objects\\n2. `low` for bulk processing, validation, cleanup, etc.\\n3. `super_low` for fixity checking and other long-term preservation actions\\n","Predictions":"We will use stack to configure the php.stack instance.n"}
{"File Name":"datalab\/0012-public-and-private-subnets.md","Context":"## Context\nWe would like to minimise our externally exposed footprint. To achieve this we have\nalready decided that access to cluster servers will be via a Bastion server. We do need\nto have some externally accessible services and need to decide how to achieve this.\n","Decision":"We have decided to have external proxy servers for each environment that expose ports 80\\nand 443 to the public Internet by assigning a NAT in the vCloud environment. These proxy\\nservers will route traffic to the Kubernetes cluster services based on the Host Headers.\\n","tokens":55,"id":729,"text":"## Context\\nWe would like to minimise our externally exposed footprint. To achieve this we have\\nalready decided that access to cluster servers will be via a Bastion server. We do need\\nto have some externally accessible services and need to decide how to achieve this.\\n\n\n##Decision\nWe have decided to have external proxy servers for each environment that expose ports 80\\nand 443 to the public Internet by assigning a NAT in the vCloud environment. These proxy\\nservers will route traffic to the Kubernetes cluster services based on the Host Headers.\\n","Predictions":"We have decided to have external proxy servers for each environment that expose ports 80nand 443 to the public Internet by assigning a NAT in the vCloud environment. These proxynservers will route traffic to the Kubernetes cluster services based on the Host Headers.n"}
{"File Name":"Horace\/0007-use-herbert-as-library-dependency.md","Context":"## Context\nThe Horace and Herbert projects are tightly coupled. Herbert build artifacts are required for integration and MATLAB testing of Horace source.\nChanges made to Herbert may change or break dependent MATLAB or C++ code in Horace.\n","Decision":"To make the depdencency explicit Herbert will be regarded as a library.\\nAs a consequence:\\n- Herbert builds will NOT trigger Horace builds\\n- Horace builds (both `PR` and `master`) will always use the latest `master` build of Herbert\\n- Build artifacts will will copied from the latest successful `master-<target-os>-<target-matlab>` build on the Herbert CI server.\\n","tokens":48,"id":4245,"text":"## Context\\nThe Horace and Herbert projects are tightly coupled. Herbert build artifacts are required for integration and MATLAB testing of Horace source.\\nChanges made to Herbert may change or break dependent MATLAB or C++ code in Horace.\\n\n\n##Decision\nTo make the depdencency explicit Herbert will be regarded as a library.\\nAs a consequence:\\n- Herbert builds will NOT trigger Horace builds\\n- Horace builds (both `PR` and `master`) will always use the latest `master` build of Herbert\\n- Build artifacts will will copied from the latest successful `master-<target-os>-<target-matlab>` build on the Herbert CI server.\\n","Predictions":"To make the depdencency explicit Herbert will be regarded as a library.nAs a consequence:n- Herbert builds will NOT trigger Horace buildsn- Horace builds (both PR and master) will always use the latest master build of Herbertn- Build artifacts will will copied from the latest successful master-target-os>-target-matlab> build on the Herbert CI server.n"}
{"File Name":"manuela\/0003-gitops-approach.md","Context":"##  Context\nSee [2. Overall context and container platform](0002-overall-context-and-container-platform.md)\n","Decision":"We use git to address the requirement of a data store:\\n* it is a replicated data store\\n* the hierarchy of deployment targets can be represented as hierarchy of directores in the git repository\\n* can host kubernetes manifests describing application deployments\\n* approval workflows, auditability, etc... is built in\\n* can create replicas with limited content, e.g. a regional DC only replicates what applies to the region with all its factories and line data servers, a factory DC then only replicates what applies to the factory dc and the line data servers, etc.\\n","tokens":25,"id":2968,"text":"##  Context\\nSee [2. Overall context and container platform](0002-overall-context-and-container-platform.md)\\n\n\n##Decision\nWe use git to address the requirement of a data store:\\n* it is a replicated data store\\n* the hierarchy of deployment targets can be represented as hierarchy of directores in the git repository\\n* can host kubernetes manifests describing application deployments\\n* approval workflows, auditability, etc... is built in\\n* can create replicas with limited content, e.g. a regional DC only replicates what applies to the region with all its factories and line data servers, a factory DC then only replicates what applies to the factory dc and the line data servers, etc.\\n","Predictions":"We use git to address the requirement of a data store:n* it is a replicated data storen* the hierarchy of deployment targets can be represented as hierarchy of directores in the git repositoryn* can host kubernetes manifests describing application deploymentsn* approval workflows, auditability, etc... is built inn* can create replicas with limited content, e.g. a regional DC only replicates what applies to the region with all its factories and line data servers, a factory DC then only replicates what applies to the factory dc and the line data servers, etc.n"}
{"File Name":"pace-developers\/0005-pace-projects-must-be-semantically-versioned.md","Context":"## Context\nThe PACE projects will evolve over time and breaking changes will be introduced. Users will need to be able to easily finding the correct documentation for their build.\n[Semantic versioning](https:\/\/semver.org\/) defines a schema in which releases are given `major.minor.patch` version numbers where increments are made to the:\n- `major` version with incompatible API changes,\n- `minor` version when functionality is added in a backwards compatible manner, and\n- `patch` version for backwards compatible bug fixes.\nSphinx plugins are available that support multiple documentation versions ([sphinx-multiversion](https:\/\/pypi.org\/project\/sphinx-multiversion\/)) - this will not work correctly for Brille where documentation is extracted from the build artifacts.\n","Decision":"Projects will be semantically versioned and documentation will be retained and be available for users to access for each major or minor version released.\\nThere is no requirement to have patch-release specific documentation.\\n","tokens":159,"id":5177,"text":"## Context\\nThe PACE projects will evolve over time and breaking changes will be introduced. Users will need to be able to easily finding the correct documentation for their build.\\n[Semantic versioning](https:\/\/semver.org\/) defines a schema in which releases are given `major.minor.patch` version numbers where increments are made to the:\\n- `major` version with incompatible API changes,\\n- `minor` version when functionality is added in a backwards compatible manner, and\\n- `patch` version for backwards compatible bug fixes.\\nSphinx plugins are available that support multiple documentation versions ([sphinx-multiversion](https:\/\/pypi.org\/project\/sphinx-multiversion\/)) - this will not work correctly for Brille where documentation is extracted from the build artifacts.\\n\n\n##Decision\nProjects will be semantically versioned and documentation will be retained and be available for users to access for each major or minor version released.\\nThere is no requirement to have patch-release specific documentation.\\n","Predictions":"We will use a Sass approach to refactor the Sass component.n"}
{"File Name":"report-a-defect\/0008-usegovuknotifyforemailing.md","Context":"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\nThis service has a need to send emails and SMS to users. As Hackney is approved as an organisation that can use GOV.UK Notify we are able to use this service and take advantage of the reliability and low costs.\nHackney already use Notify for other services, although it is not yet referenced in their playbook https:\/\/github.com\/LBHackney-IT\/API-Playbook\n","Decision":"Use Notify over a third party service such as SendGrid for the sending of emails and SMS\\n","tokens":101,"id":5213,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThis service has a need to send emails and SMS to users. As Hackney is approved as an organisation that can use GOV.UK Notify we are able to use this service and take advantage of the reliability and low costs.\\nHackney already use Notify for other services, although it is not yet referenced in their playbook https:\/\/github.com\/LBHackney-IT\/API-Playbook\\n\n\n##Decision\nUse Notify over a third party service such as SendGrid for the sending of emails and SMS\\n","Predictions":"Use Notify over a third party service such as SendGrid for the sending of emails and SMSn"}
{"File Name":"dapr\/ENG-003-test-infrastructure.md","Context":"## Context\nE2E tests ensure the functional correctness in an e2e environment in order to make sure Dapr works with the user code deployments. The tests will be run before \/ after PR is merged or by a scheduler.\nDapr E2E tests require the test infrastructure in order to not only test Dapr functionalities, but also show these test results in a consistent way. This document will decide how to bring up the test cluster, run the test, and report the test results.\n","Decision":"### Test environments\\nAlthough Dapr is designed for multi cloud environments, e2e tests will be run under Kubernetes environments for now. We will support two different options to run e2e tests with local machine and CI on the pre-built Kubernetes cluster.\\n* **Local machine**. contributors or developers will use [Minikube](https:\/\/github.com\/kubernetes\/minikube) to validate their changes and run new tests before creating Pull Request.\\n* **Continuous Integration**. E2E tests will be run in the pre-built [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) before\/after PR is merged or by a scheduler. Even if we will use [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) in our test infrastructure, contributors should run e2e tests in any  RBAC-enabled Kubernetes clusters.\\n### Bring up test cluster\\nWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3rd party test frameworks, such as ginkgo, gomega.\\n### CI\/CD and test result report for tests\\nMany Kubernetes-related projects use [Prow](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/prow), and [Testgrid](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/testgrid) for Test CI, PR, and test result management. However, we will not use them to run Dapr E2E tests and share the test result since we need to self-host them on Google cloud platform.\\nInstead, Dapr will use [Azure Pipeline](https:\/\/azure.microsoft.com\/en-us\/services\/devops\/pipelines\/) to run e2e tests and its [test report feature](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/test\/review-continuous-test-results-after-build?view=azure-devops) without self-hosted CI and test report services. Even contributors can get their own azure pipelines accounts **for free** without self-hosting them.\\n","tokens":102,"id":70,"text":"## Context\\nE2E tests ensure the functional correctness in an e2e environment in order to make sure Dapr works with the user code deployments. The tests will be run before \/ after PR is merged or by a scheduler.\\nDapr E2E tests require the test infrastructure in order to not only test Dapr functionalities, but also show these test results in a consistent way. This document will decide how to bring up the test cluster, run the test, and report the test results.\\n\n\n##Decision\n### Test environments\\nAlthough Dapr is designed for multi cloud environments, e2e tests will be run under Kubernetes environments for now. We will support two different options to run e2e tests with local machine and CI on the pre-built Kubernetes cluster.\\n* **Local machine**. contributors or developers will use [Minikube](https:\/\/github.com\/kubernetes\/minikube) to validate their changes and run new tests before creating Pull Request.\\n* **Continuous Integration**. E2E tests will be run in the pre-built [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) before\/after PR is merged or by a scheduler. Even if we will use [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) in our test infrastructure, contributors should run e2e tests in any  RBAC-enabled Kubernetes clusters.\\n### Bring up test cluster\\nWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3rd party test frameworks, such as ginkgo, gomega.\\n### CI\/CD and test result report for tests\\nMany Kubernetes-related projects use [Prow](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/prow), and [Testgrid](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/testgrid) for Test CI, PR, and test result management. However, we will not use them to run Dapr E2E tests and share the test result since we need to self-host them on Google cloud platform.\\nInstead, Dapr will use [Azure Pipeline](https:\/\/azure.microsoft.com\/en-us\/services\/devops\/pipelines\/) to run e2e tests and its [test report feature](https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/test\/review-continuous-test-results-after-build?view=azure-devops) without self-hosted CI and test report services. Even contributors can get their own azure pipelines accounts **for free** without self-hosting them.\\n","Predictions":"### Test environmentsnAlthough Dapr is designed for multi cloud environments, e2e tests will be run under Kubernetes environments for now. We will support two different options to run e2e tests with local machine and CI on the pre-built Kubernetes cluster.n* **Local machine**. contributors or developers will use [Minikube](https:\/\/github.com\/kubernetes\/minikube) to validate their changes and run new tests before creating Pull Request.n* **Continuous Integration**. E2E tests will be run in the pre-built [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) before\/after PR is merged or by a scheduler. Even if we will use [Azure Kubernetes Service](https:\/\/azure.microsoft.com\/en-us\/services\/kubernetes-service\/) in our test infrastructure, contributors should run e2e tests in any RBAC-enabled Kubernetes clusters.n### Bring up test clusternWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3 Kubernetes projects.n### Bring up test clusternWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3 Kubernetes projects.n### Bring up test clusternWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3 Kubernetes projects.n### Bring up test clusternWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3 Kubernetes projects.n### Bring up test clusternWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2e tests simpler without the dependency of the 3 Kubernetes projects.n### Bring up test clusternWe will provide the manual instruction or simple script to bring up test infrastructure unlike the other Kubernetes projects using [kubetest](https:\/\/github.com\/kubernetes\/test-infra\/tree\/master\/kubetest). Dapr E2E tests will clean up and revert all configurations in the cluster once the test is done. Without kubetest, we can create e2"}
{"File Name":"play-frontend-hmrc\/0012-create-an-endpoint-in-play-frontend-hmrc-for-surfacing-session-metadata.md","Context":"## Context and Problem Statement\nTax users are sometimes, without warning, unnecessarily signed out when accessing\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\nsuccess criterion 2.1.1 (Timing adjustable)](https:\/\/www.w3.org\/WAI\/WCAG21\/Understanding\/timing-adjustable.html).\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https:\/\/github.com\/hmrc\/play-frontend-hmrc#warning-users-before-timing-them-out)\ncurrently has no way of knowing about this activity following initial page load.\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\nremaining on the user's active session via an endpoint that is itself excluded from\nsession management.\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\nknowledge and avoiding introducing additional coupling between frontend microservices?\n## Decision Drivers\n* The need to minimise code changes for service teams other than a library upgrade.\n* The avoidance of requiring service teams to add any additional routing rules.\n* The avoidance of requiring service teams to add any additional configuration.\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\nany other library or service.\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\nof failure between frontend microservices.\n* The need for the endpoint used to interrogate the session to not itself affect the session.\n","Decision":"* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\\naround knowledge duplication.\\n","tokens":348,"id":560,"text":"## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https:\/\/www.w3.org\/WAI\/WCAG21\/Understanding\/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https:\/\/github.com\/hmrc\/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n\n\n##Decision\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\\naround knowledge duplication.\\n","Predictions":"* The need to minimise code changes for service teams other than a library upgrade.n* The avoidance of requiring service teams to add any additional routing rules.n* The avoidance of requiring service teams to add any additional configuration.n* The preference for maintaining loose coupling of frontend services and avoiding adding single pointsnof failure between frontend microservices.n* The need for the endpoint used to interrogate the session to not itself affect the session.n"}
{"File Name":"celestia-core\/adr-005-consensus-params.md","Context":"## Context\nConsensus critical parameters controlling blockchain capacity have until now been hard coded, loaded from a local config, or neglected.\nSince they may be need to be different in different networks, and potentially to evolve over time within\nnetworks, we seek to initialize them in a genesis file, and expose them through the ABCI.\nWhile we have some specific parameters now, like maximum block and transaction size, we expect to have more in the future,\nsuch as a period over which evidence is valid, or the frequency of checkpoints.\n","Decision":"### ConsensusParams\\nNo consensus critical parameters should ever be found in the `config.toml`.\\nA new `ConsensusParams` is optionally included in the `genesis.json` file,\\nand loaded into the `State`. Any items not included are set to their default value.\\nA value of 0 is undefined (see ABCI, below). A value of -1 is used to indicate the parameter does not apply.\\nThe parameters are used to determine the validity of a block (and tx) via the union of all relevant parameters.\\n```\\ntype ConsensusParams struct {\\nBlockSize\\nTxSize\\nBlockGossip\\n}\\ntype BlockSize struct {\\nMaxBytes int\\nMaxTxs int\\nMaxGas int\\n}\\ntype TxSize struct {\\nMaxBytes int\\nMaxGas int\\n}\\ntype BlockGossip struct {\\nBlockPartSizeBytes int\\n}\\n```\\nThe `ConsensusParams` can evolve over time by adding new structs that cover different aspects of the consensus rules.\\nThe `BlockPartSizeBytes` and the `BlockSize.MaxBytes` are enforced to be greater than 0.\\nThe former because we need a part size, the latter so that we always have at least some sanity check over the size of blocks.\\n### ABCI\\n#### InitChain\\nInitChain currently takes the initial validator set. It should be extended to also take parts of the ConsensusParams.\\nThere is some case to be made for it to take the entire Genesis, except there may be things in the genesis,\\nlike the BlockPartSize, that the app shouldn't really know about.\\n#### EndBlock\\nThe EndBlock response includes a `ConsensusParams`, which includes BlockSize and TxSize, but not BlockGossip.\\nOther param struct can be added to `ConsensusParams` in the future.\\nThe `0` value is used to denote no change.\\nAny other value will update that parameter in the `State.ConsensusParams`, to be applied for the next block.\\nTendermint should have hard-coded upper limits as sanity checks.\\n","tokens":111,"id":4099,"text":"## Context\\nConsensus critical parameters controlling blockchain capacity have until now been hard coded, loaded from a local config, or neglected.\\nSince they may be need to be different in different networks, and potentially to evolve over time within\\nnetworks, we seek to initialize them in a genesis file, and expose them through the ABCI.\\nWhile we have some specific parameters now, like maximum block and transaction size, we expect to have more in the future,\\nsuch as a period over which evidence is valid, or the frequency of checkpoints.\\n\n\n##Decision\n### ConsensusParams\\nNo consensus critical parameters should ever be found in the `config.toml`.\\nA new `ConsensusParams` is optionally included in the `genesis.json` file,\\nand loaded into the `State`. Any items not included are set to their default value.\\nA value of 0 is undefined (see ABCI, below). A value of -1 is used to indicate the parameter does not apply.\\nThe parameters are used to determine the validity of a block (and tx) via the union of all relevant parameters.\\n```\\ntype ConsensusParams struct {\\nBlockSize\\nTxSize\\nBlockGossip\\n}\\ntype BlockSize struct {\\nMaxBytes int\\nMaxTxs int\\nMaxGas int\\n}\\ntype TxSize struct {\\nMaxBytes int\\nMaxGas int\\n}\\ntype BlockGossip struct {\\nBlockPartSizeBytes int\\n}\\n```\\nThe `ConsensusParams` can evolve over time by adding new structs that cover different aspects of the consensus rules.\\nThe `BlockPartSizeBytes` and the `BlockSize.MaxBytes` are enforced to be greater than 0.\\nThe former because we need a part size, the latter so that we always have at least some sanity check over the size of blocks.\\n### ABCI\\n#### InitChain\\nInitChain currently takes the initial validator set. It should be extended to also take parts of the ConsensusParams.\\nThere is some case to be made for it to take the entire Genesis, except there may be things in the genesis,\\nlike the BlockPartSize, that the app shouldn't really know about.\\n#### EndBlock\\nThe EndBlock response includes a `ConsensusParams`, which includes BlockSize and TxSize, but not BlockGossip.\\nOther param struct can be added to `ConsensusParams` in the future.\\nThe `0` value is used to denote no change.\\nAny other value will update that parameter in the `State.ConsensusParams`, to be applied for the next block.\\nTendermint should have hard-coded upper limits as sanity checks.\\n","Predictions":"We will use the ConsensusParams in the config.toml file,nand the genesis.json file to initialize the ConsensusParams.nThe ConsensusParams is optionally included in the genesis.json file,nand loaded into the State. Any items not included are set to their default value.nA value of 0 is undefined (see ABCI, below). A value of -1 is used to indicate the parameter does not apply.nThe parameters are used to determine the validity of a block (and tx) via the union of all relevant parameters.ntype ConsensusParams struct nBlockSizenTxSizenBlockGossipnntype BlockSize struct nMaxBytes intnMaxTxs intnMaxGas intnntype TxSize struct nMaxBytes intnMaxGas intnntype BlockGossip struct nBlockPartSizeBytes intnnnThe ConsensusParams can evolve over time by adding new structs that cover different aspects of the consensus rules.nThe BlockPartSizeBytes and the BlockSize.MaxBytes are enforced to be greater than 0.nThe former because we need a part size, the latter because we always have at least some sanity check over the size of blocks.nThe latter because we need a part size, the latter because we always have at least some sanity check over the size of blocks.n"}
{"File Name":"up-fiscal-data\/001-source.md","Context":"## Context and Problem Statement\nSource of data and information to be decided based on the requirements for **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.\n**_Platform 1_**\n**Name** : Budget - Government of Uttar Pradesh\n**URL** : http:\/\/budget.up.nic.in\/\n**Usage** : Analyse budget documents and understand accounting information available.\n**_Platform 2_**\n**Name** : Koshvani web -- A Gateway to Finance Activities in the State of Uttar Pradesh\n**URL** : http:\/\/koshvani.up.nic.in\/\n**Usage** : Scrape, analyse and publish open budget data for public consumption.\n## Decision Drivers\n- Create better underdstanding Uttar Pradesh fiscal data\n- Scrape data to be consumed in machine readable format\n","Decision":"- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\nExplore both platforms to understand the open fiscal data available and techinical requirements.\\n","tokens":183,"id":446,"text":"## Context and Problem Statement\\nSource of data and information to be decided based on the requirements for **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.\\n**_Platform 1_**\\n**Name** : Budget - Government of Uttar Pradesh\\n**URL** : http:\/\/budget.up.nic.in\/\\n**Usage** : Analyse budget documents and understand accounting information available.\\n**_Platform 2_**\\n**Name** : Koshvani web -- A Gateway to Finance Activities in the State of Uttar Pradesh\\n**URL** : http:\/\/koshvani.up.nic.in\/\\n**Usage** : Scrape, analyse and publish open budget data for public consumption.\\n## Decision Drivers\\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\n\n\n##Decision\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\nExplore both platforms to understand the open fiscal data available and techinical requirements.\\n","Predictions":"* Create better underdstanding Uttar Pradesh fiscal datan* Scrape data to be consumed in machine readable formatn* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app, to be reused for SQLite electron desktop version.n* Use PostgreSQL for the web app"}
{"File Name":"dotcom-rendering\/018-react-context-api.md","Context":"# React Context API\n## Context\nWe don't use any state management in dotcom at the moment and this means props have to be\nexplicitly passed around; this can lead to 'prop drilling'.\n[This PR](https:\/\/github.com\/guardian\/dotcom-rendering\/pull\/801) was a spike to demonstrate using\nthe react context api to extract the `edition` property to prevent this.\n","Decision":"-   Our component tree is shallow so we shouldn't implement any 'magic' that\\nsteps away from having explicit props showing where and what data a component is using.\\n-   This decision should be revisited as the codebase grows.\\n","tokens":88,"id":2667,"text":"# React Context API\\n## Context\\nWe don't use any state management in dotcom at the moment and this means props have to be\\nexplicitly passed around; this can lead to 'prop drilling'.\\n[This PR](https:\/\/github.com\/guardian\/dotcom-rendering\/pull\/801) was a spike to demonstrate using\\nthe react context api to extract the `edition` property to prevent this.\\n\n\n##Decision\n-   Our component tree is shallow so we shouldn't implement any 'magic' that\\nsteps away from having explicit props showing where and what data a component is using.\\n-   This decision should be revisited as the codebase grows.\\n","Predictions":"We will use stack to handle this.n"}
{"File Name":"princeton_ansible\/0004-key-scope","Context":"## Context\nIn many cases we can generate keys or tokens as-needed. We may also be able to create keys with varying permissions.\nWe'd like to reduce our vulnerability in the case of one of these keys becoming compromised, either via a compromised box or via a software exploit to application code.\n","Decision":"Keys should be minted on a per-feature, per-application basis.\\n","tokens":61,"id":2256,"text":"## Context\\nIn many cases we can generate keys or tokens as-needed. We may also be able to create keys with varying permissions.\\nWe'd like to reduce our vulnerability in the case of one of these keys becoming compromised, either via a compromised box or via a software exploit to application code.\\n\n\n##Decision\nKeys should be minted on a per-feature, per-application basis.\\n","Predictions":"We recommend replacing the current Keybase backend based on LevelDB with [Keyring](https:\/\/github.com\/99designs\/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.nThis appears to fulfill the requirement of protecting both key material and metadata from rouge software on a user\u2019s machine.n"}
{"File Name":"island.is\/0012-chart-library.md","Context":"## Context and Problem Statement\nMultiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.\n## Decision Drivers\n- Meet all requirements listed above\n- API quality\n- Pricing\n- Bundle size\n- Typescript support\n","Decision":"- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\nChosen option: \"Recharts\", because it meets all requirements, and overall has a very nice, dev-friendly API. It is the most popular (downloads per week) react charting library on github, and recommended across the community. We can customize how it looks, and start using it quickly without much groundwork.\\n### Positive Consequences\\n- We can start implementing charts and graphs as needed in island.is\\n### Negative Consequences\\n- It is a big dependency, but almost all chart libraries are big due to their nature. We will minimize the impact of this by enforcing charts to be lazy loaded in our codebase.\\n","tokens":79,"id":1119,"text":"## Context and Problem Statement\\nMultiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.\\n## Decision Drivers\\n- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\n\n\n##Decision\n- Meet all requirements listed above\\n- API quality\\n- Pricing\\n- Bundle size\\n- Typescript support\\nChosen option: \"Recharts\", because it meets all requirements, and overall has a very nice, dev-friendly API. It is the most popular (downloads per week) react charting library on github, and recommended across the community. We can customize how it looks, and start using it quickly without much groundwork.\\n### Positive Consequences\\n- We can start implementing charts and graphs as needed in island.is\\n### Negative Consequences\\n- It is a big dependency, but almost all chart libraries are big due to their nature. We will minimize the impact of this by enforcing charts to be lazy loaded in our codebase.\\n","Predictions":"* We will use a Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic Semantic"}
{"File Name":"molgenis-js-auth\/0001-use-adr-to-describe-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","tokens":16,"id":4291,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions.\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"support-rota\/0003-use-dotenv-for-managing-environment-variables.md","Context":"## Context\nAccessing ENV directly without a wrapper is limited and can introduce problems.\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\n","Decision":"Use DotEnv to load our environment variables.\\n","tokens":324,"id":3389,"text":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n\n\n##Decision\nUse DotEnv to load our environment variables.\\n","Predictions":"Instead of adding nil guards throughout the codebase for required environment variables (eg. ENV.fetch('FOO', 'default'), this should be managed centrally.nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg.  allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. allow(Figaro).to receive(:env).with(:foo).and_return(\"1234\")). We should then consider how we can stub environment variables.n"}
{"File Name":"digital-paper-edit-firebase\/2019-04-29-SQL-database-schema.md","Context":"## Context and Problem Statement\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\nEg have 3 version of the schema for the 3 main milestones:\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\n## Decision Drivers <!-- optional -->\n* easy to reason around\n* \u2026 <!-- numbers of drivers can vary -->\n","Decision":"* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","tokens":248,"id":5244,"text":"## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nA schema for the SQL database in PostgreSQL for the web app, to be reused for SQLite electron desktop version.\\nIt should consider [the project Milestones](https:\/\/github.com\/bbc\/digital-paper-edit\/milestones) and be easy to version incrementally.\\nEg have 3 version of the schema for the 3 main milestones:\\n- [Feature: Transcript Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/3)\\n- [Feature: Transcript Annotations](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/4)\\n- [Feature: Paper Editing](https:\/\/github.com\/bbc\/digital-paper-edit\/milestone\/5)\\n[DB Designer](https:\/\/dbdesigner.page.link\/cq9FMHVVxsYqTasf7) - let's you export SQL code to Create and drop tables.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* easy to reason around\\n* \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\n### Positive Consequences <!-- optional -->\\n<!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n### Negative consequences <!-- optional -->\\n<!-- * [e.g., compromising quality attribute, follow-up decisions required, \u2026] -->\\n* \u2026\\n_TBC_\\n","Predictions":"* easy to reason aroundn*...!-- numbers of drivers can vary -->n!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force |... | comes out best (see below)]. -->n_TBC_n### Positive Consequences!-- optional -->n!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required,...] -->n*...n_TBC_n### Negative consequences !-- optional -->n!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force |... | comes out best (see below)]. -->n_TBC_n### Negative consequences !-- optional -->n!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required,...] -->n*...n_TBC_n### Positive consequences!-- optional -->n!-- * [e.g., improvement of quality attribute satisfaction, follow-up decisions required,...] -->n_TBC_n### Negative consequences !--"}
{"File Name":"nso.aurora\/MicroserviceDesgin.md","Context":"## Context\nTo ensure scalability, reliability and performance, microservice architecture should be used\n","Decision":"All agreed with this decision\\n","tokens":18,"id":302,"text":"## Context\\nTo ensure scalability, reliability and performance, microservice architecture should be used\\n\n\n##Decision\nAll agreed with this decision\\n","Predictions":"We will use a stack class to store the stack data.n"}
{"File Name":"saas-platform-frontend\/0003-use-javascript-over-typescript.md","Context":"## Context\nI need to decide in which language I implement the frontend.\n","Decision":"I use Javascript.\\n","tokens":16,"id":3715,"text":"## Context\\nI need to decide in which language I implement the frontend.\\n\n\n##Decision\nI use Javascript.\\n","Predictions":"I use Javascript.n"}
{"File Name":"architecture-decision-log\/0002-github-template-for-issues-and-pull-requests.md","Context":"* [Context](#context)\n* [Decision](#decision)\n* [Status](#status)\n* [Consequences](#consequences)\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\nAn easy way to improve our communication is by creating templates for both `issues` and `pull requests`. Those templates must be simple, easy to use, and express all required sections for our developers and users.\\nYou can check the suggested templates [in the following folder](..\/assets\/0002-github-template-for-issues-and-pull-requests).\\n","tokens":34,"id":4919,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\nAn easy way to improve our communication is by creating templates for both `issues` and `pull requests`. Those templates must be simple, easy to use, and express all required sections for our developers and users.\\nYou can check the suggested templates [in the following folder](..\/assets\/0002-github-template-for-issues-and-pull-requests).\\n","Predictions":"* [Status](#status)n* [Consequences](#consequences)nAn easy way to improve our communication is by creating templates for both issues and pull requests. Those templates must be simple, easy to use, and express all required sections for our developers and users.nYou can check the suggested templates [in the following folder](..\/assets\/0002-github-template-for-issues-and-pull-requests).n"}
{"File Name":"react-library-template\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4229,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"adr-poc\/0003-write-help-file.md","Context":"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4963,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","Predictions":"The change that we're proposing or have agreed to implement.n"}
{"File Name":"GDD-app\/0002-use-preferences-for-data-storage.md","Context":"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4867,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n\n##Decision\nThe change that we're proposing or have agreed to implement.\\n","Predictions":"The change that we're proposing or have agreed to implement.n"}
{"File Name":"mlmc\/0002-use-mongodb-for-operation.md","Context":"## Context and Problem Statement\nOperation API need some data storage to be able to list\/commission\/decommission\/launch missiles.\n## Decision Drivers\n* Easy to use storage\n* Fast storage\n* Dynamic and\/or schemaless storage\n","Decision":"* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\nDecided to use MongoDB since\\n* The product growths without strict requirements, so entitites are changed very often. In this circumstances schemaless approach is better than predefined schema approach\\n* No need to use RDBMS just to store some random missiless on a stock\\n* I wanted to get some experience in MongoDB\\n### Positive Consequences\\n* Implementation will be faster and easier since schema is going to be changed lot of times\\n### Negative Consequences\\n* n\/a\\n","tokens":48,"id":1574,"text":"## Context and Problem Statement\\nOperation API need some data storage to be able to list\/commission\/decommission\/launch missiles.\\n## Decision Drivers\\n* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\n\n\n##Decision\n* Easy to use storage\\n* Fast storage\\n* Dynamic and\/or schemaless storage\\nDecided to use MongoDB since\\n* The product growths without strict requirements, so entitites are changed very often. In this circumstances schemaless approach is better than predefined schema approach\\n* No need to use RDBMS just to store some random missiless on a stock\\n* I wanted to get some experience in MongoDB\\n### Positive Consequences\\n* Implementation will be faster and easier since schema is going to be changed lot of times\\n### Negative Consequences\\n* n\/a\\n","Predictions":"* Easy to usen* Fast storagen* Dynamic and\/or schemaless storagenDecided to use MongoDB sincen* The product growths without strict requirements, so entitites are changed very often. In this circumstance schemaless approach is better than predefined schema approachn* No need to use RDBMS just to store some random missiless on a stockn* I wanted to get some experience in MongoDBn### Positive Consequences!-- optional -->n* Direct compatibility with additional tool libraries, e.g. for XPM handlingn* Lots of resources and with Openbox a very complete WM (in sense of ICCCM and EWMH) as possible templaten### Negative Consequences!-- optional -->n* n\/an"}
{"File Name":"james-project\/0031-distributed-mail-queue.md","Context":"## Context\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\nto not overload a server.\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\nallows, among others:\n- Delaying retries upon MX delivery failure to a remote site.\n- Throttling, which could be helpful for not being considered a spammer.\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\ndelays, purging the queue, etc.\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\nto interact with all its James servers, which is not friendly in a distributed setup.\nDistributed James relies on the following third party softwares (among other):\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\n- **ObjectStorage** (Swift or S3) holds byte content.\n","Decision":"Distributed James should ship a distributed MailQueue composing the following softwares with the following\\nresponsibilities:\\n- **RabbitMQ** for messaging. A rabbitMQ consumer will trigger dequeue operations.\\n- A time series projection of the queue content (order by time list of mail metadata) will be maintained in **Cassandra** (see later). Time series avoid the\\naforementioned tombstone anti-pattern, and no polling is performed on this projection.\\n- **ObjectStorage** (Swift or S3) holds large byte content. This avoids overwhelming other softwares which do not scale\\nas well in term of Input\/Output operation per seconds.\\nHere are details of the tables composing Cassandra MailQueue View data-model:\\n- **enqueuedMailsV3** holds the time series. The primary key holds the queue name, the (rounded) time of enqueue\\ndesigned as a slice, and a bucketCount. Slicing enables listing a large amount of items from a given point in time, in an\\nfashion that is not achievable with a classic partition approach. The bucketCount enables sharding and avoids all writes\\nat a given point in time to go to the same Cassandra partition. The clustering key is composed of an enqueueId - a\\nunique identifier. The content holds the metadata of the email. This table enables, from a starting date, to load all of\\nthe emails that have ever been in the mailQueue. Its content is never deleted.\\n- **deletedMailsV2** tells wether a mail stored in *enqueuedMailsV3* had been deleted or not. The queueName and\\nenqueueId are used as primary key. This table is updated upon dequeue and deletes. This table is queried upon dequeue\\nto filter out deleted\/purged items.\\n- **browseStart** store the latest known point in time from which all previous emails had been deleted\/dequeued. It\\nenables to skip most deleted items upon browsing\/deleting queue content. Its update is probability based and\\nasynchronously piggy backed on dequeue.\\nHere are the main mail operation sequences:\\n- Upon **enqueue** mail content is stored in the *object storage*, an entry is added in *enqueuedMailsV3* and a message\\nis fired on *rabbitMQ*.\\n- **dequeue** is triggered by a rabbitMQ message to be received. *deletedMailsV2* is queried to know if the message had\\nalready been deleted. If not, the mail content is retrieved from the *object storage*, then an entry is added in\\n*deletedMailsV2* to notice the email had been dequeued. A dequeue has a random probability to trigger a browse start\\nupdate. If so, from current browse start, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*\\nuntil the first non deleted \/ dequeued email is found. This point becomes the new browse start. BrowseStart can never\\npoint after the start of the current slice. A grace period upon browse start update is left to tolerate clock skew.\\nUpdate of the browse start is done randomly as it is a simple way to avoid synchronisation in a distributed system: we\\nensure liveness while uneeded browseStart updates being triggered would simply waste a few resources.\\n- Upon **browse**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*, starting from the\\ncurrent browse start.\\n- Upon **delete\/purge**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*. Mails matching\\nthe condition are marked as deleted in *enqueuedMailsV3*.\\n- Upon **getSize**, we perform a browse and count the returned elements.\\nThe distributed mail queue requires a fine tuned configuration, which mostly depends of the count of Cassandra servers,\\nand of the mailQueue throughput:\\n- **sliceWindow** is the time period of a slice. All the elements of **enqueuedMailsV3** sharing the same slice are\\nretrieved at once. The bigger, the more elements are going to be read at once, the less frequent browse start update\\nwill be. Lower values might result in many almost empty slices to be read, generating higher read load. We recommend\\n**sliceWindow** to be chosen from users maximum throughput so that approximately 10.000 emails be contained in a slice.\\nOnly values dividing the current *sliceWindow* are allowed as new values (otherwize previous slices might not be found).\\n- **bucketCount** enables spreading the writes in your Cassandra cluster using a bucketting strategy. Low values will\\nlead to workload not to be spread evenly, higher values might result in uneeded reads upon browse. The count of Cassandra\\nservers should be a good starting value. Only increasing the count of buckets is supported as a configuration update as\\ndecreasing the bucket count might result in some buckets to be lost.\\n- **updateBrowseStartPace** governs the probability of updating browseStart upon dequeue\/deletes. We recommend choosing\\na value guarantying a reasonable probability of updating the browse start every few slices. Too big values will lead to\\nuneeded update of not yet finished slices. Too low values will end up in a more expensive browseStart update and browse\\niterating through slices with all their content deleted. This value can be changed freely.\\nWe rely on eventSourcing to validate the mailQueue configuration changes upon James start following the aforementioned rules.\\n","tokens":321,"id":2892,"text":"## Context\\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\\nto not overload a server.\\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\\nallows, among others:\\n- Delaying retries upon MX delivery failure to a remote site.\\n- Throttling, which could be helpful for not being considered a spammer.\\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\\ndelays, purging the queue, etc.\\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\\nto interact with all its James servers, which is not friendly in a distributed setup.\\nDistributed James relies on the following third party softwares (among other):\\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\\n- **ObjectStorage** (Swift or S3) holds byte content.\\n\n\n##Decision\nDistributed James should ship a distributed MailQueue composing the following softwares with the following\\nresponsibilities:\\n- **RabbitMQ** for messaging. A rabbitMQ consumer will trigger dequeue operations.\\n- A time series projection of the queue content (order by time list of mail metadata) will be maintained in **Cassandra** (see later). Time series avoid the\\naforementioned tombstone anti-pattern, and no polling is performed on this projection.\\n- **ObjectStorage** (Swift or S3) holds large byte content. This avoids overwhelming other softwares which do not scale\\nas well in term of Input\/Output operation per seconds.\\nHere are details of the tables composing Cassandra MailQueue View data-model:\\n- **enqueuedMailsV3** holds the time series. The primary key holds the queue name, the (rounded) time of enqueue\\ndesigned as a slice, and a bucketCount. Slicing enables listing a large amount of items from a given point in time, in an\\nfashion that is not achievable with a classic partition approach. The bucketCount enables sharding and avoids all writes\\nat a given point in time to go to the same Cassandra partition. The clustering key is composed of an enqueueId - a\\nunique identifier. The content holds the metadata of the email. This table enables, from a starting date, to load all of\\nthe emails that have ever been in the mailQueue. Its content is never deleted.\\n- **deletedMailsV2** tells wether a mail stored in *enqueuedMailsV3* had been deleted or not. The queueName and\\nenqueueId are used as primary key. This table is updated upon dequeue and deletes. This table is queried upon dequeue\\nto filter out deleted\/purged items.\\n- **browseStart** store the latest known point in time from which all previous emails had been deleted\/dequeued. It\\nenables to skip most deleted items upon browsing\/deleting queue content. Its update is probability based and\\nasynchronously piggy backed on dequeue.\\nHere are the main mail operation sequences:\\n- Upon **enqueue** mail content is stored in the *object storage*, an entry is added in *enqueuedMailsV3* and a message\\nis fired on *rabbitMQ*.\\n- **dequeue** is triggered by a rabbitMQ message to be received. *deletedMailsV2* is queried to know if the message had\\nalready been deleted. If not, the mail content is retrieved from the *object storage*, then an entry is added in\\n*deletedMailsV2* to notice the email had been dequeued. A dequeue has a random probability to trigger a browse start\\nupdate. If so, from current browse start, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*\\nuntil the first non deleted \/ dequeued email is found. This point becomes the new browse start. BrowseStart can never\\npoint after the start of the current slice. A grace period upon browse start update is left to tolerate clock skew.\\nUpdate of the browse start is done randomly as it is a simple way to avoid synchronisation in a distributed system: we\\nensure liveness while uneeded browseStart updates being triggered would simply waste a few resources.\\n- Upon **browse**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*, starting from the\\ncurrent browse start.\\n- Upon **delete\/purge**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*. Mails matching\\nthe condition are marked as deleted in *enqueuedMailsV3*.\\n- Upon **getSize**, we perform a browse and count the returned elements.\\nThe distributed mail queue requires a fine tuned configuration, which mostly depends of the count of Cassandra servers,\\nand of the mailQueue throughput:\\n- **sliceWindow** is the time period of a slice. All the elements of **enqueuedMailsV3** sharing the same slice are\\nretrieved at once. The bigger, the more elements are going to be read at once, the less frequent browse start update\\nwill be. Lower values might result in many almost empty slices to be read, generating higher read load. We recommend\\n**sliceWindow** to be chosen from users maximum throughput so that approximately 10.000 emails be contained in a slice.\\nOnly values dividing the current *sliceWindow* are allowed as new values (otherwize previous slices might not be found).\\n- **bucketCount** enables spreading the writes in your Cassandra cluster using a bucketting strategy. Low values will\\nlead to workload not to be spread evenly, higher values might result in uneeded reads upon browse. The count of Cassandra\\nservers should be a good starting value. Only increasing the count of buckets is supported as a configuration update as\\ndecreasing the bucket count might result in some buckets to be lost.\\n- **updateBrowseStartPace** governs the probability of updating browseStart upon dequeue\/deletes. We recommend choosing\\na value guarantying a reasonable probability of updating the browse start every few slices. Too big values will lead to\\nuneeded update of not yet finished slices. Too low values will end up in a more expensive browseStart update and browse\\niterating through slices with all their content deleted. This value can be changed freely.\\nWe rely on eventSourcing to validate the mailQueue configuration changes upon James start following the aforementioned rules.\\n","Predictions":"- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**n- **Status**"}
{"File Name":"support-rota\/0004-add-rollbar-for-application-monitoring.md","Context":"## Context\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\n","Decision":"Use Rollbar to collect and manage our application errors.\\n","tokens":187,"id":3388,"text":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n\n\n##Decision\nUse Rollbar to collect and manage our application errors.\\n","Predictions":"Use Rollbar to collect and manage our application errors.n"}
{"File Name":"adr\/ADR-6-git-style-guide.md","Context":"## Context or problem to solve\n* How should git histories look like?\n* What format should we use to write commit messages?\n","Decision":"We agreed on normalizing commit messages to master branches to avoid things like `Merge pull request #15 from client\/menduz-patch-10` in persuit of more semantic messages like `fix: commit style guide, closes #15`. That is particularly helpful in repositories with several contributors and fosters professionalism in open source repositories.\\n### Branches\\nWhen you work on a branch on a specific issue, we keep the spirit of [semantic branch naming](https:\/\/medium.com\/@hanuman_95739\/how-to-integrate-branch-naming-commit-message-and-push-rules-in-gitlab-fe9cd642cc1a). Think of this as writing what is and what you are doing in a three word sentence The first one must be oune of the list. For instance:\\n```\\nfix\/wrong_host\\n^  ^^------------^\\n|  ||\\n|  |+----> Summary in present tense.\\n|  +-----> Slash\\n+--------> Type: chore, docs, feat, fix, refactor, style, or test.\\n```\\nOther examples are:\\n```\\ndocs\/update_readme\\nrefactor\/new_welcome_message\\n```\\nLook for the *Examples* in section *Commit messages* for a description of the allowed branch types.\\nIt's OK to use hyphens (`-`) or underscores (`_`) to replace spaces. Avoid any other special characters, like `#` or `$`, as they might lead to problems, for example, when deploying the content using the branch name as part of the URL. The branch name should match this regexp: `(chore|docs|feat|fix|refactor|style|test)\/[0-9a-zA-Z_-]+`\\n#### Exceptions:\\nThis rules are ignored when you work on *environment* branches like `master`, `development`, `staging` and so on.\\n","tokens":28,"id":4600,"text":"## Context or problem to solve\\n* How should git histories look like?\\n* What format should we use to write commit messages?\\n\n\n##Decision\nWe agreed on normalizing commit messages to master branches to avoid things like `Merge pull request #15 from client\/menduz-patch-10` in persuit of more semantic messages like `fix: commit style guide, closes #15`. That is particularly helpful in repositories with several contributors and fosters professionalism in open source repositories.\\n### Branches\\nWhen you work on a branch on a specific issue, we keep the spirit of [semantic branch naming](https:\/\/medium.com\/@hanuman_95739\/how-to-integrate-branch-naming-commit-message-and-push-rules-in-gitlab-fe9cd642cc1a). Think of this as writing what is and what you are doing in a three word sentence The first one must be oune of the list. For instance:\\n```\\nfix\/wrong_host\\n^  ^^------------^\\n|  ||\\n|  |+----> Summary in present tense.\\n|  +-----> Slash\\n+--------> Type: chore, docs, feat, fix, refactor, style, or test.\\n```\\nOther examples are:\\n```\\ndocs\/update_readme\\nrefactor\/new_welcome_message\\n```\\nLook for the *Examples* in section *Commit messages* for a description of the allowed branch types.\\nIt's OK to use hyphens (`-`) or underscores (`_`) to replace spaces. Avoid any other special characters, like `#` or `$`, as they might lead to problems, for example, when deploying the content using the branch name as part of the URL. The branch name should match this regexp: `(chore|docs|feat|fix|refactor|style|test)\/[0-9a-zA-Z_-]+`\\n#### Exceptions:\\nThis rules are ignored when you work on *environment* branches like `master`, `development`, `staging` and so on.\\n","Predictions":"We agreed on normalizing commit messages to master branches to avoid things like Merge pull request #15 from client\/menduz-patch-10 in persuit of more semantic messages like fix: commit style guide, closes #15. That is particularly helpful in repositories with several contributors and fosters professionalism in open source repositories.n### BranchesnWhen you work on a branch on a specific issue, we keep the spirit of [semantic branch naming](https:\/\/medium.com\/@hanuman_95739\/how-to-integrate-branch-naming-commit-message-and-push-rules-in-gitlab-fe9cd642cc1a). Think of this as writing what is and what you are doing in three word sentence. The first one must be oune of the list. For instance:nnfix\/wrong_hostn ---------------n| ||n| |+----> Summary in present tense.n| +-----> Slashn+-----> Slashn+-----> Type: chore, docs, feat, fix, refactor, style, or test.nnOther examples are:ndocs\/update_readmenrefactor\/new_welcome_messagenLook for the *Examples* in section *Commit messages* for a description of the allowed branch types.nIt's OK to use hyphens (-) or underscores (_) to replace spaces. Avoid any other special characters, like # or $, as they might lead to problems, for example, when deploying the content using the branch name as part of the URL. The branch name should match this regexp: (chore|docs|feat|fix|refactor|style|test)\/[0-9a-zA-Z_-]+n"}
{"File Name":"FlowKit\/0006-JWTs.md","Context":"## Context\nAuthentication and access control for FlowKit has previously been very limited, making use of only of postgres usernames and passwords. WIth the introduction of the API, we can be much more granular in allowing access through the use of [JSON Web Tokens](https:\/\/jwt.io).\nThese are an encoded and cryptographically signed string, which permit access to some set of API functionality. The string specifies the identity of the user, exactly what they can access, and the time period for which the token is valid.\nTampering with the token will result in the signature not matching, an event which can be logged an subsequently investigated.\nA key advantage of JWTs is that they can be centrally managed, even where the service they will be used with is not accessible to the internet. Another significant advantage is that the tokens are inherently ephemeral - should a token be stolen, the time window for it to lead to a data breach will (given appropriate usage of the system) be small.\nJWTs are also advantageous in that the add relatively minimal overhead to the day to day experience of analysts, and have broad support across many languages.\nBut perhaps the most important advantage is the granularity of access they afford - for example it is possible to allow access to only one specific result with this method. This is useful where developing dashboards and the like, or where access to outputs but not data must be provided to other parties.\nOther alternatives would be to use a more traditional login system, backed by a database, integrate with the Docker host's authentication system, or to use public key based authentication. All of these require more and local administration, or necessitate the use of either a JWT equivalent to actually provide API access, or the use of cookies to avoid a need to log in for every communication with the API.\n","Decision":"API authentication and access control will use JWTs.\\n","tokens":369,"id":5057,"text":"## Context\\nAuthentication and access control for FlowKit has previously been very limited, making use of only of postgres usernames and passwords. WIth the introduction of the API, we can be much more granular in allowing access through the use of [JSON Web Tokens](https:\/\/jwt.io).\\nThese are an encoded and cryptographically signed string, which permit access to some set of API functionality. The string specifies the identity of the user, exactly what they can access, and the time period for which the token is valid.\\nTampering with the token will result in the signature not matching, an event which can be logged an subsequently investigated.\\nA key advantage of JWTs is that they can be centrally managed, even where the service they will be used with is not accessible to the internet. Another significant advantage is that the tokens are inherently ephemeral - should a token be stolen, the time window for it to lead to a data breach will (given appropriate usage of the system) be small.\\nJWTs are also advantageous in that the add relatively minimal overhead to the day to day experience of analysts, and have broad support across many languages.\\nBut perhaps the most important advantage is the granularity of access they afford - for example it is possible to allow access to only one specific result with this method. This is useful where developing dashboards and the like, or where access to outputs but not data must be provided to other parties.\\nOther alternatives would be to use a more traditional login system, backed by a database, integrate with the Docker host's authentication system, or to use public key based authentication. All of these require more and local administration, or necessitate the use of either a JWT equivalent to actually provide API access, or the use of cookies to avoid a need to log in for every communication with the API.\\n\n\n##Decision\nAPI authentication and access control will use JWTs.\\n","Predictions":"We will use a SecurityType field to identify the service that will be used.n"}
{"File Name":"nada-kafkarator\/0003-use-poetry-for-dependency-management.md","Context":"## Context\nPython projects should use virtualenvs to isolate them from the system Python. In addition, it is useful\nto use a tool to manage dependencies installed in that virtualenv. There are a number of options in this\nspace, and the Python community has not landed on a single standard. Each project needs to select which\ntools to use, usually a selection of tools that integrate well.\nThe Python Package Authority usually recommends Pipenv, but it has several problems that are not being\ndealt with for various reasons. A newcomer in the space is Poetry, which has a bit of a following, and\nclaims to support the use case well. It is also following the new pyproject.toml standard.\n","Decision":"We will use Poetry for this project. (https:\/\/python-poetry.org\/)\\n","tokens":148,"id":864,"text":"## Context\\nPython projects should use virtualenvs to isolate them from the system Python. In addition, it is useful\\nto use a tool to manage dependencies installed in that virtualenv. There are a number of options in this\\nspace, and the Python community has not landed on a single standard. Each project needs to select which\\ntools to use, usually a selection of tools that integrate well.\\nThe Python Package Authority usually recommends Pipenv, but it has several problems that are not being\\ndealt with for various reasons. A newcomer in the space is Poetry, which has a bit of a following, and\\nclaims to support the use case well. It is also following the new pyproject.toml standard.\\n\n\n##Decision\nWe will use Poetry for this project. (https:\/\/python-poetry.org\/)\\n","Predictions":"Use Rollbar to collect and manage our application errors.n"}
{"File Name":"TruBudget\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1226,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"deeplearning4j\/0008 - Nd4j eager  shape computation .md","Context":"## Context\nNd4j's model import framework often has the need to\ncompute  shapes as variables are created.\nThis is  in order to resolve how to properly\ncreate a graph based on a graph descriptor from another framework\nsuch as tensorflow or pytorch.\nThis is often called eager mode. This proposal focuses on just eager shape computation\nintended for use in model import. The assumption is that we could\nbuild on this later for fully eager computation.\n","Decision":"In order to aid building model import easier,\\nthis proposal is focused on implementing just dynamic shape computation\\nfor use in the model import context.\\nThis will be composed of a few parts:\\n1. Each outputVariables() call in SDVariable triggers\\nan Nd4j.getExecutioner().exec(..) call on the relevant operation\\nto extract out op shapes. It then sets the appropriate shapes\\nbased on the result for each SDVariable field.\\n2. This will intentionally include dummy calls for control flow ops\\nsuch as if, enter, and while. Shapes from these don't matter\\nbeyond knowing the number of outputs.\\n3. Each SameDiff instance will have an eager mode boolean\\nthat will determine whether this functionality is invoked.\\nThis eager mode variable will be required for some model import use cases.\\nUsually the model import framework will turn eager on as needed\\nwithout the user needing to be involved.\\n4. Each SameDiff instance will have a separate ArrayHolder\\nthat will be used for looking up ndarrays relevant\\nto the eager computation. This will not use proper sessions\\nbut instead store that will be used once for computing shapes.\\n","tokens":97,"id":2933,"text":"## Context\\nNd4j's model import framework often has the need to\\ncompute  shapes as variables are created.\\nThis is  in order to resolve how to properly\\ncreate a graph based on a graph descriptor from another framework\\nsuch as tensorflow or pytorch.\\nThis is often called eager mode. This proposal focuses on just eager shape computation\\nintended for use in model import. The assumption is that we could\\nbuild on this later for fully eager computation.\\n\n\n##Decision\nIn order to aid building model import easier,\\nthis proposal is focused on implementing just dynamic shape computation\\nfor use in the model import context.\\nThis will be composed of a few parts:\\n1. Each outputVariables() call in SDVariable triggers\\nan Nd4j.getExecutioner().exec(..) call on the relevant operation\\nto extract out op shapes. It then sets the appropriate shapes\\nbased on the result for each SDVariable field.\\n2. This will intentionally include dummy calls for control flow ops\\nsuch as if, enter, and while. Shapes from these don't matter\\nbeyond knowing the number of outputs.\\n3. Each SameDiff instance will have an eager mode boolean\\nthat will determine whether this functionality is invoked.\\nThis eager mode variable will be required for some model import use cases.\\nUsually the model import framework will turn eager on as needed\\nwithout the user needing to be involved.\\n4. Each SameDiff instance will have a separate ArrayHolder\\nthat will be used for looking up ndarrays relevant\\nto the eager computation. This will not use proper sessions\\nbut instead store that will be used once for computing shapes.\\n","Predictions":"In order to aid building model import easier,nthis proposal is focused on implementing just dynamic shape computationnfor use in the model import context.nThis will be composed of a few parts:n1. Each outputVariables() call in SDVariable triggersnan Nd4j.getExecutioner().exec(..) call on the relevant operationnto extract out op shapes. It then sets the appropriate shapesnbased on the result for each SDVariable field.n2. This will intentionally include dummy calls for control flow opsnsuch as if, enter, and while. Shapes from these don't matternbeyond knowing the number of outputs.n3. Each SameDiff instance will have an eager mode booleannthat will determine whether this functionality is invoked.nThis eager mode variable will be required for some model import use cases.nUsually the model import framework will turn eager on as needednwithout the user needing to be involved.n4. Each SameDiff instance will have a separate ArrayHoldernthat will be used for looking up ndarrays relevantnto the eager computation. This will not use proper sessionsnbut instead store that will be used once for computing shapes.n"}
{"File Name":"modernisation-platform\/0009-use-secrets-manager-for-secrets.md","Context":"## Context\nThe Modernisation Platform team and its users need a way to store secrets securely. There are several different methods currently used across the MoJ, including [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/), [Parameter Store](https:\/\/docs.aws.amazon.com\/systems-manager\/latest\/userguide\/systems-manager-parameter-store.html), [LastPass](https:\/\/www.lastpass.com\/) and [Git-Crypt](https:\/\/github.com\/AGWA\/git-crypt).\nThere are also other well known industry solutions such as [HashiCorp Vault](https:\/\/www.vaultproject.io\/). We want to have a consistent solution across the Modernisation Platform.\n","Decision":"We've decided to use [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/) for our secrets storage.\\nParameter store can be used to store non secret parameters if needed for environment specific configuration, but the first choice should be using an app_variables.json like [here](https:\/\/github.com\/ministryofjustice\/modernisation-platform-environments\/blob\/main\/terraform\/environments\/sprinkler\/app_variables.json)\\n","tokens":138,"id":3142,"text":"## Context\\nThe Modernisation Platform team and its users need a way to store secrets securely. There are several different methods currently used across the MoJ, including [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/), [Parameter Store](https:\/\/docs.aws.amazon.com\/systems-manager\/latest\/userguide\/systems-manager-parameter-store.html), [LastPass](https:\/\/www.lastpass.com\/) and [Git-Crypt](https:\/\/github.com\/AGWA\/git-crypt).\\nThere are also other well known industry solutions such as [HashiCorp Vault](https:\/\/www.vaultproject.io\/). We want to have a consistent solution across the Modernisation Platform.\\n\n\n##Decision\nWe've decided to use [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/) for our secrets storage.\\nParameter store can be used to store non secret parameters if needed for environment specific configuration, but the first choice should be using an app_variables.json like [here](https:\/\/github.com\/ministryofjustice\/modernisation-platform-environments\/blob\/main\/terraform\/environments\/sprinkler\/app_variables.json)\\n","Predictions":"We've decided to use [Secrets Manager](https:\/\/aws.amazon.com\/secrets-manager\/) for our secrets storage.nParameter store can be used to store non secret parameters if needed for environment specific configuration, but the first choice should be using an app_variables.json like [here](https:\/\/github.com\/ministryofjustice\/modernisation-platform-environments\/blob\/main\/terraform\/environments\/sprinkler\/app_variables.json)n"}
{"File Name":"docnow-infra\/0002-use-terraform-workspaces-for-environments.md","Context":"## Context\nThere are two primary patterns of use when managing multiple environments (staging, prod, etc) in Terraform. The first is to use multiple directories--one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https:\/\/www.terraform.io\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform. The latter which we will use here allows a DRY approach across the environment.\n","Decision":"Use workspaces to manage multiple environments.\\n","tokens":129,"id":328,"text":"## Context\\nThere are two primary patterns of use when managing multiple environments (staging, prod, etc) in Terraform. The first is to use multiple directories--one for each environment. This has the advantage of being explicit, with an associated cost of repeated TF configuration. The second alternative uses TF workspaces to switch between environments. This option appears to be [recommended](https:\/\/www.terraform.io\/docs\/enterprise\/guides\/recommended-practices\/part1.html#one-workspace-per-environment-per-terraform-configuration) by Terraform. The latter which we will use here allows a DRY approach across the environment.\\n\n\n##Decision\nUse workspaces to manage multiple environments.\\n","Predictions":"Use a single tier for a single EC2 instancen"}
{"File Name":"heptaconnect-docs\/2021-10-30-route-capabilities.md","Context":"## Context\nRoutes define directions for data to flow.\nThe interpretation or use-case for a flow can be different for various reasons.\nIn general, we support read-multiple-times write-multiple-times scenarios, and they are very generous in options to work with but often needs to be limited in integrations.\nLimitations like transferring data only once or transferring data for finding the equivalent on the target are missing but requested.\nWe need a way to configure route behaviour in core without adding more work to the integrators.\n","Decision":"All limitations (e.g. transferring once) will be implemented as skipping existing steps.\\nThese changes in behaviour can be represented by simple boolean flags.\\nEvery step that is not a limitation will result in further flow components that will get a boolean flag.\\n","tokens":106,"id":3210,"text":"## Context\\nRoutes define directions for data to flow.\\nThe interpretation or use-case for a flow can be different for various reasons.\\nIn general, we support read-multiple-times write-multiple-times scenarios, and they are very generous in options to work with but often needs to be limited in integrations.\\nLimitations like transferring data only once or transferring data for finding the equivalent on the target are missing but requested.\\nWe need a way to configure route behaviour in core without adding more work to the integrators.\\n\n\n##Decision\nAll limitations (e.g. transferring once) will be implemented as skipping existing steps.\\nThese changes in behaviour can be represented by simple boolean flags.\\nEvery step that is not a limitation will result in further flow components that will get a boolean flag.\\n","Predictions":"We will use a react-response method to handle the response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-response-"}
{"File Name":"tove\/adr-03.md","Context":"## Context\nWhen a transcription is approved, a set of flat files containing the transcription data will be saved to Azure. Users will have the option to download a zip file containing their requested subject, group, workflow, or project. Depending on the speed at which we are able to zip the necessary files, we will either trigger a direct download, or provide a link to the location of the zip file to the user.\nThe goal is to investigate Azure\u2019s storage options (specifically Blob Storage and File Services) and decide which tool is best suited for our needs.\n### Factors to consider:\n* How easy is it to share a file to the end user? What is the process for this?\n* Ease of use, how complicated is it to set up, maintain, edit\n* access permission features\n* Speed of accessing and iterating through files (e.g. getting all files in a given directory)\n### Terminology:\n**Blob:** acronym for \u201cBinary Large Object\u201d\n**Container:** synonym for \u201dS3 Bucket\u201d\n**Shared Access Signature:** similar functionality as \u201cS3 Presigned URLs\u201d\n","Decision":"We don't appear to have any need for most of the additional functionality that comes with File Service, which makes me reluctant to want to use it. In addition, the number of articles and resources available on communicating with Blob Storage to set up file zipping is much greater than what's available for File Service. My initial understanding of Blob Storage led me to believe that permissions could only be set at the container level, but this turned out to be wrong. With the ability to set blob-specific permissions, we will be able to use a single container to store the transcription-specific files, and the user-requested zip files.\\nUltimately, my choice is to go with Blob Storage: the more basic, simple storage tool that gives us what we need and nothing more. That being said, I'd still like to keep the option of using Azure File Service on the table, in case it turns out that we *would* benefit from the additional functionality that it offers.\\nAs for what type of blob we will use, my choice would be to store each data file in its own block blob. If we were to choose to store multiple files within a single blob (and have each file be associated with a block ID on that blob), we would lose the ability to name each individual file. Hypothetically, it would be possible to create a database table with columns \u201cblock ID\u201d and \u201cname\u201d, to emulate a naming functionality, but this seems far more complicated than its worth. In addition, the [azure-storage-blob](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob) gem gives us a simple interface for working with block blobs and saves us the trouble of having to write HTTP requests ourselves.\\nFinal questions:\\n1. Q: Blob Storage doesn't have any concrete hierarchy beyond Storage Account\/Blob Container - within a container, directories are virtual, demarcated by prefixes in the file name. Will this end up being problematic for us? Will it complicate file retrieval?\\nA: Retrieving files from a file system with virtual directories shouldn't be any different than retrieving files from a normal file system. As long as blob prefixes are constructed in a way that reflects the organizational system used within the application\/database, there should be no trouble. File retrieval may be helped by append blobs - final decision on blob type is still TBD.\\n2. Q: Would there be any benefit to caching files on on-premises file servers? If this sounds like something we'd like to employ, it would be worth reconsidering Azure File Service.\\nA: This doesn't appear to be something we will need.\\n### Links and Articles:\\n1. [Microsoft: Deciding when to use Azure Blobs, Azure Files, or Azure Disks](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-decide-blobs-files-disks)\\n2. [Azure Files FAQ](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/files\/storage-files-faq) (see \u2018Why would I use an Azure file share versus Azure Blob Storage for my data?\u2019)\\n3. [Stack Overflow: Blob Storage vs File Service](https:\/\/stackoverflow.com\/questions\/24880430\/azure-blob-storage-vs-file-service)\\n4. [Microsoft: Introducing Azure File Service](https:\/\/blogs.msdn.microsoft.com\/windowsazurestorage\/2014\/05\/12\/introducing-microsoft-azure-file-service\/) (scroll to When to use Azure Files vs Azure Blobs vs Azure Disks)\\n5. [Microsoft: Azure Storage scalability and performance targets for storage accounts](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-scalability-targets)\\n6. [Azure Blob Overview](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-overview)\\n7. [Azure Blob Introduction](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-introduction)\\n8. [How to mount Blob storage as a file system with blobfuse](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-how-to-mount-container-linux)\\n9. [Block blobs, append blobs, and page blobs](https:\/\/docs.microsoft.com\/en-us\/rest\/api\/storageservices\/understanding-block-blobs--append-blobs--and-page-blobs)\\n10. [Azure Blob Storage gem](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob)\\n","tokens":231,"id":4533,"text":"## Context\\nWhen a transcription is approved, a set of flat files containing the transcription data will be saved to Azure. Users will have the option to download a zip file containing their requested subject, group, workflow, or project. Depending on the speed at which we are able to zip the necessary files, we will either trigger a direct download, or provide a link to the location of the zip file to the user.\\nThe goal is to investigate Azure\u2019s storage options (specifically Blob Storage and File Services) and decide which tool is best suited for our needs.\\n### Factors to consider:\\n* How easy is it to share a file to the end user? What is the process for this?\\n* Ease of use, how complicated is it to set up, maintain, edit\\n* access permission features\\n* Speed of accessing and iterating through files (e.g. getting all files in a given directory)\\n### Terminology:\\n**Blob:** acronym for \u201cBinary Large Object\u201d\\n**Container:** synonym for \u201dS3 Bucket\u201d\\n**Shared Access Signature:** similar functionality as \u201cS3 Presigned URLs\u201d\\n\n\n##Decision\nWe don't appear to have any need for most of the additional functionality that comes with File Service, which makes me reluctant to want to use it. In addition, the number of articles and resources available on communicating with Blob Storage to set up file zipping is much greater than what's available for File Service. My initial understanding of Blob Storage led me to believe that permissions could only be set at the container level, but this turned out to be wrong. With the ability to set blob-specific permissions, we will be able to use a single container to store the transcription-specific files, and the user-requested zip files.\\nUltimately, my choice is to go with Blob Storage: the more basic, simple storage tool that gives us what we need and nothing more. That being said, I'd still like to keep the option of using Azure File Service on the table, in case it turns out that we *would* benefit from the additional functionality that it offers.\\nAs for what type of blob we will use, my choice would be to store each data file in its own block blob. If we were to choose to store multiple files within a single blob (and have each file be associated with a block ID on that blob), we would lose the ability to name each individual file. Hypothetically, it would be possible to create a database table with columns \u201cblock ID\u201d and \u201cname\u201d, to emulate a naming functionality, but this seems far more complicated than its worth. In addition, the [azure-storage-blob](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob) gem gives us a simple interface for working with block blobs and saves us the trouble of having to write HTTP requests ourselves.\\nFinal questions:\\n1. Q: Blob Storage doesn't have any concrete hierarchy beyond Storage Account\/Blob Container - within a container, directories are virtual, demarcated by prefixes in the file name. Will this end up being problematic for us? Will it complicate file retrieval?\\nA: Retrieving files from a file system with virtual directories shouldn't be any different than retrieving files from a normal file system. As long as blob prefixes are constructed in a way that reflects the organizational system used within the application\/database, there should be no trouble. File retrieval may be helped by append blobs - final decision on blob type is still TBD.\\n2. Q: Would there be any benefit to caching files on on-premises file servers? If this sounds like something we'd like to employ, it would be worth reconsidering Azure File Service.\\nA: This doesn't appear to be something we will need.\\n### Links and Articles:\\n1. [Microsoft: Deciding when to use Azure Blobs, Azure Files, or Azure Disks](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-decide-blobs-files-disks)\\n2. [Azure Files FAQ](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/files\/storage-files-faq) (see \u2018Why would I use an Azure file share versus Azure Blob Storage for my data?\u2019)\\n3. [Stack Overflow: Blob Storage vs File Service](https:\/\/stackoverflow.com\/questions\/24880430\/azure-blob-storage-vs-file-service)\\n4. [Microsoft: Introducing Azure File Service](https:\/\/blogs.msdn.microsoft.com\/windowsazurestorage\/2014\/05\/12\/introducing-microsoft-azure-file-service\/) (scroll to When to use Azure Files vs Azure Blobs vs Azure Disks)\\n5. [Microsoft: Azure Storage scalability and performance targets for storage accounts](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/common\/storage-scalability-targets)\\n6. [Azure Blob Overview](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-overview)\\n7. [Azure Blob Introduction](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-blobs-introduction)\\n8. [How to mount Blob storage as a file system with blobfuse](https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/storage-how-to-mount-container-linux)\\n9. [Block blobs, append blobs, and page blobs](https:\/\/docs.microsoft.com\/en-us\/rest\/api\/storageservices\/understanding-block-blobs--append-blobs--and-page-blobs)\\n10. [Azure Blob Storage gem](https:\/\/github.com\/azure\/azure-storage-ruby\/tree\/master\/blob)\\n","Predictions":"We will use a Container container to store the files.n"}
{"File Name":"cloud_controller_ng\/0003-switching-to-request-specs-for-controllers.md","Context":"Context\n-------\nThere are two approaches to unit testing controllers, neither of which is very useful: If we mock most of the\ndependencies of the controller, our tests become tightly coupled to the implementation and do not give us a\nlot of confidence. If we make them more integration-style tests, we end up duplicating lots of tests between the\ncontroller and the request specs.\nAdditionally, writing controller specs is a discouraged pattern:\n> Controller specs can be used to describe the behaviour of Rails controllers. As of version 3.5, however, controller specs are discouraged in favour of request specs (which also focus largely on controllers, but capture other critical aspects of application behaviour as well). Controller specs will continue to be supported until at least version 4.0 (see the release notes for details).\nFrom [RSpec\u200a\u2014\u200aController or Request Specs?][]\n**Warning**: Request specs in the `cloud_controller_ng` codebase are not actually Rspec\/Rails request specs (which would be [declared with `type: :request`](https:\/\/relishapp.com\/rspec\/rspec-rails\/docs\/request-specs\/request-spec)), but use [Rack Test](https:\/\/github.com\/rack-test\/rack-test).  Rack Test has similar looking helper methods for making requests (e.g. `get`, `post`, etc), but these take different arguments than the equivalent Rspec methods.\nDecision\n--------\nMoving forward, we will only write request specs for controllers.\nConsequences\n------------\n- Reduced duplication in testing controllers\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\n- Delete controller specs once they've been transitioned to request specs.\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\n","Decision":"--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n","tokens":403,"id":1829,"text":"Context\\n-------\\nThere are two approaches to unit testing controllers, neither of which is very useful: If we mock most of the\\ndependencies of the controller, our tests become tightly coupled to the implementation and do not give us a\\nlot of confidence. If we make them more integration-style tests, we end up duplicating lots of tests between the\\ncontroller and the request specs.\\nAdditionally, writing controller specs is a discouraged pattern:\\n> Controller specs can be used to describe the behaviour of Rails controllers. As of version 3.5, however, controller specs are discouraged in favour of request specs (which also focus largely on controllers, but capture other critical aspects of application behaviour as well). Controller specs will continue to be supported until at least version 4.0 (see the release notes for details).\\nFrom [RSpec\u200a\u2014\u200aController or Request Specs?][]\\n**Warning**: Request specs in the `cloud_controller_ng` codebase are not actually Rspec\/Rails request specs (which would be [declared with `type: :request`](https:\/\/relishapp.com\/rspec\/rspec-rails\/docs\/request-specs\/request-spec)), but use [Rack Test](https:\/\/github.com\/rack-test\/rack-test).  Rack Test has similar looking helper methods for making requests (e.g. `get`, `post`, etc), but these take different arguments than the equivalent Rspec methods.\\nDecision\\n--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n\n\n##Decision\n--------\\nMoving forward, we will only write request specs for controllers.\\nConsequences\\n------------\\n- Reduced duplication in testing controllers\\n- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.\\n- Delete controller specs once they've been transitioned to request specs.\\n[RSpec\u200a\u2014\u200aController or Request Specs?]: https:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563ef11\\n","Predictions":"--------nWe will only write request specs for controllers.nConsequencesn------------n- Reduced duplication in testing controllersn- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.n- Delete controller specs once they've been transitioned to request specs.n[RSpec \u2014 Controller or Request Specs?][]nhttps:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563n### Positive Consequencesn------------n- Reduced duplication in testing controllersn- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.n- Delete controller specs once they've been transitioned to request specs.n[RSpec \u2014 Controller or Request Specs?][]nhttps:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563n### Negative Consequencesn------------n- Reduced duplication in testing controllersn- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.n- Delete controller specs once they've been transitioned to request specs.n[RSpec \u2014 Controller or Request Specs?][]nhttps:\/\/medium.com\/just-tech\/rspec-controller-or-request-specs-d93ef563n### Negative Consequencesn------------n- Reduced duplication in testing controllersn- Controller spec helpers need to be adjusted to no longer mock out CC app directly - ie they need to be transformed into Request spec helpers.n"}
{"File Name":"klokwrk-project\/0014-commit-message-format.md","Context":"## Context\nUsing commit messages without any structure looks convenient for developers as they do not have to think about messages too much. Unfortunately, that freedom and lack of thinking can impose some\nadditional burden on long-term project maintenance.\nQuite often, we can find incomprehensible commit messages that do not communicate anything useful. Hopefully, imposing some lightweight rules and guidance will help developers create commit messages\nthat are helpful for their colleagues.\nIn addition, with unstructured commit messages, there is much less opportunity to introduce any tools on top of commit history. For example, we would like to employ an automated changelog generator\nbased on extracting some semantical meaning from commits, but this will not work if commit messages lack any structure. Without the commit message structure, we can just dump the commit log in the\nchangelog, which does not make the changelog more helpful than looking at the history of commits in the first place.\n### Architectural Context\n* System (`klokwrk-project`)\n","Decision":"**We will use a customized [conventional commits](https:\/\/www.conventionalcommits.org\/en\/v1.0.0\/) format for writing commit messages.**\\nConventional commits format is nice and short and defines the simple structure that is easy to learn and follow. Here is basic structure of our customized conventional commits format:\\n<type>(optional <scope>): <description> {optional <metadata>}\\nOur customization:\\n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96b9\/CONTRIBUTING.md#-commit-message-guidelines)\\n- allows adding additional metadata in the message title if useful and appropriate (see details section for more info)\\n- requires format compliance only for messages of \"significant\" commits (see details section for more info)\\n### Decision details\\nDetails about the decision are given mainly as a set of strong recommendations (strongly recommended) and rules enforced by tooling (rule). In our case, the tooling is implemented as git commit hooks.\\nEvery contributor should install git hooks provided in this repository. That can be done with following command (executed from the project root):\\ngit config core.hooksPath support\/git\/hooks\\nThere might be cases when implemented rules are not appropriate and should be updated or removed or just temporarily ignored. In such scenarios, hooks can be skipped with git's `--no-verify` option.\\nWhile describing details, following terms are used as described:\\n- *commit message title*: refers to the first line of a commit message\\n- *commit message description*: refers to the part of the title describing a commit with human-readable message. In conventional commits specification that part is called `description`.\\n#### General guidance and rules for all commit messages\\n- (strongly recommended) - avoid trivial commit messages titles or descriptions\\n- (strongly recommended) - use imperative mood in title or description (add instead of adding or added, update instead of updating or updated etc.) as you are spelling out a command\\n- (rule) - message title or description must start with the uppercase letter <br\/>\\n<br\/>\\nThe main reason is a desire for better readability as we want easily spot the beginning message description or title. There are some arguments for using the lowercase like \"message titles are not\\nsentences\". While this is true, we prefer to have better readability than comply with some vague constraints.<br\/>\\n<br\/>\\n- (rule) - message title or description should not end with common punctuation characters: `.!?`\\n- (strongly recommended) - message title or description should not be comprised of multiple sentences\\n- (rule) - message title should not be longer than 120 characters. Use the message body if more space for description is needed<br\/>\\n<br\/>\\nActually, there is a common convention that we should not use more than 69 characters in the message title. It looks like the main reason for it is that GitHub truncates anything above 69 chars\\nfrom message titles. Having such a tight constraint seems unreasonable today, and the apparent shortcomings of any tool shouldn't restrict us, even if the tool is GitHub.<br\/>\\n<br\/>\\n- (strongly recommended) - commit message title or description should describe \"what\" (and sometimes \"why\"), instead of \"how\"<br\/>\\n<br\/>\\nFor describing \"why\", the message body is more appropriate as we have more space there. If needed, the message body may contain \"how\" too, but it should be clearly separated (at least with a blank\\nline) from \"what\" and \"why\".<br\/>\\n<br\/>\\n- (recommended) - commit message title should provide optional scope (from conventional commit specification) if applicable\\n- if commit refers to multiple scopes, scopes should be separated with `\/` character\\n- if commit refers to the work which influences the whole project, the scope should be `project` or it can be left out\\n- the scope should be a single word in lowercase<br\/>\\n<br\/>\\n- (strongly recommended) - message body must be separated from message title with a single blank line\\n- (option) - message body can contain additional blank lines\\n- (recommended) - message body should not use lines longer than 150 characters\\n- (strongly recommended) - include relevant references to issues or pull request to the metadata section of message title<br\/>\\n<br\/>\\nExample: `feat(some-module): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}`<br\/>\\n<br\/>\\n- (option) - include relevant feature\/bug ticket links in message footer according to conventional commits guidelines<br\/>\\n<br\/>\\nFooter is separated from body with a single blank line.\\n#### Guidance and rules for \"normal\" commits to the main development branch\\n- (rule) - all commits to the main development branch must have a message title in customized conventional commit format\\n#### Guidance and rules for merge commits to the main development branch\\nWhen used with [semi-linear commit history](.\/0007-git-workflow-with-linear-history.md), merge commits are the primary carriers of completed work units. As such, they are the most interesting for\\ncreating a changelog.\\nBefore merging, merge commits must be rebased against main development branch, and merging must be executed with no-fast-forward option (`--no-ff`).\\n- (rule) - merge commits must have 'merge' metadata (`{m}`) present at the end of the title <br\/>\\n<br\/>\\nThat way, merge commits can be easily distinguished on GitHub and in the changelog.\\n- (option) - merge commit metadata can carry additional information related to the issues and PRs references like in the following example\\nfeat(klokwrk-tool-gradle-source-repack): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}\\nHere, `i#123` is a reference to the issue, while `pr#12` is a reference to the pull request. Additional metadata are not controlled or enforced by git hooks.\\n#### Guidance and rules for normal commits to the feature branches\\n- (option) - normal commits don't have to follow custom conventional commits format for message title\\n- (strongly recommended) - normal commits should use conventional commits format when contained change is significant enough on its own to be placed in the changelog\\nWhen all useful changelog entries are contained in normal commits of a feature branch, we can do two different things depending on the situation:\\n- use merge commit with type of `notype`. Such merge commit will be ignored when creating a changelog.\\n- merge a branch with fast-forward option (no merge commit will be present)\\nPreferably, use `notype` merge commits, as they are still useful for clear separation of related work.\\n#### Types for conventional commits format\\n- common (angular)\\n- `feat` or `feature` - a new feature\\n- `fix` - a bug fix\\n- `docs` - documentation only changes\\n- `style` - changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)\\n- `test` - adding missing tests or correcting existing tests\\n- `build` - changes that affect the build system or external dependencies\\n- `ci` - changes to our CI configuration files and scripts\\n- `refactor` - a code change that neither fixes a bug nor adds a feature\\n- `perf` - a code change that improves performance\\n- `chore` - routine task\\n- custom\\n- `enhance` or `enhancement` - improvements to the existing features\\n- `deps` - dependencies updates (use instead of `build` when commit only updates dependencies)<br\/>\\n<br\/>\\nThere are two main scenarios when upgrading dependencies, a simple version bump and the more involved upgrade requiring resolving various issues like compilation errors, API upgrades, etc.<br\/>\\n<br\/>\\nSimple version bumps should be contained in a single individual commit with a description message starting with the word \"Bump\". For example: `deps: Bump Micronaut to 2.5.2 version`.<br\/>\\n<br\/>\\nMore complicated upgrades should be organized as feature branches where each non-conventional commit resolves a single step in the process. When finished, the feature branch should be merged\\ninto the main development branch with a description starting with the word \"Upgrade\". For example: `deps: Upgrade Spring Boot to 2.5.0 version {m}`.<br\/>\\n<br\/>\\n- `task` - same meaning as `chore`. Prefer using `task`.\\n- `article` - use instead of `docs` when changes are related only to articles\\n- `misc` - anything that does not fit into previous categories\\n- `notype` - only for merge commits in situations where contained plain commits carries all relevant types and merge commit doesn't add anything useful. It is ignored in changelog.<br\/>\\n<br\/>\\nTypical example is when we are updating number of dependencies through commits in a feature branch. In this situation each feature branch commit should contain updates to a single dependency\\nand communicate the update through its own commit message in conventional commit format. When all updates in the branch are tested, we can merge them in the main branch. However, that merge\\nshould be ignored by changelog as it does not communicate anything new.\\n","tokens":205,"id":4890,"text":"## Context\\nUsing commit messages without any structure looks convenient for developers as they do not have to think about messages too much. Unfortunately, that freedom and lack of thinking can impose some\\nadditional burden on long-term project maintenance.\\nQuite often, we can find incomprehensible commit messages that do not communicate anything useful. Hopefully, imposing some lightweight rules and guidance will help developers create commit messages\\nthat are helpful for their colleagues.\\nIn addition, with unstructured commit messages, there is much less opportunity to introduce any tools on top of commit history. For example, we would like to employ an automated changelog generator\\nbased on extracting some semantical meaning from commits, but this will not work if commit messages lack any structure. Without the commit message structure, we can just dump the commit log in the\\nchangelog, which does not make the changelog more helpful than looking at the history of commits in the first place.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n\n##Decision\n**We will use a customized [conventional commits](https:\/\/www.conventionalcommits.org\/en\/v1.0.0\/) format for writing commit messages.**\\nConventional commits format is nice and short and defines the simple structure that is easy to learn and follow. Here is basic structure of our customized conventional commits format:\\n<type>(optional <scope>): <description> {optional <metadata>}\\nOur customization:\\n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96b9\/CONTRIBUTING.md#-commit-message-guidelines)\\n- allows adding additional metadata in the message title if useful and appropriate (see details section for more info)\\n- requires format compliance only for messages of \"significant\" commits (see details section for more info)\\n### Decision details\\nDetails about the decision are given mainly as a set of strong recommendations (strongly recommended) and rules enforced by tooling (rule). In our case, the tooling is implemented as git commit hooks.\\nEvery contributor should install git hooks provided in this repository. That can be done with following command (executed from the project root):\\ngit config core.hooksPath support\/git\/hooks\\nThere might be cases when implemented rules are not appropriate and should be updated or removed or just temporarily ignored. In such scenarios, hooks can be skipped with git's `--no-verify` option.\\nWhile describing details, following terms are used as described:\\n- *commit message title*: refers to the first line of a commit message\\n- *commit message description*: refers to the part of the title describing a commit with human-readable message. In conventional commits specification that part is called `description`.\\n#### General guidance and rules for all commit messages\\n- (strongly recommended) - avoid trivial commit messages titles or descriptions\\n- (strongly recommended) - use imperative mood in title or description (add instead of adding or added, update instead of updating or updated etc.) as you are spelling out a command\\n- (rule) - message title or description must start with the uppercase letter <br\/>\\n<br\/>\\nThe main reason is a desire for better readability as we want easily spot the beginning message description or title. There are some arguments for using the lowercase like \"message titles are not\\nsentences\". While this is true, we prefer to have better readability than comply with some vague constraints.<br\/>\\n<br\/>\\n- (rule) - message title or description should not end with common punctuation characters: `.!?`\\n- (strongly recommended) - message title or description should not be comprised of multiple sentences\\n- (rule) - message title should not be longer than 120 characters. Use the message body if more space for description is needed<br\/>\\n<br\/>\\nActually, there is a common convention that we should not use more than 69 characters in the message title. It looks like the main reason for it is that GitHub truncates anything above 69 chars\\nfrom message titles. Having such a tight constraint seems unreasonable today, and the apparent shortcomings of any tool shouldn't restrict us, even if the tool is GitHub.<br\/>\\n<br\/>\\n- (strongly recommended) - commit message title or description should describe \"what\" (and sometimes \"why\"), instead of \"how\"<br\/>\\n<br\/>\\nFor describing \"why\", the message body is more appropriate as we have more space there. If needed, the message body may contain \"how\" too, but it should be clearly separated (at least with a blank\\nline) from \"what\" and \"why\".<br\/>\\n<br\/>\\n- (recommended) - commit message title should provide optional scope (from conventional commit specification) if applicable\\n- if commit refers to multiple scopes, scopes should be separated with `\/` character\\n- if commit refers to the work which influences the whole project, the scope should be `project` or it can be left out\\n- the scope should be a single word in lowercase<br\/>\\n<br\/>\\n- (strongly recommended) - message body must be separated from message title with a single blank line\\n- (option) - message body can contain additional blank lines\\n- (recommended) - message body should not use lines longer than 150 characters\\n- (strongly recommended) - include relevant references to issues or pull request to the metadata section of message title<br\/>\\n<br\/>\\nExample: `feat(some-module): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}`<br\/>\\n<br\/>\\n- (option) - include relevant feature\/bug ticket links in message footer according to conventional commits guidelines<br\/>\\n<br\/>\\nFooter is separated from body with a single blank line.\\n#### Guidance and rules for \"normal\" commits to the main development branch\\n- (rule) - all commits to the main development branch must have a message title in customized conventional commit format\\n#### Guidance and rules for merge commits to the main development branch\\nWhen used with [semi-linear commit history](.\/0007-git-workflow-with-linear-history.md), merge commits are the primary carriers of completed work units. As such, they are the most interesting for\\ncreating a changelog.\\nBefore merging, merge commits must be rebased against main development branch, and merging must be executed with no-fast-forward option (`--no-ff`).\\n- (rule) - merge commits must have 'merge' metadata (`{m}`) present at the end of the title <br\/>\\n<br\/>\\nThat way, merge commits can be easily distinguished on GitHub and in the changelog.\\n- (option) - merge commit metadata can carry additional information related to the issues and PRs references like in the following example\\nfeat(klokwrk-tool-gradle-source-repack): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}\\nHere, `i#123` is a reference to the issue, while `pr#12` is a reference to the pull request. Additional metadata are not controlled or enforced by git hooks.\\n#### Guidance and rules for normal commits to the feature branches\\n- (option) - normal commits don't have to follow custom conventional commits format for message title\\n- (strongly recommended) - normal commits should use conventional commits format when contained change is significant enough on its own to be placed in the changelog\\nWhen all useful changelog entries are contained in normal commits of a feature branch, we can do two different things depending on the situation:\\n- use merge commit with type of `notype`. Such merge commit will be ignored when creating a changelog.\\n- merge a branch with fast-forward option (no merge commit will be present)\\nPreferably, use `notype` merge commits, as they are still useful for clear separation of related work.\\n#### Types for conventional commits format\\n- common (angular)\\n- `feat` or `feature` - a new feature\\n- `fix` - a bug fix\\n- `docs` - documentation only changes\\n- `style` - changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)\\n- `test` - adding missing tests or correcting existing tests\\n- `build` - changes that affect the build system or external dependencies\\n- `ci` - changes to our CI configuration files and scripts\\n- `refactor` - a code change that neither fixes a bug nor adds a feature\\n- `perf` - a code change that improves performance\\n- `chore` - routine task\\n- custom\\n- `enhance` or `enhancement` - improvements to the existing features\\n- `deps` - dependencies updates (use instead of `build` when commit only updates dependencies)<br\/>\\n<br\/>\\nThere are two main scenarios when upgrading dependencies, a simple version bump and the more involved upgrade requiring resolving various issues like compilation errors, API upgrades, etc.<br\/>\\n<br\/>\\nSimple version bumps should be contained in a single individual commit with a description message starting with the word \"Bump\". For example: `deps: Bump Micronaut to 2.5.2 version`.<br\/>\\n<br\/>\\nMore complicated upgrades should be organized as feature branches where each non-conventional commit resolves a single step in the process. When finished, the feature branch should be merged\\ninto the main development branch with a description starting with the word \"Upgrade\". For example: `deps: Upgrade Spring Boot to 2.5.0 version {m}`.<br\/>\\n<br\/>\\n- `task` - same meaning as `chore`. Prefer using `task`.\\n- `article` - use instead of `docs` when changes are related only to articles\\n- `misc` - anything that does not fit into previous categories\\n- `notype` - only for merge commits in situations where contained plain commits carries all relevant types and merge commit doesn't add anything useful. It is ignored in changelog.<br\/>\\n<br\/>\\nTypical example is when we are updating number of dependencies through commits in a feature branch. In this situation each feature branch commit should contain updates to a single dependency\\nand communicate the update through its own commit message in conventional commit format. When all updates in the branch are tested, we can merge them in the main branch. However, that merge\\nshould be ignored by changelog as it does not communicate anything new.\\n","Predictions":"**We will use a customized [conventional commits](https:\/\/www.conventionalcommits.org\/en\/v1.0.0\/) format for writing commit messages.**nConventional commits format is nice and short and defines the simple structure that is easy to learn and follow. Here is basic structure of our customized conventional commits format:ntype>(optional scope>): description> optional metadata>nOur customization:n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96b9\/CONTRIBUTING.md#-commit-message-guidelines)n- allows adding additional metadata in the message title if useful and appropriate (see details section for more info)n- requires format compliance only for messages of \"significant\" commits (see details section for more info)n### Decision detailsnDetails about the decision are given mainly as a set of strong recommendations (strongly recommended) and rules enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In our case, the tooling is enforced by tooling (rule). In"}
{"File Name":"gsp\/ADR041-service-operated-policies.md","Context":"## Context\nOur service-operator allows service teams to provision various AWS services by\ndeclaratively defining resources and submitting them via the kubernetes api.\nSome of these resources require IAM to authorise how the provisioned service\ncan be used. The types of actions that can be performed on.\n#### Example\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\n```\n---\napiVersion: storage.govsvc.uk\/v1beta1\nkind: S3Bucket\nmetadata:\nname: s3-bucket-sample\nspec:\naws:\nLifecycleRules:\n- Expiration: 90days\nVersioning:\nEnabled: true\n```\nIn order to access a provisioned bucket via the the AWS SDK users will require\nan IAM role\/policy that allows access.\nWe want things like bucket ACL, versioning configuration and lifecycle policy\nto be defined declaratively via the resource manifest (see example above), and continuously managed\nby the service operator.\nWe want users of the provisioned bucket to be able to read back all\nconfiguration, and be able to fully utilise the specific bucket for reading,\nwriting and managing their objects within the provisioned bucket, but we want\nto avoid giving permissions to users that could cause conflicts with the\nproperties that are managed by the service operator's reconcile loop.\nFor example, given the example manifest above, we would like to avoid giving\npermissions that would allow a user to alter the Expiration LifeCycleRules,\nsince any changes the user made would be periodically overwritten by the\nservice operator's reconciliation.\n","Decision":"* We will provision policy that gives full access for users to _use_ the\\nprovisioned service.\\n* We will avoid provisioning policy that allows users to create, destroy or\\nconfigure the provisioned service, so that this can remain the declarative\\ndomain of the service-operator.\\n","tokens":335,"id":3907,"text":"## Context\\nOur service-operator allows service teams to provision various AWS services by\\ndeclaratively defining resources and submitting them via the kubernetes api.\\nSome of these resources require IAM to authorise how the provisioned service\\ncan be used. The types of actions that can be performed on.\\n#### Example\\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\\n```\\n---\\napiVersion: storage.govsvc.uk\/v1beta1\\nkind: S3Bucket\\nmetadata:\\nname: s3-bucket-sample\\nspec:\\naws:\\nLifecycleRules:\\n- Expiration: 90days\\nVersioning:\\nEnabled: true\\n```\\nIn order to access a provisioned bucket via the the AWS SDK users will require\\nan IAM role\/policy that allows access.\\nWe want things like bucket ACL, versioning configuration and lifecycle policy\\nto be defined declaratively via the resource manifest (see example above), and continuously managed\\nby the service operator.\\nWe want users of the provisioned bucket to be able to read back all\\nconfiguration, and be able to fully utilise the specific bucket for reading,\\nwriting and managing their objects within the provisioned bucket, but we want\\nto avoid giving permissions to users that could cause conflicts with the\\nproperties that are managed by the service operator's reconcile loop.\\nFor example, given the example manifest above, we would like to avoid giving\\npermissions that would allow a user to alter the Expiration LifeCycleRules,\\nsince any changes the user made would be periodically overwritten by the\\nservice operator's reconciliation.\\n\n\n##Decision\n* We will provision policy that gives full access for users to _use_ the\\nprovisioned service.\\n* We will avoid provisioning policy that allows users to create, destroy or\\nconfigure the provisioned service, so that this can remain the declarative\\ndomain of the service-operator.\\n","Predictions":"* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n* We will provide a service-specific service_name that is a generic name for the service.n"}
{"File Name":"adr-tools\/0009-help-scripts.md","Context":"## Context\nCurrently help text is generated by extracting specially formatted comments from the top of the command script.\nThis makes it easy for developers of the tool:  documentation and code is all in one place.\nBut, it means that help text cannot include calculated values, such as the location of files.\n","Decision":"Where necessary, help text can be generated by a script.\\nThe script will be called _adr_help_<command>_<subcommand>\\n","tokens":63,"id":3571,"text":"## Context\\nCurrently help text is generated by extracting specially formatted comments from the top of the command script.\\nThis makes it easy for developers of the tool:  documentation and code is all in one place.\\nBut, it means that help text cannot include calculated values, such as the location of files.\\n\n\n##Decision\nWhere necessary, help text can be generated by a script.\\nThe script will be called _adr_help_<command>_<subcommand>\\n","Predictions":"The decision is to use Prettier as the only code formatter for JavaScript-based projects.n"}
{"File Name":"winery\/0022-tosca-model-is-more-relaxed-than-the-xsd.md","Context":"## Context and Problem Statement\nThere is a data model for a) serializing\/deserializing the XML contents, b) internal backend handling, c) working with algorithms, d) communicating with the REST service.\nCurrently, this is the same model.\nThe UI might generate non-valid XML files (in the sence of not passing the XSD validation).\nFor instance, if a user creates a service template, that service template does not contain a topology template.\nFurthermore, a topolgoy template needs to have at least one node template.\n","Decision":"Chosen option: \"Keep one model and allow non-XSD validating models in `org.eclipse.winery.model.tosca`\", because\\n- XSD is meant for \"executable\" TOSCA definitions, not for intermediate modeling results\\n- currently too much effort to develop two models\\n","tokens":114,"id":4307,"text":"## Context and Problem Statement\\nThere is a data model for a) serializing\/deserializing the XML contents, b) internal backend handling, c) working with algorithms, d) communicating with the REST service.\\nCurrently, this is the same model.\\nThe UI might generate non-valid XML files (in the sence of not passing the XSD validation).\\nFor instance, if a user creates a service template, that service template does not contain a topology template.\\nFurthermore, a topolgoy template needs to have at least one node template.\\n\n\n##Decision\nChosen option: \"Keep one model and allow non-XSD validating models in `org.eclipse.winery.model.tosca`\", because\\n- XSD is meant for \"executable\" TOSCA definitions, not for intermediate modeling results\\n- currently too much effort to develop two models\\n","Predictions":"Chosen option: \"Keep one model and allow non-XSD validating models in org.eclipse.winery.model.tosca\", becausen- XSD is meant for \"executable\" TOSCA definitions, not for intermediate modeling resultsn- currently too much effort to develop two modelsn"}
{"File Name":"git-en-boite\/0016-handle-concurrent-git-operations","Context":"## Context\nConcurrent git operations can fail because git is locking the repo. The current implementation is waiting for jobs to finish, but multiple workers in different containers can still take jobs in the queue and proccess them in parallel.\n","Decision":"Reduce worker concurrency to 1 by using a single container (for now).\\n","tokens":47,"id":3497,"text":"## Context\\nConcurrent git operations can fail because git is locking the repo. The current implementation is waiting for jobs to finish, but multiple workers in different containers can still take jobs in the queue and proccess them in parallel.\\n\n\n##Decision\nReduce worker concurrency to 1 by using a single container (for now).\\n","Predictions":"We will continue to use the Heroku scheduler on a ten-minute interval, but will instead use the Heroku CLI to run the management command (.\/manage.py build_static_site) in a separate, detached worker dyno. This worker dyno has a lifespan of 24 hours and is not limited to the interval of the scheduled job.nThe command we will use is heroku run:detached -a app_name>.\/manage.py build_static_site.nProsn------n* Very easy to implement.n* Will allow static site builds that take up to 24 hours (which is greater than any reasonable build length).nConsn------n* We remain unable to responsively schedule static site builds at the moment they are needed and rely on the scheduler to kick off a build.n* We have to expose a Heroku API key and install the Heroku CLI buildpack on our main Publisher app.n"}
{"File Name":"opg-lpa\/0006-modernise-the-code-base.md","Context":"## Context\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\nPHP [appears to be on a downwards trend as a language](https:\/\/pypl.github.io\/PYPL.html?country=GB),\nespecially in contrast with Python. It's likely it will become increasingly difficult\nto find good PHP developers in future.\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\nlanguages of similar pedigree, such as Python.\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\nA mix of contractors and developers working on the code base over several years has\nresulted in a mix of styles and approaches. While we have already cleared out a lot\nof unused and\/or broken code, there is likely to be more we haven't found yet.\nWe are also lagging behind the latest Design System guidelines, as our application was one\nof the first to go live, before the current iteration of the Design System existed.\nThis means that any changes to design have to be done piecemeal and manually: we can't\nsimply import the newest version of the design system and have everything magically update.\nThis combination of factors means that the code base can be difficult to work with:\nresistant to change and easy to break.\n","Decision":"We have decided to modernise the code base to make it easier to work with and better\\naligned with modern web architecture and standards. This is not a small job, but\\nthe guiding principles we've decided on, shown below, should help us achieve our aims.\\n(\"Modernising the code base\" is not to be confused with \"modernising LPAs\". Here\\nwe're just talking about modernising the code base for the Make an LPA tool.)\\n* **Don't rewrite everything at once**\\nWhere possible, migrate part of an application to a new\\ncomponent and split traffic coming into the domain so that some paths are diverted to that\\ncomponent. This will typically use nginx in dev, but may be done at the AWS level if\\nappropriate (e.g in a load balancer or application gateway).\\nThis is challenging, but means that we don't have to do a \"big bang\" release of the new\\nversion of the tool. Our aim is to gradually replace existing components with new\\nones, which are (hopefully) simpler, future-proofed, more efficient, and don't rely on PHP.\\n* **Use Python for new work**\\nWe considered golang, but don't have the experience in the team to build applications with it.\\nWe felt that learning a new language + frameworks would only reduce our ability to deliver, with\\nminimal benefits: our application is not under heavy load and responds in an\\nacceptable amount of time, so golang's super efficiency isn't essential.\\nWe feel that we could scale horizontally if necessary and have not had any major issues\\nwith capacity in the past.\\n* **Choose containers or lambdas as appropriate**\\nUse a container for components which stay up most of the time, and lambdas for\\n\"bursty\" applications (e.g. background processes like PDF generation, daily statistics aggregation).\\n* **Choose the right lambda for the job**\\nUse \"pure\" lambdas where possible. This is only the case where an application has simple dependencies\\nwhich don't require unusual native libraries outside the\\n[stock AWS Docker images for lambdas](https:\/\/gallery.ecr.aws\/lambda\/python)).\\nIf a component is problematic to run as a pure lambda, use a lambda running a Docker image based\\non one of the stock AWS Docker images for lambdas.\\n* **Choose the right Docker image**\\nWhen using Docker images, prefer the following:\\n* Images based on AWS Lambda images (if writing a component which will run as a Docker lambda).\\n* Images based on Alpine (for other cases).\\n* Images based on a non-Alpine foundation like Ubuntu, but only if an Alpine image is not available.\\n* **Use Flask and gunicorn**\\nUse [Flask](https:\/\/flask.palletsprojects.com\/) for new Python web apps, fronted by\\n[gunicorn](https:\/\/gunicorn.org\/) for the WSGI implementation.\\n* **Use the latest Design System**\\nUse the [Government Design System](https:\/\/design-system.service.gov.uk\/) guidelines for new UI. In\\nparticular, use the\\n[Land Registry's Python implementation of the design system](https:\/\/github.com\/LandRegistry\/govuk-frontend-jinja),\\nwritten as [Jinja2 templates](https:\/\/jinja.palletsprojects.com\/).\\nOur aim should be to utilise it without modification as far as possible, so that we can easily upgrade\\nif it is changed by developers at the Land Registry.\\n* **Migrate legacy code to PHP 8**\\nWhere possible, upgrade PHP applications to PHP 8, when supported by [Laminas](https:\/\/getlaminas.org\/).\\nAt the time of writing, Laminas support for PHP 8 is only partial, so we are stuck with PHP 7 for now,\\nas large parts of our stack are implemented on top of Laminas.\\n* **Specify new APIs with OpenAPI**\\nSpecify new APIs using [OpenAPI](https:\/\/swagger.io\/specification\/). Ideally, use tooling\\nwhich enables an API to be automatically built from an OpenAPI specification, binding to\\ncode only when necessary, to avoid repetitive boilerplate.\\n* **Controlled, incremental releases**\\nProvision new infrastructure behind a feature flag wherever possible. This allows us to\\nwork on new components, moving them into the live environment as they are ready, but hidden\\nfrom the outside world. When ready for delivery, we switch the flag over to make that piece\\nof infrastructure live.\\n* **Follow good practices for web security**\\nBe aware of the [OWASP Top Ten](https:\/\/owasp.org\/www-project-top-ten\/) and code to avoid those\\nissues. Use tools like [Talisman](https:\/\/github.com\/GoogleCloudPlatform\/flask-talisman) to\\nimprove security.\\n* **Be mindful of accessibility**\\nConsider accessibility requirements at every step of the design and coding phases. Aim to\\ncomply with [WCAG 2.1 Level AA](https:\/\/www.w3.org\/WAI\/WCAG22\/quickref\/) as a minimum. While the\\nDesign System helps a lot with this, always bear accessibility in mind when building workflows\\nand custom components it doesn't cover.\\n* **Be properly open source**\\nMake the code base properly open source. While our code is open, there are still barriers to entry\\nfor developers outside the Ministry of Justice, such as the requirement to have access to AWS secrets,\\nS3, postcode API, the Government payment gateway and SendGrid for the system to work correctly. We\\nwill work towards removing these barriers so that onboarding of new developers (internally and\\nexternally) is seamless, and to enable potentially anyone to fully contribute to the project.\\n* **Improve test coverage everywhere**\\nAs we work on the code, be aware of gaps in testing and plug them as they arise. Don't wait for\\nan opportunity to fix everything at once: make refactoring and adding unit tests part of the\\nwork on an issue (unless it's going to take longer than working on the issue!).\\nWhere a whole category of testing is missing, add it (for example, we\\nhave recently implemented the foundations for load testing; see\\n[0004-implement-load-testing](.\/0004-implement-load-testing.md)).\\n* **Automate code quality metrics**\\nIntroduce tools to lint and scan code as we go, to ensure consistent, easy-to-follow code. See\\n[0003-linting-and-scanning](.\/0003-linting-and-scanning.md)) for a starting point.\\n* **Peer review everything**\\nAll commits to the code base must go through peer review before merging. No lone wolf developers.\\n* **Be pragmatic**\\nSee the [pragmatic quick reference](https:\/\/www.ccs.neu.edu\/home\/lieber\/courses\/csg110\/sp08\/Pragmatic%20Quick%20Reference.htm)\\nfor a summary. These are generally good principles for software engineering.\\n","tokens":306,"id":1652,"text":"## Context\\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\\nPHP [appears to be on a downwards trend as a language](https:\/\/pypl.github.io\/PYPL.html?country=GB),\\nespecially in contrast with Python. It's likely it will become increasingly difficult\\nto find good PHP developers in future.\\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\\nlanguages of similar pedigree, such as Python.\\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\\nA mix of contractors and developers working on the code base over several years has\\nresulted in a mix of styles and approaches. While we have already cleared out a lot\\nof unused and\/or broken code, there is likely to be more we haven't found yet.\\nWe are also lagging behind the latest Design System guidelines, as our application was one\\nof the first to go live, before the current iteration of the Design System existed.\\nThis means that any changes to design have to be done piecemeal and manually: we can't\\nsimply import the newest version of the design system and have everything magically update.\\nThis combination of factors means that the code base can be difficult to work with:\\nresistant to change and easy to break.\\n\n\n##Decision\nWe have decided to modernise the code base to make it easier to work with and better\\naligned with modern web architecture and standards. This is not a small job, but\\nthe guiding principles we've decided on, shown below, should help us achieve our aims.\\n(\"Modernising the code base\" is not to be confused with \"modernising LPAs\". Here\\nwe're just talking about modernising the code base for the Make an LPA tool.)\\n* **Don't rewrite everything at once**\\nWhere possible, migrate part of an application to a new\\ncomponent and split traffic coming into the domain so that some paths are diverted to that\\ncomponent. This will typically use nginx in dev, but may be done at the AWS level if\\nappropriate (e.g in a load balancer or application gateway).\\nThis is challenging, but means that we don't have to do a \"big bang\" release of the new\\nversion of the tool. Our aim is to gradually replace existing components with new\\nones, which are (hopefully) simpler, future-proofed, more efficient, and don't rely on PHP.\\n* **Use Python for new work**\\nWe considered golang, but don't have the experience in the team to build applications with it.\\nWe felt that learning a new language + frameworks would only reduce our ability to deliver, with\\nminimal benefits: our application is not under heavy load and responds in an\\nacceptable amount of time, so golang's super efficiency isn't essential.\\nWe feel that we could scale horizontally if necessary and have not had any major issues\\nwith capacity in the past.\\n* **Choose containers or lambdas as appropriate**\\nUse a container for components which stay up most of the time, and lambdas for\\n\"bursty\" applications (e.g. background processes like PDF generation, daily statistics aggregation).\\n* **Choose the right lambda for the job**\\nUse \"pure\" lambdas where possible. This is only the case where an application has simple dependencies\\nwhich don't require unusual native libraries outside the\\n[stock AWS Docker images for lambdas](https:\/\/gallery.ecr.aws\/lambda\/python)).\\nIf a component is problematic to run as a pure lambda, use a lambda running a Docker image based\\non one of the stock AWS Docker images for lambdas.\\n* **Choose the right Docker image**\\nWhen using Docker images, prefer the following:\\n* Images based on AWS Lambda images (if writing a component which will run as a Docker lambda).\\n* Images based on Alpine (for other cases).\\n* Images based on a non-Alpine foundation like Ubuntu, but only if an Alpine image is not available.\\n* **Use Flask and gunicorn**\\nUse [Flask](https:\/\/flask.palletsprojects.com\/) for new Python web apps, fronted by\\n[gunicorn](https:\/\/gunicorn.org\/) for the WSGI implementation.\\n* **Use the latest Design System**\\nUse the [Government Design System](https:\/\/design-system.service.gov.uk\/) guidelines for new UI. In\\nparticular, use the\\n[Land Registry's Python implementation of the design system](https:\/\/github.com\/LandRegistry\/govuk-frontend-jinja),\\nwritten as [Jinja2 templates](https:\/\/jinja.palletsprojects.com\/).\\nOur aim should be to utilise it without modification as far as possible, so that we can easily upgrade\\nif it is changed by developers at the Land Registry.\\n* **Migrate legacy code to PHP 8**\\nWhere possible, upgrade PHP applications to PHP 8, when supported by [Laminas](https:\/\/getlaminas.org\/).\\nAt the time of writing, Laminas support for PHP 8 is only partial, so we are stuck with PHP 7 for now,\\nas large parts of our stack are implemented on top of Laminas.\\n* **Specify new APIs with OpenAPI**\\nSpecify new APIs using [OpenAPI](https:\/\/swagger.io\/specification\/). Ideally, use tooling\\nwhich enables an API to be automatically built from an OpenAPI specification, binding to\\ncode only when necessary, to avoid repetitive boilerplate.\\n* **Controlled, incremental releases**\\nProvision new infrastructure behind a feature flag wherever possible. This allows us to\\nwork on new components, moving them into the live environment as they are ready, but hidden\\nfrom the outside world. When ready for delivery, we switch the flag over to make that piece\\nof infrastructure live.\\n* **Follow good practices for web security**\\nBe aware of the [OWASP Top Ten](https:\/\/owasp.org\/www-project-top-ten\/) and code to avoid those\\nissues. Use tools like [Talisman](https:\/\/github.com\/GoogleCloudPlatform\/flask-talisman) to\\nimprove security.\\n* **Be mindful of accessibility**\\nConsider accessibility requirements at every step of the design and coding phases. Aim to\\ncomply with [WCAG 2.1 Level AA](https:\/\/www.w3.org\/WAI\/WCAG22\/quickref\/) as a minimum. While the\\nDesign System helps a lot with this, always bear accessibility in mind when building workflows\\nand custom components it doesn't cover.\\n* **Be properly open source**\\nMake the code base properly open source. While our code is open, there are still barriers to entry\\nfor developers outside the Ministry of Justice, such as the requirement to have access to AWS secrets,\\nS3, postcode API, the Government payment gateway and SendGrid for the system to work correctly. We\\nwill work towards removing these barriers so that onboarding of new developers (internally and\\nexternally) is seamless, and to enable potentially anyone to fully contribute to the project.\\n* **Improve test coverage everywhere**\\nAs we work on the code, be aware of gaps in testing and plug them as they arise. Don't wait for\\nan opportunity to fix everything at once: make refactoring and adding unit tests part of the\\nwork on an issue (unless it's going to take longer than working on the issue!).\\nWhere a whole category of testing is missing, add it (for example, we\\nhave recently implemented the foundations for load testing; see\\n[0004-implement-load-testing](.\/0004-implement-load-testing.md)).\\n* **Automate code quality metrics**\\nIntroduce tools to lint and scan code as we go, to ensure consistent, easy-to-follow code. See\\n[0003-linting-and-scanning](.\/0003-linting-and-scanning.md)) for a starting point.\\n* **Peer review everything**\\nAll commits to the code base must go through peer review before merging. No lone wolf developers.\\n* **Be pragmatic**\\nSee the [pragmatic quick reference](https:\/\/www.ccs.neu.edu\/home\/lieber\/courses\/csg110\/sp08\/Pragmatic%20Quick%20Reference.htm)\\nfor a summary. These are generally good principles for software engineering.\\n","Predictions":"We have decided to modernise the code base to make it easier to work with and betternaligned with modern web architecture and standards. This is not a small job, butnthe guiding principles we've decided on, shown below, should help us achieve our aims.n(\"Modernising the code base\" is not to be confused with \"modernising LPAs\". Herenwe're just talking about modernising the code base for the Make an LPA tool.)n* **Don't rewrite everything at once**nWhere possible, migrate part of an application to a newncommonncommonnwhere possible, migrate part of an application to a newncommonn"}
{"File Name":"reaction-component-library\/0007-publish-components.md","Context":"## Context\nWe need to be able to pull in these components to other apps as one or more NPM packages. There are pros and cons to a single-package approach versus one package per component.\n### One Package Per Component\nPros:\n- Minimum dependencies pulled in with each component used. You don't download or package anything you aren't using.\n- When you need to pull in a fix to one component, you aren't unintentionally changing the behavior of a hundred other components.\nCons:\n- How do we track dependencies per component but also install them so that the whole Style Guide app can run as one?\n- Each component package has to be installed by developers as it is needed\n- Publishing them will be tricky, especially if there are any interdependencies. Lerna + semantic-release may help, but the typical Lerna repo structure may not be ideal with the context of the Style Guide app we have here.\n### A Single Package Exporting All Components\nPros:\n- Install a single package and you get access to them all in your app\n- Much simpler to publish vs multiple packages\n- Easier for people working on the style guide repo to understand.\nCons:\n- Every time you bump the dependency version of the component package, you'll pull in potentially changed versions of every component in your app.\n- Extra work will be required to ensure that bundled JavaScript does not include any components that an app does not use.\n### What about the style guide\nThere is a further complication here, which is \"how does the style guide release cycle relate to the release cycle of the components within it, and how does the style guide represent multiple versions of components as they change?\"\n","Decision":"The complexity of setting up, maintaining, and understanding a one-package-per-component approach, even with the help of tools like Lerna, is a very strong con. So if we assume that a single package is our preference, let's look at how we might mitigate the cons of that apprach.\\nThe first con, pulling in unwanted component changes, is MAJOR. This causes a lot of headaches for a lot of people. There are a few things we can do to avoid it:\\n- Every component change that changes the appearance of a component in any way should be exported as a new component. The whole component folder can be copied into a `v2` folder. That way, any app that pulls in an updated package will not see any changes until they change `import { Button } from \"@reactioncommerce\/components\/v1` to `import { Button } from \"@reactioncommerce\/components\/v2`\\n- No React component in this library will import any other React component in this library. All components that a component needs to render will be passed in as props or children.\\n- Every component will have Jest snapshot testing, which will give a clue to developers and reviewers that a component's appearance may have changed. Part of the review process can be deciding whether the changes are visual and whether they require splitting off a new version of the component (i.e. are considered \"breaking\" changes). There is the possibility of automating this even more through automated screen shot comparison.\\nThis approach also answers the question of how the style guide will show all versions of the component. Since all versions will remain in the repo in versioned folder names, they'll just naturally appear.\\nThe second con of a single package can be avoided by the following suggestion:\\n- Apps using the component package should set up a \"tree shaking\" solution to avoid bundling any components they do not import.\\n","tokens":350,"id":3031,"text":"## Context\\nWe need to be able to pull in these components to other apps as one or more NPM packages. There are pros and cons to a single-package approach versus one package per component.\\n### One Package Per Component\\nPros:\\n- Minimum dependencies pulled in with each component used. You don't download or package anything you aren't using.\\n- When you need to pull in a fix to one component, you aren't unintentionally changing the behavior of a hundred other components.\\nCons:\\n- How do we track dependencies per component but also install them so that the whole Style Guide app can run as one?\\n- Each component package has to be installed by developers as it is needed\\n- Publishing them will be tricky, especially if there are any interdependencies. Lerna + semantic-release may help, but the typical Lerna repo structure may not be ideal with the context of the Style Guide app we have here.\\n### A Single Package Exporting All Components\\nPros:\\n- Install a single package and you get access to them all in your app\\n- Much simpler to publish vs multiple packages\\n- Easier for people working on the style guide repo to understand.\\nCons:\\n- Every time you bump the dependency version of the component package, you'll pull in potentially changed versions of every component in your app.\\n- Extra work will be required to ensure that bundled JavaScript does not include any components that an app does not use.\\n### What about the style guide\\nThere is a further complication here, which is \"how does the style guide release cycle relate to the release cycle of the components within it, and how does the style guide represent multiple versions of components as they change?\"\\n\n\n##Decision\nThe complexity of setting up, maintaining, and understanding a one-package-per-component approach, even with the help of tools like Lerna, is a very strong con. So if we assume that a single package is our preference, let's look at how we might mitigate the cons of that apprach.\\nThe first con, pulling in unwanted component changes, is MAJOR. This causes a lot of headaches for a lot of people. There are a few things we can do to avoid it:\\n- Every component change that changes the appearance of a component in any way should be exported as a new component. The whole component folder can be copied into a `v2` folder. That way, any app that pulls in an updated package will not see any changes until they change `import { Button } from \"@reactioncommerce\/components\/v1` to `import { Button } from \"@reactioncommerce\/components\/v2`\\n- No React component in this library will import any other React component in this library. All components that a component needs to render will be passed in as props or children.\\n- Every component will have Jest snapshot testing, which will give a clue to developers and reviewers that a component's appearance may have changed. Part of the review process can be deciding whether the changes are visual and whether they require splitting off a new version of the component (i.e. are considered \"breaking\" changes). There is the possibility of automating this even more through automated screen shot comparison.\\nThis approach also answers the question of how the style guide will show all versions of the component. Since all versions will remain in the repo in versioned folder names, they'll just naturally appear.\\nThe second con of a single package can be avoided by the following suggestion:\\n- Apps using the component package should set up a \"tree shaking\" solution to avoid bundling any components they do not import.\\n","Predictions":"We will use a package approach to package components.nThe package will be a component that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable.nThe version variable will be a version variable that is loaded with a version variable."}
{"File Name":"sdbmss\/0002-build-crud-and-search.md","Context":"## Context\nThe Schoenberg Institute won three-year NEH grant to rebuild the existing Schoenberg Database of Manuscripts to\nreplace the then current SDBM which was written in ColdFusion, lacked critical data model features and user-based editing.\nThe new application needed a more flexible, user contribution, complex search and an up-to-date technology stack.\n","Decision":"The following technologies were select for the following reasons.\\n- Blacklight, which uses Solr, was chosen to provide complex, configurable search, and uses technology common in the library, and for which long term product support could be expected. Blacklight was also used for bookmarking.\\n- Rails is required by Blacklight\\n- MySQL was selected as it was the database of choice within library technology services, and supported by sysadmin staff\\n- Solr is required by Blacklight\\n- Delayed Job (URL) was chosen as for orchestrating background jobs, namely sending updates from the database to the Solr index\\n- Sunspot was chosen for the Rails-Solr interaction\\n- AngularJS was chosen to support complex, dynamic page interactions\\n- cancancan was selected to provide tiered, role-based access for users with different permission levels\\n","tokens":72,"id":1837,"text":"## Context\\nThe Schoenberg Institute won three-year NEH grant to rebuild the existing Schoenberg Database of Manuscripts to\\nreplace the then current SDBM which was written in ColdFusion, lacked critical data model features and user-based editing.\\nThe new application needed a more flexible, user contribution, complex search and an up-to-date technology stack.\\n\n\n##Decision\nThe following technologies were select for the following reasons.\\n- Blacklight, which uses Solr, was chosen to provide complex, configurable search, and uses technology common in the library, and for which long term product support could be expected. Blacklight was also used for bookmarking.\\n- Rails is required by Blacklight\\n- MySQL was selected as it was the database of choice within library technology services, and supported by sysadmin staff\\n- Solr is required by Blacklight\\n- Delayed Job (URL) was chosen as for orchestrating background jobs, namely sending updates from the database to the Solr index\\n- Sunspot was chosen for the Rails-Solr interaction\\n- AngularJS was chosen to support complex, dynamic page interactions\\n- cancancan was selected to provide tiered, role-based access for users with different permission levels\\n","Predictions":"The following technologies were select for the following reasons:n- Blacklight, which uses Solr, was chosen to provide complex, configurable search, and uses technology common in the library, and for which long term product support could be expected. Blacklight was also used for bookmarking.n- Rails is required by Blacklightn- MySQL was selected as it was the database of choice within library technology services, and supported by sysadmin staffn- Solr is required by Blacklightn- Delayed Job (URL) was chosen as for orchestrating background jobs, namely sending updates from the database to the Solr indexn- Sunspot was chosen for the Rails-Solr interactionn- AngularJS was chosen to support complex, dynamic page interactionsn- cancancan was selected to provide tiered, role-based access for users with different permission levelsn"}
{"File Name":"james-project\/0009-disable-elasticsearch-dynamic-mapping.md","Context":"## Context\nWe rely on dynamic mappings to expose our mail headers as a JSON map. Dynamic mapping is enabled for adding not yet encountered headers in the mapping.\nThis causes a serie of functional issues:\n- Maximum field count can easily be exceeded\n- Field type 'guess' can be wrong, leading to subsequent headers omissions [1]\n- Document indexation needs to be paused at the index level during mapping changes to avoid concurrent changes, impacting negatively performance.\n","Decision":"Rely on nested objects to represent mail headers within a mapping\\n","tokens":96,"id":2884,"text":"## Context\\nWe rely on dynamic mappings to expose our mail headers as a JSON map. Dynamic mapping is enabled for adding not yet encountered headers in the mapping.\\nThis causes a serie of functional issues:\\n- Maximum field count can easily be exceeded\\n- Field type 'guess' can be wrong, leading to subsequent headers omissions [1]\\n- Document indexation needs to be paused at the index level during mapping changes to avoid concurrent changes, impacting negatively performance.\\n\n\n##Decision\nRely on nested objects to represent mail headers within a mapping\\n","Predictions":"* Add the possibility to check for feature flags inside SCSS, similar to the twig implementation.n* The feature configuration from Feature::getAll() is converted to a SCSS map inside ShopwareStorefrontThemeThemeCompiler::getFeatureConfigScssMap.n* This SCSS map is always added to the SCSS string which gets processed by ShopwareStorefrontThemeThemeCompiler::compileTheme.n* For webpack hot-proxy the var\/config_js_features.json is used instead.n* The SCSS map looks like this: $sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);n* See https:\/\/sass-lang.com\/documentation\/values\/mapsn* A globally available function feature() is used instead.n* The SCSS map looks like this: $sw-features: (\"FEATURE_NEXT_1234\": false, \"FEATURE_NEXT_1235\": true);n* See https:\/\/sass-lang.com\/documentation\/values\/mapsn* A globally available function feature() is used instead.n"}
{"File Name":"webwritertechandhumanity.com\/0002-how-to-implement-special-pages.md","Context":"## Context\nI need to implement two special pages, the privacy policy page and the about me page.\nThey can't be in the map, I don't want them there because the user would\nbe forced to read them when scanning the spiral.\n","Decision":"I'll implement solution 1 because solution 2 is complicated and I want to prioritise\\nthe release of a first working website.\\n","tokens":52,"id":5001,"text":"## Context\\nI need to implement two special pages, the privacy policy page and the about me page.\\nThey can't be in the map, I don't want them there because the user would\\nbe forced to read them when scanning the spiral.\\n\n\n##Decision\nI'll implement solution 1 because solution 2 is complicated and I want to prioritise\\nthe release of a first working website.\\n","Predictions":"1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security api to store the user's id and password.n1. We will use a Security"}
{"File Name":"nr-arch\/2020-04-24-ARCH-OpenJDK-Versions.md","Context":"## Context\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\n* Status: proposed\n* Deciders: Licence Change\n* Date: 2020-08-27\nTechnical Story: [description | <https:\/\/apps.nrs.gov.bc.ca\/int\/jira\/browse\/ARCH-62]>\n## Context and Problem Statement\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\n## Decision Drivers\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\n* Java 10 is the suggested release by Oracle\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\n* Scope - IITD all server and client side applications owned by IITD that run Java.\n","Decision":"* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\nJDK 8 & Later\\nOracles OpenJDK JDK binaries for Windows, macOS, and Linux are available on release-specific pages of jdk.java.net as .tar.gz or .zip archives.\\nAs an example, the archives for JDK 13 may be found on jdk.java.net\/13 and may be extracted on the command line using\\n$ tar xvf openjdk-13*_bin.tar.gz\\nor\\n$ unzip openjdk-13*_bin.zip\\ndepending on the archive type.\\n### Positive Consequences\\n* removes the dependencies on Oracle JDK Licensing\\n* reduces security vulnerabilities of older JDK versions\\nJava 7 is still in predominant use. It goes without saying that any version of Java below 7 should be updated immediately even version 7 needs significant remediation for its fleet of vulnerabilities.\\nFurther vulnerabilities -\\n* <https:\/\/www.cvedetails.com\/product\/19117\/Oracle-JRE.html?vendor_id=93>\\n*\t<https:\/\/www.cvedetails.com\/product\/23642\/Oracle-Openjdk.html?vendor_id=93>\\n### Negative Consequences\\n* slow performance may occur\\n* migration issues will need to be addressed\\n* Migrate all Java JDK dependencies from Oracle JDK to OpenJDK.\\n* Upgrade all older versions to at least JDK 8, preference is to encourage teams to target move to JDK 11.\\n","tokens":403,"id":4802,"text":"## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https:\/\/apps.nrs.gov.bc.ca\/int\/jira\/browse\/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n\n\n##Decision\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java\/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade\/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\nJDK 8 & Later\\nOracles OpenJDK JDK binaries for Windows, macOS, and Linux are available on release-specific pages of jdk.java.net as .tar.gz or .zip archives.\\nAs an example, the archives for JDK 13 may be found on jdk.java.net\/13 and may be extracted on the command line using\\n$ tar xvf openjdk-13*_bin.tar.gz\\nor\\n$ unzip openjdk-13*_bin.zip\\ndepending on the archive type.\\n### Positive Consequences\\n* removes the dependencies on Oracle JDK Licensing\\n* reduces security vulnerabilities of older JDK versions\\nJava 7 is still in predominant use. It goes without saying that any version of Java below 7 should be updated immediately even version 7 needs significant remediation for its fleet of vulnerabilities.\\nFurther vulnerabilities -\\n* <https:\/\/www.cvedetails.com\/product\/19117\/Oracle-JRE.html?vendor_id=93>\\n*\t<https:\/\/www.cvedetails.com\/product\/23642\/Oracle-Openjdk.html?vendor_id=93>\\n### Negative Consequences\\n* slow performance may occur\\n* migration issues will need to be addressed\\n* Migrate all Java JDK dependencies from Oracle JDK to OpenJDK.\\n* Upgrade all older versions to at least JDK 8, preference is to encourage teams to target move to JDK 11.\\n","Predictions":"* Status: proposedn* Deciders: License Changen* Date: 2020-08-27nTechnical Story: [description | https:\/\/apps.nrs.gov.bc.ca\/int\/jira\/browse\/ARCH-62]>n"}
{"File Name":"exercise3-group-6\/0006-create-observation-repository-search.md","Context":"## Context\nResearchers may require methods to search for a particular Whale Observation or set of Whale Observations during their\nstudies. For the program to accomplish this task two search methods may require implementation.\n","Decision":"Create `getByDate` and `getById` methods in ObservationRepository.\\n`getByDate` uses the Observation iterator to compare Observation objects `sightingTime` (Date) to the date passed to the\\nmethod and creates an array list of matching objects. This function uses an iterator rather than `Collections.binarySearch()`\\nto reduce the runtime given that there may exist multiple objects of a similar date.\\n`getById` uses `Collections.binarySearch()` to search and possibly return the object with the `ObservationId` matching\\nthe long passed to the method.\\nCollection.sort()` used in `getById` implements the abstract strategy of the `Comparator` interface using the concrete\\nstrategy `compareByDate` and returns an integer referring to the order of the objects.\\n","tokens":41,"id":2772,"text":"## Context\\nResearchers may require methods to search for a particular Whale Observation or set of Whale Observations during their\\nstudies. For the program to accomplish this task two search methods may require implementation.\\n\n\n##Decision\nCreate `getByDate` and `getById` methods in ObservationRepository.\\n`getByDate` uses the Observation iterator to compare Observation objects `sightingTime` (Date) to the date passed to the\\nmethod and creates an array list of matching objects. This function uses an iterator rather than `Collections.binarySearch()`\\nto reduce the runtime given that there may exist multiple objects of a similar date.\\n`getById` uses `Collections.binarySearch()` to search and possibly return the object with the `ObservationId` matching\\nthe long passed to the method.\\nCollection.sort()` used in `getById` implements the abstract strategy of the `Comparator` interface using the concrete\\nstrategy `compareByDate` and returns an integer referring to the order of the objects.\\n","Predictions":"Create getByDate and getById methods in ObservationRepository.ngetByDate uses the Observation iterator to compare Observation objects sightingTime (Date) to the date passed to thenmethod and create an array list of matching objects. This function uses an iterator rather than Collections.binarySearch()nto reduce the runtime given that there may exist multiple objects of a similar date.ngetById uses Collections.binarySearch() to search and possibly return the object with the ObservationId matchingnthe long passed to the method.nCollection.sort() used in getById implements the abstract strategy of the Comparator interface using the concretenstrategy compareByDate and returns an integer referring to the order of the objects.n"}
{"File Name":"cosmos-sdk\/adr-002-docs-structure.md","Context":"## Context\nThere is a need for a scalable structure of the Cosmos SDK documentation. Current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.\nIdeally, we would have:\n* All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\n* All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\n","Decision":"Re-structure the `\/docs` folder of the Cosmos SDK github repo as follows:\\n```text\\ndocs\/\\n\u251c\u2500\u2500 README\\n\u251c\u2500\u2500 intro\/\\n\u251c\u2500\u2500 concepts\/\\n\u2502   \u251c\u2500\u2500 baseapp\\n\u2502   \u251c\u2500\u2500 types\\n\u2502   \u251c\u2500\u2500 store\\n\u2502   \u251c\u2500\u2500 server\\n\u2502   \u251c\u2500\u2500 modules\/\\n\u2502   \u2502   \u251c\u2500\u2500 keeper\\n\u2502   \u2502   \u251c\u2500\u2500 handler\\n\u2502   \u2502   \u251c\u2500\u2500 cli\\n\u2502   \u251c\u2500\u2500 gas\\n\u2502   \u2514\u2500\u2500 commands\\n\u251c\u2500\u2500 clients\/\\n\u2502   \u251c\u2500\u2500 lite\/\\n\u2502   \u251c\u2500\u2500 service-providers\\n\u251c\u2500\u2500 modules\/\\n\u251c\u2500\u2500 spec\/\\n\u251c\u2500\u2500 translations\/\\n\u2514\u2500\u2500 architecture\/\\n```\\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\\n* `README`: Landing page of the docs.\\n* `intro`: Introductory material. Goal is to have a short explainer of the Cosmos SDK and then channel people to the resource they need. The [Cosmos SDK tutorial](https:\/\/github.com\/cosmos\/sdk-application-tutorial\/) will be highlighted, as well as the `godocs`.\\n* `concepts`: Contains high-level explanations of the abstractions of the Cosmos SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\\n* `clients`: Contains specs and info about the various Cosmos SDK clients.\\n* `spec`: Contains specs of modules, and others.\\n* `modules`: Contains links to `godocs` and the spec of the modules.\\n* `architecture`: Contains architecture-related docs like the present one.\\n* `translations`: Contains different translations of the documentation.\\nWebsite docs sidebar will only include the following sections:\\n* `README`\\n* `intro`\\n* `concepts`\\n* `clients`\\n`architecture` need not be displayed on the website.\\n","tokens":115,"id":816,"text":"## Context\\nThere is a need for a scalable structure of the Cosmos SDK documentation. Current documentation includes a lot of non-related Cosmos SDK material, is difficult to maintain and hard to follow as a user.\\nIdeally, we would have:\\n* All docs related to dev frameworks or tools live in their respective github repos (sdk repo would contain sdk docs, hub repo would contain hub docs, lotion repo would contain lotion docs, etc.)\\n* All other docs (faqs, whitepaper, high-level material about Cosmos) would live on the website.\\n\n\n##Decision\nRe-structure the `\/docs` folder of the Cosmos SDK github repo as follows:\\n```text\\ndocs\/\\n\u251c\u2500\u2500 README\\n\u251c\u2500\u2500 intro\/\\n\u251c\u2500\u2500 concepts\/\\n\u2502   \u251c\u2500\u2500 baseapp\\n\u2502   \u251c\u2500\u2500 types\\n\u2502   \u251c\u2500\u2500 store\\n\u2502   \u251c\u2500\u2500 server\\n\u2502   \u251c\u2500\u2500 modules\/\\n\u2502   \u2502   \u251c\u2500\u2500 keeper\\n\u2502   \u2502   \u251c\u2500\u2500 handler\\n\u2502   \u2502   \u251c\u2500\u2500 cli\\n\u2502   \u251c\u2500\u2500 gas\\n\u2502   \u2514\u2500\u2500 commands\\n\u251c\u2500\u2500 clients\/\\n\u2502   \u251c\u2500\u2500 lite\/\\n\u2502   \u251c\u2500\u2500 service-providers\\n\u251c\u2500\u2500 modules\/\\n\u251c\u2500\u2500 spec\/\\n\u251c\u2500\u2500 translations\/\\n\u2514\u2500\u2500 architecture\/\\n```\\nThe files in each sub-folders do not matter and will likely change. What matters is the sectioning:\\n* `README`: Landing page of the docs.\\n* `intro`: Introductory material. Goal is to have a short explainer of the Cosmos SDK and then channel people to the resource they need. The [Cosmos SDK tutorial](https:\/\/github.com\/cosmos\/sdk-application-tutorial\/) will be highlighted, as well as the `godocs`.\\n* `concepts`: Contains high-level explanations of the abstractions of the Cosmos SDK. It does not contain specific code implementation and does not need to be updated often. **It is not an API specification of the interfaces**. API spec is the `godoc`.\\n* `clients`: Contains specs and info about the various Cosmos SDK clients.\\n* `spec`: Contains specs of modules, and others.\\n* `modules`: Contains links to `godocs` and the spec of the modules.\\n* `architecture`: Contains architecture-related docs like the present one.\\n* `translations`: Contains different translations of the documentation.\\nWebsite docs sidebar will only include the following sections:\\n* `README`\\n* `intro`\\n* `concepts`\\n* `clients`\\n`architecture` need not be displayed on the website.\\n","Predictions":"* docs:   docsn  docsn  docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn docsn"}
{"File Name":"tendermint\/adr-012-peer-transport.md","Context":"## Context\nOne of the more apparent problems with the current architecture in the p2p\npackage is that there is no clear separation of concerns between different\ncomponents. Most notably the `Switch` is currently doing physical connection\nhandling. An artifact is the dependency of the Switch on\n`[config.P2PConfig`](https:\/\/github.com\/tendermint\/tendermint\/blob\/05a76fb517f50da27b4bfcdc7b4cf185fc61eff6\/config\/config.go#L272-L339).\nAddresses:\n- [#2046](https:\/\/github.com\/tendermint\/tendermint\/issues\/2046)\n- [#2047](https:\/\/github.com\/tendermint\/tendermint\/issues\/2047)\nFirst iteraton in [#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)\n","Decision":"Transport concerns will be handled by a new component (`PeerTransport`) which\\nwill provide Peers at its boundary to the caller. In turn `Switch` will use\\nthis new component accept new `Peer`s and dial them based on `NetAddress`.\\n### PeerTransport\\nResponsible for emitting and connecting to Peers. The implementation of `Peer`\\nis left to the transport, which implies that the chosen transport dictates the\\ncharacteristics of the implementation handed back to the `Switch`. Each\\ntransport implementation is responsible to filter establishing peers specific\\nto its domain, for the default multiplexed implementation the following will\\napply:\\n- connections from our own node\\n- handshake fails\\n- upgrade to secret connection fails\\n- prevent duplicate ip\\n- prevent duplicate id\\n- nodeinfo incompatibility\\n```go\\n\/\/ PeerTransport proxies incoming and outgoing peer connections.\\ntype PeerTransport interface {\\n\/\/ Accept returns a newly connected Peer.\\nAccept() (Peer, error)\\n\/\/ Dial connects to a Peer.\\nDial(NetAddress) (Peer, error)\\n}\\n\/\/ EXAMPLE OF DEFAULT IMPLEMENTATION\\n\/\/ multiplexTransport accepts tcp connections and upgrades to multiplexted\\n\/\/ peers.\\ntype multiplexTransport struct {\\nlistener net.Listener\\nacceptc chan accept\\nclosec  <-chan struct{}\\nlistenc <-chan struct{}\\ndialTimeout      time.Duration\\nhandshakeTimeout time.Duration\\nnodeAddr         NetAddress\\nnodeInfo         NodeInfo\\nnodeKey          NodeKey\\n\/\/ TODO(xla): Remove when MConnection is refactored into mPeer.\\nmConfig conn.MConnConfig\\n}\\nvar _ PeerTransport = (*multiplexTransport)(nil)\\n\/\/ NewMTransport returns network connected multiplexed peers.\\nfunc NewMTransport(\\nnodeAddr NetAddress,\\nnodeInfo NodeInfo,\\nnodeKey NodeKey,\\n) *multiplexTransport\\n```\\n### Switch\\nFrom now the Switch will depend on a fully setup `PeerTransport` to\\nretrieve\/reach out to its peers. As the more low-level concerns are pushed to\\nthe transport, we can omit passing the `config.P2PConfig` to the Switch.\\n```go\\nfunc NewSwitch(transport PeerTransport, opts ...SwitchOption) *Switch\\n```\\n","tokens":179,"id":1956,"text":"## Context\\nOne of the more apparent problems with the current architecture in the p2p\\npackage is that there is no clear separation of concerns between different\\ncomponents. Most notably the `Switch` is currently doing physical connection\\nhandling. An artifact is the dependency of the Switch on\\n`[config.P2PConfig`](https:\/\/github.com\/tendermint\/tendermint\/blob\/05a76fb517f50da27b4bfcdc7b4cf185fc61eff6\/config\/config.go#L272-L339).\\nAddresses:\\n- [#2046](https:\/\/github.com\/tendermint\/tendermint\/issues\/2046)\\n- [#2047](https:\/\/github.com\/tendermint\/tendermint\/issues\/2047)\\nFirst iteraton in [#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)\\n\n\n##Decision\nTransport concerns will be handled by a new component (`PeerTransport`) which\\nwill provide Peers at its boundary to the caller. In turn `Switch` will use\\nthis new component accept new `Peer`s and dial them based on `NetAddress`.\\n### PeerTransport\\nResponsible for emitting and connecting to Peers. The implementation of `Peer`\\nis left to the transport, which implies that the chosen transport dictates the\\ncharacteristics of the implementation handed back to the `Switch`. Each\\ntransport implementation is responsible to filter establishing peers specific\\nto its domain, for the default multiplexed implementation the following will\\napply:\\n- connections from our own node\\n- handshake fails\\n- upgrade to secret connection fails\\n- prevent duplicate ip\\n- prevent duplicate id\\n- nodeinfo incompatibility\\n```go\\n\/\/ PeerTransport proxies incoming and outgoing peer connections.\\ntype PeerTransport interface {\\n\/\/ Accept returns a newly connected Peer.\\nAccept() (Peer, error)\\n\/\/ Dial connects to a Peer.\\nDial(NetAddress) (Peer, error)\\n}\\n\/\/ EXAMPLE OF DEFAULT IMPLEMENTATION\\n\/\/ multiplexTransport accepts tcp connections and upgrades to multiplexted\\n\/\/ peers.\\ntype multiplexTransport struct {\\nlistener net.Listener\\nacceptc chan accept\\nclosec  <-chan struct{}\\nlistenc <-chan struct{}\\ndialTimeout      time.Duration\\nhandshakeTimeout time.Duration\\nnodeAddr         NetAddress\\nnodeInfo         NodeInfo\\nnodeKey          NodeKey\\n\/\/ TODO(xla): Remove when MConnection is refactored into mPeer.\\nmConfig conn.MConnConfig\\n}\\nvar _ PeerTransport = (*multiplexTransport)(nil)\\n\/\/ NewMTransport returns network connected multiplexed peers.\\nfunc NewMTransport(\\nnodeAddr NetAddress,\\nnodeInfo NodeInfo,\\nnodeKey NodeKey,\\n) *multiplexTransport\\n```\\n### Switch\\nFrom now the Switch will depend on a fully setup `PeerTransport` to\\nretrieve\/reach out to its peers. As the more low-level concerns are pushed to\\nthe transport, we can omit passing the `config.P2PConfig` to the Switch.\\n```go\\nfunc NewSwitch(transport PeerTransport, opts ...SwitchOption) *Switch\\n```\\n","Predictions":"The Switch will be responsible for establishing a peer at its boundary to the caller. In turn Switch will usenthis new component accept new Peers and dial them based on NetAddress.n### PeerTransportnResponsible for emitting and connecting to Peers. The implementation of Peernis left to the transport, which implies that the chosen transport dictates thencharacteristics of the implementation handed back to the Switch. Eachntransport implementation is responsible to filter establishing peers specificnto its domain, for the default multiplexed implementation the following willnapply:n- connections from our own noden- handshake failsn- upgrade to secret connection failsn- prevent duplicate ipn- prevent duplicate idn- nodeinfo incompatibilityngon\/\/ PeerTransport proxies incoming and outgoing incoming and"}
{"File Name":"connecting-to-services\/0008-add-basic-authentication-to-test-environments.md","Context":"## Context\nEvery environment the application is available in, be that development, review,\nstaging or public are openly available to anybody. There is no access control\nat all.  This isn't suitable for a site that could be misinterpreted as the\n'real' version either by people or by search engines and other bots. There\nneeds to be some form of hurdle to overcome in order to prompt people to\nconsider whether this is the correct site. There should also be a barrier to\nprevent access by bots.\n","Decision":"We have decided to use basic authentication on the service in all environments\\napart from the public facing one. This will only be applicable to the\\nenvironments hosted within the Rancher environment. The username and password\\nwill not be secret and will be included within the `README` of the application.\\n","tokens":107,"id":2207,"text":"## Context\\nEvery environment the application is available in, be that development, review,\\nstaging or public are openly available to anybody. There is no access control\\nat all.  This isn't suitable for a site that could be misinterpreted as the\\n'real' version either by people or by search engines and other bots. There\\nneeds to be some form of hurdle to overcome in order to prompt people to\\nconsider whether this is the correct site. There should also be a barrier to\\nprevent access by bots.\\n\n\n##Decision\nWe have decided to use basic authentication on the service in all environments\\napart from the public facing one. This will only be applicable to the\\nenvironments hosted within the Rancher environment. The username and password\\nwill not be secret and will be included within the `README` of the application.\\n","Predictions":"We have decided to use basic authentication on the service in all environmentsnapart from the public facing one. This will only be applicable to thenenvironments hosted within the Rancher environment. The username and passwordnwill not be secret and will be included within the README of the application.n"}
{"File Name":"commcare-cloud\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5146,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"opg-use-an-lpa\/0009-use-aws-kms-to-manage-session-encryption-keys.md","Context":"## Context\nFollowing on from [8. Session storage using an encrypted cookie](0008-session-storage-using-an-encrypted-cookie.md)\n* The encrypted cookie will need to be encrypted using a key.\n* Keys should be able to be rotated easily and often.\n* Key rotations should have no effect on active users.\n","Decision":"* We will use AWS' KMS to manage our encryption keys.\\n* Keys will be cached at the contained level in volatile memory.\\n","tokens":67,"id":4848,"text":"## Context\\nFollowing on from [8. Session storage using an encrypted cookie](0008-session-storage-using-an-encrypted-cookie.md)\\n* The encrypted cookie will need to be encrypted using a key.\\n* Keys should be able to be rotated easily and often.\\n* Key rotations should have no effect on active users.\\n\n\n##Decision\n* We will use AWS' KMS to manage our encryption keys.\\n* Keys will be cached at the contained level in volatile memory.\\n","Predictions":"* We will use AWS' KMS to manage our encryption keys.n* Keys will be cached at the contained level in volatile memory.n"}
{"File Name":"dogma\/0015-routing-unrecognized-messages.md","Context":"## Context\n[ADR-14](0014-apply-historical-events-to-aggregates.md) relaxed the\nspecification such that `AggregateRoot.ApplyEvent()` implementations were no\nlonger required to panic with an `UnrecognizedMessage` value when passed an\nunexpected message type.\nPrompted by this requirement, we relaxed the requirement for ALL handler\nmethods, which was likely too broad of a change.\nSpecifically, unlike when handling a message, the routing methods\n`AggregateMessageHandler.RouteCommandToInstance()` and\n`ProcessMessageHandler.RouteEventToInstance()` do not have the option of \"doing\nnothing\" when passed an unexpected message type.\n","Decision":"Reinstate the hard requirement that the handlers MUST panic with\\n`UnexpectedMessage` when asked to route a message type that was not configured\\nas being consumed by that handler.\\n","tokens":138,"id":1612,"text":"## Context\\n[ADR-14](0014-apply-historical-events-to-aggregates.md) relaxed the\\nspecification such that `AggregateRoot.ApplyEvent()` implementations were no\\nlonger required to panic with an `UnrecognizedMessage` value when passed an\\nunexpected message type.\\nPrompted by this requirement, we relaxed the requirement for ALL handler\\nmethods, which was likely too broad of a change.\\nSpecifically, unlike when handling a message, the routing methods\\n`AggregateMessageHandler.RouteCommandToInstance()` and\\n`ProcessMessageHandler.RouteEventToInstance()` do not have the option of \"doing\\nnothing\" when passed an unexpected message type.\\n\n\n##Decision\nReinstate the hard requirement that the handlers MUST panic with\\n`UnexpectedMessage` when asked to route a message type that was not configured\\nas being consumed by that handler.\\n","Predictions":"We will use the [ADR-0014](0014-apply-historical-events-to-aggregates.md) specification to provide anreducednrequirement for the AggregateRoot.ApplyEvent() implementation.n"}
{"File Name":"runner\/0280-command-input-echoing.md","Context":"## Context\nCommand echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.\nCommand outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.\nEcho output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.\n","Decision":"- The only commands that currently echo output are\\n- `remove-matcher`\\n- `add-matcher`\\n- `add-path`\\n- These will no longer echo the command, if processed successfully\\n- All commands echo the input when any of these conditions is fulfilled:\\n1. When such commands fail with an error\\n2. When `::echo::on` is set\\n3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`\\n- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019\/11\/04):\\n- `add-mask`\\n- `debug`\\n- `warning`\\n- `error`\\n- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these \"echo-suppressing\" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit.\\n","tokens":124,"id":3741,"text":"## Context\\nCommand echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.\\nCommand outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.\\nEcho output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.\\n\n\n##Decision\n- The only commands that currently echo output are\\n- `remove-matcher`\\n- `add-matcher`\\n- `add-path`\\n- These will no longer echo the command, if processed successfully\\n- All commands echo the input when any of these conditions is fulfilled:\\n1. When such commands fail with an error\\n2. When `::echo::on` is set\\n3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`\\n- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019\/11\/04):\\n- `add-mask`\\n- `debug`\\n- `warning`\\n- `error`\\n- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these \"echo-suppressing\" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit.\\n","Predictions":"- The only commands that currently echo output aren- remove-matchern- add-matchern- add-pathn- These will no longer echo the command, if processed successfullyn- All commands echo the input when any of these conditions is fulfilled:n1. When such commands fail with an errorn2. When ::echo::on is setn3. When the ACTIONS_STEP_DEBUG is set, and echoing hasn't been explicitly disabled with ::echo::offn- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019\/11\/04):n- add-maskn- debugn- warningn- errorn- The three commands above will not echo, either because echoing the command would leak secrets (e.g. add-mask), or it would not add any additional troubleshooting information to the logs (e.g. debug). It's expected that future commands will follow these \"echo-suppressing\" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit.n"}
{"File Name":"alfresco-anaxes-shipyard\/0008-applying-extensions-to-docker-images.md","Context":"## Context\nOne of the main advantages of Docker images is their immutability. This means once an image has been tested and verified there is a high confidence it will work as intended in other environments as it doesn't change.\nAlfresco allows the core product to the enhanced via external modules in the form of [AMPs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-amps.html) or [simple JARs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-jar-files.html).\nThis results in two big problems, how do we release containers with every combination of AMP available and how do customers apply their own extensions?\nWe have three options; apply extensions at build time (thus retaining the immutability advantage), apply extensions at runtime using a mechanism that doesn't change the contents of the container or apply extensions as the container initializes (breaking immutability).\nApplying extensions at build time means we will be forcing customers to build their own images depending on which official and custom extensions they require.\nHowever, there are a number of disadvantages of applying extensions at runtime; The immutability advantage is lost (the WAR file is changed), an extension could fail to apply and prevent the container from starting, it has the potential to introduce performance issues as the AMP or JAR has to be fetched and applied and finally it opens a potential security hole as code can be applied to a container via a simple environment variable.\nWe investigated potential approaches to keep the main container immutable by using volumes and initContainers but this increases the complexity of the solution and doesn't resolve all the issues outlined above.\n","Decision":"We will proceed with the build time option as there are a number of disadvantages to applying extensions at runtime, customers are already used to applying their own extensions and they're having to learn a new deployment mechanism anyway.\\nWe will release a small number of images with and without common AMPs applied.\\n","tokens":351,"id":3604,"text":"## Context\\nOne of the main advantages of Docker images is their immutability. This means once an image has been tested and verified there is a high confidence it will work as intended in other environments as it doesn't change.\\nAlfresco allows the core product to the enhanced via external modules in the form of [AMPs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-amps.html) or [simple JARs](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-jar-files.html).\\nThis results in two big problems, how do we release containers with every combination of AMP available and how do customers apply their own extensions?\\nWe have three options; apply extensions at build time (thus retaining the immutability advantage), apply extensions at runtime using a mechanism that doesn't change the contents of the container or apply extensions as the container initializes (breaking immutability).\\nApplying extensions at build time means we will be forcing customers to build their own images depending on which official and custom extensions they require.\\nHowever, there are a number of disadvantages of applying extensions at runtime; The immutability advantage is lost (the WAR file is changed), an extension could fail to apply and prevent the container from starting, it has the potential to introduce performance issues as the AMP or JAR has to be fetched and applied and finally it opens a potential security hole as code can be applied to a container via a simple environment variable.\\nWe investigated potential approaches to keep the main container immutable by using volumes and initContainers but this increases the complexity of the solution and doesn't resolve all the issues outlined above.\\n\n\n##Decision\nWe will proceed with the build time option as there are a number of disadvantages to applying extensions at runtime, customers are already used to applying their own extensions and they're having to learn a new deployment mechanism anyway.\\nWe will release a small number of images with and without common AMPs applied.\\n","Predictions":"We will use the [ADR](https:\/\/docs.alfresco.com\/5.2\/concepts\/dev-extensions-packaging-techniques-jar-files.html) to provide a way to create a new ADR file. This will be used to create a new ADR file for each ADR.n"}
{"File Name":"docspell\/0001_components.md","Context":"# Context and Problem Statement\nHow should the application be structured into its main components? The\ngoal is to be able to have multiple rest servers\/webapps and multiple\ndocument processor components working togehter.\n# Decision Outcome\nThe following are the \"main\" modules. There may be more helper modules\nand libraries that support implementing a feature.\n","Decision":"The following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n","tokens":75,"id":276,"text":"# Context and Problem Statement\\nHow should the application be structured into its main components? The\\ngoal is to be able to have multiple rest servers\/webapps and multiple\\ndocument processor components working togehter.\\n# Decision Outcome\\nThe following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n\n\n##Decision\nThe following are the \"main\" modules. There may be more helper modules\\nand libraries that support implementing a feature.\\n","Predictions":"Chosen option: \"Protobuf\", because it comes out best (see below).n### Positive Consequencesn- We need to start defining interfaces outside of their implementations.n- We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example, use POST \/get_projects BODY  \"index\": \"name\"  instead of GET \/projects?index=name.n### Negative Consequencesn- The team needs to learn a new language.n- Follow-up decisions required for the actual implementation.n"}
{"File Name":"gatemint-sdk\/adr-020-protobuf-transaction-encoding.md","Context":"## Context\nThis ADR is a continuation of the motivation, design, and context established in\n[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design the\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\nSpecifically, the client-side migration path primarily includes tx generation and\nsigning, message construction and routing, in addition to CLI & REST handlers and\nbusiness logic (i.e. queriers).\nWith this in mind, we will tackle the migration path via two main areas, txs and\nquerying. However, this ADR solely focuses on transactions. Querying should be\naddressed in a future ADR, but it should build off of these proposals.\nBased on detailed discussions ([\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)), the original\ndesign for transactions was changed substantially from an `oneof` \/JSON-signing\napproach to the approach described below.\n","Decision":"### Transactions\\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\\n`sdk.Msg`s are encoding with `Any` in transactions.\\nOne of the main goals of using `Any` to encode interface values is to have a\\ncore set of types which is reused by apps so that\\nclients can safely be compatible with as many chains as possible.\\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\\nformat that can serve a wide variety of use cases without breaking client\\ncompatibility.\\nIn order to facilitate signing, transactions are separated into `TxBody`,\\nwhich will be re-used by `SignDoc` below, and `signatures`:\\n```proto\\n\/\/ types\/types.proto\\npackage cosmos_sdk.v1;\\nmessage Tx {\\nTxBody body = 1;\\nAuthInfo auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\n\/\/ A variant of Tx that pins the signer's exact binary represenation of body and\\n\/\/ auth_info. This is used for signing, broadcasting and verification. The binary\\n\/\/ `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\\n\/\/ becomes the \"txhash\", commonly used as the transaction ID.\\nmessage TxRaw {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in SignDoc.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\\nbytes auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\nmessage TxBody {\\n\/\/ A list of messages to be executed. The required signers of those messages define\\n\/\/ the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\\n\/\/ Each required signer address is added to the list only the first time it occurs.\\n\/\/\\n\/\/ By convention, the first required signer (usually from the first message) is referred\\n\/\/ to as the primary signer and pays the fee for the whole transaction.\\nrepeated google.protobuf.Any messages = 1;\\nstring memo = 2;\\nint64 timeout_height = 3;\\nrepeated google.protobuf.Any extension_options = 1023;\\n}\\nmessage AuthInfo {\\n\/\/ This list defines the signing modes for the required signers. The number\\n\/\/ and order of elements must match the required signers from TxBody's messages.\\n\/\/ The first element is the primary signer and the one which pays the fee.\\nrepeated SignerInfo signer_infos = 1;\\n\/\/ The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\\nFee fee = 2;\\n}\\nmessage SignerInfo {\\n\/\/ The public key is optional for accounts that already exist in state. If unset, the\\n\/\/ verifier can use the required signer address for this position and lookup the public key.\\nPublicKey public_key = 1;\\n\/\/ ModeInfo describes the signing mode of the signer and is a nested\\n\/\/ structure to support nested multisig pubkey's\\nModeInfo mode_info = 2;\\n\/\/ sequence is the sequence of the account, which describes the\\n\/\/ number of committed transactions signed by a given address. It is used to prevent\\n\/\/ replay attacks.\\nuint64 sequence = 3;\\n}\\nmessage ModeInfo {\\noneof sum {\\nSingle single = 1;\\nMulti multi = 2;\\n}\\n\/\/ Single is the mode info for a single signer. It is structured as a message\\n\/\/ to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\\nmessage Single {\\nSignMode mode = 1;\\n}\\n\/\/ Multi is the mode info for a multisig public key\\nmessage Multi {\\n\/\/ bitarray specifies which keys within the multisig are signing\\nCompactBitArray bitarray = 1;\\n\/\/ mode_infos is the corresponding modes of the signers of the multisig\\n\/\/ which could include nested multisig public keys\\nrepeated ModeInfo mode_infos = 2;\\n}\\n}\\nenum SignMode {\\nSIGN_MODE_UNSPECIFIED = 0;\\nSIGN_MODE_DIRECT = 1;\\nSIGN_MODE_TEXTUAL = 2;\\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\\n}\\n```\\nAs will be discussed below, in order to include as much of the `Tx` as possible\\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\\nraw signatures themselves live outside of what is signed over.\\nBecause we are aiming for a flexible, extensible cross-chain transaction\\nformat, new transaction processing options should be added to `TxBody` as soon\\nthose use cases are discovered, even if they can't be implemented yet.\\nBecause there is coordination overhead in this, `TxBody` includes an\\n`extension_options` field which can be used for any transaction processing\\noptions that are not already covered. App developers should, nevertheless,\\nattempt to upstream important improvements to `Tx`.\\n### Signing\\nAll of the signing modes below aim to provide the following guarantees:\\n- **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\\nis signed\\n- **Predictable Gas**: if I am signing a transaction where I am paying a fee,\\nthe final gas is fully dependent on what I am signing\\nThese guarantees give the maximum amount confidence to message signers that\\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\\n#### `SIGN_MODE_DIRECT`\\nThe \"direct\" signing behavior is to sign the raw `TxBody` bytes as broadcast over\\nthe wire. This has the advantages of:\\n- requiring the minimum additional client capabilities beyond a standard protocol\\nbuffers implementation\\n- leaving effectively zero holes for transaction malleability (i.e. there are no\\nsubtle differences between the signing and encoding formats which could\\npotentially be exploited by an attacker)\\nSignatures are structured using the `SignDoc` below which reuses the serialization of\\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\\n```proto\\n\/\/ types\/types.proto\\nmessage SignDoc {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in TxRaw.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\\nbytes auth_info = 2;\\nstring chain_id = 3;\\nuint64 account_number = 4;\\n}\\n```\\nIn order to sign in the default mode, clients take the following steps:\\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\\n2. Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n3. Sign the encoded `SignDoc` bytes.\\n4. Build a `TxRaw` and serialize it for broadcasting.\\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\\nbytes encoded in `TxRaw` not based on any [\"canonicalization\"](https:\/\/github.com\/regen-network\/canonical-proto3)\\nalgorithm which creates added complexity for clients in addition to preventing\\nsome forms of upgradeability (to be addressed later in this document).\\nSignature verifiers do:\\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\\n2. Create a list of required signer addresses from the messages.\\n3. For each required signer:\\n- Pull account number and sequence from the state.\\n- Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\\n- Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n- Verify the signature at the the same list position against the serialized `SignDoc`.\\n#### `SIGN_MODE_LEGACY_AMINO`\\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\\nsupported transaction signing. Once wallets and exchanges have had a\\nchance to upgrade to protobuf based signing, this option will be disabled. In\\nthe meantime, it is foreseen that disabling the current Amino signing would cause\\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\\nCosmos Hub and other chains may choose to disable Amino signing immediately.\\nLegacy clients will be able to sign a transaction using the current Amino\\nJSON format and have it encoded to protobuf using the REST `\/tx\/encode`\\nendpoint before broadcasting.\\n#### `SIGN_MODE_TEXTUAL`\\nAs was discussed extensively in [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078),\\nthere is a desire for a human-readable signing encoding, especially for hardware\\nwallets like the [Ledger](https:\/\/www.ledger.com) which display\\ntransaction contents to users before signing. JSON was an attempt at this but\\nfalls short of the ideal.\\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\\nencoding which will replace Amino JSON. This new encoding should be even more\\nfocused on readability than JSON, possibly based on formatting strings like\\n[MessageFormat](http:\/\/userguide.icu-project.org\/formatparse\/messages).\\nIn order to ensure that the new human-readable format does not suffer from\\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\\nto generate sign bytes.\\nMultiple human-readable formats (maybe even localized messages) may be supported\\nby `SIGN_MODE_TEXTUAL` when it is implemented.\\n### Unknown Field Filtering\\nUnknown fields in protobuf messages should generally be rejected by transaction\\nprocessors because:\\n- important data may be present in the unknown fields, that if ignored, will\\ncause unexpected behavior for clients\\n- they present a malleability vulnerability where attackers can bloat tx size\\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\\nnot `TxBody`)\\nThere are also scenarios where we may choose to safely ignore unknown fields\\n(https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078#issuecomment-624400188) to\\nprovide graceful forwards compatibility with newer clients.\\nWe propose that field numbers with bit 11 set (for most use cases this is\\nthe range of 1024-2047) be considered non-critical fields that can safely be\\nignored if unknown.\\nTo handle this we will need a unknown field filter that:\\n- always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\\nunsigned parts of `AuthInfo` if present based on the signing mode)\\n- rejects unknown fields in all messages (including nested `Any`s) other than\\nfields with bit 11 set\\nThis will likely need to be a custom protobuf parser pass that takes message bytes\\nand `FileDescriptor`s and returns a boolean result.\\n### Public Key Encoding\\nPublic keys in the Cosmos SDK implement Tendermint's `crypto.PubKey` interface,\\nso a natural solution might be to use `Any` as we are doing for other interfaces.\\nThere are, however, a limited number of public keys in existence and new ones\\naren't created overnight. The proposed solution is to use a `oneof` that:\\n- attempts to catalog all known key types even if a given app can't use them all\\n- has an `Any` member that can be used when a key type isn't present in the `oneof`\\nEx:\\n```proto\\nmessage PublicKey {\\noneof sum {\\nbytes secp256k1 = 1;\\nbytes ed25519 = 2;\\n...\\ngoogle.protobuf.Any any_pubkey = 15;\\n}\\n}\\n```\\nApps should only attempt to handle a registered set of public keys that they\\nhave tested. The provided signature verification ante handler decorators will\\nenforce this.\\n### CLI & REST\\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\\nin the client can be interfaces, similar to how we described in [ADR 019](.\/adr-019-protobuf-state-encoding.md),\\nthe client logic will now need to take a codec interface that knows not only how\\nto handle all the types, but also knows how to generate transactions, signatures,\\nand messages.\\n```go\\ntype AccountRetriever interface {\\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\\n}\\ntype Generator interface {\\nNewTx() TxBuilder\\nNewFee() ClientFee\\nNewSignature() ClientSignature\\nMarshalTx(tx types.Tx) ([]byte, error)\\n}\\ntype TxBuilder interface {\\nGetTx() sdk.Tx\\nSetMsgs(...sdk.Msg) error\\nGetSignatures() []sdk.Signature\\nSetSignatures(...sdk.Signature)\\nGetFee() sdk.Fee\\nSetFee(sdk.Fee)\\nGetMemo() string\\nSetMemo(string)\\n}\\n```\\nWe then update `Context` to have new fields: `JSONMarshaler`, `TxGenerator`,\\nand `AccountRetriever`, and we update `AppModuleBasic.GetTxCmd` to take\\na `Context` which should have all of these fields pre-populated.\\nEach client method should then use one of the `Init` methods to re-initialize\\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\\ngenerate or broadcast a transaction. For example:\\n```go\\nimport \"github.com\/spf13\/cobra\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\/tx\"\\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\\nreturn &cobra.Command{\\nRunE: func(cmd *cobra.Command, args []string) error {\\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\\nmsg := NewSomeMsg{...}\\ntx.GenerateOrBroadcastTx(clientCtx, msg)\\n},\\n}\\n}\\n```\\n","tokens":234,"id":21,"text":"## Context\\nThis ADR is a continuation of the motivation, design, and context established in\\n[ADR 019](.\/adr-019-protobuf-state-encoding.md), namely, we aim to design the\\nProtocol Buffer migration path for the client-side of the Cosmos SDK.\\nSpecifically, the client-side migration path primarily includes tx generation and\\nsigning, message construction and routing, in addition to CLI & REST handlers and\\nbusiness logic (i.e. queriers).\\nWith this in mind, we will tackle the migration path via two main areas, txs and\\nquerying. However, this ADR solely focuses on transactions. Querying should be\\naddressed in a future ADR, but it should build off of these proposals.\\nBased on detailed discussions ([\\#6030](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6030)\\nand [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078)), the original\\ndesign for transactions was changed substantially from an `oneof` \/JSON-signing\\napproach to the approach described below.\\n\n\n##Decision\n### Transactions\\nSince interface values are encoded with `google.protobuf.Any` in state (see [ADR 019](adr-019-protobuf-state-encoding.md)),\\n`sdk.Msg`s are encoding with `Any` in transactions.\\nOne of the main goals of using `Any` to encode interface values is to have a\\ncore set of types which is reused by apps so that\\nclients can safely be compatible with as many chains as possible.\\nIt is one of the goals of this specification to provide a flexible cross-chain transaction\\nformat that can serve a wide variety of use cases without breaking client\\ncompatibility.\\nIn order to facilitate signing, transactions are separated into `TxBody`,\\nwhich will be re-used by `SignDoc` below, and `signatures`:\\n```proto\\n\/\/ types\/types.proto\\npackage cosmos_sdk.v1;\\nmessage Tx {\\nTxBody body = 1;\\nAuthInfo auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\n\/\/ A variant of Tx that pins the signer's exact binary represenation of body and\\n\/\/ auth_info. This is used for signing, broadcasting and verification. The binary\\n\/\/ `serialize(tx: TxRaw)` is stored in Tendermint and the hash `sha256(serialize(tx: TxRaw))`\\n\/\/ becomes the \"txhash\", commonly used as the transaction ID.\\nmessage TxRaw {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in SignDoc.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in SignDoc.\\nbytes auth_info = 2;\\n\/\/ A list of signatures that matches the length and order of AuthInfo's signer_infos to\\n\/\/ allow connecting signature meta information like public key and signing mode by position.\\nrepeated bytes signatures = 3;\\n}\\nmessage TxBody {\\n\/\/ A list of messages to be executed. The required signers of those messages define\\n\/\/ the number and order of elements in AuthInfo's signer_infos and Tx's signatures.\\n\/\/ Each required signer address is added to the list only the first time it occurs.\\n\/\/\\n\/\/ By convention, the first required signer (usually from the first message) is referred\\n\/\/ to as the primary signer and pays the fee for the whole transaction.\\nrepeated google.protobuf.Any messages = 1;\\nstring memo = 2;\\nint64 timeout_height = 3;\\nrepeated google.protobuf.Any extension_options = 1023;\\n}\\nmessage AuthInfo {\\n\/\/ This list defines the signing modes for the required signers. The number\\n\/\/ and order of elements must match the required signers from TxBody's messages.\\n\/\/ The first element is the primary signer and the one which pays the fee.\\nrepeated SignerInfo signer_infos = 1;\\n\/\/ The fee can be calculated based on the cost of evaluating the body and doing signature verification of the signers. This can be estimated via simulation.\\nFee fee = 2;\\n}\\nmessage SignerInfo {\\n\/\/ The public key is optional for accounts that already exist in state. If unset, the\\n\/\/ verifier can use the required signer address for this position and lookup the public key.\\nPublicKey public_key = 1;\\n\/\/ ModeInfo describes the signing mode of the signer and is a nested\\n\/\/ structure to support nested multisig pubkey's\\nModeInfo mode_info = 2;\\n\/\/ sequence is the sequence of the account, which describes the\\n\/\/ number of committed transactions signed by a given address. It is used to prevent\\n\/\/ replay attacks.\\nuint64 sequence = 3;\\n}\\nmessage ModeInfo {\\noneof sum {\\nSingle single = 1;\\nMulti multi = 2;\\n}\\n\/\/ Single is the mode info for a single signer. It is structured as a message\\n\/\/ to allow for additional fields such as locale for SIGN_MODE_TEXTUAL in the future\\nmessage Single {\\nSignMode mode = 1;\\n}\\n\/\/ Multi is the mode info for a multisig public key\\nmessage Multi {\\n\/\/ bitarray specifies which keys within the multisig are signing\\nCompactBitArray bitarray = 1;\\n\/\/ mode_infos is the corresponding modes of the signers of the multisig\\n\/\/ which could include nested multisig public keys\\nrepeated ModeInfo mode_infos = 2;\\n}\\n}\\nenum SignMode {\\nSIGN_MODE_UNSPECIFIED = 0;\\nSIGN_MODE_DIRECT = 1;\\nSIGN_MODE_TEXTUAL = 2;\\nSIGN_MODE_LEGACY_AMINO_JSON = 127;\\n}\\n```\\nAs will be discussed below, in order to include as much of the `Tx` as possible\\nin the `SignDoc`, `SignerInfo` is separated from signatures so that only the\\nraw signatures themselves live outside of what is signed over.\\nBecause we are aiming for a flexible, extensible cross-chain transaction\\nformat, new transaction processing options should be added to `TxBody` as soon\\nthose use cases are discovered, even if they can't be implemented yet.\\nBecause there is coordination overhead in this, `TxBody` includes an\\n`extension_options` field which can be used for any transaction processing\\noptions that are not already covered. App developers should, nevertheless,\\nattempt to upstream important improvements to `Tx`.\\n### Signing\\nAll of the signing modes below aim to provide the following guarantees:\\n- **No Malleability**: `TxBody` and `AuthInfo` cannot change once the transaction\\nis signed\\n- **Predictable Gas**: if I am signing a transaction where I am paying a fee,\\nthe final gas is fully dependent on what I am signing\\nThese guarantees give the maximum amount confidence to message signers that\\nmanipulation of `Tx`s by intermediaries can't result in any meaningful changes.\\n#### `SIGN_MODE_DIRECT`\\nThe \"direct\" signing behavior is to sign the raw `TxBody` bytes as broadcast over\\nthe wire. This has the advantages of:\\n- requiring the minimum additional client capabilities beyond a standard protocol\\nbuffers implementation\\n- leaving effectively zero holes for transaction malleability (i.e. there are no\\nsubtle differences between the signing and encoding formats which could\\npotentially be exploited by an attacker)\\nSignatures are structured using the `SignDoc` below which reuses the serialization of\\n`TxBody` and `AuthInfo` and only adds the fields which are needed for signatures:\\n```proto\\n\/\/ types\/types.proto\\nmessage SignDoc {\\n\/\/ A protobuf serialization of a TxBody that matches the representation in TxRaw.\\nbytes body = 1;\\n\/\/ A protobuf serialization of an AuthInfo that matches the representation in TxRaw.\\nbytes auth_info = 2;\\nstring chain_id = 3;\\nuint64 account_number = 4;\\n}\\n```\\nIn order to sign in the default mode, clients take the following steps:\\n1. Serialize `TxBody` and `AuthInfo` using any valid protobuf implementation.\\n2. Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n3. Sign the encoded `SignDoc` bytes.\\n4. Build a `TxRaw` and serialize it for broadcasting.\\nSignature verification is based on comparing the raw `TxBody` and `AuthInfo`\\nbytes encoded in `TxRaw` not based on any [\"canonicalization\"](https:\/\/github.com\/regen-network\/canonical-proto3)\\nalgorithm which creates added complexity for clients in addition to preventing\\nsome forms of upgradeability (to be addressed later in this document).\\nSignature verifiers do:\\n1. Deserialize a `TxRaw` and pull out `body` and `auth_info`.\\n2. Create a list of required signer addresses from the messages.\\n3. For each required signer:\\n- Pull account number and sequence from the state.\\n- Obtain the public key either from state or `AuthInfo`'s `signer_infos`.\\n- Create a `SignDoc` and serialize it using [ADR 027](.\/adr-027-deterministic-protobuf-serialization.md).\\n- Verify the signature at the the same list position against the serialized `SignDoc`.\\n#### `SIGN_MODE_LEGACY_AMINO`\\nIn order to support legacy wallets and exchanges, Amino JSON will be temporarily\\nsupported transaction signing. Once wallets and exchanges have had a\\nchance to upgrade to protobuf based signing, this option will be disabled. In\\nthe meantime, it is foreseen that disabling the current Amino signing would cause\\ntoo much breakage to be feasible. Note that this is mainly a requirement of the\\nCosmos Hub and other chains may choose to disable Amino signing immediately.\\nLegacy clients will be able to sign a transaction using the current Amino\\nJSON format and have it encoded to protobuf using the REST `\/tx\/encode`\\nendpoint before broadcasting.\\n#### `SIGN_MODE_TEXTUAL`\\nAs was discussed extensively in [\\#6078](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078),\\nthere is a desire for a human-readable signing encoding, especially for hardware\\nwallets like the [Ledger](https:\/\/www.ledger.com) which display\\ntransaction contents to users before signing. JSON was an attempt at this but\\nfalls short of the ideal.\\n`SIGN_MODE_TEXTUAL` is intended as a placeholder for a human-readable\\nencoding which will replace Amino JSON. This new encoding should be even more\\nfocused on readability than JSON, possibly based on formatting strings like\\n[MessageFormat](http:\/\/userguide.icu-project.org\/formatparse\/messages).\\nIn order to ensure that the new human-readable format does not suffer from\\ntransaction malleability issues, `SIGN_MODE_TEXTUAL`\\nrequires that the _human-readable bytes are concatenated with the raw `SignDoc`_\\nto generate sign bytes.\\nMultiple human-readable formats (maybe even localized messages) may be supported\\nby `SIGN_MODE_TEXTUAL` when it is implemented.\\n### Unknown Field Filtering\\nUnknown fields in protobuf messages should generally be rejected by transaction\\nprocessors because:\\n- important data may be present in the unknown fields, that if ignored, will\\ncause unexpected behavior for clients\\n- they present a malleability vulnerability where attackers can bloat tx size\\nby adding random uninterpreted data to unsigned content (i.e. the master `Tx`,\\nnot `TxBody`)\\nThere are also scenarios where we may choose to safely ignore unknown fields\\n(https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/6078#issuecomment-624400188) to\\nprovide graceful forwards compatibility with newer clients.\\nWe propose that field numbers with bit 11 set (for most use cases this is\\nthe range of 1024-2047) be considered non-critical fields that can safely be\\nignored if unknown.\\nTo handle this we will need a unknown field filter that:\\n- always rejects unknown fields in unsigned content (i.e. top-level `Tx` and\\nunsigned parts of `AuthInfo` if present based on the signing mode)\\n- rejects unknown fields in all messages (including nested `Any`s) other than\\nfields with bit 11 set\\nThis will likely need to be a custom protobuf parser pass that takes message bytes\\nand `FileDescriptor`s and returns a boolean result.\\n### Public Key Encoding\\nPublic keys in the Cosmos SDK implement Tendermint's `crypto.PubKey` interface,\\nso a natural solution might be to use `Any` as we are doing for other interfaces.\\nThere are, however, a limited number of public keys in existence and new ones\\naren't created overnight. The proposed solution is to use a `oneof` that:\\n- attempts to catalog all known key types even if a given app can't use them all\\n- has an `Any` member that can be used when a key type isn't present in the `oneof`\\nEx:\\n```proto\\nmessage PublicKey {\\noneof sum {\\nbytes secp256k1 = 1;\\nbytes ed25519 = 2;\\n...\\ngoogle.protobuf.Any any_pubkey = 15;\\n}\\n}\\n```\\nApps should only attempt to handle a registered set of public keys that they\\nhave tested. The provided signature verification ante handler decorators will\\nenforce this.\\n### CLI & REST\\nCurrently, the REST and CLI handlers encode and decode types and txs via Amino\\nJSON encoding using a concrete Amino codec. Being that some of the types dealt with\\nin the client can be interfaces, similar to how we described in [ADR 019](.\/adr-019-protobuf-state-encoding.md),\\nthe client logic will now need to take a codec interface that knows not only how\\nto handle all the types, but also knows how to generate transactions, signatures,\\nand messages.\\n```go\\ntype AccountRetriever interface {\\nEnsureExists(clientCtx client.Context, addr sdk.AccAddress) error\\nGetAccountNumberSequence(clientCtx client.Context, addr sdk.AccAddress) (uint64, uint64, error)\\n}\\ntype Generator interface {\\nNewTx() TxBuilder\\nNewFee() ClientFee\\nNewSignature() ClientSignature\\nMarshalTx(tx types.Tx) ([]byte, error)\\n}\\ntype TxBuilder interface {\\nGetTx() sdk.Tx\\nSetMsgs(...sdk.Msg) error\\nGetSignatures() []sdk.Signature\\nSetSignatures(...sdk.Signature)\\nGetFee() sdk.Fee\\nSetFee(sdk.Fee)\\nGetMemo() string\\nSetMemo(string)\\n}\\n```\\nWe then update `Context` to have new fields: `JSONMarshaler`, `TxGenerator`,\\nand `AccountRetriever`, and we update `AppModuleBasic.GetTxCmd` to take\\na `Context` which should have all of these fields pre-populated.\\nEach client method should then use one of the `Init` methods to re-initialize\\nthe pre-populated `Context`. `tx.GenerateOrBroadcastTx` can be used to\\ngenerate or broadcast a transaction. For example:\\n```go\\nimport \"github.com\/spf13\/cobra\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\"\\nimport \"github.com\/cosmos\/cosmos-sdk\/client\/tx\"\\nfunc NewCmdDoSomething(clientCtx client.Context) *cobra.Command {\\nreturn &cobra.Command{\\nRunE: func(cmd *cobra.Command, args []string) error {\\nclientCtx := ctx.InitWithInput(cmd.InOrStdin())\\nmsg := NewSomeMsg{...}\\ntx.GenerateOrBroadcastTx(clientCtx, msg)\\n},\\n}\\n}\\n```\\n","Predictions":"We will use google.protobuf.Any in state as the encoding language for the sdk.Msgs.n"}
{"File Name":"architecture\/0007-use-static-code-analysis.md","Context":"## Context\nYou never think of everything. Sticking to standards is a very good\nthing to prevent you from doing things that can go bad. Those also\nhelps making the code be more readable and structured.\n","Decision":"Use Static Code Analysis to find violations of standards.\\n","tokens":45,"id":1418,"text":"## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n\n\n##Decision\nUse Static Code Analysis to find violations of standards.\\n","Predictions":"We will use a reference file to help us understand the motivation behind the decision.n"}
{"File Name":"csc-swr-architecture\/003-Platform-and-Hosting.html.md.erb","Context":"## Context\nThe Childrens Social Care Placement service is in Alpha and needs to explore solutions to the domain problem. Given this is still very much an exploratory phase in the Software Delivery LifeCycle (SDLC) it seems appropriate to optimise around developer productivity so that the service can flex to the most appropriate fit.\n","Decision":"The decision is to use the [Gov.UK PaaS].\\n**What does Gov.UK PaaS offer?**\\nGov.UK PaaS is a fully managed platform service which has central government accredition for Offical data and provides teams with:-\\n* Fast application delivery, with short cycles\\n* Flexible development tools\\n* Simplified DevOps\\n* Seamless scalability\\n* Integrated data services\\n* A secure, trusted platform\\n* 24x7 support\\nIt is also important to note that Ruby on Rails is a first class citizen on Gov.UK as one of its core buildpack languages and frameworks.\\n**Why not DfE Azure?**\\nThere are two primary reasons why; simplified DevOps and increased developer productivity.\\n**Is it secure?**\\nOne of the main drivers for Departmental Platforms is security assurance. Gov.UK PaaS is accredited by for Official information.\\nSome useful URLs for security in general on Gov.UK PaaS are [here](https:\/\/www.cloud.service.gov.uk\/security\/) and the [security principles](https:\/\/www.cloud.service.gov.uk\/cloud-security-principles\/).\\nAnd for the DfE there is a specific [paper](https:\/\/docs.google.com\/document\/d\/1_81bqO2snLDn95p9W5OyMXwqzUqX5qcHSYJvV5CUeeQ\/edit#heading=h.du8tl2830srf) from MBS.\\n","tokens":65,"id":920,"text":"## Context\\nThe Childrens Social Care Placement service is in Alpha and needs to explore solutions to the domain problem. Given this is still very much an exploratory phase in the Software Delivery LifeCycle (SDLC) it seems appropriate to optimise around developer productivity so that the service can flex to the most appropriate fit.\\n\n\n##Decision\nThe decision is to use the [Gov.UK PaaS].\\n**What does Gov.UK PaaS offer?**\\nGov.UK PaaS is a fully managed platform service which has central government accredition for Offical data and provides teams with:-\\n* Fast application delivery, with short cycles\\n* Flexible development tools\\n* Simplified DevOps\\n* Seamless scalability\\n* Integrated data services\\n* A secure, trusted platform\\n* 24x7 support\\nIt is also important to note that Ruby on Rails is a first class citizen on Gov.UK as one of its core buildpack languages and frameworks.\\n**Why not DfE Azure?**\\nThere are two primary reasons why; simplified DevOps and increased developer productivity.\\n**Is it secure?**\\nOne of the main drivers for Departmental Platforms is security assurance. Gov.UK PaaS is accredited by for Official information.\\nSome useful URLs for security in general on Gov.UK PaaS are [here](https:\/\/www.cloud.service.gov.uk\/security\/) and the [security principles](https:\/\/www.cloud.service.gov.uk\/cloud-security-principles\/).\\nAnd for the DfE there is a specific [paper](https:\/\/docs.google.com\/document\/d\/1_81bqO2snLDn95p9W5OyMXwqzUqX5qcHSYJvV5CUeeQ\/edit#heading=h.du8tl2830srf) from MBS.\\n","Predictions":"The decision is to use the [Gov.UK PaaS].n**What does Gov.UK PaaS offer?**nGov.UK PaaS is a fully managed platform service which has central government accredition for Offical data and provides teams with:-n* Fast application delivery, with short cyclesn* Flexible development toolsn* Simplified DevOpsn* Seamless scalabilityn* Integrated data servicesn* A secure, trusted platformn* 24x7 supportnIt is also important to note that Ruby on Rails is a first class citizen on Gov.UK as one of its core buildpack languages and frameworks.nSome useful URLs for security in general on Gov.UK PaaS are [here](https:\/\/www.cloud.service.gov.uk\/security\/) and the [security principles](https:\/\/www.cloud.service.gov.uk\/cloud-security-principles\/).nAnd for the DfE there is a specific [paper](https:\/\/docs.google.com\/document\/d\/1_81bqO2snLDn95p9W5OyMXwqzUqX5qcHSYJvV5CUeeQ\/edit#heading=h.du8tl2830srf) from MBS.n"}
{"File Name":"abracadabra\/0008-don-t-propose-quick-fix-for-react-convert-to-pure-component.md","Context":"## Context\nAfter analysing performances of the extension on a big JavaScript file, [we noticed][comment] the \"Convert to Pure Component\" Quick Fix took most of the time:\n![][flame-chart]\nThis refactoring can only be applied under specific circumstances. It doesn't worth the toll for everyone, on every trigger.\nAlso, there's nothing we can do to improve the performance of this refactoring. The code is implemented by [react-codemod](https:\/\/github.com\/reactjs\/react-codemod). It was originally meant to be run through a CLI command.\n","Decision":"We will stop proposing a Quick Fix for this refactoring.\\n","tokens":122,"id":586,"text":"## Context\\nAfter analysing performances of the extension on a big JavaScript file, [we noticed][comment] the \"Convert to Pure Component\" Quick Fix took most of the time:\\n![][flame-chart]\\nThis refactoring can only be applied under specific circumstances. It doesn't worth the toll for everyone, on every trigger.\\nAlso, there's nothing we can do to improve the performance of this refactoring. The code is implemented by [react-codemod](https:\/\/github.com\/reactjs\/react-codemod). It was originally meant to be run through a CLI command.\\n\n\n##Decision\nWe will stop proposing a Quick Fix for this refactoring.\\n","Predictions":"We will use the [stack](https:\/\/github.com\/stackjs\/stack) library to build the stack.n"}
{"File Name":"latis3\/0005-Operations-defined-in-FDML-schema-as-elements.md","Context":"## Context and Problem Statement\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\n```\n<xs:element name=\"take\" type=\"xs:integer\">\n```\nor with attributes as:\n```\n<xs:element name=\"take\">\n<xs:complexType>\n<xs:attribute name=\"value\"\/>\n<\/xs:complexType>\n<\/xs:element>\n```\n## Decision Drivers\n* consistency\n* expressiveness\n","Decision":"* consistency\\n* expressiveness\\nChosen option: Try to describe operations as elements only\\n### Positive Consequences\\n* consistency\\n### Negative Consequences\\n* future operations may not be definable\\n","tokens":115,"id":4713,"text":"## Context and Problem Statement\\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\\n```\\n<xs:element name=\"take\" type=\"xs:integer\">\\n```\\nor with attributes as:\\n```\\n<xs:element name=\"take\">\\n<xs:complexType>\\n<xs:attribute name=\"value\"\/>\\n<\/xs:complexType>\\n<\/xs:element>\\n```\\n## Decision Drivers\\n* consistency\\n* expressiveness\\n\n\n##Decision\n* consistency\\n* expressiveness\\nChosen option: Try to describe operations as elements only\\n### Positive Consequences\\n* consistency\\n### Negative Consequences\\n* future operations may not be definable\\n","Predictions":"* consistencyn* expressivenessnChosen option: Try to describe operations as elements onlyn### Positive Consequencesn* consistencyn### Negative Consequencesn* future operations may not be definablen"}
{"File Name":"operational-visualizations\/0001-about-cursors.md","Context":"## Context\nIn the grid in cell function we recieve `FragmentFrame` which is vertical slice of original `DataFrame` and then when we render visualaisation we iterate through raw data with `mapRows` method of `IterableFrame` and then we need to access some exact value in raw row. One of the ideas on how to do this universally was to use some kind of **cursor**.\nCurrent implementation of **cursor** looks like this\n```tsx\nexport interface ColumnCursor<Name extends string, ValueInRawRow = any> {\n(row: RowCursor): ValueInRawRow;\ncolumn: Name;\nindex: number;\n}\n```\nso we can access value like this `cursor(row)` or more hacky and optimised way `row[cursor.index]`. Second option is fragile, in case we would want to change implementation of `row` (for example, from row-oriented storage to column-orinted) it will brake in many places (at least we have types, which will make this refactoring easier).\nCursor at the moment can be recieved from \"root\" `DataFrame`, which we think of as source of all derivative `PivotFrame`s and `FragmentFrame`s. Cursors are cached for referential transparency, so if they got passed down to React components, they will not trigger rerender.\n**Question** raised in [one of PR](https:\/\/github.com\/contiamo\/operational-visualizations\/pull\/70\/files#diff-2044c7c7ba6c3fbf04dd49cf3cfa68b9R38): Shall we provide `getCursor` functionality in `FragmentFrame`? From one side it may make code simpler, on the other side we need to make sure that `FragmentFrame`s are referentially transperent (which is not the case, for example, for `cell` method) and it will create a lot of copies of the same cursors. We can as well pass reference to \"root\" `DataFrame` along all derivative structures and \"proxy\" `getCursor` method call to it.\n","Decision":"At the moment we decided to use proposed version of cursors and adjust it in the future depending if we like DX or not. We will provide `getCursor` method only for `DataFrame` for now, but we can chnage this in the future.\\n","tokens":439,"id":977,"text":"## Context\\nIn the grid in cell function we recieve `FragmentFrame` which is vertical slice of original `DataFrame` and then when we render visualaisation we iterate through raw data with `mapRows` method of `IterableFrame` and then we need to access some exact value in raw row. One of the ideas on how to do this universally was to use some kind of **cursor**.\\nCurrent implementation of **cursor** looks like this\\n```tsx\\nexport interface ColumnCursor<Name extends string, ValueInRawRow = any> {\\n(row: RowCursor): ValueInRawRow;\\ncolumn: Name;\\nindex: number;\\n}\\n```\\nso we can access value like this `cursor(row)` or more hacky and optimised way `row[cursor.index]`. Second option is fragile, in case we would want to change implementation of `row` (for example, from row-oriented storage to column-orinted) it will brake in many places (at least we have types, which will make this refactoring easier).\\nCursor at the moment can be recieved from \"root\" `DataFrame`, which we think of as source of all derivative `PivotFrame`s and `FragmentFrame`s. Cursors are cached for referential transparency, so if they got passed down to React components, they will not trigger rerender.\\n**Question** raised in [one of PR](https:\/\/github.com\/contiamo\/operational-visualizations\/pull\/70\/files#diff-2044c7c7ba6c3fbf04dd49cf3cfa68b9R38): Shall we provide `getCursor` functionality in `FragmentFrame`? From one side it may make code simpler, on the other side we need to make sure that `FragmentFrame`s are referentially transperent (which is not the case, for example, for `cell` method) and it will create a lot of copies of the same cursors. We can as well pass reference to \"root\" `DataFrame` along all derivative structures and \"proxy\" `getCursor` method call to it.\\n\n\n##Decision\nAt the moment we decided to use proposed version of cursors and adjust it in the future depending if we like DX or not. We will provide `getCursor` method only for `DataFrame` for now, but we can chnage this in the future.\\n","Predictions":"We need to implement a getCursor function in FragmentFrame.nWe need to implement a getCursor function in FragmentFrame.nWe need to implement a getCursor function in FragmentFrame.nWe need to implement a getCursor function in FragmentFrame.nWe need to implement a getCursor function in FragmentFrame.nWe need to implement a getCursor function in FragmentFrame.nWe need to implement a getCursor function in FragmentFrame.nWe need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getCursor function in FragmentFrame. We need to implement a getC"}
{"File Name":"molgenis-r-armadillo\/0002-manage-data-in-armadillo-suite.md","Context":"## Context\nIn this project we manage the data in the Armadillo suite.\n","Decision":"We will use the MolgenisArmadillo client to manage data in the Armadillo suite. This means managing folders and files in the data backend based on R-data.\\n","tokens":18,"id":545,"text":"## Context\\nIn this project we manage the data in the Armadillo suite.\\n\n\n##Decision\nWe will use the MolgenisArmadillo client to manage data in the Armadillo suite. This means managing folders and files in the data backend based on R-data.\\n","Predictions":"We will use the MolgenisArmadillo client to manage data in the Armadillo suite. This means managing folders and files in the data backend based on R-data.n"}
{"File Name":"google-cloud-cpp\/2019-03-26-binary-distributions.md","Context":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\ndesired by all users. Some users may want to install a binary package that\ncontains our public headers and pre-compiled libraries so that they can link\nagainst our library. There is no single standard binary package manager in C++,\ninstead there are a variety of others that users might want to use (e.g., dpkg,\nrpm, vcpkg).\n**Decision**: We will not directly support nor provide any binary packages. We\nwill not test binary distributions of our code. And we will not host configs for\nbinary distributions, since that would involve hosting files which we do not\ntest. However, we will do our best to make our code easily packageable by others\nin a wide variety of formats. For more context, see\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\n**Consequences**: This decision will shield from endorsing any particular binary\npackage management system. Other individuals who want to build and maintain a\nbinary distribution of our code may do so without our permission or knowledge.\nIf they need small and reasonable tweaks from us to support their binary\ndistributions, they may file an issue and we'll do our best to accommodate them\nwithin reason.\n","Decision":"will not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n","tokens":272,"id":2462,"text":"**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n\n\n##Decision\nwill not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n","Predictions":"* We will not directly support nor provide any binary packages. Wenwill not test binary distributions of our code. And we will not host configsnfor binary distributions, since that would involve hosting files which we donnot test. However, we will do our best to make our code easily packageable bynothers in a wide variety of formats. For more context, seen[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).n* **Consequences**: This decision will shield from endorsing any particularnbinary package management system. Other individuals who want to build andnmaintain a binary distribution of our code may do so without our permission ornknowledge. If they need small and reasonable tweaks from us to support theirnbinary distributions, they may file an issue and we'll do our best tonaccommodate them within reason.n"}
{"File Name":"casa\/0004-use-bootstrap.md","Context":"## Context\nWe would like to have an easy-to-use system for consistent styles that doesn't\ntake much tinkering. We propose using the `bootstrap` gem.\n","Decision":"Pending\\n","tokens":36,"id":425,"text":"## Context\\nWe would like to have an easy-to-use system for consistent styles that doesn't\\ntake much tinkering. We propose using the `bootstrap` gem.\\n\n\n##Decision\nPending\\n","Predictions":"We've decided to implement very simple feature flags that can be toggled with environment variables.n"}
{"File Name":"verify-onboarding-prototypes\/0002-how-do-we-secure-the-api.md","Context":"## Context\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\nand the server side code (which will be a dropwizard app).\nDepending on how the users want to run the service provider we may need\ndifferent security solutions.\n","Decision":"If possible users can talk to the service provider on the loopback (127.0.0.1)\\nIf that doesn't work for some reason then they can use the dropwizard config\\nto set up basic auth or tls or something.\\nSee http:\/\/www.dropwizard.io\/1.1.0\/docs\/manual\/configuration.html#connectors\\n","tokens":59,"id":578,"text":"## Context\\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\\nand the server side code (which will be a dropwizard app).\\nDepending on how the users want to run the service provider we may need\\ndifferent security solutions.\\n\n\n##Decision\nIf possible users can talk to the service provider on the loopback (127.0.0.1)\\nIf that doesn't work for some reason then they can use the dropwizard config\\nto set up basic auth or tls or something.\\nSee http:\/\/www.dropwizard.io\/1.1.0\/docs\/manual\/configuration.html#connectors\\n","Predictions":"We will use a stack database to store the settings.n"}
{"File Name":"ansible-docker\/choose_database.md","Context":"### Context: A method of data storage is necessary to fufill the requirements for the assignment.\n* Having used mySQL in previous courses and personal projects, I found mySQL a suitable choice for supporting database capabilities.\n* Many of the examples provided by my instructor are using mySQL, thus for the benefit of staying consistent with course material mySQL is a good choice.\n### Decision: The change proposed to the current implementation is to add mySQL as a method for data storage.\n### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\n","Decision":"### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n### Consequences: Acid Transactions can be considered \"pessimistic\" and not as flexible as base transactions.\\n","tokens":138,"id":4832,"text":"### Context: A method of data storage is necessary to fufill the requirements for the assignment.\\n* Having used mySQL in previous courses and personal projects, I found mySQL a suitable choice for supporting database capabilities.\\n* Many of the examples provided by my instructor are using mySQL, thus for the benefit of staying consistent with course material mySQL is a good choice.\\n### Decision: The change proposed to the current implementation is to add mySQL as a method for data storage.\\n### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n\n\n##Decision\n### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.\\n### Consequences: Acid Transactions can be considered \"pessimistic\" and not as flexible as base transactions.\\n","Predictions":"### Consequences: The change proposed to the current implementation is to add mySQL as a method for data storage.n### Consequences: MySQL is popular, however there may be a more modern tool with additional features that could be used instead. Thus, there is a risk of limitied functionality.n### Consequences: Acid Transactions can be considered \"pessimistic\" and not as flexible as base transactions.n"}
{"File Name":"gp-finder\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3475,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"PIMS\/geocoder.md","Context":"## Context\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\nAdditionally Geocoder can be used to verify addresses that are manually entered.\n- [Geocoder](https:\/\/www2.gov.bc.ca\/gov\/content\/data\/geographic-data-services\/location-services\/geocoder)\n- [Data BC](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service)\n- [API Swagger](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service\/resource\/40d6411e-ab98-4df9-a24e-67f81c45f6fa\/view\/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\n- [Developer API Keys](https:\/\/github.com\/bcgov\/gwa\/wiki\/Developer-Guide#developer-api-keys)\n- API Host = `https:\/\/geocoder.api.gov.bc.ca`\n","Decision":"Integrate with Data BC Geocoder API.\\nWhen a user types an address a list of viable matches will be displayed.\\nIf the user selects one of the matches it will be used to set the address and GIS coordinates,.\\n","tokens":262,"id":3968,"text":"## Context\\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\\nAdditionally Geocoder can be used to verify addresses that are manually entered.\\n- [Geocoder](https:\/\/www2.gov.bc.ca\/gov\/content\/data\/geographic-data-services\/location-services\/geocoder)\\n- [Data BC](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service)\\n- [API Swagger](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service\/resource\/40d6411e-ab98-4df9-a24e-67f81c45f6fa\/view\/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\\n- [Developer API Keys](https:\/\/github.com\/bcgov\/gwa\/wiki\/Developer-Guide#developer-api-keys)\\n- API Host = `https:\/\/geocoder.api.gov.bc.ca`\\n\n\n##Decision\nIntegrate with Data BC Geocoder API.\\nWhen a user types an address a list of viable matches will be displayed.\\nIf the user selects one of the matches it will be used to set the address and GIS coordinates,.\\n","Predictions":"The data will be stored in a Docker instance.nThe data will be stored in a Docker instance.nThe data will be stored in a Docker instance.n"}
{"File Name":"identity-site\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5170,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"talktofrank-beta\/0003-site-wide-cms-configuration.md","Context":"## Context\nCertain elements on the Talk To Frank site need to be loaded regardless of the user entry point,\ne.g. the current user story involves adding a warning message bar that appears on all pages.\nThe CMS needs to be updated to support the addition of site wide 'settings'.\n","Decision":"A new 'site settings' content model will be created.  A 'Sitewide' item will then be created to contain all sitewide settings.\\nThis would then allow for the following (future) setup,\\nContent items of type 'site settings'\\n- Sitewide\\n- Homepage - any settings that should override the sitewide values on the homepage.\\n- Drugs A-Z - any settings that should override the sitewide values on the Drugs A-Z page.\\nReferences:\\n- https:\/\/www.contentful.com\/r\/knowledgebase\/dynamic-microcopy\/\\n- https:\/\/www.contentfulcommunity.com\/t\/is-it-common-to-have-a-settings-content-model\/924\/2\\n","tokens":59,"id":1894,"text":"## Context\\nCertain elements on the Talk To Frank site need to be loaded regardless of the user entry point,\\ne.g. the current user story involves adding a warning message bar that appears on all pages.\\nThe CMS needs to be updated to support the addition of site wide 'settings'.\\n\n\n##Decision\nA new 'site settings' content model will be created.  A 'Sitewide' item will then be created to contain all sitewide settings.\\nThis would then allow for the following (future) setup,\\nContent items of type 'site settings'\\n- Sitewide\\n- Homepage - any settings that should override the sitewide values on the homepage.\\n- Drugs A-Z - any settings that should override the sitewide values on the Drugs A-Z page.\\nReferences:\\n- https:\/\/www.contentful.com\/r\/knowledgebase\/dynamic-microcopy\/\\n- https:\/\/www.contentfulcommunity.com\/t\/is-it-common-to-have-a-settings-content-model\/924\/2\\n","Predictions":"The --require parameter will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow the user to specify the require field in the --require field.nThe --require field will be used to allow"}
{"File Name":"james-project\/0017-file-mail-queue-deprecation.md","Context":"## Context\nJames offers several implementation for MailQueue, a component allowing asynchronous mail processing upon smtp mail\nreception. These includes:\n- Default embedded ActiveMQ mail queue implementation, leveraging the JMS APIs and using the filesystem.\n- RabbitMQMailQueue allowing several James instances to share their MailQueue content.\n- And FileMailQueue directly leveraging the file system.\nWe introduced a junit5 test contract regarding management features, concurrency issues, and FileMailQueue do not meet this\ncontract. This results in some tests being disabled and in an unstable test suite.\nFileMailQueue tries to implement a message queue within James code, which does not really makes sense as some other projects\nalready provides one.\n","Decision":"Deprecate FileMailQueue components.\\nDisable FileMailQueue tests.\\nTarget a removal as part of 3.6.0.\\n","tokens":146,"id":2888,"text":"## Context\\nJames offers several implementation for MailQueue, a component allowing asynchronous mail processing upon smtp mail\\nreception. These includes:\\n- Default embedded ActiveMQ mail queue implementation, leveraging the JMS APIs and using the filesystem.\\n- RabbitMQMailQueue allowing several James instances to share their MailQueue content.\\n- And FileMailQueue directly leveraging the file system.\\nWe introduced a junit5 test contract regarding management features, concurrency issues, and FileMailQueue do not meet this\\ncontract. This results in some tests being disabled and in an unstable test suite.\\nFileMailQueue tries to implement a message queue within James code, which does not really makes sense as some other projects\\nalready provides one.\\n\n\n##Decision\nDeprecate FileMailQueue components.\\nDisable FileMailQueue tests.\\nTarget a removal as part of 3.6.0.\\n","Predictions":"We will use swift-swift-api to implement the swift-swift-api interface.n"}
{"File Name":"docspell\/0007_convert_html_files.md","Context":"# Context and Problem Statement\nHow can HTML documents be converted into a PDF file that looks as much\nas possible like the original?\nIt would be nice to have a java-only solution. But if an external tool\nhas a better outcome, then an external tool is fine, too.\nSince Docspell is free software, the tools must also be free.\n# Considered Options\n* [pandoc](https:\/\/pandoc.org\/) external command\n* [wkhtmltopdf](https:\/\/wkhtmltopdf.org\/) external command\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\nNative (firefox) view:\n{{ figure(file=\"example-html-native.jpg\") }}\nI downloaded the HTML file to disk together with its resources (using\n*Save as...* in the browser).\n","Decision":"wkhtmltopdf.\\nIt shows the best results.\\n","tokens":176,"id":287,"text":"# Context and Problem Statement\\nHow can HTML documents be converted into a PDF file that looks as much\\nas possible like the original?\\nIt would be nice to have a java-only solution. But if an external tool\\nhas a better outcome, then an external tool is fine, too.\\nSince Docspell is free software, the tools must also be free.\\n# Considered Options\\n* [pandoc](https:\/\/pandoc.org\/) external command\\n* [wkhtmltopdf](https:\/\/wkhtmltopdf.org\/) external command\\n* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external command\\nNative (firefox) view:\\n{{ figure(file=\"example-html-native.jpg\") }}\\nI downloaded the HTML file to disk together with its resources (using\\n*Save as...* in the browser).\\n\n\n##Decision\nwkhtmltopdf.\\nIt shows the best results.\\n","Predictions":"* [Apache POI](https:\/\/poi.apache.org) together withn[this](https:\/\/central.sonatype.com\/artifact\/fr.opensagres.xdocreport\/org.apache.poi.xwpf.converter.pdf\/1.0.6)nlibraryn* [pandoc](https:\/\/pandoc.org\/) external commandn* [abiword](https:\/\/www.abisource.com\/) external commandn* [Unoconv](https:\/\/github.com\/unoconv\/unoconv) external commandnNative (firefox) view:n figure(file=\"example-html-native.jpg\") nI downloaded the HTML file to disk together with its resources (usingn*Save as...* in the browser).n"}
{"File Name":"gsp\/ADR022-logging.md","Context":"## Context\nWe have various log sources:\n* The EKS control plane (audit logs, api service, scheduler, control-manager)\n* VPC Flow logs\n* Load Balancer\n* Pod\/Application logs\n* CloudHSM\nAll of these with the exception of the Pod\/Application logs are stored by AWS in [CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/).\nWe would like a single storage location for indexing and search our logs for auditing and debugging purposes.\nGDS currently have several common storage locations for logs:\n* Logit.io (a SaaS ELK stack provider)\n* Self hosted ELK stacks\n* CloudWatch\n* Splunk\nOptions:\n### Option 1:\nWe could ship the Cloudwatch logs to logit.io using AWS lambda and ship the Pod\/Application logs to Logit.io using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Logit.io instance and would end up duplicating a large number of the logs in both CloudWatch and Logit.io\n### Option 2:\nWe could host a dedicate ELK stack (either in cluster or from AWS's managed offering) and ingest logs from both Pods and CloudWatch into the ELK stack. Managing ELK stacks has been a maintenance burden at GDS previously and this would require duplicating logs already stored in CloudWatch.\n### Option 3:\nWe could ship the Pod\/Application logs to CloudWatch using [fluentd](https:\/\/www.fluentd.org\/) and expose CloudWatch insights interface to users of the platform\n### Option 4:\nWe could ship the CloudWatch logs to Splunk using AWS lambda and ship the Pod\/Application logs to Splunk using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Splunk instance and would end up duplicating a large number of the logs in both CloudWatch and Splunk.\n","Decision":"We will use [fluentd](https:\/\/www.fluentd.org\/) to ship pod\/application logs to [AWS CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/) to aggregate all platform\/application logs to avoid double spending on log storage.\\n","tokens":416,"id":3914,"text":"## Context\\nWe have various log sources:\\n* The EKS control plane (audit logs, api service, scheduler, control-manager)\\n* VPC Flow logs\\n* Load Balancer\\n* Pod\/Application logs\\n* CloudHSM\\nAll of these with the exception of the Pod\/Application logs are stored by AWS in [CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/).\\nWe would like a single storage location for indexing and search our logs for auditing and debugging purposes.\\nGDS currently have several common storage locations for logs:\\n* Logit.io (a SaaS ELK stack provider)\\n* Self hosted ELK stacks\\n* CloudWatch\\n* Splunk\\nOptions:\\n### Option 1:\\nWe could ship the Cloudwatch logs to logit.io using AWS lambda and ship the Pod\/Application logs to Logit.io using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Logit.io instance and would end up duplicating a large number of the logs in both CloudWatch and Logit.io\\n### Option 2:\\nWe could host a dedicate ELK stack (either in cluster or from AWS's managed offering) and ingest logs from both Pods and CloudWatch into the ELK stack. Managing ELK stacks has been a maintenance burden at GDS previously and this would require duplicating logs already stored in CloudWatch.\\n### Option 3:\\nWe could ship the Pod\/Application logs to CloudWatch using [fluentd](https:\/\/www.fluentd.org\/) and expose CloudWatch insights interface to users of the platform\\n### Option 4:\\nWe could ship the CloudWatch logs to Splunk using AWS lambda and ship the Pod\/Application logs to Splunk using something like [fluentd](https:\/\/www.fluentd.org\/). This would assume that all users of the platform have a Splunk instance and would end up duplicating a large number of the logs in both CloudWatch and Splunk.\\n\n\n##Decision\nWe will use [fluentd](https:\/\/www.fluentd.org\/) to ship pod\/application logs to [AWS CloudWatch](https:\/\/aws.amazon.com\/cloudwatch\/) to aggregate all platform\/application logs to avoid double spending on log storage.\\n","Predictions":"We would like to use a single storage location for indexing and search our logs.nWe would like to use a single storage location for indexing and search our logs for auditing and debugging purposes.nGDS currently have several common storage locations for logs:n* Logit.io (a SaaS ELK stack provider)n* Self hosted ELK stacksn* CloudWatchn* SplunknOptions:n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n* [Splunk](https:\/\/aws.amazon.com\/cloudwatch\/)n"}
{"File Name":"origin\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1562,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"connecting-to-services\/0015-add-info-page.md","Context":"## Context\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\n","Decision":"The application will provide an info page.\\n","tokens":55,"id":2198,"text":"## Context\\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\\n\n\n##Decision\nThe application will provide an info page.\\n","Predictions":"The application will provide an info page.n"}
{"File Name":"automate\/adr-2019-01-23.md","Context":"## Context\nPreviously, we did not have any rules for governing Automate UI route\nlifecycle. In the past, we likely broke backwards compatibility when\nre-naming or moving routes.\nAdditionally, the top nav highlighting in Automate UI was driven off the\nroot route (e.g. everything under the `Settings` tab must live under the `\/settings`\nroute or it will not be highlighted properly for pages that live under it).\nThis means that if you want to move a page to a new tab, it will require a new route.\n","Decision":"Every route that originally linked to a page that is still in use in the webapp\\nmust still work and backwards compatibility must be maintained. Only when the webpage\\nthat the original route linked to is being removed from the app can a route be removed.\\nThis is so that deep linking from our own sources (blog posts, etc.) as well as links\\ncustomers use internally will continue to work until they no longer make sense at all\\nand the original resource no longer exists.\\nTo maintain backwards compatibility, the original route must redirect to the new route\\n(achievable in `app-routing.module.ts`) and an e2e test must be added to verify\\nthat redirection in `deprecated-routes.e2e-spec.ts`. See\\n[this commit](https:\/\/github.com\/chef\/automate\/commit\/38779c5aabb1af8d6c22e4b6e1b07eaf9c8fa06c)\\nfor an example of routes being renamed with proper backwards compatibility, what the e2e\\ntests should look like, and how a page can move between topnav tabs.\\n","tokens":115,"id":1210,"text":"## Context\\nPreviously, we did not have any rules for governing Automate UI route\\nlifecycle. In the past, we likely broke backwards compatibility when\\nre-naming or moving routes.\\nAdditionally, the top nav highlighting in Automate UI was driven off the\\nroot route (e.g. everything under the `Settings` tab must live under the `\/settings`\\nroute or it will not be highlighted properly for pages that live under it).\\nThis means that if you want to move a page to a new tab, it will require a new route.\\n\n\n##Decision\nEvery route that originally linked to a page that is still in use in the webapp\\nmust still work and backwards compatibility must be maintained. Only when the webpage\\nthat the original route linked to is being removed from the app can a route be removed.\\nThis is so that deep linking from our own sources (blog posts, etc.) as well as links\\ncustomers use internally will continue to work until they no longer make sense at all\\nand the original resource no longer exists.\\nTo maintain backwards compatibility, the original route must redirect to the new route\\n(achievable in `app-routing.module.ts`) and an e2e test must be added to verify\\nthat redirection in `deprecated-routes.e2e-spec.ts`. See\\n[this commit](https:\/\/github.com\/chef\/automate\/commit\/38779c5aabb1af8d6c22e4b6e1b07eaf9c8fa06c)\\nfor an example of routes being renamed with proper backwards compatibility, what the e2e\\ntests should look like, and how a page can move between topnav tabs.\\n","Predictions":"We will use a stack property to determine the url of the web app html file.n"}
{"File Name":"opg-data-lpa-codes\/0001-flask-in-aws-lambda-function.md","Context":"## Context\nFor this project, we would like to package up the API endpoints and logic into a small Flask app in a single lambda function.\nWe found whilst working on the Documents integration that managing multiple lambda functions quickly became quite hard work,\nespecially as in that project there was a lot of shared code about the place. In hindsight, we should have refactored the\nshared code into separate lambda functions, but we never got to it due to time constraints. Also this would just give us\nmore lambda functions to maintain.\n#### Why Flask (or any other WSGI app) in a Lambda is a good idea\n* We can re-use most of the infra setup from the last project so we can get started fast\n* Easy to run and develop locally, just run like a normal Flask app\n* Easy to test as it's a normal Flask app\n* I have written a lot of Flask apps(!) and so have a LOT of other people, so documentation is plentiful and well established\n* Is it an anti-pattern? AWS have a library called Chalice which does a very similar thing to how we propose to use Flask\n(but with more deployment stuff in that we don't need), so if AWS have an official package that does it, can it be an\nanti-pattern?\n* Maintaining a single lambda function is much easier than maintaining many\n* This project is very small and has very well defined boundaries. None of its data or other artifacts will be accessed\nby anything other than its own API, meaning it's completely independent of any other service (excluding AWS security) so\nit's a good candidate for experimenting with this, as nothing will need to be reused outside of this app\n#### Why Flask (or any other WSGI app) in a Lambda is a bad idea\n* A whole app in a FaaS seems like an anti-pattern (can't find any evidence to support\/contradict this)\n* Potentially longer cold-start as it has to initialise the whole app not just a single function\n* [Flask-Lambda](https:\/\/github.com\/sivel\/flask-lambda) is a small package with only 1 contributor, though it is a pretty\nsimple script that just maps some request variables depending on the source of the request\n","Decision":"* Single lambda function containing small Flask app that provides all endpoints\\n* Flask-Lambda to help easily switch between local dev and AWS\\n","tokens":469,"id":1540,"text":"## Context\\nFor this project, we would like to package up the API endpoints and logic into a small Flask app in a single lambda function.\\nWe found whilst working on the Documents integration that managing multiple lambda functions quickly became quite hard work,\\nespecially as in that project there was a lot of shared code about the place. In hindsight, we should have refactored the\\nshared code into separate lambda functions, but we never got to it due to time constraints. Also this would just give us\\nmore lambda functions to maintain.\\n#### Why Flask (or any other WSGI app) in a Lambda is a good idea\\n* We can re-use most of the infra setup from the last project so we can get started fast\\n* Easy to run and develop locally, just run like a normal Flask app\\n* Easy to test as it's a normal Flask app\\n* I have written a lot of Flask apps(!) and so have a LOT of other people, so documentation is plentiful and well established\\n* Is it an anti-pattern? AWS have a library called Chalice which does a very similar thing to how we propose to use Flask\\n(but with more deployment stuff in that we don't need), so if AWS have an official package that does it, can it be an\\nanti-pattern?\\n* Maintaining a single lambda function is much easier than maintaining many\\n* This project is very small and has very well defined boundaries. None of its data or other artifacts will be accessed\\nby anything other than its own API, meaning it's completely independent of any other service (excluding AWS security) so\\nit's a good candidate for experimenting with this, as nothing will need to be reused outside of this app\\n#### Why Flask (or any other WSGI app) in a Lambda is a bad idea\\n* A whole app in a FaaS seems like an anti-pattern (can't find any evidence to support\/contradict this)\\n* Potentially longer cold-start as it has to initialise the whole app not just a single function\\n* [Flask-Lambda](https:\/\/github.com\/sivel\/flask-lambda) is a small package with only 1 contributor, though it is a pretty\\nsimple script that just maps some request variables depending on the source of the request\\n\n\n##Decision\n* Single lambda function containing small Flask app that provides all endpoints\\n* Flask-Lambda to help easily switch between local dev and AWS\\n","Predictions":"* We can re-use most of the infra setup from the last project so we can get started fastn* Easy to run and develop locally, just run like a normal Flask appn* Easy to test as it's a normal Flask appn* I have written a lot of Flask apps(!) and so have a LOT of other people, so documentation is plentiful and well establishedn* Is it an anti-pattern? AWS have a library called Chalice which does a very similar thing to how we propose to use Flaskn(but with more deployment stuff in that we don't need), so if AWS have an official package that does it, can it be annanti-pattern?n* Maintaining a single lambda function is much easier than maintaining manyn* This project is very small and has very well defined boundaries. None of its data or other artifacts will be accessednby anything other than its own API, meaning it's completely independent of any other service (excluding AWS security) sonit's a good candidate for experimenting with this, as nothing will need to be reused outside of this appn### Why Flask (or any other WSGI app) in a Lambda is a good idean* We can re-use most of the infra setup from the last project so we can get started fastn* Easy to run and develop locally, just run like a normal Flask appn* Easy to test as it's a normal Flask appn* I have written a lot of Flask apps(!) and so have a LOT of other people, so documentation is plentiful and well establishedn* Is it an anti-pattern? AWS have a library called Chalice which does a very similar thing to how we propose to use Flaskn(but with more deployment stuff in that we don't need), so if AWS have an official package that does it, can it be annanti-pattern?n* Maintaining a single lambda function is much easier than maintaining manyn* This project is very small and has very well defined boundaries. None of its data or other artifacts will be accessednby anything other than its own API, meaning it's completely independent of any other service (excluding AWS security) sonit's a good candidate for experimenting with this, as nothing will need to be reused outside of this appn### Why Flask (or any other WSGI app) in a Lambda is a bad idean* A whole app in a FaaS seems like an anti-pattern (can't find any evidence to support\/contradict this)n* Potentially longer app development timen"}
{"File Name":"Marain.Instance\/0005-multitenancy-approach-for-marain.md","Context":"## Context\nTenancy has always been a first class citizen of all Marain services, however this by itself is not enough to make the system truly multitenanted. In order to do this, we need to determine how tenants should be created, managed and used within the Marain \"world\".\nWe would like the option of deploying Marain as either a managed service, hosted by us and licenced to users as a PaaS offering, or for clients to deploy private instances into their own cloud subscriptions. We also want to give clients of the managed services the option for data to be stored in their own storage accounts or databases, but still have us run the compute aspects of the platform on their behalf. This also extends to those clients who are using Marain to implement their own multi-tenanted services: these clients should also be able to isolate their own clients' storage.\nIn addition, we need to be able to differentiate between a Marain service being available for a client to use directly and one being used as a dependency of a service they are using. For example, the Workflow service makes use of the Operations service. As a result, clients that are licenced to use the Workflow service will be using the Operations service indirectly, despite the fact that they may not be licenced to use it directly.\nWe need to define a tenancy model that will support these scenarios and can be implemented using the `Marain.Tenancy` service.\n","Decision":"To support this, we have made the following decisions\\n1. Every client using a Marain instance will have a Marain tenant created for them. For the remainder of this document, these will be referred to as \"Client Tenants\".\\n1. Every Marain service will also have a Marain tenant created for it. For the remainder of this document, these will be referred to as \"Service Tenants\".\\n1. We will make use of the tenant hierarchy to group Client Tenants and Service Tenants under their own top-level parent. This means that we will have a top-level tenant called \"Client Tenants\" which parents all of the Client Tenants, and an equivalent one called \"Service Tenants\" that parents the Service Tenants (this is shown in the diagram below).\\n1. Clients will access the Marain services they are licenced for using their own tenant Id. Whilst the Marain services themselves expect this to be supplied as part of endpoint paths, there is nothing to prevent an API Gateway (e.g. Azure API Management) being put in front of this so that custom URLs can be mapped to tenants, or so that tenant IDs can be passed in headers.\\n1. When a Marain service depends on another one as part of an operation, it will pass the Id of a tenant that is a subtenant of it's own Service Tenant. This subtenant will be specific to the client that is making the original call. For example, the Workflow service has a dependency on the Operations Control service. If there are two Client Tenants for the Workflow Service, each will have a corresponding sub-tenant of the Workflow Service Tenant and these will be used to make the call to the Operation service. This approach allows the depended-upon service to be used on behalf of the client without making it available for direct usage.\\nEach of these tenants - Client, Service, and the client-specific sub-tenants of the Service Tenants - will need to hold configuration appropriate for their expected use cases. This will normally be any required storage configuration for the services they use, plus the Ids of any subtenants that have been created for them in those services, but could also include other things.\\nAs an example, suppose we have two customers; Contoso and Litware. For these customers to be able to use Marain, we must create Contoso and Litware tenants. We also have two Marain services available, Workflow and Operations. These also have tenants created for them (in the following diagrams, Service Tenants are shown in ALL CAPS and Client Tenants in normal sentence case. Service-specific client subtenants use a mix to indicate what they relate to):\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n```\\nContoso is licenced to use Workflow, and Litware is licenced to use both Workflow and Operations. This means that:\\n- The Contoso tenant will contain storage configuration for the Workflow service (as with all this configuration, the onboarding process will default this to standard Marain storage, where data is siloed by tenant in shared storage accounts - e.g. a single Cosmos database containing a collection per tenant. However, clients can supply their own storage configuration where required).\\n- The Litware tenant will contain storage configuration for both Workflow and Operations services, because it uses both directly.\\nIn addition, because both clients are licenced for workflow, they will each have a sub-tenant of the Workflow Service Tenant, containing the storage configuration that should be used with the Operations service. The Operations service does not have any sub-tenants because it does not have dependencies on any other Marain services:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|\\n+-> OPERATIONS\\n```\\nAs can be seen from the above, each tenant holds appropriate configuration for the services they use directly. In the case of the Client Tenants, they also hold the Id of the sub-tenant that the Workflow service will use when calling out to the Operations service on their behalf; this is necessary to avoid a costly search for the correct sub-tenant to use.\\nYou will notice from the above that Litware ends up with two sets of configuration for Operations storage; that which is employed when using the Operations service directly, and that used when calling the Workflow service and thus using the Operations service indirectly. This gives clients the maximum flexibility in controlling where their data is stored.\\nNow let's look at a slightly more complex example. Imagine in the scenario above, there is a third service, which we'll just call the FooBar service, and that both the Workflow and Operations service are dependent on it. In addition, Contoso are licenced to use it directly. This is what the dependency graph now looks like:\\n```\\n+------------+\\n|            |\\n+-------> WORKFLOW   +------+-----------------+\\n+---------+       |       |            |      |                 |\\n|         +-------+       +-^----------+      |                 |\\n| Contoso |                 |                 |                 |\\n|         |                 |                 |                 |\\n+----+----+                 |           +-----v------+          |\\n|                      |           |            |          |\\n|                      |     +-----> OPERATIONS +----+     |\\n|      +---------+     |     |     |            |    |     |\\n|      |         +-----+     |     +------------+    |     |\\n|      | Litware |           |                       |     |\\n|      |         +-----------+                       |     |\\n|      +---------+                               +---v-----v--+\\n|                                                |            |\\n+------------------------------------------------> FOOBAR     |\\n|            |\\n+------------+\\n```\\nIn order to support this, we start with an additional Service Tenant for the FooBar tenant.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso to use the Workflow service. This causes a chain of enrollments whereby a sub-tenant is created for WORKFLOW+Contoso, which is then enrolled to use the Operations service, creating a sub-tenant of OPERATIONS, OPERATIONS+WORKFLOW+Contoso, which is then enrolled to use the FooBar service (since FooBar does not have dependencies, this does not create any further sub tenants). The Workflow service is also directly dependent on FooBar, so WORKFLOW+Contoso is also enrolled to use FooBar resulting in storage configuration for FooBar being added to it.\\nThis leaves the tenant hierarchy looking like this:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso for the FooBar service. Since there are no additional dependencies, this does not result in any further sub-tenants being created, but does add storage configuration for FooBar to the Contoso tenant. As in the first example, Contoso now has two sets of storage configuration for the FooBar service, one for direct use and one for indirect use.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe now repeat the process of enrolling Litware for the Workflow service:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nSince Litware is not licenced to use FooBar, the Litware Client Tenant does not hold any configuration for that service itself.\\nFinally, we enroll Litware to use the Operations service. In this example, because Operations depends on FooBar, we need to create another sub-tenant of Operations to call FooBar with when Litware uses Operations directly, and enroll this new subtenant with FooBar. This leaves us with the following:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\n","tokens":297,"id":359,"text":"## Context\\nTenancy has always been a first class citizen of all Marain services, however this by itself is not enough to make the system truly multitenanted. In order to do this, we need to determine how tenants should be created, managed and used within the Marain \"world\".\\nWe would like the option of deploying Marain as either a managed service, hosted by us and licenced to users as a PaaS offering, or for clients to deploy private instances into their own cloud subscriptions. We also want to give clients of the managed services the option for data to be stored in their own storage accounts or databases, but still have us run the compute aspects of the platform on their behalf. This also extends to those clients who are using Marain to implement their own multi-tenanted services: these clients should also be able to isolate their own clients' storage.\\nIn addition, we need to be able to differentiate between a Marain service being available for a client to use directly and one being used as a dependency of a service they are using. For example, the Workflow service makes use of the Operations service. As a result, clients that are licenced to use the Workflow service will be using the Operations service indirectly, despite the fact that they may not be licenced to use it directly.\\nWe need to define a tenancy model that will support these scenarios and can be implemented using the `Marain.Tenancy` service.\\n\n\n##Decision\nTo support this, we have made the following decisions\\n1. Every client using a Marain instance will have a Marain tenant created for them. For the remainder of this document, these will be referred to as \"Client Tenants\".\\n1. Every Marain service will also have a Marain tenant created for it. For the remainder of this document, these will be referred to as \"Service Tenants\".\\n1. We will make use of the tenant hierarchy to group Client Tenants and Service Tenants under their own top-level parent. This means that we will have a top-level tenant called \"Client Tenants\" which parents all of the Client Tenants, and an equivalent one called \"Service Tenants\" that parents the Service Tenants (this is shown in the diagram below).\\n1. Clients will access the Marain services they are licenced for using their own tenant Id. Whilst the Marain services themselves expect this to be supplied as part of endpoint paths, there is nothing to prevent an API Gateway (e.g. Azure API Management) being put in front of this so that custom URLs can be mapped to tenants, or so that tenant IDs can be passed in headers.\\n1. When a Marain service depends on another one as part of an operation, it will pass the Id of a tenant that is a subtenant of it's own Service Tenant. This subtenant will be specific to the client that is making the original call. For example, the Workflow service has a dependency on the Operations Control service. If there are two Client Tenants for the Workflow Service, each will have a corresponding sub-tenant of the Workflow Service Tenant and these will be used to make the call to the Operation service. This approach allows the depended-upon service to be used on behalf of the client without making it available for direct usage.\\nEach of these tenants - Client, Service, and the client-specific sub-tenants of the Service Tenants - will need to hold configuration appropriate for their expected use cases. This will normally be any required storage configuration for the services they use, plus the Ids of any subtenants that have been created for them in those services, but could also include other things.\\nAs an example, suppose we have two customers; Contoso and Litware. For these customers to be able to use Marain, we must create Contoso and Litware tenants. We also have two Marain services available, Workflow and Operations. These also have tenants created for them (in the following diagrams, Service Tenants are shown in ALL CAPS and Client Tenants in normal sentence case. Service-specific client subtenants use a mix to indicate what they relate to):\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n```\\nContoso is licenced to use Workflow, and Litware is licenced to use both Workflow and Operations. This means that:\\n- The Contoso tenant will contain storage configuration for the Workflow service (as with all this configuration, the onboarding process will default this to standard Marain storage, where data is siloed by tenant in shared storage accounts - e.g. a single Cosmos database containing a collection per tenant. However, clients can supply their own storage configuration where required).\\n- The Litware tenant will contain storage configuration for both Workflow and Operations services, because it uses both directly.\\nIn addition, because both clients are licenced for workflow, they will each have a sub-tenant of the Workflow Service Tenant, containing the storage configuration that should be used with the Operations service. The Operations service does not have any sub-tenants because it does not have dependencies on any other Marain services:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|\\n+-> OPERATIONS\\n```\\nAs can be seen from the above, each tenant holds appropriate configuration for the services they use directly. In the case of the Client Tenants, they also hold the Id of the sub-tenant that the Workflow service will use when calling out to the Operations service on their behalf; this is necessary to avoid a costly search for the correct sub-tenant to use.\\nYou will notice from the above that Litware ends up with two sets of configuration for Operations storage; that which is employed when using the Operations service directly, and that used when calling the Workflow service and thus using the Operations service indirectly. This gives clients the maximum flexibility in controlling where their data is stored.\\nNow let's look at a slightly more complex example. Imagine in the scenario above, there is a third service, which we'll just call the FooBar service, and that both the Workflow and Operations service are dependent on it. In addition, Contoso are licenced to use it directly. This is what the dependency graph now looks like:\\n```\\n+------------+\\n|            |\\n+-------> WORKFLOW   +------+-----------------+\\n+---------+       |       |            |      |                 |\\n|         +-------+       +-^----------+      |                 |\\n| Contoso |                 |                 |                 |\\n|         |                 |                 |                 |\\n+----+----+                 |           +-----v------+          |\\n|                      |           |            |          |\\n|                      |     +-----> OPERATIONS +----+     |\\n|      +---------+     |     |     |            |    |     |\\n|      |         +-----+     |     +------------+    |     |\\n|      | Litware |           |                       |     |\\n|      |         +-----------+                       |     |\\n|      +---------+                               +---v-----v--+\\n|                                                |            |\\n+------------------------------------------------> FOOBAR     |\\n|            |\\n+------------+\\n```\\nIn order to support this, we start with an additional Service Tenant for the FooBar tenant.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|\\n+-> OPERATIONS\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso to use the Workflow service. This causes a chain of enrollments whereby a sub-tenant is created for WORKFLOW+Contoso, which is then enrolled to use the Operations service, creating a sub-tenant of OPERATIONS, OPERATIONS+WORKFLOW+Contoso, which is then enrolled to use the FooBar service (since FooBar does not have dependencies, this does not create any further sub tenants). The Workflow service is also directly dependent on FooBar, so WORKFLOW+Contoso is also enrolled to use FooBar resulting in storage configuration for FooBar being added to it.\\nThis leaves the tenant hierarchy looking like this:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe then enroll Contoso for the FooBar service. Since there are no additional dependencies, this does not result in any further sub-tenants being created, but does add storage configuration for FooBar to the Contoso tenant. As in the first example, Contoso now has two sets of storage configuration for the FooBar service, one for direct use and one for indirect use.\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nWe now repeat the process of enrolling Litware for the Workflow service:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\nSince Litware is not licenced to use FooBar, the Litware Client Tenant does not hold any configuration for that service itself.\\nFinally, we enroll Litware to use the Operations service. In this example, because Operations depends on FooBar, we need to create another sub-tenant of Operations to call FooBar with when Litware uses Operations directly, and enroll this new subtenant with FooBar. This leaves us with the following:\\n```\\nRoot tenant\\n|\\n+-> Client Tenants\\n|     |\\n|     +-> Contoso\\n|     |     +-> (Workflow storage configuration)\\n|     |     +-> (The Id of the WORKFLOW+Contoso sub-tenant for the Workflow service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> Litware\\n|           +-> (Workflow storage configuration)\\n|           +-> (The Id of the WORKFLOW+Litware sub-tenant for the Workflow service to use)\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+Litware sub-tenant for the Operations service to use)\\n|\\n+-> Service Tenants\\n|\\n+-> WORKFLOW\\n|     |\\n|     +-> WORKFLOW+Contoso\\n|     |     +-> (Operations storage configuration)\\n|     |     +-> (The Id of the OPERATIONS+WORKFLOW+Contoso sub-tenant for the Operations service to use)\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> WORKFLOW+Litware\\n|           +-> (Operations storage configuration)\\n|           +-> (The Id of the OPERATIONS+WORKFLOW+Litware sub-tenant for the Operations service to use)\\n|           +-> (FooBar storage configuration)\\n|\\n+-> OPERATIONS\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Contoso\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+WORKFLOW+Litware\\n|     |     +-> (FooBar storage configuration)\\n|     |\\n|     +-> OPERATIONS+Litware\\n|           +-> (FooBar storage configuration)\\n|\\n+-> FOOBAR\\n```\\n","Predictions":"We will use the Marain.Tenancy service to manage the tenancy of our services.n"}
{"File Name":"moneycount-api\/004-Use_Heroku_As_Deployment.md","Context":"## Context\nIt's desirable to keep the project online for testing purposes, and it has to be an easy to use environment with no cost.\n","Decision":"I decided to use Heroku, as it has an easy to use environment, CLI tool that makes it easy to deploy with just few commands and has a free plan. It also has a great integration with lots of platforms and recognizes Spring Boot out of the box.\\n","tokens":30,"id":3070,"text":"## Context\\nIt's desirable to keep the project online for testing purposes, and it has to be an easy to use environment with no cost.\\n\n\n##Decision\nI decided to use Heroku, as it has an easy to use environment, CLI tool that makes it easy to deploy with just few commands and has a free plan. It also has a great integration with lots of platforms and recognizes Spring Boot out of the box.\\n","Predictions":"I decided to use PostgreSQL, because it's a well-supported, well-supported, and well-supported database.n"}
{"File Name":"gp-finder\/0006-use-prometheus-for-exposing-metrics.md","Context":"## Context\nWe need to know what the application is doing in a more light weight way than\nscraping logs. We need to be able to monitor KPIs of the application in order\nto understand the health of the application. This will allow us to react and\npotentially pro-actively initiate measures as to ensure the application's\nhealth if sound. Ultimately providing a better service for our users.\n","Decision":"We will use Prometheus to monitor and alert on the state of the application.\\n","tokens":86,"id":3469,"text":"## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n\n\n##Decision\nWe will use Prometheus to monitor and alert on the state of the application.\\n","Predictions":"We will use Prometheus to monitor and alert on the state of the application.n"}
{"File Name":"cdk\/002-component-constuctors.md","Context":"## Context\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\nThis project contains a large number of classes, making up the various constructs and patterns. The project is intended to be used as a component library and, therefore, used by a number of people who don't have extensive knowledge of either the CDK or this library. It is therefore important to make the experience of using these classes as intuitive as possible.\n","Decision":"<!-- What is the change that we're proposing and\/or doing? -->\\nConstructors should follow the following rules for consistency.\\n1. The first parameter should be a `scope` of type `GuStack`:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then have a static `id` as it will never change, for example the `Stage` parameter.\\n2. They can also take a `props` object which should be correctly typed:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack, props: object) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then derive `id` from `props` as it will never change, for example `InstanceTypeFor${props.app}`.\\n3. They can also take an `id` of type string and a `props` object which should be correctly typed\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {...}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: any, props: object) {\\n...\\n}\\n}\\n```\\n4. Where all `props` are optional, the `props` object should be optional as a whole\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props?: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n","tokens":95,"id":1193,"text":"## Context\\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\\nThis project contains a large number of classes, making up the various constructs and patterns. The project is intended to be used as a component library and, therefore, used by a number of people who don't have extensive knowledge of either the CDK or this library. It is therefore important to make the experience of using these classes as intuitive as possible.\\n\n\n##Decision\n<!-- What is the change that we're proposing and\/or doing? -->\\nConstructors should follow the following rules for consistency.\\n1. The first parameter should be a `scope` of type `GuStack`:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then have a static `id` as it will never change, for example the `Stage` parameter.\\n2. They can also take a `props` object which should be correctly typed:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack, props: object) {\\n...\\n}\\n}\\n```\\nThe construct\/pattern will then derive `id` from `props` as it will never change, for example `InstanceTypeFor${props.app}`.\\n3. They can also take an `id` of type string and a `props` object which should be correctly typed\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {...}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: any, props: object) {\\n...\\n}\\n}\\n```\\n4. Where all `props` are optional, the `props` object should be optional as a whole\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props?: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n","Predictions":"-!-- What is the change that we're proposing and\/or doing? -->n- The first parameter should be a scope of type GuStack:n:white_check_mark: Validntsnclass MyConstruct nbuilder(scope: GuStack) n...nnn:x: Invalidntsnclass MyConstruct nbuilder(scope: Stack, props: MyConstructProps) n...nnn- The construct\/pattern will then have a static id as it will never change, for example the Stage parameter.n- The construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n- The construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n- The construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n- The construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n- The construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n- The construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n- The construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n- The construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n- The construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n- The construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n- The construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n- The construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n- The construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n- The construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n- The construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n- The construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n- The construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n- The construct\/pattern will then derive id from props as it will never change, for example InstanceTypeFor$props.app.n- The construct\/pattern will then derive id from "}
{"File Name":"register-trainee-teachers\/0004-non-integer-trainee-ids.md","Context":"## Context\nWe have been using trainee IDs in the URL as per standard Rails convention\/operation.\nIt was felt that we should use non-integer IDs for a number of reasons:\n* remove predictability\n* interoperability with other systems without depending on DB IDs\n","Decision":"We chose to use option 3 as it met the needs we had with the minimum of effort and avoided the really long URLs that\\noption 1 would have caused.\\n","tokens":57,"id":3802,"text":"## Context\\nWe have been using trainee IDs in the URL as per standard Rails convention\/operation.\\nIt was felt that we should use non-integer IDs for a number of reasons:\\n* remove predictability\\n* interoperability with other systems without depending on DB IDs\\n\n\n##Decision\nWe chose to use option 3 as it met the needs we had with the minimum of effort and avoided the really long URLs that\\noption 1 would have caused.\\n","Predictions":"Use Rollbar to collect and manage our application errors.n"}
{"File Name":"linshare-mobile-android-app\/0011-upload-document.md","Context":"## Context\nAt this time, we used `android.net.Uri` to extract information and open an `InputStream` to upload.\nBut, an `Uri` has `UriPermission` protect the data which is represented.\nThis permission will be revoked by shared application when the received Activity no-longer run.\nIt is not critical to Android 7, but with Android 9 we always get `Permission Denied` when deliver Uri to Worker to execute.\n","Decision":"- We extract all requirement information of the Document at the time we receive the Intent.\\n- Instead of using Uri directly, we create a temporary file to store the file which Uri represent\\n- We deliver temporary file path to Worker\\n","tokens":94,"id":1643,"text":"## Context\\nAt this time, we used `android.net.Uri` to extract information and open an `InputStream` to upload.\\nBut, an `Uri` has `UriPermission` protect the data which is represented.\\nThis permission will be revoked by shared application when the received Activity no-longer run.\\nIt is not critical to Android 7, but with Android 9 we always get `Permission Denied` when deliver Uri to Worker to execute.\\n\n\n##Decision\n- We extract all requirement information of the Document at the time we receive the Intent.\\n- Instead of using Uri directly, we create a temporary file to store the file which Uri represent\\n- We deliver temporary file path to Worker\\n","Predictions":"We will use stack to store the stack_name and stack_name fields in the stack_name field.n"}
{"File Name":"cloud-sdk-js\/0021-odata-url-builder.md","Context":"## Context\nThe current request builder APIs are not able to handle some odata requests like:\n- query navigation properties `GET \/People('scottketchum')\/Friends`\n- getting \"raw value\" of a property `\/People('scottketchum')\/$value`\n","Decision":"- Implement A for now as a powerful workaround.\\n- Proposal B\/C\/variant will be a `2.0` task, where it seems C might be the winner and we might review the decision later as they close to each other.\\nAt least, implement it as a separate task so we have a workaround for custom URL.\\n### Proposal B\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends, 'scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend) \/\/ single item can continue linking\\n.navigationProp(People.BestFriend); \/\/ single item can continue linking\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Better fluent API (compared to `asChildOf`) with builder pattern.\\n- Can be extended for supporting problem 5-7.\\n- Typed.\\n##### Cons:\\n- Lots of effort to build the new structure, which seems to be a `2.0` task.\\n### Proposal C\\nBasically, the same idea but with different API in terms of reaching single items.(e.g., \"getByKey\" and 1-to-1 navigation properties)\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends) \/\/ multi item can call \"key\" to become a single item\\n.key('scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n### Proposal C variants\\n```\\n\/\/frank\\nPeople.requestBuilder()\\n.getByKey('key') \/\/ xxxRequestBuilder\\n.toFriend('abc')\\n.toBestFriend()\\n.toFriends()\\n.getBuilder()\/\/create\\n\/\/marika\\nPeople.requestBuilder()\\n.key('scottketchum')\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination)\\n```\\n#### Pros and cons:\\nSame as `Proposal B`, but with more methods instead of overloading functions with more parameters.\\n### Proposal D\\nUse the similar API like `asChildOf`\\n```ts\\n\/\/ \/People(personKey)\/Friends\\nFriends.requestBuilder().getAll().asChildOf(person, People.Friends);\\n```\\n```ts\\n\/\/ \/People(personKey)\/Friends(friendKey)\\nFriends.requestBuilder().getByKey(friendKey).asChildOf(person, People.Friends);\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Consistent with `asChildOf` for generating same URL.\\n- Medium complexity\\n- Typed.\\n##### Cons:\\n- Cannot be extended for supporting problem 5-7, so we need to find solution for them.\\n- The ugly API `asChildOf` is used with additional use cases.\\n- Different order: `Entity -> NavigationProp` (url) V.S. `NavigationProp -> Entity` (API usage)\\n#### Decision:\\nNot chosen due to the lack of extension and confusing API.\\n### previous docs\\nFind related discussion [here](..\/implementation-documentation\/api-improvements.md)\\n","tokens":61,"id":3630,"text":"## Context\\nThe current request builder APIs are not able to handle some odata requests like:\\n- query navigation properties `GET \/People('scottketchum')\/Friends`\\n- getting \"raw value\" of a property `\/People('scottketchum')\/$value`\\n\n\n##Decision\n- Implement A for now as a powerful workaround.\\n- Proposal B\/C\/variant will be a `2.0` task, where it seems C might be the winner and we might review the decision later as they close to each other.\\nAt least, implement it as a separate task so we have a workaround for custom URL.\\n### Proposal B\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends, 'scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend) \/\/ single item can continue linking\\n.navigationProp(People.BestFriend); \/\/ single item can continue linking\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Better fluent API (compared to `asChildOf`) with builder pattern.\\n- Can be extended for supporting problem 5-7.\\n- Typed.\\n##### Cons:\\n- Lots of effort to build the new structure, which seems to be a `2.0` task.\\n### Proposal C\\nBasically, the same idea but with different API in terms of reaching single items.(e.g., \"getByKey\" and 1-to-1 navigation properties)\\n```ts\\n\/\/ Problem 1\\n\/\/ \/People('russellwhyte')\/Friends\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends);\\n```\\n```ts\\n\/\/ Problem 2,3,4\\n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriend\\nTripPinService.entity(People) \/\/ multi item can call \"key\" to become a single item\\n.key('russellwhyte') \/\/ single item can continue linking\\n.navigationProp(People.Friends) \/\/ multi item can call \"key\" to become a single item\\n.key('scottketchum') \/\/ single item can continue linking\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination);\\n```\\n### Proposal C variants\\n```\\n\/\/frank\\nPeople.requestBuilder()\\n.getByKey('key') \/\/ xxxRequestBuilder\\n.toFriend('abc')\\n.toBestFriend()\\n.toFriends()\\n.getBuilder()\/\/create\\n\/\/marika\\nPeople.requestBuilder()\\n.key('scottketchum')\\n.navigationProp(People.BestFriend)\\n.navigationProp(People.BestFriend)\\n.buildGetRequest() \/\/xxxRequestBuilder, which can be called by single item\/multi items and others\\n.customHeaders(headers)\\n.execute(destination)\\n```\\n#### Pros and cons:\\nSame as `Proposal B`, but with more methods instead of overloading functions with more parameters.\\n### Proposal D\\nUse the similar API like `asChildOf`\\n```ts\\n\/\/ \/People(personKey)\/Friends\\nFriends.requestBuilder().getAll().asChildOf(person, People.Friends);\\n```\\n```ts\\n\/\/ \/People(personKey)\/Friends(friendKey)\\nFriends.requestBuilder().getByKey(friendKey).asChildOf(person, People.Friends);\\n```\\n#### Pros and cons:\\n##### Pros:\\n- Consistent with `asChildOf` for generating same URL.\\n- Medium complexity\\n- Typed.\\n##### Cons:\\n- Cannot be extended for supporting problem 5-7, so we need to find solution for them.\\n- The ugly API `asChildOf` is used with additional use cases.\\n- Different order: `Entity -> NavigationProp` (url) V.S. `NavigationProp -> Entity` (API usage)\\n#### Decision:\\nNot chosen due to the lack of extension and confusing API.\\n### previous docs\\nFind related discussion [here](..\/implementation-documentation\/api-improvements.md)\\n","Predictions":"- Implement A for now as a powerful workaround.n- Proposal B\/C\/variant will be a 2.0 task, where it seems C might be the winner and we might review the decision later as they close to each other.nAt least, implement it as a separate task so we have a workaround for custom URL.n### Proposal Bntsn\/\/ Problem 1n\/\/ \/People('russellwhyte')\/Friends('scottketchum')\/BestFriend\/BestFriendnTripPinService.entity(People, 'russellwhyte') \/\/ single item can continue linkingn.navigationProp(People.Friends,'scottketchum') \/\/ single item can continue linkingn.navigationProp(People.Friends,'scottketchum') \/\/ single item can continue linkingn.navigationProp(People.BestFriend) \/\/ single item can continue linkingn.navigationProp(People.BestFriend); \/\/ single item can continue linkingnn### Pros and cons:n### Pros:n- Better fluent API (compared to asChildOf) with builder pattern.n- Can be extended for supporting problem 5-7.n- Typed.n### Cons:n- Better fluent API (compared to asChildOf) with builder pattern.n- Can be extended for supporting problem 5-7.n- Typed.n### Cons:n- Better fluent API (compared to asChildOf) with builder pattern.n- Can be extended for supporting problem 5-7.n- Typed.n### Cons:n- Better fluent API (compared to asChildOf) with builder pattern.n- Can be extended for supporting problem 5-7.n- Typed.n### Cons:n- Better fluent API (compared to asChildOf) with builder pattern.n- Can be extended for supporting problem 5-7.n- Typed.n### Cons:n- Better fluent API (compared to asChildOf) with builder pattern.n- Can be extended for supporting problem 5-7.n- Typed.n### Cons:n- Better fluent API (compared to asChildOf) with builder pattern.n- Can be extended for supporting problem 5-7.n- Typed.n### Cons:n- Better fluent API (compared to asChildOf) with builder pattern.n- Can be extended for supporting problem 5-7.n- Typed.n"}
{"File Name":"cena\/0005-use-spring-framework.md","Context":"## Context\n[Hexagonal architecture](0003-adopt-hexagonal-architecture.md) requires inversion of control to inject infrastructure\nservices implementations dependencies into the services defined in the core hexagon.\nREST API implementation requires a dedicated library to define incoming adapters handling the HTTP resources.\nSpring is a well established framework for Java. It is non-invasive and provides multiple features such as IoC, AOP,\nREST services implementation, security that will help speed up implementation in a cohesive way. The author has also used\nSpring for many years and masters many of the provided features.\n","Decision":"Spring framework will be the backbone for `menu-generation` application.\\n","tokens":120,"id":649,"text":"## Context\\n[Hexagonal architecture](0003-adopt-hexagonal-architecture.md) requires inversion of control to inject infrastructure\\nservices implementations dependencies into the services defined in the core hexagon.\\nREST API implementation requires a dedicated library to define incoming adapters handling the HTTP resources.\\nSpring is a well established framework for Java. It is non-invasive and provides multiple features such as IoC, AOP,\\nREST services implementation, security that will help speed up implementation in a cohesive way. The author has also used\\nSpring for many years and masters many of the provided features.\\n\n\n##Decision\nSpring framework will be the backbone for `menu-generation` application.\\n","Predictions":"Chosen option: \"Protobuf\", because it comes out best (see below).n### Positive Consequencesn- We need to start defining interfaces outside of their implementations.n- We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example, use POST \/get_projects BODY  \"index\": \"name\"  instead of GET \/projects?index=name.n### Negative Consequencesn- The team needs to learn a new language.n- Follow-up decisions required for the actual implementation.n"}
{"File Name":"scholarsphere\/0003-no-derivatives.md","Context":"## Context\nWhen viewing different file types over the web, large video files or other non-standard files, may not display well in\nthe browser, so the common solution is to create derivatives for a better user experience. These also include thumbnails\nfor a sample picture of what the resource looks like.\nManaging and creating derivatives can be hard and requires careful architectural considerations.\nHowever, the modern web has adapted to larger files, and generally handles a wider array of file types than it did in\nthe past. Also, thumbnails only work for image and video formats. Generic icon-based images for any file type can be\nmore informative.\n","Decision":"Scholarsphere will not create any derivative files. Icons will be used to represent file types, and the original\\nuploaded file can be rendered when applicable.\\n","tokens":129,"id":4553,"text":"## Context\\nWhen viewing different file types over the web, large video files or other non-standard files, may not display well in\\nthe browser, so the common solution is to create derivatives for a better user experience. These also include thumbnails\\nfor a sample picture of what the resource looks like.\\nManaging and creating derivatives can be hard and requires careful architectural considerations.\\nHowever, the modern web has adapted to larger files, and generally handles a wider array of file types than it did in\\nthe past. Also, thumbnails only work for image and video formats. Generic icon-based images for any file type can be\\nmore informative.\\n\n\n##Decision\nScholarsphere will not create any derivative files. Icons will be used to represent file types, and the original\\nuploaded file can be rendered when applicable.\\n","Predictions":"We will use a Semantic Model to represent the modal model.n"}
{"File Name":"push-sdk-ios\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in\\n[this article.][1]\\n","tokens":16,"id":4196,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in\\n[this article.][1]\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"Maud\/0004-python-package-versioning.md","Context":"## Context\nWe need a simple way to manage our package version.\n","Decision":"We use versioneer to do this for us.\\n","tokens":15,"id":241,"text":"## Context\\nWe need a simple way to manage our package version.\\n\n\n##Decision\nWe use versioneer to do this for us.\\n","Predictions":"We use versioneer to do this for us.n"}
{"File Name":"functionaut\/0003-test-distributed-files-only.md","Context":"## Context\nIt is not uncommon to have successful tests against development sources failing against the production bundle. We must make sure that code distributed to the public works as intended and avoid false positive in testing. The advanced compilation mode of the Google Closure Compiler makes this class of errors more likely to happen as it transforms the development sources radically.\n","Decision":"Testing will be made against the production bundle to catch compilation errors before they reach our users.\\n","tokens":67,"id":2689,"text":"## Context\\nIt is not uncommon to have successful tests against development sources failing against the production bundle. We must make sure that code distributed to the public works as intended and avoid false positive in testing. The advanced compilation mode of the Google Closure Compiler makes this class of errors more likely to happen as it transforms the development sources radically.\\n\n\n##Decision\nTesting will be made against the production bundle to catch compilation errors before they reach our users.\\n","Predictions":"We will use [ADR 2](adr-002.md), a refactored version of the Google Closure Compiler, to implement the new code base.n"}
{"File Name":"qc-atlas\/0005-use-OpenAPI.md","Context":"## Context and Problem Statement\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\n## Decision Drivers\n* readable API documentation\n* effort of manually creating client services\n","Decision":"* readable API documentation\\n* effort of manually creating client services\\nChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.\\n### Positive Consequences\\n* Standardized documentation of the API\\n* Automatic service generation for clients is possible\\n### Negative Consequences <!-- optional -->\\n* OpenAPI annotations have to be maintained\\n","tokens":56,"id":693,"text":"## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n\n\n##Decision\n* readable API documentation\\n* effort of manually creating client services\\nChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.\\n### Positive Consequences\\n* Standardized documentation of the API\\n* Automatic service generation for clients is possible\\n### Negative Consequences <!-- optional -->\\n* OpenAPI annotations have to be maintained\\n","Predictions":"* readable API documentationn* effort of manually creating client servicesnChosen option: Use OpenAPI, because the API is described in a standardized format which is human and machine readable.n### Positive Consequencesn* Standardized documentation of the APIn* Automatic service generation for clients is possiblen### Negative Consequences!-- optional -->n* OpenAPI annotations have to be maintainedn"}
{"File Name":"terraform-aws-dynamodb\/0010-terraform-backend-in-module.md","Context":"## Context\nWhen LIC teams begun using this module it became apparent that the current\nimplementation pattern does not meet their needs. Without a backend in the\nmodule teams would need to add a Terraform `backend` configuration into there\nlocal implementation for it to be picked up.\n","Decision":"Restored the `backend` into the module for the time being.\\nWe still feel this should be removed at some time and teams become familiar with\\nhow to use Terragrunt\/Terraform configuring there own `backend`.\\n","tokens":57,"id":2539,"text":"## Context\\nWhen LIC teams begun using this module it became apparent that the current\\nimplementation pattern does not meet their needs. Without a backend in the\\nmodule teams would need to add a Terraform `backend` configuration into there\\nlocal implementation for it to be picked up.\\n\n\n##Decision\nRestored the `backend` into the module for the time being.\\nWe still feel this should be removed at some time and teams become familiar with\\nhow to use Terragrunt\/Terraform configuring there own `backend`.\\n","Predictions":"Use Rollbar to collect and manage our application errors.n"}
{"File Name":"MoviesAndroid\/0001 ADR.md","Context":"## Context and Problem Statement\nI need a way to document design decisions.\n","Decision":"Chosen option: Smaller markdown files in Git of format MADR, no cmd tool, IDEA is sufficient.\\nPositive Consequences:\\n* Choices and reasoning are persisted and versioned\\nNegative consequences:\\n* Having to document\\n","tokens":16,"id":160,"text":"## Context and Problem Statement\\nI need a way to document design decisions.\\n\n\n##Decision\nChosen option: Smaller markdown files in Git of format MADR, no cmd tool, IDEA is sufficient.\\nPositive Consequences:\\n* Choices and reasoning are persisted and versioned\\nNegative consequences:\\n* Having to document\\n","Predictions":"We will use [ADR-007](https:\/\/github.com\/ADR-007-design-decisions)n"}
{"File Name":"once-ui\/0002-build-a-high-quality-ui-components-library.md","Context":"## Context\nMoving towards a suite of multiple applications, we need a library of shared Angular components that can be reused across the different apps, all with the same design language and a consistent api.\n","Decision":"- We will build a library of atomic ui components that will be used throughout the applications.\\n- Developers should be able to easily import only the components they need and use them in their applications.\\n- Components should be well tested and isolated in a way that doesn't break the host application's code.\\n","tokens":40,"id":1151,"text":"## Context\\nMoving towards a suite of multiple applications, we need a library of shared Angular components that can be reused across the different apps, all with the same design language and a consistent api.\\n\n\n##Decision\n- We will build a library of atomic ui components that will be used throughout the applications.\\n- Developers should be able to easily import only the components they need and use them in their applications.\\n- Components should be well tested and isolated in a way that doesn't break the host application's code.\\n","Predictions":"We will use PostgreSQL as the database for the API.n"}
{"File Name":"paas-team-manual\/ADR037-automated-certificate-rotation.html.md","Context":"## Context\nOur certificate rotation was a largely manual process, involving an operator triggering a series of Concourse pipeline jobs in a particular sequence. We did not have a routine for doing rotations, and would typically only do them as part of a CF upgrade.\nThe only means we had for knowing if a cert rotation was necessary was the `check-certificates` job, in the `create-cloudfoundry` Concourse pipeline, which would fail if any certificate had less than 30 days until it expired.\nIn Q2 2019 (August\/September) we moved all of our platform secrets from AWS S3 to [Credhub](https:\/\/docs.cloudfoundry.org\/credhub\/). This covered third-party service credentials, platform passwords, and certificates. Since Credhub supports [certificate rotation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), we chose to implement automatic certificate rotation. This ADR contains details of how we did it.\n","Decision":"Credhub has the notion of a transitional certificate. As written in [their documentation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), a transitional certificate is\\n> a new version that will not be used for signing yet, but can be added to your servers trusted certificate lists.\\nOur certificate rotation process is built around the setting and migration of the `transitional` flag, such that over a number of deployments an active certificate is retired and a new certificate is deployed, without downtime.\\nIn order to make certificate rotation automatic, and require no operator interaction, it is implemented as a job at the tail end of the `create-cloudfoundry` pipeline; after acceptance tests and before releases tagging.\\nThe new `rotate-certs` job has three tasks:\\n- `remove-transitional-flag-for-ca`\\n- `move-transitional-flag-for-ca`\\n- `set-transitional-flag-for-ca`\\nThese three tasks are in reverse order of the process for rotating a certificate. If the tasks were ordered normally, the first task would set up the state for the second, and the second would set up the state for the third, and Bosh would be unable to deploy the certificates without downtime. However, here the tasks are explained in the proper order to make it easier to understand how a certificate is rotated. To understand how it happens in the pipeline, assume a Bosh deploy happens between each step.\\n`set-transitional-flag-for-ca` is the first step in the process. It iterates through all CA certificates in Credhub, looking for any expiring under 30 days. Any that are, are regenerated as transitional certificates. This results in Credhub holding two certificates for the same credential name: the expiring certificate, and the new certificate with the `transitional` flag.\\n`move-transitional-flag-for-ca` is the second step in the process, and has two jobs:\\n1. It finds all CA certificates in Credhub which have 2 values, where the oldest certificate does not have the `transitional` flag and the newer one does. For each of those, it swaps the `transitional` flag to the older certificate. Finally, it looks for any leaf certificates signed by the CA certificate and regenerates them using the new CA certificate.\\n2. It looks for any leaf certificates that are expiring in less than 30 days and regenerates them. This is a one step process and they are deployed on the next Bosh deploy.\\n`remove-transitional-flag-for-ca` is the third and final step in the process. It iterates through all of the CA certificates in Credhub, looking for any with 2 values, where the older certificate is marked as `transitional` and the newer certificate is not. It then removes the `transitional` flag from the older certificate, which has the effect of dropping the certificate.\\nThe existing `check-certificates` job has also been modified to check for certificates that are expiring in less than 15 days. If a certificate fails this check, that should suggest to us that something has gone wrong in our certificate rotation process.\\n","tokens":209,"id":180,"text":"## Context\\nOur certificate rotation was a largely manual process, involving an operator triggering a series of Concourse pipeline jobs in a particular sequence. We did not have a routine for doing rotations, and would typically only do them as part of a CF upgrade.\\nThe only means we had for knowing if a cert rotation was necessary was the `check-certificates` job, in the `create-cloudfoundry` Concourse pipeline, which would fail if any certificate had less than 30 days until it expired.\\nIn Q2 2019 (August\/September) we moved all of our platform secrets from AWS S3 to [Credhub](https:\/\/docs.cloudfoundry.org\/credhub\/). This covered third-party service credentials, platform passwords, and certificates. Since Credhub supports [certificate rotation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), we chose to implement automatic certificate rotation. This ADR contains details of how we did it.\\n\n\n##Decision\nCredhub has the notion of a transitional certificate. As written in [their documentation](https:\/\/github.com\/pivotal-cf\/credhub-release\/blob\/master\/docs\/ca-rotation.md), a transitional certificate is\\n> a new version that will not be used for signing yet, but can be added to your servers trusted certificate lists.\\nOur certificate rotation process is built around the setting and migration of the `transitional` flag, such that over a number of deployments an active certificate is retired and a new certificate is deployed, without downtime.\\nIn order to make certificate rotation automatic, and require no operator interaction, it is implemented as a job at the tail end of the `create-cloudfoundry` pipeline; after acceptance tests and before releases tagging.\\nThe new `rotate-certs` job has three tasks:\\n- `remove-transitional-flag-for-ca`\\n- `move-transitional-flag-for-ca`\\n- `set-transitional-flag-for-ca`\\nThese three tasks are in reverse order of the process for rotating a certificate. If the tasks were ordered normally, the first task would set up the state for the second, and the second would set up the state for the third, and Bosh would be unable to deploy the certificates without downtime. However, here the tasks are explained in the proper order to make it easier to understand how a certificate is rotated. To understand how it happens in the pipeline, assume a Bosh deploy happens between each step.\\n`set-transitional-flag-for-ca` is the first step in the process. It iterates through all CA certificates in Credhub, looking for any expiring under 30 days. Any that are, are regenerated as transitional certificates. This results in Credhub holding two certificates for the same credential name: the expiring certificate, and the new certificate with the `transitional` flag.\\n`move-transitional-flag-for-ca` is the second step in the process, and has two jobs:\\n1. It finds all CA certificates in Credhub which have 2 values, where the oldest certificate does not have the `transitional` flag and the newer one does. For each of those, it swaps the `transitional` flag to the older certificate. Finally, it looks for any leaf certificates signed by the CA certificate and regenerates them using the new CA certificate.\\n2. It looks for any leaf certificates that are expiring in less than 30 days and regenerates them. This is a one step process and they are deployed on the next Bosh deploy.\\n`remove-transitional-flag-for-ca` is the third and final step in the process. It iterates through all of the CA certificates in Credhub, looking for any with 2 values, where the older certificate is marked as `transitional` and the newer certificate is not. It then removes the `transitional` flag from the older certificate, which has the effect of dropping the certificate.\\nThe existing `check-certificates` job has also been modified to check for certificates that are expiring in less than 15 days. If a certificate fails this check, that should suggest to us that something has gone wrong in our certificate rotation process.\\n","Predictions":"We will use the [Security Layer](https:\/\/github.com\/Security-Loader\/Security-Loader) as the default for all CI\/CD services.n"}
{"File Name":"dddsample-pelargir\/0003-install-citerus-as-a-git-submodule.md","Context":"## Context\nI can't be bothered to be installing the citerus library into my\nrepositories all the time.\nI want to keep my copy of that closely synchronized to the common\nproject, so that any progress that is made can be incorporated\ninto my work.\nI want a convenient way to introduce fixes in the existing implementation\nas I discover the need.\nI want to keep the touch on the existing code _light_; I expect\nto be making some fairly drastic changes in approach, and I don't\nwant to be debating my choices with the maintainers.\nI'm not currently comfortable working with maven projects where\nthe elements are not rooted under a common location.\n","Decision":"Add the upstream as a git submodule, so that it's clear precisely\\nwhich version of the upstream is in play at any given time.\\nChanges that are of general interest can be in that space.\\nMost radical changes (for a _gentle_ definition of radical) should\\nbe maintained elsewhere.\\n","tokens":144,"id":128,"text":"## Context\\nI can't be bothered to be installing the citerus library into my\\nrepositories all the time.\\nI want to keep my copy of that closely synchronized to the common\\nproject, so that any progress that is made can be incorporated\\ninto my work.\\nI want a convenient way to introduce fixes in the existing implementation\\nas I discover the need.\\nI want to keep the touch on the existing code _light_; I expect\\nto be making some fairly drastic changes in approach, and I don't\\nwant to be debating my choices with the maintainers.\\nI'm not currently comfortable working with maven projects where\\nthe elements are not rooted under a common location.\\n\n\n##Decision\nAdd the upstream as a git submodule, so that it's clear precisely\\nwhich version of the upstream is in play at any given time.\\nChanges that are of general interest can be in that space.\\nMost radical changes (for a _gentle_ definition of radical) should\\nbe maintained elsewhere.\\n","Predictions":"* This section describes the forces at play, including technological, political, social, and project local.nThese forces are probably in tension, and should be called out as such. The language in this section is value-neutral.nIt is simply describing facts.nThe Office of Development Transformation (ODT) is focused on helping CAD learn and adopt lean, agile, and agile.n"}
{"File Name":"blueprint\/0019-org-management.md","Context":"## Context and Problem Statement\nManaging GitHub organizations via UI is restricted to few individuals only and does not offer any review mechanisms. The same apply to repository creation, organization membership management and label management. This ADR focuses on bringing transparency into this aspect of housekeeping within a community and aims to provide a way to individual contributors, so they can influence and propose changes themselves.\nADR seeks a way to manage GitHub organization as a code, declaratively via GitOps as a natural extension of the core paradigm of Operate First movement.\n## Decision Drivers\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\n","Decision":"Improve transparency, provide auditing and reviews and allow individual contributors to propose changes.\\nChosen option: _\"1. Prow's Peribolos\"_, because it's a declarative solution that is widely embraced in upstream communities like Kubernetes.\\n### Positive Consequences\\n* Repositories can be created declaratively.\\n* Even external users can request themselves to be added to the organization via a pull request.\\n* Teams can be created and maintained declaratively.\\n* Labels can be centrally managed in a unified way as well as updated per repository bases.\\n* Solution can be easily automated via post-submit Prow jobs.\\n### Negative Consequences\\n* Using a declarative configuration may not be as straightforward as clicking buttons in the UI.\\n* Some users can still do manual changes to repositories they own, these changes are undone by the automation.\\n","tokens":128,"id":3554,"text":"## Context and Problem Statement\\nManaging GitHub organizations via UI is restricted to few individuals only and does not offer any review mechanisms. The same apply to repository creation, organization membership management and label management. This ADR focuses on bringing transparency into this aspect of housekeeping within a community and aims to provide a way to individual contributors, so they can influence and propose changes themselves.\\nADR seeks a way to manage GitHub organization as a code, declaratively via GitOps as a natural extension of the core paradigm of Operate First movement.\\n## Decision Drivers\\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\\n\n\n##Decision\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\\nChosen option: _\"1. Prow's Peribolos\"_, because it's a declarative solution that is widely embraced in upstream communities like Kubernetes.\\n### Positive Consequences\\n* Repositories can be created declaratively.\\n* Even external users can request themselves to be added to the organization via a pull request.\\n* Teams can be created and maintained declaratively.\\n* Labels can be centrally managed in a unified way as well as updated per repository bases.\\n* Solution can be easily automated via post-submit Prow jobs.\\n### Negative Consequences\\n* Using a declarative configuration may not be as straightforward as clicking buttons in the UI.\\n* Some users can still do manual changes to repositories they own, these changes are undone by the automation.\\n","Predictions":"We will use a Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate Separate"}
{"File Name":"arch\/0045-reuse-python-custom-libs.md","Context":"## Context\n\u6211\u4eec\u6709\u591a\u4e2a\u9879\u76ee\u4f7f\u7528 Python \u5f00\u53d1\uff0c\u968f\u7740\u9879\u76ee\u7684\u53d1\u5c55\uff0c\u5927\u5bb6\u4e5f\u5199\u4e86\u8bb8\u591a\u7684\u5e93\uff0c\u6bd4\u5982\uff0cprice\u3001sms\u3001mail \u7b49\u3002\u800c\u5176\u4ed6\u9879\u76ee\u4e5f\u6709\u8fd9\u6837\u7684\u9700\u6c42\uff0c\u5f53\u524d\u9879\u76ee\u4e4b\u95f4\u662f\u901a\u8fc7\u62f7\u8d1d\u7684\u65b9\u5f0f\u8fdb\u884c\u590d\u7528\uff0c\u4e0d\u662f\u5e93\u8fd8\u5b58\u5728\u9879\u76ee\u5185\u72ec\u81ea\u81ea\u884c\u66f4\u65b0\u3002\u8fd9\u5c31\u5bfc\u81f4\u9879\u76ee\u4e4b\u95f4\u6240\u4f7f\u7528\u7684\u5e93\u4ea7\u751f\u4e0d\u4e00\u81f4\uff0c\u5e76\u91cd\u590d\u9020\u4e86\u5f88\u591a\u7684\u8f6e\u5b50\u3002\n","Decision":"1. \u6784\u5efa\u81ea\u5df1\u7684 pypi \u670d\u52a1\u5668\uff1b\\n* \u4e0d\u53ea\u53ef\u4ee5\u89e3\u51b3\u81ea\u5efa\u5e93\u7684\u590d\u7528\u95ee\u9898\uff1b\\n* \u4e5f\u53ef\u4ee5\u5c06\u6211\u4eec\u7684\u5e38\u7528\u5e93\u7f13\u5b58\uff0c\u52a0\u901f pip \u7684\u5b89\u88c5\uff1b\\n* \u6709\u7ef4\u62a4\u6210\u672c\u3002\\n2. \u4f7f\u7528 git \u4f5c\u4e3a pip \u5b89\u88c5\u5305\\n* \u5355 repo \u5355 package\uff1a\u6240\u6709 libs \u805a\u5408\u5728\u4e00\u4e2a package \u91cc\uff0c\u5bf9\u4e8e\u5927\u91cf\u7b80\u5355 lib \u53ef\u884c\uff1b\\n* \u5355 repo \u591a package\uff1a\u5206\u76ee\u5f55\u6784\u5efa package\uff0c\u9002\u5408\u7a0d\u5fae\u5927\u4e9b\u7684 libs \u4e4b\u95f4\u505a\u9694\u79bb\uff1b\\n* \u591a repo \u591a package\uff1a\u6bcf\u4e2a repo \u662f\u4e00\u4e2a package\uff0c\u9002\u5408\u5927\u7684 lib \u5e93\uff0clib \u7c92\u5ea6\u8fc7\u7ec6\u4f1a\u6709\u5e93\u7ba1\u7406\u95ee\u9898\u3002\\n3. \u4f7f\u7528 Artifactory\uff0cPro+ \u7248\u672c\u624d\u63d0\u4f9b pypi \u670d\u52a1\u3002\\n\u9274\u4e8e\u5f53\u524d lib \u5e76\u4e0d\u591a\uff0c\u800c\u4e14\u6bcf\u4e2a lib \u5f88\u7b80\u5355\uff0c\u6545\u9009\u62e9\u5355 repo \u5355 package \u65b9\u6848\u3002\\n","tokens":119,"id":2446,"text":"## Context\\n\u6211\u4eec\u6709\u591a\u4e2a\u9879\u76ee\u4f7f\u7528 Python \u5f00\u53d1\uff0c\u968f\u7740\u9879\u76ee\u7684\u53d1\u5c55\uff0c\u5927\u5bb6\u4e5f\u5199\u4e86\u8bb8\u591a\u7684\u5e93\uff0c\u6bd4\u5982\uff0cprice\u3001sms\u3001mail \u7b49\u3002\u800c\u5176\u4ed6\u9879\u76ee\u4e5f\u6709\u8fd9\u6837\u7684\u9700\u6c42\uff0c\u5f53\u524d\u9879\u76ee\u4e4b\u95f4\u662f\u901a\u8fc7\u62f7\u8d1d\u7684\u65b9\u5f0f\u8fdb\u884c\u590d\u7528\uff0c\u4e0d\u662f\u5e93\u8fd8\u5b58\u5728\u9879\u76ee\u5185\u72ec\u81ea\u81ea\u884c\u66f4\u65b0\u3002\u8fd9\u5c31\u5bfc\u81f4\u9879\u76ee\u4e4b\u95f4\u6240\u4f7f\u7528\u7684\u5e93\u4ea7\u751f\u4e0d\u4e00\u81f4\uff0c\u5e76\u91cd\u590d\u9020\u4e86\u5f88\u591a\u7684\u8f6e\u5b50\u3002\\n\n\n##Decision\n1. \u6784\u5efa\u81ea\u5df1\u7684 pypi \u670d\u52a1\u5668\uff1b\\n* \u4e0d\u53ea\u53ef\u4ee5\u89e3\u51b3\u81ea\u5efa\u5e93\u7684\u590d\u7528\u95ee\u9898\uff1b\\n* \u4e5f\u53ef\u4ee5\u5c06\u6211\u4eec\u7684\u5e38\u7528\u5e93\u7f13\u5b58\uff0c\u52a0\u901f pip \u7684\u5b89\u88c5\uff1b\\n* \u6709\u7ef4\u62a4\u6210\u672c\u3002\\n2. \u4f7f\u7528 git \u4f5c\u4e3a pip \u5b89\u88c5\u5305\\n* \u5355 repo \u5355 package\uff1a\u6240\u6709 libs \u805a\u5408\u5728\u4e00\u4e2a package \u91cc\uff0c\u5bf9\u4e8e\u5927\u91cf\u7b80\u5355 lib \u53ef\u884c\uff1b\\n* \u5355 repo \u591a package\uff1a\u5206\u76ee\u5f55\u6784\u5efa package\uff0c\u9002\u5408\u7a0d\u5fae\u5927\u4e9b\u7684 libs \u4e4b\u95f4\u505a\u9694\u79bb\uff1b\\n* \u591a repo \u591a package\uff1a\u6bcf\u4e2a repo \u662f\u4e00\u4e2a package\uff0c\u9002\u5408\u5927\u7684 lib \u5e93\uff0clib \u7c92\u5ea6\u8fc7\u7ec6\u4f1a\u6709\u5e93\u7ba1\u7406\u95ee\u9898\u3002\\n3. \u4f7f\u7528 Artifactory\uff0cPro+ \u7248\u672c\u624d\u63d0\u4f9b pypi \u670d\u52a1\u3002\\n\u9274\u4e8e\u5f53\u524d lib \u5e76\u4e0d\u591a\uff0c\u800c\u4e14\u6bcf\u4e2a lib \u5f88\u7b80\u5355\uff0c\u6545\u9009\u62e9\u5355 repo \u5355 package \u65b9\u6848\u3002\\n","Predictions":",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}
{"File Name":"archcolider\/010 Feedback System separation.md","Context":"## Context\nA user would like to read or write a review for a meal that he has ordered. Businesses would like to obtain feedback on its service in order to improve it.\nTwo different feedback methods:\n- surveys (occasional questionnaires about general aspects of the app\/service)\n- feedback (is an opinion about an order or app\/service)\n","Decision":"We'll create a simple in app feedback system that will allow users to provide feedback about orders and service.\\nWe'll incorporate surveys into feedback in order to take advantage of 3rd parties services which specialize in feedback acquisitions such survey monkey, google surveys, etc..\\n","tokens":74,"id":2681,"text":"## Context\\nA user would like to read or write a review for a meal that he has ordered. Businesses would like to obtain feedback on its service in order to improve it.\\nTwo different feedback methods:\\n- surveys (occasional questionnaires about general aspects of the app\/service)\\n- feedback (is an opinion about an order or app\/service)\\n\n\n##Decision\nWe'll create a simple in app feedback system that will allow users to provide feedback about orders and service.\\nWe'll incorporate surveys into feedback in order to take advantage of 3rd parties services which specialize in feedback acquisitions such survey monkey, google surveys, etc..\\n","Predictions":"We will use a Message Delivery Service to deliver the notifications to its users.n"}
{"File Name":"deeplearning4j\/0007 - Nd4j classifiers.md","Context":"## Context\nNd4j relies upon the c++ library [libnd4j](..\/libnd4j) for native math execution.\nIt uses [javacpp](https:\/\/github.com\/bytedeco\/javacpp) to link against\nlibnd4j. Libnd4j is capable of being compiled a myriad of different ways allowing different trade offs to be made\nin terms of performance and dependencies. This presents complexity in exposing this flexibility to the end user.\n","Decision":"In order to allow users to pick which configuration they would like to use, while avoiding adding a lot of different artifact\\nids to the project, the following javacpp platform extensions are used:\\ncompiled type (avx etc or blank if normal) - software linked against (cudnn, onednn, armcompute) - version\\nAn example for the one dnn platform extension could be:\\ndnnl-2.2\\navx256-dnnl-2.2\\nThis presents 2 examples where a special compilation is enabled and one where it's not\\nboth linking against dnnl\/onednn 2.2.\\n","tokens":99,"id":2932,"text":"## Context\\nNd4j relies upon the c++ library [libnd4j](..\/libnd4j) for native math execution.\\nIt uses [javacpp](https:\/\/github.com\/bytedeco\/javacpp) to link against\\nlibnd4j. Libnd4j is capable of being compiled a myriad of different ways allowing different trade offs to be made\\nin terms of performance and dependencies. This presents complexity in exposing this flexibility to the end user.\\n\n\n##Decision\nIn order to allow users to pick which configuration they would like to use, while avoiding adding a lot of different artifact\\nids to the project, the following javacpp platform extensions are used:\\ncompiled type (avx etc or blank if normal) - software linked against (cudnn, onednn, armcompute) - version\\nAn example for the one dnn platform extension could be:\\ndnnl-2.2\\navx256-dnnl-2.2\\nThis presents 2 examples where a special compilation is enabled and one where it's not\\nboth linking against dnnl\/onednn 2.2.\\n","Predictions":"We will use a serializer file for serializing the model.n"}
{"File Name":"paas-team-manual\/ADR023-idle-cpu-alerting-change.html.md","Context":"## Context\nWith the implementation of ADR021 we have reduced the number of cells in\nproduction in order to make more efficent use of our budget. This in turn means\nthat we have increased the load on the individual cells. Originally the idle CPU\nmonitor was set in line with the free memory on cells monitor (for alerting on\na need to scale the cell pool), however CPU usage does not appear to grow\nlinearly with allocated memory for tenant applications.\n","Decision":"In order to avoid false positives from triggering due to CPU load spiking rather\\nthan being a constant level we will increase the monitoring window to 24 hours.\\nBased upon examining our CPU idle load in ADR021, we will reduce the CPU idle\\nthresholds to warn at 37% and become critical at 33%.\\n","tokens":98,"id":215,"text":"## Context\\nWith the implementation of ADR021 we have reduced the number of cells in\\nproduction in order to make more efficent use of our budget. This in turn means\\nthat we have increased the load on the individual cells. Originally the idle CPU\\nmonitor was set in line with the free memory on cells monitor (for alerting on\\na need to scale the cell pool), however CPU usage does not appear to grow\\nlinearly with allocated memory for tenant applications.\\n\n\n##Decision\nIn order to avoid false positives from triggering due to CPU load spiking rather\\nthan being a constant level we will increase the monitoring window to 24 hours.\\nBased upon examining our CPU idle load in ADR021, we will reduce the CPU idle\\nthresholds to warn at 37% and become critical at 33%.\\n","Predictions":"We will use the [ScheduleWatch](https:\/\/schedulewatch.org\/) to monitor the state of the tasks.n"}
{"File Name":"docspell\/0011_extract_text.md","Context":"# Context and Problem Statement\nWith support for more file types there must be a way to extract text\nfrom all of them. It is better to extract text from the source files,\nin contrast to extracting the text from the converted pdf file.\nThere are multiple options and multiple file types. Again, most\npriority is to use a java\/scala library to reduce external\ndependencies.\n# Considered Options\n","Decision":"- MS Office files: POI library\\n- Open Document files: Tika, but integrating the few source files that\\nmake up the open document parser. Due to its huge dependency tree,\\nthe library is not added.\\n- PDF: Apache PDFBox. I know this library better than itext.\\n","tokens":86,"id":282,"text":"# Context and Problem Statement\\nWith support for more file types there must be a way to extract text\\nfrom all of them. It is better to extract text from the source files,\\nin contrast to extracting the text from the converted pdf file.\\nThere are multiple options and multiple file types. Again, most\\npriority is to use a java\/scala library to reduce external\\ndependencies.\\n# Considered Options\\n\n\n##Decision\n- MS Office files: POI library\\n- Open Document files: Tika, but integrating the few source files that\\nmake up the open document parser. Due to its huge dependency tree,\\nthe library is not added.\\n- PDF: Apache PDFBox. I know this library better than itext.\\n","Predictions":"We will use a stack library to create a stack directory.n"}
{"File Name":"monocle\/0008-unify-idents-across-code-review-systems.md","Context":"## Context and Problem Statement\nHow do we merge contributor identities across code review systems ?\n## Decision Drivers\n* Do not complexify the EL query system\n* Identities can be configured via the Monocle configuration file\n","Decision":"* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\nChosen option: \"[option 2]\", Simpler implementation, better query performance (no complexification of queries) and no potential metrics errorneous results returned compared\\nto \"[option 1]\".\\nHere is a configuration example.\\n```\\nidents:\\n- ident: John Doe\\naliases:\\n- github.com\/john-doe\\n- review.opendev.org\/John Doe\/12345\\n```\\n### Negative Consequences\\n* A tool need to crawl the whole index to discover objects to update. However this\\noperation should not happen often.\\n","tokens":47,"id":4645,"text":"## Context and Problem Statement\\nHow do we merge contributor identities across code review systems ?\\n## Decision Drivers\\n* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\n\n\n##Decision\n* Do not complexify the EL query system\\n* Identities can be configured via the Monocle configuration file\\nChosen option: \"[option 2]\", Simpler implementation, better query performance (no complexification of queries) and no potential metrics errorneous results returned compared\\nto \"[option 1]\".\\nHere is a configuration example.\\n```\\nidents:\\n- ident: John Doe\\naliases:\\n- github.com\/john-doe\\n- review.opendev.org\/John Doe\/12345\\n```\\n### Negative Consequences\\n* A tool need to crawl the whole index to discover objects to update. However this\\noperation should not happen often.\\n","Predictions":"* The need to keep things simple for service developersn* The need to improve the operational efficiency of tracking consentnChosen option: \"Use a data attribute\", because based on the benefits listed below the teamnbelieves this is the best way forward.n### Positive Consequencesn* The Javascript bundle creation process is simplified.n* The Scala Play routing is simplifiedn* Only one endpoint needs to be managed in productionn* Future additional containers can be supported more easilyn* Service developers only have to configure a single URL to tracking consent and usena data attribute to configure the container.n* The central common configuration repo (owned by a separate team) only requires a single URL to tracking consent defining for each environment, rather than usingnthe same URL for each environment.n"}
{"File Name":"digitalrig-metal-aws\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1744,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"sre-challenge\/0008-use-taurus-for-benchmark-tests.md","Context":"## Context\nTo perform benchmarking tests, I want to set some resources capable to generate\na high volume of requests against the cluster.\nTo do this, I could use several tools, like JMeter, Garret, or Taurus.\nBut I like Blazemeter's Taurus over all because it is very simple to setup and\nexecute tests.\nWith Taurus you can create JMeter tests, or use other tools like Selenium, in\norder to execute and collect results.\nThen It generates a report that can be easily imported in a JMeter tool or\nopen it in a data analysis tool like jupyter notebook to draw new diagrams.\n","Decision":"Create an instance with an easy test (Just request the keys and the metrics).\\n","tokens":137,"id":2998,"text":"## Context\\nTo perform benchmarking tests, I want to set some resources capable to generate\\na high volume of requests against the cluster.\\nTo do this, I could use several tools, like JMeter, Garret, or Taurus.\\nBut I like Blazemeter's Taurus over all because it is very simple to setup and\\nexecute tests.\\nWith Taurus you can create JMeter tests, or use other tools like Selenium, in\\norder to execute and collect results.\\nThen It generates a report that can be easily imported in a JMeter tool or\\nopen it in a data analysis tool like jupyter notebook to draw new diagrams.\\n\n\n##Decision\nCreate an instance with an easy test (Just request the keys and the metrics).\\n","Predictions":"Use Rollbar to collect and manage our application errors.n"}
{"File Name":"moneycount-api\/003-Document_API_With_Swagger.md","Context":"## Context\nOffer an API without a good documentation is not a choice. Document this API with text documents is not efficient, and as the code changes more and more effort are necessary to keep it up-to-date. It's necessary to document APIs in a simply and efficient way, preferably in a way that developers can test it.\n","Decision":"I decided to use Swagger as a documentation tool for the APIs, as it is a de facto standard. I choosed springfox-swagger2 and springfox-swagger-ui because of its smooth integration with Spring Boot.\\n","tokens":67,"id":3069,"text":"## Context\\nOffer an API without a good documentation is not a choice. Document this API with text documents is not efficient, and as the code changes more and more effort are necessary to keep it up-to-date. It's necessary to document APIs in a simply and efficient way, preferably in a way that developers can test it.\\n\n\n##Decision\nI decided to use Swagger as a documentation tool for the APIs, as it is a de facto standard. I choosed springfox-swagger2 and springfox-swagger-ui because of its smooth integration with Spring Boot.\\n","Predictions":"I decided to use Swagger as a documentation tool for the APIs, as it is a de facto standard. I chosed springfox-swagger2 and springfox-swagger-ui because of its smooth integration with Spring Boot.n"}
{"File Name":"ibc-go\/adr-027-ibc-wasm.md","Context":"## Context\nCurrently in ibc-go light clients are defined as part of the codebase and are implemented as modules under\n`modules\/light-clients`. Adding support for new light clients or updating an existing light client in the event\nof a security issue or consensus update is a multi-step process which is both time-consuming and error-prone.\nIn order to enable new IBC light client implementations it is necessary to modify the codebase of ibc-go (if the light\nclient is part of its codebase), re-build chains' binaries, pass a governance proposal and validators upgrade their nodes.\nAnother problem stemming from the above process is that if a chain wants to upgrade its own consensus, it will\nneed to convince every chain or hub connected to it to upgrade its light client in order to stay connected. Due\nto the time consuming process required to upgrade a light client, a chain with lots of connections needs to be\ndisconnected for quite some time after upgrading its consensus, which can be very expensive in terms of time and effort.\nWe are proposing simplifying this workflow by integrating a Wasm light client module that makes adding support for\nnew light clients a simple governance-gated transaction. The light client bytecode, written in Wasm-compilable Rust,\nruns inside a Wasm VM. The Wasm light client submodule exposes a proxy light client interface that routes incoming\nmessages to the appropriate handler function, inside the Wasm VM for execution.\nWith the Wasm light client module, anybody can add new IBC light client in the form of Wasm bytecode (provided they are\nable to submit the governance proposal transaction and that it passes) as well as instantiate clients using any created\nclient type. This allows any chain to update its own light client in other chains without going through the steps outlined above.\n","Decision":"We decided to implement the Wasm light client module as a light client proxy that will interface with the actual light client\\nuploaded as Wasm bytecode. To enable usage of the Wasm light client module, users need to add it to the list of allowed clients\\nby updating the `AllowedClients` parameter in the 02-client submodule of core IBC.\\n```go\\nparams := clientKeeper.GetParams(ctx)\\nparams.AllowedClients = append(params.AllowedClients, exported.Wasm)\\nclientKeeper.SetParams(ctx, params)\\n```\\nAdding a new light client contract is governance-gated. To upload a new light client users need to submit\\na [governance v1 proposal](https:\/\/docs.cosmos.network\/main\/modules\/gov#proposals) that contains the `sdk.Msg` for storing\\nthe Wasm contract's bytecode. The required message is `MsgStoreCode` and the bytecode is provided in the field `wasm_byte_code`:\\n```proto\\n\/\/ MsgStoreCode defines the request type for the StoreCode rpc.\\nmessage MsgStoreCode {\\n\/\/ signer address\\nstring signer = 1;\\n\/\/ wasm byte code of light client contract. It can be raw or gzip compressed\\nbytes wasm_byte_code = 2;\\n}\\n```\\nThe RPC handler processing `MsgStoreCode` will make sure that the signer of the message matches the address of authority allowed to\\nsubmit this message (which is normally the address of the governance module).\\n```go\\n\/\/ StoreCode defines a rpc handler method for MsgStoreCode\\nfunc (k Keeper) StoreCode(goCtx context.Context, msg *types.MsgStoreCode) (*types.MsgStoreCodeResponse, error) {\\nif k.GetAuthority() != msg.Signer {\\nreturn nil, errorsmod.Wrapf(ibcerrors.ErrUnauthorized, \"expected %s, got %s\", k.GetAuthority(), msg.Signer)\\n}\\nctx := sdk.UnwrapSDKContext(goCtx)\\nchecksum, err := k.storeWasmCode(ctx, msg.WasmByteCode, ibcwasm.GetVM().StoreCode)\\nif err != nil {\\nreturn nil, errorsmod.Wrap(err, \"failed to store wasm bytecode\")\\n}\\nemitStoreWasmCodeEvent(ctx, checksum)\\nreturn &types.MsgStoreCodeResponse{\\nChecksum: checksum,\\n}, nil\\n}\\n```\\nThe contract's bytecode is not stored in state (it is actually unnecessary and wasteful to store it, since\\nthe Wasm VM already stores it and can be queried back, if needed). The checksum is simply the hash of the bytecode\\nof the contract and it is stored in state in an entry with key `checksums` that contains the checksums for the bytecodes that have been stored.\\n### How light client proxy works?\\nThe light client proxy behind the scenes will call a CosmWasm smart contract instance with incoming arguments serialized\\nin JSON format with appropriate environment information. Data returned by the smart contract is deserialized and\\nreturned to the caller.\\nConsider the example of the `VerifyClientMessage` function of `ClientState` interface. Incoming arguments are\\npackaged inside a payload object that is then JSON serialized and passed to `queryContract`, which executes `WasmVm.Query`\\nand returns the slice of bytes returned by the smart contract. This data is deserialized and passed as return argument.\\n```go\\ntype QueryMsg struct {\\nStatus               *StatusMsg               `json:\"status,omitempty\"`\\nExportMetadata       *ExportMetadataMsg       `json:\"export_metadata,omitempty\"`\\nTimestampAtHeight    *TimestampAtHeightMsg    `json:\"timestamp_at_height,omitempty\"`\\nVerifyClientMessage  *VerifyClientMessageMsg  `json:\"verify_client_message,omitempty\"`\\nCheckForMisbehaviour *CheckForMisbehaviourMsg `json:\"check_for_misbehaviour,omitempty\"`\\n}\\ntype verifyClientMessageMsg struct {\\nClientMessage *ClientMessage `json:\"client_message\"`\\n}\\n\/\/ VerifyClientMessage must verify a ClientMessage.\\n\/\/ A ClientMessage could be a Header, Misbehaviour, or batch update.\\n\/\/ It must handle each type of ClientMessage appropriately.\\n\/\/ Calls to CheckForMisbehaviour, UpdateSta\u00e5te, and UpdateStateOnMisbehaviour\\n\/\/ will assume that the content of the ClientMessage has been verified\\n\/\/ and can be trusted. An error should be returned\\n\/\/ if the ClientMessage fails to verify.\\nfunc (cs ClientState) VerifyClientMessage(\\nctx sdk.Context,\\n_ codec.BinaryCodec,\\nclientStore storetypes.KVStore,\\nclientMsg exported.ClientMessage\\n) error {\\nclientMessage, ok := clientMsg.(*ClientMessage)\\nif !ok {\\nreturn errorsmod.Wrapf(ibcerrors.ErrInvalidType, \"expected type: %T, got: %T\", &ClientMessage{}, clientMsg)\\n}\\npayload := QueryMsg{\\nVerifyClientMessage: &VerifyClientMessageMsg{ClientMessage: clientMessage.Data},\\n}\\n_, err := wasmQuery[EmptyResult](ctx, clientStore, &cs, payload)\\nreturn err\\n}\\n```\\n### Global Wasm VM variable\\nThe 08-wasm keeper structure keeps a reference to the Wasm VM instantiated in the keeper constructor function. The keeper uses\\nthe Wasm VM to store the bytecode of light client contracts. However, the Wasm VM is also needed in the 08-wasm implementations of\\nsome of the `ClientState` interface functions to initialise a contract, execute calls on the contract and query the contract. Since\\nthe `ClientState` functions do not have access to the 08-wasm keeper, then it has been decided to keep a global pointer variable that\\npoints to the same instance as the one in the 08-wasm keeper. This global pointer variable is then used in the implementations of\\nthe `ClientState` functions.\\n","tokens":374,"id":1441,"text":"## Context\\nCurrently in ibc-go light clients are defined as part of the codebase and are implemented as modules under\\n`modules\/light-clients`. Adding support for new light clients or updating an existing light client in the event\\nof a security issue or consensus update is a multi-step process which is both time-consuming and error-prone.\\nIn order to enable new IBC light client implementations it is necessary to modify the codebase of ibc-go (if the light\\nclient is part of its codebase), re-build chains' binaries, pass a governance proposal and validators upgrade their nodes.\\nAnother problem stemming from the above process is that if a chain wants to upgrade its own consensus, it will\\nneed to convince every chain or hub connected to it to upgrade its light client in order to stay connected. Due\\nto the time consuming process required to upgrade a light client, a chain with lots of connections needs to be\\ndisconnected for quite some time after upgrading its consensus, which can be very expensive in terms of time and effort.\\nWe are proposing simplifying this workflow by integrating a Wasm light client module that makes adding support for\\nnew light clients a simple governance-gated transaction. The light client bytecode, written in Wasm-compilable Rust,\\nruns inside a Wasm VM. The Wasm light client submodule exposes a proxy light client interface that routes incoming\\nmessages to the appropriate handler function, inside the Wasm VM for execution.\\nWith the Wasm light client module, anybody can add new IBC light client in the form of Wasm bytecode (provided they are\\nable to submit the governance proposal transaction and that it passes) as well as instantiate clients using any created\\nclient type. This allows any chain to update its own light client in other chains without going through the steps outlined above.\\n\n\n##Decision\nWe decided to implement the Wasm light client module as a light client proxy that will interface with the actual light client\\nuploaded as Wasm bytecode. To enable usage of the Wasm light client module, users need to add it to the list of allowed clients\\nby updating the `AllowedClients` parameter in the 02-client submodule of core IBC.\\n```go\\nparams := clientKeeper.GetParams(ctx)\\nparams.AllowedClients = append(params.AllowedClients, exported.Wasm)\\nclientKeeper.SetParams(ctx, params)\\n```\\nAdding a new light client contract is governance-gated. To upload a new light client users need to submit\\na [governance v1 proposal](https:\/\/docs.cosmos.network\/main\/modules\/gov#proposals) that contains the `sdk.Msg` for storing\\nthe Wasm contract's bytecode. The required message is `MsgStoreCode` and the bytecode is provided in the field `wasm_byte_code`:\\n```proto\\n\/\/ MsgStoreCode defines the request type for the StoreCode rpc.\\nmessage MsgStoreCode {\\n\/\/ signer address\\nstring signer = 1;\\n\/\/ wasm byte code of light client contract. It can be raw or gzip compressed\\nbytes wasm_byte_code = 2;\\n}\\n```\\nThe RPC handler processing `MsgStoreCode` will make sure that the signer of the message matches the address of authority allowed to\\nsubmit this message (which is normally the address of the governance module).\\n```go\\n\/\/ StoreCode defines a rpc handler method for MsgStoreCode\\nfunc (k Keeper) StoreCode(goCtx context.Context, msg *types.MsgStoreCode) (*types.MsgStoreCodeResponse, error) {\\nif k.GetAuthority() != msg.Signer {\\nreturn nil, errorsmod.Wrapf(ibcerrors.ErrUnauthorized, \"expected %s, got %s\", k.GetAuthority(), msg.Signer)\\n}\\nctx := sdk.UnwrapSDKContext(goCtx)\\nchecksum, err := k.storeWasmCode(ctx, msg.WasmByteCode, ibcwasm.GetVM().StoreCode)\\nif err != nil {\\nreturn nil, errorsmod.Wrap(err, \"failed to store wasm bytecode\")\\n}\\nemitStoreWasmCodeEvent(ctx, checksum)\\nreturn &types.MsgStoreCodeResponse{\\nChecksum: checksum,\\n}, nil\\n}\\n```\\nThe contract's bytecode is not stored in state (it is actually unnecessary and wasteful to store it, since\\nthe Wasm VM already stores it and can be queried back, if needed). The checksum is simply the hash of the bytecode\\nof the contract and it is stored in state in an entry with key `checksums` that contains the checksums for the bytecodes that have been stored.\\n### How light client proxy works?\\nThe light client proxy behind the scenes will call a CosmWasm smart contract instance with incoming arguments serialized\\nin JSON format with appropriate environment information. Data returned by the smart contract is deserialized and\\nreturned to the caller.\\nConsider the example of the `VerifyClientMessage` function of `ClientState` interface. Incoming arguments are\\npackaged inside a payload object that is then JSON serialized and passed to `queryContract`, which executes `WasmVm.Query`\\nand returns the slice of bytes returned by the smart contract. This data is deserialized and passed as return argument.\\n```go\\ntype QueryMsg struct {\\nStatus               *StatusMsg               `json:\"status,omitempty\"`\\nExportMetadata       *ExportMetadataMsg       `json:\"export_metadata,omitempty\"`\\nTimestampAtHeight    *TimestampAtHeightMsg    `json:\"timestamp_at_height,omitempty\"`\\nVerifyClientMessage  *VerifyClientMessageMsg  `json:\"verify_client_message,omitempty\"`\\nCheckForMisbehaviour *CheckForMisbehaviourMsg `json:\"check_for_misbehaviour,omitempty\"`\\n}\\ntype verifyClientMessageMsg struct {\\nClientMessage *ClientMessage `json:\"client_message\"`\\n}\\n\/\/ VerifyClientMessage must verify a ClientMessage.\\n\/\/ A ClientMessage could be a Header, Misbehaviour, or batch update.\\n\/\/ It must handle each type of ClientMessage appropriately.\\n\/\/ Calls to CheckForMisbehaviour, UpdateSta\u00e5te, and UpdateStateOnMisbehaviour\\n\/\/ will assume that the content of the ClientMessage has been verified\\n\/\/ and can be trusted. An error should be returned\\n\/\/ if the ClientMessage fails to verify.\\nfunc (cs ClientState) VerifyClientMessage(\\nctx sdk.Context,\\n_ codec.BinaryCodec,\\nclientStore storetypes.KVStore,\\nclientMsg exported.ClientMessage\\n) error {\\nclientMessage, ok := clientMsg.(*ClientMessage)\\nif !ok {\\nreturn errorsmod.Wrapf(ibcerrors.ErrInvalidType, \"expected type: %T, got: %T\", &ClientMessage{}, clientMsg)\\n}\\npayload := QueryMsg{\\nVerifyClientMessage: &VerifyClientMessageMsg{ClientMessage: clientMessage.Data},\\n}\\n_, err := wasmQuery[EmptyResult](ctx, clientStore, &cs, payload)\\nreturn err\\n}\\n```\\n### Global Wasm VM variable\\nThe 08-wasm keeper structure keeps a reference to the Wasm VM instantiated in the keeper constructor function. The keeper uses\\nthe Wasm VM to store the bytecode of light client contracts. However, the Wasm VM is also needed in the 08-wasm implementations of\\nsome of the `ClientState` interface functions to initialise a contract, execute calls on the contract and query the contract. Since\\nthe `ClientState` functions do not have access to the 08-wasm keeper, then it has been decided to keep a global pointer variable that\\npoints to the same instance as the one in the 08-wasm keeper. This global pointer variable is then used in the implementations of\\nthe `ClientState` functions.\\n","Predictions":"We decided to implement the Wasm light client module as a light client proxy that will interface with the actual light clientnuploaded as Wasm bytecode. To enable usage of the Wasm light client module, users need to add it to the list of allowednclients. To enable usage of the Wasm light client module, users need to add it to the list of allowed clients.n"}
{"File Name":"ibc-rs\/adr-005-relayer-v0-implementation.md","Context":"## Context\nThis ADR documents the implementation of the `v0.1` [relayer lib crate]\n[ibc-relayer].\nThis library is instantiated in the [Hermes][hermes] binary of the\n[ibc-relayer-cli crate][ibc-relayer-cli] (which is not the focus of this discussion).\nAs a main design goal, `v0.1` is meant to lay a foundation upon which we can\nadd more features and enhancements incrementally with later relayer versions.\nThis is to say that `v0.1` may be deficient in terms of features or\nrobustness, and rather aims to be simple, adaptable, and extensible.\nFor this reason, we primarily discuss aspects of concurrency and architecture.\n### Relayer versioning scheme\nOn the mid-term, the relayer architecture is set out to evolve across three\nversions.\nThe first of these, `v0.1`, makes several simplifying assumptions\nabout the environment of the relayer and its features. These assumptions\nare important towards limiting the scope that `v0.1` aims to\ncover, and allowing a focus on the architecture and concurrency model to\nprovide for growth in the future.\nThese assumptions are documented below in the [decision](#decision) section.\n","Decision":"### Configuration\\nFor the most part, the relayer configuration will be\\nstatic: the configuration for chains and their respective objects (clients,\\nconnections, or channels) will be fully specified in the relayer\\nconfiguration file and will not change throughout execution.\\nLight clients are also statically defined in the config file, and cannot be\\nswitched dynamically at runtime.\\nRecent changes to the ICS protocol specifies identifier\\nselection for clients, connections, and channels to be [deterministic][ids].\\nFor this reason, we will not need to specify any identifiers in the\\nconfiguration file.\\nWe only specify which pairs of chains should communicate with one\\nanother, and the port identifier to use for that purpose.\\nThis pair of chains plus their corresponding port identifiers is called a\\n__relaying path__.\\nAny relaying path is unidirectional.\\nAn example with the relevant section of the configuration file follows.\\n```toml\\n[[connections]]\\na_chain = 'ibc-0'\\nb_chain = 'ibc-1'\\n[[connections.paths]]\\na_port = 'transfer'\\nb_port = 'transfer'\\n```\\nHere there are two chains, ith one connection between them, and a path for\\nrelaying on the port called `transfer` on both chains, from chain `ibc-0`\\nto `ibc-1`.\\n### Links\\nA [link][link] is a relayer-level protocol that implements packet relay across\\none relaying path.\\nThe relayer at `v0.1` will focus on a single link.\\nThis limitation will be lifted in subsequent versions.\\n### Chain State\\nEach chain is assumed to start with an empty IBC state.\\nThis means that the relayer will take care of creating the client,\\nconnection, and channel objects respectively on each side of a link.\\n### Proof Verification\\nThe `v0.1` relayer will _not_ do proof verification.\\n### Feature set\\nThe [complete list of features is documented elsewhere][features] in detail.\\n","tokens":275,"id":4650,"text":"## Context\\nThis ADR documents the implementation of the `v0.1` [relayer lib crate]\\n[ibc-relayer].\\nThis library is instantiated in the [Hermes][hermes] binary of the\\n[ibc-relayer-cli crate][ibc-relayer-cli] (which is not the focus of this discussion).\\nAs a main design goal, `v0.1` is meant to lay a foundation upon which we can\\nadd more features and enhancements incrementally with later relayer versions.\\nThis is to say that `v0.1` may be deficient in terms of features or\\nrobustness, and rather aims to be simple, adaptable, and extensible.\\nFor this reason, we primarily discuss aspects of concurrency and architecture.\\n### Relayer versioning scheme\\nOn the mid-term, the relayer architecture is set out to evolve across three\\nversions.\\nThe first of these, `v0.1`, makes several simplifying assumptions\\nabout the environment of the relayer and its features. These assumptions\\nare important towards limiting the scope that `v0.1` aims to\\ncover, and allowing a focus on the architecture and concurrency model to\\nprovide for growth in the future.\\nThese assumptions are documented below in the [decision](#decision) section.\\n\n\n##Decision\n### Configuration\\nFor the most part, the relayer configuration will be\\nstatic: the configuration for chains and their respective objects (clients,\\nconnections, or channels) will be fully specified in the relayer\\nconfiguration file and will not change throughout execution.\\nLight clients are also statically defined in the config file, and cannot be\\nswitched dynamically at runtime.\\nRecent changes to the ICS protocol specifies identifier\\nselection for clients, connections, and channels to be [deterministic][ids].\\nFor this reason, we will not need to specify any identifiers in the\\nconfiguration file.\\nWe only specify which pairs of chains should communicate with one\\nanother, and the port identifier to use for that purpose.\\nThis pair of chains plus their corresponding port identifiers is called a\\n__relaying path__.\\nAny relaying path is unidirectional.\\nAn example with the relevant section of the configuration file follows.\\n```toml\\n[[connections]]\\na_chain = 'ibc-0'\\nb_chain = 'ibc-1'\\n[[connections.paths]]\\na_port = 'transfer'\\nb_port = 'transfer'\\n```\\nHere there are two chains, ith one connection between them, and a path for\\nrelaying on the port called `transfer` on both chains, from chain `ibc-0`\\nto `ibc-1`.\\n### Links\\nA [link][link] is a relayer-level protocol that implements packet relay across\\none relaying path.\\nThe relayer at `v0.1` will focus on a single link.\\nThis limitation will be lifted in subsequent versions.\\n### Chain State\\nEach chain is assumed to start with an empty IBC state.\\nThis means that the relayer will take care of creating the client,\\nconnection, and channel objects respectively on each side of a link.\\n### Proof Verification\\nThe `v0.1` relayer will _not_ do proof verification.\\n### Feature set\\nThe [complete list of features is documented elsewhere][features] in detail.\\n","Predictions":"We will use the [stack-init][stack-init] to implement the v1 relayer.n"}
{"File Name":"adr-demo\/0004-use-elastic-search-for-image-search.md","Context":"## Context\nAs our users base is growing and we have more and more search queries, we would like to improve current search form to:\n* Be able to search on newly added fields (e.g. created date, modified date, author, licence, etc)\n* Allow more complex searches on existing and new fields such as:\n* \"and\" queries to search on multiple tags\n* \"or\" queries to search on an exclusive list of tags\n* \"startsWith\" and \"contains\" search instead of current exact match searches\n* synonyms\n","Decision":"Given our needs it seems pretty clear that we need a search engine, as databases search capabilities are limited and would cost more and more as data is growing.\\nWe will use Elastic Search as our search engine as it is the de facto standard and is based on battle-tested search engine\\nLucene. Our current microservices will now evolve to this schema:\\n![](microservices-architecture-with-es.png)\\nNote: ES indexes do not need to be updated in real-time when a new image\/gif is uploaded, in fact we can easily\\ntolerate a few minutes of delay between an upload and its actual availability for search. All we need is to be\\neventually consistent.\\n","tokens":113,"id":2513,"text":"## Context\\nAs our users base is growing and we have more and more search queries, we would like to improve current search form to:\\n* Be able to search on newly added fields (e.g. created date, modified date, author, licence, etc)\\n* Allow more complex searches on existing and new fields such as:\\n* \"and\" queries to search on multiple tags\\n* \"or\" queries to search on an exclusive list of tags\\n* \"startsWith\" and \"contains\" search instead of current exact match searches\\n* synonyms\\n\n\n##Decision\nGiven our needs it seems pretty clear that we need a search engine, as databases search capabilities are limited and would cost more and more as data is growing.\\nWe will use Elastic Search as our search engine as it is the de facto standard and is based on battle-tested search engine\\nLucene. Our current microservices will now evolve to this schema:\\n![](microservices-architecture-with-es.png)\\nNote: ES indexes do not need to be updated in real-time when a new image\/gif is uploaded, in fact we can easily\\ntolerate a few minutes of delay between an upload and its actual availability for search. All we need is to be\\neventually consistent.\\n","Predictions":"We will use the [Semantic Search][SemanticSearch] as our search engine as it is the de facto standard and is based on battle-tested search enginenLucene. Our current microservices will now evolve to this schema:n![](microservices-architecture-with-es.png)nNote: ES indexes do not need to be updated in real-time when a new image\/gif is uploaded, in fact we can easilyntolerate a few minutes of delay between an upload and its actual availability for search. All we need is to beneventually consistent.n"}
{"File Name":"DunkMe\/0002-data-access-api.md","Context":"#### Context and Problem Statement\nA technology stack is required for the projects data access application programming interface (API).\n#### Considered Options\n- ASP.NET Web API (C#)\n#### Decision Outcome\nChosen option: \"ASP.NET Web API (C#)\", because\n- This can target .Net Core 2\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\n- A repository pattern can be applied facilitating Unit Testing.\n([back](README.md))\n","Decision":"Chosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n","tokens":111,"id":1660,"text":"#### Context and Problem Statement\\nA technology stack is required for the projects data access application programming interface (API).\\n#### Considered Options\\n- ASP.NET Web API (C#)\\n#### Decision Outcome\\nChosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n\n\n##Decision\nChosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n","Predictions":"Chosen option: \"Api\", becausen- Api is a very popular and well known programming language.n- A GitHub page is a very popular and well known library.n- A GitHub page is a very popular and well known library.n- A GitHub page is a very popular and well known library.n- A GitHub page is a very popular and well known library.n"}
{"File Name":"winery\/0031-reuse-refinement-code-for-pattern-detection.md","Context":"## Context and Problem Statement\nTo create an executable deployment model, the pattern refinement process replaces a matching subgraph with the Refinement Structure of a PRM.\nTo create a PbDCM, the pattern detection process replaces a matching subgraph with the Detector of a PRM.\nThe replacement procedure is identical for both processes, only the structures used for the replacement differ.\nTherefore, the implementation of the pattern refinement process should be reused to implement the pattern detection process.\n## Decision Drivers\n* Avoid duplicate code\n* Avoid introducing errors and inconsistencies during reimplementation\n","Decision":"* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\nChosen option: \"Swap the Detector of all PRMs with their Refinement Structures\", because reimplementation introduces too much duplicate code and a common interface requires a lot of boilerplate code while also decreasing readability.\\n### Positive Consequences <!-- optional -->\\n* Complete pattern refinement implementation can be reused\\n### Negative consequences <!-- optional -->\\n* Readability and understandability decreases\\n","tokens":115,"id":4299,"text":"## Context and Problem Statement\\nTo create an executable deployment model, the pattern refinement process replaces a matching subgraph with the Refinement Structure of a PRM.\\nTo create a PbDCM, the pattern detection process replaces a matching subgraph with the Detector of a PRM.\\nThe replacement procedure is identical for both processes, only the structures used for the replacement differ.\\nTherefore, the implementation of the pattern refinement process should be reused to implement the pattern detection process.\\n## Decision Drivers\\n* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\n\n\n##Decision\n* Avoid duplicate code\\n* Avoid introducing errors and inconsistencies during reimplementation\\nChosen option: \"Swap the Detector of all PRMs with their Refinement Structures\", because reimplementation introduces too much duplicate code and a common interface requires a lot of boilerplate code while also decreasing readability.\\n### Positive Consequences <!-- optional -->\\n* Complete pattern refinement implementation can be reused\\n### Negative consequences <!-- optional -->\\n* Readability and understandability decreases\\n","Predictions":"* Avoid duplicate coden* Avoid introducing errors and inconsistencies during reimplementationnChosen option: \"Swap the Detector of all PRMs with their Refinement Structures\", because reimplementation introduces too much duplicate code and a common interface requires a lot of boilerplate code while also declining readability.n### Positive Consequences!-- optional -->n* Complete pattern refinement implementation can be reusedn### Negative consequences!-- optional -->n* Readability and understandability decreasesn"}
{"File Name":"lbh-adrs\/Validation.md","Context":"## **Context**\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\nWe will look at two options for validation:\n- **Data Annotations**\nThis involves \"annotating\" each class model with specific validation, such as\n- `[Required(ErrorMessage = \"This field is required\")]`\n- `[MaxLength(20)]`\nThere are a number of issues with this approach:\n- Validation is scattered throughout the codebase as attributes on data model classes\n- Testing is not straightforward\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\n- Does not allow for conditional validation\n- **Fluent Validation**\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\n- Lives outside of the data model classes\n- Very easy to test validation in isolation\n- Error messaging can be externalised using dependency injection\n- Allows for chaining of validators and conditional validation\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\n","Decision":"**Fluent Validation**\\nFluent Valdation is widely used, offers a lot of flexibility and allows for a clean, customisable and testable approach to validation.\\n","tokens":276,"id":2307,"text":"## **Context**\\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\\nWe will look at two options for validation:\\n- **Data Annotations**\\nThis involves \"annotating\" each class model with specific validation, such as\\n- `[Required(ErrorMessage = \"This field is required\")]`\\n- `[MaxLength(20)]`\\nThere are a number of issues with this approach:\\n- Validation is scattered throughout the codebase as attributes on data model classes\\n- Testing is not straightforward\\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\\n- Does not allow for conditional validation\\n- **Fluent Validation**\\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\\n- Lives outside of the data model classes\\n- Very easy to test validation in isolation\\n- Error messaging can be externalised using dependency injection\\n- Allows for chaining of validators and conditional validation\\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\\n\n\n##Decision\n**Fluent Validation**\\nFluent Valdation is widely used, offers a lot of flexibility and allows for a clean, customisable and testable approach to validation.\\n","Predictions":"* **Data Annotations**nThis involves \"annotating\" each class model with specific validation, such asn- [Required(ErrorMessage = \"This field is required\")]n- [MaxLength(20)]nThere are many issues with this approach:n* Validation is scatter throughout the codebase as attributes on data model classesn* Testing is not straightforwardn* Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messagesn* Does not allow for conditional validationn* **Fluent Validation**nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:n- Is easy to configure with minimal, unintrusive setup in Startup.csn- Lives outside of the data model classesn- Very easy to test validation in isolationn- Error messaging can be externalised using dependency injectionn- Allows for chaining of validators and conditional validationn- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)n"}
{"File Name":"sre-adrs\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made for our team.\n","Decision":"We will use Architecture Decision Records, as discussed in our original proposal: https:\/\/docs.google.com\/document\/d\/1pZlYCyXcZbmQq1O-g4BNJD1uZTjluYKkk7BSu2BwOGU\/edit#\\n","tokens":16,"id":2261,"text":"## Context\\nWe need to record the architectural decisions made for our team.\\n\n\n##Decision\nWe will use Architecture Decision Records, as discussed in our original proposal: https:\/\/docs.google.com\/document\/d\/1pZlYCyXcZbmQq1O-g4BNJD1uZTjluYKkk7BSu2BwOGU\/edit#\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"corona-hackathon\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3663,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"scholarsphere\/0002-define-use-of-decorators-and-presenters.md","Context":"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\nWe need to distinguish between decorator and presenter objects in order to clarify which would be used in a given\nsituation.\n","Decision":"Decorators extend SimpleDelegator and will always delegate undefined methods to the delegated object.\\nPresenters take the form of \"plain ol' Ruby objects\" (POROs) and would generally not delegate methods to an object.\\nTheir usage is designed to be more flexible when the rendering of content isn't tied specifically to one object.\\n","tokens":46,"id":4556,"text":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nWe need to distinguish between decorator and presenter objects in order to clarify which would be used in a given\\nsituation.\\n\n\n##Decision\nDecorators extend SimpleDelegator and will always delegate undefined methods to the delegated object.\\nPresenters take the form of \"plain ol' Ruby objects\" (POROs) and would generally not delegate methods to an object.\\nTheir usage is designed to be more flexible when the rendering of content isn't tied specifically to one object.\\n","Predictions":"We will use the [Semantic Data Model](https:\/\/semanticdatamodel.org\/) to model the data we want to store.n"}
{"File Name":"pace-developers\/0002-use-github-pages-for-user-documentation.md","Context":"## Context\nA consistent platform is required for user documentation from the PACE projects: Brille, Euphonic and Horace\/Herbert.\nTwo platforms support, are well used for this service:\n- [Read the Docs](https:\/\/readthedocs.org\/)\n- [GitHub pages](https:\/\/pages.github.com\/)\nBoth platforms will display documentation built by [Sphinx](https:\/\/www.sphinx-doc.org\/) from reStructuredText source files.\nBrille includes compiled C libraries that contribute APIs to the project. The Read the Docs build\/deploy pipeline does not support inclusion of documentation generated from C source.\n","Decision":"We will use GitHub pages for all PACE project documentation.\\n","tokens":132,"id":5172,"text":"## Context\\nA consistent platform is required for user documentation from the PACE projects: Brille, Euphonic and Horace\/Herbert.\\nTwo platforms support, are well used for this service:\\n- [Read the Docs](https:\/\/readthedocs.org\/)\\n- [GitHub pages](https:\/\/pages.github.com\/)\\nBoth platforms will display documentation built by [Sphinx](https:\/\/www.sphinx-doc.org\/) from reStructuredText source files.\\nBrille includes compiled C libraries that contribute APIs to the project. The Read the Docs build\/deploy pipeline does not support inclusion of documentation generated from C source.\\n\n\n##Decision\nWe will use GitHub pages for all PACE project documentation.\\n","Predictions":"We will use GitHub pages for all PACE project documentation.n"}
{"File Name":"DunkMe\/0001-front-end.md","Context":"#### Context and Problem Statement\nA front end, client facing technology stack \/ language is needed.\n#### Considered Options\n- ASP.NET MVC (C#)\n- ASP.NET Web Forms (C#)\n- Angular (with TypeScript)\n#### Decision Outcome\nChosen option: \"Angular\", because\n- This is in-line with Trade Me's technical stack trajectory.\n- In line with what is considered industry standard for green field projects.\n","Decision":"Chosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n","tokens":93,"id":1659,"text":"#### Context and Problem Statement\\nA front end, client facing technology stack \/ language is needed.\\n#### Considered Options\\n- ASP.NET MVC (C#)\\n- ASP.NET Web Forms (C#)\\n- Angular (with TypeScript)\\n#### Decision Outcome\\nChosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n\n\n##Decision\nChosen option: \"Angular\", because\\n- This is in-line with Trade Me's technical stack trajectory.\\n- In line with what is considered industry standard for green field projects.\\n","Predictions":"Chosen option: \"Angular\", becausen- This is in-line with Trade Me's technical stack trajectory.n- In line with what is considered industry standard for green field projects.n"}
{"File Name":"AYCHAIN.github.io\/adr-002-css-structure.md","Context":"## Context\nWeb Components encapsulate their shadow dom away from the rest of the DOM, meaning that a general stylesheet cannot style the internals of a Web Component.\nAs such, we need to find a strategy to maintain component level styles but enable shared styles so that we limit code duplication.\nWe had previously decided to follow ITCSS for its way to enable scalability as well as limit specificity war, and would like to keep following this approach.\n","Decision":"Considering the outcome of the exploration, and seeing that we want to maintain a sense of scalability and separation of concerns as advocated by the ITCSS methodology, we have decided to maintain component specific stylesheets at component directory level, and use Sass `import` within these sheets to add specific classes that many component might use (basically any other layer from ITCSS beyond components).\\nShared styles will be hosted under `src\/shared-styles` folder and organized following the ITCSS nomenclature.\\nAt this moment the `main.scss` file is maintained as it contains information around the available files, and may prove worthy in the future. The value of this needs to be reassessed.\\n","tokens":91,"id":157,"text":"## Context\\nWeb Components encapsulate their shadow dom away from the rest of the DOM, meaning that a general stylesheet cannot style the internals of a Web Component.\\nAs such, we need to find a strategy to maintain component level styles but enable shared styles so that we limit code duplication.\\nWe had previously decided to follow ITCSS for its way to enable scalability as well as limit specificity war, and would like to keep following this approach.\\n\n\n##Decision\nConsidering the outcome of the exploration, and seeing that we want to maintain a sense of scalability and separation of concerns as advocated by the ITCSS methodology, we have decided to maintain component specific stylesheets at component directory level, and use Sass `import` within these sheets to add specific classes that many component might use (basically any other layer from ITCSS beyond components).\\nShared styles will be hosted under `src\/shared-styles` folder and organized following the ITCSS nomenclature.\\nAt this moment the `main.scss` file is maintained as it contains information around the available files, and may prove worthy in the future. The value of this needs to be reassessed.\\n","Predictions":"We will use stack to store the stylesheets.n"}
{"File Name":"ditto\/DADR-0004-signal-enrichment.md","Context":"## Context\nSupporting a new feature, the so called [signal enrichment](https:\/\/github.com\/eclipse-ditto\/ditto\/issues\/561), raises a few\nquestions towards throughput and scalability impact of that new feature.\nIn the current architecture, Ditto internally publishes events (as part of the applied \"event sourcing\" pattern) for\neach change which was done to a `Thing`. This event is the same as the persisted one only containing the actually\nchanged fields.\nThe \"signal enrichment\" feature shall support defining `extraFields` to be sent out to external event subscribers, e.g.\nbeing notified about changes via WebSocket, Server Sent Events (SSEs) or connections (AMQP, MQTT, Kafka, ...).\nThe following alternatives were considered on how to implement that feature:\n1. Sending along the complete `Thing` state in each event in the cluster\n* upside: \"tell, don't ask\" principle -> would lead to a minimum of required cluster remoting \/ roundtrips\n* downside: bigger payload sent around\n* downside: a lot of deserialization effort for all event consuming services\n* downside: policy filtering would have to be additionally done somewhere only included data which the `authSubject` is allowed to READ\n* downside: overall a lot of overhead for probably only few consumers\n2. Enriching the data for sessions\/connections which selected `extraFields` for each incoming event\n* upside: no additional payload for existing events\n* upside: data is only enriched for sessions\/connections really using that feature\n* upside: policy enforcement\/filtering is done by default concierge mechanism for each single request, so is always up-to-date with policy\n* downside: additional 4 remoting (e.g.: gateway-concierge-things-concierge-gateway) calls for each to be enriched event\n* delayed event publishing\n* additional deserialization efforts\n* potentially asking for the same static values each time\n3. Cache based enriching of the data for sessions\/connections which selected `extraFields` for each incoming event\n* upsides: all upsides of approach 2 except that policy is always up-to-date\n* upside: mitigating downsides of approach 2 (because of cache the additional roundtrips are reduced or even completely skipped)\n* downside: cached data as well as policy information might be outdated a configurable amount of time (e.g. 2 minutes)\n","Decision":"We provide 2 different facade implementations providing approach 2 and 3:\\n* `org.eclipse.ditto.internal.models.signalenrichment.ByRoundTripSignalEnrichmentFacade`:\\nRound-trip for each to-be-enriched event resulting in a guaranteed up-to-dateness of data and applied policy.\\n* `org.eclipse.ditto.internal.models.signalenrichment.DittoCachingSignalEnrichmentFacade`:\\nUsing cache for each to-be-enriched event resulting in reduced remoting effort and a time interval where the cache might be out of sync with the current data or policy information.\\n* the implementation uses a cluster-instance wide cache using a cache key consisting of: `thingId, authSubjects, jsonFieldSelector`\\n* the overall size of this cache is configured, by default to `20,000` entries\\n* there is an additional \"smart-update\" mechanism for cache entries related to enrichment of twin events:\\nin the absence of skipped events, the cache entry can be completely deduced from the twin events triggering\\nenrichment and will stay up-to-date with thing changes (but not with policy changes).\\nThe configured default in Ditto is the `CachingSignalEnrichmentFacade` but may be configured via\\n* connectivity service: environment variable `CONNECTIVITY_SIGNAL_ENRICHMENT_PROVIDER`\\n* gateway service: environment variable `GATEWAY_SIGNAL_ENRICHMENT_PROVIDER`\\n","tokens":500,"id":775,"text":"## Context\\nSupporting a new feature, the so called [signal enrichment](https:\/\/github.com\/eclipse-ditto\/ditto\/issues\/561), raises a few\\nquestions towards throughput and scalability impact of that new feature.\\nIn the current architecture, Ditto internally publishes events (as part of the applied \"event sourcing\" pattern) for\\neach change which was done to a `Thing`. This event is the same as the persisted one only containing the actually\\nchanged fields.\\nThe \"signal enrichment\" feature shall support defining `extraFields` to be sent out to external event subscribers, e.g.\\nbeing notified about changes via WebSocket, Server Sent Events (SSEs) or connections (AMQP, MQTT, Kafka, ...).\\nThe following alternatives were considered on how to implement that feature:\\n1. Sending along the complete `Thing` state in each event in the cluster\\n* upside: \"tell, don't ask\" principle -> would lead to a minimum of required cluster remoting \/ roundtrips\\n* downside: bigger payload sent around\\n* downside: a lot of deserialization effort for all event consuming services\\n* downside: policy filtering would have to be additionally done somewhere only included data which the `authSubject` is allowed to READ\\n* downside: overall a lot of overhead for probably only few consumers\\n2. Enriching the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upside: no additional payload for existing events\\n* upside: data is only enriched for sessions\/connections really using that feature\\n* upside: policy enforcement\/filtering is done by default concierge mechanism for each single request, so is always up-to-date with policy\\n* downside: additional 4 remoting (e.g.: gateway-concierge-things-concierge-gateway) calls for each to be enriched event\\n* delayed event publishing\\n* additional deserialization efforts\\n* potentially asking for the same static values each time\\n3. Cache based enriching of the data for sessions\/connections which selected `extraFields` for each incoming event\\n* upsides: all upsides of approach 2 except that policy is always up-to-date\\n* upside: mitigating downsides of approach 2 (because of cache the additional roundtrips are reduced or even completely skipped)\\n* downside: cached data as well as policy information might be outdated a configurable amount of time (e.g. 2 minutes)\\n\n\n##Decision\nWe provide 2 different facade implementations providing approach 2 and 3:\\n* `org.eclipse.ditto.internal.models.signalenrichment.ByRoundTripSignalEnrichmentFacade`:\\nRound-trip for each to-be-enriched event resulting in a guaranteed up-to-dateness of data and applied policy.\\n* `org.eclipse.ditto.internal.models.signalenrichment.DittoCachingSignalEnrichmentFacade`:\\nUsing cache for each to-be-enriched event resulting in reduced remoting effort and a time interval where the cache might be out of sync with the current data or policy information.\\n* the implementation uses a cluster-instance wide cache using a cache key consisting of: `thingId, authSubjects, jsonFieldSelector`\\n* the overall size of this cache is configured, by default to `20,000` entries\\n* there is an additional \"smart-update\" mechanism for cache entries related to enrichment of twin events:\\nin the absence of skipped events, the cache entry can be completely deduced from the twin events triggering\\nenrichment and will stay up-to-date with thing changes (but not with policy changes).\\nThe configured default in Ditto is the `CachingSignalEnrichmentFacade` but may be configured via\\n* connectivity service: environment variable `CONNECTIVITY_SIGNAL_ENRICHMENT_PROVIDER`\\n* gateway service: environment variable `GATEWAY_SIGNAL_ENRICHMENT_PROVIDER`\\n","Predictions":"* Serialization is a serialization function that is used to encodenthe serialized text into a text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.nThe text format is used to encode the text format.n"}
{"File Name":"smarthub\/0002-use-lerna-for-package-versioning.md","Context":"## Context\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\n","Decision":"Migrate code base to monorepo structure and use `lerna` for versioning management.\\n","tokens":39,"id":3691,"text":"## Context\\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\\n\n\n##Decision\nMigrate code base to monorepo structure and use `lerna` for versioning management.\\n","Predictions":"Use Rollbar to collect and manage our application errors.n"}
{"File Name":"raster-foundry\/adr-0004-static-asset-pipeline.md","Context":"## Context\nIn selecting a static asset pipeline, there are several factors that are important to us.\nWe don\u2019t want to lose features that speed up developer feedback, such as a dev server with linting, auto-gen, and browser reload.\nThe solution needs to support tests run with a single command similar to what we\u2019ve had in previous projects.\nThe solution needs to be fast.\nSome team members have expressed dissatisfaction with the stability of the NPM ecosystem, but pretty much everything relies on it to some extent, so the issue is probably best addressed elsewhere (possibly using a lazy NPM cache).\nIn the past, our team has used Grunt to manage and build static assets. Various team members \/ designers have expressed discontent with the idea of going forward using Grunt, primarily due to inherent factors such as long compile times and configuration complexity.\nOther teams in Azavea have had good experiences with Webpack. Webpack is not a complete replacement for Grunt, but of the difference in functionality, it is highly likely that NPM tasks will cover it if used in conjunction. Despite scattered complaints of complexity and poor documentation found while researching, I was able to set up pipeline using npm and webpack overnight without too many issues. Significant features which are built in include a dev server capable of supporting hot reloading, very fast feedback on changes, and support for all the tools we commonly use in our applications. In addition to quality of life features, Webpack also gives us a dependency graph which includes static assets and bundle splitting. Combined with NPM, it appears to be a good solution.\nA final option that we have to consider is Gulp, which has been used on a couple of projects by the Urban Apps team. Where grunt uses configuration files, Gulp uses code. Gulp\u2019s main advantage is speed: streaming operations mean that files never touch disk between chained tasks. Other than the method of configuration and speed of operating, the basic functionality of Gulp and Grunt seem to be fairly aligned.\n","Decision":"Given the good experiences that others in the company have had with Webpack, we\u2019ve decided to run the static asset pipeline using a combination of npm and webpack. Npm will be used to manage dependencies and initiating tasks, while webpack will be used to run the dev server and build static assets.\\n","tokens":408,"id":1680,"text":"## Context\\nIn selecting a static asset pipeline, there are several factors that are important to us.\\nWe don\u2019t want to lose features that speed up developer feedback, such as a dev server with linting, auto-gen, and browser reload.\\nThe solution needs to support tests run with a single command similar to what we\u2019ve had in previous projects.\\nThe solution needs to be fast.\\nSome team members have expressed dissatisfaction with the stability of the NPM ecosystem, but pretty much everything relies on it to some extent, so the issue is probably best addressed elsewhere (possibly using a lazy NPM cache).\\nIn the past, our team has used Grunt to manage and build static assets. Various team members \/ designers have expressed discontent with the idea of going forward using Grunt, primarily due to inherent factors such as long compile times and configuration complexity.\\nOther teams in Azavea have had good experiences with Webpack. Webpack is not a complete replacement for Grunt, but of the difference in functionality, it is highly likely that NPM tasks will cover it if used in conjunction. Despite scattered complaints of complexity and poor documentation found while researching, I was able to set up pipeline using npm and webpack overnight without too many issues. Significant features which are built in include a dev server capable of supporting hot reloading, very fast feedback on changes, and support for all the tools we commonly use in our applications. In addition to quality of life features, Webpack also gives us a dependency graph which includes static assets and bundle splitting. Combined with NPM, it appears to be a good solution.\\nA final option that we have to consider is Gulp, which has been used on a couple of projects by the Urban Apps team. Where grunt uses configuration files, Gulp uses code. Gulp\u2019s main advantage is speed: streaming operations mean that files never touch disk between chained tasks. Other than the method of configuration and speed of operating, the basic functionality of Gulp and Grunt seem to be fairly aligned.\\n\n\n##Decision\nGiven the good experiences that others in the company have had with Webpack, we\u2019ve decided to run the static asset pipeline using a combination of npm and webpack. Npm will be used to manage dependencies and initiating tasks, while webpack will be used to run the dev server and build static assets.\\n","Predictions":"We will use npm to run the static asset pipeline.n"}
{"File Name":"fundraising-application\/020_PayPal_IPN_Queue.md","Context":"## Context and Problem Statement\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\nand was only noticed some days after when the Project Manager needed to\nexport the data. Upon investigation, it was discovered that:\n* Error logging was inactive on the application. (Now fixed)\n* We can\u2019t debug using the responses our system returned to PayPal as\nwe don\u2019t have access to the IPN log.\nThis led to a situation where we couldn't get the information required\nto debug the error. It was suggested we queue all incoming requests from\nPayPal on our own system for processing by our system.\n## Decision Drivers\n* **Transparency**: If our system fails we would have a stored queue to\nuse for debugging.\n* **Automation**: The IPNs wouldn't need to be fired again once an error\nbecomes fixed as our system would resume processing the queue.\n","Decision":"* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\nSince this was the first occurrence of the problem, and the Fundraising\\nApplication system is now running well again, we decided against introducing\\nthe IPN queue feature.\\n","tokens":190,"id":1523,"text":"## Context and Problem Statement\\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\\nand was only noticed some days after when the Project Manager needed to\\nexport the data. Upon investigation, it was discovered that:\\n* Error logging was inactive on the application. (Now fixed)\\n* We can\u2019t debug using the responses our system returned to PayPal as\\nwe don\u2019t have access to the IPN log.\\nThis led to a situation where we couldn't get the information required\\nto debug the error. It was suggested we queue all incoming requests from\\nPayPal on our own system for processing by our system.\\n## Decision Drivers\\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\n\n\n##Decision\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\nSince this was the first occurrence of the problem, and the Fundraising\\nApplication system is now running well again, we decided against introducing\\nthe IPN queue feature.\\n","Predictions":"* **Transparency**: If our system fails we would have a stored queue tonuse for debugging.n* **Automation**: The IPNs wouldn't need to be fired again once an errornbecomes fixed as our system would resume processing the queue.nSince this was the first occurrence of the problem, and the FundraisingnApplication system is now running well again, we decided against introducingnthe IPN queue feature.n"}
{"File Name":"dotfiles\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: [http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","tokens":16,"id":117,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: [http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"volley-management\/0002-use-sendgrid-as-mailing-system.md","Context":"## Context and Problem Statement\nWe need a way to send emails to users for several workflows.\n","Decision":"Chosen option: SendGrid, because it is SaaS and has generous free tier. Integration is easy.\\n### Positive Consequences <!-- optional -->\\n* Fast time to develop\\n### Negative Consequences\\n* We need to think about setting up development environment\\n","tokens":20,"id":2523,"text":"## Context and Problem Statement\\nWe need a way to send emails to users for several workflows.\\n\n\n##Decision\nChosen option: SendGrid, because it is SaaS and has generous free tier. Integration is easy.\\n### Positive Consequences <!-- optional -->\\n* Fast time to develop\\n### Negative Consequences\\n* We need to think about setting up development environment\\n","Predictions":"Chosen option: \"Structured Objects\", because it is a scalable and well-supported framework for Object Oriented Design.n### Positive Consequences!-- optional -->n* Easy to usen### Negative Consequencesn* We need to maintain a toolchain for Object Oriented Design.n"}
{"File Name":"saas-plaform-tenant-identity-provider\/0002-use-a-hosted-identity-solution-over-a-self-hosted-one.md","Context":"## Context\nWe need an identity provider for our platform. This could be a host (SaaS) solution vs a self hosted solution. While the Cost & Lockin is higher at scale with a SaaS the self hosted has a higher Management Effort & Less Security & Expensive Bootstraping. From an evolutionary architecture perspective it is not core to value generation and does not change rapidly[source](https:\/\/www.youtube.com\/watch?v=8bEsNT7jdC4&t=112s&index=57&list=WL) - identity will mostlikly the same (OpenIDConnect, Login page ...) but it will be hard to change in the future if you decided for one provider.\n","Decision":"We will use a SaaS solution as we don't have the skill to host our own solution in the efficient quality. And from a evolutionary architecture perspective the system is commondity or support.\\n","tokens":142,"id":3421,"text":"## Context\\nWe need an identity provider for our platform. This could be a host (SaaS) solution vs a self hosted solution. While the Cost & Lockin is higher at scale with a SaaS the self hosted has a higher Management Effort & Less Security & Expensive Bootstraping. From an evolutionary architecture perspective it is not core to value generation and does not change rapidly[source](https:\/\/www.youtube.com\/watch?v=8bEsNT7jdC4&t=112s&index=57&list=WL) - identity will mostlikly the same (OpenIDConnect, Login page ...) but it will be hard to change in the future if you decided for one provider.\\n\n\n##Decision\nWe will use a SaaS solution as we don't have the skill to host our own solution in the efficient quality. And from a evolutionary architecture perspective the system is commondity or support.\\n","Predictions":"We will use a hosted service to host our own solution.n"}
{"File Name":"docspell\/0016_custom_fields.md","Context":"# Context and Problem Statement\nUsers want to add custom metadata to items. For example, for invoices\nfields like `invoice-number` or `total`\/`amount` make sense. When\nusing a pagination stamp, every item gets a pagination number.\nThis is currently not possible to realize in docspell. But it is an\nessential part when organizing and archiving documents. It should be\nsupported.\n# Considered Options\n","Decision":"- values are strings at the database\\n- values are strings when transported from\/to server\\n- client must provide the correct formatted strings per type\\n- numeric: some decimal number\\n- money: decimal number\\n- text: no restrictions\\n- date: a local date as iso string, e.g. `2011-10-09`\\n- bool: either `\"true\"` or `\"false\"`, case insensitive\\n","tokens":90,"id":285,"text":"# Context and Problem Statement\\nUsers want to add custom metadata to items. For example, for invoices\\nfields like `invoice-number` or `total`\/`amount` make sense. When\\nusing a pagination stamp, every item gets a pagination number.\\nThis is currently not possible to realize in docspell. But it is an\\nessential part when organizing and archiving documents. It should be\\nsupported.\\n# Considered Options\\n\n\n##Decision\n- values are strings at the database\\n- values are strings when transported from\/to server\\n- client must provide the correct formatted strings per type\\n- numeric: some decimal number\\n- money: decimal number\\n- text: no restrictions\\n- date: a local date as iso string, e.g. `2011-10-09`\\n- bool: either `\"true\"` or `\"false\"`, case insensitive\\n","Predictions":"* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* n* "}
{"File Name":"opg-lpa\/0002-custom-save-handler-in-service-front.md","Context":"## Context\nThe service-front component, written in PHP, uses the default Redis save\nhandler for persisting session data. In certain situations, the\napplication may request a resource *A* which takes significant time to deliver,\nsuch as LPA statuses via the Sirius data API. If resource *A*\nis requested via an Ajax request, it's possible that the client\nwill request a new resource *B* before *A* is fully processed. If processing for\n*B* then completes before processing for *A*, the process for *A* can erroneously\noverwrite session data added by *B*, resulting in loss of session data required\nby *A*.\nThis causes particular problems for CSRF tokens, as shown by this typical sequence\non service-front:\n1.  dashboard page loads in browser, triggering client-side Ajax request to statuses controller\n2.  statuses controller reads session data **S** and initiates (slow) request to Sirius\nAPI to get LPA statuses\n3.  meanwhile, user goes to replacement-attorney page; the Ajax request is now redundant, as the\nuser isn't on the dashboard page any more, but the statuses controller doesn't know this\n4.  replacement-attorney controller reads session data **S**\n5.  statuses controller continues processing Sirius response, unaware of new data about to be added to\nsession by replacement-attorney...\n6.  replacement-attorney adds CSRF data to session, creating **S'**\n7.  replacement-attorney page renders form with CSRF token, associated with data in **S'**\n8.  replacement-attorney writes **S'** to session, including CSRF data\n9.  statuses page finishes processing, unaware of **S'**; it assumes it has\nthe correct data **S** and writes it to the session, losing the delta between\n**S** and **S'** (including the CSRF token!)\n10. user submits form to replacement-attorney controller with CSRF token in the form\n11. replacement-attorney controller loads again, but retrieves **S** from session (just written by\nstatuses controller in 9); this doesn't have the CSRF token (which was in **S'**)\nfor comparison with the value in the form submitted by the user; CSRF validation fails!\n","Decision":"Use a custom save handler to prevent certain Ajax requests from writing data to the session.\\nThis will still use Redis as the storage back-end.\\nThe approach is to send some Ajax requests with a custom `X-SessionReadOnly: true` header,\\nimplying that the controller they invoke should only read from the session and never write to it.\\nThe save handler inspects the header on the incoming request and ignores any requests to write\\nthe session if accompanied by this header.\\nPHP 7+ provides a mechanism to only read from the session, via:\\n```\\nsession_start(array('read_and_close' => true))\\n```\\nHowever, the complexity of the processing in the Laminas stack, which does its own session\\nmanagement, overrides any attempts to call this function. Consequently, the pragmatic\\nsolution is to move down the stack to the lower-level save handler, and implement the read-only\\nbehaviour there for requests we know to be problematic.\\n","tokens":490,"id":1654,"text":"## Context\\nThe service-front component, written in PHP, uses the default Redis save\\nhandler for persisting session data. In certain situations, the\\napplication may request a resource *A* which takes significant time to deliver,\\nsuch as LPA statuses via the Sirius data API. If resource *A*\\nis requested via an Ajax request, it's possible that the client\\nwill request a new resource *B* before *A* is fully processed. If processing for\\n*B* then completes before processing for *A*, the process for *A* can erroneously\\noverwrite session data added by *B*, resulting in loss of session data required\\nby *A*.\\nThis causes particular problems for CSRF tokens, as shown by this typical sequence\\non service-front:\\n1.  dashboard page loads in browser, triggering client-side Ajax request to statuses controller\\n2.  statuses controller reads session data **S** and initiates (slow) request to Sirius\\nAPI to get LPA statuses\\n3.  meanwhile, user goes to replacement-attorney page; the Ajax request is now redundant, as the\\nuser isn't on the dashboard page any more, but the statuses controller doesn't know this\\n4.  replacement-attorney controller reads session data **S**\\n5.  statuses controller continues processing Sirius response, unaware of new data about to be added to\\nsession by replacement-attorney...\\n6.  replacement-attorney adds CSRF data to session, creating **S'**\\n7.  replacement-attorney page renders form with CSRF token, associated with data in **S'**\\n8.  replacement-attorney writes **S'** to session, including CSRF data\\n9.  statuses page finishes processing, unaware of **S'**; it assumes it has\\nthe correct data **S** and writes it to the session, losing the delta between\\n**S** and **S'** (including the CSRF token!)\\n10. user submits form to replacement-attorney controller with CSRF token in the form\\n11. replacement-attorney controller loads again, but retrieves **S** from session (just written by\\nstatuses controller in 9); this doesn't have the CSRF token (which was in **S'**)\\nfor comparison with the value in the form submitted by the user; CSRF validation fails!\\n\n\n##Decision\nUse a custom save handler to prevent certain Ajax requests from writing data to the session.\\nThis will still use Redis as the storage back-end.\\nThe approach is to send some Ajax requests with a custom `X-SessionReadOnly: true` header,\\nimplying that the controller they invoke should only read from the session and never write to it.\\nThe save handler inspects the header on the incoming request and ignores any requests to write\\nthe session if accompanied by this header.\\nPHP 7+ provides a mechanism to only read from the session, via:\\n```\\nsession_start(array('read_and_close' => true))\\n```\\nHowever, the complexity of the processing in the Laminas stack, which does its own session\\nmanagement, overrides any attempts to call this function. Consequently, the pragmatic\\nsolution is to move down the stack to the lower-level save handler, and implement the read-only\\nbehaviour there for requests we know to be problematic.\\n","Predictions":"1. Security:n1. Security:n2. Security:n3. Security:n4. Security:n5. Security:n6. Security:n7. Security:n8. Security:n9. Security:n9. Security:n10. Security:n11. Security:n12. Security:n13. Security:n14. Security:n15. Security:n16. Security:n17. Security:n18. Security:n19. Security:n20. Security:n21. Security:n22. Security:n23. Security:n25. Security:n26. Security:n27. Security:n28. Security:n29. Security:n30. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31. Security:n31."}
{"File Name":"pcmt\/adr-007.md","Context":"## Context\nPCMT wants to accomplish a number of goals through the use of versions:\n1. Convey to stakeholders the initial suitability of the project:\n* Pre-v1: In development for global stakeholders.\n* Version 1: Production ready for global stakeholders.\n* Version 2: Production ready for national stakeholders.\n1. Leverage semantic versioning to convey to dependents the nature of the\nrelease.  e.g. is it a patch-release, major new functionality, etc.\n1. Mark a specific state of the code-base, tied to a released asset.\n1. Indicate which version of Akeneo that PCMT is derived from.\n","Decision":"1. We will version PCMT by [Semantic Versioning 2.0][semver].  However since\\nDocker tags do not support the use of the `+` symbol, we'll use `-` in\\nits place.\\n1. We will have a build number (from the commit SHA), which is from a CI\\npipeline, that includes a distinct set of commits.\\n1. We will expire and remove old Build numbers, so they need to be pruned\\nregularly.\\n1. We will not publish assets if a pipeline fails, and conversely if a pipeline\\nis re-run, it consists of the same commits (state of the code), and therefore\\nit should have the same build number.\\n1. We will promote build numbers to a showcase or UAT server, manually.\\n1. We will promote build numbers to a release, manually.\\n1. We will keep the semantic version of the system separate from the build\\nnumber, in a file in the SCM.  When an asset is labeled from\\nthe CI pipeline, it'll use the full form by placing the build number in\\nthe build meta-data of the Semantic Version.  e.g. `1.0.0+48af4a30`\\n1. When promoting to a release we will co-label the published assets such that\\none asset will have the build number, and the released asset will not include\\nthat number. e.g. version 1.0.0 might be `image:v1.0.0+SHA` and\\n`image:v1.0.0`, which are equivalent assets.\\n","tokens":141,"id":2951,"text":"## Context\\nPCMT wants to accomplish a number of goals through the use of versions:\\n1. Convey to stakeholders the initial suitability of the project:\\n* Pre-v1: In development for global stakeholders.\\n* Version 1: Production ready for global stakeholders.\\n* Version 2: Production ready for national stakeholders.\\n1. Leverage semantic versioning to convey to dependents the nature of the\\nrelease.  e.g. is it a patch-release, major new functionality, etc.\\n1. Mark a specific state of the code-base, tied to a released asset.\\n1. Indicate which version of Akeneo that PCMT is derived from.\\n\n\n##Decision\n1. We will version PCMT by [Semantic Versioning 2.0][semver].  However since\\nDocker tags do not support the use of the `+` symbol, we'll use `-` in\\nits place.\\n1. We will have a build number (from the commit SHA), which is from a CI\\npipeline, that includes a distinct set of commits.\\n1. We will expire and remove old Build numbers, so they need to be pruned\\nregularly.\\n1. We will not publish assets if a pipeline fails, and conversely if a pipeline\\nis re-run, it consists of the same commits (state of the code), and therefore\\nit should have the same build number.\\n1. We will promote build numbers to a showcase or UAT server, manually.\\n1. We will promote build numbers to a release, manually.\\n1. We will keep the semantic version of the system separate from the build\\nnumber, in a file in the SCM.  When an asset is labeled from\\nthe CI pipeline, it'll use the full form by placing the build number in\\nthe build meta-data of the Semantic Version.  e.g. `1.0.0+48af4a30`\\n1. When promoting to a release we will co-label the published assets such that\\none asset will have the build number, and the released asset will not include\\nthat number. e.g. version 1.0.0 might be `image:v1.0.0+SHA` and\\n`image:v1.0.0`, which are equivalent assets.\\n","Predictions":"1. We will version PCMT by [Semantic Versioning 2.0][semver]. However sincenDocker tags do not support the use of the + symbol, we'll use - innits place.n1. We will have a build number (from the commit SHA), which is from a CInpipeline, that includes a distinct set of commits.n1. We will expire and remove old Build numbers, so they need to be prunednregularly.n1. We will promote build numbers to a showcase or UAT server, manually.n1. We will promote build numbers to a release, manually.n1. We will keep the semantic version of the system separate from the build number, in a file in the SCM. When an asset is labeled fromnthe CI pipeline, it'll use the full form by placing the build number innthe build meta-data of the Semantic Version. e.g. 1.0.0+48af4a30n1. When promoting to a release we will co-label the published assets such thatnone asset will have the build number, and the released asset will not includenthat number.n"}
{"File Name":"register-a-food-business-front-end\/0002-implement-open-source-govuk-react-components-library.md","Context":"## Context\nThe website needs to comply with the GDS design language. In the future, the standard GDS design language might need to be themed to match food.gov colours, fonts, etc.\n","Decision":"We will implement the open source govuk-react npm package hosted at [https:\/\/github.com\/penx\/govuk-react](https:\/\/github.com\/penx\/govuk-react) wherever it has a component that fits our needs.\\nWe will create custom components and layouts where necessary to follow the GDS design language.\\nWe will contribute back to the project to ensure it fulfils all of our needs.\\n","tokens":41,"id":3037,"text":"## Context\\nThe website needs to comply with the GDS design language. In the future, the standard GDS design language might need to be themed to match food.gov colours, fonts, etc.\\n\n\n##Decision\nWe will implement the open source govuk-react npm package hosted at [https:\/\/github.com\/penx\/govuk-react](https:\/\/github.com\/penx\/govuk-react) wherever it has a component that fits our needs.\\nWe will create custom components and layouts where necessary to follow the GDS design language.\\nWe will contribute back to the project to ensure it fulfils all of our needs.\\n","Predictions":"We will implement the open source govuk-react npm package hosted at [https:\/\/github.com\/penx\/govuk-react](https:\/\/github.com\/penx\/govuk-react) wherever it has a component that fits our needs.nWe will create custom components and layouts where necessary to follow the GDS design language.nWe will contribute back to the project to ensure it fulfils all of our needs.n"}
{"File Name":"documents-api\/0002-use-base64-content-for-file-uploading.md","Context":"## Context\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\n","Decision":"We decided to use the first option (base 64 encoded uploads), for the following reason:\\n- We do not want to expose any AWS links to outside parties. This ensures that all access to AWS resources is controlled by Hackney authentication mechanisms.\\n","tokens":113,"id":3987,"text":"## Context\\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\\n\n\n##Decision\nWe decided to use the first option (base 64 encoded uploads), for the following reason:\\n- We do not want to expose any AWS links to outside parties. This ensures that all access to AWS resources is controlled by Hackney authentication mechanisms.\\n","Predictions":"We will use the first option (base 64 encoded uploads), for the following reason:n- We do not want to expose any AWS links to outside parties. This ensures that all access to AWS resources is controlled by Hackney authentication mechanisms.n"}
{"File Name":"front-end-monorepo\/adr-15.md","Context":"## Context\nThe way the drawing tools currently function on Panoptes-Front-End (PFE) have numerous issues including:\n- Updating the classification annotation on each touch or pointer event which causes unnecessary re-rendering of the DOM\n- The separation concerns are not clear between components and stores. Multiple components can update the annotation making it hard to debug or add new features to.\n- Example: The `MarkingsRenderer` and the `FrameAnnotator` both call change handlers that update the classification annotation? Can the drawing annotation be updated by both or is one solely responsible? It is unclear by reading the code. Why does something named `MarkingsRenderer` update the annotation?\n- Drawing tools have a complex API that involves exposing static methods to be called by their parent component\n- Annotation \/ classification payloads have no consistent standards for describing data: some tools mark rotation in differing directions, for example.\n","Decision":"What we do not want to do:\\n- Re-render on every pointer or touch event.\\n- update annotation state while drawing is in progress.\\n- support more than one drawing task in a step.\\n- Use D3.js since it has its own internal data store and it would be complicated to integrate that with a observable stream.\\nWhat we do want to do:\\n- Have a component, the interactive layer, that manages the interaction with the marks and pointer and touch events.\\n- The interactive layer should not allow events to bubble so the events are encapsulated to just the interaction with the subject. This is to help prevent browser scroll during drawing. An attempted fix on PFE for reference: zooniverse\/Panoptes-Front-End#5411\\n- Events will be observed and be streamed via an observable. We will use rx.js to create an observer\/observable event stream.\\n- The last state of the event stream will be used to add an annotation to the classification when next or done is clicked.\\n- The interactive layer will have local component state to keep track of which mark is selected for editing as well as handle the edit and delete events.\\n- Have a component, the markings renderer, that manages the rendering of previous annotation marks as read only. It will hide them if hide previous marks is toggled.\\n- These two components will initially use separate SVG layers that connect to the same stores. Later any duplicated code will be refactored into a higher order component.\\n- Have a multi-image subject viewer. We will have to support projects that want each frame to have independent drawing annotations as well as projects that want each frame to have the same drawing annotations. Each frame should have the same pan\/zoom function. We have two proposed options in implementation:\\n-  Create an interactive layer and markings renderer for each frame. Each interactive layer will initialize its own event stream.\\n- Create a single interactive layer and markings renderer and filter what is rendered by the frame index.\\n- Projects have requested each frame to have the same pan\/zoom function, but we were unable to implement in PFE: zooniverse\/Panoptes-Front-End#3465\\n- Are there any cases where projects want separate pan\/zoom function for each frame?\\n- Have a schema, or set of schemas, describing annotations.\\n","tokens":187,"id":528,"text":"## Context\\nThe way the drawing tools currently function on Panoptes-Front-End (PFE) have numerous issues including:\\n- Updating the classification annotation on each touch or pointer event which causes unnecessary re-rendering of the DOM\\n- The separation concerns are not clear between components and stores. Multiple components can update the annotation making it hard to debug or add new features to.\\n- Example: The `MarkingsRenderer` and the `FrameAnnotator` both call change handlers that update the classification annotation? Can the drawing annotation be updated by both or is one solely responsible? It is unclear by reading the code. Why does something named `MarkingsRenderer` update the annotation?\\n- Drawing tools have a complex API that involves exposing static methods to be called by their parent component\\n- Annotation \/ classification payloads have no consistent standards for describing data: some tools mark rotation in differing directions, for example.\\n\n\n##Decision\nWhat we do not want to do:\\n- Re-render on every pointer or touch event.\\n- update annotation state while drawing is in progress.\\n- support more than one drawing task in a step.\\n- Use D3.js since it has its own internal data store and it would be complicated to integrate that with a observable stream.\\nWhat we do want to do:\\n- Have a component, the interactive layer, that manages the interaction with the marks and pointer and touch events.\\n- The interactive layer should not allow events to bubble so the events are encapsulated to just the interaction with the subject. This is to help prevent browser scroll during drawing. An attempted fix on PFE for reference: zooniverse\/Panoptes-Front-End#5411\\n- Events will be observed and be streamed via an observable. We will use rx.js to create an observer\/observable event stream.\\n- The last state of the event stream will be used to add an annotation to the classification when next or done is clicked.\\n- The interactive layer will have local component state to keep track of which mark is selected for editing as well as handle the edit and delete events.\\n- Have a component, the markings renderer, that manages the rendering of previous annotation marks as read only. It will hide them if hide previous marks is toggled.\\n- These two components will initially use separate SVG layers that connect to the same stores. Later any duplicated code will be refactored into a higher order component.\\n- Have a multi-image subject viewer. We will have to support projects that want each frame to have independent drawing annotations as well as projects that want each frame to have the same drawing annotations. Each frame should have the same pan\/zoom function. We have two proposed options in implementation:\\n-  Create an interactive layer and markings renderer for each frame. Each interactive layer will initialize its own event stream.\\n- Create a single interactive layer and markings renderer and filter what is rendered by the frame index.\\n- Projects have requested each frame to have the same pan\/zoom function, but we were unable to implement in PFE: zooniverse\/Panoptes-Front-End#3465\\n- Are there any cases where projects want separate pan\/zoom function for each frame?\\n- Have a schema, or set of schemas, describing annotations.\\n","Predictions":"We will use a Separate component to handle the re-rendering of the DOM.n"}
{"File Name":"dos-capacity-status-api\/002-shared_database.md","Context":"## Context\nThis is not an external API to DoS, it is part of the DoS ecosystem. The operation of the API is updating the capacity of services in DoS, for visible in the DoS UI and applications that use Core DoS data.\n","Decision":"The relevant content of the database is shared between the Core DoS application and the Capacity Status API. The decision was made to have the API use and update the Core DoS database. This approach is the most logical solution at this time, as any other alternative would likely need to incorporate some kind of interim internal API between the Capacity Status API and the Core DoS database. Having an interim API would be replicating functionality of the Capacity Status API and would therefore be redundant.\\n","tokens":54,"id":3937,"text":"## Context\\nThis is not an external API to DoS, it is part of the DoS ecosystem. The operation of the API is updating the capacity of services in DoS, for visible in the DoS UI and applications that use Core DoS data.\\n\n\n##Decision\nThe relevant content of the database is shared between the Core DoS application and the Capacity Status API. The decision was made to have the API use and update the Core DoS database. This approach is the most logical solution at this time, as any other alternative would likely need to incorporate some kind of interim internal API between the Capacity Status API and the Core DoS database. Having an interim API would be replicating functionality of the Capacity Status API and would therefore be redundant.\\n","Predictions":"The relevant content of the database is shared between the Core DoS application and the Capacity Status API. The decision was made to have the API use and update the Core DoS database. This approach is the most logical solution at this time, as any other alternative would likely need to incorporate some kind of interim internal API between the Capacity Status API and the Core DoS database. Having an interim API would be replicating functionality of the Capacity Status API and would therefore be redundant.n"}
{"File Name":"dapr\/CLI-001-cli-and-runtime-versioning.md","Context":"## Context\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\n","Decision":"* We'll keep CLI versioning and runtime versioning separate.\\n* CLI will pull down latest runtime binary during the *init()* command.\\n* Version scheme is: *major.minor.revision.build* for both CLI and runtime.\\n","tokens":28,"id":78,"text":"## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n\n\n##Decision\n* We'll keep CLI versioning and runtime versioning separate.\\n* CLI will pull down latest runtime binary during the *init()* command.\\n* Version scheme is: *major.minor.revision.build* for both CLI and runtime.\\n","Predictions":"* We'll keep CLI versioning and runtime versioning separate.n* CLI will pull down latest runtime binary during the *init()* command.n* Version scheme is: *major.minor.revision.build* for both CLI and runtime.n"}
{"File Name":"aws_infrastructure\/0005-use-terraform-as-infrastructure-as-code.md","Context":"## Context\nNeed of using Infrastructure as code to, track what resources are running in the AWS account\n","Decision":"Use Terraform\\n","tokens":21,"id":1464,"text":"## Context\\nNeed of using Infrastructure as code to, track what resources are running in the AWS account\\n\n\n##Decision\nUse Terraform\\n","Predictions":"We will use a Route method to map the data to the desired path.n"}
{"File Name":"docs\/0018-requirements-application-composition.md","Context":"## Context and Problem Statement\nWe want to have clear and simple requirements when it comes to the way applications can be created in the user interface.\n## Decision Drivers\n* MUST be compatible with Lombok\n","Decision":"* MUST be compatible with Lombok\\nChosen option: the first option, since we want a simple solution, in order to have the system running as soon as possible.\\n### Positive Consequences\\n* Lombok can be used.\\n* Better code quality.\\n### Negative consequences\\n* Applications cannot be created using other existing applications.\\n","tokens":42,"id":4695,"text":"## Context and Problem Statement\\nWe want to have clear and simple requirements when it comes to the way applications can be created in the user interface.\\n## Decision Drivers\\n* MUST be compatible with Lombok\\n\n\n##Decision\n* MUST be compatible with Lombok\\nChosen option: the first option, since we want a simple solution, in order to have the system running as soon as possible.\\n### Positive Consequences\\n* Lombok can be used.\\n* Better code quality.\\n### Negative consequences\\n* Applications cannot be created using other existing applications.\\n","Predictions":"* MUST be compatible with LomboknChosen option: the first option, since we want a simple solution, in order to have the system running as soon as possible.n### Positive Consequencesn* Lombok can be used.n* Better code quality.n### Negative consequencesn* Applications cannot be created using other existing applications.n"}
{"File Name":"james\/0005-distributed-task-termination-ackowledgement.md","Context":"## Context\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\n","Decision":"* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\\n","tokens":54,"id":2121,"text":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\\n\n\n##Decision\n* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\\n","Predictions":"* Creating a RabbitMQEventHandler which publish Events pushed to the task manager's event system to RabbitMQn* All the events which end a Task (Completed, Failed, and Canceled) have to be transmitted to other nodesn"}
{"File Name":"klokwrk-project\/0003-cqrs-and-event-sourcing-for-applications.md","Context":"## Context\nWhen building monoliths or microservices that contain non-trivial business logic, we want to have a high-level component architecture pattern that natively leverages a strategic Domain Driven\nDesign (DDD) concepts like ubiquitous language and bounded context. It should also support or allow, at least, the usage of application design architecture patterns like hexagonal architecture.\nAt the lower technical level, component architecture pattern should provide native support for tactical DDD concepts like aggregates, entities, value objects, and events.\nWhen building microservices, the chosen component architecture pattern should natively support asynchronous communication through events.\nIt would be ideal to have all of these in a single coherent open-source framework and platform.\nFor anemic CRUD style applications, more traditional approaches, like layered architecture, would be just fine.\n### Architectural Context\n* System (`klokwrk-project`)\n","Decision":"**We will use the Axon framework and Axon Server as a mature open-source implementation of the CQRS\/ES component architecture pattern.**\\nEspecially as one of the major `klokwrk-project` goals is to demonstrate the creation of complex CQRS\/ES microservice applications. We can use more traditional approaches like classic layered\\narchitecture and anemic model for anemic CRUD style applications.\\n","tokens":182,"id":4896,"text":"## Context\\nWhen building monoliths or microservices that contain non-trivial business logic, we want to have a high-level component architecture pattern that natively leverages a strategic Domain Driven\\nDesign (DDD) concepts like ubiquitous language and bounded context. It should also support or allow, at least, the usage of application design architecture patterns like hexagonal architecture.\\nAt the lower technical level, component architecture pattern should provide native support for tactical DDD concepts like aggregates, entities, value objects, and events.\\nWhen building microservices, the chosen component architecture pattern should natively support asynchronous communication through events.\\nIt would be ideal to have all of these in a single coherent open-source framework and platform.\\nFor anemic CRUD style applications, more traditional approaches, like layered architecture, would be just fine.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n\n##Decision\n**We will use the Axon framework and Axon Server as a mature open-source implementation of the CQRS\/ES component architecture pattern.**\\nEspecially as one of the major `klokwrk-project` goals is to demonstrate the creation of complex CQRS\/ES microservice applications. We can use more traditional approaches like classic layered\\narchitecture and anemic model for anemic CRUD style applications.\\n","Predictions":"* System (klokwrk-project)n* **We will use the Axon framework and Axon Server as a mature open-source implementation of the CQRS\/ES component architecture pattern.**n* Especially as one of the major klokwrk-project goals is to demonstrate the creation of complex CQRS\/ES microservice applications. We can use more traditional approaches like classic layerednarchitecture and anemic model for anemic CRUD style applications.n### StatusnAcceptedn### ConsequencesnIt will be easier to find solutions to coding problems online.n"}
{"File Name":"lockfix\/0001-documenting-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":611,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"hmpps-interventions-docs\/0005-for-release-prioritise-consistency-over-availability.md","Context":"## Context\n1. We have a fixed deadline\n1. We will have our first users week beginning 7 June\u00a02021\n1. For our Delius integration, we rely on an intricate set of reference data\n1. We are uncertain if there are any issues with the reference data for all possible combinations of offenders\n1. We rely on these integrations to set up \"container\" data in nDelius. If they fail, we need to recover before providers book appointments\nWe want to ensure when updates fail, we have a way to resume that retries the interaction.\nHowever, **we do not have retry mechanisms in place yet**.\nWe could add an admin task\/script to retry a certain event (we use Spring Application Events that don't\ncreate side effects, so we could retrigger those).\nWe feel this would create an overhead that would be too much administrative burden together with\nthe anticipated noise of most users starting the service at the same time.\n","Decision":"Due to the uncertainties and lack of convenient retry mechanism,\\nwe favour **consistency over availabilty** in the short term.\\n","tokens":201,"id":1799,"text":"## Context\\n1. We have a fixed deadline\\n1. We will have our first users week beginning 7 June\u00a02021\\n1. For our Delius integration, we rely on an intricate set of reference data\\n1. We are uncertain if there are any issues with the reference data for all possible combinations of offenders\\n1. We rely on these integrations to set up \"container\" data in nDelius. If they fail, we need to recover before providers book appointments\\nWe want to ensure when updates fail, we have a way to resume that retries the interaction.\\nHowever, **we do not have retry mechanisms in place yet**.\\nWe could add an admin task\/script to retry a certain event (we use Spring Application Events that don't\\ncreate side effects, so we could retrigger those).\\nWe feel this would create an overhead that would be too much administrative burden together with\\nthe anticipated noise of most users starting the service at the same time.\\n\n\n##Decision\nDue to the uncertainties and lack of convenient retry mechanism,\\nwe favour **consistency over availabilty** in the short term.\\n","Predictions":"We will use a stack-aws-foo-stack file to deploy to the kubernetes clusters.n"}
{"File Name":"uniffi-rs\/0004-only-threadsafe-interfaces.md","Context":"## Context and Problem Statement\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\ninterfaces\" - possibly leading to the impression that there is such a thing as\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\nthe only question is who manages this thread-safety. Interfaces which are not\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\nhidden in the generated code and therefore not obvious to the casual reader.\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\nmanaging their own locking internally. This ADR proposes that uniffi forces\ncomponent authors to explicitly manage that locking in all cases - or to put\nthis in Rust terms, that all structs supported by uniffi must already be\n`Send+Sync`\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\nany misunderstandings that might be caused by using the somewhat broad and\ngeneric \"Threadsafe\".\n## Decision Drivers\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\nhaving method calls unexpectedly block for long periods, such as\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\n* Supporting such structs will hinder uniffi growing in directions that we've\nfound are desired in practice, such as allowing structs to use [alternative\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\n","Decision":"* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\nChosen option:\\n* **[Option 2] Immediately deprecate, then remove entirely, support for\\nnon-`Send+Sync` interfaces.**\\nThis decision was taken because our real world experience tells us that\\nnon-`Send+Sync` interfaces are only useful in toy or example applications (eg,\\nthe nimbus and autofill projects didn't get very far before needing these\\ncapabilities), so the extra ongoing work in supporting these interfaces cannot\\nbe justified.\\n### Positive Consequences\\n* The locking in all uniffi supported components will be more easily\\ndiscoverable - it will be in hand-written rust code and not hidden inside\\ngenerated code. This is a benefit to the developers of the uniffi supported\\ncomponent rather than to the consumers of it; while we are considering other\\nfeatures to help communicate the lock semantics to such consumers, that is\\nbeyond the scope of this ADR.\\n* Opens the door to enhancements that would be impossible for non-`Send+Sync`\\ninterfaces, and simpler to implement for `Send+Sync` interfaces if support\\nfor non-`Send+Sync` interfaces did not exist.\\n* Simpler implementation and documentation.\\n### Negative Consequences\\n* All consumers (both inside Mozilla and external) will need to change their\\ninterfaces to be `Send+Sync`. As an example of what this entails,\\nsee [this commit](https:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985)\\nwhich converts the `todolist` example.\\n* Simple, toy applications may be more difficult to wrap - consumers will not\\nbe able to defer decisions about `Send+Sync` support and will instead need to\\nimplement simple locking as demonstrated in [this commit](\\nhttps:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985).\\n* Existing applications that are yet to consider how to make their\\nimplementations `Send+Sync` cannot be wrapped until they have.\\n* The examples which aren't currently marked with the `[Threadsafe]` attribute\\nwill become more complex as they will all need to implement and explain how\\nthey achieve being `Send+Sync`.\\n* The perception that its more difficult to wrap interfaces will lead to less\\nadoption of the tool.\\n","tokens":468,"id":4952,"text":"## Context and Problem Statement\\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\\ninterfaces\" - possibly leading to the impression that there is such a thing as\\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\\nthe only question is who manages this thread-safety. Interfaces which are not\\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\\nhidden in the generated code and therefore not obvious to the casual reader.\\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\\nmanaging their own locking internally. This ADR proposes that uniffi forces\\ncomponent authors to explicitly manage that locking in all cases - or to put\\nthis in Rust terms, that all structs supported by uniffi must already be\\n`Send+Sync`\\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\\nany misunderstandings that might be caused by using the somewhat broad and\\ngeneric \"Threadsafe\".\\n## Decision Drivers\\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\n\n\n##Decision\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https:\/\/github.com\/mozilla-mobile\/fenix\/issues\/17086)\\n(with more details available in [this JIRA ticket](https:\/\/jira.mozilla.com\/browse\/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/417) or to\\n[pass interface references over the FFI](https:\/\/github.com\/mozilla\/uniffi-rs\/issues\/419).\\nChosen option:\\n* **[Option 2] Immediately deprecate, then remove entirely, support for\\nnon-`Send+Sync` interfaces.**\\nThis decision was taken because our real world experience tells us that\\nnon-`Send+Sync` interfaces are only useful in toy or example applications (eg,\\nthe nimbus and autofill projects didn't get very far before needing these\\ncapabilities), so the extra ongoing work in supporting these interfaces cannot\\nbe justified.\\n### Positive Consequences\\n* The locking in all uniffi supported components will be more easily\\ndiscoverable - it will be in hand-written rust code and not hidden inside\\ngenerated code. This is a benefit to the developers of the uniffi supported\\ncomponent rather than to the consumers of it; while we are considering other\\nfeatures to help communicate the lock semantics to such consumers, that is\\nbeyond the scope of this ADR.\\n* Opens the door to enhancements that would be impossible for non-`Send+Sync`\\ninterfaces, and simpler to implement for `Send+Sync` interfaces if support\\nfor non-`Send+Sync` interfaces did not exist.\\n* Simpler implementation and documentation.\\n### Negative Consequences\\n* All consumers (both inside Mozilla and external) will need to change their\\ninterfaces to be `Send+Sync`. As an example of what this entails,\\nsee [this commit](https:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985)\\nwhich converts the `todolist` example.\\n* Simple, toy applications may be more difficult to wrap - consumers will not\\nbe able to defer decisions about `Send+Sync` support and will instead need to\\nimplement simple locking as demonstrated in [this commit](\\nhttps:\/\/github.com\/mozilla\/uniffi-rs\/commit\/454dfff6aa560dffad980a9258853108a44d5985).\\n* Existing applications that are yet to consider how to make their\\nimplementations `Send+Sync` cannot be wrapped until they have.\\n* The examples which aren't currently marked with the `[Threadsafe]` attribute\\nwill become more complex as they will all need to implement and explain how\\nthey achieve being `Send+Sync`.\\n* The perception that its more difficult to wrap interfaces will lead to less\\nadoption of the tool.\\n","Predictions":"* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+Sync structs will be refactored to be Send+Sync.n* The Send+"}
{"File Name":"html-diff-service\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3250,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"saas-platform-frontend\/0005-use-storybook-to-build-self-contained-components-but-not-for-uis.md","Context":"## Context\nWe need a workflow to build our appliation and components.\n","Decision":"We use Storybook only for building new self contained components.\\n","tokens":16,"id":3722,"text":"## Context\\nWe need a workflow to build our appliation and components.\\n\n\n##Decision\nWe use Storybook only for building new self contained components.\\n","Predictions":"We use Storybook only for building new self contained components.n"}
{"File Name":"branchout\/0002-language.md","Context":"## Context\nA language should be universal, simple and easily testable\nOptions\n* Shell\n* Go\n* Java\n* JavaScript\nThere should be very few dependencies\n","Decision":"Shell\\n* No dependencies\\n* Installed pretty much everywhere developers are\\n","tokens":36,"id":1472,"text":"## Context\\nA language should be universal, simple and easily testable\\nOptions\\n* Shell\\n* Go\\n* Java\\n* JavaScript\\nThere should be very few dependencies\\n\n\n##Decision\nShell\\n* No dependencies\\n* Installed pretty much everywhere developers are\\n","Predictions":"* use PythonnChosen option: \"[option 1]\", because it is well known in thenindustry. It is not the most performant but it has good performancento serve WSGI app like Flask thanks to the uwsg.n"}
{"File Name":"evidence-api\/0002-create-two-platform-apis-instead-of-a-single-api.md","Context":"## Context\nThis application was created to allow services to manage gathered evidence documents in a consistent, safe and centralised way. An early decision point was reached with regards to designing the architecture around this capability.\nThe basic functionalities known to be required are:\n- secure storage and management of documents provided by residents and other third parties\n- tracking of rights and retention over those documents by the council\n- requests and approval of evidence from third parties\n- management of evidence requests by officers\nInfluencing factors:\n- HackIT's [API Playbook](https:\/\/github.com\/LBHackney-IT\/API-Playbook-v2-beta)\n- [Clean Architecture](https:\/\/github.com\/madetech\/clean-architecture) principles\n","Decision":"We decided to create two Platform APIs (as defined in the API Playbook):\\n- A [Documents API](https:\/\/github.com\/LBHackney-IT\/documents-api) for the storage and management of documents and claims over them\\n- An [Evidence API](https:\/\/github.com\/LBHackney-IT\/evidence-api) for the management of evidence requests and reviews\\n","tokens":151,"id":1480,"text":"## Context\\nThis application was created to allow services to manage gathered evidence documents in a consistent, safe and centralised way. An early decision point was reached with regards to designing the architecture around this capability.\\nThe basic functionalities known to be required are:\\n- secure storage and management of documents provided by residents and other third parties\\n- tracking of rights and retention over those documents by the council\\n- requests and approval of evidence from third parties\\n- management of evidence requests by officers\\nInfluencing factors:\\n- HackIT's [API Playbook](https:\/\/github.com\/LBHackney-IT\/API-Playbook-v2-beta)\\n- [Clean Architecture](https:\/\/github.com\/madetech\/clean-architecture) principles\\n\n\n##Decision\nWe decided to create two Platform APIs (as defined in the API Playbook):\\n- A [Documents API](https:\/\/github.com\/LBHackney-IT\/documents-api) for the storage and management of documents and claims over them\\n- An [Evidence API](https:\/\/github.com\/LBHackney-IT\/evidence-api) for the management of evidence requests and reviews\\n","Predictions":"We decided to create two Platform APIs (as defined in the API Playbook):n- A [Documents API](https:\/\/github.com\/LBHackney-IT\/documents-api) for the storage and management of documents and claims over themn- An [Evidence API](https:\/\/github.com\/LBHackney-IT\/evidence-api) for the management of evidence requests and reviewsn"}
{"File Name":"gsp\/ADR025-ingress.md","Context":"## Context\nWe currently have two [ingress][Ingress] systems:\n* Istio (see [ADR019])\n* nginx-ingress (see the old Ingress [ADR005])\nIstio's [Virtual Service] records are essentially advanced `Ingress` records.\nDo we need both?\n","Decision":"No. We will use an [Istio Ingress Gateway](https:\/\/istio.io\/docs\/tasks\/traffic-management\/ingress\/ingress-control\/)\\n","tokens":69,"id":3905,"text":"## Context\\nWe currently have two [ingress][Ingress] systems:\\n* Istio (see [ADR019])\\n* nginx-ingress (see the old Ingress [ADR005])\\nIstio's [Virtual Service] records are essentially advanced `Ingress` records.\\nDo we need both?\\n\n\n##Decision\nNo. We will use an [Istio Ingress Gateway](https:\/\/istio.io\/docs\/tasks\/traffic-management\/ingress\/ingress-control\/)\\n","Predictions":"We will use [Security Objects](https:\/\/github.com\/SecurityObjects) to store the data.n"}
{"File Name":"akvo-product-design\/ADR-004.md","Context":"## Context\nAs we introduce new and more complex response types, we need to come up with a consistent, homogeneous solution for serializing all the values.\nThe historical solution for this issue has been to separate multiple values with pipes, and in some scenarios, creating a JSON-encoded response (i.e. geoshapes).\nWe currently need to add a new `code` property to some responses, namely to cascade and option types, but we also want to avoid introducing new properties as the response content grows.\n","Decision":"As response values are getting more and more complex, there's an increasing need for finding a common solution for all values, and adding new properties will not scale well.\\nThe decision is to extend the JSON-based schema, and encode complex answers in this packed string.\\nWe will also review the meaning of each *response type*, which will determine the encoding of each value (JSON? string? array? etc).\\nMultiple responses (barcodes, option questions, cascades, etc) will be encoded using a **JSON array**.\\n","tokens":103,"id":2802,"text":"## Context\\nAs we introduce new and more complex response types, we need to come up with a consistent, homogeneous solution for serializing all the values.\\nThe historical solution for this issue has been to separate multiple values with pipes, and in some scenarios, creating a JSON-encoded response (i.e. geoshapes).\\nWe currently need to add a new `code` property to some responses, namely to cascade and option types, but we also want to avoid introducing new properties as the response content grows.\\n\n\n##Decision\nAs response values are getting more and more complex, there's an increasing need for finding a common solution for all values, and adding new properties will not scale well.\\nThe decision is to extend the JSON-based schema, and encode complex answers in this packed string.\\nWe will also review the meaning of each *response type*, which will determine the encoding of each value (JSON? string? array? etc).\\nMultiple responses (barcodes, option questions, cascades, etc) will be encoded using a **JSON array**.\\n","Predictions":"* We will use a [semantic semantic](https:\/\/github.com\/klokwrk\/semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic-semantic"}
{"File Name":"toc-poc\/1577574698834_architecture_choice_for_template_generation.md","Context":"## Context\nThe POC assumes generation of the table of contents based on the content presented on the page. Data need to be fetched from somewhere, and then presented on the view in some way. Data model won't be simple data structure, as it can be multi-leveled list, which might also contain some behavior, to make it easier to prepare for rendering. It's not going to be simple CRUD application, even if model will not contain much behavior.\n","Decision":"Use ports and adapters architecture style.\\n","tokens":94,"id":1883,"text":"## Context\\nThe POC assumes generation of the table of contents based on the content presented on the page. Data need to be fetched from somewhere, and then presented on the view in some way. Data model won't be simple data structure, as it can be multi-leveled list, which might also contain some behavior, to make it easier to prepare for rendering. It's not going to be simple CRUD application, even if model will not contain much behavior.\\n\n\n##Decision\nUse ports and adapters architecture style.\\n","Predictions":"We will use stack to handle the stack and stack implementations.n"}
{"File Name":"digitalrig-metal-aws\/0007-build-and-deploy-pipelines-separate.md","Context":"## Context\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\n","Decision":"Pipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","tokens":56,"id":1739,"text":"## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n\n\n##Decision\nPipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n","Predictions":"Pipeline has been split into distinct build and deploy pipelines. The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip. Src.zip is required still required in the deploy pipeline to run integration tests. In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline. Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.n"}
{"File Name":"report-a-defect\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5220,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"platform\/2021-12-07-admin-extension-api-standards.md","Context":"## Context\nWe need to provide ways for extension developers to add custom components and views to different places in the administration. Multiple solutions where discussed and tested, this ADR contains a summary of the final solution.\n","Decision":"### Word definitions\\nFor a better understanding of the following text it is good to have a definition for specific words:\\n#### Location\\nExtensions can render custom views with the Admin-Extension-API via iFrames. To support multiple views in different places every \"location\" of the iFrame gets a unique ID. These can be defined by the app\/plugin developer itself.\\n*Example:*\\nAn app wants to render a custom iFrame in a card on the dashboard. The \"location\" of the iFrame has then a specific \"locationID\" like `sw-dashboard-example-app-dashboard-card`. The app can also render another iFrames which also get \"locationIDs\". In our example it is a iFrame in a custom modal: `example-app-example-modal-content`.\\nThe app want to render different views depending on the \"location\" of the iFrame. So the app developer can render the correct view depending on the \"locationID\":\\n```js\\nif (sw.location.is('sw-dashboard-example-app-dashboard-card')) {\\nrenderDashboardCard();\\n}\\nif (sw.location.is('example-app-example-modal-content')) {\\nrenderModalContent();\\n}\\n```\\n#### PositionID (PositionIdentifier)\\nDevelopers can extend existing areas or create new areas in the administration with the Admin-Extension-API. To identify the positions which the developer want to extend we need a unique ID for every position. We call these IDs \"positionID\".\\n*Example:*\\nAn app wants to add a new tab item to a tab-bar. In the administration are many tab-bars available. So the developer needs to choose the correct \"positionID\" to determine which tab-bar should be extended. In this example the developer adds a new tab item to the tab-bar in the product detail page.\\n```js\\nsw.ui.tabs('sw-product-detail').addTabItem({ ... })\\n```\\n### Solution:\\nWe use the concept of component sections for providing injection points for extension components.\\n#### Component Sections\\nIn most cases developers will directly use the extension capabilities of the UI components (e.g. adding tab items, adding button to grid, ...). This will cover most needs of many extensions.\\nTo give them more flexibility we introduce a feature named \"Component Sections\". These are sections where any extension developer can inject components. These components are prebuilt and they can also contain custom render views with iFrames. The developer needs to use the feature and choose the matching positionID for the component position.\\n```js\\n\/\/ Adding a card before the manufacturer card with custom fields entries.\\nsw.ui.componentSection('sw-manufacturer-card-custom-fields__before').add({\\n\/\/ The Extension-API provides different components out of the box\\ncomponent: 'card',\\n\/\/ Props are depending on the type of component\\nprops: {\\ntitle: 'This is the title',\\nsubtitle: 'I am the subtitle',\\n\/\/ Some components can render a custom view. In this case the extension can render custom content in the card.\\nlocationId: 'example-app-card-before-manufactuer-custom-fields-card'\\n}\\n})\\n```\\n#### Vue Devtools Plugin for finding the PositionIDs\\nIt is impossible to create a list of all potential position IDs. And they would be hard to manage. To solve this problem we are writing a custom plugin for the Vue Devtools. This plugin will be available for Vue Devtools 6+. It makes identifying the position IDs very easy.\\nJust open the plugin in the Devtools (It is available directly when you open the Administration). Then you can see all positions at the current administration view which are available for extending. If you click at one position ID you get more information about it. Like the property in the Meteor-Extension-SDK so that you directly know what functionality this position has.\\nIn summary: the Devtool plugin provides a visual way to see which parts can be extended and what are the positionIDs for the extension position.\\n","tokens":43,"id":4507,"text":"## Context\\nWe need to provide ways for extension developers to add custom components and views to different places in the administration. Multiple solutions where discussed and tested, this ADR contains a summary of the final solution.\\n\n\n##Decision\n### Word definitions\\nFor a better understanding of the following text it is good to have a definition for specific words:\\n#### Location\\nExtensions can render custom views with the Admin-Extension-API via iFrames. To support multiple views in different places every \"location\" of the iFrame gets a unique ID. These can be defined by the app\/plugin developer itself.\\n*Example:*\\nAn app wants to render a custom iFrame in a card on the dashboard. The \"location\" of the iFrame has then a specific \"locationID\" like `sw-dashboard-example-app-dashboard-card`. The app can also render another iFrames which also get \"locationIDs\". In our example it is a iFrame in a custom modal: `example-app-example-modal-content`.\\nThe app want to render different views depending on the \"location\" of the iFrame. So the app developer can render the correct view depending on the \"locationID\":\\n```js\\nif (sw.location.is('sw-dashboard-example-app-dashboard-card')) {\\nrenderDashboardCard();\\n}\\nif (sw.location.is('example-app-example-modal-content')) {\\nrenderModalContent();\\n}\\n```\\n#### PositionID (PositionIdentifier)\\nDevelopers can extend existing areas or create new areas in the administration with the Admin-Extension-API. To identify the positions which the developer want to extend we need a unique ID for every position. We call these IDs \"positionID\".\\n*Example:*\\nAn app wants to add a new tab item to a tab-bar. In the administration are many tab-bars available. So the developer needs to choose the correct \"positionID\" to determine which tab-bar should be extended. In this example the developer adds a new tab item to the tab-bar in the product detail page.\\n```js\\nsw.ui.tabs('sw-product-detail').addTabItem({ ... })\\n```\\n### Solution:\\nWe use the concept of component sections for providing injection points for extension components.\\n#### Component Sections\\nIn most cases developers will directly use the extension capabilities of the UI components (e.g. adding tab items, adding button to grid, ...). This will cover most needs of many extensions.\\nTo give them more flexibility we introduce a feature named \"Component Sections\". These are sections where any extension developer can inject components. These components are prebuilt and they can also contain custom render views with iFrames. The developer needs to use the feature and choose the matching positionID for the component position.\\n```js\\n\/\/ Adding a card before the manufacturer card with custom fields entries.\\nsw.ui.componentSection('sw-manufacturer-card-custom-fields__before').add({\\n\/\/ The Extension-API provides different components out of the box\\ncomponent: 'card',\\n\/\/ Props are depending on the type of component\\nprops: {\\ntitle: 'This is the title',\\nsubtitle: 'I am the subtitle',\\n\/\/ Some components can render a custom view. In this case the extension can render custom content in the card.\\nlocationId: 'example-app-card-before-manufactuer-custom-fields-card'\\n}\\n})\\n```\\n#### Vue Devtools Plugin for finding the PositionIDs\\nIt is impossible to create a list of all potential position IDs. And they would be hard to manage. To solve this problem we are writing a custom plugin for the Vue Devtools. This plugin will be available for Vue Devtools 6+. It makes identifying the position IDs very easy.\\nJust open the plugin in the Devtools (It is available directly when you open the Administration). Then you can see all positions at the current administration view which are available for extending. If you click at one position ID you get more information about it. Like the property in the Meteor-Extension-SDK so that you directly know what functionality this position has.\\nIn summary: the Devtool plugin provides a visual way to see which parts can be extended and what are the positionIDs for the extension position.\\n","Predictions":"We will use the stack-id method to create a new stack-id for each field in the stack-id.n"}
{"File Name":"nso.aurora\/AsynchronousMessages.md","Context":"## Context\nThe messages that are sent from Order Management, Rating Manager and Recommendation Manager can be asynchronous?\n","Decision":"Yes it can be asynchronous because we don't need to wait for an ack before sending the next one.\\n","tokens":22,"id":307,"text":"## Context\\nThe messages that are sent from Order Management, Rating Manager and Recommendation Manager can be asynchronous?\\n\n\n##Decision\nYes it can be asynchronous because we don't need to wait for an ack before sending the next one.\\n","Predictions":"The decision is to use a separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separate separ"}
{"File Name":"openlobby-server\/0011-add-relational-database.md","Context":"## Context\nNumber of document types which does not use Elasticsearch's fulltext\ncapabilities is growing. Recently released Elasticsearch 6 is bringing one type\nper index which means management of many indices.\n","Decision":"We will add relational database as primary database. Elasticsearch will be used\\nfor denormalized reports and related data intended for fulltext search.\\n","tokens":42,"id":475,"text":"## Context\\nNumber of document types which does not use Elasticsearch's fulltext\\ncapabilities is growing. Recently released Elasticsearch 6 is bringing one type\\nper index which means management of many indices.\\n\n\n##Decision\nWe will add relational database as primary database. Elasticsearch will be used\\nfor denormalized reports and related data intended for fulltext search.\\n","Predictions":"We will use Tegola as the backbone for menu-generation application.n"}
{"File Name":"james\/0013-precompute-jmap-preview.md","Context":"## Context\nJMAP messages have a handy preview property displaying the firsts 256 characters of meaningful test of a message.\nThis property is often displayed for message listing in JMAP clients, thus it is queried a lot.\nCurrently, to get the preview, James retrieves the full message body, parse it using MIME parsers, removes HTML and keep meaningful text.\n","Decision":"We should pre-compute message preview.\\nA MailboxListener will compute the preview and store it in a MessagePreviewStore.\\nWe should have a Cassandra and memory implementation.\\nWhen the preview is precomputed then for these messages we can consider the \"preview\" property as a metadata.\\nWhen the preview is not precomputed then we should compute the preview for these messages, and save the result for later.\\nWe should provide a webAdmin task allowing to rebuild the projection. The computing and storing in MessagePreviewStore\\nis idempotent and the task can be run in live without any concurrency problem.\\nSome performance tests will be run in order to evaluate the improvements.\\n","tokens":75,"id":2134,"text":"## Context\\nJMAP messages have a handy preview property displaying the firsts 256 characters of meaningful test of a message.\\nThis property is often displayed for message listing in JMAP clients, thus it is queried a lot.\\nCurrently, to get the preview, James retrieves the full message body, parse it using MIME parsers, removes HTML and keep meaningful text.\\n\n\n##Decision\nWe should pre-compute message preview.\\nA MailboxListener will compute the preview and store it in a MessagePreviewStore.\\nWe should have a Cassandra and memory implementation.\\nWhen the preview is precomputed then for these messages we can consider the \"preview\" property as a metadata.\\nWhen the preview is not precomputed then we should compute the preview for these messages, and save the result for later.\\nWe should provide a webAdmin task allowing to rebuild the projection. The computing and storing in MessagePreviewStore\\nis idempotent and the task can be run in live without any concurrency problem.\\nSome performance tests will be run in order to evaluate the improvements.\\n","Predictions":"We should pre-compute message preview.nA MailboxListener will compute the preview and store it in a MessagePreviewStore.nWe should have a Cassandra and memory implementation.nWhen the preview is precomputed then for these messages we can consider the \"preview\" property as a metadata.nWhen the preview is not precomputed then we should compute the preview for these messages, and save the result for later.nWe should provide a webAdmin task allowing to rebuild the projection. The computing and storing in MessagePreviewStorenis idempotent and the task can be run in live without any concurrency problem.nSome performance tests will be run in order to evaluate the improvements.n"}
{"File Name":"gsp\/ADR004-tenant-isolation.md","Context":"## Context\nThe two main isolation models for Kubernetes are:\n* Namespaces within a single cluster\n* Running multiple clusters\nAll Service Teams currently have separate AWS accounts.\nSome Service Teams have separate AWS accounts for separate environment (ie. Staging, Production etc)\nMany Service Teams have micro-service architectures\nSome Service Teams have unique network isolation requirements that may be hard to implement in a shared environment.\nTo ensure \"smooth transition\" during a migration it would be preferable to have clusters deployed to Service Team's VPCs.\nTo ensure separation of billing it would be preferable to deploy clusters to Service Team's AWS accounts.\nTo ensure strong network\/compute isolation between Service Teams it would be preferable to deploy separate clusters for separate environments.\n","Decision":"We will manage multiple Kubernetes clusters, one per environment per tenant.\\nThis should provide:\\n* Strong network\/compute isolation between service teams\\n* Clear separation of billing\\n* Ability to deploy into target VPCs to make piecemeal microservice migration practical\\n","tokens":153,"id":3889,"text":"## Context\\nThe two main isolation models for Kubernetes are:\\n* Namespaces within a single cluster\\n* Running multiple clusters\\nAll Service Teams currently have separate AWS accounts.\\nSome Service Teams have separate AWS accounts for separate environment (ie. Staging, Production etc)\\nMany Service Teams have micro-service architectures\\nSome Service Teams have unique network isolation requirements that may be hard to implement in a shared environment.\\nTo ensure \"smooth transition\" during a migration it would be preferable to have clusters deployed to Service Team's VPCs.\\nTo ensure separation of billing it would be preferable to deploy clusters to Service Team's AWS accounts.\\nTo ensure strong network\/compute isolation between Service Teams it would be preferable to deploy separate clusters for separate environments.\\n\n\n##Decision\nWe will manage multiple Kubernetes clusters, one per environment per tenant.\\nThis should provide:\\n* Strong network\/compute isolation between service teams\\n* Clear separation of billing\\n* Ability to deploy into target VPCs to make piecemeal microservice migration practical\\n","Predictions":"* Scalability (down): must have a single operator running on the cluster.n* Ability to deploy multiple Kubernetes clusters, one per environment per tenant.nThis should provide:n* Strong network\/compute isolation between service teamsn* Clear separation of billingn* Ability to deploy into target VPCs to make piecemeal microservice migration practicaln"}
{"File Name":"mediawiki-extensions-Kartographer\/webgl_renderer.md","Context":"## Context\nIn order to decrease maintenance overhead in the maps infrastructure, the tile server will receive improvements in the vector-tile generation using Tegola. Although Kartotherian already have a vector-tile endpoint, now there is some effort to evolve the platform as a whole, and it includes client-side rendering, that will take advantage of future capabilities supported by Tegola.\nThe client-side render process will provide decoupling of some server-side components and will make it possible to replace mapnik, which is a maintenance overhead.\nAfter analyzing all the open-source options in the market, we have reached the decision of using the mapbox-gl-style specification as the next-level in terms of maps rendering. The reason is because it can be easily integrated with the top client-side maps frameworks: maplibre-gl, Leaflet, and OpenLayers.\nBecause of its flexibility and support across frameworks, it gives us freedom to experiment different engines in the early stages of this work.\n","Decision":"Accepted (02-16-2021)\\n","tokens":198,"id":3286,"text":"## Context\\nIn order to decrease maintenance overhead in the maps infrastructure, the tile server will receive improvements in the vector-tile generation using Tegola. Although Kartotherian already have a vector-tile endpoint, now there is some effort to evolve the platform as a whole, and it includes client-side rendering, that will take advantage of future capabilities supported by Tegola.\\nThe client-side render process will provide decoupling of some server-side components and will make it possible to replace mapnik, which is a maintenance overhead.\\nAfter analyzing all the open-source options in the market, we have reached the decision of using the mapbox-gl-style specification as the next-level in terms of maps rendering. The reason is because it can be easily integrated with the top client-side maps frameworks: maplibre-gl, Leaflet, and OpenLayers.\\nBecause of its flexibility and support across frameworks, it gives us freedom to experiment different engines in the early stages of this work.\\n\n\n##Decision\nAccepted (02-16-2021)\\n","Predictions":"We have decided to use the [Static Asset Pipeline](https:\/\/github.com\/static-assets\/static-assets) framework as the static asset pipeline.n"}
{"File Name":"govuk-kubernetes-discovery\/0005-consistent-structure-for-terraform-files.md","Context":"## Context\nWe should have a consistent structure for working with our Terraform files. We\nshould be able to look into a file and understand it's function is and what it\ncreates, what variables it needs to run and what it outputs after it's finished.\nWe should not have to spend time picking apart the code to understand what is\ncreated by Terraform.\nWe should not have to work with multiple files to understand a single component.\n","Decision":"Create style guidelines based upon the following:\\n- Header with title and description\\n- Variables listed\\n- Outputs listed\\n- Full descriptions of what variables and outputs do in the code\\n- `terraform fmt` across each file\\nCreate a style guideline document explaining this structure.\\n","tokens":93,"id":2793,"text":"## Context\\nWe should have a consistent structure for working with our Terraform files. We\\nshould be able to look into a file and understand it's function is and what it\\ncreates, what variables it needs to run and what it outputs after it's finished.\\nWe should not have to spend time picking apart the code to understand what is\\ncreated by Terraform.\\nWe should not have to work with multiple files to understand a single component.\\n\n\n##Decision\nCreate style guidelines based upon the following:\\n- Header with title and description\\n- Variables listed\\n- Outputs listed\\n- Full descriptions of what variables and outputs do in the code\\n- `terraform fmt` across each file\\nCreate a style guideline document explaining this structure.\\n","Predictions":"We will use a components style guideline to guide our code.n"}
{"File Name":"clone_difitalcitizenship\/0007-choice-of-azure-region.md","Context":"## Context\nWe must guarantee that the data stored, transferred to and from the cloud services will never leave the borders of the European Union.\n","Decision":"We decide to deploy our services on the [West Europe](https:\/\/azure.microsoft.com\/en-us\/regions\/) region, located in the Netherlands.\\n","tokens":29,"id":1186,"text":"## Context\\nWe must guarantee that the data stored, transferred to and from the cloud services will never leave the borders of the European Union.\\n\n\n##Decision\nWe decide to deploy our services on the [West Europe](https:\/\/azure.microsoft.com\/en-us\/regions\/) region, located in the Netherlands.\\n","Predictions":"We will use AWS Lambda as our containerized deployment provider. This conforms to the [GDS Tech Forum Hosting Guide](https:\/\/github.com\/alphagov\/gds-tech\/pull\/7).n"}
{"File Name":"teaching-vacancies\/0010_migrate_from_algolia_to_postgres.md","Context":"## Context and Problem Statement\nGiven the user needs we want to meet with our search functionality, and the cost and complexity involved in our integration with Algolia, would we be better off with a simpler solution of leveraging our existing database for search?\n## Decision Drivers\n* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\n* Technical complexity and brittleness of Algolia integration\n* Lack of customisability and lack of configuration-as-code on Algolia\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\n* Lack of ability to integration test search results when using SaaS search engine\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\n* Potential for richer geographical querying using PostGIS\n","Decision":"* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\nDecided to migrate away from Algolia and move to using our database for search functionality.\\n### Positive Consequences\\n* Simplified infrastructure and one fewer third-party service integration\\n* Significant complexity savings in search code\\n* Ability to integration test search results and keep search configuration in code (versioned and auditable)\\n* End of reliance on Algolia's goodwill in keeping us grandfathered on their legacy plan\\n* Minor cost savings in the short term (and we no longer need to avoid potential increases in queries in the long term)\\n* Improved performance and reliability due to not having to interact with a third-party service\\n* Ability to completely control and debug all aspects of how search works\\n* Richer geographical querying through use of PostGIS\\n### Negative Consequences\\n* Some implementation effort (estimated at 2x developers for ~3-4 sprints)\\n* Synonym logic will need custom implementation (but at the same time, this allows us to make it significantly \"smarter\" than on Algolia)\\n* Typo tolerance\/fuzzy search will need custom implementation (but this is an edge-case for us because the majority of queries are straightforward)\\n","tokens":162,"id":4378,"text":"## Context and Problem Statement\\nGiven the user needs we want to meet with our search functionality, and the cost and complexity involved in our integration with Algolia, would we be better off with a simpler solution of leveraging our existing database for search?\\n## Decision Drivers\\n* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\n\n\n##Decision\n* Financial cost of Algolia (and risk of eventually losing our cheap grandfathered legacy plan)\\n* Technical complexity and brittleness of Algolia integration\\n* Lack of customisability and lack of configuration-as-code on Algolia\\n* Simplicity of underlying search requirements means enterprise-grade search engine is overkill\\n* Lack of ability to integration test search results when using SaaS search engine\\n* Availability of constantly improving full-text search functionality in our database (PostgreSQL)\\n* Potential for richer geographical querying using PostGIS\\nDecided to migrate away from Algolia and move to using our database for search functionality.\\n### Positive Consequences\\n* Simplified infrastructure and one fewer third-party service integration\\n* Significant complexity savings in search code\\n* Ability to integration test search results and keep search configuration in code (versioned and auditable)\\n* End of reliance on Algolia's goodwill in keeping us grandfathered on their legacy plan\\n* Minor cost savings in the short term (and we no longer need to avoid potential increases in queries in the long term)\\n* Improved performance and reliability due to not having to interact with a third-party service\\n* Ability to completely control and debug all aspects of how search works\\n* Richer geographical querying through use of PostGIS\\n### Negative Consequences\\n* Some implementation effort (estimated at 2x developers for ~3-4 sprints)\\n* Synonym logic will need custom implementation (but at the same time, this allows us to make it significantly \"smarter\" than on Algolia)\\n* Typo tolerance\/fuzzy search will need custom implementation (but this is an edge-case for us because the majority of queries are straightforward)\\n","Predictions":"* Technical complexity and brittleness of Algolia integrationn* Lack of customisability and lack of configuration-as-code on Algolian* Simplicity of underlying search requirements means enterprise-grade search engine is overkilln* Lack of ability to integration test search results when using SaaS search enginen* Availability of constantly improving full-text search functionality in our database (PostgreSQL)n* Potential for richer geographical querying using PostGISnDecided to migrate away from Algolia and move to using our database for search functionality.n### Positive Consequencesn* Simplified infrastructure and one fewer third-party service integrationn* Significant complexity savings in search coden* Ability to integration test search results and keep search configuration in code (versioned and auditable)n* End of reliance on Algolia's goodwill in keeping us grandfathered on their legacy plann* Minor cost savings in the short term (and we no longer need to avoid potential increases in queries in the long term)n* Improved performance and reliability due to not having to interact with a third-party servicen* Ability to completely control and debug all aspects of how how Algolia worksn* Ability to completely control and debug all aspects of how Algolia functionsn* Ability to completely control and debug all aspects of how Algolia functionsn* Ability to completely control and debug all aspects of how Algolia functionsn* Ability to completely control and debug all aspects of how Algolia functionsn* Ability to completely control and debug all aspects of how Algolia functionsn*"}
{"File Name":"openfido-app-service\/0003-deployment.md","Context":"## Context\nWe plan to use AWS as a development\/staging environment and as a production environment, on ECS.\n","Decision":"Use CircleCI for CI, and deploy docker images to AWS ECR for use in deployments.\\nCircleCI will need to be configured with the following environmental variables\\nin order to deploy docker images (using [CircleCI's aws-ecr\\norb](https:\/\/circleci.com\/developer\/orbs\/orb\/circleci\/aws-ecr)):\\n- DEV_AWS_ACCESS_KEY_ID\\n- DEV_AWS_SECRET_ACCESS_KEY\\n- PROD_AWS_ACCESS_KEY_ID\\n- PROD_AWS_SECRET_ACCESS_KEY\\n- AWS_REGION\\n- AWS_ECR_ACCOUNT_URL\\nThis project's Dockerfile requires access to a privately hosted github project\\n(openfido-utils). Use [Buildkit](https:\/\/docs.docker.com\/develop\/develop-images\/build_enhancements\/) on CircleCI to grant SSH access to the docker build processes.\\n","tokens":24,"id":2471,"text":"## Context\\nWe plan to use AWS as a development\/staging environment and as a production environment, on ECS.\\n\n\n##Decision\nUse CircleCI for CI, and deploy docker images to AWS ECR for use in deployments.\\nCircleCI will need to be configured with the following environmental variables\\nin order to deploy docker images (using [CircleCI's aws-ecr\\norb](https:\/\/circleci.com\/developer\/orbs\/orb\/circleci\/aws-ecr)):\\n- DEV_AWS_ACCESS_KEY_ID\\n- DEV_AWS_SECRET_ACCESS_KEY\\n- PROD_AWS_ACCESS_KEY_ID\\n- PROD_AWS_SECRET_ACCESS_KEY\\n- AWS_REGION\\n- AWS_ECR_ACCOUNT_URL\\nThis project's Dockerfile requires access to a privately hosted github project\\n(openfido-utils). Use [Buildkit](https:\/\/docs.docker.com\/develop\/develop-images\/build_enhancements\/) on CircleCI to grant SSH access to the docker build processes.\\n","Predictions":"Use CircleCI for CI, and deploy docker images to AWS ECR for use in deployments.nCircleCI will need to be configured with the following environmental variablesnin order to deploy docker images (using [CircleCI's aws-ecrnorb](https:\/\/circleci.com\/developer\/orbs\/orb\/circleci\/aws-ecr)):n- DEV_AWS_ACCESS_KEY_IDn- DEV_AWS_SECRET_ACCESS_KEYn- PROD_AWS_ACCESS_KEY_IDn- PROD_AWS_SECRET_ACCESS_KEYn- AWS_REGIONn- AWS_ECR_ACCOUNT_URLnThis project's Dockerfile requires access to a privately hosted github projectn(openfido-utils). Use [Buildkit](https:\/\/docs.docker.com\/develop\/develop-images\/build_enhancements\/) on CircleCI to grant SSH access to the docker build processes.n"}
{"File Name":"content-data-api\/adr-009-track-metrics-by-basepath.md","Context":"## Context\nWe started tracking performance and quality metrics by `content_id`, but after\nthe first 3 months it was clear that the user needs to track metrics at the\nbase_path level in Guides and Travel Advice.\n","Decision":"Track metrics at the base_path level\\n","tokens":46,"id":1866,"text":"## Context\\nWe started tracking performance and quality metrics by `content_id`, but after\\nthe first 3 months it was clear that the user needs to track metrics at the\\nbase_path level in Guides and Travel Advice.\\n\n\n##Decision\nTrack metrics at the base_path level\\n","Predictions":"We will use the Security API to store the data we need to store in the cloud.n"}
{"File Name":"green_log\/0006-use-lock-free-io.md","Context":"## Context\nWe want to be able to write log entries (to file, or STDOUT), without them being interleaved.\nBut also, we want logging to perform well.\n","Decision":"_Unlike_ the Ruby standard `Logger`, GreenLog will use a [lock-free logging](https:\/\/www.jstorimer.com\/blogs\/workingwithcode\/7982047-is-lock-free-logging-safe) approach. That is, we will:\\n- avoid using of mutexes to serialise output\\n- perform atomic writes to `IO` streams (using `<<`)\\n","tokens":37,"id":1927,"text":"## Context\\nWe want to be able to write log entries (to file, or STDOUT), without them being interleaved.\\nBut also, we want logging to perform well.\\n\n\n##Decision\n_Unlike_ the Ruby standard `Logger`, GreenLog will use a [lock-free logging](https:\/\/www.jstorimer.com\/blogs\/workingwithcode\/7982047-is-lock-free-logging-safe) approach. That is, we will:\\n- avoid using of mutexes to serialise output\\n- perform atomic writes to `IO` streams (using `<<`)\\n","Predictions":"We will use a [serialization](https:\/\/github.com\/serializer\/serializer) approach to encode all the data in the response field.nWe will also review the meaning of each response type, which will determine the encoding of each value (JSON? string? array? etc).nMultiple responses (barcodes, option questions, cascades, etc) will be encoded using a **JSON array**.n"}
{"File Name":"editions\/06-\u2705-to-use-step-functions.md","Context":"## Context\nThe archiving process is time consuming and consists of many stages.\n","Decision":"For the archiving process to be run as an amazon step function so we can break the process down into component stages.\\nThis will allow anyone supporting the system to immediately find what broke in case something goes wrong.\\nThis will also allow the publication process to fire the notification at the desired time.\\n","tokens":17,"id":672,"text":"## Context\\nThe archiving process is time consuming and consists of many stages.\\n\n\n##Decision\nFor the archiving process to be run as an amazon step function so we can break the process down into component stages.\\nThis will allow anyone supporting the system to immediately find what broke in case something goes wrong.\\nThis will also allow the publication process to fire the notification at the desired time.\\n","Predictions":"The build pipeline will be split into separate pipelines.nThe build pipeline will include the build and deploy pipelines.nThe deploy pipeline will include the deploy pipelines.n"}
{"File Name":"Head-Start-TTADP\/0007-use-plantuml-for-diagrams.md","Context":"## Context\nCertain things we need to document, such as our data model and boundary diagrams are\nbetter represented as diagrams than text. Choosing a single service will make it easier to\nlearn the system and keep diagrams up to date.\n","Decision":"We will use https:\/\/www.plantuml.com to generate diagrams with the following checks:\\n1. We will not rely on the plantuml.com service to be the sole storage medium for the actual UML text\\n1. We will store each type of diagram in its own markdown file in the \/docs folder\\nPlantUML can also be run locally, in case the plantuml.com service ever goes away. This risk is also why\\nwe must keep the UML source in our source control.\\n","tokens":48,"id":1172,"text":"## Context\\nCertain things we need to document, such as our data model and boundary diagrams are\\nbetter represented as diagrams than text. Choosing a single service will make it easier to\\nlearn the system and keep diagrams up to date.\\n\n\n##Decision\nWe will use https:\/\/www.plantuml.com to generate diagrams with the following checks:\\n1. We will not rely on the plantuml.com service to be the sole storage medium for the actual UML text\\n1. We will store each type of diagram in its own markdown file in the \/docs folder\\nPlantUML can also be run locally, in case the plantuml.com service ever goes away. This risk is also why\\nwe must keep the UML source in our source control.\\n","Predictions":"We will use a docs folder in the \/docs folder to store the diagrams.n"}
{"File Name":"datalab\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":774,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"katas\/002-no-eslint.md","Context":"## Context\nSee [ADR1](.\/001-formatting-katas.md), which describes the formatting of a kata.\nThis would normally lead to using a linter.\n","Decision":"But I am not a big fan of a huge set of rules\\nand currently configuring a linter just for those couple of rules is overhead.\\nSomething like prettier which automatically applies a rule set is fine, but a linter\\nis kinda like someone who just says that a cleanup is needed, an auto-fixer (like prettier)\\ndoes clean up instead of just talking about it, my 2 cents.\\n","tokens":36,"id":4336,"text":"## Context\\nSee [ADR1](.\/001-formatting-katas.md), which describes the formatting of a kata.\\nThis would normally lead to using a linter.\\n\n\n##Decision\nBut I am not a big fan of a huge set of rules\\nand currently configuring a linter just for those couple of rules is overhead.\\nSomething like prettier which automatically applies a rule set is fine, but a linter\\nis kinda like someone who just says that a cleanup is needed, an auto-fixer (like prettier)\\ndoes clean up instead of just talking about it, my 2 cents.\\n","Predictions":"We will use the [ADR](https:\/\/github.com\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/ADR\/"}
{"File Name":"ichatsdk-evolution\/0001-use-c-language.md","Context":"## Context and Problem Statement\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\n## Decision Drivers <!-- optional -->\n* [driver 1, e.g., a force, facing concern, \u2026]\n* [driver 2, e.g., a force, facing concern, \u2026]\n* \u2026 <!-- numbers of drivers can vary -->\n","Decision":"* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","tokens":96,"id":4062,"text":"## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","Predictions":"* [driver 1, e.g., a force, facing concern,...]n* [driver 2, e.g., a force, facing concern,...]n*...!-- numbers of drivers can vary -->nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force |... | comes out best (see below)].n### Positive Consequences!-- optional -->n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required,...]n*...n### Negative Consequences!-- optional -->n* [e.g., compromising quality attribute, follow-up decisions required,...]n*...n"}
{"File Name":"pul_solr\/0001-document-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3131,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"plant-shop\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3349,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"ibc-rs\/adr-003-handler-implementation.md","Context":"## Context\nIn this ADR, we provide recommendations for implementing the IBC\nhandlers within the `ibc` (modules) crate.\n","Decision":"Concepts are introduced in the order given by a topological sort of their dependencies on each other.\\n### Events\\nIBC handlers must be able to emit events which will then be broadcasted via the node's pub\/sub mechanism,\\nand eventually picked up by the IBC relayer.\\nAn event has an arbitrary structure, depending on the handler that produces it.\\nHere is the [list of all IBC-related events][events], as seen by the relayer.\\nNote that the consumer of these events in production would not be the relayer directly\\n(instead the consumer is the node\/SDK where the IBC module executes),\\nbut nevertheless handlers will reuse these event definitions.\\n[events]: https:\/\/github.com\/informalsystems\/hermes\/blob\/bf84a73ef7b3d5e9a434c9af96165997382dcc9d\/modules\/src\/events.rs#L15-L43\\n```rust\\npub enum IBCEvent {\\nNewBlock(NewBlock),\\nCreateClient(ClientEvents::CreateClient),\\nUpdateClient(ClientEvents::UpdateClient),\\nClientMisbehavior(ClientEvents::ClientMisbehavior),\\nOpenInitConnection(ConnectionEvents::OpenInit),\\nOpenTryConnection(ConnectionEvents::OpenTry),\\n\/\/     ...\\n}\\n```\\n### Logging\\nIBC handlers must be able to log information for introspectability and ease of debugging.\\nA handler can output multiple log records, which are expressed as a pair of a status and a\\nlog line. The interface for emitting log records is described in the next section.\\n```rust\\npub enum LogStatus {\\nSuccess,\\nInfo,\\nWarning,\\nError,\\n}\\npub struct Log {\\nstatus: LogStatus,\\nbody: String,\\n}\\nimpl Log {\\nfn success(msg: impl Display) -> Self;\\nfn info(msg: impl Display) -> Self;\\nfn warning(msg: impl Display) -> Self;\\nfn error(msg: impl Display) -> Self;\\n}\\n```\\n### Handler output\\nIBC handlers must be able to return arbitrary data, together with events and log records, as described above.\\nAs a handler may fail, it is necessary to keep track of errors.\\nTo this end, we introduce a type for the return value of a handler:\\n```rust\\npub type HandlerResult<T, E> = Result<HandlerOutput<T>, E>;\\npub struct HandlerOutput<T> {\\npub result: T,\\npub log: Vec<Log>,\\npub events: Vec<Event>,\\n}\\n```\\nWe introduce a builder interface to be used within the handler implementation to incrementally build a `HandlerOutput` value.\\n```rust\\nimpl<T> HandlerOutput<T> {\\npub fn builder() -> HandlerOutputBuilder<T> {\\nHandlerOutputBuilder::new()\\n}\\n}\\npub struct HandlerOutputBuilder<T> {\\nlog: Vec<String>,\\nevents: Vec<Event>,\\nmarker: PhantomData<T>,\\n}\\nimpl<T> HandlerOutputBuilder<T> {\\npub fn log(&mut self, log: impl Into<Log>);\\npub fn emit(&mut self, event: impl Into<Event>);\\npub fn with_result(self, result: T) -> HandlerOutput<T>;\\n}\\n```\\nWe provide below an example usage of the builder API:\\n```rust\\nfn some_ibc_handler() -> HandlerResult<u64, Error> {\\nlet mut output = HandlerOutput::builder();\\n\/\/ ...\\noutput.log(Log::info(\"did something\"))\\n\/\/ ...\\noutput.log(Log::success(\"all good\"));\\noutput.emit(SomeEvent::AllGood);\\nOk(output.with_result(42));\\n}\\n```\\n### IBC Submodule\\nThe various IBC messages and their processing logic, as described in the IBC specification,\\nare split into a collection of submodules, each pertaining to a specific aspect of\\nthe IBC protocol, eg. client lifecycle management, connection lifecycle management,\\npacket relay, etc.\\nIn this section we propose a general approach to implement the handlers for a submodule.\\nAs a running example we will use a dummy submodule that deals with connections, which should not\\nbe mistaken for the actual ICS 003 Connection submodule.\\n#### Reader\\nA typical handler will need to read data from the chain state at the current height,\\nvia the private and provable stores.\\nTo avoid coupling between the handler interface and the store API, we introduce an interface\\nfor accessing this data. This interface, called a `Reader`, is shared between all handlers\\nin a submodule, as those typically access the same data.\\nHaving a high-level interface for this purpose helps avoiding coupling which makes\\nwriting unit tests for the handlers easier, as one does not need to provide a concrete\\nstore, or to mock one.\\n```rust\\npub trait ConnectionReader\\n{\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd>;\\n}\\n```\\nA production implementation of this `Reader` would hold references to both the private and provable\\nstore at the current height where the handler executes, but we omit the actual implementation as\\nthe store interfaces are yet to be defined, as is the general IBC top-level module machinery.\\nA mock implementation of the `ConnectionReader` trait could looks as follows:\\n```rust\\nstruct MockConnectionReader {\\nconnection_id: ConnectionId,\\nconnection_end: Option<ConnectionEnd>,\\nclient_reader: MockClientReader,\\n}\\nimpl ConnectionReader for MockConnectionReader {\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd> {\\nif connection_id == &self.connection_id {\\nself.connection_end.clone()\\n} else {\\nNone\\n}\\n}\\n}\\n```\\n#### Keeper\\nOnce a handler executes successfully, some data will typically need to be persisted in the chain state\\nvia the private\/provable store interfaces. In the same vein as for the reader defined in the previous section,\\na submodule should define a trait which provides operations to persist such data.\\nThe same considerations w.r.t. to coupling and unit-testing apply here as well.\\n```rust\\npub trait ConnectionKeeper {\\nfn store_connection(\\n&mut self,\\nclient_id: ConnectionId,\\nclient_type: ConnectionType,\\n) -> Result<(), Error>;\\nfn add_connection_to_client(\\n&mut self,\\nclient_id: ClientId,\\nconnection_id: ConnectionId,\\n) -> Result<(), Error>;\\n}\\n```\\n#### Submodule implementation\\nWe now come to the actual definition of a handler for a submodule.\\nWe recommend each handler to be defined within its own Rust module, named\\nafter the handler itself. For example, the \"Create Client\" handler of ICS 002 would\\nbe defined in `modules::ics02_client::handler::create_client`.\\n##### Message type\\nEach handler must define a datatype which represent the message it can process.\\n```rust\\npub struct MsgConnectionOpenInit {\\nconnection_id: ConnectionId,\\nclient_id: ClientId,\\ncounterparty: Counterparty,\\n}\\n```\\n##### Handler implementation\\nIn this section we provide guidelines for implementing an actual handler.\\nWe divide the handler in two parts: processing and persistence.\\n###### Processing\\nThe actual logic of the handler is expressed as a pure function, typically named\\n`process`, which takes as arguments a `Reader` and the corresponding message, and returns\\na `HandlerOutput<T, E>`, where `T` is a concrete datatype and `E` is an error type which defines\\nall potential errors yielded by the handlers of the current submodule.\\n```rust\\npub struct ConnectionMsgProcessingResult {\\nconnection_id: ConnectionId,\\nconnection_end: ConnectionEnd,\\n}\\n```\\nThe `process` function will typically read data via the `Reader`, perform checks and validation, construct new\\ndatatypes, emit log records and events, and eventually return some data together with objects to be persisted.\\nTo this end, this `process` function will create and manipulate a `HandlerOutput` value like described in\\nthe corresponding section.\\n```rust\\npub fn process(\\nreader: &dyn ConnectionReader,\\nmsg: MsgConnectionOpenInit,\\n) -> HandlerResult<ConnectionMsgProcessingResult, Error>\\n{\\nlet mut output = HandlerOutput::builder();\\nlet MsgConnectionOpenInit { connection_id, client_id, counterparty, } = msg;\\nif reader.connection_end(&connection_id).is_some() {\\nreturn Err(Kind::ConnectionAlreadyExists(connection_id).into());\\n}\\noutput.log(\"success: no connection state found\");\\nif reader.client_reader.client_state(&client_id).is_none() {\\nreturn Err(Kind::ClientForConnectionMissing(client_id).into());\\n}\\noutput.log(\"success: client found\");\\noutput.emit(IBCEvent::ConnectionOpenInit(connection_id.clone()));\\nOk(output.with_result(ConnectionMsgProcessingResult {\\nconnection_id,\\nclient_id,\\ncounterparty,\\n}))\\n}\\n```\\n###### Persistence\\nIf the `process` function specified above succeeds, the result value it yielded is then\\npassed to a function named `keep`, which is responsible for persisting the objects constructed\\nby the processing function. This `keep` function takes the submodule's `Keeper` and the result\\ntype defined above, and performs side-effecting calls to the keeper's methods to persist the result.\\nBelow is given an implementation of the `keep` function for the \"Create Connection\" handlers:\\n```rust\\npub fn keep(\\nkeeper: &mut dyn ConnectionKeeper,\\nresult: ConnectionMsgProcessingResult,\\n) -> Result<(), Error>\\n{\\nkeeper.store_connection(result.connection_id.clone(), result.connection_end)?;\\nkeeper.add_connection_to_client(result.client_id, result.connection_id)?;\\nOk(())\\n}\\n```\\n##### Submodule dispatcher\\n> This section is very much a work in progress, as further investigation into what\\n> a production-ready implementation of the `ctx` parameter of the top-level dispatcher\\n> is required. As such, implementers should feel free to disregard the recommendations\\n> below, and are encouraged to come up with amendments to this ADR to better capture\\n> the actual requirements.\\nEach submodule is responsible for dispatching the messages it is given to the appropriate\\nmessage processing function and, if successful, pass the resulting data to the persistence\\nfunction defined in the previous section.\\nTo this end, the submodule should define an enumeration of all messages, in order\\nfor the top-level submodule dispatcher to forward them to the appropriate processor.\\nSuch a definition for the ICS 003 Connection submodule is given below.\\n```rust\\npub enum ConnectionMsg {\\nConnectionOpenInit(MsgConnectionOpenInit),\\nConnectionOpenTry(MsgConnectionOpenTry),\\n...\\n}\\n```\\nThe actual implementation of a submodule dispatcher is quite straightforward and unlikely to vary\\nmuch in substance between submodules. We give an implementation for the ICS 003 Connection module below.\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: Msg) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ConnectionReader + ConnectionKeeper,\\n{\\nmatch msg {\\nMsg::ConnectionOpenInit(msg) => {\\nlet HandlerOutput {\\nresult,\\nlog,\\nevents,\\n} = connection_open_init::process(ctx, msg)?;\\nconnection::keep(ctx, result)?;\\nOk(HandlerOutput::builder()\\n.with_log(log)\\n.with_events(events)\\n.with_result(()))\\n}\\nMsg::ConnectionOpenTry(msg) => \/\/ omitted\\n}\\n}\\n```\\nIn essence, a top-level dispatcher is a function of a message wrapped in the enumeration introduced above,\\nand a \"context\" which implements both the `Reader` and `Keeper` interfaces.\\n### Dealing with chain-specific datatypes\\nThe ICS 002 Client submodule stands out from the other submodules as it needs\\nto deal with chain-specific datatypes, such as `Header`, `ClientState`, and\\n`ConsensusState`.\\nTo abstract over chain-specific datatypes, we introduce a trait which specifies\\nboth which types we need to abstract over, and their interface.\\nFor the ICS 002 Client submodule, this trait looks as follow:\\n```rust\\npub trait ClientDef {\\ntype Header: Header;\\ntype ClientState: ClientState;\\ntype ConsensusState: ConsensusState;\\n}\\n```\\nThe `ClientDef` trait specifies three datatypes, and their corresponding interface, which is provided\\nvia a trait defined in the same submodule.\\nA production implementation of this interface would instantiate these types with the concrete\\ntypes used by the chain, eg. Tendermint datatypes. Each concrete datatype must be provided\\nwith a `From` instance to lift it into its corresponding `Any...` enumeration.\\nFor the purpose of unit-testing, a mock implementation of the `ClientDef` trait could look as follows:\\n```rust\\nstruct MockHeader(u32);\\nimpl Header for MockHeader {\\n\/\/ omitted\\n}\\nimpl From<MockHeader> for AnyHeader {\\nfn from(mh: MockHeader) -> Self {\\nSelf::Mock(mh)\\n}\\n}\\nstruct MockClientState(u32);\\nimpl ClientState for MockClientState {\\n\/\/ omitted\\n}\\nimpl From<MockClientState> for AnyClientState {\\nfn from(mcs: MockClientState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockConsensusState(u32);\\nimpl ConsensusState for MockConsensusState {\\n\/\/ omitted\\n}\\nimpl From<MockConsensusState> for AnyConsensusState {\\nfn from(mcs: MockConsensusState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockClient;\\nimpl ClientDef for MockClient {\\ntype Header = MockHeader;\\ntype ClientState = MockClientState;\\ntype ConsensusState = MockConsensusState;\\n}\\n```\\nSince the actual type of client can only be determined at runtime, we cannot encode\\nthe type of client within the message itself.\\nBecause of some limitations of the Rust type system, namely the lack of proper support\\nfor existential types, it is currently impossible to define `Reader` and `Keeper` traits\\nwhich are agnostic to the actual type of client being used.\\nWe could alternatively model all chain-specific datatypes as boxed trait objects (`Box<dyn Trait>`),\\nbut this approach runs into a lot of limitations of trait objects, such as the inability to easily\\nrequire such trait objects to be Clonable, or Serializable, or to define an equality relation on them.\\nSome support for such functionality can be found in third-party libraries, but the overall experience\\nfor the developer is too subpar.\\nWe thus settle on a different strategy: lifting chain-specific data into an `enum` over all\\npossible chain types.\\nFor example, to model a chain-specific `Header` type, we would define an enumeration in the following\\nway:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] \/\/ TODO: Add Eq\\npub enum AnyHeader {\\nMock(mocks::MockHeader),\\nTendermint(tendermint::header::Header),\\n}\\nimpl Header for AnyHeader {\\nfn height(&self) -> Height {\\nmatch self {\\nSelf::Mock(header) => header.height(),\\nSelf::Tendermint(header) => header.height(),\\n}\\n}\\nfn client_type(&self) -> ClientType {\\nmatch self {\\nSelf::Mock(header) => header.client_type(),\\nSelf::Tendermint(header) => header.client_type(),\\n}\\n}\\n}\\n```\\nThis enumeration dispatches method calls to the underlying datatype at runtime, while\\nhiding the latter, and is thus akin to a proper existential type without running\\ninto any limitations of the Rust type system (`impl Header` bounds not being allowed\\neverywhere, `Header` not being able to be treated as a trait objects because of `Clone`,\\n`PartialEq` and `Serialize`, `Deserialize` bounds, etc.)\\nOther chain-specific datatypes, such as `ClientState` and `ConsensusState` require their own\\nenumeration over all possible implementations.\\nOn top of that, we also need to lift the specific client definitions (`ClientDef` instances),\\ninto their own enumeration, as follows:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Eq)]\\npub enum AnyClient {\\nMock(mocks::MockClient),\\nTendermint(tendermint::TendermintClient),\\n}\\nimpl ClientDef for AnyClient {\\ntype Header = AnyHeader;\\ntype ClientState = AnyClientState;\\ntype ConsensusState = AnyConsensusState;\\n}\\n```\\nMessages can now be defined generically over the `ClientDef` instance:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]\\npub struct MsgCreateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub client_type: ClientType,\\npub consensus_state: CD::ConsensusState,\\n}\\npub struct MsgUpdateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub header: CD::Header,\\n}\\n```\\nThe `Keeper` and `Reader` traits are defined for any client:\\n```rust\\npub trait ClientReader {\\nfn client_type(&self, client_id: &ClientId) -> Option<ClientType>;\\nfn client_state(&self, client_id: &ClientId) -> Option<AnyClientState>;\\nfn consensus_state(&self, client_id: &ClientId, height: Height) -> Option<AnyConsensusState>;\\n}\\npub trait ClientKeeper {\\nfn store_client_type(\\n&mut self,\\nclient_id: ClientId,\\nclient_type: ClientType,\\n) -> Result<(), Error>;\\nfn store_client_state(\\n&mut self,\\nclient_id: ClientId,\\nclient_state: AnyClientState,\\n) -> Result<(), Error>;\\nfn store_consensus_state(\\n&mut self,\\nclient_id: ClientId,\\nconsensus_state: AnyConsensusState,\\n) -> Result<(), Error>;\\n}\\n```\\nThis way, only one implementation of the `ClientReader` and `ClientKeeper` trait is required,\\nas it can delegate eg. the serialization of the underlying datatypes to the `Serialize` bound\\nof the `Any...` wrapper.\\nBoth the `process` and `keep` function are defined to take a message generic over\\nthe actual client type:\\n```rust\\npub fn process(\\nctx: &dyn ClientReader,\\nmsg: MsgCreateClient<AnyClient>,\\n) -> HandlerResult<CreateClientResult<AnyClient>, Error>;\\npub fn keep(\\nkeeper: &mut dyn ClientKeeper,\\nresult: CreateClientResult<AnyClient>,\\n) -> Result<(), Error>;\\n```\\nSame for the top-level dispatcher:\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: ClientMsg<AnyClient>) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ClientReader + ClientKeeper;\\n```\\nWith this boilerplate out of way, one can write tests using a mock client, and associated mock datatypes\\nin a fairly straightforward way, taking advantage of the `From` instance to lift concerete mock datatypes\\ninto the `Any...` enumeration:\\n```rust\\n#[test]\\nfn test_create_client_ok() {\\nlet client_id: ClientId = \"mockclient\".parse().unwrap();\\nlet reader = MockClientReader {\\nclient_id: client_id.clone(),\\nclient_type: None,\\nclient_state: None,\\nconsensus_state: None,\\n};\\nlet msg = MsgCreateClient {\\nclient_id,\\nclient_type: ClientType::Tendermint,\\nconsensus_state: MockConsensusState(42).into(), \/\/ lift into `AnyConsensusState`\\n};\\nlet output = process(&reader, msg.clone());\\nmatch output {\\nOk(HandlerOutput {\\nresult,\\nevents,\\nlog,\\n}) => {\\n\/\/ snip\\n}\\nErr(err) => {\\npanic!(\"unexpected error: {}\", err);\\n}\\n}\\n}\\n```\\n","tokens":30,"id":4656,"text":"## Context\\nIn this ADR, we provide recommendations for implementing the IBC\\nhandlers within the `ibc` (modules) crate.\\n\n\n##Decision\nConcepts are introduced in the order given by a topological sort of their dependencies on each other.\\n### Events\\nIBC handlers must be able to emit events which will then be broadcasted via the node's pub\/sub mechanism,\\nand eventually picked up by the IBC relayer.\\nAn event has an arbitrary structure, depending on the handler that produces it.\\nHere is the [list of all IBC-related events][events], as seen by the relayer.\\nNote that the consumer of these events in production would not be the relayer directly\\n(instead the consumer is the node\/SDK where the IBC module executes),\\nbut nevertheless handlers will reuse these event definitions.\\n[events]: https:\/\/github.com\/informalsystems\/hermes\/blob\/bf84a73ef7b3d5e9a434c9af96165997382dcc9d\/modules\/src\/events.rs#L15-L43\\n```rust\\npub enum IBCEvent {\\nNewBlock(NewBlock),\\nCreateClient(ClientEvents::CreateClient),\\nUpdateClient(ClientEvents::UpdateClient),\\nClientMisbehavior(ClientEvents::ClientMisbehavior),\\nOpenInitConnection(ConnectionEvents::OpenInit),\\nOpenTryConnection(ConnectionEvents::OpenTry),\\n\/\/     ...\\n}\\n```\\n### Logging\\nIBC handlers must be able to log information for introspectability and ease of debugging.\\nA handler can output multiple log records, which are expressed as a pair of a status and a\\nlog line. The interface for emitting log records is described in the next section.\\n```rust\\npub enum LogStatus {\\nSuccess,\\nInfo,\\nWarning,\\nError,\\n}\\npub struct Log {\\nstatus: LogStatus,\\nbody: String,\\n}\\nimpl Log {\\nfn success(msg: impl Display) -> Self;\\nfn info(msg: impl Display) -> Self;\\nfn warning(msg: impl Display) -> Self;\\nfn error(msg: impl Display) -> Self;\\n}\\n```\\n### Handler output\\nIBC handlers must be able to return arbitrary data, together with events and log records, as described above.\\nAs a handler may fail, it is necessary to keep track of errors.\\nTo this end, we introduce a type for the return value of a handler:\\n```rust\\npub type HandlerResult<T, E> = Result<HandlerOutput<T>, E>;\\npub struct HandlerOutput<T> {\\npub result: T,\\npub log: Vec<Log>,\\npub events: Vec<Event>,\\n}\\n```\\nWe introduce a builder interface to be used within the handler implementation to incrementally build a `HandlerOutput` value.\\n```rust\\nimpl<T> HandlerOutput<T> {\\npub fn builder() -> HandlerOutputBuilder<T> {\\nHandlerOutputBuilder::new()\\n}\\n}\\npub struct HandlerOutputBuilder<T> {\\nlog: Vec<String>,\\nevents: Vec<Event>,\\nmarker: PhantomData<T>,\\n}\\nimpl<T> HandlerOutputBuilder<T> {\\npub fn log(&mut self, log: impl Into<Log>);\\npub fn emit(&mut self, event: impl Into<Event>);\\npub fn with_result(self, result: T) -> HandlerOutput<T>;\\n}\\n```\\nWe provide below an example usage of the builder API:\\n```rust\\nfn some_ibc_handler() -> HandlerResult<u64, Error> {\\nlet mut output = HandlerOutput::builder();\\n\/\/ ...\\noutput.log(Log::info(\"did something\"))\\n\/\/ ...\\noutput.log(Log::success(\"all good\"));\\noutput.emit(SomeEvent::AllGood);\\nOk(output.with_result(42));\\n}\\n```\\n### IBC Submodule\\nThe various IBC messages and their processing logic, as described in the IBC specification,\\nare split into a collection of submodules, each pertaining to a specific aspect of\\nthe IBC protocol, eg. client lifecycle management, connection lifecycle management,\\npacket relay, etc.\\nIn this section we propose a general approach to implement the handlers for a submodule.\\nAs a running example we will use a dummy submodule that deals with connections, which should not\\nbe mistaken for the actual ICS 003 Connection submodule.\\n#### Reader\\nA typical handler will need to read data from the chain state at the current height,\\nvia the private and provable stores.\\nTo avoid coupling between the handler interface and the store API, we introduce an interface\\nfor accessing this data. This interface, called a `Reader`, is shared between all handlers\\nin a submodule, as those typically access the same data.\\nHaving a high-level interface for this purpose helps avoiding coupling which makes\\nwriting unit tests for the handlers easier, as one does not need to provide a concrete\\nstore, or to mock one.\\n```rust\\npub trait ConnectionReader\\n{\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd>;\\n}\\n```\\nA production implementation of this `Reader` would hold references to both the private and provable\\nstore at the current height where the handler executes, but we omit the actual implementation as\\nthe store interfaces are yet to be defined, as is the general IBC top-level module machinery.\\nA mock implementation of the `ConnectionReader` trait could looks as follows:\\n```rust\\nstruct MockConnectionReader {\\nconnection_id: ConnectionId,\\nconnection_end: Option<ConnectionEnd>,\\nclient_reader: MockClientReader,\\n}\\nimpl ConnectionReader for MockConnectionReader {\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd> {\\nif connection_id == &self.connection_id {\\nself.connection_end.clone()\\n} else {\\nNone\\n}\\n}\\n}\\n```\\n#### Keeper\\nOnce a handler executes successfully, some data will typically need to be persisted in the chain state\\nvia the private\/provable store interfaces. In the same vein as for the reader defined in the previous section,\\na submodule should define a trait which provides operations to persist such data.\\nThe same considerations w.r.t. to coupling and unit-testing apply here as well.\\n```rust\\npub trait ConnectionKeeper {\\nfn store_connection(\\n&mut self,\\nclient_id: ConnectionId,\\nclient_type: ConnectionType,\\n) -> Result<(), Error>;\\nfn add_connection_to_client(\\n&mut self,\\nclient_id: ClientId,\\nconnection_id: ConnectionId,\\n) -> Result<(), Error>;\\n}\\n```\\n#### Submodule implementation\\nWe now come to the actual definition of a handler for a submodule.\\nWe recommend each handler to be defined within its own Rust module, named\\nafter the handler itself. For example, the \"Create Client\" handler of ICS 002 would\\nbe defined in `modules::ics02_client::handler::create_client`.\\n##### Message type\\nEach handler must define a datatype which represent the message it can process.\\n```rust\\npub struct MsgConnectionOpenInit {\\nconnection_id: ConnectionId,\\nclient_id: ClientId,\\ncounterparty: Counterparty,\\n}\\n```\\n##### Handler implementation\\nIn this section we provide guidelines for implementing an actual handler.\\nWe divide the handler in two parts: processing and persistence.\\n###### Processing\\nThe actual logic of the handler is expressed as a pure function, typically named\\n`process`, which takes as arguments a `Reader` and the corresponding message, and returns\\na `HandlerOutput<T, E>`, where `T` is a concrete datatype and `E` is an error type which defines\\nall potential errors yielded by the handlers of the current submodule.\\n```rust\\npub struct ConnectionMsgProcessingResult {\\nconnection_id: ConnectionId,\\nconnection_end: ConnectionEnd,\\n}\\n```\\nThe `process` function will typically read data via the `Reader`, perform checks and validation, construct new\\ndatatypes, emit log records and events, and eventually return some data together with objects to be persisted.\\nTo this end, this `process` function will create and manipulate a `HandlerOutput` value like described in\\nthe corresponding section.\\n```rust\\npub fn process(\\nreader: &dyn ConnectionReader,\\nmsg: MsgConnectionOpenInit,\\n) -> HandlerResult<ConnectionMsgProcessingResult, Error>\\n{\\nlet mut output = HandlerOutput::builder();\\nlet MsgConnectionOpenInit { connection_id, client_id, counterparty, } = msg;\\nif reader.connection_end(&connection_id).is_some() {\\nreturn Err(Kind::ConnectionAlreadyExists(connection_id).into());\\n}\\noutput.log(\"success: no connection state found\");\\nif reader.client_reader.client_state(&client_id).is_none() {\\nreturn Err(Kind::ClientForConnectionMissing(client_id).into());\\n}\\noutput.log(\"success: client found\");\\noutput.emit(IBCEvent::ConnectionOpenInit(connection_id.clone()));\\nOk(output.with_result(ConnectionMsgProcessingResult {\\nconnection_id,\\nclient_id,\\ncounterparty,\\n}))\\n}\\n```\\n###### Persistence\\nIf the `process` function specified above succeeds, the result value it yielded is then\\npassed to a function named `keep`, which is responsible for persisting the objects constructed\\nby the processing function. This `keep` function takes the submodule's `Keeper` and the result\\ntype defined above, and performs side-effecting calls to the keeper's methods to persist the result.\\nBelow is given an implementation of the `keep` function for the \"Create Connection\" handlers:\\n```rust\\npub fn keep(\\nkeeper: &mut dyn ConnectionKeeper,\\nresult: ConnectionMsgProcessingResult,\\n) -> Result<(), Error>\\n{\\nkeeper.store_connection(result.connection_id.clone(), result.connection_end)?;\\nkeeper.add_connection_to_client(result.client_id, result.connection_id)?;\\nOk(())\\n}\\n```\\n##### Submodule dispatcher\\n> This section is very much a work in progress, as further investigation into what\\n> a production-ready implementation of the `ctx` parameter of the top-level dispatcher\\n> is required. As such, implementers should feel free to disregard the recommendations\\n> below, and are encouraged to come up with amendments to this ADR to better capture\\n> the actual requirements.\\nEach submodule is responsible for dispatching the messages it is given to the appropriate\\nmessage processing function and, if successful, pass the resulting data to the persistence\\nfunction defined in the previous section.\\nTo this end, the submodule should define an enumeration of all messages, in order\\nfor the top-level submodule dispatcher to forward them to the appropriate processor.\\nSuch a definition for the ICS 003 Connection submodule is given below.\\n```rust\\npub enum ConnectionMsg {\\nConnectionOpenInit(MsgConnectionOpenInit),\\nConnectionOpenTry(MsgConnectionOpenTry),\\n...\\n}\\n```\\nThe actual implementation of a submodule dispatcher is quite straightforward and unlikely to vary\\nmuch in substance between submodules. We give an implementation for the ICS 003 Connection module below.\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: Msg) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ConnectionReader + ConnectionKeeper,\\n{\\nmatch msg {\\nMsg::ConnectionOpenInit(msg) => {\\nlet HandlerOutput {\\nresult,\\nlog,\\nevents,\\n} = connection_open_init::process(ctx, msg)?;\\nconnection::keep(ctx, result)?;\\nOk(HandlerOutput::builder()\\n.with_log(log)\\n.with_events(events)\\n.with_result(()))\\n}\\nMsg::ConnectionOpenTry(msg) => \/\/ omitted\\n}\\n}\\n```\\nIn essence, a top-level dispatcher is a function of a message wrapped in the enumeration introduced above,\\nand a \"context\" which implements both the `Reader` and `Keeper` interfaces.\\n### Dealing with chain-specific datatypes\\nThe ICS 002 Client submodule stands out from the other submodules as it needs\\nto deal with chain-specific datatypes, such as `Header`, `ClientState`, and\\n`ConsensusState`.\\nTo abstract over chain-specific datatypes, we introduce a trait which specifies\\nboth which types we need to abstract over, and their interface.\\nFor the ICS 002 Client submodule, this trait looks as follow:\\n```rust\\npub trait ClientDef {\\ntype Header: Header;\\ntype ClientState: ClientState;\\ntype ConsensusState: ConsensusState;\\n}\\n```\\nThe `ClientDef` trait specifies three datatypes, and their corresponding interface, which is provided\\nvia a trait defined in the same submodule.\\nA production implementation of this interface would instantiate these types with the concrete\\ntypes used by the chain, eg. Tendermint datatypes. Each concrete datatype must be provided\\nwith a `From` instance to lift it into its corresponding `Any...` enumeration.\\nFor the purpose of unit-testing, a mock implementation of the `ClientDef` trait could look as follows:\\n```rust\\nstruct MockHeader(u32);\\nimpl Header for MockHeader {\\n\/\/ omitted\\n}\\nimpl From<MockHeader> for AnyHeader {\\nfn from(mh: MockHeader) -> Self {\\nSelf::Mock(mh)\\n}\\n}\\nstruct MockClientState(u32);\\nimpl ClientState for MockClientState {\\n\/\/ omitted\\n}\\nimpl From<MockClientState> for AnyClientState {\\nfn from(mcs: MockClientState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockConsensusState(u32);\\nimpl ConsensusState for MockConsensusState {\\n\/\/ omitted\\n}\\nimpl From<MockConsensusState> for AnyConsensusState {\\nfn from(mcs: MockConsensusState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockClient;\\nimpl ClientDef for MockClient {\\ntype Header = MockHeader;\\ntype ClientState = MockClientState;\\ntype ConsensusState = MockConsensusState;\\n}\\n```\\nSince the actual type of client can only be determined at runtime, we cannot encode\\nthe type of client within the message itself.\\nBecause of some limitations of the Rust type system, namely the lack of proper support\\nfor existential types, it is currently impossible to define `Reader` and `Keeper` traits\\nwhich are agnostic to the actual type of client being used.\\nWe could alternatively model all chain-specific datatypes as boxed trait objects (`Box<dyn Trait>`),\\nbut this approach runs into a lot of limitations of trait objects, such as the inability to easily\\nrequire such trait objects to be Clonable, or Serializable, or to define an equality relation on them.\\nSome support for such functionality can be found in third-party libraries, but the overall experience\\nfor the developer is too subpar.\\nWe thus settle on a different strategy: lifting chain-specific data into an `enum` over all\\npossible chain types.\\nFor example, to model a chain-specific `Header` type, we would define an enumeration in the following\\nway:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] \/\/ TODO: Add Eq\\npub enum AnyHeader {\\nMock(mocks::MockHeader),\\nTendermint(tendermint::header::Header),\\n}\\nimpl Header for AnyHeader {\\nfn height(&self) -> Height {\\nmatch self {\\nSelf::Mock(header) => header.height(),\\nSelf::Tendermint(header) => header.height(),\\n}\\n}\\nfn client_type(&self) -> ClientType {\\nmatch self {\\nSelf::Mock(header) => header.client_type(),\\nSelf::Tendermint(header) => header.client_type(),\\n}\\n}\\n}\\n```\\nThis enumeration dispatches method calls to the underlying datatype at runtime, while\\nhiding the latter, and is thus akin to a proper existential type without running\\ninto any limitations of the Rust type system (`impl Header` bounds not being allowed\\neverywhere, `Header` not being able to be treated as a trait objects because of `Clone`,\\n`PartialEq` and `Serialize`, `Deserialize` bounds, etc.)\\nOther chain-specific datatypes, such as `ClientState` and `ConsensusState` require their own\\nenumeration over all possible implementations.\\nOn top of that, we also need to lift the specific client definitions (`ClientDef` instances),\\ninto their own enumeration, as follows:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Eq)]\\npub enum AnyClient {\\nMock(mocks::MockClient),\\nTendermint(tendermint::TendermintClient),\\n}\\nimpl ClientDef for AnyClient {\\ntype Header = AnyHeader;\\ntype ClientState = AnyClientState;\\ntype ConsensusState = AnyConsensusState;\\n}\\n```\\nMessages can now be defined generically over the `ClientDef` instance:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]\\npub struct MsgCreateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub client_type: ClientType,\\npub consensus_state: CD::ConsensusState,\\n}\\npub struct MsgUpdateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub header: CD::Header,\\n}\\n```\\nThe `Keeper` and `Reader` traits are defined for any client:\\n```rust\\npub trait ClientReader {\\nfn client_type(&self, client_id: &ClientId) -> Option<ClientType>;\\nfn client_state(&self, client_id: &ClientId) -> Option<AnyClientState>;\\nfn consensus_state(&self, client_id: &ClientId, height: Height) -> Option<AnyConsensusState>;\\n}\\npub trait ClientKeeper {\\nfn store_client_type(\\n&mut self,\\nclient_id: ClientId,\\nclient_type: ClientType,\\n) -> Result<(), Error>;\\nfn store_client_state(\\n&mut self,\\nclient_id: ClientId,\\nclient_state: AnyClientState,\\n) -> Result<(), Error>;\\nfn store_consensus_state(\\n&mut self,\\nclient_id: ClientId,\\nconsensus_state: AnyConsensusState,\\n) -> Result<(), Error>;\\n}\\n```\\nThis way, only one implementation of the `ClientReader` and `ClientKeeper` trait is required,\\nas it can delegate eg. the serialization of the underlying datatypes to the `Serialize` bound\\nof the `Any...` wrapper.\\nBoth the `process` and `keep` function are defined to take a message generic over\\nthe actual client type:\\n```rust\\npub fn process(\\nctx: &dyn ClientReader,\\nmsg: MsgCreateClient<AnyClient>,\\n) -> HandlerResult<CreateClientResult<AnyClient>, Error>;\\npub fn keep(\\nkeeper: &mut dyn ClientKeeper,\\nresult: CreateClientResult<AnyClient>,\\n) -> Result<(), Error>;\\n```\\nSame for the top-level dispatcher:\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: ClientMsg<AnyClient>) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ClientReader + ClientKeeper;\\n```\\nWith this boilerplate out of way, one can write tests using a mock client, and associated mock datatypes\\nin a fairly straightforward way, taking advantage of the `From` instance to lift concerete mock datatypes\\ninto the `Any...` enumeration:\\n```rust\\n#[test]\\nfn test_create_client_ok() {\\nlet client_id: ClientId = \"mockclient\".parse().unwrap();\\nlet reader = MockClientReader {\\nclient_id: client_id.clone(),\\nclient_type: None,\\nclient_state: None,\\nconsensus_state: None,\\n};\\nlet msg = MsgCreateClient {\\nclient_id,\\nclient_type: ClientType::Tendermint,\\nconsensus_state: MockConsensusState(42).into(), \/\/ lift into `AnyConsensusState`\\n};\\nlet output = process(&reader, msg.clone());\\nmatch output {\\nOk(HandlerOutput {\\nresult,\\nevents,\\nlog,\\n}) => {\\n\/\/ snip\\n}\\nErr(err) => {\\npanic!(\"unexpected error: {}\", err);\\n}\\n}\\n}\\n```\\n","Predictions":"We will use the Status class to store the state of the Status.n"}
{"File Name":"island.is-glosur\/0009-naming-files-and-directories.md","Context":"## Context and Problem Statement\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\ndefaults that differ between schematic types.\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\nand directories.\n## Decision Drivers\n* Provide consistency when navigating the codebase\n* The earlier we decide on this, the better\n","Decision":"* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","tokens":93,"id":4766,"text":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n\n\n##Decision\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","Predictions":"* The earlier we decide on this, the betternChosen option: Name files after their default export. If that default export is a React Component, or a class, then thenfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoidnusing kebab-case and snake_case and make sure the name follows the default export of the file.nNaming directories should follow these guidelines:nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. island-ui instead of islandUi:nimport  Box  from '@island.is\/island-ui\/core'nUse PascalCase for directories only containing React components:nncomponents\/CtaButton\/index.tsnimport 'components\/CtaButton'nor:nncomponents\/CtaButton\/CtaButton.tsxnimport 'components\/CtaButton\/CtaButton'nrather thannncomponents\/cta-button\/CtaButton'nnIn all other cases, use camelCase.n### Positive Consequencesn* Easier to navigate the codebasen* File names are more readable, and more readablen"}
{"File Name":"adr-demo\/0001-record-architecture-decisions.md","Context":"## Context\nMost projects suffer from a common problem : they usually don't keep a clear history of all the architectural decisions if the project.\nIt might not appear as an issue at first but as the project evolves it becomes less and less clear why each change was made,\nleading to somewhat wrong decisions : should we change existing code and take the risk of breaking the application because\nwe might have missed an important decision, or should we keep it (fearing we might break something) and take the risk of\nparalyzing the project with an accumulation of potentially wrongly-kept decisions and changes ?\nTo avoid this dilemna it appears we have to do something to keep a record of all architectural decisions.\n","Decision":"We will start using Lightweight Architecture Decision Records (further refered as ADR) as explained\\n[here](https:\/\/blog.stack-labs.com\/code\/adr-to-remember-past-archiectural-decisions\/#format-d-un-adr)\\nor [here](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nHere are a few hints of how we will use ADRs :\\n* We will keep all ADRs in a Git respository so they can be versioned\\n* We will materialize Each ADR as a separate file within the repository\\n* We will prefix each ADR by an ordered number (0001 to 9999), and keep ADRs numbers sequential\\n* We will keep each ADR as short as possible, trying to limit ourselves to 1-2 pages max\\n* We will use Markdown as the template engine of our ADRs\\n* We will always keep ALL written ADRs but we will mark old ADRs as superseded if they are\\nThe markdown format we will use when writing an ADR is the following :\\n```markdown\\n# Title: These documents have names that are short noun phrases. For example, \"ADR-0001 - Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\nThis section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","tokens":146,"id":2517,"text":"## Context\\nMost projects suffer from a common problem : they usually don't keep a clear history of all the architectural decisions if the project.\\nIt might not appear as an issue at first but as the project evolves it becomes less and less clear why each change was made,\\nleading to somewhat wrong decisions : should we change existing code and take the risk of breaking the application because\\nwe might have missed an important decision, or should we keep it (fearing we might break something) and take the risk of\\nparalyzing the project with an accumulation of potentially wrongly-kept decisions and changes ?\\nTo avoid this dilemna it appears we have to do something to keep a record of all architectural decisions.\\n\n\n##Decision\nWe will start using Lightweight Architecture Decision Records (further refered as ADR) as explained\\n[here](https:\/\/blog.stack-labs.com\/code\/adr-to-remember-past-archiectural-decisions\/#format-d-un-adr)\\nor [here](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\nHere are a few hints of how we will use ADRs :\\n* We will keep all ADRs in a Git respository so they can be versioned\\n* We will materialize Each ADR as a separate file within the repository\\n* We will prefix each ADR by an ordered number (0001 to 9999), and keep ADRs numbers sequential\\n* We will keep each ADR as short as possible, trying to limit ourselves to 1-2 pages max\\n* We will use Markdown as the template engine of our ADRs\\n* We will always keep ALL written ADRs but we will mark old ADRs as superseded if they are\\nThe markdown format we will use when writing an ADR is the following :\\n```markdown\\n# Title: These documents have names that are short noun phrases. For example, \"ADR-0001 - Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"\\nThis section describes our response to these forces. It is stated in full sentences, with active voice. \"We will ...\"\\n","Predictions":"We will start using Lightweight Architecture Decision Records (further refered as ADR) as describedn[here](https:\/\/blog.stack-labs.com\/code\/adr-to-remember-past-archiectural-decisions\/#format-d-un-adr)nor [here](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).nHere are a few hints of how we will use ADRs :n* We will keep all ADRs in a Git respository so they can be versionedn* We will materialize Each ADR as a separate file within the repositoryn* We will prefix each ADR by an ordered number (0001 to 9999), and keep ADRs numbers sequentialn* We will keep each ADR as short as possible, trying to limit ourselves to 1-2 pages maxn* We will use Markdown as the template engine of our ADRsn* We will always keep ALL written ADRs but we will mark old ADRs as superseded if they arenThe markdown format we will use when writing an ADR is the following :nmarkdownn# Title: These documents have names that are short noun phrases. For example, \"ADR-0001 - Deployment on Ruby on Rails 3.0.10\" or \"ADR-0002 - Deployment on Rails 3.0.10\".nmarkdownn# Title: These documents have names that are short noun phrases. For example, \"ADR-0002 - Deployment on Ruby on Rails 3.0.10\" or \"ADR-0002 - Deployment on Rails 3.0.10\".nThe markdown format we will use when writing an ADR is the following :nmarkdownn# Title: These documents have names that are short noun phrases. For example, \"ADR-0001 - Deployment on Ruby on Rails 3.0.10\" or \"ADR-0002 - Deployment on Rails 3.0.10\".nThe format we will use when writing an ADR is the following :nmarkdownn# Title: These documents have names that are short noun phrases. For example, \"ADR-0001 - Deployment on Ruby on Rails 3.0.10\" or \"ADR-0002 - Deployment on Rails 3.0.10\".n"}
{"File Name":"james-project\/0047-jmap-push-over-websockets.md","Context":"## Context\nJMAP Push notifications allow clients to efficiently update (almost) instantly to stay in sync with data changes on the server.\nIn order to support this, we need to have the server emit newest state changes to the client over a PUSH channel.\n","Decision":"We will implement [RFC-8887 - A JSON Meta Application Protocol (JMAP) Subprotocol for WebSocket](https:\/\/tools.ietf.org\/html\/rfc8887)\\nas a transport mechanism for PUSH notifications.\\nWe will [generalize EventBus](0046-generalize-event-bus.md) in order to achieve an out-of-the box Publish-Subscribe\\nsystem for JMAP related events, that does not fit in the mailbox API.\\nWe will implement listeners registered on the JMAP event bus for WebSocket clients in order to carry over state changes\\nto the client.\\n","tokens":53,"id":2886,"text":"## Context\\nJMAP Push notifications allow clients to efficiently update (almost) instantly to stay in sync with data changes on the server.\\nIn order to support this, we need to have the server emit newest state changes to the client over a PUSH channel.\\n\n\n##Decision\nWe will implement [RFC-8887 - A JSON Meta Application Protocol (JMAP) Subprotocol for WebSocket](https:\/\/tools.ietf.org\/html\/rfc8887)\\nas a transport mechanism for PUSH notifications.\\nWe will [generalize EventBus](0046-generalize-event-bus.md) in order to achieve an out-of-the box Publish-Subscribe\\nsystem for JMAP related events, that does not fit in the mailbox API.\\nWe will implement listeners registered on the JMAP event bus for WebSocket clients in order to carry over state changes\\nto the client.\\n","Predictions":"We will implement [RFC-8887 - A JSON Meta Application Protocol (JMAP) Subprotocol for WebSocket](https:\/\/tools.ietf.org\/html\/rfc8887)nas a transport mechanism for PUSH notifications.nWe will [generalize EventBus](0046-generalize-event-bus.md) in order to achieve an out-of-the box Publish-Subscribensystem for JMAP related events, that does not fit in the mailbox API.nWe will implement listeners registered on the JMAP event bus for WebSocket clients in order to carry over state changesnto the client.n"}
{"File Name":"buildit-all\/0006-use-okta-as-identity-provider.md","Context":"## Context\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\nLater on, we could add additional social logins (or support AD directly if needed).\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\n","Decision":"* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","tokens":279,"id":4414,"text":"## Context\\nOkta is an Identity as a Service provider (similar to Auth0) that can provide authentication and authorization support to an application.  They have a forever free developer account that allows up to 7000 active users\/mo (well below our anticipated usage).\\nOkta provides the ability to authenticate against a variety of providers (okta, social (facebook, github, microsoft azure, etc), SAML, AD\/LDAP).  We would use the Microsoft social OpenConnect ID provider.  It will map the Microsoft user into an Okta user, essentially acting as a proxy or facade to Microsoft while providing a more uniform and standards adhering API.\\nThey have integrations into Spring Boot 1.x (via okta spring security starter) and Spring Security 5.x (via Spring Boot 2.x) supports Okta out of the box.\\nThey have a React client SDK instead of using our handrolled client code for Microsoft Azure AD.\\nThe real value add comes when we need to add roles\/groups (admin level users).  You get admin screens\/dashboards \"for free\" to add\/remove users etc.\\nLater on, we could add additional social logins (or support AD directly if needed).\\nSpike can be found at: https:\/\/github.com\/buildit\/bookit-api\/tree\/okta-spring-boot-1.x\\n\n\n##Decision\n* Delay for now - because we think we still need a local user table, we can add this in later\\n* Use Okta as IdP for Bookit\\n* Use Okta's React client for client side code\\n* Use Okta's Spring Boot 1.x starter for\\n* Use Okta Access Tokens as Bearer tokens\\n","Predictions":"* Delay for now - because we think we still need a local user table, we can add this in latern* Use OKta as IdP for Bookitn* Use OKta's React client for client side coden* Use OKta's Spring Boot 1.x starter forn* Use OKta Access Tokens as Bearer tokensn"}
{"File Name":"rails-template\/0009-use-scripts-to-rule-them-all.md","Context":"## Context\ndxw have approved an RFC for following the pattern of Scripts To Rule Them\nAll[1].\nThis repository should include reference and document this decision.\n[1]\nhttps:\/\/github.com\/dxw\/tech-team-rfcs\/blob\/main\/rfc-023-use-scripts-to-rule-them-all.md\n","Decision":"By default we will follow the Scripts To Rule Them All pattern for common tasks\\nin this template.\\n","tokens":67,"id":4178,"text":"## Context\\ndxw have approved an RFC for following the pattern of Scripts To Rule Them\\nAll[1].\\nThis repository should include reference and document this decision.\\n[1]\\nhttps:\/\/github.com\/dxw\/tech-team-rfcs\/blob\/main\/rfc-023-use-scripts-to-rule-them-all.md\\n\n\n##Decision\nBy default we will follow the Scripts To Rule Them All pattern for common tasks\\nin this template.\\n","Predictions":"By default we will follow the Scripts To Rule Them All pattern for common tasksnin this template.n"}
{"File Name":"status-react\/0007-masking-sensitive-data.md","Context":"## Context\nWe have some data that we don't want to appear in the logs (user passwords are\na good example). Currently, they are passed around as strings, that could be\nprinted out by mistake in a log entry (see https:\/\/github.com\/status-im\/status-react\/issues\/4053)\n","Decision":"To minimize the risk of leaking passwords through logs, we should not pass\\npasswords as strings in our codebase. We introduced a new type `MaskedData` in\\n`status-im.utils.security`.\\nWe use `(security\/mask-data <data to hide>` to wrap sensitive data into this\\ntype and then use `(security\/unmask <masked-data>)` to get the plaintext back.\\nIt is important to keep that sensitive data masked as much as possible, until\\nyou need the plaintext to pass to the extenral APIs.\\nExample:\\n```clojure\\n(println (security\/mask-data \"my-plaintext-password\")) ;; Outputs \"******\"\\n(println (security\/unmask (security\/mask-data \"my-plaintext-password\"))) ;; Outputs \"my-plaintext-password\"\\n```\\n","tokens":63,"id":2958,"text":"## Context\\nWe have some data that we don't want to appear in the logs (user passwords are\\na good example). Currently, they are passed around as strings, that could be\\nprinted out by mistake in a log entry (see https:\/\/github.com\/status-im\/status-react\/issues\/4053)\\n\n\n##Decision\nTo minimize the risk of leaking passwords through logs, we should not pass\\npasswords as strings in our codebase. We introduced a new type `MaskedData` in\\n`status-im.utils.security`.\\nWe use `(security\/mask-data <data to hide>` to wrap sensitive data into this\\ntype and then use `(security\/unmask <masked-data>)` to get the plaintext back.\\nIt is important to keep that sensitive data masked as much as possible, until\\nyou need the plaintext to pass to the extenral APIs.\\nExample:\\n```clojure\\n(println (security\/mask-data \"my-plaintext-password\")) ;; Outputs \"******\"\\n(println (security\/unmask (security\/mask-data \"my-plaintext-password\"))) ;; Outputs \"my-plaintext-password\"\\n```\\n","Predictions":"We will use the [semver.io](https:\/\/github.com\/semver-io\/semver-io) versioning scheme to ensure that the version number is the same as the version number in the releases directory.n"}
{"File Name":"bob\/0004-introduce-temporal-db.md","Context":"## Context\nCurrently we are using a traditional RDBMS, PostgreSQL as the storage of all the CI\/CD state.\nThe DB is the one and only state of the whole CI\/CD cluster, storing all of the pipeline, resource, artifact definitions, runs, logs etc. For all of these, tracking historical changes is of utmost importance.\nUsing a traditional CRUD workflow with PostgeSQL raises the following issues:\n- Analytics are really hard as the Storage and Querying is coupled and a heavy query slows the whole cluster down\n- Central locking is a severe impediment to scale\n- When a change occurs in the pipeline, resource provider or artifact store definition its quite difficult to track these changes across time for audit and rollback needs\n- To debug failures we need to have extended historical logging and is difficult to trace back in an update-in place system\n- The tabular structure comes in our way with rigidity and is not simple to introduce schema changes\n- Need of elaborate migration strategies which is further complicated with distributed state\n","Decision":"Based on the above facts the following is decided:\\n- Use [XTDB](https:\/\/xtdb.com) as the temporal, document store for the following reasons:\\n- Though being newer compared to [Datomic](https:\/\/www.datomic.com\/) its free and open source and ready for production use\\n- Has a quite unbundled design and uses a variety of storage backends and is transactor free\\n- Is [bi-temporal](https:\/\/xtdb.com\/articles\/bitemporality.html) and thereby offering more systematic analytical queries across time\\n- Has an HTTP interface for non JVM clients\\n- Use it with JDBC\/PostgreSQL backend which is quite readily available and managed in all popular cloud providers\\n- Remove the CRUD way of doing things, expose the DB too via API for more powerful, direct analytical querying\\n","tokens":209,"id":4285,"text":"## Context\\nCurrently we are using a traditional RDBMS, PostgreSQL as the storage of all the CI\/CD state.\\nThe DB is the one and only state of the whole CI\/CD cluster, storing all of the pipeline, resource, artifact definitions, runs, logs etc. For all of these, tracking historical changes is of utmost importance.\\nUsing a traditional CRUD workflow with PostgeSQL raises the following issues:\\n- Analytics are really hard as the Storage and Querying is coupled and a heavy query slows the whole cluster down\\n- Central locking is a severe impediment to scale\\n- When a change occurs in the pipeline, resource provider or artifact store definition its quite difficult to track these changes across time for audit and rollback needs\\n- To debug failures we need to have extended historical logging and is difficult to trace back in an update-in place system\\n- The tabular structure comes in our way with rigidity and is not simple to introduce schema changes\\n- Need of elaborate migration strategies which is further complicated with distributed state\\n\n\n##Decision\nBased on the above facts the following is decided:\\n- Use [XTDB](https:\/\/xtdb.com) as the temporal, document store for the following reasons:\\n- Though being newer compared to [Datomic](https:\/\/www.datomic.com\/) its free and open source and ready for production use\\n- Has a quite unbundled design and uses a variety of storage backends and is transactor free\\n- Is [bi-temporal](https:\/\/xtdb.com\/articles\/bitemporality.html) and thereby offering more systematic analytical queries across time\\n- Has an HTTP interface for non JVM clients\\n- Use it with JDBC\/PostgreSQL backend which is quite readily available and managed in all popular cloud providers\\n- Remove the CRUD way of doing things, expose the DB too via API for more powerful, direct analytical querying\\n","Predictions":"Use [Security Data Warehouse](https:\/\/securitydataware.com\/) as the backend for the following reasons:n- Though being newer compared to [Datomic](https:\/\/www.datomic.com\/) its free and open source and ready for production usen- Has a quite unbundled design and uses a variety of storage backends and is transactor freen- Is [bi-temporal](https:\/\/xtdb.com\/articles\/bitemporality.html) and thereby offering more systematic analytical queries across time for audit and rollback needsn- To debug failures we need to have extended historical logging and is difficult to trace back in an update-in place systemn- The tabular structure comes in our way with rigidity and is not simple to introduce schema changesn- Need of elaborate migration strategies which is further complicated with distributed staten- Use [XTDB](https:\/\/xtdb.com) as the temporal, document store for the following reasons:n- Though being newer compared to [Datomic](https:\/\/www.datomic.com\/) its free and open source and ready for production usen- Has a quite unbundled design and uses a variety of storage backends and is transactor freen- Is [bi-temporal](https:\/\/xtdb.com\/articles\/bitemporality.html) and thereby offering more systematic analytical queries across time for audit and rollback needsn- To debug failures we need to have extended historical logging and is difficult to trace back in an update-in place systemn- The tabular structure comes in our way with rigidity and is not simple to introduce schema changesn- Need of elaborate migration strategies which is further complicated with distributed staten- Use [XTDB](https:\/\/xtdb.com\/articles\/bitemporality.html) as the backend for the following reasons:n- Analytics are really hard as the Storage and Querying is coupled and a heavy query slows the whole cluster downn- Central locking is a severe impediment to scalen- When a change occurs in the pipeline, resource provider or artifact store definition its quite difficult to track these changes across time for audit and rollback needsn- To debug failures we need to have extended historical logging and is difficult to trace back in an update-in place systemn- The tabular structure comes in our way with rigidity and is not simple to introduce schema changesn- Need of elaborate migration strategies which is further complicated with distributed staten- Use [XTDB](https:\/\/xtdb.com\/articles\/bitemporality.html) as the temporal, document store for the following reasons:n- Though being newer compared to [Datomic](https:\/\/www.datomic.com\/) its free and open source and ready for production usen- Has a quite unbundled design and uses a variety of storage backends and is transactor freen- Is [bi-temporal](https:\/\/xtdb.com\/articles\/bitemporality.html) and thereby offering more systematic analytical queries across time for audit and rollback needsn- To debug failures we need to have extended historical logging and is difficult to trace back in an update-in place systemn- The tabular structure comes in our way with rigidity and is not simple to introduce schema changesn- Need of elaborate migration strategies which is further complicated with distributed staten- Use [XTDB](https:\/\/xtdb.com\/articles\/bitemporality.html) and thereby offering more"}
{"File Name":"agentframework\/0008-minify-build.md","Context":"## Context\nAgentFramework is also use for Browser\n","Decision":"We need minify build because package.json don't have a property called `minMain`\\n","tokens":11,"id":2359,"text":"## Context\\nAgentFramework is also use for Browser\\n\n\n##Decision\nWe need minify build because package.json don't have a property called `minMain`\\n","Predictions":"We need minify build because package.json don't have a property called minMainn"}
{"File Name":"gatemint-sdk\/adr-006-secret-store-replacement.md","Context":"## Context\nCurrently, an SDK application's CLI directory stores key material and metadata in a plain text database in the user\u2019s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user\/computer.\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\n","Decision":"We recommend replacing the current Keybase backend based on LevelDB with [Keyring](https:\/\/github.com\/99designs\/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\\nThis appears to fulfill the requirement of protecting both key material and metadata from rouge software on a user\u2019s machine.\\n","tokens":214,"id":13,"text":"## Context\\nCurrently, an SDK application's CLI directory stores key material and metadata in a plain text database in the user\u2019s home directory.  Key material is encrypted by a passphrase, protected by bcrypt hashing algorithm. Metadata (e.g. addresses, public keys, key storage details) is available in plain text.\\nThis is not desirable for a number of reasons. Perhaps the biggest reason is insufficient security protection of key material and metadata. Leaking the plain text allows an attacker to surveil what keys a given computer controls via a number of techniques, like compromised dependencies without any privilege execution. This could be followed by a more targeted attack on a particular user\/computer.\\nAll modern desktop computers OS (Ubuntu, Debian, MacOS, Windows) provide a built-in secret store that is designed to allow applications to store information that is isolated from all other applications and requires passphrase entry to access the data.\\nWe are seeking solution that provides a common abstraction layer to the many different backends and reasonable fallback for minimal platforms that don\u2019t provide a native secret store.\\n\n\n##Decision\nWe recommend replacing the current Keybase backend based on LevelDB with [Keyring](https:\/\/github.com\/99designs\/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.\\nThis appears to fulfill the requirement of protecting both key material and metadata from rouge software on a user\u2019s machine.\\n","Predictions":"We recommend replacing the current Keybase backend based on LevelDB with [Keyring](https:\/\/github.com\/99designs\/keyring) by 99 designs. This application is designed to provide a common abstraction and uniform interface between many secret stores and is used by AWS Vault application by 99-designs application.nThis appears to fulfill the requirement of protecting both key material and metadata from rouge software on a user\u2019s machine.n"}
{"File Name":"banking-cqrs-es-go\/0002-use-adr-tools-to-manage-the-adrs.md","Context":"## Context\nA tool to make creating these ADRs helps reduce friction around creating documentation.\n`adr-tools` is a simple cli tool that has been used in the past on previous projects to great effect (https:\/\/github.com\/npryce\/adr-tools)\n","Decision":"Install `adr-tools` locally and use it to help quickly generate consistent ADRs\\n","tokens":56,"id":2914,"text":"## Context\\nA tool to make creating these ADRs helps reduce friction around creating documentation.\\n`adr-tools` is a simple cli tool that has been used in the past on previous projects to great effect (https:\/\/github.com\/npryce\/adr-tools)\\n\n\n##Decision\nInstall `adr-tools` locally and use it to help quickly generate consistent ADRs\\n","Predictions":"We will use the minikube-service to create a Kubernetes cluster.n"}
{"File Name":"tech-team\/0006-supported-node-and-npm-versions.md","Context":"## Context\neLife has projects built in Node.\neLife has projects that use the Node Package Manager (npm) in at least part of their own build process, even if the main technology of the project is not Node.\nIn order to provide language version consistency across projects we need to get a consensus on which Node versions we are going to support.\nIn order to provide dependency management consistency, we need a consensus on which npm versions to support.\nStaying up to date with the major and minor versions of Node is important:\n- to be able to use new features (nice to have)\n- to keep working on a version that receives bug fixes and security updates (must have)\n","Decision":"To use 8.x, the current Long Term Support (LTS) line.\\nIn or around January 2019 to review upgrading to the next LTS line before 8.x drops out of active LTS in April 2019 (see [node release schedule](https:\/\/github.com\/nodejs\/Release#release-schedule)).\\nTo use whichever npm version is bundled with the Node version we use.\\nWhen upgrading, we will make a concerted effort to upgrade all projects as part of the same piece of work.\\n","tokens":143,"id":939,"text":"## Context\\neLife has projects built in Node.\\neLife has projects that use the Node Package Manager (npm) in at least part of their own build process, even if the main technology of the project is not Node.\\nIn order to provide language version consistency across projects we need to get a consensus on which Node versions we are going to support.\\nIn order to provide dependency management consistency, we need a consensus on which npm versions to support.\\nStaying up to date with the major and minor versions of Node is important:\\n- to be able to use new features (nice to have)\\n- to keep working on a version that receives bug fixes and security updates (must have)\\n\n\n##Decision\nTo use 8.x, the current Long Term Support (LTS) line.\\nIn or around January 2019 to review upgrading to the next LTS line before 8.x drops out of active LTS in April 2019 (see [node release schedule](https:\/\/github.com\/nodejs\/Release#release-schedule)).\\nTo use whichever npm version is bundled with the Node version we use.\\nWhen upgrading, we will make a concerted effort to upgrade all projects as part of the same piece of work.\\n","Predictions":"To use 8.x, the current Long Term Support (LTS) line.nIn or around January 2019 to review upgrading to the next LTS line before 8.x drops out of active LTS in April 2019 (see [node release schedule](https:\/\/github.com\/nodejs\/Release#release-schedule)).nTo use whichever npm version is bundled with the Node version we use.nWhen upgrading, we will make a concerted effort to upgrade all projects as part of the same piece of work.n"}
{"File Name":"mlmc\/0003-use-rabbit-mq-for-eda.md","Context":"## Context and Problem Statement\nDifferent services are going to push different kind of messages so there is a need in EDA architecture and some messaging server.\n## Decision Drivers\n* Easy to use\n* Easy to learn\n* Possibility to run from Docker\n","Decision":"* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\nDecided to use RabbitMQ since\\n* It provides exact functionality that is neded for the product (publish-subscribe event messaging)\\n* It is easy to learn and use and I already had some knowledge of how to use it\\n* It has official container on Docker Hub\\n* It seems to be popular and trending these days\\n### Positive Consequences\\n* I will be able to setup EDA messaging in the product.\\n### Negative Consequences\\n* n\/a\\n","tokens":53,"id":1575,"text":"## Context and Problem Statement\\nDifferent services are going to push different kind of messages so there is a need in EDA architecture and some messaging server.\\n## Decision Drivers\\n* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\n\n\n##Decision\n* Easy to use\\n* Easy to learn\\n* Possibility to run from Docker\\nDecided to use RabbitMQ since\\n* It provides exact functionality that is neded for the product (publish-subscribe event messaging)\\n* It is easy to learn and use and I already had some knowledge of how to use it\\n* It has official container on Docker Hub\\n* It seems to be popular and trending these days\\n### Positive Consequences\\n* I will be able to setup EDA messaging in the product.\\n### Negative Consequences\\n* n\/a\\n","Predictions":"* Easy to usen* Easy to learnn* Possibility to run from DockernDecided to use RabbitMQ sincen* It provides exact functionality that is nededed for the product (publish-subscribe event messaging)n* It is easy to learn and use and I already had some knowledge of how to use itn* It has official container on Docker Hubn* It seems to be popular and trending these daysn### Positive Consequences!-- optional -->n* Persistence bill should be down from $5\/month to less than $1\/monthn### Negative Consequences!-- optional -->n* Students in IT Academy won't be exposed to a relational model, which at this moment dominates work they will have to be doing.n"}
{"File Name":"unfinished-design-system\/004-tokenization-and-static-assets.md","Context":"* [Context](#context)\n* [Decision](#decision)\n* [Status](#status)\n* [Consequences](#consequences)\n* [More reading](#more-reading)\n* [Updates](#updates)\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Style Dictionary](https:\/\/amzn.github.io\/style-dictionary\/#\/) as our framework to handle tokens. It is easy to use since we can define our tickets in standard JSON but empowering them with string interpolation, variables, and other features.\\nFor our static assets, we're going to host them at [AWS S3](https:\/\/aws.amazon.com\/s3\/), defining the proper CORS rules, and refer the location of those as tokens for our applications to use.\\nInstead of increasing the size of our Javascript bundle with static assets, we prefer to keep it simple and light by hosting those in an S3 bucket and asking for the application to download it.\\n","tokens":53,"id":4550,"text":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\n\n\n##Decision\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n* [Updates](#updates)\\nWe've decided to use [Style Dictionary](https:\/\/amzn.github.io\/style-dictionary\/#\/) as our framework to handle tokens. It is easy to use since we can define our tickets in standard JSON but empowering them with string interpolation, variables, and other features.\\nFor our static assets, we're going to host them at [AWS S3](https:\/\/aws.amazon.com\/s3\/), defining the proper CORS rules, and refer the location of those as tokens for our applications to use.\\nInstead of increasing the size of our Javascript bundle with static assets, we prefer to keep it simple and light by hosting those in an S3 bucket and asking for the application to download it.\\n","Predictions":"* [Status](#status)n* [Consequences](#consequences)n* [More reading](#more-reading)n* [Updates](#updates)nWe are going to use Behavior-Driven Development to help us reduce that gap. BDD is a technique where we use User Stories to create acceptance tests based on user behaviors.nInstead of writing complex test statements, we use a single-choice approach to test the application.n"}
{"File Name":"digital-paper-edit-firebase\/2020-03-12-integrating-with-new-pstt-module.md","Context":"## Context and Problem Statement\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\n","Decision":"Ashley is building a service that will have an API Gateway and an S3 bucket. Uploading to the S3 bucket will trigger the STT event. We will be uploading to a bucket, as the previous architecture, with limitations that will define how we will communicate with Newslabs' PSTT service.\\nThere will only be one shared bucket per environment for services using this service. The name of the bucket is \"newslabs-stt-media-to-transcribe\" and \"newslabs-stt-media-to-transcribe-test\".\\n### Limitations\\n- Newslabs pstt will only handle audio files (`mp3`, `mp4`, `wav`, `flac`)\\n- the client (dpe) need to ensure they upload audio not video\\n- do not use the eTag as the reference, use object key instead: `<service_name>\/<object_key>.<ext>`\\n- assume that the client is sending a unique object key\\n### Example\\n#### Upload\\nWhen uploading a file with Object Key: `280612.mp3`, the Object Key should be prepended with the service name: `dpe\/280612.mp3`\\n#### Status\\nThe endpoint for requesting the status of a transcription uses [this lambda](https:\/\/github.com\/bbc\/newslabs-stt\/tree\/master\/newslabs-stt-check-transcription), which returns the transcription status.\\nThere is an example response in the README.\\nMake a request to an API Gateway endpoint (please ask) with something like this in the request body:\\n```json\\n{\\n\"objectKey\": \"dpe\/uuid.ext\"\\n}\\n```\\n","tokens":28,"id":5243,"text":"## Context and Problem Statement\\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\\n\n\n##Decision\nAshley is building a service that will have an API Gateway and an S3 bucket. Uploading to the S3 bucket will trigger the STT event. We will be uploading to a bucket, as the previous architecture, with limitations that will define how we will communicate with Newslabs' PSTT service.\\nThere will only be one shared bucket per environment for services using this service. The name of the bucket is \"newslabs-stt-media-to-transcribe\" and \"newslabs-stt-media-to-transcribe-test\".\\n### Limitations\\n- Newslabs pstt will only handle audio files (`mp3`, `mp4`, `wav`, `flac`)\\n- the client (dpe) need to ensure they upload audio not video\\n- do not use the eTag as the reference, use object key instead: `<service_name>\/<object_key>.<ext>`\\n- assume that the client is sending a unique object key\\n### Example\\n#### Upload\\nWhen uploading a file with Object Key: `280612.mp3`, the Object Key should be prepended with the service name: `dpe\/280612.mp3`\\n#### Status\\nThe endpoint for requesting the status of a transcription uses [this lambda](https:\/\/github.com\/bbc\/newslabs-stt\/tree\/master\/newslabs-stt-check-transcription), which returns the transcription status.\\nThere is an example response in the README.\\nMake a request to an API Gateway endpoint (please ask) with something like this in the request body:\\n```json\\n{\\n\"objectKey\": \"dpe\/uuid.ext\"\\n}\\n```\\n","Predictions":"We will use a stt-api-gateway to connect to the S3 bucket.n"}
{"File Name":"interlok\/0003-deprecate-dynamicservicelocator.md","Context":"## Context and Problem Statement\nWith the standard document trading network, there is a move towards centralised processing such that companies need only define their \"mapping specification\" and key information, and everything is handled centrally.\nTraditionally, this was done via DynamicServiceLocator with files located on the fileystem named in the form : `SRC-DST-MSG_TYPE.xml`; recently, this has been done in a dedicated application which is now deprecated.\nThe way that DynamicServiceLocator works contains a lot of extraneous configuration that only have a single implementation; it was designed for extensibility, but it's over complicated in terms of XML and coupling. It needs to be simplified so that it's more understandable in the UI.\n","Decision":"Deprecate DynamicServiceLocator, leave it available but marked as deprecated. Improve DynamicServiceExcutor so that it can be used instead.\\n### Enhancing DynamicServiceExecutor.\\nWhat this means is to implement additional ServiceExtractor implementations that allow the user to extract the _`_service-to-execute_ from an external location; currently the two supplied implementations simply use the _AdaptrisMessage_ object to extract the services.\\nThe additions required are :\\n* Extract from a URL such that you can configure `http:\/\/my.server.com\/%message{source}\/%message{destination}\/%message{messageType}.xml`. This would replace the existing DynamicServiceLocator functionality from _RemoteServiceStore_ and _LocalServiceStore_.\\n* Extract from Database -> such that you can configure something like `SELECT dynamicService FROM services WHERE src='%message{source}' AND dest='%message{destination}' AND msgType='%message{messageType}'`\\n* The above would be quite \"open\" to SQL injection style attacks; so people should probably use _JdbcDataQueryService_ and then handle it from metadata.\\n* Extract from a Cache -> similar to doing a RetrieveFromCache\\n* Extract it from metadata.\\n","tokens":146,"id":2342,"text":"## Context and Problem Statement\\nWith the standard document trading network, there is a move towards centralised processing such that companies need only define their \"mapping specification\" and key information, and everything is handled centrally.\\nTraditionally, this was done via DynamicServiceLocator with files located on the fileystem named in the form : `SRC-DST-MSG_TYPE.xml`; recently, this has been done in a dedicated application which is now deprecated.\\nThe way that DynamicServiceLocator works contains a lot of extraneous configuration that only have a single implementation; it was designed for extensibility, but it's over complicated in terms of XML and coupling. It needs to be simplified so that it's more understandable in the UI.\\n\n\n##Decision\nDeprecate DynamicServiceLocator, leave it available but marked as deprecated. Improve DynamicServiceExcutor so that it can be used instead.\\n### Enhancing DynamicServiceExecutor.\\nWhat this means is to implement additional ServiceExtractor implementations that allow the user to extract the _`_service-to-execute_ from an external location; currently the two supplied implementations simply use the _AdaptrisMessage_ object to extract the services.\\nThe additions required are :\\n* Extract from a URL such that you can configure `http:\/\/my.server.com\/%message{source}\/%message{destination}\/%message{messageType}.xml`. This would replace the existing DynamicServiceLocator functionality from _RemoteServiceStore_ and _LocalServiceStore_.\\n* Extract from Database -> such that you can configure something like `SELECT dynamicService FROM services WHERE src='%message{source}' AND dest='%message{destination}' AND msgType='%message{messageType}'`\\n* The above would be quite \"open\" to SQL injection style attacks; so people should probably use _JdbcDataQueryService_ and then handle it from metadata.\\n* Extract from a Cache -> similar to doing a RetrieveFromCache\\n* Extract it from metadata.\\n","Predictions":"We will use a Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security Security"}
{"File Name":"platform\/2021-09-22-refactor-theme-inheritance.md","Context":"## Context\nCurrently, the themes can only inherit config fields from the default Storefront theme.\nAlso, this inheritance is only a snapshot by activation time of the theme - The configs are copied to the new theme and changes to the default theme config will not appear in the new theme without a re-activation.\nThe different possibilities to inherit different parts of a theme, like scripts, templates and config, can also cause problems on later updates.\n","Decision":"To take this points into account, we have decided to add a new inheritance key for the `configFields` in the `theme.json` which allow a theme to inherit its config from other themes in a given order:\\n```json\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n```\\n<details>\\n<summary>Complete theme.json with part inheritances<\/summary>\\n```json\\n{\\n\"name\": \"MyDevelopmentTheme\",\\n\"author\": \"Shopware AG\",\\n\"views\": [\\n\"@Storefront\",\\n\"@Plugins\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"style\": [\\n\"app\/storefront\/src\/scss\/overrides.scss\",\\n\"@Storefront\",\\n\"app\/storefront\/src\/scss\/base.scss\"\\n],\\n\"script\": [\\n\"@Storefront\",\\n\"app\/storefront\/dist\/storefront\/js\/my-development-theme.js\"\\n],\\n\"asset\": [\\n\"@Storefront\",\\n\"app\/storefront\/src\/assets\"\\n],\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"config\": {\\n\"blocks\": {\\n\"exampleBlock\": {\\n\"label\": {\\n\"en-GB\": \"Example block\",\\n\"de-DE\": \"Beispiel Block\"\\n}\\n}\\n},\\n\"sections\": {\\n\"exampleSection\": {\\n\"label\": {\\n\"en-GB\": \"Example section\",\\n\"de-DE\": \"Beispiel Sektion\"\\n}\\n}\\n},\\n\"fields\": {\\n\"my-single-test-select-field\": {\\n\"editable\": false\\n},\\n\"my-single-select-field\": {\\n\"label\": {\\n\"en-GB\": \"Select a font size\",\\n\"de-DE\": \"W\u00e4hle ein Schriftgr\u00f6\u00dfe\"\\n},\\n\"type\": \"text\",\\n\"value\": \"24\",\\n\"custom\": {\\n\"componentName\": \"sw-single-select\",\\n\"options\": [\\n{\\n\"value\": \"16\",\\n\"label\": {\\n\"en-GB\": \"16px\",\\n\"de-DE\": \"16px\"\\n}\\n},\\n{\\n\"value\": \"20\",\\n\"label\": {\\n\"en-GB\": \"20px\",\\n\"de-DE\": \"20px\"\\n}\\n},\\n{\\n\"value\": \"24\",\\n\"label\": {\\n\"en-GB\": \"24px\",\\n\"de-DE\": \"24px\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n},\\n\"usps-positions\": {\\n\"label\":\\n{\\n\"en-GB\": \"Position\",\\n\"de-DE\": \"Position\"\\n},\\n\"scss\": true,\\n\"type\": \"text\",\\n\"value\": [\\n\"top\",\\n\"bottom\"\\n],\\n\"custom\": {\\n\"componentName\": \"sw-multi-select\",\\n\"options\": [\\n{\\n\"value\": \"bottom\",\\n\"label\":\\n{\\n\"en-GB\": \"bottom\",\\n\"de-DE\": \"unten\"\\n}\\n},\\n{\\n\"value\": \"top\",\\n\"label\":\\n{\\n\"en-GB\": \"top\",\\n\"de-DE\": \"oben\"\\n}\\n},\\n{\\n\"value\": \"middle\",\\n\"label\":\\n{\\n\"en-GB\": \"middle\",\\n\"de-DE\": \"mittel\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"tab\": \"usps\",\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n}\\n}\\n}\\n}\\n```\\n<\/details>\\n","tokens":90,"id":4498,"text":"## Context\\nCurrently, the themes can only inherit config fields from the default Storefront theme.\\nAlso, this inheritance is only a snapshot by activation time of the theme - The configs are copied to the new theme and changes to the default theme config will not appear in the new theme without a re-activation.\\nThe different possibilities to inherit different parts of a theme, like scripts, templates and config, can also cause problems on later updates.\\n\n\n##Decision\nTo take this points into account, we have decided to add a new inheritance key for the `configFields` in the `theme.json` which allow a theme to inherit its config from other themes in a given order:\\n```json\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n```\\n<details>\\n<summary>Complete theme.json with part inheritances<\/summary>\\n```json\\n{\\n\"name\": \"MyDevelopmentTheme\",\\n\"author\": \"Shopware AG\",\\n\"views\": [\\n\"@Storefront\",\\n\"@Plugins\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"style\": [\\n\"app\/storefront\/src\/scss\/overrides.scss\",\\n\"@Storefront\",\\n\"app\/storefront\/src\/scss\/base.scss\"\\n],\\n\"script\": [\\n\"@Storefront\",\\n\"app\/storefront\/dist\/storefront\/js\/my-development-theme.js\"\\n],\\n\"asset\": [\\n\"@Storefront\",\\n\"app\/storefront\/src\/assets\"\\n],\\n\"configInheritance\": [\\n\"@Storefront\",\\n\"@PreviousTheme\",\\n\"@MyDevelopmentTheme\"\\n],\\n\"config\": {\\n\"blocks\": {\\n\"exampleBlock\": {\\n\"label\": {\\n\"en-GB\": \"Example block\",\\n\"de-DE\": \"Beispiel Block\"\\n}\\n}\\n},\\n\"sections\": {\\n\"exampleSection\": {\\n\"label\": {\\n\"en-GB\": \"Example section\",\\n\"de-DE\": \"Beispiel Sektion\"\\n}\\n}\\n},\\n\"fields\": {\\n\"my-single-test-select-field\": {\\n\"editable\": false\\n},\\n\"my-single-select-field\": {\\n\"label\": {\\n\"en-GB\": \"Select a font size\",\\n\"de-DE\": \"W\u00e4hle ein Schriftgr\u00f6\u00dfe\"\\n},\\n\"type\": \"text\",\\n\"value\": \"24\",\\n\"custom\": {\\n\"componentName\": \"sw-single-select\",\\n\"options\": [\\n{\\n\"value\": \"16\",\\n\"label\": {\\n\"en-GB\": \"16px\",\\n\"de-DE\": \"16px\"\\n}\\n},\\n{\\n\"value\": \"20\",\\n\"label\": {\\n\"en-GB\": \"20px\",\\n\"de-DE\": \"20px\"\\n}\\n},\\n{\\n\"value\": \"24\",\\n\"label\": {\\n\"en-GB\": \"24px\",\\n\"de-DE\": \"24px\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n},\\n\"usps-positions\": {\\n\"label\":\\n{\\n\"en-GB\": \"Position\",\\n\"de-DE\": \"Position\"\\n},\\n\"scss\": true,\\n\"type\": \"text\",\\n\"value\": [\\n\"top\",\\n\"bottom\"\\n],\\n\"custom\": {\\n\"componentName\": \"sw-multi-select\",\\n\"options\": [\\n{\\n\"value\": \"bottom\",\\n\"label\":\\n{\\n\"en-GB\": \"bottom\",\\n\"de-DE\": \"unten\"\\n}\\n},\\n{\\n\"value\": \"top\",\\n\"label\":\\n{\\n\"en-GB\": \"top\",\\n\"de-DE\": \"oben\"\\n}\\n},\\n{\\n\"value\": \"middle\",\\n\"label\":\\n{\\n\"en-GB\": \"middle\",\\n\"de-DE\": \"mittel\"\\n}\\n}\\n]\\n},\\n\"editable\": true,\\n\"tab\": \"usps\",\\n\"block\": \"exampleBlock\",\\n\"section\": \"exampleSection\"\\n}\\n}\\n}\\n}\\n```\\n<\/details>\\n","Predictions":"The configInheritance field is a configField that is a previousTheme that is a MyDevelopmentTheme.nThe configFields field is a previousTheme that is a MyDevelopmentTheme.nThe configInheritance field is a previousTheme that is a MyDevelopmentTheme.nThe configFields field is a previousTheme that is a MyDevelopmentTheme.nThe configFields field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a previousTheme that is a MyDevelopmentTheme.nThe previousTheme field is a pre"}
{"File Name":"tendermint\/adr-051-double-signing-risk-reduction.md","Context":"## Context\nTo provide a risk reduction method for double signing incidents mistakenly executed by validators\n- Validators often mistakenly run duplicated validators to cause double-signing incident\n- This proposed feature is to reduce the risk of mistaken double-signing incident by checking recent N blocks before voting begins\n- When we think of such serious impact on double-signing incident, it is very reasonable to have multiple risk reduction algorithm built in node daemon\n","Decision":"We would like to suggest a double signing risk reduction method.\\n- Methodology : query recent consensus results to find out whether node's consensus key is used on consensus recently or not\\n- When to check\\n- When the state machine starts `ConsensusReactor` after fully synced\\n- When the node is validator ( with privValidator )\\n- When `cs.config.DoubleSignCheckHeight > 0`\\n- How to check\\n1. When a validator is transformed from syncing status to fully synced status, the state machine check recent N blocks (`latest_height - double_sign_check_height`) to find out whether there exists consensus votes using the validator's consensus key\\n2. If there exists votes from the validator's consensus key, exit state machine program\\n- Configuration\\n- We would like to suggest by introducing `double_sign_check_height` parameter in `config.toml` and cli, how many blocks state machine looks back to check votes\\n- <span v-pre>`double_sign_check_height = {{ .Consensus.DoubleSignCheckHeight }}`<\/span> in `config.toml`\\n- `tendermint node --consensus.double_sign_check_height` in cli\\n- State machine ignore checking procedure when `double_sign_check_height == 0`\\n","tokens":85,"id":1944,"text":"## Context\\nTo provide a risk reduction method for double signing incidents mistakenly executed by validators\\n- Validators often mistakenly run duplicated validators to cause double-signing incident\\n- This proposed feature is to reduce the risk of mistaken double-signing incident by checking recent N blocks before voting begins\\n- When we think of such serious impact on double-signing incident, it is very reasonable to have multiple risk reduction algorithm built in node daemon\\n\n\n##Decision\nWe would like to suggest a double signing risk reduction method.\\n- Methodology : query recent consensus results to find out whether node's consensus key is used on consensus recently or not\\n- When to check\\n- When the state machine starts `ConsensusReactor` after fully synced\\n- When the node is validator ( with privValidator )\\n- When `cs.config.DoubleSignCheckHeight > 0`\\n- How to check\\n1. When a validator is transformed from syncing status to fully synced status, the state machine check recent N blocks (`latest_height - double_sign_check_height`) to find out whether there exists consensus votes using the validator's consensus key\\n2. If there exists votes from the validator's consensus key, exit state machine program\\n- Configuration\\n- We would like to suggest by introducing `double_sign_check_height` parameter in `config.toml` and cli, how many blocks state machine looks back to check votes\\n- <span v-pre>`double_sign_check_height = {{ .Consensus.DoubleSignCheckHeight }}`<\/span> in `config.toml`\\n- `tendermint node --consensus.double_sign_check_height` in cli\\n- State machine ignore checking procedure when `double_sign_check_height == 0`\\n","Predictions":"We would like to suggest a double signing risk reduction method.n- Methodology : query recent consensus results to find out whether node's consensus key is used on consensus recently or notn- When to checkn- When the state machine starts ConsensusReactor after fully syncedn- When the node is validator ( with privValidator )n- When cs.config.DoubleSignCheckHeight > 0n- How to checkn1. When a validator is transformed from syncing status to fully synced status, the state machine check recent N blocks (latest_height - double_sign_check_height) to find out whether there exists consensus votes using the validator's consensus keyn2. If there exists votes from the validator's consensus key, exit state machine programn- Configurationn- We would like to suggest by introducing double_sign_check_height parameter in config.toml and cli, how many blocks state machine looks back to check votesn- span v-pre>double_sign_check_height = .Consensus.DoubleSignCheckHeight \/span> in config.tomln- tendermint node --consensus.double_sign_check_height in clin- State machine ignore checking procedure when double_sign_check_height == 0n"}
{"File Name":"caia\/0011-pin-mbtest-library-to-v2.5.1.md","Context":"## Context\nOn March 22, 2021, it was discovered that the \"caia\" Jenkins builds were\nfailing. All the failing tests were failing with the following error, related\nto a \"get_actual_requests\" method call:\n```\nTypeError: 'generator' object is not subscriptable\n```\nThe \"caia\" build was last successful in Jenkins on October 7, 2020. No builds\nwere performed again until March 22, 2021, as there was no development work\nbeing done on the project.\nBuilds were made on March 22, 2021 because of a move to the\n\"GitHub organization\" pipeline in LIBITD-1880, which triggered rebuilds in all\nexisting projects.\nWhen the last successful build was made in October, the \"mbtest\" library\n([https:\/\/github.com\/brunns\/mbtest](mbtest)) was at v2.5.1. In v2.5.2, the\n\"src\/mbtest\/server.py\" file was modified, changing the \"get_actual_requests\"\nmethod signature (see [this commit e398f2f1f32420](mbtest_commit)). from:\n```\ndef get_actual_requests(self) -> Mapping[int, JsonStructure]:\n```\nto\n```\ndef get_actual_requests(self) -> Iterable[Request]:\n```\nThe change from a Mapping to an Iterable is the cause of the error in the tests.\n","Decision":"The simplest solution for the moment is to \"pin\" the version of the \"mbtest\"\\nlibrary to v2.5.1 in the \"setup.py\" file. This will preserve the current\\nbehavior, until further \"caia\" development warrants additional testing.\\n","tokens":315,"id":3507,"text":"## Context\\nOn March 22, 2021, it was discovered that the \"caia\" Jenkins builds were\\nfailing. All the failing tests were failing with the following error, related\\nto a \"get_actual_requests\" method call:\\n```\\nTypeError: 'generator' object is not subscriptable\\n```\\nThe \"caia\" build was last successful in Jenkins on October 7, 2020. No builds\\nwere performed again until March 22, 2021, as there was no development work\\nbeing done on the project.\\nBuilds were made on March 22, 2021 because of a move to the\\n\"GitHub organization\" pipeline in LIBITD-1880, which triggered rebuilds in all\\nexisting projects.\\nWhen the last successful build was made in October, the \"mbtest\" library\\n([https:\/\/github.com\/brunns\/mbtest](mbtest)) was at v2.5.1. In v2.5.2, the\\n\"src\/mbtest\/server.py\" file was modified, changing the \"get_actual_requests\"\\nmethod signature (see [this commit e398f2f1f32420](mbtest_commit)). from:\\n```\\ndef get_actual_requests(self) -> Mapping[int, JsonStructure]:\\n```\\nto\\n```\\ndef get_actual_requests(self) -> Iterable[Request]:\\n```\\nThe change from a Mapping to an Iterable is the cause of the error in the tests.\\n\n\n##Decision\nThe simplest solution for the moment is to \"pin\" the version of the \"mbtest\"\\nlibrary to v2.5.1 in the \"setup.py\" file. This will preserve the current\\nbehavior, until further \"caia\" development warrants additional testing.\\n","Predictions":"The simplest solution for the moment is to \"pin\" the version of the \"mbtest\"nlibrary to v2.5.1 in the \"setup.py\" file. This will preserve the currentnbehavior, until further \"caia\" development warrants additional testing.n"}
{"File Name":"godspeed-you-blocked-developer\/003. Serverless.md","Context":"## Context\nServices become unique and fragile when you configure them by hand. I want to have a provisioning strategy before I actually start adding components, so I don't end up with behavior that's hard to reproduce.\nI like the principles in the book Infrastructure as Code. It suggests avoiding making changes to services except in an automated, configured manner.\nI've had some exposure to the infrastructure provisioning tool [Serverless](https:\/\/serverless.com), so I'd like to try using it on this project. Serverless abstracts AWS CloudFormation and similar tools from other cloud vendors.\n","Decision":"I will use Serverless to describe and provision my AWS components.\\n","tokens":119,"id":4937,"text":"## Context\\nServices become unique and fragile when you configure them by hand. I want to have a provisioning strategy before I actually start adding components, so I don't end up with behavior that's hard to reproduce.\\nI like the principles in the book Infrastructure as Code. It suggests avoiding making changes to services except in an automated, configured manner.\\nI've had some exposure to the infrastructure provisioning tool [Serverless](https:\/\/serverless.com), so I'd like to try using it on this project. Serverless abstracts AWS CloudFormation and similar tools from other cloud vendors.\\n\n\n##Decision\nI will use Serverless to describe and provision my AWS components.\\n","Predictions":"I will use a stack folder to store the configurations.n"}
{"File Name":"aws-sync-routes\/0005-uri.md","Context":"## Context\nThe requested functionality was to synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event, which would have limited information available to construct the request.\nThere isn't an obvious fit for this in the official REST API URI specifications.\n","Decision":"The `\/vpcs\/{vpcId}\/route-tables\/{routeTableId}` URI will be used.\\n* `{vpcId}` is the VPC ID.\\n* `{routeTableId}` is the main route table ID\\n","tokens":64,"id":1050,"text":"## Context\\nThe requested functionality was to synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event, which would have limited information available to construct the request.\\nThere isn't an obvious fit for this in the official REST API URI specifications.\\n\n\n##Decision\nThe `\/vpcs\/{vpcId}\/route-tables\/{routeTableId}` URI will be used.\\n* `{vpcId}` is the VPC ID.\\n* `{routeTableId}` is the main route table ID\\n","Predictions":"We decided to use the first option (base 64 encoded uploads), for the following reason:n- We do not want to expose any AWS links to outside parties. This ensures that all access to AWS resources is controlled by Hackney authentication mechanisms.n"}
{"File Name":"php-docker-template\/0004-default-php-settings-memory-limit.md","Context":"## Context\nThis set of Docker images are opinionated and meant to run within a Docker orchestrator, kubernetes for instance.\nSince most (if not all) the orchestrators have resource management built-in there are certain PHP settings which can be tweaked to make use of them, if PHP has memory limits itself, it'll die as a fatal error, in which case the orchestrator would be unaware that it's actually a out of memory situation.\n","Decision":"Set php ini configuration to have `memory_limit = -1`, this will affect both fpm and cli processes since it's added in the `default.ini` file of this repository.\\n","tokens":92,"id":5235,"text":"## Context\\nThis set of Docker images are opinionated and meant to run within a Docker orchestrator, kubernetes for instance.\\nSince most (if not all) the orchestrators have resource management built-in there are certain PHP settings which can be tweaked to make use of them, if PHP has memory limits itself, it'll die as a fatal error, in which case the orchestrator would be unaware that it's actually a out of memory situation.\\n\n\n##Decision\nSet php ini configuration to have `memory_limit = -1`, this will affect both fpm and cli processes since it's added in the `default.ini` file of this repository.\\n","Predictions":"We will use the Storefront storefront bundle to configure the storefront configuration.n"}
{"File Name":"monitoring-doc\/0003-use-ecs-for-initial-beta-buildout.md","Context":"## Context\nExisting self-hosted infrastructure at GDS has been managed in code\nusing tools like puppet, but in a somewhat ad hoc way with each team\ndoing things differently, little sharing of code, and much reinvention\nof wheels.  We would like to learn about other ways of deploying\ninfrastructure which encourage consistency: in terms of code\nartifacts, configuration methods, and such like.\nSystems such as Kubernetes and Amazon ECS are coalescing around Docker\nas a standard for packaging software and managing configuration.\n","Decision":"We will build our initial prometheus beta in Amazon ECS, and assess\\nhow effective it is.  We will review this decision once we have learnt\\nmore about both prometheus and ECS.\\n","tokens":110,"id":1376,"text":"## Context\\nExisting self-hosted infrastructure at GDS has been managed in code\\nusing tools like puppet, but in a somewhat ad hoc way with each team\\ndoing things differently, little sharing of code, and much reinvention\\nof wheels.  We would like to learn about other ways of deploying\\ninfrastructure which encourage consistency: in terms of code\\nartifacts, configuration methods, and such like.\\nSystems such as Kubernetes and Amazon ECS are coalescing around Docker\\nas a standard for packaging software and managing configuration.\\n\n\n##Decision\nWe will build our initial prometheus beta in Amazon ECS, and assess\\nhow effective it is.  We will review this decision once we have learnt\\nmore about both prometheus and ECS.\\n","Predictions":"We will use Docker to deploy our initial prometheus beta in Amazon ECS, and assessnhow effective it is. We will review this decision once we have learntnmore about both prometheus and ECS.n"}
{"File Name":"early-careers-framework\/0003-separate-ecf-and-npq-calculation-engines-and-hashes-as-interfaces.md","Context":"## Context\nThere are currently two different training schemes in scope for the track-and-pay project. There are rumours of a possible third in the future.\nThere are some similarities and many differences in the inputs, outputs and maths for payments for these training schemes. For example:\n1. Both have fixed payments at 40%\n2. Both allow pulling some of the fixed payment into earlier \"setup\" payments (for cashflow).\n3. The output payments are very different in detail.\n4. They have different banding systems.\n### People\nTim Abell and Pavel Lisovin (track and pay developers) discussed the issue amongst ourselves and came to this as a decision for the time being.\n","Decision":"1. Build two payment engines that do not share code.\\n2. Have similar input\/output interfaces (ruby hash structures) that can later be easily converted to JSON.\\n3. Use similar patterns for both engines of Gherkin BDD driven unit tests plus normal rspec unit tests.\\n","tokens":146,"id":4277,"text":"## Context\\nThere are currently two different training schemes in scope for the track-and-pay project. There are rumours of a possible third in the future.\\nThere are some similarities and many differences in the inputs, outputs and maths for payments for these training schemes. For example:\\n1. Both have fixed payments at 40%\\n2. Both allow pulling some of the fixed payment into earlier \"setup\" payments (for cashflow).\\n3. The output payments are very different in detail.\\n4. They have different banding systems.\\n### People\\nTim Abell and Pavel Lisovin (track and pay developers) discussed the issue amongst ourselves and came to this as a decision for the time being.\\n\n\n##Decision\n1. Build two payment engines that do not share code.\\n2. Have similar input\/output interfaces (ruby hash structures) that can later be easily converted to JSON.\\n3. Use similar patterns for both engines of Gherkin BDD driven unit tests plus normal rspec unit tests.\\n","Predictions":"We will use a Sign-On method to sign on to the app.n"}
{"File Name":"LogLady\/0006-use-electron-updater-autoupdate.md","Context":"## Context\nThere are multiple packages available for handling auto-update on Electron apps, and one was already installed in the project but not used - which package should the project use?\n","Decision":"We will use [electron-updater](https:\/\/github.com\/electron-userland\/electron-builder) as the package for auto-update.\\nThe package already installed in the project was replaced. It was decided earlier to use it instead of electron-updater because support for platforms was limited - but with new versions that has been added.\\nElectron-updater is the most popular package for auto-updating Electron apps, with more weekly downloads on npm and more dependents than the package that was already installed.\\nOverall, electron-updater seems like a more reliable package for handling auto-update.\\n","tokens":36,"id":1852,"text":"## Context\\nThere are multiple packages available for handling auto-update on Electron apps, and one was already installed in the project but not used - which package should the project use?\\n\n\n##Decision\nWe will use [electron-updater](https:\/\/github.com\/electron-userland\/electron-builder) as the package for auto-update.\\nThe package already installed in the project was replaced. It was decided earlier to use it instead of electron-updater because support for platforms was limited - but with new versions that has been added.\\nElectron-updater is the most popular package for auto-updating Electron apps, with more weekly downloads on npm and more dependents than the package that was already installed.\\nOverall, electron-updater seems like a more reliable package for handling auto-update.\\n","Predictions":"We will use [electron-updater](https:\/\/github.com\/electron-userland\/electron-builder) as the package for auto-update.nThe package already installed in the project was replaced. It was decided earlier to use it instead of electron-updater because support for platforms was limited - but with new versions that has been added.nElectron-updater is the most popular package for auto-update on Electron apps, with more weekly downloads on npm and more dependents than the package that was already installed.nElectron-updater seems like a more reliable package for handling auto-update.n"}
{"File Name":"paas-csls-splunk-broker\/ADR004-deploy-broker-as-lambda.md","Context":"## Context\nWe need to deploy the Broker somewhere.\nThe Broker implements the service broker API to generate per-application syslog\ndrain URLs (Adapter URLs).\nThe Adapter is written in Go.\nThe Broker is written in Go.\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\n","Decision":"We will deploy the Broker as an AWS Lambda\\n","tokens":77,"id":3295,"text":"## Context\\nWe need to deploy the Broker somewhere.\\nThe Broker implements the service broker API to generate per-application syslog\\ndrain URLs (Adapter URLs).\\nThe Adapter is written in Go.\\nThe Broker is written in Go.\\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\\n\n\n##Decision\nWe will deploy the Broker as an AWS Lambda\\n","Predictions":"We will use the [RabbitMQ Connector](https:\/\/www.rabbitmq.org\/) to connect to the Exchange.n"}
{"File Name":"cena\/0014-adopt-commmand-query-separation.md","Context":"## Context\n[Command-query separation](https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html) states that every method should\neither be a command that performs an action, or a query that returns data to the caller, but not both.\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\nuse queries with much more confidence, and only be careful with commands orchestration.\nCommands and queries terminology is already used in the `menu-generation` application.\n","Decision":"Command-query separation will be enforced in the [core hexagon](.\/0003-adopt-hexagonal-architecture.md), especially in\\napplication services.\\n","tokens":113,"id":647,"text":"## Context\\n[Command-query separation](https:\/\/martinfowler.com\/bliki\/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n\n\n##Decision\nCommand-query separation will be enforced in the [core hexagon](.\/0003-adopt-hexagonal-architecture.md), especially in\\napplication services.\\n","Predictions":"We will use a ContentType property to store the data.n"}
{"File Name":"gatemint-sdk\/adr-004-split-denomination-keys.md","Context":"## Context\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5467) and [4982](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4982) for additional context.\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\n","Decision":"Balances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\\n### Account interface (x\/auth)\\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\\nnow be stored in & managed by the bank module.\\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\\naccount balance.\\nVesting accounts will continue to store original vesting, delegated free, and delegated\\nvesting coins (which is safe since these cannot contain arbitrary denominations).\\n### Bank keeper (x\/bank)\\nThe following APIs will be added to the `x\/bank` keeper:\\n- `GetAllBalances(ctx Context, addr AccAddress) Coins`\\n- `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\\n- `SetBalance(ctx Context, addr AccAddress, coin Coin)`\\n- `LockedCoins(ctx Context, addr AccAddress) Coins`\\n- `SpendableCoins(ctx Context, addr AccAddress) Coins`\\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\\ncore functionality or persistence.\\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\\nbut retrieval of all balances for a single account is presumed to be more frequent):\\n```golang\\nvar BalancesPrefix = []byte(\"balances\")\\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\\nif !balance.IsValid() {\\nreturn err\\n}\\nstore := ctx.KVStore(k.storeKey)\\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\\nbz := Marshal(balance)\\naccountStore.Set([]byte(balance.Denom), bz)\\nreturn nil\\n}\\n```\\nThis will result in the balances being indexed by the byte representation of\\n`balances\/{address}\/{denom}`.\\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\\naccount balance by denomination found in the (un)delegation amount. As a result,\\nany mutations to the account balance by will made by denomination.\\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\\ndirectly instead of calling `GetCoins()` \/ `SetCoins()` (which no longer exist).\\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\\naccount balances.\\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\\npossible as to not load the entire account balance.\\n### Supply module\\nThe supply module, in order to implement the total supply invariant, will now need\\nto scan all accounts & call `GetAllBalances` using the `x\/bank` Keeper, then sum\\nthe balances and check that they match the expected total supply.\\n","tokens":175,"id":33,"text":"## Context\\nWith permissionless IBC, anyone will be able to send arbitrary denominations to any other account. Currently, all non-zero balances are stored along with the account in an `sdk.Coins` struct, which creates a potential denial-of-service concern, as too many denominations will become expensive to load & store each time the account is modified. See issues [5467](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/5467) and [4982](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/4982) for additional context.\\nSimply rejecting incoming deposits after a denomination count limit doesn't work, since it opens up a griefing vector: someone could send a user lots of nonsensical coins over IBC, and then prevent the user from receiving real denominations (such as staking rewards).\\n\n\n##Decision\nBalances shall be stored per-account & per-denomination under a denomination- and account-unique key, thus enabling O(1) read & write access to the balance of a particular account in a particular denomination.\\n### Account interface (x\/auth)\\n`GetCoins()` and `SetCoins()` will be removed from the account interface, since coin balances will\\nnow be stored in & managed by the bank module.\\nThe vesting account interface will replace `SpendableCoins` in favor of `LockedCoins` which does\\nnot require the account balance anymore. In addition, `TrackDelegation()`  will now accept the\\naccount balance of all tokens denominated in the vesting balance instead of loading the entire\\naccount balance.\\nVesting accounts will continue to store original vesting, delegated free, and delegated\\nvesting coins (which is safe since these cannot contain arbitrary denominations).\\n### Bank keeper (x\/bank)\\nThe following APIs will be added to the `x\/bank` keeper:\\n- `GetAllBalances(ctx Context, addr AccAddress) Coins`\\n- `GetBalance(ctx Context, addr AccAddress, denom string) Coin`\\n- `SetBalance(ctx Context, addr AccAddress, coin Coin)`\\n- `LockedCoins(ctx Context, addr AccAddress) Coins`\\n- `SpendableCoins(ctx Context, addr AccAddress) Coins`\\nAdditional APIs may be added to facilitate iteration and auxiliary functionality not essential to\\ncore functionality or persistence.\\nBalances will be stored first by the address, then by the denomination (the reverse is also possible,\\nbut retrieval of all balances for a single account is presumed to be more frequent):\\n```golang\\nvar BalancesPrefix = []byte(\"balances\")\\nfunc (k Keeper) SetBalance(ctx Context, addr AccAddress, balance Coin) error {\\nif !balance.IsValid() {\\nreturn err\\n}\\nstore := ctx.KVStore(k.storeKey)\\nbalancesStore := prefix.NewStore(store, BalancesPrefix)\\naccountStore := prefix.NewStore(balancesStore, addr.Bytes())\\nbz := Marshal(balance)\\naccountStore.Set([]byte(balance.Denom), bz)\\nreturn nil\\n}\\n```\\nThis will result in the balances being indexed by the byte representation of\\n`balances\/{address}\/{denom}`.\\n`DelegateCoins()` and `UndelegateCoins()` will be altered to only load each individual\\naccount balance by denomination found in the (un)delegation amount. As a result,\\nany mutations to the account balance by will made by denomination.\\n`SubtractCoins()` and `AddCoins()` will be altered to read & write the balances\\ndirectly instead of calling `GetCoins()` \/ `SetCoins()` (which no longer exist).\\n`trackDelegation()` and `trackUndelegation()` will be altered to no longer update\\naccount balances.\\nExternal APIs will need to scan all balances under an account to retain backwards-compatibility. It\\nis advised that these APIs use `GetBalance` and `SetBalance` instead of `GetAllBalances` when\\npossible as to not load the entire account balance.\\n### Supply module\\nThe supply module, in order to implement the total supply invariant, will now need\\nto scan all accounts & call `GetAllBalances` using the `x\/bank` Keeper, then sum\\nthe balances and check that they match the expected total supply.\\n","Predictions":"The GetAllBalances API will be used to store denominations.nThe GetAllDentures API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes API will be used to store denominations.nThe GetStakes"}
{"File Name":"ethnicity-facts-and-figures-publisher\/0002-use-heroku-run-detached-for-static-site-build.md","Context":"## Context\nAt the moment, we have a set of scheduled tasks on Heroku that run every 10 minutes, hour, or day. These range from building the static site (every 10 minutes), to reporting stalled builds (every hour), to refreshing the database from production data (in dev\/staging, every night). The main job here, building the static site, has started to fail because it occasionally runs longer than 10 minutes. The Heroku scheduler will only allow a scheduled instance to live for as long as the interval on the task. Heroku is therefore killing the instance before it can finish, leading to the incomplete builds and frequent reports of 'stalled' (in this instance, killed) builds.\nWe need to schedule static site builds in a way that removes this strict time constraint so that they will always finish building.\n","Decision":"We will continue to use the Heroku scheduler on a ten-minute interval, but will instead use the Heroku CLI to run the management command (.\/manage.py build_static_site) in a separate, detached worker dyno. This worker dyno has a lifespan of 24 hours and is not limited to the interval of the scheduled job.\\nThe command we will use is `heroku run:detached -a <app_name> .\/manage.py build_static_site`.\\nPros\\n----\\n* Very easy to implement.\\n* Will allow static site builds that take up to 24 hours (which is greater than any reasonable build length).\\nCons\\n----\\n* We remain unable to responsively schedule static site builds at the moment they are needed and rely on the scheduler to kick off a build.\\n* We have to expose a Heroku API key and install the Heroku CLI buildpack on our main Publisher app.\\n","tokens":172,"id":3361,"text":"## Context\\nAt the moment, we have a set of scheduled tasks on Heroku that run every 10 minutes, hour, or day. These range from building the static site (every 10 minutes), to reporting stalled builds (every hour), to refreshing the database from production data (in dev\/staging, every night). The main job here, building the static site, has started to fail because it occasionally runs longer than 10 minutes. The Heroku scheduler will only allow a scheduled instance to live for as long as the interval on the task. Heroku is therefore killing the instance before it can finish, leading to the incomplete builds and frequent reports of 'stalled' (in this instance, killed) builds.\\nWe need to schedule static site builds in a way that removes this strict time constraint so that they will always finish building.\\n\n\n##Decision\nWe will continue to use the Heroku scheduler on a ten-minute interval, but will instead use the Heroku CLI to run the management command (.\/manage.py build_static_site) in a separate, detached worker dyno. This worker dyno has a lifespan of 24 hours and is not limited to the interval of the scheduled job.\\nThe command we will use is `heroku run:detached -a <app_name> .\/manage.py build_static_site`.\\nPros\\n----\\n* Very easy to implement.\\n* Will allow static site builds that take up to 24 hours (which is greater than any reasonable build length).\\nCons\\n----\\n* We remain unable to responsively schedule static site builds at the moment they are needed and rely on the scheduler to kick off a build.\\n* We have to expose a Heroku API key and install the Heroku CLI buildpack on our main Publisher app.\\n","Predictions":"We will continue to use the Heroku scheduler on a ten-minute interval, but will instead use the Heroku CLI to run the management command (.\/manage.py build_static_site) in a separate, detached worker dyno. This worker dyno has a lifespan of 24 hours and is not limited to the interval of the scheduled job.nThe command we will use is heroku run:detached -a app_name>.\/manage.py build_static_site.nProsn------n* Very easy to implement.n* Will allow static site builds that take up to 24 hours (which is greater than any reasonable build length).nConsn------n* We remain unable to responsively schedule static site builds at the moment they are needed and rely on the scheduler to kick off a build.nWe have to expose a Heroku API key and install the Heroku CLI buildpack on our main Publisher app.n"}
{"File Name":"cosmos-sdk\/adr-032-typed-events.md","Context":"## Context\nCurrently in the Cosmos SDK, events are defined in the handlers for each message, meaning each module doesn't have a canonical set of types for each event. Above all else this makes these events difficult to consume as it requires a great deal of raw string matching and parsing. This proposal focuses on updating the events to use **typed events** defined in each module such that emitting and subscribing to events will be much easier. This workflow comes from the experience of the Akash Network team.\n[Our platform](http:\/\/github.com\/ovrclk\/akash) requires a number of programmatic on chain interactions both on the provider (datacenter - to bid on new orders and listen for leases created) and user (application developer - to send the app manifest to the provider) side. In addition the Akash team is now maintaining the IBC [`relayer`](https:\/\/github.com\/ovrclk\/relayer), another very event driven process. In working on these core pieces of infrastructure, and integrating lessons learned from Kubernetes development, our team has developed a standard method for defining and consuming typed events in Cosmos SDK modules. We have found that it is extremely useful in building this type of event driven application.\nAs the Cosmos SDK gets used more extensively for apps like `peggy`, other peg zones, IBC, DeFi, etc... there will be an exploding demand for event driven applications to support new features desired by users. We propose upstreaming our findings into the Cosmos SDK to enable all Cosmos SDK applications to quickly and easily build event driven apps to aid their core application. Wallets, exchanges, explorers, and defi protocols all stand to benefit from this work.\nIf this proposal is accepted, users will be able to build event driven Cosmos SDK apps in go by just writing `EventHandler`s for their specific event types and passing them to `EventEmitters` that are defined in the Cosmos SDK.\nThe end of this proposal contains a detailed example of how to consume events after this refactor.\nThis proposal is specifically about how to consume these events as a client of the blockchain, not for intermodule communication.\n","Decision":"**Step-1**:  Implement additional functionality in the `types` package: `EmitTypedEvent` and `ParseTypedEvent` functions\\n```go\\n\/\/ types\/events.go\\n\/\/ EmitTypedEvent takes typed event and emits converting it into sdk.Event\\nfunc (em *EventManager) EmitTypedEvent(event proto.Message) error {\\nevtType := proto.MessageName(event)\\nevtJSON, err := codec.ProtoMarshalJSON(event)\\nif err != nil {\\nreturn err\\n}\\nvar attrMap map[string]json.RawMessage\\nerr = json.Unmarshal(evtJSON, &attrMap)\\nif err != nil {\\nreturn err\\n}\\nvar attrs []abci.EventAttribute\\nfor k, v := range attrMap {\\nattrs = append(attrs, abci.EventAttribute{\\nKey:   []byte(k),\\nValue: v,\\n})\\n}\\nem.EmitEvent(Event{\\nType:       evtType,\\nAttributes: attrs,\\n})\\nreturn nil\\n}\\n\/\/ ParseTypedEvent converts abci.Event back to typed event\\nfunc ParseTypedEvent(event abci.Event) (proto.Message, error) {\\nconcreteGoType := proto.MessageType(event.Type)\\nif concreteGoType == nil {\\nreturn nil, fmt.Errorf(\"failed to retrieve the message of type %q\", event.Type)\\n}\\nvar value reflect.Value\\nif concreteGoType.Kind() == reflect.Ptr {\\nvalue = reflect.New(concreteGoType.Elem())\\n} else {\\nvalue = reflect.Zero(concreteGoType)\\n}\\nprotoMsg, ok := value.Interface().(proto.Message)\\nif !ok {\\nreturn nil, fmt.Errorf(\"%q does not implement proto.Message\", event.Type)\\n}\\nattrMap := make(map[string]json.RawMessage)\\nfor _, attr := range event.Attributes {\\nattrMap[string(attr.Key)] = attr.Value\\n}\\nattrBytes, err := json.Marshal(attrMap)\\nif err != nil {\\nreturn nil, err\\n}\\nerr = jsonpb.Unmarshal(strings.NewReader(string(attrBytes)), protoMsg)\\nif err != nil {\\nreturn nil, err\\n}\\nreturn protoMsg, nil\\n}\\n```\\nHere, the `EmitTypedEvent` is a method on `EventManager` which takes typed event as input and apply json serialization on it. Then it maps the JSON key\/value pairs to `event.Attributes` and emits it in form of `sdk.Event`. `Event.Type` will be the type URL of the proto message.\\nWhen we subscribe to emitted events on the CometBFT websocket, they are emitted in the form of an `abci.Event`. `ParseTypedEvent` parses the event back to it's original proto message.\\n**Step-2**: Add proto definitions for typed events for msgs in each module:\\nFor example, let's take `MsgSubmitProposal` of `gov` module and implement this event's type.\\n```protobuf\\n\/\/ proto\/cosmos\/gov\/v1beta1\/gov.proto\\n\/\/ Add typed event definition\\npackage cosmos.gov.v1beta1;\\nmessage EventSubmitProposal {\\nstring from_address   = 1;\\nuint64 proposal_id    = 2;\\nTextProposal proposal = 3;\\n}\\n```\\n**Step-3**: Refactor event emission to use the typed event created and emit using `sdk.EmitTypedEvent`:\\n```go\\n\/\/ x\/gov\/handler.go\\nfunc handleMsgSubmitProposal(ctx sdk.Context, keeper keeper.Keeper, msg types.MsgSubmitProposalI) (*sdk.Result, error) {\\n...\\ntypes.Context.EventManager().EmitTypedEvent(\\n&EventSubmitProposal{\\nFromAddress: fromAddress,\\nProposalId: id,\\nProposal: proposal,\\n},\\n)\\n...\\n}\\n```\\n### How to subscribe to these typed events in `Client`\\n> NOTE: Full code example below\\nUsers will be able to subscribe using `client.Context.Client.Subscribe` and consume events which are emitted using `EventHandler`s.\\nAkash Network has built a simple [`pubsub`](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/pubsub\/bus.go#L20). This can be used to subscribe to `abci.Events` and [publish](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/events\/publish.go#L21) them as typed events.\\nPlease see the below code sample for more detail on this flow looks for clients.\\n","tokens":433,"id":842,"text":"## Context\\nCurrently in the Cosmos SDK, events are defined in the handlers for each message, meaning each module doesn't have a canonical set of types for each event. Above all else this makes these events difficult to consume as it requires a great deal of raw string matching and parsing. This proposal focuses on updating the events to use **typed events** defined in each module such that emitting and subscribing to events will be much easier. This workflow comes from the experience of the Akash Network team.\\n[Our platform](http:\/\/github.com\/ovrclk\/akash) requires a number of programmatic on chain interactions both on the provider (datacenter - to bid on new orders and listen for leases created) and user (application developer - to send the app manifest to the provider) side. In addition the Akash team is now maintaining the IBC [`relayer`](https:\/\/github.com\/ovrclk\/relayer), another very event driven process. In working on these core pieces of infrastructure, and integrating lessons learned from Kubernetes development, our team has developed a standard method for defining and consuming typed events in Cosmos SDK modules. We have found that it is extremely useful in building this type of event driven application.\\nAs the Cosmos SDK gets used more extensively for apps like `peggy`, other peg zones, IBC, DeFi, etc... there will be an exploding demand for event driven applications to support new features desired by users. We propose upstreaming our findings into the Cosmos SDK to enable all Cosmos SDK applications to quickly and easily build event driven apps to aid their core application. Wallets, exchanges, explorers, and defi protocols all stand to benefit from this work.\\nIf this proposal is accepted, users will be able to build event driven Cosmos SDK apps in go by just writing `EventHandler`s for their specific event types and passing them to `EventEmitters` that are defined in the Cosmos SDK.\\nThe end of this proposal contains a detailed example of how to consume events after this refactor.\\nThis proposal is specifically about how to consume these events as a client of the blockchain, not for intermodule communication.\\n\n\n##Decision\n**Step-1**:  Implement additional functionality in the `types` package: `EmitTypedEvent` and `ParseTypedEvent` functions\\n```go\\n\/\/ types\/events.go\\n\/\/ EmitTypedEvent takes typed event and emits converting it into sdk.Event\\nfunc (em *EventManager) EmitTypedEvent(event proto.Message) error {\\nevtType := proto.MessageName(event)\\nevtJSON, err := codec.ProtoMarshalJSON(event)\\nif err != nil {\\nreturn err\\n}\\nvar attrMap map[string]json.RawMessage\\nerr = json.Unmarshal(evtJSON, &attrMap)\\nif err != nil {\\nreturn err\\n}\\nvar attrs []abci.EventAttribute\\nfor k, v := range attrMap {\\nattrs = append(attrs, abci.EventAttribute{\\nKey:   []byte(k),\\nValue: v,\\n})\\n}\\nem.EmitEvent(Event{\\nType:       evtType,\\nAttributes: attrs,\\n})\\nreturn nil\\n}\\n\/\/ ParseTypedEvent converts abci.Event back to typed event\\nfunc ParseTypedEvent(event abci.Event) (proto.Message, error) {\\nconcreteGoType := proto.MessageType(event.Type)\\nif concreteGoType == nil {\\nreturn nil, fmt.Errorf(\"failed to retrieve the message of type %q\", event.Type)\\n}\\nvar value reflect.Value\\nif concreteGoType.Kind() == reflect.Ptr {\\nvalue = reflect.New(concreteGoType.Elem())\\n} else {\\nvalue = reflect.Zero(concreteGoType)\\n}\\nprotoMsg, ok := value.Interface().(proto.Message)\\nif !ok {\\nreturn nil, fmt.Errorf(\"%q does not implement proto.Message\", event.Type)\\n}\\nattrMap := make(map[string]json.RawMessage)\\nfor _, attr := range event.Attributes {\\nattrMap[string(attr.Key)] = attr.Value\\n}\\nattrBytes, err := json.Marshal(attrMap)\\nif err != nil {\\nreturn nil, err\\n}\\nerr = jsonpb.Unmarshal(strings.NewReader(string(attrBytes)), protoMsg)\\nif err != nil {\\nreturn nil, err\\n}\\nreturn protoMsg, nil\\n}\\n```\\nHere, the `EmitTypedEvent` is a method on `EventManager` which takes typed event as input and apply json serialization on it. Then it maps the JSON key\/value pairs to `event.Attributes` and emits it in form of `sdk.Event`. `Event.Type` will be the type URL of the proto message.\\nWhen we subscribe to emitted events on the CometBFT websocket, they are emitted in the form of an `abci.Event`. `ParseTypedEvent` parses the event back to it's original proto message.\\n**Step-2**: Add proto definitions for typed events for msgs in each module:\\nFor example, let's take `MsgSubmitProposal` of `gov` module and implement this event's type.\\n```protobuf\\n\/\/ proto\/cosmos\/gov\/v1beta1\/gov.proto\\n\/\/ Add typed event definition\\npackage cosmos.gov.v1beta1;\\nmessage EventSubmitProposal {\\nstring from_address   = 1;\\nuint64 proposal_id    = 2;\\nTextProposal proposal = 3;\\n}\\n```\\n**Step-3**: Refactor event emission to use the typed event created and emit using `sdk.EmitTypedEvent`:\\n```go\\n\/\/ x\/gov\/handler.go\\nfunc handleMsgSubmitProposal(ctx sdk.Context, keeper keeper.Keeper, msg types.MsgSubmitProposalI) (*sdk.Result, error) {\\n...\\ntypes.Context.EventManager().EmitTypedEvent(\\n&EventSubmitProposal{\\nFromAddress: fromAddress,\\nProposalId: id,\\nProposal: proposal,\\n},\\n)\\n...\\n}\\n```\\n### How to subscribe to these typed events in `Client`\\n> NOTE: Full code example below\\nUsers will be able to subscribe using `client.Context.Client.Subscribe` and consume events which are emitted using `EventHandler`s.\\nAkash Network has built a simple [`pubsub`](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/pubsub\/bus.go#L20). This can be used to subscribe to `abci.Events` and [publish](https:\/\/github.com\/ovrclk\/akash\/blob\/90d258caeb933b611d575355b8df281208a214f8\/events\/publish.go#L21) them as typed events.\\nPlease see the below code sample for more detail on this flow looks for clients.\\n","Predictions":"We will use the EventHandler method to define the type of events that are sent to the EventEmitters.nThis will allow us to easily define the types of events that are sent to the EventHandler and to easily add them to the EventEmitters.nThis will also allow us to easily add a new EventHandler to the EventHandler and to add a new EventEmitters to the EventHandler.nThis will also allow us to easily add a new EventHandler to the EventHandler and to add a new EventEmitters to the EventHandler.nThis will also allow us to easily add a new EventHandler to the EventHandler and to add a new EventEmitters to the EventHandler.nThis will also allow us to easily add a new EventHandler to the EventHandler and to add a new EventEmitters to the EventHandler.nThis will also allow us to easily add a new EventHandler to the EventHandler and to add a new EventEmitters to the EventHandler.nThis will also allow us to easily add a new EventHandler to the EventHandler and to add a new EventEmitters to the EventHandler.nThis will also allow us to easily add a new EventHandler to the EventHandler and to add a new EventEmitters to the EventHandler.nThis will also allow us to add a new EventHandler to the EventHandler and to add a new EventEmitters to the EventHandler.nThis will also allow us to add a new EventHandler to the EventHandler and to add a new EventEmitters to the EventHandler.nThis will also allow us to add a new EventHandler to the EventHandler and to add a new EventEmitters to the EventHandler.nThis will also allow us to add a new EventHandler to the EventHandler and to add a new EventEmitters to the EventHandler.nThis will also allow us to add a new EventHandler to the EventHandler and to add a new EventEmitters to the EventHandler.nThis will also allow us to add a new EventHandler to the EventHandler and to add a new EventEmitters to the EventHandler.nThis will also allow us to add a new EventHandler to the EventHandler"}
{"File Name":"jabref\/0002-use-slf4j-for-logging.md","Context":"## Context and Problem Statement\nUp to version 4.1 JabRef uses apache-commons-logging 1.2 for logging errors and messages. However, this is not compatible with java 9 and is superseded by log4j.\n## Decision Drivers\n* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\n","Decision":"* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\nChosen option: \"SLF4J with Log4j2 binding\", because comes out best \\(see below\\).\\n","tokens":104,"id":4742,"text":"## Context and Problem Statement\\nUp to version 4.1 JabRef uses apache-commons-logging 1.2 for logging errors and messages. However, this is not compatible with java 9 and is superseded by log4j.\\n## Decision Drivers\\n* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\n\n\n##Decision\n* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9\\n* Log4j is already defined as dependency and slf4j has already been required by a third party dependency\\nChosen option: \"SLF4J with Log4j2 binding\", because comes out best \\(see below\\).\\n","Predictions":"* SLF4J provides a fa\u00e7ade for several logging frameworks, including log4j and supports already java 9n* Log4j is already defined as dependency and slf4j has already been required by a third party dependencynChosen option: \"SLF4J with Log4j2 binding\", because comes out best (see below).n"}
{"File Name":"wordpress-template\/0003-theme-namespace-naming-convention.md","Context":"## Context\nWe need to pick a namespace for themes, and renaming can be tedious.\nOnly one theme is ever active at a time, so there's no risk in naming collisions except when dealing with child themes.\n","Decision":"We'll use the `\\Theme` namespace for every theme we make except for child themes which will have the namespace `\\ChildTheme`.\\n","tokens":45,"id":2262,"text":"## Context\\nWe need to pick a namespace for themes, and renaming can be tedious.\\nOnly one theme is ever active at a time, so there's no risk in naming collisions except when dealing with child themes.\\n\n\n##Decision\nWe'll use the `\\Theme` namespace for every theme we make except for child themes which will have the namespace `\\ChildTheme`.\\n","Predictions":"* We will use a [stackable](https:\/\/github.com\/klokwrk-project\/stackable\/) format for writing commit messages.**nConventional commits format is nice and short and defines the simple structure that is easy to learn and follow. Here is basic structure of our customized conventional commits format:ntype>(optional scope>): description> optional metadata>nOur customization:n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96b9\/CONTRIBUTING.md#-commit-mess)ntype>(optional scope>): description> optional metadata>nOur customization:n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96b9\/CONTRIBUTING.md#-commit-mess)ntype>(optional scope>): description> optional metadata>nOur customization:n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96b9\/CONTRIBUTING.md#-commit-mess)ntype>(optional scope>): description> optional metadata>nOur customization:n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96b9\/CONTRIBUTING.md#-commit-mess)ntype>(optional scope>): description> optional metadata>nOur customization:n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96b9\/CONTRIBUTING.md#-commit-mess)ntype>(optional scope>): description> optional metadata>nOur customization:n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96b9\/CONTRIBUTING.md#-commit-mess)ntype>(optional scope>): description> optional metadata>ntype>(optional metadata>): optional metadata>ntype>(optional metadata>): optional metadata>ntype>(optional metadata>): optional metadata>ntype>(optional metadata>): optional metadata>ntype>(optional metadata>): optional metadata>ntype>(optional metadata>): optional metadata>ntype>(optional metadata>): optional metadata>ntype>(optional metadata>): optional metadata>ntype>(optional metadata>): optional metadata>ntype>(optional metadata>): optional metadata>ntype>(optional metadata>): optional metadata>ntype>(optional metadata>): optional metadata>ntype>(optional"}
{"File Name":"stamper\/0007-docker.md","Context":"## Context\nWe need to choose how we will deploy our app.\n","Decision":"We use Docker because it allows us to isolate app with a container and easily deploy to the server with Terraform.\\n","tokens":15,"id":1239,"text":"## Context\\nWe need to choose how we will deploy our app.\\n\n\n##Decision\nWe use Docker because it allows us to isolate app with a container and easily deploy to the server with Terraform.\\n","Predictions":"We will use a Scala database to store the data we need to build our application.n"}
{"File Name":"adr-tools\/0004-markdown-format.md","Context":"## Context\nThe decision records must be stored in a plain text format:\n* This works well with version control systems.\n* It allows the tool to modify the status of records and insert\nhyperlinks when one decision supercedes another.\n* Decisions can be read in the terminal, IDE, version control\nbrowser, etc.\nPeople will want to use some formatting: lists, code examples,\nand so on.\nPeople will want to view the decision records in a more readable\nformat than plain text, and maybe print them out.\n","Decision":"browser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\\nDecisions will be formatted nicely and hyperlinked by the\\nbrowsers of project hosting sites like GitHub and Bitbucket.\\nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convert\\nthe decision records into HTML or PDF.\\n","tokens":114,"id":3568,"text":"## Context\\nThe decision records must be stored in a plain text format:\\n* This works well with version control systems.\\n* It allows the tool to modify the status of records and insert\\nhyperlinks when one decision supercedes another.\\n* Decisions can be read in the terminal, IDE, version control\\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\n\n\n##Decision\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\\nDecisions will be formatted nicely and hyperlinked by the\\nbrowsers of project hosting sites like GitHub and Bitbucket.\\nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convert\\nthe decision records into HTML or PDF.\\n","Predictions":"* The decision records must be stored in a plain text format:n* This works well with version control systems.n* It allows the tool to modify the status of records and insertnhyperlinks when one decision supercedes another.n* Decisions can be read in the terminal, IDE, version controlnbrowser, etc.nPeople will want to use some formatting: lists, code examples,nand so on.nPeople will want to view the decision records in a more readablenformat than plain text, and maybe print them out.nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).nDecisions will be formatted nicely and hyperlinked by the browsers of project hosting sites like GitHub and Bitbucket.nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convertnthe decision records into HTML or PDF.n"}
{"File Name":"mario\/0008-use-one-s3-bucket-per-source.md","Context":"## Context\nEach data source will need to upload one or more files to S3 in order to trigger processing. S3 events, which will drive Lambda execution (See [5. Use AWS Lambda](0005-use-aws-lambda.md)), are configured at the bucket level. We may or may not have much control over the environment which is sending data to S3, for example, if it came directly from a vendor. At minimum we must be able to specify a bucket, but we should not assume we will have much more control than this.\nEach data source will also need different processing. This implies the need to identify which source a data file came from.\n","Decision":"Use one S3 bucket per data source.\\n","tokens":137,"id":3416,"text":"## Context\\nEach data source will need to upload one or more files to S3 in order to trigger processing. S3 events, which will drive Lambda execution (See [5. Use AWS Lambda](0005-use-aws-lambda.md)), are configured at the bucket level. We may or may not have much control over the environment which is sending data to S3, for example, if it came directly from a vendor. At minimum we must be able to specify a bucket, but we should not assume we will have much more control than this.\\nEach data source will also need different processing. This implies the need to identify which source a data file came from.\\n\n\n##Decision\nUse one S3 bucket per data source.\\n","Predictions":"We will keep the Processor as a Domain Service, because the three different kinds of execution we want to support really only differ for how data is handled in memory, and the domain doesn't concern itself with what is contained in memory (it doesn't even mention memory, since it's a detail of the interpreter, and we might as well use interpreters that don't use any memory):n- in the classic virtual machine, the Program is the entire program, that is executed from start to endn- in the REPL situation, each time a new instruction is inserted at the prompt, it is added to the Program, which is then fed to the Processor.n"}
{"File Name":"buy-for-your-school\/0020-use-accessible-autocomplete-for-autocomplete-fields.md","Context":"## Context\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\n","Decision":"We will use [accessible-autocomplete](https:\/\/github.com\/alphagov\/accessible-autocomplete) to provide the autocomplete capability in our pages.\\nThis package has been chosen because accessibility has been carefully considered when developing the package.\\nAlso it is designed to be used with `govuk` form styles so it will be in keeping with other form fields\\nand not be jarring to the user.\\n","tokens":26,"id":1255,"text":"## Context\\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\\n\n\n##Decision\nWe will use [accessible-autocomplete](https:\/\/github.com\/alphagov\/accessible-autocomplete) to provide the autocomplete capability in our pages.\\nThis package has been chosen because accessibility has been carefully considered when developing the package.\\nAlso it is designed to be used with `govuk` form styles so it will be in keeping with other form fields\\nand not be jarring to the user.\\n","Predictions":"We will use [stackoverflow](https:\/\/stackoverflow.com\/) to provide the stackoverflow functionality in our pages.n"}
{"File Name":"galaxy-zoo-touch-table\/adr-2.md","Context":"### Context\nThe touch table app should support multi touch while also being robust enough to handle various UI demands. Although we initially considered a 3D model of the universe in the center of the table, that approach was dropped as it didn't help accomplish our learning goals. Given that we use a Windows device for the touch table, we must choose a technology that can run on such a machine.\n### Decision\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\n### Status\nAccepted\n### Consequences\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\n","Decision":"Although considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n","tokens":409,"id":5076,"text":"### Context\\nThe touch table app should support multi touch while also being robust enough to handle various UI demands. Although we initially considered a 3D model of the universe in the center of the table, that approach was dropped as it didn't help accomplish our learning goals. Given that we use a Windows device for the touch table, we must choose a technology that can run on such a machine.\\n### Decision\\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n\n\n##Decision\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https:\/\/github.com\/zooniverse\/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n","Predictions":"We dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.n### StatusnAcceptedn### ConsequencesnInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https:\/\/lifeonearth.seas.harvard.edu\/learning-activities\/deeptree\/), another app built in WPF.n### StatusnAcceptedn### ConsequencesnInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and we had more resources and shared knowledge in C#.nInitially, we were able to use Javascript in the app, and"}
{"File Name":"simple-server\/011-rollup-summary-tables-for-dashboards.md","Context":"## Context\nEarlier we went with a [materialized view approach](https:\/\/github.com\/simpledotorg\/simple-server\/blob\/master\/doc\/arch\/008-materialized-views-for-dashboards.md) for dashboard reporting data. This approach has worked out well for the current reports but has limitations in terms of scaling and complexity.\nWe currently have a need for a new set of dashboard reports that rely on similar sets of data, and anticipate more needs for summary data for monthly and quarterly reports. We'd like to try to find a simpler, more stable, and more scalable approach to making that data easily accessible.\n","Decision":"We will create rollup summary tables for to capture the \"most recent blood pressure per patient per period\". These rollup tables will be updated in real-time as blood pressures are captured, removing the need for recurring batch jobs. We will denormalize as much data as we need to to make reporting queries simple and cheap to execute, to prevent the need for any complicated SQL queries or additional data layer caching.\\nOnce the current period has completed, we will treat the data in these rollup tables for that period as immutable. This will let us and our users have more confidence in reports from past periods, as they will not change due to patients moving to different facilities or changes in how we calculate things.\\nWe will focus on the District controlled blood pressure trend report as a first test case for this sort of approach, as it is a key report and one we'd like to gather feedback on as quickly as possible.\\n","tokens":123,"id":1701,"text":"## Context\\nEarlier we went with a [materialized view approach](https:\/\/github.com\/simpledotorg\/simple-server\/blob\/master\/doc\/arch\/008-materialized-views-for-dashboards.md) for dashboard reporting data. This approach has worked out well for the current reports but has limitations in terms of scaling and complexity.\\nWe currently have a need for a new set of dashboard reports that rely on similar sets of data, and anticipate more needs for summary data for monthly and quarterly reports. We'd like to try to find a simpler, more stable, and more scalable approach to making that data easily accessible.\\n\n\n##Decision\nWe will create rollup summary tables for to capture the \"most recent blood pressure per patient per period\". These rollup tables will be updated in real-time as blood pressures are captured, removing the need for recurring batch jobs. We will denormalize as much data as we need to to make reporting queries simple and cheap to execute, to prevent the need for any complicated SQL queries or additional data layer caching.\\nOnce the current period has completed, we will treat the data in these rollup tables for that period as immutable. This will let us and our users have more confidence in reports from past periods, as they will not change due to patients moving to different facilities or changes in how we calculate things.\\nWe will focus on the District controlled blood pressure trend report as a first test case for this sort of approach, as it is a key report and one we'd like to gather feedback on as quickly as possible.\\n","Predictions":"We will create rollup summary tables for to capture the \"most recent blood pressure per patient per period\". These rollup tables will be updated in real-time as blood pressures are captured, removing the need for recurring batch jobs. We will denormalize as much data as we need to to make reporting queries simple and cheap to execute, to prevent the need for any complicated SQL queries or additional data layer caching.nOnce the current period has completed, we will treat the data in these rollup tables for that period as immutable. This will let us and our users have more confidence in reports from past periods, as they will not change due to patients moving to different facilities or changes in how we calculate things.nWe will focus on the District controlled blood pressure trend report as a first test case for this sort of approach, as it is a key report and one we'd like to gather feedback on as quickly as possible.n"}
{"File Name":"PerformanceTestDocs\/0006-vellum-node-fails.md","Context":"## Context\nVellum node fails\n","Decision":"Vellum is always addressed by its cluster name and all of its data is stored in distributed databases with replicas of data on multiple nodes.\\n","tokens":9,"id":3952,"text":"## Context\\nVellum node fails\\n\n\n##Decision\nVellum is always addressed by its cluster name and all of its data is stored in distributed databases with replicas of data on multiple nodes.\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"digital-paper-edit-storybook\/adr-28-08.md","Context":"## Context and Problem Statement\nWe needed to clarify the relationship between the DPE Client repository, where the components we are using to populate the Storybook repo have already been written, and the Storybook repository.\n- From which repo would components be published?\n- Which repos would consume components from NPM?\n- Should the Storybook live inside the Client repo?\n## Decision Drivers\nN\/A\n","Decision":"N\/A\\nChosen option: Option 2, because this allows us to refactor components' code and preview changes within the Storybook locally \u2014 before publishing the component to the hosted Storybook and NPM.\\nThis means that our workflow for populating the Storybook and refactoring the Client code is as follows:\\n1. Duplicate component code to Storybook repo\\n2. Publish completed components to NPM\\n3. Remove the original component code from the Client and import via NPM\\n### Positive Consequences\\n### Negative consequences\\nCaveat: If more than one person is working on the Storybook and DPE Client, they'll need to sync up to ensure that details in code refactors are not lost due to overlapping work.\\nIf possible, also avoid having people working simultaneously on a component that consumes \/ is consumed by another component (i.e., one person working on a card component and another person working on a list component that consumes card components).\\n","tokens":83,"id":36,"text":"## Context and Problem Statement\\nWe needed to clarify the relationship between the DPE Client repository, where the components we are using to populate the Storybook repo have already been written, and the Storybook repository.\\n- From which repo would components be published?\\n- Which repos would consume components from NPM?\\n- Should the Storybook live inside the Client repo?\\n## Decision Drivers\\nN\/A\\n\n\n##Decision\nN\/A\\nChosen option: Option 2, because this allows us to refactor components' code and preview changes within the Storybook locally \u2014 before publishing the component to the hosted Storybook and NPM.\\nThis means that our workflow for populating the Storybook and refactoring the Client code is as follows:\\n1. Duplicate component code to Storybook repo\\n2. Publish completed components to NPM\\n3. Remove the original component code from the Client and import via NPM\\n### Positive Consequences\\n### Negative consequences\\nCaveat: If more than one person is working on the Storybook and DPE Client, they'll need to sync up to ensure that details in code refactors are not lost due to overlapping work.\\nIf possible, also avoid having people working simultaneously on a component that consumes \/ is consumed by another component (i.e., one person working on a card component and another person working on a list component that consumes card components).\\n","Predictions":"We decided to use [Semantic Validation](https:\/\/github.com\/SemanticValidation\/semantic-validation) as our validation library.n"}
{"File Name":"sexual-health-service-finder\/0002-use-express-web-framework.md","Context":"## Context\nA web framework is needed in order to provide a level of abstraction over low\nlevel language features. Effort spent solving non-domain specific problems\nshould be kept to an absolute minimum.\n","Decision":"We will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","tokens":41,"id":4271,"text":"## Context\\nA web framework is needed in order to provide a level of abstraction over low\\nlevel language features. Effort spent solving non-domain specific problems\\nshould be kept to an absolute minimum.\\n\n\n##Decision\nWe will use the [Express](http:\/\/expressjs.com\/) web framework. This will\\nprovide a lot of well\\n[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features and\\nfunctionality. Whilst providing a fast, unopinionated, minimalist web\\nframework.\\nExpress is one of the most popular web frameworks available for NodeJS and as\\nsuch has been well tested and is deployed across many\\n[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).\\n","Predictions":"We will use the [Express](http:\/\/expressjs.com\/) web framework. This willnprovide a lot of welln[documented](http:\/\/expressjs.com\/en\/resources\/books-blogs.html) features andnfunctionality. Whilst providing a fast, unopinionated, minimalist webnframework.nExpress is one of the most popular web frameworks available for NodeJS and asnsuch has been well tested and is deployed across manyn[organisations](http:\/\/expressjs.com\/en\/resources\/companies-using-express.html).n"}
{"File Name":"ensemblejs\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":3940,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"monocle\/0009-changes-extended-with-task-data.md","Context":"## Context and Problem Statement\nAs a user, I want to get Changes metrics related to tasks defined\nin a task tracker. A simple usecase example is to get insight of the\nratio of changes related to Feature Requests vs Bug fixing.\n## Decision Drivers\n* Simple implementation\n* No assumption about the tasks tracker\n* Support of a set of generic fields related to a task\n","Decision":"* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\nChosen option: \"Monocle API provides an interface for external task data crawlers\", because it will ease integration between Monocle and the\\nvarious task trackers available in the market.\\nA Monocle operator will need to write the crawler for its\\nown task tracker. However, over the time, operators might have written crawlers for most popular systems and released them under a open source license.\\nFrom the Monocle side, we provide a clear API for a task tracker\\ncrawler to push task related data to Monocle. Each task data sent to Monocle must at least set a predifined set of generic attributes like\\n\"severity\", \"change_url\", \"title\", ... The monocle API is then able to\\nfind corresponding Changes in the database that match the \"change_url\" field.\\nEach changes get a new attribute called \"task_data\" that is a list of\\nrelated task data records. Indeed multiple tasks might be related to a\\nsingle change.\\nAlso, Monocle keeps track of task data that do not match any Changes in the\\ndatabase. The Monocle change crawlers engine triggers a search for adoption of orphaned task data.\\n","tokens":79,"id":4636,"text":"## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n\n\n##Decision\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\nChosen option: \"Monocle API provides an interface for external task data crawlers\", because it will ease integration between Monocle and the\\nvarious task trackers available in the market.\\nA Monocle operator will need to write the crawler for its\\nown task tracker. However, over the time, operators might have written crawlers for most popular systems and released them under a open source license.\\nFrom the Monocle side, we provide a clear API for a task tracker\\ncrawler to push task related data to Monocle. Each task data sent to Monocle must at least set a predifined set of generic attributes like\\n\"severity\", \"change_url\", \"title\", ... The monocle API is then able to\\nfind corresponding Changes in the database that match the \"change_url\" field.\\nEach changes get a new attribute called \"task_data\" that is a list of\\nrelated task data records. Indeed multiple tasks might be related to a\\nsingle change.\\nAlso, Monocle keeps track of task data that do not match any Changes in the\\ndatabase. The Monocle change crawlers engine triggers a search for adoption of orphaned task data.\\n","Predictions":"* Simple implementationn* No assumption about the tasks trackern* Support of a set of generic fields related to a tasknChosen option: \"Monocle API provides an interface for external task data crawlers\", because it will ease integration between Monocle and thenvarious task trackers available in the market.nA Monocle operator will need to write the crawler for itsntask tracker. However, over the time, operators might have written crawlers for most popular systems and released them under a open source license.nFrom the Monocle side, we provide a clear API for a task tracker. Each task data sent to Monocle must at least set a predifined set of generic attributes liken\"severity\", \"change_url\", \"title\",... The monocle API is then able tonfind corresponding Changes in the database that match the \"change_url\" field. Each change gets a new attribute called \"task_data\" that is a list ofnrelated task data records. Indeed multiple tasks might be related to ansingle change.nAlso, Monocle keeps track of task data that do not match any Changes in thendatabase. The monocle change crawlers engine triggers a search for adoption of orphaned task data.n"}
{"File Name":"form-builder\/0006-reduce-number-of-emails-per-submission.md","Context":"## Context\nForm Builder sends one email per file uploaded for each application. For those forms that allow multiple files to be uploaded this generates a lot of email noise for both form owners and the applicants. This can result in additional work to sift these emails or even create unforced errors in workflows that are manual in nature.\nThe use of AWS SES as the email service means that there is a 10MB upper limit on the size of each email that is sent.\nOptions discussed:\n1. Attach as many files as possible to a single email up to the 10MB limit.\n2. Hard cap limit on 10MB in total accross _all_ the files uploaded per form.\n3. Admin interface for form owners to log in and retrieve file uploads per submission.\n4. Email a link to the form owner allowing the ability to download all attachments in one go.\n","Decision":"We feel that it is reasonable to reduce the number of emails that recipients of each submission get to the absolute minimum.\\nOption 1 is the one which is the most reasonable solution at this time. It requires the least amount of code change and does not change the architecture or increase the threat surface of Form Builder.\\n","tokens":181,"id":5099,"text":"## Context\\nForm Builder sends one email per file uploaded for each application. For those forms that allow multiple files to be uploaded this generates a lot of email noise for both form owners and the applicants. This can result in additional work to sift these emails or even create unforced errors in workflows that are manual in nature.\\nThe use of AWS SES as the email service means that there is a 10MB upper limit on the size of each email that is sent.\\nOptions discussed:\\n1. Attach as many files as possible to a single email up to the 10MB limit.\\n2. Hard cap limit on 10MB in total accross _all_ the files uploaded per form.\\n3. Admin interface for form owners to log in and retrieve file uploads per submission.\\n4. Email a link to the form owner allowing the ability to download all attachments in one go.\\n\n\n##Decision\nWe feel that it is reasonable to reduce the number of emails that recipients of each submission get to the absolute minimum.\\nOption 1 is the one which is the most reasonable solution at this time. It requires the least amount of code change and does not change the architecture or increase the threat surface of Form Builder.\\n","Predictions":"We feel that it is reasonable to reduce the number of emails that recipients of each submission get to the absolute minimum.nOption 1 is the one which is the most reasonable solution at this time. It requires the least amount of code change and does not change the architecture or increase the threat surface of Form Builder.n"}
{"File Name":"visit-plannr\/0002-dynamodb-composite-primary-key-design.md","Context":"## Context\n\"[In DynamoDB, tables, items, and attributes are the core components that you work with. A table is a collection of items, and each item is a collection of attributes. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility.](https:\/\/docs.aws.amazon.com\/amazondynamodb\/latest\/developerguide\/HowItWorks.CoreComponents.html)\"\nThere are two types of primary key in dynamodb.\nThe first kind is having just a partition key. The partition key is a hash and determines where on physical storage the item is placed. The partition key must be unique.\nThe second kind is a composite primary key. It consists of a partition key and a sort key. The partition key stays the same but doesn't need to be unique in isolation. Rather the sort key\/ partition key pair must be unique.\nIn a real system this would probably push towards StreamName as the partition key: so that events that logically live together physically live together. And Event Number in the stream as the sort key. So that the order of items as they are stored on physical media matches the order they are likely to be read.\nThis introduces unwanted complexity at this time in the code for tracking event numbers.\n","Decision":"For now instead of an Event Number as the sort key we will introduce a UUID EventId. And use StreamName as the HASH key. StreamNames will need to be unique anyway.\\n","tokens":260,"id":1990,"text":"## Context\\n\"[In DynamoDB, tables, items, and attributes are the core components that you work with. A table is a collection of items, and each item is a collection of attributes. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility.](https:\/\/docs.aws.amazon.com\/amazondynamodb\/latest\/developerguide\/HowItWorks.CoreComponents.html)\"\\nThere are two types of primary key in dynamodb.\\nThe first kind is having just a partition key. The partition key is a hash and determines where on physical storage the item is placed. The partition key must be unique.\\nThe second kind is a composite primary key. It consists of a partition key and a sort key. The partition key stays the same but doesn't need to be unique in isolation. Rather the sort key\/ partition key pair must be unique.\\nIn a real system this would probably push towards StreamName as the partition key: so that events that logically live together physically live together. And Event Number in the stream as the sort key. So that the order of items as they are stored on physical media matches the order they are likely to be read.\\nThis introduces unwanted complexity at this time in the code for tracking event numbers.\\n\n\n##Decision\nFor now instead of an Event Number as the sort key we will introduce a UUID EventId. And use StreamName as the HASH key. StreamNames will need to be unique anyway.\\n","Predictions":"For now instead of an Event Number as the sort key we will introduce a UUID EventId. And use StreamName as the HASH key. StreamNames will need to be unique anyway.n"}
{"File Name":"hello\/008-deployment.md","Context":"## Context\nWe need a way to deploy our application to AWS.\n","Decision":"We will use [Terraform] to deploy our application. More specifically,\\nwe will use [Chalice] to create the build artifact for the source\\ncode (.zip) and then use [Terraform] to manage the infrastructure\\nresources.\\n\ud83d\udc49 **Note:** Ideally, we would publish the build artifacts to S3 before\\ndeployment. That way the deployment process could happen independently\\nof the build process.\\n\ud83d\udc49 **Note:** To begin with, we'll just convert the JSON data created\\nby [Chalice] to [HCL] and add any other resources needed. Going forward,\\nwe will use modules to create reusable components.\\n","tokens":15,"id":4358,"text":"## Context\\nWe need a way to deploy our application to AWS.\\n\n\n##Decision\nWe will use [Terraform] to deploy our application. More specifically,\\nwe will use [Chalice] to create the build artifact for the source\\ncode (.zip) and then use [Terraform] to manage the infrastructure\\nresources.\\n\ud83d\udc49 **Note:** Ideally, we would publish the build artifacts to S3 before\\ndeployment. That way the deployment process could happen independently\\nof the build process.\\n\ud83d\udc49 **Note:** To begin with, we'll just convert the JSON data created\\nby [Chalice] to [HCL] and add any other resources needed. Going forward,\\nwe will use modules to create reusable components.\\n","Predictions":"We will use [Terraform] to deploy our application. More specifically,nwe will use [Chalice] to create the build artifact for the sourcencode (.zip) and then use [Terraform] to manage the infrastructurenresources.n **Note:** Ideally, we would publish the build artifacts to S3 beforendeployment. That way the deployment process could happen independentlynof the build process.n **Note:** To begin with, we'll just convert the JSON data creatednby [Chalice] to [HCL] and add any other resources needed. Going forward,nwe will use modules to create reusable components.n"}
{"File Name":"infra\/0002-use-helm-for-service-deployment-definitions.md","Context":"## Context\nOne of the primary problems encountered in deployments is drift between different environments.  An important element that creates drift is deltas between the deployment of different environments.  We are already mitigating that by storing our configurations as code, and doing automated deployments of those configurations.  An extension of these practices is to practice code reuse, or DRY (don't repeat yourself).\nGiven that we have existing kubernetes deployments, and that our primary supported services (bedrock\/www, snippets, basket) are already deployed there, it seems reasonable to invest further in the kubernetes eco-system.  Our current pattern is to define a separate set of yml files which are deployed via `kubectl apply -f` per region\/cloud. For example, if we have gcp iowa-a and an aws frankfurt cluster, if we wish to deploy to both we'd have two copies of nearly identical files to define the deployments to those two clusters.\n","Decision":"Use helm3 in order to define the 'template' of our services.  Helm calls these templates 'charts', templates can have injected 'values'. The deployment, and associated kubernetes objects (such as services, and scaling policies) should be defined once, with sensible defaults chosen for the primary chart (these should be the 'prod' values).  Secrets should be referenced, but not included in the charts (paths to secrets, not the secrets themselves). Then environments that need different values should have an override file in their repo, which can be combined with defaults at deploy time.\\nThere should be a single mozmeao helm repo, that contains all of our custom written charts. We would expect there to be a single chart per service, where bedrock\/www is a service.  There should be a pipeline for that helm repo (that includes testing).  The pipeline for each service would then reference and deploy those charts for the dev\/staging\/prod versions of the service.\\nOne advantage of undertaking this work is making it easier to read and understand our deployments.  Answering 'what's different between dev and prod' is difficult when the full configuration is repeated.  It's much easier to answer when dev is defaults + a small override file, and prod is the same.  We should also end up with fewer differences, since each difference is clearly visible in the charts, and we can seek to reduce that count.\\nThe other primary advantage is reducing the class of errors where some new feature worked in dev, but doesn't in prod because you forgot to do X. Where X is likely adding an environment variable, or creating a secret.  Having a template means we should be able to fail the deployment earlier in each environment if that configuration is not present.\\n","tokens":190,"id":856,"text":"## Context\\nOne of the primary problems encountered in deployments is drift between different environments.  An important element that creates drift is deltas between the deployment of different environments.  We are already mitigating that by storing our configurations as code, and doing automated deployments of those configurations.  An extension of these practices is to practice code reuse, or DRY (don't repeat yourself).\\nGiven that we have existing kubernetes deployments, and that our primary supported services (bedrock\/www, snippets, basket) are already deployed there, it seems reasonable to invest further in the kubernetes eco-system.  Our current pattern is to define a separate set of yml files which are deployed via `kubectl apply -f` per region\/cloud. For example, if we have gcp iowa-a and an aws frankfurt cluster, if we wish to deploy to both we'd have two copies of nearly identical files to define the deployments to those two clusters.\\n\n\n##Decision\nUse helm3 in order to define the 'template' of our services.  Helm calls these templates 'charts', templates can have injected 'values'. The deployment, and associated kubernetes objects (such as services, and scaling policies) should be defined once, with sensible defaults chosen for the primary chart (these should be the 'prod' values).  Secrets should be referenced, but not included in the charts (paths to secrets, not the secrets themselves). Then environments that need different values should have an override file in their repo, which can be combined with defaults at deploy time.\\nThere should be a single mozmeao helm repo, that contains all of our custom written charts. We would expect there to be a single chart per service, where bedrock\/www is a service.  There should be a pipeline for that helm repo (that includes testing).  The pipeline for each service would then reference and deploy those charts for the dev\/staging\/prod versions of the service.\\nOne advantage of undertaking this work is making it easier to read and understand our deployments.  Answering 'what's different between dev and prod' is difficult when the full configuration is repeated.  It's much easier to answer when dev is defaults + a small override file, and prod is the same.  We should also end up with fewer differences, since each difference is clearly visible in the charts, and we can seek to reduce that count.\\nThe other primary advantage is reducing the class of errors where some new feature worked in dev, but doesn't in prod because you forgot to do X. Where X is likely adding an environment variable, or creating a secret.  Having a template means we should be able to fail the deployment earlier in each environment if that configuration is not present.\\n","Predictions":"We will use a staging_version attribute to define the deployment. This will be used to define the deployments for the dev and staging_version versions of the service.n"}
{"File Name":"hee-web-blueprint\/0013-use-bloomreach-channel-manager.md","Context":"## Context\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\nAs part of this, considerations were made around:\n* Ease of use for creating new sites\n* Ability to share components\n* Ability to segregate content for specific channels (sites)\n* Ability to share content up and down the stack where needed and appropriate\n* Permissions model required to support this model\n","Decision":"Bloomreach's concept of channels is well suited to meet the needs of running the sites required under the NWP platform umbrella. Channels offer the ability to build new sites that share components and modules, which enables for greater consistency. By utilising roles and permissions from within BR, content can be segregated to be available only where it is most relevant, whilst allowing for content to be made available up or down the organisational stack (e.g. national content being aggregated at a regional level).\\nBR's 'blueprinting' functionality allows for sites to be created using a series of parameters, further standardising the creation of sites where needed in an easy fashion.\\n","tokens":141,"id":1205,"text":"## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\\nAs part of this, considerations were made around:\\n* Ease of use for creating new sites\\n* Ability to share components\\n* Ability to segregate content for specific channels (sites)\\n* Ability to share content up and down the stack where needed and appropriate\\n* Permissions model required to support this model\\n\n\n##Decision\nBloomreach's concept of channels is well suited to meet the needs of running the sites required under the NWP platform umbrella. Channels offer the ability to build new sites that share components and modules, which enables for greater consistency. By utilising roles and permissions from within BR, content can be segregated to be available only where it is most relevant, whilst allowing for content to be made available up or down the organisational stack (e.g. national content being aggregated at a regional level).\\nBR's 'blueprinting' functionality allows for sites to be created using a series of parameters, further standardising the creation of sites where needed in an easy fashion.\\n","Predictions":"We will use [Azure](https:\/\/azure.io\/) for the internal and protected image tiers.n"}
{"File Name":"devops-challenge\/0001-record-architecture-decisions.md","Context":"## Context\nThe goal of this challenge is to implement a support plaform for a\nmicroservices architecture with several requirements:\n* is scalable on load\n* is fully automated\n* identifies Single Point of Failures\n* implements resilience and fault tolerance mechanisms\nWe need to record the architectural decisions made on this project as\na way to document all the thinking and building process.  This will help\nothers to answer their wondering questions about why and how this solution was\nimplemented.\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":112,"id":3488,"text":"## Context\\nThe goal of this challenge is to implement a support plaform for a\\nmicroservices architecture with several requirements:\\n* is scalable on load\\n* is fully automated\\n* identifies Single Point of Failures\\n* implements resilience and fault tolerance mechanisms\\nWe need to record the architectural decisions made on this project as\\na way to document all the thinking and building process.  This will help\\nothers to answer their wondering questions about why and how this solution was\\nimplemented.\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"spin-archetype-base\/0002-unified-terminology.md","Context":"## Context\nIt is easy to use different terms meaning the same thing in code. A common aphorism is that the most difficult thing in coding is naming.\n","Decision":"A `Terminology.md` document will be created, that will be used to collect the official terms for the project. If naming diverts from this terminology, it should be considerd a bug.\\n","tokens":34,"id":2270,"text":"## Context\\nIt is easy to use different terms meaning the same thing in code. A common aphorism is that the most difficult thing in coding is naming.\\n\n\n##Decision\nA `Terminology.md` document will be created, that will be used to collect the official terms for the project. If naming diverts from this terminology, it should be considerd a bug.\\n","Predictions":"We will use a Status file to record all architectural decisions.n"}
{"File Name":"ikforth\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5120,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"sepa-customer-platform\/0006-relating-documents-to-cases.md","Context":"## Context and Problem Statement\nDocument uploads will be stored in Azure Blob storage because of the requirement to store very large files & the fact that Azure Blob storage offers high availability and low costs.\nThe Azure Blob storage system is a separate cloud service which is not directly linked to Dynamics CRM, therefore we need to consider how the permits, stored using CRM case\/incident entities, will be linked to the the files within Azure.\n## Decision Drivers\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\n","Decision":"* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n[Option 1] Use the existing 'annotation' entity in the CRM & set the entity attributes to match Azure blob data.\\n### Positive Consequences\\n* Minimises customisation of the CRM data, reuse of existing\\n### Negative Consequences\\n* Labelling within the CRM does not match the context of the upload.\\n","tokens":142,"id":5163,"text":"## Context and Problem Statement\\nDocument uploads will be stored in Azure Blob storage because of the requirement to store very large files & the fact that Azure Blob storage offers high availability and low costs.\\nThe Azure Blob storage system is a separate cloud service which is not directly linked to Dynamics CRM, therefore we need to consider how the permits, stored using CRM case\/incident entities, will be linked to the the files within Azure.\\n## Decision Drivers\\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n\n\n##Decision\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n[Option 1] Use the existing 'annotation' entity in the CRM & set the entity attributes to match Azure blob data.\\n### Positive Consequences\\n* Minimises customisation of the CRM data, reuse of existing\\n### Negative Consequences\\n* Labelling within the CRM does not match the context of the upload.\\n","Predictions":"* The Azure Blob storage system is a separate service which is not directly linked to Dynamics CRM, therefore we need to consider how the permits, stored using CRM case\/incident entities, will be linked to the files within Azure Blob storage.n[Option 1] Use the existing 'annotation' entity in the CRM & set the entity attributes to match Azure blob data.n### Positive Consequencesn* Minimises customisation of the CRM data, reuse of existingn### Negative Consequencesn* Labelling within the CRM does not match the context of the upload.n"}
{"File Name":"up-fiscal-data\/006-time.md","Context":"## Context and Problem Statement\nThe data extraction w\/ transaction timestamp from Koshvani platform is a challenging task.\n- The temporal data directly available on the platform is not easy to extract\n- The validation of temporal data to ensure accuracy depends on factors\n- Districts\/Treasuries which have updated their data\n- Data\/Time of data upload vs extraction from departments\n## Decision Drivers\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\n","Decision":"The platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n- Do a daily level data extraction from the platform\\n- Conduct analysis to identify daily, weekly and monthly expenditure numbers\\n- Select the accurate time period and mention methodology for the same\\n- Ignore previous time period missed in this analysis.\\nConduct the analysis on accurate data collection at in November, 2020 to take decision.\\n","tokens":120,"id":442,"text":"## Context and Problem Statement\\nThe data extraction w\/ transaction timestamp from Koshvani platform is a challenging task.\\n- The temporal data directly available on the platform is not easy to extract\\n- The validation of temporal data to ensure accuracy depends on factors\\n- Districts\/Treasuries which have updated their data\\n- Data\/Time of data upload vs extraction from departments\\n## Decision Drivers\\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n\n\n##Decision\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n- Do a daily level data extraction from the platform\\n- Conduct analysis to identify daily, weekly and monthly expenditure numbers\\n- Select the accurate time period and mention methodology for the same\\n- Ignore previous time period missed in this analysis.\\nConduct the analysis on accurate data collection at in November, 2020 to take decision.\\n","Predictions":"* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store the datan* PostgreSQL is the only database that can store"}
{"File Name":"cloud-platform\/018-Dedicated-Ingress-Controllers.md","Context":"## Context\nThe Cloud Platform was originally set up to have a single ingress controller to\nmanage all ingresses in the cluster. So, every new ingress added a config block\nto one, large nginx config file, and all traffic to all services hosted on the\nCloud Platform is routed through a single AWS load balancer.\nAlthough this was both easy to manage, and saved us some money on load\nbalancers (approx. $25\/month per ingress), it has become unsustainable. We\nusually have 6 replicas of the ingress controller pod, and we have started to\nsee instances of several of these pods crash-looping (usually because they have\nrun out of shared memory, which cannot be increased in kubernetes. See [this\nissue] for more information).\nWe believe this is because the nginx config has become so large (over 100K\nlines), that sometimes pods fail to reload it when it is changed, or the pod is\nmoved.\n","Decision":"We will create a separate AWS load balancer and ingress-controller for every\\nnamespace in the cluster. An \"ingress class\" annotation will cause traffic for\\na particular ingress to be routed through the appropriate AWS load balancer and\\ningress-controller. See our [module repository] for more details.\\n\"System\" ingresses (e.g. those used for concourse, grafana, etc.) will continue\\nto use the default ingress-controller. There should only ever be a handful of\\nthese, compared with hundreds of team ingresses, so the load on the default\\ningress-controller should stay within acceptable limits.\\n","tokens":200,"id":620,"text":"## Context\\nThe Cloud Platform was originally set up to have a single ingress controller to\\nmanage all ingresses in the cluster. So, every new ingress added a config block\\nto one, large nginx config file, and all traffic to all services hosted on the\\nCloud Platform is routed through a single AWS load balancer.\\nAlthough this was both easy to manage, and saved us some money on load\\nbalancers (approx. $25\/month per ingress), it has become unsustainable. We\\nusually have 6 replicas of the ingress controller pod, and we have started to\\nsee instances of several of these pods crash-looping (usually because they have\\nrun out of shared memory, which cannot be increased in kubernetes. See [this\\nissue] for more information).\\nWe believe this is because the nginx config has become so large (over 100K\\nlines), that sometimes pods fail to reload it when it is changed, or the pod is\\nmoved.\\n\n\n##Decision\nWe will create a separate AWS load balancer and ingress-controller for every\\nnamespace in the cluster. An \"ingress class\" annotation will cause traffic for\\na particular ingress to be routed through the appropriate AWS load balancer and\\ningress-controller. See our [module repository] for more details.\\n\"System\" ingresses (e.g. those used for concourse, grafana, etc.) will continue\\nto use the default ingress-controller. There should only ever be a handful of\\nthese, compared with hundreds of team ingresses, so the load on the default\\ningress-controller should stay within acceptable limits.\\n","Predictions":"We will create a separate AWS load balancer and ingress-controller for everynnamespace in the cluster. An \"ingress class\" annotation will cause traffic forna particular ingress to be routed through the appropriate AWS load balancer andningress-controller. See our [module repository] for more details.n\"System\" ingresses (e.g. those used for concourse, grafana, etc.) will continue to use the default ingress-controller. There should only ever be a handful ofnthese, compared with hundreds of team ingresses, so the load on the defaultningress-controller should stay within acceptable limits.n"}
{"File Name":"tendermint\/adr-073-libp2p.md","Context":"## Context\nAs part of the 0.35 development cycle, the Tendermint team completed\nthe first phase of the work described in ADRs 61 and 62, which included a\nlarge scale refactoring of the reactors and the p2p message\nrouting. This replaced the switch and many of the other legacy\ncomponents without breaking protocol or network-level\ninteroperability and left the legacy connection\/socket handling code.\nFollowing the release, the team has reexamined the state of the code\nand the design, as well as Tendermint's requirements. The notes\nfrom that process are available in the [P2P Roadmap\nRFC][rfc].\nThis ADR supersedes the decisions made in ADRs 60 and 61, but\nbuilds on the completed portions of this work. Previously, the\nboundaries of peer management, message handling, and the higher level\nbusiness logic (e.g., \"the reactors\") were intermingled, and core\nelements of the p2p system were responsible for the orchestration of\nhigher-level business logic. Refactoring the legacy components\nmade it more obvious that this entanglement of responsibilities\nhad outsized influence on the entire implementation, making\nit difficult to iterate within the current abstractions.\nIt would not be viable to maintain interoperability with legacy\nsystems while also achieving many of our broader objectives.\nLibP2P is a thoroughly-specified implementation of a peer-to-peer\nnetworking stack, designed specifically for systems such as\nours. Adopting LibP2P as the basis of Tendermint will allow the\nTendermint team to focus more of their time on other differentiating\naspects of the system, and make it possible for the ecosystem as a\nwhole to take advantage of tooling and efforts of the LibP2P\nplatform.\n","Decision":"Tendermint will adopt libp2p during the 0.37 development cycle,\\nreplacing the bespoke Tendermint P2P stack. This will remove the\\n`Endpoint`, `Transport`, `Connection`, and `PeerManager` abstractions\\nand leave the reactors, `p2p.Router` and `p2p.Channel`\\nabstractions.\\nLibP2P may obviate the need for a dedicated peer exchange (PEX)\\nreactor, which would also in turn obviate the need for a dedicated\\nseed mode. If this is the case, then all of this functionality would\\nbe removed.\\nIf it turns out (based on the advice of Protocol Labs) that it makes\\nsense to maintain separate pubsub or gossipsub topics\\nper-message-type, then the `Router` abstraction could also\\nbe entirely subsumed.\\n","tokens":387,"id":1982,"text":"## Context\\nAs part of the 0.35 development cycle, the Tendermint team completed\\nthe first phase of the work described in ADRs 61 and 62, which included a\\nlarge scale refactoring of the reactors and the p2p message\\nrouting. This replaced the switch and many of the other legacy\\ncomponents without breaking protocol or network-level\\ninteroperability and left the legacy connection\/socket handling code.\\nFollowing the release, the team has reexamined the state of the code\\nand the design, as well as Tendermint's requirements. The notes\\nfrom that process are available in the [P2P Roadmap\\nRFC][rfc].\\nThis ADR supersedes the decisions made in ADRs 60 and 61, but\\nbuilds on the completed portions of this work. Previously, the\\nboundaries of peer management, message handling, and the higher level\\nbusiness logic (e.g., \"the reactors\") were intermingled, and core\\nelements of the p2p system were responsible for the orchestration of\\nhigher-level business logic. Refactoring the legacy components\\nmade it more obvious that this entanglement of responsibilities\\nhad outsized influence on the entire implementation, making\\nit difficult to iterate within the current abstractions.\\nIt would not be viable to maintain interoperability with legacy\\nsystems while also achieving many of our broader objectives.\\nLibP2P is a thoroughly-specified implementation of a peer-to-peer\\nnetworking stack, designed specifically for systems such as\\nours. Adopting LibP2P as the basis of Tendermint will allow the\\nTendermint team to focus more of their time on other differentiating\\naspects of the system, and make it possible for the ecosystem as a\\nwhole to take advantage of tooling and efforts of the LibP2P\\nplatform.\\n\n\n##Decision\nTendermint will adopt libp2p during the 0.37 development cycle,\\nreplacing the bespoke Tendermint P2P stack. This will remove the\\n`Endpoint`, `Transport`, `Connection`, and `PeerManager` abstractions\\nand leave the reactors, `p2p.Router` and `p2p.Channel`\\nabstractions.\\nLibP2P may obviate the need for a dedicated peer exchange (PEX)\\nreactor, which would also in turn obviate the need for a dedicated\\nseed mode. If this is the case, then all of this functionality would\\nbe removed.\\nIf it turns out (based on the advice of Protocol Labs) that it makes\\nsense to maintain separate pubsub or gossipsub topics\\nper-message-type, then the `Router` abstraction could also\\nbe entirely subsumed.\\n","Predictions":"The decision is to use a bespoke libp2p implementation,nwhich will be used during the 0.37 development cycle,nreplacing the bespoke Tendermintnplatform.n"}
{"File Name":"optimint\/adr-005-serialization.md","Context":"## Context\nAll the basic data types needs to be efficiently serialized into binary format before saving in KV store or sending to network.\n","Decision":"`protobuf` is used for data serialization both for storing and network communication.\\n`protobuf` is used widely in entire Cosmos ecosystem, and we would need to use it anyways.\\n","tokens":27,"id":3746,"text":"## Context\\nAll the basic data types needs to be efficiently serialized into binary format before saving in KV store or sending to network.\\n\n\n##Decision\n`protobuf` is used for data serialization both for storing and network communication.\\n`protobuf` is used widely in entire Cosmos ecosystem, and we would need to use it anyways.\\n","Predictions":"We will use [Semantic Data Structure](https:\/\/github.com\/semantic-datastructure) to provide a framework for storing and storing data.n"}
{"File Name":"educational-platform\/0011-use-axon-event-publishing-mechanism.md","Context":"## Context\nIn [0002-integration-events-implementation.md](0002-integration-events-implementation.md) was defined the solution for using Spring-event related classes for publishing and listening integration events. In current implementation of application we have Axon Framework which have rich tools for implementing such functionality. After migrating to Axon implementation of integration events, in future, we can enable event sourcing.\n","Decision":"Axon Framework will be used for integration events implementation.\\n","tokens":79,"id":2500,"text":"## Context\\nIn [0002-integration-events-implementation.md](0002-integration-events-implementation.md) was defined the solution for using Spring-event related classes for publishing and listening integration events. In current implementation of application we have Axon Framework which have rich tools for implementing such functionality. After migrating to Axon implementation of integration events, in future, we can enable event sourcing.\\n\n\n##Decision\nAxon Framework will be used for integration events implementation.\\n","Predictions":"We will use Axon Framework for integration events implementation.n"}
{"File Name":"monitoring-doc\/0002-configuration-management.md","Context":"## Context\nWe have the requirement of adding some resources to the base cloud instances. We currently do\nthis via the [cloud.conf](https:\/\/github.com\/alphagov\/prometheus-aws-configuration\/blob\/375f34600e373aa0e4c66fcae032ceee361d8c21\/terraform\/modules\/prometheus\/cloud.conf) system. This presents us with some limitations, such as configuration\nbeing limited to 16kb, duplication in each instance terraform and a lack of fast feedback testing.\n","Decision":"We have decided to move away from cloud.conf as much as possible and instead use it to instantiate\\na masterless puppet agent which will manage the resources.\\n","tokens":109,"id":1372,"text":"## Context\\nWe have the requirement of adding some resources to the base cloud instances. We currently do\\nthis via the [cloud.conf](https:\/\/github.com\/alphagov\/prometheus-aws-configuration\/blob\/375f34600e373aa0e4c66fcae032ceee361d8c21\/terraform\/modules\/prometheus\/cloud.conf) system. This presents us with some limitations, such as configuration\\nbeing limited to 16kb, duplication in each instance terraform and a lack of fast feedback testing.\\n\n\n##Decision\nWe have decided to move away from cloud.conf as much as possible and instead use it to instantiate\\na masterless puppet agent which will manage the resources.\\n","Predictions":"We have decided to introduce a HistoricalEvent model to record changes to models, starting with Activitys.nOnce we have applied HistoricalEvents to all the models we need to track, we can remove all the calls to PublicActivity and remove the gem entirely.n"}
{"File Name":"contact-frontend\/0003-remove-login-redirection.md","Context":"## Context and Problem Statement\nWithin contact-frontend, for the standalone pages, two routes into the page exist. The first requires a tax service\nuser to be logged in, and redirects to login if the service user is not logged in. The second, on a different URL\nsuffixed with \"-unauthenticated\", does not require login, but serves the same page. After discussion, the PlatUI team\ndecided that the requirement for a user to be logged in was making a worse experience for the service end user, adding\nthe requirement of login,when all the same functionality is also available without logging in via a different URL.\n## Decision Drivers\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\navailable without login\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\npersisted to Deskpro agents\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\nthe codebase, making our development process slower and our testing time longer\n","Decision":"* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\nChosen option: \"Remove login redirect from all pages and keep existing endpoints for now\", because:\\n* The benefits to end users in terms of accessibility feel significant enough to warrant the change\\n* Information about logged in users will still be persisted to Deskpro but there will not be a redirect for users\\nwho have been logged out in the background\\n* Codebase is significantly simplified for developers working on contact-frontend\\n* However, deleting the \"-unauthenticated\" routes will be a major breaking change across the Platform, and should be\\ndecoupled from this\\n### Positive Consequences\\n* Capturing information about all logged in users, not just the ones who have clicked on a contact-frontend route not\\nending in \"-unauthenticated\"\\n* More accessible experience for tax platform end users who are not forced to log in to report technical problems\\n* Significant simplification of codebase for developers\\n### Negative Consequences\\n* There is a risk that without forcing users to log in to particular implementations of the contact-frontend forms,\\nthere will be less information sent to DCST in certain cases. On the other hand, for certain users, the login\\nrequirement might lock them out of the contact-frontend forms completely, e.g. if they are trying to report a problem\\nwith login\\n","tokens":314,"id":1754,"text":"## Context and Problem Statement\\nWithin contact-frontend, for the standalone pages, two routes into the page exist. The first requires a tax service\\nuser to be logged in, and redirects to login if the service user is not logged in. The second, on a different URL\\nsuffixed with \"-unauthenticated\", does not require login, but serves the same page. After discussion, the PlatUI team\\ndecided that the requirement for a user to be logged in was making a worse experience for the service end user, adding\\nthe requirement of login,when all the same functionality is also available without logging in via a different URL.\\n## Decision Drivers\\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\n\n\n##Decision\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\nChosen option: \"Remove login redirect from all pages and keep existing endpoints for now\", because:\\n* The benefits to end users in terms of accessibility feel significant enough to warrant the change\\n* Information about logged in users will still be persisted to Deskpro but there will not be a redirect for users\\nwho have been logged out in the background\\n* Codebase is significantly simplified for developers working on contact-frontend\\n* However, deleting the \"-unauthenticated\" routes will be a major breaking change across the Platform, and should be\\ndecoupled from this\\n### Positive Consequences\\n* Capturing information about all logged in users, not just the ones who have clicked on a contact-frontend route not\\nending in \"-unauthenticated\"\\n* More accessible experience for tax platform end users who are not forced to log in to report technical problems\\n* Significant simplification of codebase for developers\\n### Negative Consequences\\n* There is a risk that without forcing users to log in to particular implementations of the contact-frontend forms,\\nthere will be less information sent to DCST in certain cases. On the other hand, for certain users, the login\\nrequirement might lock them out of the contact-frontend forms completely, e.g. if they are trying to report a problem\\nwith login\\n","Predictions":"* The PlatUI team decided that the -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to the unauthenticated URL.n* The -unauthenticated URL should be used to redirect users to"}
{"File Name":"CrossyToad\/adr-0008-use-lenses.md","Context":"## Context\nBecause we're writing a game we have lots of nested state that we need to update, traverse and\notherwise manipulate. Querying data is relatively straightforward in vanilla haskell but\nupdating nested records is a pain.\nThis is the problem [lens](https:\/\/github.com\/ekmett\/lens) is supposed to help us with.\nAdditionally by using lenses we have the option to use the \"classy\" lenses style to let\nindividual subsystems of our game target a subset of our State monad instead of requiring the\nfull state.\n","Decision":"It's Lens time!\\n","tokens":118,"id":2480,"text":"## Context\\nBecause we're writing a game we have lots of nested state that we need to update, traverse and\\notherwise manipulate. Querying data is relatively straightforward in vanilla haskell but\\nupdating nested records is a pain.\\nThis is the problem [lens](https:\/\/github.com\/ekmett\/lens) is supposed to help us with.\\nAdditionally by using lenses we have the option to use the \"classy\" lenses style to let\\nindividual subsystems of our game target a subset of our State monad instead of requiring the\\nfull state.\\n\n\n##Decision\nIt's Lens time!\\n","Predictions":"We will have a handful of projects using Sentry and possible others.nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-project> and paas-project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.n"}
{"File Name":"handbook\/0009-naming-files-and-directories.md","Context":"## Context and Problem Statement\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\ndefaults that differ between schematic types.\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\nand directories.\n## Decision Drivers\n* Provide consistency when navigating the codebase\n* The earlier we decide on this, the better\n","Decision":"* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","tokens":93,"id":1996,"text":"## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n\n\n##Decision\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is\/island-ui\/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents\/CtaButton\/index.ts\\nimport 'components\/CtaButton'\\n````\\nor:\\n````\\ncomponents\/CtaButton\/CtaButton.tsx\\nimport 'components\/CtaButton\/CtaButton'\\n````\\nrather than\\n````\\ncomponents\/cta-button\/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n","Predictions":"* The earlier we decide on this, the betternChosen option: Name files after their default export. If that default export is a React Component, or a class, then thenfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoidnusing kebab-case and snake_case and make sure the name follows the default export of the file.nNaming directories should follow these guidelines:nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. island-ui instead of islandUi:nimport  Box  from '@island.is\/island-ui\/core'nUse PascalCase for directories only containing React components:nncomponents\/CtaButton\/index.tsnimport 'components\/CtaButton'nor:nncomponents\/CtaButton\/CtaButton.tsxnimport 'components\/CtaButton\/CtaButton'nrather thannncomponents\/cta-button\/CtaButton'nnIn all other cases, use camelCase.n### Positive Consequencesn* Easier to navigate the codebasen* File names are more readable, and more readablen"}
{"File Name":"lbh-frontend-react\/0010-use-typedoc-to-generate-api-documentation.md","Context":"## Context\nWe want to have API documentation for the code we publish. Rather than writing\nseparate API docs that quickly get out of date, we would like to be able to\ngenerate it from our code and comments in our code that live next to the thing\nthey refer to. For JavaScript, the standard is [JSDoc](https:\/\/jsdoc.app\/), but\nwith TypeScript, we're already defining the types in our code, and duplicating\nthat in the associated comments is repeated effort and requires manual action to\nkeep up-to-date.\n[TypeDoc](https:\/\/typedoc.org\/) is a documentation generator based on JSDoc. It\nuses a combination of comments and TypeScripts own types to generate API\ndocumentation automatically.\n","Decision":"We will use TypeDoc to generate documentation.\\nWe will document all exported code for the benefit of end users.\\nWe will commit the documentation we generate to the repository alongside changes\\nto behaviour.\\n","tokens":159,"id":1410,"text":"## Context\\nWe want to have API documentation for the code we publish. Rather than writing\\nseparate API docs that quickly get out of date, we would like to be able to\\ngenerate it from our code and comments in our code that live next to the thing\\nthey refer to. For JavaScript, the standard is [JSDoc](https:\/\/jsdoc.app\/), but\\nwith TypeScript, we're already defining the types in our code, and duplicating\\nthat in the associated comments is repeated effort and requires manual action to\\nkeep up-to-date.\\n[TypeDoc](https:\/\/typedoc.org\/) is a documentation generator based on JSDoc. It\\nuses a combination of comments and TypeScripts own types to generate API\\ndocumentation automatically.\\n\n\n##Decision\nWe will use TypeDoc to generate documentation.\\nWe will document all exported code for the benefit of end users.\\nWe will commit the documentation we generate to the repository alongside changes\\nto behaviour.\\n","Predictions":"We will use TypeDoc to generate documentation.nWe will document all exported code for the benefit of end users.nWe will commit the documentation we generate to the repository alongside changesnto behaviour.n"}
{"File Name":"alfresco-anaxes-shipyard\/0006-helm-chart-repository.md","Context":"## Context\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to decide on a Helm Chart Repository that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Helm Charts. When we looked for a solution, we considered that this is going to be a public-facing customer repository. We also took into consideration the need for a custom domain for it.\nThe criterias we looked for when we investigated each option are: if it uses AWS and Quay, if it is a PaaS\/Cloud Storage solution, if the project is mature enough for our use case, if it is a personal project or if it is maintained by a company and if it requires IT support (if it does can it be avoided by altering the pipeline).  We considered that authentication and SSL is not needed.\nWe had several options that we considered. The full list of pros and cons for each option can be found [here](https:\/\/issues.alfresco.com\/jira\/secure\/attachment\/97743\/DEPLOY-150%20Helm%20Chart%20Repos.xlsx). More comments on the investigation we have done can be found [here](https:\/\/issues.alfresco.com\/jira\/browse\/DEPLOY-150).\n","Decision":"We will use Github Pages to store the Helm Charts. The reasons why we elected this solution are: it is a cloud storage solution, it is mature project that has been actively mantained, doesn't require IT support and it offers an easy, well-known pipeline.\\n","tokens":266,"id":3602,"text":"## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to decide on a Helm Chart Repository that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Helm Charts. When we looked for a solution, we considered that this is going to be a public-facing customer repository. We also took into consideration the need for a custom domain for it.\\nThe criterias we looked for when we investigated each option are: if it uses AWS and Quay, if it is a PaaS\/Cloud Storage solution, if the project is mature enough for our use case, if it is a personal project or if it is maintained by a company and if it requires IT support (if it does can it be avoided by altering the pipeline).  We considered that authentication and SSL is not needed.\\nWe had several options that we considered. The full list of pros and cons for each option can be found [here](https:\/\/issues.alfresco.com\/jira\/secure\/attachment\/97743\/DEPLOY-150%20Helm%20Chart%20Repos.xlsx). More comments on the investigation we have done can be found [here](https:\/\/issues.alfresco.com\/jira\/browse\/DEPLOY-150).\\n\n\n##Decision\nWe will use Github Pages to store the Helm Charts. The reasons why we elected this solution are: it is a cloud storage solution, it is mature project that has been actively mantained, doesn't require IT support and it offers an easy, well-known pipeline.\\n","Predictions":"We will use Github Pages to store the Helm Charts. The reasons why we elected this solution are: it is a cloud storage solution, it is mature project that has been actively mantained, doesn't require IT support and it offers an easy, well-known pipeline.n"}
{"File Name":"hee-web-blueprint\/0008-use-npm-to-host-hee-frontend-framework.md","Context":"## Context\nWe need to identify how and where we will store packages that describe our front end framework. There are a number of NHS UK and NHS Digital projects that already use NPM as their package repository for front end code.\n","Decision":"We have chosen to store our packages on NPM.\\n","tokens":47,"id":1202,"text":"## Context\\nWe need to identify how and where we will store packages that describe our front end framework. There are a number of NHS UK and NHS Digital projects that already use NPM as their package repository for front end code.\\n\n\n##Decision\nWe have chosen to store our packages on NPM.\\n","Predictions":"We have chosen to use [Azure Cloud](https:\/\/azure.cloud.io\/) as our hosting provider of choice. This conforms to the [GDS Tech Forum Hosting Guide](https:\/\/github.com\/alphagov\/gds-tech\/pull\/7).nWe will initially be using the eu-west-1 region, Ireland. This region has 3 availability zones and also contains the GDS PaaS which will allow easier sharing and peering.n"}
{"File Name":"hospitalrun\/redux_to_react_query.md","Context":"# Migrate to React Query\/Context API\/Hooks from Redux\n## Context\nCurrently, HospitalRun Frontend uses [redux](https:\/\/react-redux.js.org\/) to manage async data\nfetching through [redux thunk](https:\/\/github.com\/reduxjs\/redux-thunk).\nIt also uses redux for handling the business logic. Redux's main use case is for handling and\nmanaging shared application state. HospitalRun Frontend has almost no shared state across components\nsince it always goes back to PouchDB\/CouchDB for the most recent data. Redux code is often verbose and\ncontains a lot of boilerplate. Although, HospitalRun Frontend uses [redux toolkit](https:\/\/redux-toolkit.js.org\/),\nthe application still has a lot of boilerplate. Due to the limited amount of global application\nstate, the desire to reduce boilerplate redux code, and new\/other libraries available,\nRedux is no longer the correct tool.\nRedux also makes testing more complicated. Since components that display data (i.e. patient data) are\nconnected to the redux store, a mock redux store must be provided during tests. This makes it\ndifficult to isolate testing just to the specific component. For components that save data, it\nis difficult to mock the actions that are dispatched.\n[react-query](https:\/\/github.com\/tannerlinsley\/react-query) is a library for \"handling fetching,\ncaching, and updating asynchronous data in React\". This library has become a popular replacement\nfor the redux pattern of dispatching an action and storing the result of the data fetch in the redux\nstore.\nFor the few uses cases that require global application state (i.e. session information),\nthe [React Context API](https:\/\/reactjs.org\/docs\/context.html) is an alternative from the\nReact library. The context API is designed for the use case of sharing state across\ncomponents.\n","Decision":"HospitalRun has chosen to use React Query to manage asynchronous requests for fetching data, the\\ncontext api to manage shared application state such as user information, and hooks for sharing\\ncode and business logic.\\n","tokens":396,"id":334,"text":"# Migrate to React Query\/Context API\/Hooks from Redux\\n## Context\\nCurrently, HospitalRun Frontend uses [redux](https:\/\/react-redux.js.org\/) to manage async data\\nfetching through [redux thunk](https:\/\/github.com\/reduxjs\/redux-thunk).\\nIt also uses redux for handling the business logic. Redux's main use case is for handling and\\nmanaging shared application state. HospitalRun Frontend has almost no shared state across components\\nsince it always goes back to PouchDB\/CouchDB for the most recent data. Redux code is often verbose and\\ncontains a lot of boilerplate. Although, HospitalRun Frontend uses [redux toolkit](https:\/\/redux-toolkit.js.org\/),\\nthe application still has a lot of boilerplate. Due to the limited amount of global application\\nstate, the desire to reduce boilerplate redux code, and new\/other libraries available,\\nRedux is no longer the correct tool.\\nRedux also makes testing more complicated. Since components that display data (i.e. patient data) are\\nconnected to the redux store, a mock redux store must be provided during tests. This makes it\\ndifficult to isolate testing just to the specific component. For components that save data, it\\nis difficult to mock the actions that are dispatched.\\n[react-query](https:\/\/github.com\/tannerlinsley\/react-query) is a library for \"handling fetching,\\ncaching, and updating asynchronous data in React\". This library has become a popular replacement\\nfor the redux pattern of dispatching an action and storing the result of the data fetch in the redux\\nstore.\\nFor the few uses cases that require global application state (i.e. session information),\\nthe [React Context API](https:\/\/reactjs.org\/docs\/context.html) is an alternative from the\\nReact library. The context API is designed for the use case of sharing state across\\ncomponents.\\n\n\n##Decision\nHospitalRun has chosen to use React Query to manage asynchronous requests for fetching data, the\\ncontext api to manage shared application state such as user information, and hooks for sharing\\ncode and business logic.\\n","Predictions":"# Migrate to React Query\/Context API\/Hooks from ReduxnCurrently, HospitalRun Frontend uses [redux](https:\/\/react-redux.js.org\/) to manage async datanfetching through [redux thunk](https:\/\/github.com\/reduxjs\/redux-thunk).nIt also uses redux for handling the business logic. Redux's main use case is for handling andnmanaging shared application state. Redux code is often verbose andncontains a lot of boilerplate. Although, HospitalRun Frontend uses [redux toolkit](https:\/\/redux-toolkit.js.org\/),nthe application still has a lot of boilerplate. Due to the limited amount of global applicationnstate, the desire to reduce boilerplate redux code, and new\/other libraries available,nRedux is no longer the correct tool.nRedux also makes testing more complicated. Since components that display data (i.e. patient data) arenconnected to the redux store, a mock redux store must be provided during tests. This makes itndifficult to isolate testing just to the specific component. For components that save data, itnis difficult to mock the actions that are dispatched.n[react-query](https:\/\/github.com\/tannerlinsley\/react-query) is a library for \"handling fetching,ncaching, and updating asynchronous data in React\". This library has become a popular replacementnfor the redux pattern of dispatching an action and storing the result of the data fetch in the reduxnstore.nFor the few uses cases that require global application state (i.e. session information),nthe [React Context API](https:\/\/reactjs.org\/docs\/context.html) is an alternative from thenReact library. The context API is designed for use cases that require global application state (i.e. session information).nFor the few uses cases that require global application state (i.e. session information),nthe [React Context API](https:\/\/reactjs.org\/docs\/context.html) is an alternative from thenReact library. The context API is designed for use cases that require global application state (i.e. session information).nFor the few uses cases that require global application state (i.e. session information),nthe [React Context API](https:\/\/reactjs.org\/docs\/context.html) is an alternative from thenReact library. The context API is designed for use cases that require global application state (i.e. session information).nFor the few uses cases that require global application state (i.e. session information),nthe [React Context API](https:\/\/reactjs.org\/docs\/context.html) is an alternative from thenReact library. The context API is designed for use cases that require global application state (i.e. session information).nFor the few uses cases that require global application state (i.e. session information),nthe [React Context API](https:\/\/reactjs.org\/docs\/context.html) is an alternative from thenReact library. The context API is designed for use cases that require global application state (i.e. session information).nFor the few uses cases that require global application state (i.e. session information),nthe [React Context API](https:\/\/reactjs.org\/docs\/context.html) is an alternative from thenReact library. The context API is designed for use cases that require global application state (i.e. session information)"}
{"File Name":"php-docker-template\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Usabilla Architecture Chapter](https:\/\/github.com\/usabilla\/architecture-chapter\/blob\/master\/docs\/adr\/0001-record-architecture-decisions.md).\\n","tokens":16,"id":5237,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Usabilla Architecture Chapter](https:\/\/github.com\/usabilla\/architecture-chapter\/blob\/master\/docs\/adr\/0001-record-architecture-decisions.md).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"paas-team-manual\/ADR017-cell-capacity-assignment.md","Context":"Context\n=======\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\n```\nMemory reserved by orgs: 368640 MB (360 GB)\nMemory reserved by apps: 107108 MB (104 GB)\nMemory actually used by apps: 32868 (32 GB)\n```\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\nDecision\n========\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\nStatus\n======\nAccepted\nConsequences\n============\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\n","Decision":"========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n","tokens":428,"id":231,"text":"Context\\n=======\\nWe want to ensure our platform remains available when a single AZ fails. This means that we need to have enough spare memory capacity left on cells to cover deploying apps from the failed zone. In case of 3 zones, that means each zone should be able to host 50% more apps (memory capacity wise). We can calculate maximum memory usable by all orgs by doing sum of their quotas. However, in practice much less memory is consumed. This is because\\n1. Org quotas come in T-shirt sizes and have considerable size jumps (e.g. 2, 10, 60 100G). You need to reserve next quota if previous one is too small for your needs, yet it doesn't mean you will be using all the capacity of the larger quota.\\n1. App instance memory limits are set as upper memory consumption limit. Because of that, they tend to be set larger for safety. Actual app memory consumption is always lower, many times considerably.\\nPractical example - this is a snapsot of our prod deployment in Feb 2017:\\n```\\nMemory reserved by orgs: 368640 MB (360 GB)\\nMemory reserved by apps: 107108 MB (104 GB)\\nMemory actually used by apps: 32868 (32 GB)\\n```\\nThis is not unusual and CF v1 had default overprovisioning factor of 2 (that is, it advertised 2 times more capacity than actual).\\nDecision\\n========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n\n\n##Decision\n========\\nWe will maintain at least 50% of total org reserved capacity available when a zone fails. That is, remaining zones will have to be able to cover 50% of total reserved capacity.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.\\n* We have implemented `show-cf-memory-usage` makefile target to help us get current org and app reservation and real usage totals.\\n","Predictions":"* We will keep at least 50% of total org reserved capacity available when a zone fails. That is, remaining cells will have to be able to cover 50% of total reserved capacity.nStatusn======nAcceptednConsequencesn============n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.n* We have implemented show-cf-memory-usage makefile target to help us get more capacity.n* We will keep at least 50% of total org reserved capacity available when a zone fails. That is, remaining cells will have to be able to cover 50% of total reserved capacity.nStatusn=======nAcceptednConsequencesn=============n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.n* We have implemented show-cf-memory-usage makefile target to help us get more capacity.n* We will keep at least 50% of total org reserved capacity available when a zone fails. That is, remaining cells will have to be able to cover 50% of total reserved capacity.nStatusn=======nAcceptednConsequencesn=============n* We will check if we have enough capacity available whenever we add a new organisation or increase quota of existing one. We will deploy more cells if we need more capacity.n* We have implemented show-cf-memory-usage makefile target to help us get more capacity.n* We will keep at least 50% of total org reserved capacity available when a zone fails. That is,"}
{"File Name":"paas-team-manual\/ADR020-deletion_of_ci_environment.html.md","Context":"## Context\nWe have three environments in our deployment pipeline. Two non-production ones - CI and Staging and one Production. We think that it takes to much time for a change to reach production state in the current setup. We don't think having two environments before production is providing us much value, compared to the cost of running, maintaining, and waiting for deployments to be promoted.\n","Decision":"We will delete CI environment and migrate it's customizations, like tests , apps etc. to staging. We have decided to delete CI instead of staging as we want to separate build CI in it's own AWS account. Also, staging environment has valid certificates.\\n","tokens":78,"id":187,"text":"## Context\\nWe have three environments in our deployment pipeline. Two non-production ones - CI and Staging and one Production. We think that it takes to much time for a change to reach production state in the current setup. We don't think having two environments before production is providing us much value, compared to the cost of running, maintaining, and waiting for deployments to be promoted.\\n\n\n##Decision\nWe will delete CI environment and migrate it's customizations, like tests , apps etc. to staging. We have decided to delete CI instead of staging as we want to separate build CI in it's own AWS account. Also, staging environment has valid certificates.\\n","Predictions":"1. Flexibilityn2. How easy it is to implement and follow all the best standardsn3. Implementation difficultynWe have decided to proceed with Airbyte. It meets almost every specification that we have. It is extremelly easy to implement and follows all the best standards. It isn't an in-house solution, but in the current scenario we're on that would not be a problem.n"}
{"File Name":"katas\/003-katafication.md","Context":"## Context\nAll katas in here are supposed to be used to fix failing tests.\nThe problem with storing failing tests (the katas) in the repo is that there is no\nway but fixing them each time to find out if they actually work or the engine\nthe kata is run on supports a certain feature.\n","Decision":"All tests have to pass and the special comment `\/\/\/\/` marks how to katafy\\nthis test.\\nFor example:\\n```\\n\/\/\/\/ const expectedType = 'not a function!';\\nconst expectedType = 'object';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nThe above test passes. Katafy means uncomment the first line and remove the following line, so\\nthat the code looks like this afterwards:\\n```\\nconst expectedType = 'not a function!';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nNow this test fails, and it can be used as a kata.\\n","tokens":66,"id":4338,"text":"## Context\\nAll katas in here are supposed to be used to fix failing tests.\\nThe problem with storing failing tests (the katas) in the repo is that there is no\\nway but fixing them each time to find out if they actually work or the engine\\nthe kata is run on supports a certain feature.\\n\n\n##Decision\nAll tests have to pass and the special comment `\/\/\/\/` marks how to katafy\\nthis test.\\nFor example:\\n```\\n\/\/\/\/ const expectedType = 'not a function!';\\nconst expectedType = 'object';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nThe above test passes. Katafy means uncomment the first line and remove the following line, so\\nthat the code looks like this afterwards:\\n```\\nconst expectedType = 'not a function!';\\nassert.equal(typeof Reflect, expectedType);\\n```\\nNow this test fails, and it can be used as a kata.\\n","Predictions":"We will use a \/var\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src\/var\/src"}
{"File Name":"mediawiki-extensions-WikibaseLexeme\/0003-adr-with-all-team-members.md","Context":"## Context\nWe use architectural decision records (ADR) on this project.\nThe project's code base is owned by the team, the team organizes itself (see Journey model) into smaller, short lived \"journey\" sub-units to - amongst others - increase focus. Architecture decisions affect the project for a long time, and will likely soon be faced by developers that were not part of the journey at the time. Consequently their feedback about the architectural decision is inevitable. Additionally, given the intentionally small size of an individual journey's group, the amount of opinions concerning any given ADR could be as small as one or two, should the ADR be voted upon by members of the same journey exclusively. To avoid a flood of ADRs trying to unwrite each other and to increase the standing of ADRs in general and the quality of the individual ADR they should be vetted (RFC) by the entire team.\nTo avoid long-running feedback loops that block the individual journey team from fulfilling their commitments RFCs should be time-limited.\n","Decision":"We put proposed Architecture Decision Records up for feedback by the entire team. ADRs will be proposed as dedicated changes and iterated upon through the tools provided by our code review system. ADR RFCs will have a deadline; it should be no sooner than one working day after initial request.\\n","tokens":214,"id":1563,"text":"## Context\\nWe use architectural decision records (ADR) on this project.\\nThe project's code base is owned by the team, the team organizes itself (see Journey model) into smaller, short lived \"journey\" sub-units to - amongst others - increase focus. Architecture decisions affect the project for a long time, and will likely soon be faced by developers that were not part of the journey at the time. Consequently their feedback about the architectural decision is inevitable. Additionally, given the intentionally small size of an individual journey's group, the amount of opinions concerning any given ADR could be as small as one or two, should the ADR be voted upon by members of the same journey exclusively. To avoid a flood of ADRs trying to unwrite each other and to increase the standing of ADRs in general and the quality of the individual ADR they should be vetted (RFC) by the entire team.\\nTo avoid long-running feedback loops that block the individual journey team from fulfilling their commitments RFCs should be time-limited.\\n\n\n##Decision\nWe put proposed Architecture Decision Records up for feedback by the entire team. ADRs will be proposed as dedicated changes and iterated upon through the tools provided by our code review system. ADR RFCs will have a deadline; it should be no sooner than one working day after initial request.\\n","Predictions":"We put proposed Architecture Decision Records up for feedback by the entire team. ADRs will be proposed as dedicated changes and iterated upon through the tools provided by our code review system. ADR RFCs will have a deadline; it should be no sooner than one working day after initial request.n"}
{"File Name":"claim-additional-payments-for-teaching\/0004-deployment-on-heroku.md","Context":"## Context\nDepartment for Education have a Cloud Infrastructure Program based on Azure that\nthey would like digital services to use. Access to Azure is heavily restricted\nfor production, and slightly restricted for lower environments.\nWe need to be able to work quickly, particularly in the early stages of this\nproject.\nWe need to be able to deploy prototypes and experimental features and versions\nof the service for user research.\n","Decision":"We will use Heroku to deploy the application.\\nWe will use Heroku's pipeline feature to run CI and deploy the application.\\n","tokens":84,"id":2093,"text":"## Context\\nDepartment for Education have a Cloud Infrastructure Program based on Azure that\\nthey would like digital services to use. Access to Azure is heavily restricted\\nfor production, and slightly restricted for lower environments.\\nWe need to be able to work quickly, particularly in the early stages of this\\nproject.\\nWe need to be able to deploy prototypes and experimental features and versions\\nof the service for user research.\\n\n\n##Decision\nWe will use Heroku to deploy the application.\\nWe will use Heroku's pipeline feature to run CI and deploy the application.\\n","Predictions":"We will use Azure to deploy the application.nWe will use Heroku's pipeline feature to run CI and deploy the application.n"}
{"File Name":"open-apparel-registry\/adr-002-decide-how-to-display-more-facilities.md","Context":"## Context\nThe Open Apparel Registry currently includes more than 18,000 facilities. For\nperformance reasons, we have paginated the facilities data API endpoint data so\nthat it will [return a maximum of 500 results][pagination-pr] for any single\nrequest. In turn this means that the frontend client will only ever display a\nmaximum of 500 facilities at a time, rendered as clustered Leaflet markers via\nReact-Leaflet. Facilities API requests are currently filtered using Django\nquerysets whose inputs are querystring parameters included in the API requests.\nTo enable users to view all of the OAR's facilities on the map simultaneously,\nwe'll need to update how the API returns facilities for display and how the\nclient renders them on the map. At present this means updating the application\nso that it can display 18,000+ facilities simultaneously. Following upcoming MSI\nintegration work, we anticipate that the number of OAR facilities will increase\nto around 100,000 -- which the application should be able to map. In addition,\nwe also want users to be able to filter these vector tiles by query parameters\nlike contributor, facility name, and country, along with the map bounding box.\nTo accomplish this we have decided to use vector tiles generated, ultimately,\nby PostGIS's [`ST_AsMVT`][st-asmvt] function, rendering them in the frontend\nwith [Leaflet Vector Grid][leaflet-vector-grid] (possibly via\n[react-leaflet-vector-grid][react-leaflet-vector-grid]). We've decided to have\nthe vector tiles cluster facilities by zoom level, which would limit the number\nof actual points the frontend needs to display at any given time.\nThis ADR documents a subsequent decision between setting up a dedicated\n`ST_AsMVT`-based vector tile server, like [Martin][martin] or adding a new\nvector tile endpoint to the existing Django web application which would make\nthe `ST_AsMVT` query.\n","Decision":"We have decided to add a vector tile endpoint to the existing Django app.\\nWhile Martin, in particular, seemed like a compelling solution, we had enough\\nopen questions about it to discourage us from taking on the complexity of\\nusing it here.\\nOur main apprehension about adding a tile endpoint to the existing web app is\\nthat it'll mingle tile requests with other requests in a way that could cause\\nperformance problems. However, given the size of the OAR's traffic and the\\npossibility of addressing traffic increases by scaling the number of app\\ninstances, this seemed like an acceptable tradeoff.\\n","tokens":412,"id":4281,"text":"## Context\\nThe Open Apparel Registry currently includes more than 18,000 facilities. For\\nperformance reasons, we have paginated the facilities data API endpoint data so\\nthat it will [return a maximum of 500 results][pagination-pr] for any single\\nrequest. In turn this means that the frontend client will only ever display a\\nmaximum of 500 facilities at a time, rendered as clustered Leaflet markers via\\nReact-Leaflet. Facilities API requests are currently filtered using Django\\nquerysets whose inputs are querystring parameters included in the API requests.\\nTo enable users to view all of the OAR's facilities on the map simultaneously,\\nwe'll need to update how the API returns facilities for display and how the\\nclient renders them on the map. At present this means updating the application\\nso that it can display 18,000+ facilities simultaneously. Following upcoming MSI\\nintegration work, we anticipate that the number of OAR facilities will increase\\nto around 100,000 -- which the application should be able to map. In addition,\\nwe also want users to be able to filter these vector tiles by query parameters\\nlike contributor, facility name, and country, along with the map bounding box.\\nTo accomplish this we have decided to use vector tiles generated, ultimately,\\nby PostGIS's [`ST_AsMVT`][st-asmvt] function, rendering them in the frontend\\nwith [Leaflet Vector Grid][leaflet-vector-grid] (possibly via\\n[react-leaflet-vector-grid][react-leaflet-vector-grid]). We've decided to have\\nthe vector tiles cluster facilities by zoom level, which would limit the number\\nof actual points the frontend needs to display at any given time.\\nThis ADR documents a subsequent decision between setting up a dedicated\\n`ST_AsMVT`-based vector tile server, like [Martin][martin] or adding a new\\nvector tile endpoint to the existing Django web application which would make\\nthe `ST_AsMVT` query.\\n\n\n##Decision\nWe have decided to add a vector tile endpoint to the existing Django app.\\nWhile Martin, in particular, seemed like a compelling solution, we had enough\\nopen questions about it to discourage us from taking on the complexity of\\nusing it here.\\nOur main apprehension about adding a tile endpoint to the existing web app is\\nthat it'll mingle tile requests with other requests in a way that could cause\\nperformance problems. However, given the size of the OAR's traffic and the\\npossibility of addressing traffic increases by scaling the number of app\\ninstances, this seemed like an acceptable tradeoff.\\n","Predictions":"We will use the [React-Leaflet][react-leaflet] to render the frontend.n"}
{"File Name":"kitsune\/0002-es-l10n-content.md","Context":"## Context\nKitsune supports many locales,\nand has content which we want to be searchable in those locales.\nElasticsearch has support for many language-specific analyzers:\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/analysis-lang-analyzer.html\nSearch v1 used per-document analyzers,\nthat is to say, within the same index:\n```\ndoc_1: { \"content\": \"Hello world\" }\ndoc_2: { \"content\": \"Hallo Welt\" }\n```\n`doc_1.content` could be analyzed using an english analyzer,\nand `doc_2.content` could be analyzed using a german analyzer.\nWell before version 7 ES removed this feature,\nand now all fields of the same name across an index must be analyzed the same,\nso we must take a different approach with the current Search implementation.\nWe can either place separate locales in their own index,\nand set up locale-specific analyzers for the same field name across indices.\nOr we can keep separate locales within the same index,\nand define unique field names for each field which needs to be analyzed under a specific locale.\n","Decision":"Heavily influenced by: https:\/\/www.elastic.co\/blog\/multilingual-search-using-language-identification-in-elasticsearch\\nWe will store all documents within the same index and use an Object field for fields which need to use locale-specific analyzers.\\nWe will call this field `SumoLocaleAwareTextField` and will have a key for each locale,\\nwith the appropriate analyzer defined on that key,\\nsuch that:\\n```\\ndoc_1: { \"content\": { \"en-US\": \"Hello world\" }}\\ndoc_2: { \"content\": { \"de\": \"Hallo Welt\" }}\\n```\\n`doc_1.content.en-US` is analyzed using an english analyzer,\\nand `doc_2.content.de` is analyzed using a german analyzer.\\n","tokens":249,"id":102,"text":"## Context\\nKitsune supports many locales,\\nand has content which we want to be searchable in those locales.\\nElasticsearch has support for many language-specific analyzers:\\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/analysis-lang-analyzer.html\\nSearch v1 used per-document analyzers,\\nthat is to say, within the same index:\\n```\\ndoc_1: { \"content\": \"Hello world\" }\\ndoc_2: { \"content\": \"Hallo Welt\" }\\n```\\n`doc_1.content` could be analyzed using an english analyzer,\\nand `doc_2.content` could be analyzed using a german analyzer.\\nWell before version 7 ES removed this feature,\\nand now all fields of the same name across an index must be analyzed the same,\\nso we must take a different approach with the current Search implementation.\\nWe can either place separate locales in their own index,\\nand set up locale-specific analyzers for the same field name across indices.\\nOr we can keep separate locales within the same index,\\nand define unique field names for each field which needs to be analyzed under a specific locale.\\n\n\n##Decision\nHeavily influenced by: https:\/\/www.elastic.co\/blog\/multilingual-search-using-language-identification-in-elasticsearch\\nWe will store all documents within the same index and use an Object field for fields which need to use locale-specific analyzers.\\nWe will call this field `SumoLocaleAwareTextField` and will have a key for each locale,\\nwith the appropriate analyzer defined on that key,\\nsuch that:\\n```\\ndoc_1: { \"content\": { \"en-US\": \"Hello world\" }}\\ndoc_2: { \"content\": { \"de\": \"Hallo Welt\" }}\\n```\\n`doc_1.content.en-US` is analyzed using an english analyzer,\\nand `doc_2.content.de` is analyzed using a german analyzer.\\n","Predictions":"We will store all documents within the same index and use an Object field for fields which need to use locale-specific analyzers.nWe will call this field SumoLocaleAwareTextField and will have a key for each locale,nwith the appropriate analyzer defined on that key,nsuch as:ndoc_1:  \"content\":  \"en-US\": \"Hello world\"ndoc_2:  \"content\":  \"de\": \"Hallo Welt\"nndoc_1.content.en-US is analyzed using an English analyzer,nand doc_2.content.en-US is analyzed using a German analyzer.nWe will store all documents within the same index and use an Object field for fields which need to use locale-specific analyzers.nWe will call this field SumoLocaleAwareTextField and will have a key for each locale,nwith the appropriate analyzer defined on that key,nsuch as:ndoc_1:  \"content\":  \"en-US\": \"Hello world\"ndoc_2:  \"content\":  \"de\": \"Hallo Welt\"nndoc_1.content.en-US is analyzed using an English analyzer,nand doc_2.content.en-US is analyzed using a German analyzer.n"}
{"File Name":"architecture\/0004-waiting-is-waste.md","Context":"## Context\nHaving a developer waiting for any process to finish is a stupid thing. It's not only that it's to long for the developer to wait, it also hinders you from fixing issues fast.\n","Decision":"Any processes and workflows need to be very fast and fully automated.\\n","tokens":43,"id":1434,"text":"## Context\\nHaving a developer waiting for any process to finish is a stupid thing. It's not only that it's to long for the developer to wait, it also hinders you from fixing issues fast.\\n\n\n##Decision\nAny processes and workflows need to be very fast and fully automated.\\n","Predictions":"We will keep a collection of records for \"arch\" in the project's documentation.n"}
{"File Name":"terraform-aws-dynamodb\/0007-map-list-variables.md","Context":"## Context\nDynamoDB provides the ability to supply additional attributes, a local\nsecondary index and a global secondary index. These additional attributes\nconsumed by the DynamoDB AWS Provider as maps\/lists.\nIn addtion to the consumption as maps\/lists there are additional requirements\nthat the range\/hash keys be added to the additional attributes if declared. They\nare not added if undeclared.\n","Decision":"The module will use a `null_resource` to manage the secondary indexes. Creating\\nthem from the existences of appropriate maps\/lists.\\nProperties related to these resources will consume a list of maps as input.\\nThese will them be mapped to the appropriate resource within in the module.\\nThe range\/hash key will be added automatically to the additional attributes by\\nthe module, reducing the load on the consumer with implementation detail.\\n","tokens":80,"id":2543,"text":"## Context\\nDynamoDB provides the ability to supply additional attributes, a local\\nsecondary index and a global secondary index. These additional attributes\\nconsumed by the DynamoDB AWS Provider as maps\/lists.\\nIn addtion to the consumption as maps\/lists there are additional requirements\\nthat the range\/hash keys be added to the additional attributes if declared. They\\nare not added if undeclared.\\n\n\n##Decision\nThe module will use a `null_resource` to manage the secondary indexes. Creating\\nthem from the existences of appropriate maps\/lists.\\nProperties related to these resources will consume a list of maps as input.\\nThese will them be mapped to the appropriate resource within in the module.\\nThe range\/hash key will be added automatically to the additional attributes by\\nthe module, reducing the load on the consumer with implementation detail.\\n","Predictions":"The module will use a ull_resource to manage the secondary indexes. Creatingnthem from the existences of appropriate maps\/lists.nProperties related to these resources will consume a list of maps as input.nThese will them be mapped to the appropriate resource within in the module, reducing the load on the consumer with implementation detail.n"}
{"File Name":"k8s-vagrant-centos-cluster\/0002-why-not-use-kubeadm.md","Context":"## Context\nkubeadm can be used to setup Kubernetes cluster with apiserver, etcd, controller, scheduler...\n","Decision":"Setup Kubernetes cluster with apiserver, etcd, controller, scheduler without using kubeadm and docker\\n","tokens":26,"id":3440,"text":"## Context\\nkubeadm can be used to setup Kubernetes cluster with apiserver, etcd, controller, scheduler...\\n\n\n##Decision\nSetup Kubernetes cluster with apiserver, etcd, controller, scheduler without using kubeadm and docker\\n","Predictions":"We will use a stack class to store the stack file.n"}
{"File Name":"csc-swr-architecture\/004-Use-Fake-Data.html.md.erb","Context":"## Context\nThe Childrens Social Care Placement Service is in the [Alpha] phase of delivery. As such it is not expected to process real data for users, more to the point, it is an exploratory phase for building prototypes and testing different ideas.\n","Decision":"The placement alpha prototypes and development work will only process fake data.\\nThe reasoning for this is to allow the Service Team to focus on developing features in fast iteration feedback loops, rather than undertaking the additional non-functional needs for processing live data. The Security needs, including passing the \"Authority to Operate\" governance gateway alone, would jeopardise the desired delivery timeframe for Alpha of January 2021.\\n","tokens":53,"id":915,"text":"## Context\\nThe Childrens Social Care Placement Service is in the [Alpha] phase of delivery. As such it is not expected to process real data for users, more to the point, it is an exploratory phase for building prototypes and testing different ideas.\\n\n\n##Decision\nThe placement alpha prototypes and development work will only process fake data.\\nThe reasoning for this is to allow the Service Team to focus on developing features in fast iteration feedback loops, rather than undertaking the additional non-functional needs for processing live data. The Security needs, including passing the \"Authority to Operate\" governance gateway alone, would jeopardise the desired delivery timeframe for Alpha of January 2021.\\n","Predictions":"The decision is to use the [Gov.UK PaaS].n**What does Gov.UK PaaS offer?**nGov.UK PaaS is a fully managed platform service which has central government accredition for Offical data and provides teams with:-n* Fast application delivery, with short cyclesn* Flexible development toolsn* Simplified DevOpsn* Seamless scalabilityn* Integrated data servicesn* A secure, trusted platformn* 24x7 supportnIt is also important to note that Ruby on Rails is a first class citizen on Gov.UK as one of its core buildpack languages and frameworks.n**Why not DfE Azure?**nThere are two primary reasons why; simplified DevOps and increased developer productivity.n**Is it secure?**nOne of the main drivers for Departmental Platforms is security assurance. Gov.UK PaaS is accredited by for Official information.nSome useful URLs for security in general on Gov.UK PaaS are [here](https:\/\/www.cloud.service.gov.uk\/security\/) and the [security principles](https:\/\/www.gov.uk\/security\/principles\/).nSome useful URLs for security in general on Gov.UK PaaS are [here](https:\/\/www.cloud.service.gov.uk\/security\/principles\/).n"}
{"File Name":"js-sdk\/0003-allow-only-tft-token.md","Context":"## Context\nDrop FreeTFT, TFTA tokens from SDK\n","Decision":"Completely drop FreeTFT, TFTA from currency options and don't ask for it at all. should always be TFT in the UI\\n","tokens":14,"id":5198,"text":"## Context\\nDrop FreeTFT, TFTA tokens from SDK\\n\n\n##Decision\nCompletely drop FreeTFT, TFTA from currency options and don't ask for it at all. should always be TFT in the UI\\n","Predictions":"We will use a stt-api-gateway to connect to the S3 bucket.n"}
{"File Name":"dos-server\/adr-14-persistent-file-links.md","Context":"## Context\nDOS returns internally generated unique identifiers for files it ingests.\nThe URLs are roughly of the form (at the time of this ADR, plain integer identifiers are returned):\nhttps:\/\/dos-stage.mitlib.net\/file?id=123e4567-e89b-12d3-a456-556642440000\nOnce files are ingested, the identifiers are stored in ArchivesSpace.\nThis may result in a situation where if URLs to files need to change (e.g., if\nthe AWS bucket name for DOS changes), all links in ArchivesSpace need to be updated for\nall ingested files.\nAlthough DOS can be made to generate persistent identifies, it is desirable to externalize\nthis functionality for reuse and portability considerations.\nThe Handle System can be used for this purpose. Handles are already being used in\nDome and DSpace, and there is organizational familiarity with the system.\n","Decision":"Handle System will be used to generate persistent links. DOS will use the\\nHandle server API to generate handles. Handles will be returned when the\\nrelevant DOS end point is invoked.\\n","tokens":187,"id":3369,"text":"## Context\\nDOS returns internally generated unique identifiers for files it ingests.\\nThe URLs are roughly of the form (at the time of this ADR, plain integer identifiers are returned):\\nhttps:\/\/dos-stage.mitlib.net\/file?id=123e4567-e89b-12d3-a456-556642440000\\nOnce files are ingested, the identifiers are stored in ArchivesSpace.\\nThis may result in a situation where if URLs to files need to change (e.g., if\\nthe AWS bucket name for DOS changes), all links in ArchivesSpace need to be updated for\\nall ingested files.\\nAlthough DOS can be made to generate persistent identifies, it is desirable to externalize\\nthis functionality for reuse and portability considerations.\\nThe Handle System can be used for this purpose. Handles are already being used in\\nDome and DSpace, and there is organizational familiarity with the system.\\n\n\n##Decision\nHandle System will be used to generate persistent links. DOS will use the\\nHandle server API to generate handles. Handles will be returned when the\\nrelevant DOS end point is invoked.\\n","Predictions":"We will use the [SecurityId](https:\/\/github.com\/securityid\/securityid) to generate the identifiers for the files.n"}
{"File Name":"tendermint\/adr-063-privval-grpc.md","Context":"## Context\nValidators use remote signers to help secure their keys. This system is Tendermint's recommended way to secure validators, but the path to integration with Tendermint's private validator client is plagued with custom protocols.\nTendermint uses its own custom secure connection protocol (`SecretConnection`) and a raw tcp\/unix socket connection protocol. The secure connection protocol until recently was exposed to man in the middle attacks and can take longer to integrate if not using Golang. The raw tcp connection protocol is less custom, but has been causing minute issues with users.\nMigrating Tendermint's private validator client to a widely adopted protocol, gRPC, will ease the current maintenance and integration burden experienced with the current protocol.\n","Decision":"After discussing with multiple stake holders, [gRPC](https:\/\/grpc.io\/) was decided on to replace the current private validator protocol. gRPC is a widely adopted protocol in the micro-service and cloud infrastructure world. gRPC uses [protocol-buffers](https:\/\/developers.google.com\/protocol-buffers) to describe its services, providing a language agnostic implementation. Tendermint uses protobuf for on disk and over the wire encoding already making the integration with gRPC simpler.\\n","tokens":147,"id":1949,"text":"## Context\\nValidators use remote signers to help secure their keys. This system is Tendermint's recommended way to secure validators, but the path to integration with Tendermint's private validator client is plagued with custom protocols.\\nTendermint uses its own custom secure connection protocol (`SecretConnection`) and a raw tcp\/unix socket connection protocol. The secure connection protocol until recently was exposed to man in the middle attacks and can take longer to integrate if not using Golang. The raw tcp connection protocol is less custom, but has been causing minute issues with users.\\nMigrating Tendermint's private validator client to a widely adopted protocol, gRPC, will ease the current maintenance and integration burden experienced with the current protocol.\\n\n\n##Decision\nAfter discussing with multiple stake holders, [gRPC](https:\/\/grpc.io\/) was decided on to replace the current private validator protocol. gRPC is a widely adopted protocol in the micro-service and cloud infrastructure world. gRPC uses [protocol-buffers](https:\/\/developers.google.com\/protocol-buffers) to describe its services, providing a language agnostic implementation. Tendermint uses protobuf for on disk and over the wire encoding already making the integration with gRPC simpler.\\n","Predictions":"After discussing with multiple stake holders, [gRPC](https:\/\/grpc.io\/) was decided on to replace the current private validator protocol. gRPC is a widely adopted protocol in the micro-service and cloud infrastructure world. gRPC uses [protocol-buffers](https:\/\/developers.google.com\/protocol-buffers) to describe its services, providing a language agnostic implementation. Tendermint uses protobuf for on disk and over the wire encoding already making the integration with gRPC simpler.n"}
{"File Name":"archcolider\/011 Every meal delivery has pick up pin code.md","Context":"## Context\nNetwork connection might be lost, when a meal is delivered to a fridge and a user comes to grab it. Then the fridge can't check data of the user online by card swapping, or in-app distance opening.\nBut the fridge has a pin pad keyboard and still has quite sophisticated software and internal memory to process orders.\nUpdate 2020-11-24:\nWe expect that every meal has its own unique id provided by the kitchen because some meals might be customized from the general catalog. Let's say lactose-free lasagna should be addressed to a specific user.\nThen, at the purchase or production process, we can update the user's device with the meal's unique id and generate an access code based on meal ID.\n","Decision":"Meals dispatched from a Ghost Kitchen, will have a special 6-8 digit code.\\n","tokens":154,"id":2678,"text":"## Context\\nNetwork connection might be lost, when a meal is delivered to a fridge and a user comes to grab it. Then the fridge can't check data of the user online by card swapping, or in-app distance opening.\\nBut the fridge has a pin pad keyboard and still has quite sophisticated software and internal memory to process orders.\\nUpdate 2020-11-24:\\nWe expect that every meal has its own unique id provided by the kitchen because some meals might be customized from the general catalog. Let's say lactose-free lasagna should be addressed to a specific user.\\nThen, at the purchase or production process, we can update the user's device with the meal's unique id and generate an access code based on meal ID.\\n\n\n##Decision\nMeals dispatched from a Ghost Kitchen, will have a special 6-8 digit code.\\n","Predictions":"We will use the SMS field in the email to send notifications.n"}
{"File Name":"beis-report-official-development-assistance\/0004-add-rollbar-for-application-monitoring.md","Context":"## Context\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\n","Decision":"Use Rollbar to collect and manage our application errors.\\n","tokens":187,"id":2387,"text":"## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n\n\n##Decision\nUse Rollbar to collect and manage our application errors.\\n","Predictions":"Use Rollbar to collect and manage our application errors.n"}
{"File Name":"operational-data-hub\/0029-components-are-named-according-to-naming-conventions.md","Context":"## Context\nOrganize your cloud assets to support operational management and accounting requirements. Well-defined naming and metadata tagging conventions help to quickly locate and manage resources. These conventions also help associate cloud usage costs with business teams via chargeback and showback accounting mechanisms.\n","Decision":"The naming cloud object naming convention is based on the Ironstone Naming conventions and can be found on our interal Wiki\\n","tokens":52,"id":2757,"text":"## Context\\nOrganize your cloud assets to support operational management and accounting requirements. Well-defined naming and metadata tagging conventions help to quickly locate and manage resources. These conventions also help associate cloud usage costs with business teams via chargeback and showback accounting mechanisms.\\n\n\n##Decision\nThe naming cloud object naming convention is based on the Ironstone Naming conventions and can be found on our interal Wiki\\n","Predictions":"We will use the Security model to provide a way to manage the security of the data we store.n"}
{"File Name":"adr\/ADR-24-decouple-kernel-and-unity-apis.md","Context":"## Context and Problem Statement\nAs of today, kernel and unity have huge dependencies in business logic: sagas in TS, requests are made in TS, and everything is used from Unity.\nThat generates coupling between technologies and teams, generating blockers and planning problems.\n","Decision":"* Go for this proposal \u2705\\n### Actionables\\n- When starting development on new features, aim for this approach from the get-go\\n- Features that are already advanced in development or finished will be refactored gradually\\n- We will start by exposing signed requests and migrating the quests feature saga\\nNew features for the ECS\/SDK will have the common product process (RFC, roadmap, backlog, refinement). Alvaro will gather together the team when necessary to discuss implementation of new changes to the SDK.\\nCLI should be handled in the same way as the kernel and SDK developments.\\n","tokens":54,"id":4614,"text":"## Context and Problem Statement\\nAs of today, kernel and unity have huge dependencies in business logic: sagas in TS, requests are made in TS, and everything is used from Unity.\\nThat generates coupling between technologies and teams, generating blockers and planning problems.\\n\n\n##Decision\n* Go for this proposal \u2705\\n### Actionables\\n- When starting development on new features, aim for this approach from the get-go\\n- Features that are already advanced in development or finished will be refactored gradually\\n- We will start by exposing signed requests and migrating the quests feature saga\\nNew features for the ECS\/SDK will have the common product process (RFC, roadmap, backlog, refinement). Alvaro will gather together the team when necessary to discuss implementation of new changes to the SDK.\\nCLI should be handled in the same way as the kernel and SDK developments.\\n","Predictions":"* Go for this proposal n### Actionablesn- When starting development on new features, aim for this approach from the get-gon- Features that are already advanced in development or finished will be refactored graduallyn- We will start by exposing signed requests and migrating the quests feature saganNew features for the ECS\/SDK will have the common product process (RFC, roadmap, backlog, refinement). Alvaro will gather together the team when necessary to discuss implementation of new changes to the SDK.nCLI should be handled in the same way as the kernel and SDK developments.n"}
{"File Name":"openchs-adr\/0005-deploy-openchs-server-on-the-cloud.md","Context":"## Context\n1. Cost - Most users of OpenCHS are NGOs having 5-10 health workers. A running postgres instance and a tomcat server per installation is hard to support\/maintain.\n2. Availability of internet - While users might not have internet connectivity in the field, they are usually attached to a head office where there is internet connectivity. They usually come to this common location regularly. Since sync is the only reason for internet connectivity, it can be limited during these visits.\n3. Future direction - 3g\/4g is getting more common across the country, and cloud-based solutions are also getting more pervasive.\n","Decision":"Openchs server will have capabilities to be deployed on the cloud\\n","tokens":132,"id":2824,"text":"## Context\\n1. Cost - Most users of OpenCHS are NGOs having 5-10 health workers. A running postgres instance and a tomcat server per installation is hard to support\/maintain.\\n2. Availability of internet - While users might not have internet connectivity in the field, they are usually attached to a head office where there is internet connectivity. They usually come to this common location regularly. Since sync is the only reason for internet connectivity, it can be limited during these visits.\\n3. Future direction - 3g\/4g is getting more common across the country, and cloud-based solutions are also getting more pervasive.\\n\n\n##Decision\nOpenchs server will have capabilities to be deployed on the cloud\\n","Predictions":"We will use the SMS field in all emails sent to users of any newly built CCS digital services.nThis will ensure that Notify offers the same user experience across all CCS digital services.n"}
{"File Name":"tendermint\/adr-014-secp-malleability.md","Context":"## Context\nSecp256k1 has two layers of malleability.\nThe signer has a random nonce, and thus can produce many different valid signatures.\nThis ADR is not concerned with that.\nThe second layer of malleability basically allows one who is given a signature\nto produce exactly one more valid signature for the same message from the same public key.\n(They don't even have to know the message!)\nThe math behind this will be explained in the subsequent section.\nNote that in many downstream applications, signatures will appear in a transaction, and therefore in the tx hash.\nThis means that if someone broadcasts a transaction with secp256k1 signature, the signature can be altered into the other form by anyone in the p2p network.\nThus the tx hash will change, and this altered tx hash may be committed instead.\nThis breaks the assumption that you can broadcast a valid transaction and just wait for its hash to be included on chain.\nOne example is if you are broadcasting a tx in cosmos,\nand you wait for it to appear on chain before incrementing your sequence number.\nYou may never increment your sequence number if a different tx hash got committed.\nRemoving this second layer of signature malleability concerns could ease downstream development.\n### ECDSA context\nSecp256k1 is ECDSA over a particular curve.\nThe signature is of the form `(r, s)`, where `s` is a field element.\n(The particular field is the `Z_n`, where the elliptic curve has order `n`)\nHowever `(r, -s)` is also another valid solution.\nNote that anyone can negate a group element, and therefore can get this second signature.\n","Decision":"We can just distinguish a canonical form for the ECDSA signatures.\\nThen we require that all ECDSA signatures be in the form which we defined as canonical.\\nWe reject signatures in non-canonical form.\\nA canonical form is rather easy to define and check.\\nIt would just be the smaller of the two values for `s`, defined lexicographically.\\nThis is a simple check, instead of checking if `s < n`, instead check `s <= (n - 1)\/2`.\\nAn example of another cryptosystem using this\\nis the parity definition here https:\/\/github.com\/zkcrypto\/pairing\/pull\/30#issuecomment-372910663.\\nThis is the same solution Ethereum has chosen for solving secp malleability.\\n","tokens":356,"id":1957,"text":"## Context\\nSecp256k1 has two layers of malleability.\\nThe signer has a random nonce, and thus can produce many different valid signatures.\\nThis ADR is not concerned with that.\\nThe second layer of malleability basically allows one who is given a signature\\nto produce exactly one more valid signature for the same message from the same public key.\\n(They don't even have to know the message!)\\nThe math behind this will be explained in the subsequent section.\\nNote that in many downstream applications, signatures will appear in a transaction, and therefore in the tx hash.\\nThis means that if someone broadcasts a transaction with secp256k1 signature, the signature can be altered into the other form by anyone in the p2p network.\\nThus the tx hash will change, and this altered tx hash may be committed instead.\\nThis breaks the assumption that you can broadcast a valid transaction and just wait for its hash to be included on chain.\\nOne example is if you are broadcasting a tx in cosmos,\\nand you wait for it to appear on chain before incrementing your sequence number.\\nYou may never increment your sequence number if a different tx hash got committed.\\nRemoving this second layer of signature malleability concerns could ease downstream development.\\n### ECDSA context\\nSecp256k1 is ECDSA over a particular curve.\\nThe signature is of the form `(r, s)`, where `s` is a field element.\\n(The particular field is the `Z_n`, where the elliptic curve has order `n`)\\nHowever `(r, -s)` is also another valid solution.\\nNote that anyone can negate a group element, and therefore can get this second signature.\\n\n\n##Decision\nWe can just distinguish a canonical form for the ECDSA signatures.\\nThen we require that all ECDSA signatures be in the form which we defined as canonical.\\nWe reject signatures in non-canonical form.\\nA canonical form is rather easy to define and check.\\nIt would just be the smaller of the two values for `s`, defined lexicographically.\\nThis is a simple check, instead of checking if `s < n`, instead check `s <= (n - 1)\/2`.\\nAn example of another cryptosystem using this\\nis the parity definition here https:\/\/github.com\/zkcrypto\/pairing\/pull\/30#issuecomment-372910663.\\nThis is the same solution Ethereum has chosen for solving secp malleability.\\n","Predictions":"We can just distinguish a canonical form for the ECDSA signatures.nThen we require that all ECDSA signatures be in the form which we defined as canonical.nWe reject signatures in non-canonical form.n"}
{"File Name":"dogma\/0008-location-of-testing-features.md","Context":"## Context\nWe need to decide whether Dogma's testing features should reside in the `dogma`\nmodule itself, or a separate `dogmatest` module.\n### Benefits to separate module\n1. They can be versioned separately. A `dogma` release can be made without\nhaving to update the testing tools. This is a _pro_ for the releaser, but\npossibly a _con_ for the users.\n1. Users that opt not to test, or not to test using our tools do not need to\ndownload `dogmatest` at all. This is not a particularly strong argument.\n1. We can make BC breaking changes to `dogmatest`, without having to make\na new major release of `dogma`. We would need to document clearly which\nmajor `dogma` versions are supported by which major `dogmatest` versions.\n### Benefits to a single module\n1. The user does not need to determine which `dogmatest` version to use with\ntheir application; compatible tools are always available right from `dogma`. If\nwe want to encourage users to test their domain logic using these utilities;\nthey should be easy to access.\n1. As mentioned in [#16](github.com\/dogmatiq\/dogma), splitting the testing\ntools would mean that the example code would also need to be moved elsewhere.\nHowever, we have since already decided to move the example code to a separate\nmodule in [ADR-7](0007-location-of-examples.md).\n","Decision":"For the same reasons [ADR-7](0007-location-of-examples.md) we've decided to\\nimplement testing tools in their own module.\\n","tokens":324,"id":1608,"text":"## Context\\nWe need to decide whether Dogma's testing features should reside in the `dogma`\\nmodule itself, or a separate `dogmatest` module.\\n### Benefits to separate module\\n1. They can be versioned separately. A `dogma` release can be made without\\nhaving to update the testing tools. This is a _pro_ for the releaser, but\\npossibly a _con_ for the users.\\n1. Users that opt not to test, or not to test using our tools do not need to\\ndownload `dogmatest` at all. This is not a particularly strong argument.\\n1. We can make BC breaking changes to `dogmatest`, without having to make\\na new major release of `dogma`. We would need to document clearly which\\nmajor `dogma` versions are supported by which major `dogmatest` versions.\\n### Benefits to a single module\\n1. The user does not need to determine which `dogmatest` version to use with\\ntheir application; compatible tools are always available right from `dogma`. If\\nwe want to encourage users to test their domain logic using these utilities;\\nthey should be easy to access.\\n1. As mentioned in [#16](github.com\/dogmatiq\/dogma), splitting the testing\\ntools would mean that the example code would also need to be moved elsewhere.\\nHowever, we have since already decided to move the example code to a separate\\nmodule in [ADR-7](0007-location-of-examples.md).\\n\n\n##Decision\nFor the same reasons [ADR-7](0007-location-of-examples.md) we've decided to\\nimplement testing tools in their own module.\\n","Predictions":"1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n1. The evn module should be a separate module for each evn module.n"}
{"File Name":"atlasdb\/0006-create-schema-lock-table-using-configuration.md","Context":"## Context\nDue to [table creation issue](0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md) we need to be able to safely create _lock table as a one off operation for each keyspace that atlas uses. The discussed options include:\n- Have Atlas clients refuse to start if the _locks table is missing and provide a CLI to create it.\n- This does require manual interview for whoever is handling operations\n- Is very, very hard to get wrong\n- Should be easily automatable in most setups, removing the manual step\n- Use the lock service for locking rather than the _locks table, then we don't have to create it.\n- Completely automatic and removes code\n- Leaves open the possibility of locks failing and operations never realising that the issue has been triggered\n- Have each node create a unique table, then run paxos to decide which one is the winner\n- This requires a bunch of extra, error prone code\n- Create an additional entry in the configuration, lockLeader, to denote which host is responsible for creating the locks table.\n- The host whose name is the same as lockLeader will create the lock table, others will wait until the lockLeader is up.\n- Requires all hosts to have the same configuration for lock leader\n","Decision":"We decided to use an extra item of configuration, because:\\n1. Internal tools enable us to be confident that different Atlas servers on the same cluster are configured consistently.\\n2. Running paxos to decide which table is the winner was more complex than anticipated.\\n","tokens":276,"id":3092,"text":"## Context\\nDue to [table creation issue](0002-prevent-tables-from-being-creating-simultaneously-in-cassandra-via-a-locks-table.md) we need to be able to safely create _lock table as a one off operation for each keyspace that atlas uses. The discussed options include:\\n- Have Atlas clients refuse to start if the _locks table is missing and provide a CLI to create it.\\n- This does require manual interview for whoever is handling operations\\n- Is very, very hard to get wrong\\n- Should be easily automatable in most setups, removing the manual step\\n- Use the lock service for locking rather than the _locks table, then we don't have to create it.\\n- Completely automatic and removes code\\n- Leaves open the possibility of locks failing and operations never realising that the issue has been triggered\\n- Have each node create a unique table, then run paxos to decide which one is the winner\\n- This requires a bunch of extra, error prone code\\n- Create an additional entry in the configuration, lockLeader, to denote which host is responsible for creating the locks table.\\n- The host whose name is the same as lockLeader will create the lock table, others will wait until the lockLeader is up.\\n- Requires all hosts to have the same configuration for lock leader\\n\n\n##Decision\nWe decided to use an extra item of configuration, because:\\n1. Internal tools enable us to be confident that different Atlas servers on the same cluster are configured consistently.\\n2. Running paxos to decide which table is the winner was more complex than anticipated.\\n","Predictions":"We decided to use an extra item of configuration, because:n1. Internal tools enable us to be confident that different Atlas servers on the same cluster are configured consistently.n2. Running paxos to decide which table is the winner was more complex than anticipated.n"}
{"File Name":"wikiindex\/adr-003-testing_library.md","Context":"## Context\n* We want to write programmer tests to support a TDD workflow.\n* We want to be able to mock out functions.\n","Decision":"* We will use Midje to test our code.\\n* Despite it's heavy macro design, it allows you to write expressive code and easily makes mocks\\n","tokens":30,"id":1387,"text":"## Context\\n* We want to write programmer tests to support a TDD workflow.\\n* We want to be able to mock out functions.\\n\n\n##Decision\n* We will use Midje to test our code.\\n* Despite it's heavy macro design, it allows you to write expressive code and easily makes mocks\\n","Predictions":"* We will use Jest to test our code.n* Despite it's heavy macro design, it allows you to write expressive code and easily makes mocksn"}
{"File Name":"cena\/0009-test-features-with-cucumber.md","Context":"## Context\nAcceptance tests aim to test the application behaviors as a whole from a consumer point of view.\nThose tests must be understandable by any user, either technical or not and help documenting the provided features.\nDefining new acceptance tests must be easy, through reusable step definitions.\nApplication sources must be self-contained, including the acceptance tests definition and implementation, so that\nacceptance tests can be run during the development lifecycle in a [Behavior-driven development](https:\/\/en.wikipedia.org\/wiki\/Behavior-driven_development)\napproach.\n","Decision":"[Cucumber](https:\/\/cucumber.io\/) will be used to describe and execute acceptance tests in `menu-generation` application.\\n","tokens":111,"id":650,"text":"## Context\\nAcceptance tests aim to test the application behaviors as a whole from a consumer point of view.\\nThose tests must be understandable by any user, either technical or not and help documenting the provided features.\\nDefining new acceptance tests must be easy, through reusable step definitions.\\nApplication sources must be self-contained, including the acceptance tests definition and implementation, so that\\nacceptance tests can be run during the development lifecycle in a [Behavior-driven development](https:\/\/en.wikipedia.org\/wiki\/Behavior-driven_development)\\napproach.\\n\n\n##Decision\n[Cucumber](https:\/\/cucumber.io\/) will be used to describe and execute acceptance tests in `menu-generation` application.\\n","Predictions":"We will use [ADR-007](https:\/\/adr-007.org\/) to describe and execute acceptance tests in menu-generation application.n"}
{"File Name":"cnp-design-documentation\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1077,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"SiebenApp\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1922,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"libelektra\/spec_metadata.md","Context":"## Problem\nTo make storage-plugins suitable for `spec` they need to be able to store\nall the metadata as specified in [METADATA.ini](\/doc\/METADATA.ini).\nMost file formats do not have support for that.\nIf metadata is merged from different namespaces, e.g., `spec:` and `user:`,\nmetadata from one namespace might end up in keys of other namespaces, e.g.,\nmetadata from `spec:` might end up in `user:`.\n","Decision":"Do not store metadata unrelated to the configuration file structure in any namespace except in `spec:\/`.\\n- Trying to store any other metadata in any other namespace leads to an error.\\nE.g. `kdb set-meta user:\/data metadata_not_suitable_for_storage_plugins something` would fail\\n(validated by `spec` plugin).\\n- Metadata that is designed to be stored by storage plugins to preserve configuration file structure.\\nE.g. `comment` or `order`, might be stored in any namespace.\\nSometimes, the same metadata can be used in several namespaces but with different meanings\\nand ways of serialization, e.g. `type` and `array`:\\n- In `spec:\/` the metadata `array=` (empty value) means \"this is an array\".\\nIf you give it a value e.g. `array=#4` it means \"this is an array with default size X\" (e.g. `#4` = size 5).\\n- In any other namespace `array=` means \"this is an empty array\" and e.g. `array=#4` means \"this is an array with max index #4\".\\n`array=#4` is not stored literally but inferred.\\n- Either the storage plugin does not support arrays, then the metadata will be discarded\\non `kdbSet` but `spec` will keep on adding it for every `kdbGet`.\\n- Or, if the storage plugin supports arrays, the data will be serialized as array\\n(even if the metadata comes from `spec`) and as such available in the next `kdbGet`\\nfrom the storage plugin to be validated by `spec`.\\nUse different storage plugins, or plugins with different configurations,\\nfor the `spec:\/` namespace:\\n- `ni`\\n- TOML with `meta` configuration\\nThe `kdb mount` tool will add the `meta` plugin configuration when mounting a storage plugin to `spec:\/`.\\n- [Spec Expressiveness](spec_expressiveness.md)\\n- [Arrays](array.md)\\n","tokens":103,"id":1278,"text":"## Problem\\nTo make storage-plugins suitable for `spec` they need to be able to store\\nall the metadata as specified in [METADATA.ini](\/doc\/METADATA.ini).\\nMost file formats do not have support for that.\\nIf metadata is merged from different namespaces, e.g., `spec:` and `user:`,\\nmetadata from one namespace might end up in keys of other namespaces, e.g.,\\nmetadata from `spec:` might end up in `user:`.\\n\n\n##Decision\nDo not store metadata unrelated to the configuration file structure in any namespace except in `spec:\/`.\\n- Trying to store any other metadata in any other namespace leads to an error.\\nE.g. `kdb set-meta user:\/data metadata_not_suitable_for_storage_plugins something` would fail\\n(validated by `spec` plugin).\\n- Metadata that is designed to be stored by storage plugins to preserve configuration file structure.\\nE.g. `comment` or `order`, might be stored in any namespace.\\nSometimes, the same metadata can be used in several namespaces but with different meanings\\nand ways of serialization, e.g. `type` and `array`:\\n- In `spec:\/` the metadata `array=` (empty value) means \"this is an array\".\\nIf you give it a value e.g. `array=#4` it means \"this is an array with default size X\" (e.g. `#4` = size 5).\\n- In any other namespace `array=` means \"this is an empty array\" and e.g. `array=#4` means \"this is an array with max index #4\".\\n`array=#4` is not stored literally but inferred.\\n- Either the storage plugin does not support arrays, then the metadata will be discarded\\non `kdbSet` but `spec` will keep on adding it for every `kdbGet`.\\n- Or, if the storage plugin supports arrays, the data will be serialized as array\\n(even if the metadata comes from `spec`) and as such available in the next `kdbGet`\\nfrom the storage plugin to be validated by `spec`.\\nUse different storage plugins, or plugins with different configurations,\\nfor the `spec:\/` namespace:\\n- `ni`\\n- TOML with `meta` configuration\\nThe `kdb mount` tool will add the `meta` plugin configuration when mounting a storage plugin to `spec:\/`.\\n- [Spec Expressiveness](spec_expressiveness.md)\\n- [Arrays](array.md)\\n","Predictions":"- Do not store metadata unrelated to the configuration file structure in any namespace except in spec:\/.n- Trying to store any other metadata in any other namespace leads to an error.nE.g. kdb set-meta user:\/data metadata_not_suitable_for_storage_plugins something would failn(validated by spec plugin).n- Metadata that is designed to be stored by storage plugins to preserve configuration file structure.nE.g. comment or order, might be stored in any namespace.nSometimes, the same metadata can be used in several namespaces but with different meaningsnand ways of serialization, e.g. type and array:n- In spec:\/ the metadata array= (empty value) means \"this is an array\".nIf you give it a value e.g. array=#4 it means \"this is an array with default size X\" (e.g. #4 = size 5).n- In any other namespace array= means \"this is an empty array\" and e.g. array=#4 means \"this is an array with max index #4\".narray=#4 is not stored literally but inferred.n- Either the storage plugin does not support arrays, then the metadata will be discardednon kdbSet but kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet but kdbSet will be discardednon kdbSet"}
{"File Name":"operational-data-hub\/0013-dataflow-diagrams.md","Context":"## Context\nWe feel the need to define that every config folder should contain at least one dataflow diagram.\n","Decision":"We decided that every config folder should contain at least one dataflow diagram.\\n### Dataflow Diagram\\nEvery config with a cloudbuild should contain a dataflow diagram. This diagram contains the dataflow of the GCP project the config is for. It is also possible to have multiple diagrams if there are multiple dataflows. [Here](https:\/\/github.com\/vwt-digital-config\/snb-link2-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) and [here](https:\/\/github.com\/vwt-digital-config\/snb-tmob-problm-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) examples of dataflow diagrams can be found.\\nThe diagrams are made via the python package [diagrams](https:\/\/github.com\/mingrammer\/diagrams).\\n","tokens":23,"id":2706,"text":"## Context\\nWe feel the need to define that every config folder should contain at least one dataflow diagram.\\n\n\n##Decision\nWe decided that every config folder should contain at least one dataflow diagram.\\n### Dataflow Diagram\\nEvery config with a cloudbuild should contain a dataflow diagram. This diagram contains the dataflow of the GCP project the config is for. It is also possible to have multiple diagrams if there are multiple dataflows. [Here](https:\/\/github.com\/vwt-digital-config\/snb-link2-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) and [here](https:\/\/github.com\/vwt-digital-config\/snb-tmob-problm-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) examples of dataflow diagrams can be found.\\nThe diagrams are made via the python package [diagrams](https:\/\/github.com\/mingrammer\/diagrams).\\n","Predictions":"We decided that every config folder should contain at least one dataflow diagram.n### Dataflow DiagramnEvery config with a cloudbuild should contain a dataflow diagram. This diagram contains the dataflow of the GCP project the config is for. It is also possible to have multiple diagrams if there are multiple dataflows. [Here](https:\/\/github.com\/vwt-digital-config\/snb-link2-int-config\/tree\/develop\/dataflow_information\/tmob_emails_link2) examples of dataflow diagrams can be found.nThe diagrams are made via the python package [diagrams](https:\/\/github.com\/mingrammer\/diagrams).n"}
{"File Name":"dogma\/0004-adr-process.md","Context":"## Context\nWe need a documented process for proposing, discussing and ultimate accepting or\nrejecting ADRs via pull requests.\n","Decision":"We have decided to adopt a process that favours rapid changes, at least while\\nthe project is in its infancy. To this end, we will allow ADRs in the `proposed`\\nstatus to be merged to `main` via PRs. The PRs may contain related code changes.\\nWhenever a `proposed` ADR is merged to `main`, a GitHub issue is created to\\ncapture discussion about the ADR. Such issues are given the `adr` label.\\nAny `proposed` ADRs remaining on `main` must be resolved either by approving\\nthe ADR, or by rejecting it and reverting any associated code changes.\\n","tokens":27,"id":1614,"text":"## Context\\nWe need a documented process for proposing, discussing and ultimate accepting or\\nrejecting ADRs via pull requests.\\n\n\n##Decision\nWe have decided to adopt a process that favours rapid changes, at least while\\nthe project is in its infancy. To this end, we will allow ADRs in the `proposed`\\nstatus to be merged to `main` via PRs. The PRs may contain related code changes.\\nWhenever a `proposed` ADR is merged to `main`, a GitHub issue is created to\\ncapture discussion about the ADR. Such issues are given the `adr` label.\\nAny `proposed` ADRs remaining on `main` must be resolved either by approving\\nthe ADR, or by rejecting it and reverting any associated code changes.\\n","Predictions":"We have decided to adopt a process that favours rapid changes, at least while the project is in its infancy. To this end, we will allow ADRs in the proposedstatus to be merged to main via PRs. The PRs may contain related code changes.nWhenever a proposed ADR is merged to main, a GitHub issue is created toncapture discussion about the ADR. Such issues are given the adr label.nAny proposed ADRs remaining on main must be resolved either by approvingnthe ADR, or by rejecting it and reverting any associated code changes.n"}
{"File Name":"profiles-db-elastic\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":5139,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"smarthub\/0003-create-backend-client-lib.md","Context":"## Context\nOff-chain data is accessible via REST API. Currently all system components uses direct REST calls in various places making unit test hard.\n","Decision":"Create client library and use it as dependency in components that want to read the off-chain data. Include the mocked version of the service so unit-tests does not have to rely on the implementation.\\n","tokens":29,"id":3692,"text":"## Context\\nOff-chain data is accessible via REST API. Currently all system components uses direct REST calls in various places making unit test hard.\\n\n\n##Decision\nCreate client library and use it as dependency in components that want to read the off-chain data. Include the mocked version of the service so unit-tests does not have to rely on the implementation.\\n","Predictions":"Chosen option: \"DoS\", because it was already used for the additional tooling and there's lots of documentations and examples.n### Positive Consequences!-- optional -->n* Direct compatibility with additional tool libraries, e.g. for XPM handlingn* Lots of resources and with Openbox a very complete WM (in sense of ICCCM and EWMH) as possible templaten### Negative Consequences!-- optional -->n* Refactoring coden* Wayland might be more modernn* XCB is faster at times due to asynchronousn"}
{"File Name":"CICD-pipeline\/001-open-source-shared-jenkins-library.md","Context":"## Context\n1) We are [open by default](https:\/\/github.com\/baloise\/baloise.github.io\/blob\/master\/design\/OpenByDefault.adoc)\n1) The myCloud API spec [is not confidential](https:\/\/github.com\/baloise\/CICD-pipeline\/issues\/15)\n1) Openness is a driver for clean, secure design and good documentation\n1) There are currently no obvious drivers for confidentiality\n","Decision":"We release the Shared Jenkins Library under Apache 2.0.\\n","tokens":90,"id":2768,"text":"## Context\\n1) We are [open by default](https:\/\/github.com\/baloise\/baloise.github.io\/blob\/master\/design\/OpenByDefault.adoc)\\n1) The myCloud API spec [is not confidential](https:\/\/github.com\/baloise\/CICD-pipeline\/issues\/15)\\n1) Openness is a driver for clean, secure design and good documentation\\n1) There are currently no obvious drivers for confidentiality\\n\n\n##Decision\nWe release the Shared Jenkins Library under Apache 2.0.\\n","Predictions":"We will use the [Security & Privacy API](https:\/\/github.com\/baloise\/baloise.github.io\/blob\/master\/design\/OpenByDefault.adoc) for our security and privacy APIs.n"}
{"File Name":"community\/dr-012-Prettier_as_JavaScript_code_formatter.md","Context":"## Context\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\n[Prettier](https:\/\/prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\n","Decision":"The decision is to use Prettier as the only code formatter for JavaScript-based projects.\\n","tokens":301,"id":3451,"text":"## Context\\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\\n[Prettier](https:\/\/prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\\n\n\n##Decision\nThe decision is to use Prettier as the only code formatter for JavaScript-based projects.\\n","Predictions":"The decision is to use Prettier as the only code formatter for JavaScript-based projects.n"}
{"File Name":"hello\/001-record-decisions.md","Context":"## Context\nWe want to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael\\nNygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4361,"text":"## Context\\nWe want to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael\\nNygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"insight\/0002-tweepy.md","Context":"## Context\nTo programmatically access twitter we need to use a Python library able to support APIs we need. The chosen library should be well reviewed and have wide community support.\n","Decision":"We choose [Tweepy](https:\/\/github.com\/tweepy\/tweepy) as our Twitter API Pyhon library.\\n","tokens":36,"id":4760,"text":"## Context\\nTo programmatically access twitter we need to use a Python library able to support APIs we need. The chosen library should be well reviewed and have wide community support.\\n\n\n##Decision\nWe choose [Tweepy](https:\/\/github.com\/tweepy\/tweepy) as our Twitter API Pyhon library.\\n","Predictions":"We will use [Attribute](https:\/\/github.com\/Attribute\/Attribute) as the UI framework.n"}
{"File Name":"delayer-aws\/0003-dymamodb-update-strategy.md","Context":"## Context and Problem Statement\nIn order to maintain the serverless nature of the project, a DynamoDB table was\nchosen as main persistence mechanism, which means that a schedule is primarily\nstored in DynamoDB and then distributed to other components, which in turn\ngoes back to DynamoDB to update state. So, DynamoDB holds both state and\nhistorical data.\nThe problem here is that both ~~warmer~~ `task-1minute-enqueuer` and ~~poller~~ `task-1minute-sqs2sns` will concur by Dynamo resources and probably will be throttled (it's easy to reproduce this behavior only by setting Dynamo's read and write capacity to 1 and trying to send some hundreds of schedules while some other are ~~moving from *WARM* state~~ being enqueued in delayer queue).\n## Decision Drivers\n*   Solution must kept as simple as possible\n*   Even it could delay the problem, increase read and write capacity of\nDynamoDB is not an architectural solution\n","Decision":"*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\nTake the decision for the use of the DynamoDB introduced a new concept for the entire architecture: the layered concern.\\nThe `delayer-aws` solution aims to provide a way to schedule future operations reliably. It's not part of this system store or ingest or even present information about these schedules. In this sense, the use of DynamoDB is needed only because there's a need of store schedules that could not be posted in delayer queue, and there's only 2 options for those records: or they are in the delayer queue, or they're not. That's why the \"state\" field is needed, but it will not hold the *entire* lifecycle of a schedule.\\nWith this in mind, we realize that all 3 options will be considered, but in different contexts:\\n-   Present data of scheduler is not `delayer-aws`'s concern, but it will be needed. So all the data events should be published by `delayer-aws` to be consumed by another \"view\" platform - this is a kind of *event driven approach*.\\n-   In this sense, if another system will ingest all of this published data, state and historical data will be stored in different persistent stores;\\n### How it should work\\nWhen a schedule is inserted, if their timeframe was greater then the timeframe of the delayer queue, the schedule is stored in DynamoDB with a *currentStatus* marked as `NEW` and a TTL of 1 hour after schedule time.\\nWhen `task-1minute-enqueuer` runs and finds this scheduler, it will be updated to `PROCESSED`. After that, no more updates are made in DynamoDB.\\nFor each event that occurs with this schedule, a message should be posted in the correspondent topic.\\n### Don't use state as a field in database\\nThe state of a schedule is noted by currentStatus\/currentState attribute in\\nDynamoDB. The proposal of this optional is to treat state as part of the\\ngeneral flow of application:\\n1.  When scheduled, it's COLD\\n1.  When *warmer* puts it on SQS, it's WARM\\n1.  When *poller* publishes it on SNS is DOING\\n1.  When *actions* execute\\n*   Good, simplified update policy\\n*   Good, reduced DynamoDB costs\\n*   Bad, no information about past events\\n","tokens":211,"id":2791,"text":"## Context and Problem Statement\\nIn order to maintain the serverless nature of the project, a DynamoDB table was\\nchosen as main persistence mechanism, which means that a schedule is primarily\\nstored in DynamoDB and then distributed to other components, which in turn\\ngoes back to DynamoDB to update state. So, DynamoDB holds both state and\\nhistorical data.\\nThe problem here is that both ~~warmer~~ `task-1minute-enqueuer` and ~~poller~~ `task-1minute-sqs2sns` will concur by Dynamo resources and probably will be throttled (it's easy to reproduce this behavior only by setting Dynamo's read and write capacity to 1 and trying to send some hundreds of schedules while some other are ~~moving from *WARM* state~~ being enqueued in delayer queue).\\n## Decision Drivers\\n*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\n\n\n##Decision\n*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\nTake the decision for the use of the DynamoDB introduced a new concept for the entire architecture: the layered concern.\\nThe `delayer-aws` solution aims to provide a way to schedule future operations reliably. It's not part of this system store or ingest or even present information about these schedules. In this sense, the use of DynamoDB is needed only because there's a need of store schedules that could not be posted in delayer queue, and there's only 2 options for those records: or they are in the delayer queue, or they're not. That's why the \"state\" field is needed, but it will not hold the *entire* lifecycle of a schedule.\\nWith this in mind, we realize that all 3 options will be considered, but in different contexts:\\n-   Present data of scheduler is not `delayer-aws`'s concern, but it will be needed. So all the data events should be published by `delayer-aws` to be consumed by another \"view\" platform - this is a kind of *event driven approach*.\\n-   In this sense, if another system will ingest all of this published data, state and historical data will be stored in different persistent stores;\\n### How it should work\\nWhen a schedule is inserted, if their timeframe was greater then the timeframe of the delayer queue, the schedule is stored in DynamoDB with a *currentStatus* marked as `NEW` and a TTL of 1 hour after schedule time.\\nWhen `task-1minute-enqueuer` runs and finds this scheduler, it will be updated to `PROCESSED`. After that, no more updates are made in DynamoDB.\\nFor each event that occurs with this schedule, a message should be posted in the correspondent topic.\\n### Don't use state as a field in database\\nThe state of a schedule is noted by currentStatus\/currentState attribute in\\nDynamoDB. The proposal of this optional is to treat state as part of the\\ngeneral flow of application:\\n1.  When scheduled, it's COLD\\n1.  When *warmer* puts it on SQS, it's WARM\\n1.  When *poller* publishes it on SNS is DOING\\n1.  When *actions* execute\\n*   Good, simplified update policy\\n*   Good, reduced DynamoDB costs\\n*   Bad, no information about past events\\n","Predictions":"* Solution must kept as simple as possiblen* Even it could delay the problem, increase read and write capacity ofnDynamoDB is not an architectural solutionn"}
{"File Name":"zendesk_app_scaffold_react_redux\/replace-migration-scaffold-with-clean-scaffold-in-master-branch.md","Context":"## Context\nAt the moment, the master branch has the scaffold specifically built to help developers migrate v1 apps to v2. From monitor, the majority usage comes from [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator) and [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) which use the master branch as the download source.\nIt is more helpful to show developers coming directly to this repo a clean (from scratch) scaffold, one usable as a starting point to build new apps respecting our current conventions out of the box.\n","Decision":"### New app scaffold\\nA new *clean* app scaffold has been built on branch [offapps-migration](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/offapps-migration) based on branch [from-scratch](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/from-scratch)\\n### Move migration scaffold into App Migrator repository\\nMigration scaffold in master branch will be moved into [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator), used and maintained as a *built-in* template resource\\n### Move new app scaffold to master branch\\n### Add support in [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) to create new app with the new scaffold\\n* New command option `zat new --scaffold` will create a v2 app using the new app scaffold.\\n","tokens":124,"id":2378,"text":"## Context\\nAt the moment, the master branch has the scaffold specifically built to help developers migrate v1 apps to v2. From monitor, the majority usage comes from [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator) and [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) which use the master branch as the download source.\\nIt is more helpful to show developers coming directly to this repo a clean (from scratch) scaffold, one usable as a starting point to build new apps respecting our current conventions out of the box.\\n\n\n##Decision\n### New app scaffold\\nA new *clean* app scaffold has been built on branch [offapps-migration](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/offapps-migration) based on branch [from-scratch](https:\/\/github.com\/zendesk\/app_scaffold\/tree\/from-scratch)\\n### Move migration scaffold into App Migrator repository\\nMigration scaffold in master branch will be moved into [App Migrator](https:\/\/github.com\/zendesk\/zendesk_app_migrator), used and maintained as a *built-in* template resource\\n### Move new app scaffold to master branch\\n### Add support in [ZAT](https:\/\/github.com\/zendesk\/zendesk_apps_tools) to create new app with the new scaffold\\n* New command option `zat new --scaffold` will create a v2 app using the new app scaffold.\\n","Predictions":"We will use the [Security Layer](https:\/\/github.com\/zendesk\/zendesk_security_layer) as the default layer for the v2 app scaffold.n"}
{"File Name":"cf-k8s-networking\/0006-rewrite-http-liveness-readiness-probes-for-healthchecks.md","Context":"## Context\nWith Istio auto mTLS enabled in `STRICT` mode, [http liveness and readiness\nprobes](https:\/\/kubernetes.io\/docs\/tasks\/configure-pod-container\/configure-liveness-readiness-startup-probes\/) no longer work because the `kubelet`, which makes the http requests, does not have Istio issued certificates.\nIstio [supports rewriting `http` probes](https:\/\/istio.io\/docs\/ops\/configuration\/mesh\/app-health-check\/#enable-globally-via-install-option) during the sidecar injection process.\n#### Figure 1\n_Liveness probe flow when Istio mTLS is disabled or `PERMISSIVE`. Probe `GET` request regularly travels through the Envoy sidecar to the app._\n![No mTLS\/PERMISSIVE mTLS mode liveness probe diagram](..\/assets\/liveness-probe-adr-1.png)\n#### Figure 2\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe is not rewritten. Probe `GET` request fails at the Envoy sidecar because it does not include the correct certificates._\n![STRICT mTLS liveness probe diagram with no probe rewrite](..\/assets\/liveness-probe-adr-2.png)\n#### Figure 3\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe **is rewritten by Istio**. Probe `GET` request bypasses the sidecar and goes through the Istio `pilot-agent` instead. The `pilot-agent` is configured to direct the request to the app._\n![STRICT mTLS liveness probe diagram with probe rewrite](..\/assets\/liveness-probe-adr-3.png)\n","Decision":"We have decided to install Istio with\\n`--set values.sidecarInjectorWebhook.rewriteAppHTTPProbe=true`\\nThis will rewrite the liveness and readiness probes on any app pods when injecting a sidecar into the app pod. Any namespaces that have the label `istio-injection=enabled` will have their liveness and readiness probes rewritten, as shown in Figure 3.\\n","tokens":360,"id":2243,"text":"## Context\\nWith Istio auto mTLS enabled in `STRICT` mode, [http liveness and readiness\\nprobes](https:\/\/kubernetes.io\/docs\/tasks\/configure-pod-container\/configure-liveness-readiness-startup-probes\/) no longer work because the `kubelet`, which makes the http requests, does not have Istio issued certificates.\\nIstio [supports rewriting `http` probes](https:\/\/istio.io\/docs\/ops\/configuration\/mesh\/app-health-check\/#enable-globally-via-install-option) during the sidecar injection process.\\n#### Figure 1\\n_Liveness probe flow when Istio mTLS is disabled or `PERMISSIVE`. Probe `GET` request regularly travels through the Envoy sidecar to the app._\\n![No mTLS\/PERMISSIVE mTLS mode liveness probe diagram](..\/assets\/liveness-probe-adr-1.png)\\n#### Figure 2\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe is not rewritten. Probe `GET` request fails at the Envoy sidecar because it does not include the correct certificates._\\n![STRICT mTLS liveness probe diagram with no probe rewrite](..\/assets\/liveness-probe-adr-2.png)\\n#### Figure 3\\n_Liveness probe flow when Istio mTLS is `STRICT` and the probe **is rewritten by Istio**. Probe `GET` request bypasses the sidecar and goes through the Istio `pilot-agent` instead. The `pilot-agent` is configured to direct the request to the app._\\n![STRICT mTLS liveness probe diagram with probe rewrite](..\/assets\/liveness-probe-adr-3.png)\\n\n\n##Decision\nWe have decided to install Istio with\\n`--set values.sidecarInjectorWebhook.rewriteAppHTTPProbe=true`\\nThis will rewrite the liveness and readiness probes on any app pods when injecting a sidecar into the app pod. Any namespaces that have the label `istio-injection=enabled` will have their liveness and readiness probes rewritten, as shown in Figure 3.\\n","Predictions":"### StatusnThe Status Status is a Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Status Stat"}
{"File Name":"kotlin-guice\/0002-use-packaging-for-major-version-interoperability.md","Context":"## Context\nMajor version updates to libraries often include breaking API changes. Upgrading an application to a new major version of a library can be difficult since other libraries may also have dependencies on the updated library.\nFor this library, `kotlin-guice`, the API tracks heavily to the Guice API. When Guice\nmakes a major version upgrade, this library will also likely need to make a corresponding major version upgrade.\n","Decision":"We will use the Java package and Maven group ID to allow interoperability of major versions of this library. As described by Jake Wharton in the blog post below, we will accomplish this in three ways.\\n1. The Java package name will include the version number.\\n1. The library name will be part of the group ID.\\n1. The group ID will include the version number.\\nSince this library is meant mostly as an extension to the corresponding Guice libraries, we will use the Guice major version. If we need to make a major version change to `kotlin-guice` within a single version of Guice, then we may have to include both version numbers. Until such a situation arises, we do not have to make that decision, but one possible option is `com.authzee.kotlinguice4_2`.\\nhttp:\/\/jakewharton.com\/java-interoperability-policy-for-major-version-updates\/\\n","tokens":88,"id":872,"text":"## Context\\nMajor version updates to libraries often include breaking API changes. Upgrading an application to a new major version of a library can be difficult since other libraries may also have dependencies on the updated library.\\nFor this library, `kotlin-guice`, the API tracks heavily to the Guice API. When Guice\\nmakes a major version upgrade, this library will also likely need to make a corresponding major version upgrade.\\n\n\n##Decision\nWe will use the Java package and Maven group ID to allow interoperability of major versions of this library. As described by Jake Wharton in the blog post below, we will accomplish this in three ways.\\n1. The Java package name will include the version number.\\n1. The library name will be part of the group ID.\\n1. The group ID will include the version number.\\nSince this library is meant mostly as an extension to the corresponding Guice libraries, we will use the Guice major version. If we need to make a major version change to `kotlin-guice` within a single version of Guice, then we may have to include both version numbers. Until such a situation arises, we do not have to make that decision, but one possible option is `com.authzee.kotlinguice4_2`.\\nhttp:\/\/jakewharton.com\/java-interoperability-policy-for-major-version-updates\/\\n","Predictions":"We will use the Java package and Maven group ID to allow interoperability of major versions of this library. As described by Jake Wharton in the blog post below, we will accomplish this in three ways.n1. The Java package name will include the version number.n1. The library name will be part of the group ID.n1. The group ID will include the version number.nSince this library is meant mostly as an extension to the corresponding Guice libraries, we will use the Guice major version. If we need to make a major version change to kotlin-guice within a single version of Guice, then we may have to include both version numbers.nUntil such a situation arises, we do not have to make that decision, but one possible option is com.authzee.kotlinguice4_2.nhttp:\/\/jakewharton.com\/java-interoperability-policy-for-major-version-updates\/n"}
{"File Name":"react-template\/0003-use-prettier-to-format-code.md","Context":"## Context\nWe want to ensure we're all using one code style, that is familiar across\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\nof writing, it is used by over\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\non GitHub, including React itself, and has become a standard.\n","Decision":"We will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","tokens":125,"id":3321,"text":"## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, including React itself, and has become a standard.\\n\n\n##Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n","Predictions":"We will enforce that everything supported by Prettier has its style enforced bynit.nWe will set up Git hooks to automatically run the formatter before committing.nWe will set continuous integration up to reject commits that are not correctlynformatted.n"}
{"File Name":"linshare-mobile-flutter-app\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3308,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"alfa\/adr-003.md","Context":"## Context\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\n","Decision":"We will use [TypeScript](https:\/\/github.com\/Microsoft\/TypeScript) for implementing all of Alfa. Being a superset of JavaScript, TypeScript has a low learning curve for people already familiar with JavaScript while providing a solid type system. We will enforce API contracts through generation of [declaration files](https:\/\/www.typescriptlang.org\/docs\/handbook\/declaration-files\/introduction.html) which will dictate the API surface that consumers can access. To the extent possible, we will keep a strict TypeScript configuration in order to catch as many issues as possible at compile time. In particular, this entails strict `undefined` and `null` checking in order to get rid of a previously common type of bug.\\n","tokens":195,"id":3153,"text":"## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n\n\n##Decision\nWe will use [TypeScript](https:\/\/github.com\/Microsoft\/TypeScript) for implementing all of Alfa. Being a superset of JavaScript, TypeScript has a low learning curve for people already familiar with JavaScript while providing a solid type system. We will enforce API contracts through generation of [declaration files](https:\/\/www.typescriptlang.org\/docs\/handbook\/declaration-files\/introduction.html) which will dictate the API surface that consumers can access. To the extent possible, we will keep a strict TypeScript configuration in order to catch as many issues as possible at compile time. In particular, this entails strict `undefined` and `null` checking in order to get rid of a previously common type of bug.\\n","Predictions":"We will use [JavaScript](https:\/\/github.com\/Microsoft\/TypeScript) for implementing all of Alfa. Being a superset of JavaScript, TypeScript has a low learning curve for people already familiar with JavaScript while providing a solid type system. We will enforce API contracts through generation of [declaration files](https:\/\/www.typescriptlang.org\/docs\/handbook\/declaration-files\/introduction.html) which will dictate the API surface that consumers can access. To the extent possible, we will keep a strict TypeScript configuration in order to catch as many issues as possible at compile time. In particular, this entails strict undefined and null checking in order to get rid of a previously common type of bug.n"}
{"File Name":"frontend\/001-spa.md","Context":"## Context\nManaging playing audio between page loads is impossible. This means that either, we use a pop-up player that plays audio in another window or we build a Single Page Application that doesn't perform page loads.\nBuilding a single page application will add complexity and other issues to work around like SEO. Pop-up players are clunky, feel dis-jointed from the rest of the site and can be annoying to use in mobile browsers.\n","Decision":"Build a SPA.\\n","tokens":89,"id":1096,"text":"## Context\\nManaging playing audio between page loads is impossible. This means that either, we use a pop-up player that plays audio in another window or we build a Single Page Application that doesn't perform page loads.\\nBuilding a single page application will add complexity and other issues to work around like SEO. Pop-up players are clunky, feel dis-jointed from the rest of the site and can be annoying to use in mobile browsers.\\n\n\n##Decision\nBuild a SPA.\\n","Predictions":"* We will use a [semantic semantic](https:\/\/github.com\/semantic-semantic\/semantic-semantic) format for writing commit messages.**nConventional commits format is nice and short and defines the simple structure that is easy to learn and follow. Here is basic structure of our customized conventional commits format:ntype>(optional scope>): description> optional metadata>nOur customization:n- defines additional message types as an extension of [types defined by the Angular team](https:\/\/github.com\/angular\/angular\/blob\/22b96)nhttps:\/\/github.com\/angular\/angular\/blob\/22b96\/nn### Architectural Contextn* We will use a [semantic semantic](https:\/\/github.com\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-semantic\/semantic-se"}
{"File Name":"operational-data-hub\/0039-least-privilege-access.md","Context":"## Context\nThe principle of least privilege means that every module (such as a process, a user, or a program, depending on the subject) must be able to access only the information and resources that are necessary for its legitimate purpose.\nThe principle of least privilege works by allowing only enough access to perform the required job. In an IT environment, adhering to the principle of least privilege reduces the risk of attackers gaining access to critical systems or sensitive data by compromising a low-level user account, device, or application. Implementing the principle helps contain compromises to their area of origin, stopping them from spreading to the system at large.\n","Decision":"We will apply the principle of least privilege.\\n","tokens":128,"id":2737,"text":"## Context\\nThe principle of least privilege means that every module (such as a process, a user, or a program, depending on the subject) must be able to access only the information and resources that are necessary for its legitimate purpose.\\nThe principle of least privilege works by allowing only enough access to perform the required job. In an IT environment, adhering to the principle of least privilege reduces the risk of attackers gaining access to critical systems or sensitive data by compromising a low-level user account, device, or application. Implementing the principle helps contain compromises to their area of origin, stopping them from spreading to the system at large.\\n\n\n##Decision\nWe will apply the principle of least privilege.\\n","Predictions":"We will apply the principle of least privilege.n"}
{"File Name":"aspan_micro_front_ends\/0002-derived-attributes.md","Context":"## Context\nDerived attributes are read-only.\n","Decision":"Following attributes are derived from file system:\\n1. name - file name without extension\\n2. contentType - file extension without dot, lowercased\\n","tokens":10,"id":3192,"text":"## Context\\nDerived attributes are read-only.\\n\n\n##Decision\nFollowing attributes are derived from file system:\\n1. name - file name without extension\\n2. contentType - file extension without dot, lowercased\\n","Predictions":"Following attributes are derived from file system:n1. name - file name without extensionn2. contentType - file extension without dot, lowercasednChosen option: \"custom solution\", because resolves all forces.n### Positive Consequencesn* Works with floats and umlautsn### Negative Consequencesn* A little bit more than minimal code is presented to the usern"}
{"File Name":"status-react\/0002-extension-uri.md","Context":"## Context\nExtensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.\n","Decision":"URI follows ethereum URI schema as specified in [EIP 831](https:\/\/github.com\/ethereum\/EIPs\/blob\/master\/EIPS\/eip-831.md)\\n```\\nuri                     = \"ethereum:\" path\\npath                    = \"status:extension\" \":\" storage \"@\" id\\nstorage                 = STRING (e.g. IPFS)\\nid                      = STRING (e.g. IPFS hash)\\n```\\nAn example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`\\n","tokens":26,"id":2963,"text":"## Context\\nExtensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.\\n\n\n##Decision\nURI follows ethereum URI schema as specified in [EIP 831](https:\/\/github.com\/ethereum\/EIPs\/blob\/master\/EIPS\/eip-831.md)\\n```\\nuri                     = \"ethereum:\" path\\npath                    = \"status:extension\" \":\" storage \"@\" id\\nstorage                 = STRING (e.g. IPFS)\\nid                      = STRING (e.g. IPFS hash)\\n```\\nAn example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`\\n","Predictions":"We will use [the 'tag' URI Scheme](https:\/\/tools.ietf.org\/html\/rfc4151).n"}
{"File Name":"modernisation-platform\/0015-use-aws-image-builder-for-managing-amis.md","Context":"## Context\nThere is likely to  be a common requirement across Modernisation Platform consumers to utilise the benefits of using customised AMIs, such as managed consistency of configuration and speed of EC2 instance deployment.\nSome options to tackle AMI generation and management were considered\n* A custom approach - based on a reference architecture (see <https:\/\/aws.amazon.com\/blogs\/awsmarketplace\/announcing-the-golden-ami-pipeline\/>) that brings together a number of tools and techniques to manage AMIs through their lifecycle. This approach brings with it a fair amount of complexity and arguably extra operational overhead.\n* [Packer](https:\/\/www.packer.io\/) - a common approach across private and public cloud platforms, using this Hashicorp tool to programmatically build AMIs. Such a solution would also need to manage the distribution of AMIs across accounts and lifecycle management of AMIs\n* [AWS Image Builder](https:\/\/docs.aws.amazon.com\/imagebuilder\/latest\/userguide\/what-is-image-builder.html) - a managed AWS service incorporating concepts such as pipelines, recipes and components, and even a marketplace of build and test components. Image builder is based on the use of AWS Systems Manager (so no ssh connections and exposed ports). A solution based on this (at the time of writing) would need to also handle the lifecycle management of AMIs (as it the case with Packer)\n","Decision":"We will create an AMI generation and management capability based on AWS Image Builder.\\n","tokens":287,"id":3145,"text":"## Context\\nThere is likely to  be a common requirement across Modernisation Platform consumers to utilise the benefits of using customised AMIs, such as managed consistency of configuration and speed of EC2 instance deployment.\\nSome options to tackle AMI generation and management were considered\\n* A custom approach - based on a reference architecture (see <https:\/\/aws.amazon.com\/blogs\/awsmarketplace\/announcing-the-golden-ami-pipeline\/>) that brings together a number of tools and techniques to manage AMIs through their lifecycle. This approach brings with it a fair amount of complexity and arguably extra operational overhead.\\n* [Packer](https:\/\/www.packer.io\/) - a common approach across private and public cloud platforms, using this Hashicorp tool to programmatically build AMIs. Such a solution would also need to manage the distribution of AMIs across accounts and lifecycle management of AMIs\\n* [AWS Image Builder](https:\/\/docs.aws.amazon.com\/imagebuilder\/latest\/userguide\/what-is-image-builder.html) - a managed AWS service incorporating concepts such as pipelines, recipes and components, and even a marketplace of build and test components. Image builder is based on the use of AWS Systems Manager (so no ssh connections and exposed ports). A solution based on this (at the time of writing) would need to also handle the lifecycle management of AMIs (as it the case with Packer)\\n\n\n##Decision\nWe will create an AMI generation and management capability based on AWS Image Builder.\\n","Predictions":"We will use a Security approach to manage the security of the EC2 instances.n"}
{"File Name":"nada-kafkarator\/0004-combine-topic-creation-and-credentials-management-in-same-app.md","Context":"## Context\nThe project requires dealing with two relatively separate concerns:\n1. Create topics when needed\n2. Supply credentials for working with topics.\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\nso it makes sense to keep them in the same application.\n","Decision":"We will ignore the SRP in this instance, and keep the two concerns in the same application.\\n","tokens":83,"id":866,"text":"## Context\\nThe project requires dealing with two relatively separate concerns:\\n1. Create topics when needed\\n2. Supply credentials for working with topics.\\nIf we were to strictly follow the Single Responsibility Principle, these should be in separate apps.\\nHowever, the two concerns are conceptually quite connected, even if they are separate in implementation,\\nso it makes sense to keep them in the same application.\\n\n\n##Decision\nWe will ignore the SRP in this instance, and keep the two concerns in the same application.\\n","Predictions":"We will define one new CRD object to configure topics and access to thisn- App developers will create this in the cluster when deploying their applicationn- Kafkarator will watch this CRD and take needed actionsn- App developers will add configuration to their Application resource listing kafka pools they need access ton"}
{"File Name":"operational-data-hub\/0009-uri-identifies-data.md","Context":"## Context\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme (e.g. http:\/\/).\nTo be able to unambiguously identify any element of data, a URI will be defined for every element of data on the ODH platform. Building this URI from meaningful attributes instead of technical randomly generated ids makes it recognizable and easier to use. However, selecting the right attributes and context information to assure uniqueness is challenging.\nThe URI scheme to use is [tag](https:\/\/tools.ietf.org\/html\/rfc4151). Tag URIs are quite human readable, but, unlike http URIs, not resolvable. This makes them suitable for identifying real-life things (entities) that have a representation on the ODH. A URI can be defined from the hierarchy of the organisation, forming a path to the resource. For example, if mycompany has a catalog with articles and one of the articles has article# 313, an article URI could be defined as `tag:vwt.digital,2020:mycompany\/catalog\/articles\/313`\n","Decision":"We define a URI according to the [tag scheme](https:\/\/tools.ietf.org\/html\/rfc4151) for each entity that has a representation on the ODH platform.\\n","tokens":252,"id":2740,"text":"## Context\\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme (e.g. http:\/\/).\\nTo be able to unambiguously identify any element of data, a URI will be defined for every element of data on the ODH platform. Building this URI from meaningful attributes instead of technical randomly generated ids makes it recognizable and easier to use. However, selecting the right attributes and context information to assure uniqueness is challenging.\\nThe URI scheme to use is [tag](https:\/\/tools.ietf.org\/html\/rfc4151). Tag URIs are quite human readable, but, unlike http URIs, not resolvable. This makes them suitable for identifying real-life things (entities) that have a representation on the ODH. A URI can be defined from the hierarchy of the organisation, forming a path to the resource. For example, if mycompany has a catalog with articles and one of the articles has article# 313, an article URI could be defined as `tag:vwt.digital,2020:mycompany\/catalog\/articles\/313`\\n\n\n##Decision\nWe define a URI according to the [tag scheme](https:\/\/tools.ietf.org\/html\/rfc4151) for each entity that has a representation on the ODH platform.\\n","Predictions":"We define a URI according to the [tag scheme](https:\/\/tools.ietf.org\/html\/rfc4151) for each entity that has a representation on the ODH platform.n"}
{"File Name":"docs\/0012-winery-topology-modeler.md","Context":"## Context and Problem Statement\nWe want to use a component to graphically model nodes and edges in a web application.\n## Decision Drivers <!-- optional -->\n* usable with Angular\n* number of dependecies\n* features\n+ create Nodes\n+ create Connections\n* usability\n","Decision":"* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\nChosen option: Choosen a Combination of ngx-graph and plain d3.js, because ngx-graph might miss some usefull features but has good usability which is worth to have a look at\\n### Positive Consequences <!-- optional -->\\n* easy to use graph modeler\\n* only a few dependencies\\n* full control over features and behavior\\n### Negative consequences <!-- optional -->\\n* needs to be implemented\\n","tokens":59,"id":4689,"text":"## Context and Problem Statement\\nWe want to use a component to graphically model nodes and edges in a web application.\\n## Decision Drivers <!-- optional -->\\n* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\n\n\n##Decision\n* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\nChosen option: Choosen a Combination of ngx-graph and plain d3.js, because ngx-graph might miss some usefull features but has good usability which is worth to have a look at\\n### Positive Consequences <!-- optional -->\\n* easy to use graph modeler\\n* only a few dependencies\\n* full control over features and behavior\\n### Negative consequences <!-- optional -->\\n* needs to be implemented\\n","Predictions":"* adr-apin* adr-servicen* adr-service-apinChosen option: **[RabbitMQ](TODO) with [MQTT plugin](https:\/\/www.rabbitmq.com\/mqtt.html)**, because:n- It is already well-known among the teamn- It has some [existing \"official\" Helm chart](https:\/\/www.helmmq.com\/mqtt.html)**, because:n- It is already well-known among the teamn- It has some [existing \"official\" Helm chart](https:\/\/www.helmmq.com\/mqtt.html)**, because:n- It is already well-known among the teamn- It has some [existing \"official\" Helm chart](https:\/\/www.helmmq.com\/mqtt.html)**, because:n- It is already well-known among the teamn- It has some [existing \"official\" Helm chart](https:\/\/www.helmmq.com\/mqtt.html)**, because:n- It is already well-known among the teamn- It has some [existing \"official\" Helm chart](https:\/\/www.helmmq.com\/mqtt.html)**, because:n- It is already well-known among the teamn- It has some [existing \"official\" Helm chart](https:\/\/www.helmmq.com\/mqtt.html)**, because:n- It is already well-known among the teamn- It has some [existing \"official\" Helm chart](https:\/\/www.helmmq.com\/mqtt.html)**, because:n- It is already well-known among the teamn- It has some [existing \"official\" Helm chart](https:\/\/www.helmmq.com\/mqtt.html)**, because:n- It is already well-known among the teamn- It has some [existing \"official\" Helm chart](https:\/\/www.helmmq.com\/mqtt.html)**, because:n- It is already well-known among the teamn- It has some [existing \"official\" Helm chart](https:\/\/www.helmmq.com\/mqtt.html)**, because:n- It is already well-known among the teamn- It has some [existing \"official\" Helm chart](https:\/\/www.helmmq.com\/mqtt.html)**, because:n- It is already well-known among the teamn- It has some [existing \"official\" Helm chart](https:\/\/www.helmmq.com\/mqtt.html)**, because:n- It is already well-known among the teamn- It has some [existing \"official\" Helm chart](https:\/\/www.helmmq.com\/mqtt.html)**, because:n- It is already well-known among the teamn- It has some [existing \"official\" Helm chart](https:\/\/www.helmmq.com\/mqtt.html)**, because:n- It is already well-known among the teamn- It has some [existing \"official\" Helm chart](https:\/\/www.helmmq.com\/mqtt.html)**, because:n- It is already well-known among the teamn- It has some [existing \"official\" Helm chart]"}
{"File Name":"dlp-lux\/0003-Shibboleth.md","Context":"## Context\nBoth Lux and Curate need to authenticate users to enforce authorization. Emory uses Shibboleth to manage\nauthentication for University applications. Curate has gone through the lengthy process of requesting access to\nShibboleth, and implemented user management through it. Part of the request was enumerating the \"attributes\" or\nmetadata Curate needs to receive when a person logs in. Curate currently has three \"endpoints\", or applications,\nthat can use its access to Shibboleth (prod, test, and arch).\nLux also has access to Shibboleth, with the same three configured endpoints, and the same attributes.\nLux does need to allow new users to sign in, where Curate does not.\nIn production, neither application needs to allow users to authenticate from another source.\n","Decision":"Lux will not use endpoints from Curate's Shibboleth access, instead using its own config. From the Lux side,\\na configuration option will be available to use database auth instead, for development, and to ease rollout.\\nIf the configuration option is not present, database auth will be used.\\n","tokens":167,"id":3214,"text":"## Context\\nBoth Lux and Curate need to authenticate users to enforce authorization. Emory uses Shibboleth to manage\\nauthentication for University applications. Curate has gone through the lengthy process of requesting access to\\nShibboleth, and implemented user management through it. Part of the request was enumerating the \"attributes\" or\\nmetadata Curate needs to receive when a person logs in. Curate currently has three \"endpoints\", or applications,\\nthat can use its access to Shibboleth (prod, test, and arch).\\nLux also has access to Shibboleth, with the same three configured endpoints, and the same attributes.\\nLux does need to allow new users to sign in, where Curate does not.\\nIn production, neither application needs to allow users to authenticate from another source.\\n\n\n##Decision\nLux will not use endpoints from Curate's Shibboleth access, instead using its own config. From the Lux side,\\na configuration option will be available to use database auth instead, for development, and to ease rollout.\\nIf the configuration option is not present, database auth will be used.\\n","Predictions":"We will not use endpoints from Curate's Shibboleth access, instead using its own config. From the Lux side,na configuration option will be available to use database auth instead, for development, and to ease rollout.nIf the configuration option is not present, database auth will be used.n"}
{"File Name":"ea-talk\/0008-define-appropriate-schema-types.md","Context":"## Context\nIn the course of trying to standardize how we do database development, we have had lots of discussion around schemas (much of the conversation around how oracle specifically views schemas, but the conversation *may* be relavant to other databases). This conversation has been mostly around how do do our database development, and how do we provide appropriate access to the required data.\nThrougout this discussion (mostly happening in our DB Working Group), we have agreed on a set of definitions for different types of schemas.\nHere are the notes from the original discussion:\n","Decision":"We will use the following definitions for the different types of schemas in our databases:\\n### System Schemas\\nThese are schemas in the database that are completely outside of our control, used by the database itself as necessary. Even though we don't create or manage these, we are including them here for completness and categorization.\\n### DBA User Schemas\\nDBA User Schemas will exist for the DBAs to perform necessary functions in our databases.\\nThese schemas will have the highest level of access in our systems, and thus need to be the most careful about credentials and access.\\nDBA User Schemas should follow this naming convention:\\n```\\n{}_DBA\\n```\\nWhere {} is some useful identifier (i.e. `EXAMPLE_DBA`).\\nUsually, we have only one of these per database, `EXAMPLE_DBA`. If more are necessary, they should follow this convention.\\n### Application User Schemas\\nApplication user schemas will exist for each application that needs to access data in our databases.\\nApplication users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nApplication users should not have any object creation permissions (i.e. they should not be able to perform DDL operations).\\nApplication User Schemas should follow this naming convention:\\n```\\n{}_APP\\n```\\nWhere {} is the application name (i.e. `CALENDAR_APP`).\\n### Individual User Schemas\\nIndividual user schemas represent unique individual people that need access to database systems.\\nThese types of schemas are primarly used by developers and people in the organization that perform data analytics functions.\\nIndividual users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nIndividual users should have object creation permissions only for their own user schema.\\nIndividual User Schemas should be named the same as the user's email address. We prefer this to the AD account credentials because it follows a human-readable format, and better allows us to understand who the user is.\\n### Domain Schemas\\nDomain schemas are where the actual objects in the database exist (i.e. tables, views, functions, packages).\\nDomain schemas should have object creation permissions only for their own schema.\\nWhen domain schemas are created, the appropriate roles should also be created to be able to control access to the objects.\\nNote that these might exist in multiple database instances, e.g. custorder might have extract logic in POSP and tables in DSSP.\\nDomain Schemas should be named logically so their purpose can at least be guessed at.\\n### Replicated Schemas\\nReplicated Schmeas are similar to Domain schemas, except they will *only* include data whose system of record is elsewhere. They should not include any objects that aren't represented in the \"parent\" schema.\\nLike Domain schemas, the appropriate roles should also be created to be able to control access to the objects.\\nReplicated Schemas should follow this naming convention:\\n```\\n{}_REPL\\n```\\nWhere {} is the application name (i.e. `CALENDAR_REPL`).\\n","tokens":117,"id":1106,"text":"## Context\\nIn the course of trying to standardize how we do database development, we have had lots of discussion around schemas (much of the conversation around how oracle specifically views schemas, but the conversation *may* be relavant to other databases). This conversation has been mostly around how do do our database development, and how do we provide appropriate access to the required data.\\nThrougout this discussion (mostly happening in our DB Working Group), we have agreed on a set of definitions for different types of schemas.\\nHere are the notes from the original discussion:\\n\n\n##Decision\nWe will use the following definitions for the different types of schemas in our databases:\\n### System Schemas\\nThese are schemas in the database that are completely outside of our control, used by the database itself as necessary. Even though we don't create or manage these, we are including them here for completness and categorization.\\n### DBA User Schemas\\nDBA User Schemas will exist for the DBAs to perform necessary functions in our databases.\\nThese schemas will have the highest level of access in our systems, and thus need to be the most careful about credentials and access.\\nDBA User Schemas should follow this naming convention:\\n```\\n{}_DBA\\n```\\nWhere {} is some useful identifier (i.e. `EXAMPLE_DBA`).\\nUsually, we have only one of these per database, `EXAMPLE_DBA`. If more are necessary, they should follow this convention.\\n### Application User Schemas\\nApplication user schemas will exist for each application that needs to access data in our databases.\\nApplication users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nApplication users should not have any object creation permissions (i.e. they should not be able to perform DDL operations).\\nApplication User Schemas should follow this naming convention:\\n```\\n{}_APP\\n```\\nWhere {} is the application name (i.e. `CALENDAR_APP`).\\n### Individual User Schemas\\nIndividual user schemas represent unique individual people that need access to database systems.\\nThese types of schemas are primarly used by developers and people in the organization that perform data analytics functions.\\nIndividual users should be granted the appropriate roles to access the necessary data. They should not be granted individual object grants.\\nIndividual users should have object creation permissions only for their own user schema.\\nIndividual User Schemas should be named the same as the user's email address. We prefer this to the AD account credentials because it follows a human-readable format, and better allows us to understand who the user is.\\n### Domain Schemas\\nDomain schemas are where the actual objects in the database exist (i.e. tables, views, functions, packages).\\nDomain schemas should have object creation permissions only for their own schema.\\nWhen domain schemas are created, the appropriate roles should also be created to be able to control access to the objects.\\nNote that these might exist in multiple database instances, e.g. custorder might have extract logic in POSP and tables in DSSP.\\nDomain Schemas should be named logically so their purpose can at least be guessed at.\\n### Replicated Schemas\\nReplicated Schmeas are similar to Domain schemas, except they will *only* include data whose system of record is elsewhere. They should not include any objects that aren't represented in the \"parent\" schema.\\nLike Domain schemas, the appropriate roles should also be created to be able to control access to the objects.\\nReplicated Schemas should follow this naming convention:\\n```\\n{}_REPL\\n```\\nWhere {} is the application name (i.e. `CALENDAR_REPL`).\\n","Predictions":"We will use the following definitions for the different types of schemas in our databases:n### System SchemasnThese are schemas in the database that are completely outside of our control, used by the database itself as necessary. Even though we don't create or manage these, we are including them here for completness and categorization.n### DBA User SchemasnDBA User Schemas will exist for the DBAs to perform necessary functions in our databases.nThese schemas will have the highest level of access in our systems, and thus need to be the most careful about credentials and access.nDBA User Schemas should follow this naming convention:n_DBAnWhere  is some useful identifier (i.e. EXAMPLE_DBA).nUsually, we have only one of these per database, EXAMPLE_DBA. If more are necessary, they should follow this convention.n### Application User SchemasnApplication users should be granted the appropriate roles to access the necessary data in our databases.nApplication users should not have any object creation permissions (i.e. they should not be able to perform DDL operations).nApplication User Schemas should follow this naming convention:n_APPnnWhere  is the application name (i.e. CALENDAR_APP).n### Individual User SchemasnIndividual user schemas represent individual users.n"}
{"File Name":"sexual-health-service-finder\/0006-use-in-house-postcodes-search-rather-than-postcodes-io.md","Context":"## Context\nThe application has been using https:\/\/postcodes.io\/. Postcodes.io is available\nfor free and does not require any relationship between consumer and supplier.\nOne of the drawbacks to this is that although support is available when using\nthe free tier, there is no provision for any specific support relationship.\nMore formal relationships are available but this is through the paid product\nhttps:\/\/ideal-postcodes.co.uk\/.  Given the number of requests and the fact the\nfree tier was supplying the information required, it had previously been\ndecided there was no need to migrate to a paid product. However, the strategic\ndirection has been set such that high value APIs, usable across many products\nshould be supplied in-house. To that end, there is now an in-house postcode\nsearch available.\n","Decision":"The decision is to replace the use of postcodes.io with the in-house postcode\\nlookup, as per the strategic direction.\\n","tokens":165,"id":4276,"text":"## Context\\nThe application has been using https:\/\/postcodes.io\/. Postcodes.io is available\\nfor free and does not require any relationship between consumer and supplier.\\nOne of the drawbacks to this is that although support is available when using\\nthe free tier, there is no provision for any specific support relationship.\\nMore formal relationships are available but this is through the paid product\\nhttps:\/\/ideal-postcodes.co.uk\/.  Given the number of requests and the fact the\\nfree tier was supplying the information required, it had previously been\\ndecided there was no need to migrate to a paid product. However, the strategic\\ndirection has been set such that high value APIs, usable across many products\\nshould be supplied in-house. To that end, there is now an in-house postcode\\nsearch available.\\n\n\n##Decision\nThe decision is to replace the use of postcodes.io with the in-house postcode\\nlookup, as per the strategic direction.\\n","Predictions":"We will use the [AWS Lambda](https:\/\/aws.amazon.com\/aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amazon-aws-amaz"}
{"File Name":"linshare-mobile-flutter-app\/0006-download-file-manager.md","Context":"## Context\nIn LinShare Flutter application, we can download files one by one from \"My Space\", \"Received Shares\" or \"Shared Spaces\" with the library `flutter_downloader` by adding it to the queue :\n```\nTaskId enqueue(String url, String saveDir, String header...)\n```\nWe can also clear or cancel file queue with dedicated functions.\n","Decision":"We decided that LinShare could currently download a file one by one.\\n","tokens":80,"id":3297,"text":"## Context\\nIn LinShare Flutter application, we can download files one by one from \"My Space\", \"Received Shares\" or \"Shared Spaces\" with the library `flutter_downloader` by adding it to the queue :\\n```\\nTaskId enqueue(String url, String saveDir, String header...)\\n```\\nWe can also clear or cancel file queue with dedicated functions.\\n\n\n##Decision\nWe decided that LinShare could currently download a file one by one.\\n","Predictions":"We decided that LinShare could currently download a file one by one.n"}
{"File Name":"ockam\/0005-routing-data-formats.md","Context":"## Context\nWe need a standard format for messages to be exchanged by the routing protocol.\nThis format would be used by routers on different implementations.\n","Decision":"We use the following formats:\\nFor a message:\\n```\\n{\\nonward_route: Route,\\nreturn_route: Route,\\npayload: Any\\n}\\n```\\nWhere\\n`Route` - an ordered list of addresses.\\nFor an address:\\n```\\n{\\ntype: Integer,\\ndata: Any\\n}\\n```\\n","tokens":31,"id":4538,"text":"## Context\\nWe need a standard format for messages to be exchanged by the routing protocol.\\nThis format would be used by routers on different implementations.\\n\n\n##Decision\nWe use the following formats:\\nFor a message:\\n```\\n{\\nonward_route: Route,\\nreturn_route: Route,\\npayload: Any\\n}\\n```\\nWhere\\n`Route` - an ordered list of addresses.\\nFor an address:\\n```\\n{\\ntype: Integer,\\ndata: Any\\n}\\n```\\n","Predictions":"We will use the following formats:nFor a message:nnonward_route: Route,nreturn_route: Route,npayload: AnynnWherenRoute - an ordered list of addresses.nFor an address:nntype: Integer,ndata: Anynnn"}
{"File Name":"linshare-mobile-android-app\/0008-download-with-downloadmanager-service.md","Context":"## Context\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\n","Decision":"Instead of implementing a `Worker` like `Upload`, we will delegate downloading task to `DownloadManager` system service.\\n","tokens":99,"id":1647,"text":"## Context\\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\\n\n\n##Decision\nInstead of implementing a `Worker` like `Upload`, we will delegate downloading task to `DownloadManager` system service.\\n","Predictions":"We will use stack to generate a stack containing the following urls:nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"libelektra\/capabilities.md","Context":"## Problem\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\n(as they are designed to do so). Limitations of other storage plugins are\ne.g., that not every structure of configuration is allowed.\nSome of these limitations were documented `infos\/status`, others were not.\n","Decision":"Add `infos\/features\/storage` to document limitations of storage plugins.\\nIdeally, storage plugins should throw an error in `kdbSet` for\\nunrepresentable KeySets.\\nElektra cannot guarantee that any configuration file format can\\nbe mounted anywhere.\\nDevelopers, maintainers and administrators are responsible for what\\nthey mount. They need to test the setup.\\n- [Base Name](base_name.md)\\n","tokens":66,"id":1318,"text":"## Problem\\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\\n(as they are designed to do so). Limitations of other storage plugins are\\ne.g., that not every structure of configuration is allowed.\\nSome of these limitations were documented `infos\/status`, others were not.\\n\n\n##Decision\nAdd `infos\/features\/storage` to document limitations of storage plugins.\\nIdeally, storage plugins should throw an error in `kdbSet` for\\nunrepresentable KeySets.\\nElektra cannot guarantee that any configuration file format can\\nbe mounted anywhere.\\nDevelopers, maintainers and administrators are responsible for what\\nthey mount. They need to test the setup.\\n- [Base Name](base_name.md)\\n","Predictions":"Add infos\/features\/storage to document limitations of storage plugins.nIdeally, storage plugins should throw an error in kdbSet fornunrepresentable KeySets.nElektra cannot guarantee that any configuration file format cannbe mounted anywhere.nDevelopers, maintainers and administrators are responsible for whatnthey mount. They need to test the setup.n- [Base Name](base_name.md)n"}
{"File Name":"govuk-docker\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\nThe [original govuk-docker repo](https:\/\/github.com\/benthorner\/govuk-docker) was written outside of GOV.UK. Major architectural decisions, as well as more minor decisions, were written as [documentation in the repo][DECISIONS], together with their associated user [NEEDS]. While these documents have historical value, they are not being maintained and increasingly differ from the current state of the repo. As part of adopting an ADR approach, we should clearly deprecate these historical documents to avoid confusion.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":127,"id":1669,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\nThe [original govuk-docker repo](https:\/\/github.com\/benthorner\/govuk-docker) was written outside of GOV.UK. Major architectural decisions, as well as more minor decisions, were written as [documentation in the repo][DECISIONS], together with their associated user [NEEDS]. While these documents have historical value, they are not being maintained and increasingly differ from the current state of the repo. As part of adopting an ADR approach, we should clearly deprecate these historical documents to avoid confusion.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisionsn"}
{"File Name":"celestia-core\/adr-018-ABCI-Validators.md","Context":"## Context\nADR 009 introduced major improvements to the ABCI around validators and the use\nof Amino. Here we follow up with some additional changes to improve the naming\nand expected use of Validator messages.\n","Decision":"### Validator\\nCurrently a Validator contains `address` and `pub_key`, and one or the other is\\noptional\/not-sent depending on the use case. Instead, we should have a\\n`Validator` (with just the address, used for RequestBeginBlock)\\nand a `ValidatorUpdate` (with the pubkey, used for ResponseEndBlock):\\n```\\nmessage Validator {\\nbytes address\\nint64 power\\n}\\nmessage ValidatorUpdate {\\nPubKey pub_key\\nint64 power\\n}\\n```\\nAs noted in [ADR-009](adr-009-ABCI-design.md),\\nthe `Validator` does not contain a pubkey because quantum public keys are\\nquite large and it would be wasteful to send them all over ABCI with every block.\\nThus, applications that want to take advantage of the information in BeginBlock\\nare _required_ to store pubkeys in state (or use much less efficient lazy means\\nof verifying BeginBlock data).\\n### RequestBeginBlock\\nLastCommitInfo currently has an array of `SigningValidator` that contains\\ninformation for each validator in the entire validator set.\\nInstead, this should be called `VoteInfo`, since it is information about the\\nvalidator votes.\\nNote that all votes in a commit must be from the same round.\\n```\\nmessage LastCommitInfo {\\nint64 round\\nrepeated VoteInfo commit_votes\\n}\\nmessage VoteInfo {\\nValidator validator\\nbool signed_last_block\\n}\\n```\\n### ResponseEndBlock\\nUse ValidatorUpdates instead of Validators. Then it's clear we don't need an\\naddress, and we do need a pubkey.\\nWe could require the address here as well as a sanity check, but it doesn't seem\\nnecessary.\\n### InitChain\\nUse ValidatorUpdates for both Request and Response. InitChain\\nis about setting\/updating the initial validator set, unlike BeginBlock\\nwhich is just informational.\\n","tokens":44,"id":4119,"text":"## Context\\nADR 009 introduced major improvements to the ABCI around validators and the use\\nof Amino. Here we follow up with some additional changes to improve the naming\\nand expected use of Validator messages.\\n\n\n##Decision\n### Validator\\nCurrently a Validator contains `address` and `pub_key`, and one or the other is\\noptional\/not-sent depending on the use case. Instead, we should have a\\n`Validator` (with just the address, used for RequestBeginBlock)\\nand a `ValidatorUpdate` (with the pubkey, used for ResponseEndBlock):\\n```\\nmessage Validator {\\nbytes address\\nint64 power\\n}\\nmessage ValidatorUpdate {\\nPubKey pub_key\\nint64 power\\n}\\n```\\nAs noted in [ADR-009](adr-009-ABCI-design.md),\\nthe `Validator` does not contain a pubkey because quantum public keys are\\nquite large and it would be wasteful to send them all over ABCI with every block.\\nThus, applications that want to take advantage of the information in BeginBlock\\nare _required_ to store pubkeys in state (or use much less efficient lazy means\\nof verifying BeginBlock data).\\n### RequestBeginBlock\\nLastCommitInfo currently has an array of `SigningValidator` that contains\\ninformation for each validator in the entire validator set.\\nInstead, this should be called `VoteInfo`, since it is information about the\\nvalidator votes.\\nNote that all votes in a commit must be from the same round.\\n```\\nmessage LastCommitInfo {\\nint64 round\\nrepeated VoteInfo commit_votes\\n}\\nmessage VoteInfo {\\nValidator validator\\nbool signed_last_block\\n}\\n```\\n### ResponseEndBlock\\nUse ValidatorUpdates instead of Validators. Then it's clear we don't need an\\naddress, and we do need a pubkey.\\nWe could require the address here as well as a sanity check, but it doesn't seem\\nnecessary.\\n### InitChain\\nUse ValidatorUpdates for both Request and Response. InitChain\\nis about setting\/updating the initial validator set, unlike BeginBlock\\nwhich is just informational.\\n","Predictions":"We will use the Validator as the Validator in the ValidatorUpdate.n"}
{"File Name":"HES_pipeline\/data_storage_and_access.md","Context":"## Context\nThe raw HES data is large, complex, unweildly and potentially dirty. A way to\nclean, store and later easily access the data for analysis required, which means\nthat repeated cleaning and prep of the raw data is avoided.\nThere are multiple datasets (with different variables), which once cleaned, will\nbe accompanied by additional derived variables, and additional derived datasets.\n","Decision":"The raw data will be read into a SQLite database through R. R is The Health\\nFoundation's open-source language of choice, and is well equiped to\\nhandle later analysis of said data. An SQLite database is not stored in memory,\\navoiding holding very large datasets an R session. It is a standard database\\nformat which can be easily queried with SQL which can in turn be written within\\nmost other common languagses, including R.\\nIn an SQLite database the database consistents of all the data, and within that\\nthere are tables; here each table corresponds to a HES dataset or a derived\\ndataset.\\n","tokens":83,"id":3253,"text":"## Context\\nThe raw HES data is large, complex, unweildly and potentially dirty. A way to\\nclean, store and later easily access the data for analysis required, which means\\nthat repeated cleaning and prep of the raw data is avoided.\\nThere are multiple datasets (with different variables), which once cleaned, will\\nbe accompanied by additional derived variables, and additional derived datasets.\\n\n\n##Decision\nThe raw data will be read into a SQLite database through R. R is The Health\\nFoundation's open-source language of choice, and is well equiped to\\nhandle later analysis of said data. An SQLite database is not stored in memory,\\navoiding holding very large datasets an R session. It is a standard database\\nformat which can be easily queried with SQL which can in turn be written within\\nmost other common languagses, including R.\\nIn an SQLite database the database consistents of all the data, and within that\\nthere are tables; here each table corresponds to a HES dataset or a derived\\ndataset.\\n","Predictions":"The raw data will be read into a SQLite database through R. R is The HealthnFoundation's open-source language of choice, and is well equiped tonhandle later analysis of said data. An SQLite database is not stored in memory, and is well equiped tonhandle later analysis of said data. An SQLite database is not stored in memory, and is not stored innany other common langauges, including R.nIn an SQLite database the database consistents of all the data, and within thatnthere are tables; here each table corresponds to a HES dataset or a derivedndataset.n"}
{"File Name":"stentor\/0001-record-architecture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2166,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"crispy-dragon\/ADR-2-Frontend-Framework.md","Context":"## Context\nTo make it easy for users to interact with our API we need a frontend that can be used by modern devices.\n","Decision":"The frontend will be build with [React](https:\/\/reactjs.org\/) and [Snowpack](https:\/\/www.snowpack.dev) will be used as build tool.\\nThe application will be styled with [Tailwind CSS](https:\/\/tailwindcss.com)\\nWe also considered the following alternative solutions:\\n* Vanilla HTML would work for the scope of this application.\\n* Vanilla CSS would work, but take more time to generate nice layouts.\\n","tokens":27,"id":847,"text":"## Context\\nTo make it easy for users to interact with our API we need a frontend that can be used by modern devices.\\n\n\n##Decision\nThe frontend will be build with [React](https:\/\/reactjs.org\/) and [Snowpack](https:\/\/www.snowpack.dev) will be used as build tool.\\nThe application will be styled with [Tailwind CSS](https:\/\/tailwindcss.com)\\nWe also considered the following alternative solutions:\\n* Vanilla HTML would work for the scope of this application.\\n* Vanilla CSS would work, but take more time to generate nice layouts.\\n","Predictions":"We will use [Standard Rails](https:\/\/github.com\/StandardRailers\/standardrails) as our Rails framework.n"}
{"File Name":"monocle\/0010-choice-of-protobuf.md","Context":"## Context and Problem Statement\n* The current API implementations is spread across multiple modules (such as webapp, config, db and query) without a formal definition of the inputs\/outputs. This induces inconsistencies and we foresee technical difficulties to maintain and evole the service.\n* We would like to use an interface description language to define the API between the various component (such as between the webapi and webclients, worker and database).\n* How do we define and maintain the interfaces?\n","Decision":"Chosen option: \"Protobuf\", because it comes out best (see below).\\n### Positive Consequences\\n- We need to start defining interfaces outside of their implementations.\\n- We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.\\nFor example, use `POST \/get_projects BODY { \"index\": \"name\" }` instead of `GET \/projects?index=name`.\\n### Negative Consequences\\n- The team needs to learn a new language.\\n- Follow-up decisions required for the actual implementation.\\n","tokens":100,"id":4644,"text":"## Context and Problem Statement\\n* The current API implementations is spread across multiple modules (such as webapp, config, db and query) without a formal definition of the inputs\/outputs. This induces inconsistencies and we foresee technical difficulties to maintain and evole the service.\\n* We would like to use an interface description language to define the API between the various component (such as between the webapi and webclients, worker and database).\\n* How do we define and maintain the interfaces?\\n\n\n##Decision\nChosen option: \"Protobuf\", because it comes out best (see below).\\n### Positive Consequences\\n- We need to start defining interfaces outside of their implementations.\\n- We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.\\nFor example, use `POST \/get_projects BODY { \"index\": \"name\" }` instead of `GET \/projects?index=name`.\\n### Negative Consequences\\n- The team needs to learn a new language.\\n- Follow-up decisions required for the actual implementation.\\n","Predictions":"* We need to start defining interfaces outside of their implementations.n* We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example, use POST \/get_projects BODY  \"index\": \"name\"  instead of GET \/projects?index=name.n### Positive Consequencesn* We need to start defining interfaces outside of their implementations.n* We need to start defining interfaces outside of their implementations.n* We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.nFor example, use POST \/get_projects BODY  \"index\": \"name\"  instead of GET \/projects?index=name.n### Negative Consequencesn* The team needs to learn a new language.n* Follow-up decisions required for the actual implementation.n"}
{"File Name":"hee-web-blueprint\/0003-use-github-actions-for-automation.md","Context":"## Context\nWe should determine the platform we use to automate the build, test and deployment of the HEE National Website Platform.\n","Decision":"Having determined to use Github to manage the source code of the platform, the simplest answer to this question was to look at Github actions. We determined after some investigative work to prove out our ability to deploy to the BloomReach cloud that we should use Github Actions to manage platform automation.\\n","tokens":27,"id":1196,"text":"## Context\\nWe should determine the platform we use to automate the build, test and deployment of the HEE National Website Platform.\\n\n\n##Decision\nHaving determined to use Github to manage the source code of the platform, the simplest answer to this question was to look at Github actions. We determined after some investigative work to prove out our ability to deploy to the BloomReach cloud that we should use Github Actions to manage platform automation.\\n","Predictions":"We will use [Azure](https:\/\/azure.io\/) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and\/or enterprise artifacts).n"}
{"File Name":"texas-holdem-code-challenge\/0008-use-typescript.md","Context":"## Context\nThe approach I have chosen (see docs\/model.md and docs\/algorithm.md) leads to\nmany small unit-testable components and a main function that ties them together.\nThe options I can see for the main function are:\n1. Write it with unit tests, heavily mocking the dependencies\n2. Use a type system to check that the plumbing matches up, don't unit test the\nmain function\n3. Don't test the main function, don't use types. My guess is I'll make some\nmistake along the way and have to debug it.\nIn all cases I'll run integration tests, but without (1) or (2) I won't know\nwhere bugs are when the integration test fails.\nMy guess is that (2) will be more efficient than (1), and either of them will be\nmore efficient than (3).\nFor JavaScript static typing, I'm familiar with TypeScript.\n","Decision":"Use TypeScript.\\n","tokens":193,"id":2290,"text":"## Context\\nThe approach I have chosen (see docs\/model.md and docs\/algorithm.md) leads to\\nmany small unit-testable components and a main function that ties them together.\\nThe options I can see for the main function are:\\n1. Write it with unit tests, heavily mocking the dependencies\\n2. Use a type system to check that the plumbing matches up, don't unit test the\\nmain function\\n3. Don't test the main function, don't use types. My guess is I'll make some\\nmistake along the way and have to debug it.\\nIn all cases I'll run integration tests, but without (1) or (2) I won't know\\nwhere bugs are when the integration test fails.\\nMy guess is that (2) will be more efficient than (1), and either of them will be\\nmore efficient than (3).\\nFor JavaScript static typing, I'm familiar with TypeScript.\\n\n\n##Decision\nUse TypeScript.\\n","Predictions":"We will use the [Semantic Validation](https:\/\/github.com\/SemanticValidation) library to validate the domain.n"}
{"File Name":"dapr\/API-009-bidirectional-bindings.md","Context":"## Context\nAs we want to provide bi-directional capabilities for bindings to allow for cases such as getting a blob from a storage account,\nAn API change is needed to account for the requested type of operation.\n","Decision":"### Naming\\nIt was decided to keep the bindings name as is. Alternative proposals were included changing bindings to connectors, but a strong case couldn't be made in favor of connectors to justify the breaking change it would cause.\\n### Types\\nIt was decided to keep the same YAML format for both input bindings and bi-directional bindings as it is today.\\nAfter careful inspection, splitting to two types (for example, trigger bindings and bindings) would incur significant maintanace overhead for the app operator and\\nDid not provide meaningful value.\\nIn addition, there was no feedback from community or prospecive users that input bindings and output bindings were confusing in any way.\\n### API structure\\nIt was decided that the API url will be kept as: `http:\/\/localhost:<port>\/v1.0\/bindings\/<name>`.\\nThe verb for the HTTP API will remain POST\/PUT, and the type of operation will be part of a versioned, structured schema for bindings.\\nThis is not a breaking change.\\n### Schema and versioning\\nIn accordance with our decision to work towards enterprise versioning, it was accepted that schemas will include a `version` field in\\nThe payload to specify which version of given component needs to be used that corresponds to the given payload.\\nIn addition, an extra field will be added to denote the type of operation that binding supports, for example: `get`, `list`, `create` etc.\\nBindings components will provide the means for the Dapr runtime to query for their supported capabilities and return a validaton error if the operation type is not supported.\\n","tokens":44,"id":75,"text":"## Context\\nAs we want to provide bi-directional capabilities for bindings to allow for cases such as getting a blob from a storage account,\\nAn API change is needed to account for the requested type of operation.\\n\n\n##Decision\n### Naming\\nIt was decided to keep the bindings name as is. Alternative proposals were included changing bindings to connectors, but a strong case couldn't be made in favor of connectors to justify the breaking change it would cause.\\n### Types\\nIt was decided to keep the same YAML format for both input bindings and bi-directional bindings as it is today.\\nAfter careful inspection, splitting to two types (for example, trigger bindings and bindings) would incur significant maintanace overhead for the app operator and\\nDid not provide meaningful value.\\nIn addition, there was no feedback from community or prospecive users that input bindings and output bindings were confusing in any way.\\n### API structure\\nIt was decided that the API url will be kept as: `http:\/\/localhost:<port>\/v1.0\/bindings\/<name>`.\\nThe verb for the HTTP API will remain POST\/PUT, and the type of operation will be part of a versioned, structured schema for bindings.\\nThis is not a breaking change.\\n### Schema and versioning\\nIn accordance with our decision to work towards enterprise versioning, it was accepted that schemas will include a `version` field in\\nThe payload to specify which version of given component needs to be used that corresponds to the given payload.\\nIn addition, an extra field will be added to denote the type of operation that binding supports, for example: `get`, `list`, `create` etc.\\nBindings components will provide the means for the Dapr runtime to query for their supported capabilities and return a validaton error if the operation type is not supported.\\n","Predictions":"We decided to use the Status field in the Routes table to indicate which version of the synchronized route is supported.nThe Status field will be used to indicate which synchronized route is supported.n"}
{"File Name":"tendermint\/adr-061-p2p-refactor-scope.md","Context":"## Context\nThe `p2p` package responsible for peer-to-peer networking is rather old and has a number of weaknesses, including tight coupling, leaky abstractions, lack of tests, DoS vulnerabilites, poor performance, custom protocols, and incorrect behavior. A refactor has been discussed for several years ([#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)).\nInformal Systems are also building a Rust implementation of Tendermint, [Tendermint-rs](https:\/\/github.com\/informalsystems\/tendermint-rs), and plan to implement P2P networking support over the next year. As part of this work, they have requested adopting e.g. [QUIC](https:\/\/datatracker.ietf.org\/doc\/draft-ietf-quic-transport\/) as a transport protocol instead of implementing the custom application-level `MConnection` stream multiplexing protocol that Tendermint currently uses.\nThis ADR summarizes recent discussion with stakeholders on the scope of a P2P refactor. Specific designs and implementations will be submitted as separate ADRs.\n","Decision":"The P2P stack will be refactored and improved iteratively, in several phases:\\n* **Phase 1:** code and API refactoring, maintaining protocol compatibility as far as possible.\\n* **Phase 2:** additional transports and incremental protocol improvements.\\n* **Phase 3:** disruptive protocol changes.\\nThe scope of phases 2 and 3 is still uncertain, and will be revisited once the preceding phases have been completed as we'll have a better sense of requirements and challenges.\\n","tokens":227,"id":1979,"text":"## Context\\nThe `p2p` package responsible for peer-to-peer networking is rather old and has a number of weaknesses, including tight coupling, leaky abstractions, lack of tests, DoS vulnerabilites, poor performance, custom protocols, and incorrect behavior. A refactor has been discussed for several years ([#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)).\\nInformal Systems are also building a Rust implementation of Tendermint, [Tendermint-rs](https:\/\/github.com\/informalsystems\/tendermint-rs), and plan to implement P2P networking support over the next year. As part of this work, they have requested adopting e.g. [QUIC](https:\/\/datatracker.ietf.org\/doc\/draft-ietf-quic-transport\/) as a transport protocol instead of implementing the custom application-level `MConnection` stream multiplexing protocol that Tendermint currently uses.\\nThis ADR summarizes recent discussion with stakeholders on the scope of a P2P refactor. Specific designs and implementations will be submitted as separate ADRs.\\n\n\n##Decision\nThe P2P stack will be refactored and improved iteratively, in several phases:\\n* **Phase 1:** code and API refactoring, maintaining protocol compatibility as far as possible.\\n* **Phase 2:** additional transports and incremental protocol improvements.\\n* **Phase 3:** disruptive protocol changes.\\nThe scope of phases 2 and 3 is still uncertain, and will be revisited once the preceding phases have been completed as we'll have a better sense of requirements and challenges.\\n","Predictions":"The P2P stack will be refactored and improved iteratively, in several phases:n* **Phase 1:** code and API refactoring, maintaining protocol compatibility as far as possible.n* **Phase 2:** additional transports and incremental protocol improvements.n* **Phase 3:** disruptive protocol changes.nThe scope of phases 2 and 3 is still uncertain, and will be revisited once the preceding phases have been completed as we'll have a better sense of requirements and challenges.n"}
{"File Name":"cloud-on-k8s\/adr-template.md","Context":"## Context and Problem Statement\n[Describe the context and problem statement, for example, in free form using two to three sentences. You may want to articulate the problem in form of a question.]\n## Decision Drivers <!-- optional -->\n* [driver 1, for example, a force, facing concern, \u2026]\n* [driver 2, for example, a force, facing concern, \u2026]\n* \u2026 <!-- numbers of drivers can vary -->\n","Decision":"* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. For example, only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best].\\n### Positive Consequences <!-- optional -->\\n* [For example, improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [For example, compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","tokens":96,"id":4701,"text":"## Context and Problem Statement\\n[Describe the context and problem statement, for example, in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\n\n\n##Decision\n* [driver 1, for example, a force, facing concern, \u2026]\\n* [driver 2, for example, a force, facing concern, \u2026]\\n* \u2026 <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. For example, only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best].\\n### Positive Consequences <!-- optional -->\\n* [For example, improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative Consequences <!-- optional -->\\n* [For example, compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n","Predictions":"* [driver 1, e.g., a force, facing concern,...]n* [driver 2, for example, a force, facing concern,...]n*...!-- numbers of drivers can vary -->nChosen option: \"[option 1]\", because [justification. For example, only option, which meets k.o. criterion decision driver | which resolves force force |... | comes out best].n### Positive Consequences!-- optional -->n* [For example, improvement of quality attribute satisfaction, follow-up decisions required,...]n*...n### Negative Consequences!-- optional -->n* [For example, compromising quality attribute, follow-up decisions required,...]n*...n"}
{"File Name":"trade-access-program\/0005-viewflow-for-grant-management-portal.md","Context":"## Context\nWe need a system to manage the lifecycle of a grant application.\n","Decision":"Viewflow was chosen as a framework to help us achieve this goal while making the development process efficient.\\n### Build vs Buy Paper\\nVarious alternatives to Viewflow were considered when making this decision. Those alternatives are details in the\\nalpha stage of the project here: https:\/\/uktrade.atlassian.net\/l\/c\/zEAEM37j\\n### Viewflow analysis\\nSome initial analysis on how viewflow could be used was also done in alpha: https:\/\/uktrade.atlassian.net\/l\/c\/Lc77C4mq\\n","tokens":17,"id":5009,"text":"## Context\\nWe need a system to manage the lifecycle of a grant application.\\n\n\n##Decision\nViewflow was chosen as a framework to help us achieve this goal while making the development process efficient.\\n### Build vs Buy Paper\\nVarious alternatives to Viewflow were considered when making this decision. Those alternatives are details in the\\nalpha stage of the project here: https:\/\/uktrade.atlassian.net\/l\/c\/zEAEM37j\\n### Viewflow analysis\\nSome initial analysis on how viewflow could be used was also done in alpha: https:\/\/uktrade.atlassian.net\/l\/c\/Lc77C4mq\\n","Predictions":"We will use a stack library to store the data we need to store.n"}
{"File Name":"Nosedive\/0003-evolutionary-architecture.md","Context":"## Context\nThere is a lot of things to learn, not only the ecosystem of clojure, we what create a high scalable application deploy in cloud, with autoscale, resilience, geographic distribution, multyples clouds providers, etc.\n","Decision":"We start with the most simple solution, a console application, after that we try to evolve to an API, Web Page, Microservices, etc.\\n","tokens":49,"id":108,"text":"## Context\\nThere is a lot of things to learn, not only the ecosystem of clojure, we what create a high scalable application deploy in cloud, with autoscale, resilience, geographic distribution, multyples clouds providers, etc.\\n\n\n##Decision\nWe start with the most simple solution, a console application, after that we try to evolve to an API, Web Page, Microservices, etc.\\n","Predictions":"We start with the most simple solution, a console application, after that we try to evolve to an API, Web Page, Microservices, etc.n"}
{"File Name":"BMMRO\/2019-11-20_firestore.md","Context":"## Context\nWe need a database in order to store the data provided by the user in the different forms.\n","Decision":"To reduce the amount of integrations and complexity, we've decided to use one of the databases that Firebase offers.\\nUnder the Firebase umbrella are Real-time database and Firestore which are similar in many features.\\nGoogle recommends Firestore for new developers, as it is built upon \u201cthe successes of Real-time database\u201d. Also, for our purposes, Firestore is a better choice because:\\n- It offers offline support for iOS, Android and Web clients whereas real-time DB supports only iOS and Android clients.\\n- The one-big-JSON-tree structure of real-time can become difficult to maintain as the DB grows. Document-based Firestore offers better organisation provisions using collections.\\n- Scaling on firestore is automatic whereas scaling on real-time database requires sharding.\\n","tokens":23,"id":2488,"text":"## Context\\nWe need a database in order to store the data provided by the user in the different forms.\\n\n\n##Decision\nTo reduce the amount of integrations and complexity, we've decided to use one of the databases that Firebase offers.\\nUnder the Firebase umbrella are Real-time database and Firestore which are similar in many features.\\nGoogle recommends Firestore for new developers, as it is built upon \u201cthe successes of Real-time database\u201d. Also, for our purposes, Firestore is a better choice because:\\n- It offers offline support for iOS, Android and Web clients whereas real-time DB supports only iOS and Android clients.\\n- The one-big-JSON-tree structure of real-time can become difficult to maintain as the DB grows. Document-based Firestore offers better organisation provisions using collections.\\n- Scaling on firestore is automatic whereas scaling on real-time database requires sharding.\\n","Predictions":"We will use a resources folder in the \/docs folder.n"}
{"File Name":"adr\/ADR-10-profile-deployment-debouncing.md","Context":"## Context and Problem Statement\nHow do we debouncing profile\/scene deployments to avoid DDoS and wasted bandwidth?\n- As of today we only deploy profiles when changing the avatar\n- Soon, a new deployment will happen after mute\/unmute\n","Decision":"We decided to do Client side debouncing (both options 2 and 3). We may measure deployments in the future to revisit the server conversation.\\n","tokens":51,"id":4626,"text":"## Context and Problem Statement\\nHow do we debouncing profile\/scene deployments to avoid DDoS and wasted bandwidth?\\n- As of today we only deploy profiles when changing the avatar\\n- Soon, a new deployment will happen after mute\/unmute\\n\n\n##Decision\nWe decided to do Client side debouncing (both options 2 and 3). We may measure deployments in the future to revisit the server conversation.\\n","Predictions":"We decided to do Client side debouncing (both options 2 and 3). We may measure deployments in the future to revisit the server conversation.n"}
{"File Name":"publicservice-registry\/0009-use-more-value-objects.md","Context":"## Context\nA value object is a small object that represents a simple entity whose equality is not based on identity: i.e. two value objects are equal when they have the same value, not necessarily being the same object. Examples of value objects are objects representing an amount of money or a date range.\nIn `Wegwijs`, we experienced great value towards type safety from using VOs. We want to bank in even more on the use of VOs.\n","Decision":"Use a Value Object wherever possible.\\n","tokens":96,"id":3124,"text":"## Context\\nA value object is a small object that represents a simple entity whose equality is not based on identity: i.e. two value objects are equal when they have the same value, not necessarily being the same object. Examples of value objects are objects representing an amount of money or a date range.\\nIn `Wegwijs`, we experienced great value towards type safety from using VOs. We want to bank in even more on the use of VOs.\\n\n\n##Decision\nUse a Value Object wherever possible.\\n","Predictions":"We will use a key for each entity.n"}
{"File Name":"architecture\/0002-apply-clean-code-guidelines.md","Context":"## Context\nAs it's hard to understand code, it is crucial that anybody can easily\nunderstand the code you're working on. This applies to all levels of code, not\nonly the code itself, but groups of code, complete applications and groups of\napplications.\n","Decision":"I will always focus on simplicity, readaility and the ease of understanding\\nany code or structure.\\n","tokens":57,"id":1416,"text":"## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n\n\n##Decision\nI will always focus on simplicity, readaility and the ease of understanding\\nany code or structure.\\n","Predictions":"I will always focus on simplicity, readaility and the ease of understandingnany code or structure.n"}
{"File Name":"molgenis-service-armadillo\/0006-use-rdata-format-as-data-input.md","Context":"## Context\nWe want to make the MOLGENIS \"Armadillo\" service data provider agnostic. There are a couple of reasons why we are doing this\n- the service is usable for other parties as well\n- the service can still integrate with MOLGENIS.\n- the release cycle of the service is data provider independent\n- the service can be developed by other parties as well\n","Decision":"We implement an endpoint to upload and load RData files in the MOLGENIS \"Armadillo\" service to manage data for the use in DataSHIELD.\\n","tokens":83,"id":1326,"text":"## Context\\nWe want to make the MOLGENIS \"Armadillo\" service data provider agnostic. There are a couple of reasons why we are doing this\\n- the service is usable for other parties as well\\n- the service can still integrate with MOLGENIS.\\n- the release cycle of the service is data provider independent\\n- the service can be developed by other parties as well\\n\n\n##Decision\nWe implement an endpoint to upload and load RData files in the MOLGENIS \"Armadillo\" service to manage data for the use in DataSHIELD.\\n","Predictions":"We implement an endpoint to upload and load RData files in the MOLGENIS \"Armadillo\" service to manage data for the use in DataSHIELD.n"}
{"File Name":"verify-hub\/0001-record-architechture-decisions.md","Context":"## Context\nWe need to record the architectural decisions made on this project.\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4206,"text":"## Context\\nWe need to record the architectural decisions made on this project.\\n\n\n##Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","Predictions":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).n"}
{"File Name":"dapr\/API-004-binding-manifests.md","Context":"## Context\nAs we rename Event Sources to Bindings, and formally separate State Stores, Message Buses, and Bindings, we need to decide if we need to introduce different manifest types.\n","Decision":"### Dapr\\n* All components use the same **Component** manifests, identified by a component **type**.\\n* We'll come up with a mechanism to support pluggable secret stores. We'll support Kubernetes native secret store and Azure Key Vault in the initial release.\\n","tokens":40,"id":84,"text":"## Context\\nAs we rename Event Sources to Bindings, and formally separate State Stores, Message Buses, and Bindings, we need to decide if we need to introduce different manifest types.\\n\n\n##Decision\n### Dapr\\n* All components use the same **Component** manifests, identified by a component **type**.\\n* We'll come up with a mechanism to support pluggable secret stores. We'll support Kubernetes native secret store and Azure Key Vault in the initial release.\\n","Predictions":"* All components use the same **Component** manifests, identified by a component **type**.n* We'll come up with a mechanism to support pluggable secret stores. We'll support Kubernetes native secret store and Azure Key Vault in the initial release.n"}
{"File Name":"modular-monolith-with-ddd\/0015-use-in-memory-events-bus.md","Context":"## Context\nAs we want to base inter-modular communication on asynchronous communication in the form of event-driven architecture, we need some \"events bus\" to do that.\n","Decision":"Solution number 1 - In Memory Events Bus<\/br>\\nAt that moment we don't see more advanced integration scenarios in our system than simple publish\/subscribe scenario. We decided to follow the simplest scenario and if it will be necessary - move to more advanced.\\n","tokens":35,"id":885,"text":"## Context\\nAs we want to base inter-modular communication on asynchronous communication in the form of event-driven architecture, we need some \"events bus\" to do that.\\n\n\n##Decision\nSolution number 1 - In Memory Events Bus<\/br>\\nAt that moment we don't see more advanced integration scenarios in our system than simple publish\/subscribe scenario. We decided to follow the simplest scenario and if it will be necessary - move to more advanced.\\n","Predictions":"**We will use the Axon framework and Axon Server as a mature open-source implementation of the CQRS\/ES component architecture pattern.**n"}
{"File Name":"cape-cod-coastal-planner\/0004-use-postgresql-as-db.md","Context":"## Context\nThe API for CHIP needs some sort of persistence for storing its data. Although the data needs for the app are pretty lightweight, it's likely that they will grow in the future as new features are desired or more data is acquired. To this end, sticking with a simple, powerful, reliable, and flexible technology that is widely understood makes sense.\n","Decision":"A RDBMS fits this bill and PostgreSQL does so even more snugly. It's a top contender, it's free, it's currently the best supported traditional database for the Elixir\/Erlang ecosystem and it's unparalleled at its geospatial capabilities.\\n","tokens":73,"id":4859,"text":"## Context\\nThe API for CHIP needs some sort of persistence for storing its data. Although the data needs for the app are pretty lightweight, it's likely that they will grow in the future as new features are desired or more data is acquired. To this end, sticking with a simple, powerful, reliable, and flexible technology that is widely understood makes sense.\\n\n\n##Decision\nA RDBMS fits this bill and PostgreSQL does so even more snugly. It's a top contender, it's free, it's currently the best supported traditional database for the Elixir\/Erlang ecosystem and it's unparalleled at its geospatial capabilities.\\n","Predictions":"We will use PostgreSQL for the API implementation. PostgreSQL is a free, open-source database that is currently the best supported traditional database for the Elixir\/Erlang ecosystem.n"}
{"File Name":"viplab-websocket-api\/0004-use-asymmetric-jwt-signing.md","Context":"## Context and Problem Statement\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\nJWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\nWhen implementing JWTs one must decide which method to use.\n## Decision Drivers\n* Multi tenant support with own keys for each tenant\n","Decision":"* Multi tenant support with own keys for each tenant\\nChosen option: \"Asymmetric JWT signing\", because it the only option which allow to use different keys for different tenants.\\n### Positive Consequences\\n* multiple keys are supported\\n### Negative Consequences\\n* complex management of keys\\n","tokens":77,"id":1721,"text":"## Context and Problem Statement\\nWhen using JSON Web Tokens generating signatures and verifying them is an important task.\\nJWTs can be signed using a secret (with the HMAC algorithm) or a public\/private key pair using RSA or ECDSA.\\nWhen implementing JWTs one must decide which method to use.\\n## Decision Drivers\\n* Multi tenant support with own keys for each tenant\\n\n\n##Decision\n* Multi tenant support with own keys for each tenant\\nChosen option: \"Asymmetric JWT signing\", because it the only option which allow to use different keys for different tenants.\\n### Positive Consequences\\n* multiple keys are supported\\n### Negative Consequences\\n* complex management of keys\\n","Predictions":"* Authentication of KeysnChosen option: \"Asymmetric Signature\", because it is the only option which allow to use different keys for different tenants.n### Positive Consequencesn* multiple keys are supportedn### Negative Consequencesn* complex management of keysn"}
{"File Name":"front-end-monorepo\/adr-10.md","Context":"## Context\nIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.\nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse\/Panoptes-Front-End#4992] (https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/issues\/4992)).\nThe default behavior in existing libraries like [JSON8](https:\/\/github.com\/sonnyp\/JSON8\/tree\/master\/packages\/json8#ooserialize) or MobX's [`toJS`](https:\/\/mobx.js.org\/refguide\/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object. However, for us, we are using maps for workflow steps because a key requirement is the ordering of key-value pairs, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.\n","Decision":"We decided to instead implement our own [type conversion utility function](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/store\/utils\/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:\\n``` js\\nconst workflow = {\\nid: '1',\\nsteps: [['S1', { taskKeys: ['T1', 'T2'] }], ['S2', { taskKeys: ['T3'] }]] \/\/ How they will be stored on Panoptes\\n}\\n```\\nAnd when a workflow request is received by the classifier store, it is converted by Mobx-State-Tree into an observable map when added to the store.\\n**A note about the use of arrays for the key-value pairs**\\nSubject locations are an array of objects. It would make sense to do an array of objects here too, however the array of two values is closest to the format expected by maps when you instantiate them: https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Map\\nSo there's less type conversion happening if we store it this way. MobX will take objects too when setting an observable map, but if we ever want to instantiate an ES6 map independent of the store we would have to do another conversion from object to array of the key-value pair.\\n","tokens":320,"id":523,"text":"## Context\\nIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.\\nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse\/Panoptes-Front-End#4992] (https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/issues\/4992)).\\nThe default behavior in existing libraries like [JSON8](https:\/\/github.com\/sonnyp\/JSON8\/tree\/master\/packages\/json8#ooserialize) or MobX's [`toJS`](https:\/\/mobx.js.org\/refguide\/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object. However, for us, we are using maps for workflow steps because a key requirement is the ordering of key-value pairs, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.\\n\n\n##Decision\nWe decided to instead implement our own [type conversion utility function](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/store\/utils\/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:\\n``` js\\nconst workflow = {\\nid: '1',\\nsteps: [['S1', { taskKeys: ['T1', 'T2'] }], ['S2', { taskKeys: ['T3'] }]] \/\/ How they will be stored on Panoptes\\n}\\n```\\nAnd when a workflow request is received by the classifier store, it is converted by Mobx-State-Tree into an observable map when added to the store.\\n**A note about the use of arrays for the key-value pairs**\\nSubject locations are an array of objects. It would make sense to do an array of objects here too, however the array of two values is closest to the format expected by maps when you instantiate them: https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Map\\nSo there's less type conversion happening if we store it this way. MobX will take objects too when setting an observable map, but if we ever want to instantiate an ES6 map independent of the store we would have to do another conversion from object to array of the key-value pair.\\n","Predictions":"We decided to instead implement our own [type conversion utility function](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/store\/utils\/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:n jsnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn"}
{"File Name":"tamr-client\/0007-tamr-client-package.md","Context":"## Context\nWe have an existing userbase that relies on `tamr_unify_client` and cannot painlessly make backwards-incompatible changes.\nBut, we want to rearchitect this codebase as a [library of composable functions](\/contributor-guide\/adr\/0005-composable-functions).\n","Decision":"Implement rearchitected design as a new package named `tamr_client`.\\nRequire the `TAMR_CLIENT_BETA=1` feature flag for `tamr_client` package usage.\\nWarn users who attempt to use `tamr_client` package to opt-in if they want to beta test the new design.\\n","tokens":64,"id":539,"text":"## Context\\nWe have an existing userbase that relies on `tamr_unify_client` and cannot painlessly make backwards-incompatible changes.\\nBut, we want to rearchitect this codebase as a [library of composable functions](\/contributor-guide\/adr\/0005-composable-functions).\\n\n\n##Decision\nImplement rearchitected design as a new package named `tamr_client`.\\nRequire the `TAMR_CLIENT_BETA=1` feature flag for `tamr_client` package usage.\\nWarn users who attempt to use `tamr_client` package to opt-in if they want to beta test the new design.\\n","Predictions":"We will use a Serializer library to generate a serializer.nWe will use Serializer to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use Graph to generate a Graph file.nWe will use "}
{"File Name":"up-fiscal-data\/008-grant-wise.md","Context":"## Context and Problem Statement\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\n## Decision Drivers\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\n","Decision":"The challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\nThe `Grant-wise expenditure` section is being extracted over the `DDO-wise expenditure` section.\\n","tokens":82,"id":447,"text":"## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n\n\n##Decision\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\nThe `Grant-wise expenditure` section is being extracted over the `DDO-wise expenditure` section.\\n","Predictions":"1. Build two payment methodsn2. Create two payment methods for each of the two training schemesn3. Create two payment methods for each of the two training schemesn4. Create two payment methods for each of the two training schemesn5. Create two payment methods for each of the two training schemesn"}
{"File Name":"osticket-ticket-options-plugin\/001-required-properties.md","Context":"# Context\nScott recently added a `path_url` prompt to get the eventual url a web1 app would end up. The purpose of this property was to update the css and javascript url paths in a web app html file. The problem is that typical useage of the lifecorp-init app is to press enter through each prompt and accept the defaults. For a web1 app, there is no default for the url path because there is no standard for where the app will be placed on the lifecorp network. If this field does NOT get filled in when the app is intiated, then the developer must manually fill in this information. The purpose of lifecorp-init is to get a project skeleton up and running to a \"Hello World\" state with little to no modifications.\n# Decision\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\n# Consequences\nThe lifcorp-init will be able to block users from skipping required parameters.\n# Status\nAccepted\n","Decision":"Scott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","tokens":236,"id":139,"text":"# Context\\nScott recently added a `path_url` prompt to get the eventual url a web1 app would end up. The purpose of this property was to update the css and javascript url paths in a web app html file. The problem is that typical useage of the lifecorp-init app is to press enter through each prompt and accept the defaults. For a web1 app, there is no default for the url path because there is no standard for where the app will be placed on the lifecorp network. If this field does NOT get filled in when the app is intiated, then the developer must manually fill in this information. The purpose of lifecorp-init is to get a project skeleton up and running to a \"Hello World\" state with little to no modifications.\\n# Decision\\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n\n\n##Decision\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n","Predictions":"We will use the --require parameter to allow the user to specify a string to be passed to lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.n# ConsequencesnThe lifcorp-init will be able to block users from skipping required parameters.n# StatusnAcceptedn"}
{"File Name":"publicservice-registry\/0012-use-dutch-resource-names.md","Context":"## Context\nWhen exposing API endpoints, exposing a UI or exposing resource field names, we have to decide on the language used. Do we cater to the majority of the world by using English, or do we use Dutch because we are creating resources for the Flemish Government?\n","Decision":"We will use Dutch terms because it is an external requirement dictated by the organisation.\\n","tokens":56,"id":3126,"text":"## Context\\nWhen exposing API endpoints, exposing a UI or exposing resource field names, we have to decide on the language used. Do we cater to the majority of the world by using English, or do we use Dutch because we are creating resources for the Flemish Government?\\n\n\n##Decision\nWe will use Dutch terms because it is an external requirement dictated by the organisation.\\n","Predictions":"We will use English because it is a popular language in the world.n"}
{"File Name":"grout\/adr-2-rename-package.md","Context":"## Context\nThe name `ashlar` [is already taken on PyPi](https:\/\/pypi.org\/project\/ashlar\/).\nSince PyPi requires unique names for packages, this means that if we want to\ndistribute our package on PyPi, we'll have to either:\n1. Convince the owners of `ashlar` to give it to us\n2. Name the PyPi package something similar to `ashlar` but slightly different,\nlike `ashlar-core`\n3. Come up with a new name for the project\nOption 1 seems unlikely, given the maturity of the ashlar package on PyPi and\nhow recent the last release was (April 2018, less than four months ago). Number\n2 is perfectly functional but frustrating from a branding and distribution perspective,\nsince it has the potential to introduce some confusion and\/or competition with\nthe existing `ashlar` package.\nInstead, I believe that the best course of action is to choose option 3 and rename the project.\nThis will require us to come up with a new name for Ashlar, a [notoriously\ndifficult decision](https:\/\/martinfowler.com\/bliki\/TwoHardThings.html).\nSome options that I considered, all based on the idea of \"flexible\nconstruction materials\":\n- [Joist](https:\/\/en.wikipedia.org\/wiki\/Joist)\n- [Lintel](https:\/\/en.wikipedia.org\/wiki\/Lintel)\n- [Silicone](https:\/\/en.wikipedia.org\/wiki\/Silicone)\n- [Grout](https:\/\/en.wikipedia.org\/wiki\/Grout)\n","Decision":"I propose that we rename the project to **Grout**. Among the options above,\\n\"Grout\" is the name that sounds the best to me, and it's the one that I believe\\noffers the closest allegory for the project.\\nGrout is a construction material widely known for its physical flexibility and its\\npractical versatility: a fluid concrete used to create waterproof seals in\\nmasonry structures.\\nSome advantages of the name \"Grout\" include:\\n- \"Grout\" respects the origins of the project by referencing a masonry material,\\nbut unlike \"Ashlar\", the name \"Grout\" emphasizes the core features of the project\\n-- its flexibility and versatility as a base material\\nthat can scaffold and tie together much more complex projects.\\n- \"Grout\" is one syllable (one fewer than \"Ashlar\") and the easiest word to\\npronounce among the options I considered.\\n- Perhaps most importantly, `grout` is [available on\\nPyPi](https:\/\/pypi.org\/project\/grout).\\n","tokens":339,"id":2980,"text":"## Context\\nThe name `ashlar` [is already taken on PyPi](https:\/\/pypi.org\/project\/ashlar\/).\\nSince PyPi requires unique names for packages, this means that if we want to\\ndistribute our package on PyPi, we'll have to either:\\n1. Convince the owners of `ashlar` to give it to us\\n2. Name the PyPi package something similar to `ashlar` but slightly different,\\nlike `ashlar-core`\\n3. Come up with a new name for the project\\nOption 1 seems unlikely, given the maturity of the ashlar package on PyPi and\\nhow recent the last release was (April 2018, less than four months ago). Number\\n2 is perfectly functional but frustrating from a branding and distribution perspective,\\nsince it has the potential to introduce some confusion and\/or competition with\\nthe existing `ashlar` package.\\nInstead, I believe that the best course of action is to choose option 3 and rename the project.\\nThis will require us to come up with a new name for Ashlar, a [notoriously\\ndifficult decision](https:\/\/martinfowler.com\/bliki\/TwoHardThings.html).\\nSome options that I considered, all based on the idea of \"flexible\\nconstruction materials\":\\n- [Joist](https:\/\/en.wikipedia.org\/wiki\/Joist)\\n- [Lintel](https:\/\/en.wikipedia.org\/wiki\/Lintel)\\n- [Silicone](https:\/\/en.wikipedia.org\/wiki\/Silicone)\\n- [Grout](https:\/\/en.wikipedia.org\/wiki\/Grout)\\n\n\n##Decision\nI propose that we rename the project to **Grout**. Among the options above,\\n\"Grout\" is the name that sounds the best to me, and it's the one that I believe\\noffers the closest allegory for the project.\\nGrout is a construction material widely known for its physical flexibility and its\\npractical versatility: a fluid concrete used to create waterproof seals in\\nmasonry structures.\\nSome advantages of the name \"Grout\" include:\\n- \"Grout\" respects the origins of the project by referencing a masonry material,\\nbut unlike \"Ashlar\", the name \"Grout\" emphasizes the core features of the project\\n-- its flexibility and versatility as a base material\\nthat can scaffold and tie together much more complex projects.\\n- \"Grout\" is one syllable (one fewer than \"Ashlar\") and the easiest word to\\npronounce among the options I considered.\\n- Perhaps most importantly, `grout` is [available on\\nPyPi](https:\/\/pypi.org\/project\/grout).\\n","Predictions":"I propose that we rename the project to **Grout**. Among the options above,n\"Grout\" is the name that sounds the best to me, and it's the one that I believenoffers the closest allegory for the project.nGrout is Joist(https:\/\/en.wikipedia.org\/wiki\/Joist)nand Lintel(https:\/\/en.wikipedia.org\/wiki\/Lintel)n"}
{"File Name":"verify-onboarding-prototypes\/0008-provide-an-end-to-end-stub.md","Context":"## Context\nWe wish to run regular user research and testing against the prototype Service Provider.\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\nThe area we are most interested in is the interface between the Service and the Hub.\n","Decision":"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","tokens":55,"id":581,"text":"## Context\\nWe wish to run regular user research and testing against the prototype Service Provider.\\nTo support user research we need to provide a user journey that resembles a typical Verify journey.\\nThe area we are most interested in is the interface between the Service and the Hub.\\n\n\n##Decision\nWe will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.\\nIt will not provide a SAML implementation.\\nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.\\nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in their\\njourney.  Continuing from this page will take the user to a page containing a series of possible response scenarios that\\ncan be selected.\\nAny information that needs to be stored from the original request or between pages will be stored in a session\\ncookie. The session cookie does not need to be signed or encrypted initially.\\nSome scenarios will require additional user input such as a providing a pid value or user account creation attributes.\\nThese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should\\nsend its responses to.\\nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SP\\nprototype.\\nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.\\nThe prototype service provider will be able to understand each canned response and produce a specific response to the\\nService\/client.\\nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.\\nThe Stub Verify Hub will provide the following responses:\\n| Response Scenario     | Message                                                                                                                   |\\n| --                    | --                                                                                                                         |\\n| Successful Match      | scenario: SUCCESS_MATCH, levelOfAssurance: ${specified loa}, pid: ${specified PID}                                         |\\n| Account Creation      | scenario: ACCOUNT_CREATION, levelOfAssurance: ${specified loa}, pid: ${specified PID}, attributes: [${user attributes}]  |\\n| No Match              | scenario: NO_MATCH                                                                                                         |\\n| Cancellation          | scenario: CANCELLATION                                                                                                     |\\n| Authentication failed | scenario: AUTHENTICATION_FAILED                                                                                           |\\n| Requestor Error       | scenario: REQUEST_ERROR                                                                                                 |\\n| Internal Server Error | scenario: INTERNAL_SERVER_ERROR\\nInitially, the Stub Verify Hub will be deployed to Government PaaS.\\nA diagram of a potential stub architecture is available at: `prototypes\/prototype-0\/docs\/diagrams\/stub_service_architecture.png`\\n","Predictions":"We will create a Stub Verify Hub that will allow for end-to-end testing of the prototype.nIt will expect a form submission on a web resource that mimics the behaviour of receiving an AuthnRequest.nIf the form post is successful then a browser redirect will be issued to a page explaining where the user is in theirnjourney. Continue from this page will take the user to a page containing a series of possible response scenarios thatncan be selected.nAny information that needs to be stored from the original request or between pages will be stored in a session cookie. The session cookie does not need to be signed or encrypted initially.nSome scenarios will require additional user input such as a providing a pid value or user account creation attributesnthese will be provided on subsequent pages. In every case the user will need to provide the URL that the stub-hub should send its responses to.nEach response will mimic the shape of a Hub SAML Response form that can be submitted back to the Service and SPnprototype.nThe details of each canned response will be encoded as base64 encoded JSON in the SAMLResponse parameter of the form.nThe prototype service provider will be able to understand each canned response and produce a specific response to thenService\/client.nUsing these responses it will be possible able to test different scenarios in the client, Service, and browser.nThe Stub Verify Hub will provide the following responses:n| Response Scenario | Message |n| -- | --n| -- |n| Successful Match | scenario: SUCCESS_MATCH, levelOfAssurance: $specified loa, pid: $specified PID |n| Account Creation | scenario: ACCOUNT_CREATION, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOfFurthermore, levelOf"}
