{"File Name":"libelektra\/ingroup_removal.md","Context":"## Problem\\nCurrently, an error or warning message in elektra causes the following line to be shown:\\n```\\nIngroup: <group>\\n```\\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\\n","Decision":"The `ingroup` message will be removed as it does not yield any notable benefit.\\nSee [Error concept](error_codes.md)\\n","tokens":83,"id":1308,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWhen HapiJS plugins are registered they expect an object like this:\n```JavaScript\n{\nname: 'plugin-name',\nversion: '1.0.0',\nregister: (server, options) => {}\n}\n```\nThe problem with `export default` is that it exports an object whose property `default` is the value of the exported object. This makes my convention of having the `index.ts` register the plugin not work because HapiJS still uses CommonJs and won't get the `default` value E.g.\n```JavaScript\nimport plugin from '.\/plugin';\nexport default {\nplugin,\n}\n```\nSwitching between `module.exports` and `export default` will make using my packages tricky. It won't be clear when to use CommonJS or ES6 modules so I'll pick one and stick with it.\nSo let's use `module.exports` instead of `export default`.\n\n## Decision\nRejected because if I try and import a module that uses `module.exports` Typescript shows an error that the file `is not a module`. The implication of this is that I have to replace [Glue][hapi-glue] to compose the API.\n\n\n## Context\nThere no agreed-upon way of handling application configuration. There are two issues this ADR aims to address.\n### Issue 1: Nested keys\nIn `config.exs`, configuration keys can be nested by using a Keyword as value. Unfortunately, there is no built-in support for this nesting. For example:\n```\nApplication.fetch_env!(:rig, :required_key)\n```\nwill show a nice error, while\n```\nApplication.fetch_env!(:rig, :required_key)[:required_subkey]\n```\nwill simply return nil in case `:required_key` is present but `:required_subkey` is not.\n### Issue 2: Default values\nSome default values are defined where they are needed in the code, which is a problem once a key is used more than once. Also, defaults are no easily inspectable.\n\n## Decision\n* We prefer not to nest configuration keys; instead, we prefix them where it makes sense. For example, instead of `config :rig, :kafka, client_id: :rig_brod_client` we write `config :rig, kafka_client_id: :rig_brod_client`. This allows us to leverage the built-in methods (e.g., `Application.fetch_env!`), which produce sensible error messages in case required values are not defined.\n* We always set default values in config.exs (in turn this means that we prefer `Application.fetch_env!` over `Application.get_env`). This way, it is easy to reason about default values, and using a default value in more than one place is not an issue.\n\n\n\nProvide a decision given the context below:\n## Problem\nCurrently, an error or warning message in elektra causes the following line to be shown:\n```\nIngroup: <group>\n```\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\n<end_of_turn>\n<start_of_turn>model\nThe `ingroup` message will be removed as it does not yield any notable benefit.\nSee [Error concept](error_codes.md)\n<end_of_turn>\n"}
{"File Name":"knot-documentation\/ARC-003-amqp-exc-name.md","Context":"## Context\\nAs defined by CloudAMQP Blog, \"exchanges are message routing agents, defined by the virtual host within RabbitMQ. An exchange is responsible for routing the messages to different queues with the help of header attributes, bindings, and routing keys.\"\\nThe Exchange name needs to be defined taking into account the architecture defined for the system. Thus, there will be a client that communicates with BabelTower, which has the Connector as one of its services (in this case BabelTower is the Connector client) or the client communicates directly with the Connector.\\nThe goal is to define names that make it clear what this Exchange is used for.\\n","Decision":"* The communication between two components will be done with two queues, where one component is consumer in one and producer in another and the other component follows the opposite.\\n* The reference for choosing the queue name will be the client.\\n* The queue where BabelTower publishes messages will be called 'fogOut'.\\n* The queue where BabelTower consumes messages will be called 'fogIn'.\\n* The queue where Connector publishes messages will be called 'connOut'.\\n* The queue where Connector consumes messages will be called 'connIn'.\\n","tokens":131,"id":3339,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context and Problem Statement\nTo implement the EAI patterns we use a combination of a generic component\nwhich handles the communication with Kafka and a FaaS solution. The business logic of the EAI patterns (message splitting\/aggregation or transformation) is provided via functions which are hosted on the FaaS solution. The generic component communicates with Kafka and de\/serializes the messages. We need a means to wire the instances of the generic component with the functions. E.g. A user wants to insert a message splitter between two message-based components. To realize this an instance of the generic component in combination with a splitting FaaS function will be used. The generic component needs the address of the FaaS gateway and the function name (e.g. http:\/\/address:8080\/function\/msg-payload-splitter) to call the function. To provide the necessary information to instances of the generic component we considered the following techniques.\n## Decision Drivers <!-- optional -->\n* MUST be supported by the language\/technology which is used to implement the generic component\n* MUST be easy to integrate into MICO\n* SHOULD be a well known and proven solution\n\n## Decision\n* MUST be supported by the language\/technology which is used to implement the generic component\n* MUST be easy to integrate into MICO\n* SHOULD be a well known and proven solution\nChosen option: \"Environment variables\", because MICO already supports this and it is easy to implement in the generic component.\n\n\n## **Context**\nWhen the microservices need to interact with each other (e.g. to sync common data), the preferable approach is to adopt an event-driven architecture. This approach guarantees loosely coupled services which can be run and deployed in isolation.\nIn order to implement an events-driven architecture we need to use a message broker which should be responsible for sending the messages.\nIn AWS, two options can be considered:\n1. SNS (Simple Notification Service)\nSNS implements pub\/sub messaging which allow applications to send messages to multiple subscribers through push mechanisms.\n2. SNS (Simple Notification Service) + SQS (Simple Queue Service)\nSQS is a message queue service used by distributed applications to exchange messages through a polling model and can be used to decouple sending and receiving components. Using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event, and also persisted in an Amazon SQS queue for other applications to process at a later time.\n\n## Decision\n**SNS (Simple Notification Service) + SQS (Simple Queue Service)**\nBy coupling SQS with SNS, the subscriber can receive messages at \"their peace\". It allows subscribers to be offline, tolerant to network and host failures. Although SNS has got a 4-phase retry policy, the message can be lost if the consumer is not available. Instead if the subscriber uses a queue, we are able to achieve guaranteed delivery.\n\n\n\nProvide a decision given the context below:\n## Context\nAs defined by CloudAMQP Blog, \"exchanges are message routing agents, defined by the virtual host within RabbitMQ. An exchange is responsible for routing the messages to different queues with the help of header attributes, bindings, and routing keys.\"\nThe Exchange name needs to be defined taking into account the architecture defined for the system. Thus, there will be a client that communicates with BabelTower, which has the Connector as one of its services (in this case BabelTower is the Connector client) or the client communicates directly with the Connector.\nThe goal is to define names that make it clear what this Exchange is used for.\n<end_of_turn>\n<start_of_turn>model\n* The communication between two components will be done with two queues, where one component is consumer in one and producer in another and the other component follows the opposite.\n* The reference for choosing the queue name will be the client.\n* The queue where BabelTower publishes messages will be called 'fogOut'.\n* The queue where BabelTower consumes messages will be called 'fogIn'.\n* The queue where Connector publishes messages will be called 'connOut'.\n* The queue where Connector consumes messages will be called 'connIn'.\n<end_of_turn>\n"}
{"File Name":"decodeweb\/0003-store-configuration-into-local-storage.md","Context":"## Context\\nTo provide the required functionality for the pilot we need to store some\\nconfiguration in a persistent way while just operating within a browser\\nenvironment. This state could be written to a backend database, or we could\\njust store into localstorage.\\n","Decision":"We will store the user's configuration into localstorage protected by a PIN\\ncode.\\n### Proposed storage structure\\n```json\\n{\\n\"1234\": {\\n\"keypair\": {},\\n\"blindSignature\": {},\\n\"devices\": {\\n\"abc123\": {\\n\"longitude\": 12.2,\\n\"latitude\": 52.4,\\n\"exposure\": \"INDOOR\",\\n\"communities\": {\\n\"2a745dac-015a-4cde-a348-75bccc21502b\": {\\n\"authorizable_attribute_id\": \"6a1ba9b1-3f20-4e12-ae65-154c5eb7e00f\",\\n\"credential\": {},\\n\"blindproofCredential\": {}\\n}\\n}\\n}\\n}\\n}\\n}\\n```\\nPlease see `.\/localstorage.json` for a complete example including zenroom credentials.\\n","tokens":52,"id":2221,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nIn order to interact with a remote workspace, for example, at structurizr.com.\nThe remote or local workspace has to be (de-)serialized from or to JSON.\n\n## Decision\nIn order to perform these operations we choose\n[pydantic](https:\/\/pydantic-docs.helpmanual.io\/) which has a nice API, active\ncommunity, good data validation, helpful documentation, and good performance.\n\n\n## Context\nThe OpenFIDO project will mediate access to a workflow service that runs\nGridLabD jobs. The frontend React application will need access to these\nresources, but only if they have permission to access via an organization.\n\n## Decision\nCreate a Flask Rest service - since the other microservices in this suite of\nservices are also Flask based, keep this using the same kind of infrastructure\nso that common utilities can be shared (openfido-utils) and the maintenance will\nbe simplified.\nOrganize the database logic into a simplified CQRS-inspired style code\nstructure. Since we anticipate many conceptual resources, each resource will\nhave its own module:\n* app\/RESOURCE\/models.py - contains all models for RESOURCE.\n* app\/RESOURCE\/routes.py - contains all Rest routes specific to RESOURCE.\n* app\/RESOURCE\/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\n* app\/RESOURCE\/services.py - all db commands that modify database state.\n* app\/RESOURCE\/queries.py - all db queries to the database.\nAdditional libraries we anticipate using:\n* [marshmallow](https:\/\/marshmallow.readthedocs.io\/en\/stable\/) will be used since there are many rest api endpoints that will take nested bodies.\n* [alembic](https:\/\/alembic.sqlalchemy.org\/en\/latest\/) to manage database schema migrations.\nOther thoughts on design:\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\nUse the structure of the [openfido-workflow-service](https:\/\/github.com\/slacgismo\/openfido-workflow-service) project as a reference.\n\n\n\nProvide a decision given the context below:\n## Context\nTo provide the required functionality for the pilot we need to store some\nconfiguration in a persistent way while just operating within a browser\nenvironment. This state could be written to a backend database, or we could\njust store into localstorage.\n<end_of_turn>\n<start_of_turn>model\nWe will store the user's configuration into localstorage protected by a PIN\ncode.\n### Proposed storage structure\n```json\n{\n\"1234\": {\n\"keypair\": {},\n\"blindSignature\": {},\n\"devices\": {\n\"abc123\": {\n\"longitude\": 12.2,\n\"latitude\": 52.4,\n\"exposure\": \"INDOOR\",\n\"communities\": {\n\"2a745dac-015a-4cde-a348-75bccc21502b\": {\n\"authorizable_attribute_id\": \"6a1ba9b1-3f20-4e12-ae65-154c5eb7e00f\",\n\"credential\": {},\n\"blindproofCredential\": {}\n}\n}\n}\n}\n}\n}\n```\nPlease see `.\/localstorage.json` for a complete example including zenroom credentials.\n<end_of_turn>\n"}
{"File Name":"holochain-rust\/0006-splitting-agent-into-front-house-back-house-or-not.md","Context":"## Context\\nFor Holo, we need to have user agent's keys and source chain on the client machine and the rest of the Holochain agent (the DHT shard etc.) be held by HoloPorts.\\nIn February 2018, Arthur, Eric and Nico discussed this during a co-creative session in London and made the assumption to have the Holochain agent be split up into two pieces, called front-house and back-house (prior left and right hemisphere). The front-house was meant to entail the source chain and private key management as well as a ribosome to run the app and provide interfaces for the UI and in the non-Holo case for bridging - everything that is user\/agent facing. The back-house should have been the DHT shard, routing table, metrics, etc. Basically everything that is network facing.\\nWith this separation, the reasoning in February was to rewrite (only) the front-house in JS to be able to run this piece in a web browser, as needed for Holo.\\nEric and Nico continued to map out the specifics of how these two modules would interface. In that process it became apparent that dividing an agent in these two pieces and have them run on different remote machines has a huge problem:\\n* every network communication (including the world model which happens on an ongoing basis) that the back-house is doing has to be signed with the agent's keys\\n* the agent's keys are by definition part of the front-house\\n* **-> the back-house can't live up to its main accountability without communicating with the front-house and requesting a signature for every packet of communication that might be even triggered from the network\/DHT while the user is offline and keys not accessible**\\nFurther conversation including Arthur on May 17th 2018 makes it clear that thinking in terms of two different kinds of agency seems appropriate. We discussed separating the authoring or creative agency from the part that runs validations and holds DHT shards and world model information, and allows the later to proxy for the former, **with separate keys**.\\n","Decision":"We decide to **not** emulate a single agency (as represented by one key) across two remote devices. In other words: we decide to not try to implement distributed agents.\\nInstead, we solve the initial Holo front-end problem by establishing **two sub-agencies with distinct keys**, where the Holo client's authoring agency explicitly grants proxy rights to a HoloPort's DHT agency.\\nIn other words: the Holo user uses their local key to sign a statement that grants another agent (the HoloPort) to act on their behalf for all the cases needed to have the HoloPort carry the DHT weight for this agent. But technically, it is another agent with its own key.\\n","tokens":423,"id":1500,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n# Context\nThe way the Government Digital Service (GDS) [makes technology choices is\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which technology will to use to provide\npersistence for the Performance Platform.\nGDS has experience in running MongoDB and MySQL in production.\nWe envisage the Performance Platform as taking in unstructured data from a\nvariety of data sources (spreadsheets, analytics, logs, other databases and\napplications) and allowing people to collect this data in a single place. This\nshould enable service managers to:\n- make comparisons\n- see how well their service is performing\n- see how the performance changes over time, as they iterate the service\nSo we want a persistent data store that will store unstructured data, and\nallow us to apply a structure either by post-processing the data, or at query\ntime.\nThe volume of the data that we are envisaging at this stage is pretty small.\nWe will be building a small thing to start; as we learn more about the\nuser needs and problem space, then we will revisit this decision. Since the\nvolume is small, it does not seem likely that we need Hadoop \/ HDFS or\nCassandra.\nWe are not the canonical source of this data. We are an aggregator; the\ncanonical source remains the data sources which will be providing feeds or\npushing the data into the Performance Platform.\nBecause of this position, we do not need ACID properties for this data, nor\nneed worry about the CAP theorem in any detail.\n# Decision\nWe will use MongoDB. We are comfortable operating it in production,\nit will allow unstructured data (in the form of JSON documents) and we can\napply structure at query time.\n# Status\nAccepted.\n# Consequences\nUse MongoDB with an appropriate replica-set configuration.\n\n## Decision\nWe will use MongoDB. We are comfortable operating it in production,\nit will allow unstructured data (in the form of JSON documents) and we can\napply structure at query time.\n# Status\nAccepted.\n# Consequences\nUse MongoDB with an appropriate replica-set configuration.\n\n\n## Context and Problem Statement\nWithin contact-frontend, for the standalone pages, two routes into the page exist. The first requires a tax service\nuser to be logged in, and redirects to login if the service user is not logged in. The second, on a different URL\nsuffixed with \"-unauthenticated\", does not require login, but serves the same page. After discussion, the PlatUI team\ndecided that the requirement for a user to be logged in was making a worse experience for the service end user, adding\nthe requirement of login,when all the same functionality is also available without logging in via a different URL.\n## Decision Drivers\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\navailable without login\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\npersisted to Deskpro agents\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\nthe codebase, making our development process slower and our testing time longer\n\n## Decision\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\navailable without login\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\npersisted to Deskpro agents\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\nthe codebase, making our development process slower and our testing time longer\nChosen option: \"Remove login redirect from all pages and keep existing endpoints for now\", because:\n* The benefits to end users in terms of accessibility feel significant enough to warrant the change\n* Information about logged in users will still be persisted to Deskpro but there will not be a redirect for users\nwho have been logged out in the background\n* Codebase is significantly simplified for developers working on contact-frontend\n* However, deleting the \"-unauthenticated\" routes will be a major breaking change across the Platform, and should be\ndecoupled from this\n### Positive Consequences\n* Capturing information about all logged in users, not just the ones who have clicked on a contact-frontend route not\nending in \"-unauthenticated\"\n* More accessible experience for tax platform end users who are not forced to log in to report technical problems\n* Significant simplification of codebase for developers\n### Negative Consequences\n* There is a risk that without forcing users to log in to particular implementations of the contact-frontend forms,\nthere will be less information sent to DCST in certain cases. On the other hand, for certain users, the login\nrequirement might lock them out of the contact-frontend forms completely, e.g. if they are trying to report a problem\nwith login\n\n\n\nProvide a decision given the context below:\n## Context\nFor Holo, we need to have user agent's keys and source chain on the client machine and the rest of the Holochain agent (the DHT shard etc.) be held by HoloPorts.\nIn February 2018, Arthur, Eric and Nico discussed this during a co-creative session in London and made the assumption to have the Holochain agent be split up into two pieces, called front-house and back-house (prior left and right hemisphere). The front-house was meant to entail the source chain and private key management as well as a ribosome to run the app and provide interfaces for the UI and in the non-Holo case for bridging - everything that is user\/agent facing. The back-house should have been the DHT shard, routing table, metrics, etc. Basically everything that is network facing.\nWith this separation, the reasoning in February was to rewrite (only) the front-house in JS to be able to run this piece in a web browser, as needed for Holo.\nEric and Nico continued to map out the specifics of how these two modules would interface. In that process it became apparent that dividing an agent in these two pieces and have them run on different remote machines has a huge problem:\n* every network communication (including the world model which happens on an ongoing basis) that the back-house is doing has to be signed with the agent's keys\n* the agent's keys are by definition part of the front-house\n* **-> the back-house can't live up to its main accountability without communicating with the front-house and requesting a signature for every packet of communication that might be even triggered from the network\/DHT while the user is offline and keys not accessible**\nFurther conversation including Arthur on May 17th 2018 makes it clear that thinking in terms of two different kinds of agency seems appropriate. We discussed separating the authoring or creative agency from the part that runs validations and holds DHT shards and world model information, and allows the later to proxy for the former, **with separate keys**.\n<end_of_turn>\n<start_of_turn>model\nWe decide to **not** emulate a single agency (as represented by one key) across two remote devices. In other words: we decide to not try to implement distributed agents.\nInstead, we solve the initial Holo front-end problem by establishing **two sub-agencies with distinct keys**, where the Holo client's authoring agency explicitly grants proxy rights to a HoloPort's DHT agency.\nIn other words: the Holo user uses their local key to sign a statement that grants another agent (the HoloPort) to act on their behalf for all the cases needed to have the HoloPort carry the DHT weight for this agent. But technically, it is another agent with its own key.\n<end_of_turn>\n"}
{"File Name":"raster-foundry\/adr-0006-workflow-manager.md","Context":"## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n","Decision":"Raster Foundry will use Airflow as a task manager. There are a number of advantages over some of the\\nalternatives. First, Airflow has a large, active, user base that have used it in production. The\\nproject itself is in the Apache incubator, providing a strong signal that the project is of high quality\\nSecond, Airflow's UI for monitoring task and workflow progress is great. It provides\\nhigh-level relevant information that will enable us to diagnose issues quickly. Additionally, it\\nprovides a means to view log output of tasks directly in the admin interface. Third, Airflow\\nsupports both scheduled and ad hoc tasks. Lastly, Airflow's architecture would re-use many\\ncomponents that will already be a part of Raster Foundry's infrastructure - including a Postgres\\ndatabase and a redis cache.\\n","tokens":353,"id":1688,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nI wanted to create a website blog using [Markdown][1], a [static site generator][2] and [HapiJS][3] to serve the files. Static site generators I tried, in order, were -\n### [11ty][4]\nPros\n- Easy setup\n- Predictable static file output\n- Relatively small codebase\n- Supporst different template engines\nCons\n- The frontmatter parser [didn't work as I expected][7]\n- Template errors were not reported and would only fail on build\n- Needs some attention to make production ready e.g. SEO, images, favicons etc\n### [Gatsby][5]\nPros\n- Modern [JAMstack][8] project with React\n- Batteries included by default - includes helpers for favicons, image loading, SEO etc.\n- Opportunity to use GraphQL\nCons\n- Doesn't output static files - [totally depends on clientside JavaScript][9]\n- It relies on service workers to cache responses which causes flakey e2e tests and unpredictable behaviour\n- Developing static files based on API was difficult because of caching and unexpected error handling\n- Relatively complex project with separate build, develop steps, config, plugins etc\n### [Hugo][6]\nPros\n- Fast and mature static site generator\n- Small footprint\nCons\n- Written in Go - I prefer fullstack JavaScript for this site\n\n## Decision\nIn the context of using several static site generators each of them had with their own downsides. And facing the concern of having blog that I can easily maintain and customise I decided to build my blog with HapiJS to achieve a smaller blog footprint, predictable behaviour and an opportunity to learn something new. I accept that I'm re-inventing the wheel.\n\n\n## Context and Problem Statement\nThe FxA back-end service stack contains an application architecture that is ad-hoc, not documented, and missing modern features (such as Dependency Injection) which results in the following problems:\n- New developers struggle to get up to speed as they must learn the architecture by reading the code as we have no documentation on the application structure, why they're structured the way they are, or how new components should be added to fit in. Each back-end service may vary in its ad-hoc architecture as well.\n- Adding new objects needed in a route handler can be time-consuming as the object must be plumbed through the entire initialization chain vs. more elegant methods like Dependency Injection (DI).\n- Not clear where\/how to add new components and takes time to study\/understand how things are currently setup in an attempt to mimic the structure for the new component.\n- Time consuming to setup boiler-plate for components, as we have no tooling to work with the current ad-hoc application architectures.\n- Our ad-hoc architecture frequently mixes concerns such as having business logic mixed in with request handling logic, and has other warts from its evolution over time vs. being planned up front.\n- New back-end services evolve differently resulting in more ad-hoc application architectures to learn.\n- Shared components in `fxa-shared` can't rely on basic object lifecycles or setup approaches as they may be used in multiple different ad-hoc application architectures.\nNot choosing an application framework means that we have choosen to make ad-hoc application architectures which will continue to exhibit the problems above.\nIt is assumed that the four newest FxA back-end services (admin-server, support-panel, event-broker, gql-api) will be switched to the chosen approach for consistency.\n## Decision Drivers\n- Documented application architecture.\n- Tooling that reduces boiler-plate and creates consistent code architecture.\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\n- Training materials so that new developers can understand and work on application features easily.\n- Ability to migrate legacy applications to similar conventions\/setup.\n- Handles situations we need such as:\n- Exception handling\n- Validation\n- Service instantiation (DI, etc)\n- Authentication\n- RESTful API's (Swagger\/OpenAPI a bonus)\n- GraphQL\n- Use of components\/services easily in a script\n\n## Decision\n- Documented application architecture.\n- Tooling that reduces boiler-plate and creates consistent code architecture.\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\n- Training materials so that new developers can understand and work on application features easily.\n- Ability to migrate legacy applications to similar conventions\/setup.\n- Handles situations we need such as:\n- Exception handling\n- Validation\n- Service instantiation (DI, etc)\n- Authentication\n- RESTful API's (Swagger\/OpenAPI a bonus)\n- GraphQL\n- Use of components\/services easily in a script\nChosen Option: Use an existing framework: NestJS\nNestJS and LoopBack are the two most compelling options, however NestJS has substantially better GraphQL support and a much larger user-base. LoopBack has its user-base divided between two versions with substantial changes between them. We will use NestJS for new projects and update the newest services (admin-server, support-panel, event-broker, gql-api) to NestJS.\nTo reduce documentation needs and address that auth-server will not be reasonable to migrate, we will backport code organization schemes and directory\/filename conventions from NestJS and use `typedi` as a DI system to ease configuration\/testing in auth-server and possibly other legacy packages. This will allow developers familiar with NestJS projects to still locate and easily work on FxA legacy packages.\n### Positive Consequences\n- Less documentation to create, as we only document differences for legacy services.\n- NestJS GraphQL support matches existing typegraphql paradigm closely for minimal effort switch.\n### Negative Consequences\n- There will be some effort involved to migrate the 4 packages to NestJS.\n- We will need to document the differences between NestJS and other legacy services.\n\n\n\nProvide a decision given the context below:\n## Context\nThis ADR has been superseded by `ADR-0019`.\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\ntools it seemed prudent to explore additional options as well.\n| Workflow Tool   | Pros | Cons |\n|-----------------|------|------|\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\n<end_of_turn>\n<start_of_turn>model\nRaster Foundry will use Airflow as a task manager. There are a number of advantages over some of the\nalternatives. First, Airflow has a large, active, user base that have used it in production. The\nproject itself is in the Apache incubator, providing a strong signal that the project is of high quality\nSecond, Airflow's UI for monitoring task and workflow progress is great. It provides\nhigh-level relevant information that will enable us to diagnose issues quickly. Additionally, it\nprovides a means to view log output of tasks directly in the admin interface. Third, Airflow\nsupports both scheduled and ad hoc tasks. Lastly, Airflow's architecture would re-use many\ncomponents that will already be a part of Raster Foundry's infrastructure - including a Postgres\ndatabase and a redis cache.\n<end_of_turn>\n"}
{"File Name":"occurrent\/0001-mongodb-database-structure.md","Context":"## Context\\nWe need to record events in MongoDB in a specific structure\/schema.\\n","Decision":"The [CloudEvents](https:\/\/cloudevents.io\/) are persisted like this in the \"events collection\" in the database (1):\\n```json\\n{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"dataschema\" : \"http:\/\/someschema.com\/schema.json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n},\\n\"streamid\" : \"streamid\"\\n}\\n```\\nNote that \"streamid\" is added as an extension by the MongoDB event stores in order to read all events for a particular stream.\\nIf stream consistency is enabled then another collection, the \"stream consistency\" collection is also written to the database (2):\\n```json\\n{\\n\"_id\" : \"streamid\",\\n\"version\" : 1\\n}\\n```\\nWhen appending cloud events to the stream the consistency of the stream is maintained by comparing the version supplied by the user\\nwith the version present in (2). If they don't match then the cloud events are not written. Also if there are two threads writing to the same\\nstream at once then one of them will run into an error which means it has to retry (optimistic locking). For this to work, transactions are required!\\nAnother previous approach was instead to store the events like this:\\n```json\\n{\\n\"_id\": \"streamid\",\\n\"version\" : 1,\\n\"events\": [{\\n\"specversion\": \"1.0\",\\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\\n\"source\": \"http:\/\/name\",\\n\"type\": \"org.occurrent.domain.NameDefined\",\\n\"datacontenttype\": \"application\/json\",\\n\"subject\": \"name1\",\\n\"time\": \"2020-07-10T14:48:23.272Z\",\\n\"data\": {\\n\"timestamp\": 1594392503272,\\n\"name\": \"name1\"\\n}\\n}]\\n}\\n```\\nI.e. the events were stored inside a single document. While there are several benefits of using this approach, such as:\\n1. No transactions required, just do;\\n```java\\neventCollection.updateOne(and(eq(\"_id\", streamId), eq(\"version\", expectedStreamVersion)),\\ncombine(pushEach(\"events\", serializedEvents), set(\"version\", expectedStreamVersion + 1)),\\nnew UpdateOptions().upsert(true));\\n```\\n1. Reads could be done in a streaming fashion even though the events were stored as a subarray using aggregations\\n1. Subscriptions could take a `List<CloudEvent>`, i.e. all events written in the same transaction to the event store.\\nWhen not using the approach subscriptions gets notified once for each event and the consumer needs to reassemble\\nthe \"transaction\" somehow. This is a major drawback when not using this approach.\\nThere are however two major drawbacks that lead to not using this approach:\\n1. There's 16Mb document size limit in MongoDB so this approach wouldn't work for large streams\\n1. It's much hard to implement queries\/filters for subscriptions. The aggregation support is\\n[limited](https:\/\/stackoverflow.com\/questions\/62846085\/remove-element-from-subarray-using-an-aggregation-stage-applied-to-a-change-stre)\\nwhen working with subscriptions preventing simple filters (it would have been much simpler if `unwind`\\nwas supported since then we could flatten out the `events` subarray before applying the queries, i.e. something like\\n`(unwind(\"$events\"), replaceRoot(\"$events\"), match(filter.apply(\"type\", item))`).\\nAnother problem with subscriptions is the format, when a document is _created_ the content is specified\\nin the `fullDocument` property but it's a different property when the document is updated. Thus a filter\/query\\nwould not need to consider both these cases which is very difficult. With the new approach a query\/filter is much\\neasier since we only need to care about inserts.\\n","tokens":17,"id":4385,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe need to choose a frontend framework.\n\n## Decision\nWe don't have any specific requirements yet, so use Bootstrap because it is the most popular framework, and has a lot of documentation.\n\n\n## Context and Problem Statement\nWe want to present LaTeX examples.\nThey should be shown both as LaTeX source and as rendered output.\n\n## Decision\n* Write once - have two outputs (source and rendered)\n* Good interaction with all pacakges\n* Should support floats\n* Should support compilation without special configuration (such as `--shell-escape`)\nChosen option: \"custom solution\", because resolves all forces.\n### Positive Consequences\n* Works with floats and umlauts\n### Negative Consequences\n* A little bit more than minimal code is presented to the user\n\n\n\nProvide a decision given the context below:\n## Context\nWe need to record events in MongoDB in a specific structure\/schema.\n<end_of_turn>\n<start_of_turn>model\nThe [CloudEvents](https:\/\/cloudevents.io\/) are persisted like this in the \"events collection\" in the database (1):\n```json\n{\n\"specversion\": \"1.0\",\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\n\"source\": \"http:\/\/name\",\n\"type\": \"org.occurrent.domain.NameDefined\",\n\"datacontenttype\": \"application\/json\",\n\"dataschema\" : \"http:\/\/someschema.com\/schema.json\",\n\"subject\": \"name1\",\n\"time\": \"2020-07-10T14:48:23.272Z\",\n\"data\": {\n\"timestamp\": 1594392503272,\n\"name\": \"name1\"\n},\n\"streamid\" : \"streamid\"\n}\n```\nNote that \"streamid\" is added as an extension by the MongoDB event stores in order to read all events for a particular stream.\nIf stream consistency is enabled then another collection, the \"stream consistency\" collection is also written to the database (2):\n```json\n{\n\"_id\" : \"streamid\",\n\"version\" : 1\n}\n```\nWhen appending cloud events to the stream the consistency of the stream is maintained by comparing the version supplied by the user\nwith the version present in (2). If they don't match then the cloud events are not written. Also if there are two threads writing to the same\nstream at once then one of them will run into an error which means it has to retry (optimistic locking). For this to work, transactions are required!\nAnother previous approach was instead to store the events like this:\n```json\n{\n\"_id\": \"streamid\",\n\"version\" : 1,\n\"events\": [{\n\"specversion\": \"1.0\",\n\"id\": \"86282094-5344-4309-932a-129a7774735e\",\n\"source\": \"http:\/\/name\",\n\"type\": \"org.occurrent.domain.NameDefined\",\n\"datacontenttype\": \"application\/json\",\n\"subject\": \"name1\",\n\"time\": \"2020-07-10T14:48:23.272Z\",\n\"data\": {\n\"timestamp\": 1594392503272,\n\"name\": \"name1\"\n}\n}]\n}\n```\nI.e. the events were stored inside a single document. While there are several benefits of using this approach, such as:\n1. No transactions required, just do;\n```java\neventCollection.updateOne(and(eq(\"_id\", streamId), eq(\"version\", expectedStreamVersion)),\ncombine(pushEach(\"events\", serializedEvents), set(\"version\", expectedStreamVersion + 1)),\nnew UpdateOptions().upsert(true));\n```\n1. Reads could be done in a streaming fashion even though the events were stored as a subarray using aggregations\n1. Subscriptions could take a `List<CloudEvent>`, i.e. all events written in the same transaction to the event store.\nWhen not using the approach subscriptions gets notified once for each event and the consumer needs to reassemble\nthe \"transaction\" somehow. This is a major drawback when not using this approach.\nThere are however two major drawbacks that lead to not using this approach:\n1. There's 16Mb document size limit in MongoDB so this approach wouldn't work for large streams\n1. It's much hard to implement queries\/filters for subscriptions. The aggregation support is\n[limited](https:\/\/stackoverflow.com\/questions\/62846085\/remove-element-from-subarray-using-an-aggregation-stage-applied-to-a-change-stre)\nwhen working with subscriptions preventing simple filters (it would have been much simpler if `unwind`\nwas supported since then we could flatten out the `events` subarray before applying the queries, i.e. something like\n`(unwind(\"$events\"), replaceRoot(\"$events\"), match(filter.apply(\"type\", item))`).\nAnother problem with subscriptions is the format, when a document is _created_ the content is specified\nin the `fullDocument` property but it's a different property when the document is updated. Thus a filter\/query\nwould not need to consider both these cases which is very difficult. With the new approach a query\/filter is much\neasier since we only need to care about inserts.\n<end_of_turn>\n"}
{"File Name":"adr-demo\/0002-use-microservices-architecture.md","Context":"## Context\\nThe web application we want to develop and offer to our users is a picture service that allows :\\n* Uploading images and gifs with a title and tags\\n* Searching images by title or tag\\n* Downloading an image in various file formats and sizes\\nAs we want to quicly build a first version to be able to quickly go online and answer competitors, we choose\\nto deploy our application to Google Cloud (as we do not have any bare metal servers).\\nThe application should meet the following criteria:\\n* it should be quick enough so the search answers in less than 500ms\\n* it should be scalable so we can meet future usage\\n","Decision":"In order to meet required criteria we choose to develop our application using a microservices approach :\\neach business need will be addressed by a specific microservice, as described in the following schema:\\n![](microservices-architecture.png)\\nEach microservice should take a simple input and produce a simple output, each microservice should act as a black box\\nregarding other microservices, meaning state should not be shared across different instances of same or differents microservices.\\n","tokens":137,"id":2516,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context and Problem Statement\nWe have to define the initial overall architecture of the project.\nFor now, we are sure that we want to provide these features:\n- Local preview web UI\n- Static Site Generation from the CI\/CD\n- CLI to create a new ADR quickly\nIn the future, we might want to provide these features:\n- Create\/edit ADRs from the local web UI\n- VSCode extension to create and maybe edit an ADR from the IDE\n- Support ADR aggregation from multiple repositories\n\n## Decision\nChosen option: \"Multi-packages, monorepo, with Yarn and Lerna\", because\n- We don't want a monolith because we want the core library\/API to be very well tested and probably developed with DDD and hexagonal architecture. The other packages will just call this core API, they will contain fewer business rules as possible. As we are not so sure about the features we will provide in the future, this is good for extensibility.\n- Yarn + Lerna seems to be a very good practice used by a lot of other open-source projects to publish npm packages.\n\n\n## Context\nAn integration test framework (and possibly a combo unit\/integration test) has to be chosen for this project.\nAfter reviewing what's the state of testing with JS in 2020, here's a list of contenders.\n### Selenium\nHas been the standard of the industry for many years. Start to struggle with single page application framework (SPA) like Angular, Vue.js or React.js.\n- Desktop browsers, emulators \/ simulators and real devices (E2E testing all the way)\n- Highly scalable\n- Has many \"sister frameworks\" that enhance the feature set (like Appium for mobile testing, Nightwach or WebdriverIO)\nWhile very efficient for E2E testing, this framework would be cumbersome to have in development on a local machine.\nCould be used for E2E testing for cross browser compatibility tests.\n### TestCafe\nGood alternative to Selenium, is compatible with Saucelab (we already have some account with them).\n- Fast to set up\n- Cross Browser and Devices (with SauceLabs or BrowserStack)\nThis could also be a good cross browser testing framework, but the cross browser feature will be hard to have deployed on every developers' machine.\n### Cypress\nMade by devs for devs.\n- Very convenient running and debugging tools\n- Native access to all your application\u2019s variables\n- Solid and clear documentation\n- Newest addition (2017) to the big frameworks (very active community)\n\n## Decision\nAfter playing a bit with these frameworks, Cypress clearly had something for it.\nThe documentation is well maintained and complete, tests are described with Mocha which is already known by many in the team (it's what we used to write our tests on the older viewer)\nCypress will be a great help during the development phase of the new viewer.\nBut we have to keep in mind that it doesn't really do cross browser testing.\n\n\n\nProvide a decision given the context below:\n## Context\nThe web application we want to develop and offer to our users is a picture service that allows :\n* Uploading images and gifs with a title and tags\n* Searching images by title or tag\n* Downloading an image in various file formats and sizes\nAs we want to quicly build a first version to be able to quickly go online and answer competitors, we choose\nto deploy our application to Google Cloud (as we do not have any bare metal servers).\nThe application should meet the following criteria:\n* it should be quick enough so the search answers in less than 500ms\n* it should be scalable so we can meet future usage\n<end_of_turn>\n<start_of_turn>model\nIn order to meet required criteria we choose to develop our application using a microservices approach :\neach business need will be addressed by a specific microservice, as described in the following schema:\n![](microservices-architecture.png)\nEach microservice should take a simple input and produce a simple output, each microservice should act as a black box\nregarding other microservices, meaning state should not be shared across different instances of same or differents microservices.\n<end_of_turn>\n"}
{"File Name":"stamper\/0004-bootsptrap-as-frontend-framework.md","Context":"## Context\\nWe need to choose a frontend framework.\\n","Decision":"We don't have any specific requirements yet, so use Bootstrap because it is the most popular framework, and has a lot of documentation.\\n","tokens":12,"id":1240,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe need to choose webserver.\n\n## Decision\nWe will use Flask. Server should be simple - pretty much just with a GraphQL\nendpoint and GraphiQL.\n\n\n## Context\nWe need to choose a web framework for the app.\n\n## Decision\nWe use Spring Boot because it allows us starting fast, and concentrating on business logic of the app, rather than working on infrastructure tasks.\n\n\n\nProvide a decision given the context below:\n## Context\nWe need to choose a frontend framework.\n<end_of_turn>\n<start_of_turn>model\nWe don't have any specific requirements yet, so use Bootstrap because it is the most popular framework, and has a lot of documentation.\n<end_of_turn>\n"}
{"File Name":"tdr-dev-documentation\/0018-govkuk-notify-staging-configuration.md","Context":"## Context\\nIt was decided to set the GovUK Notify Staging service to go live: [0015 GovUK Notify Staging Configuration](0015-govuuk-notify-staging-configuration.md)\\n","Decision":"The GovUK Notify team recommended that usually they do not go live with non-production services.\\nIt was decided to not go live with the GovUK Notify Staging services following this advice from the GovUK Notify team.\\nIt is still possible to send emails to users participating with UAT via adding their email addresses to the service's guest list.\\nThe Staging service is sufficiently consistent with the Production service if it were to remain in \"trial mode\".\\nFor full details of the \"trial mode\" for GovUK Notify services see here: https:\/\/www.notifications.service.gov.uk\/using-notify\/trial-mode\\n","tokens":44,"id":1767,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\n\n## Decision\nIt was decided to pass the entire Keycloak message into the GovUK Notify template via the `keycloakMessage` personalisation parameter, rather than having a mix of text and parameters in the Keycloak template.\nThis is because the Keycloak message is passed in as a complete constructed string. It would be very difficult to extract particular portions of this string, for example a link for use within the GovUK Notify template.\nThis approach does mean it is harder for non-developers to make changes to the email messages. The decision may be revisited in the future if this proves to be causing issues.\n\n\n## Context\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\nGovUK services have a \"trial mode\", and a \"live mode\".\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\n\n## Decision\nThe \"staging\" GovUK Notify service is to be live to allow for user testing on this environment.\nAlso, it will mirror the TDR production environment as closely as possible.\n\n\n\nProvide a decision given the context below:\n## Context\nIt was decided to set the GovUK Notify Staging service to go live: [0015 GovUK Notify Staging Configuration](0015-govuuk-notify-staging-configuration.md)\n<end_of_turn>\n<start_of_turn>model\nThe GovUK Notify team recommended that usually they do not go live with non-production services.\nIt was decided to not go live with the GovUK Notify Staging services following this advice from the GovUK Notify team.\nIt is still possible to send emails to users participating with UAT via adding their email addresses to the service's guest list.\nThe Staging service is sufficiently consistent with the Production service if it were to remain in \"trial mode\".\nFor full details of the \"trial mode\" for GovUK Notify services see here: https:\/\/www.notifications.service.gov.uk\/using-notify\/trial-mode\n<end_of_turn>\n"}
{"File Name":"operational-data-hub\/0023-iso-8601-to-specify-date-and-time-with-timezone.md","Context":"## Context\\nJSON does not specify how a date(time) string should be formatted. The ISO 8601 standard is widely used within the JSON community to specify date-time objects. [RFC 3339]([https:\/\/tools.ietf.org\/html\/rfc3339) describes the usage of the ISO-8601 standard.\\n","Decision":"We will use the ISO-8601 (latest version) standard (as described in RFC-3339) for formatting date(time) objects whenever a date(time) object is serialized. This applies (but is not limited) to JSON messages, logging, data-store\/firestore timestamps.\\nAll date objects must have a time-zone included.\\n","tokens":66,"id":2755,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.\nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse\/Panoptes-Front-End#4992] (https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/issues\/4992)).\nThe default behavior in existing libraries like [JSON8](https:\/\/github.com\/sonnyp\/JSON8\/tree\/master\/packages\/json8#ooserialize) or MobX's [`toJS`](https:\/\/mobx.js.org\/refguide\/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object. However, for us, we are using maps for workflow steps because a key requirement is the ordering of key-value pairs, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.\n\n## Decision\nWe decided to instead implement our own [type conversion utility function](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/store\/utils\/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:\n``` js\nconst workflow = {\nid: '1',\nsteps: [['S1', { taskKeys: ['T1', 'T2'] }], ['S2', { taskKeys: ['T3'] }]] \/\/ How they will be stored on Panoptes\n}\n```\nAnd when a workflow request is received by the classifier store, it is converted by Mobx-State-Tree into an observable map when added to the store.\n**A note about the use of arrays for the key-value pairs**\nSubject locations are an array of objects. It would make sense to do an array of objects here too, however the array of two values is closest to the format expected by maps when you instantiate them: https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Map\nSo there's less type conversion happening if we store it this way. MobX will take objects too when setting an observable map, but if we ever want to instantiate an ES6 map independent of the store we would have to do another conversion from object to array of the key-value pair.\n\n\n## Context\nA Uniform Resource Identifier (URI) is a string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules, but also maintain extensibility through a separately defined hierarchical naming scheme (e.g. http:\/\/).\nTo be able to unambiguously identify any element of data, a URI will be defined for every element of data on the ODH platform. Building this URI from meaningful attributes instead of technical randomly generated ids makes it recognizable and easier to use. However, selecting the right attributes and context information to assure uniqueness is challenging.\nThe URI scheme to use is [tag](https:\/\/tools.ietf.org\/html\/rfc4151). Tag URIs are quite human readable, but, unlike http URIs, not resolvable. This makes them suitable for identifying real-life things (entities) that have a representation on the ODH. A URI can be defined from the hierarchy of the organisation, forming a path to the resource. For example, if mycompany has a catalog with articles and one of the articles has article# 313, an article URI could be defined as `tag:vwt.digital,2020:mycompany\/catalog\/articles\/313`\n\n## Decision\nWe define a URI according to the [tag scheme](https:\/\/tools.ietf.org\/html\/rfc4151) for each entity that has a representation on the ODH platform.\n\n\n\nProvide a decision given the context below:\n## Context\nJSON does not specify how a date(time) string should be formatted. The ISO 8601 standard is widely used within the JSON community to specify date-time objects. [RFC 3339]([https:\/\/tools.ietf.org\/html\/rfc3339) describes the usage of the ISO-8601 standard.\n<end_of_turn>\n<start_of_turn>model\nWe will use the ISO-8601 (latest version) standard (as described in RFC-3339) for formatting date(time) objects whenever a date(time) object is serialized. This applies (but is not limited) to JSON messages, logging, data-store\/firestore timestamps.\nAll date objects must have a time-zone included.\n<end_of_turn>\n"}
{"File Name":"lcarsde\/display-server-library.md","Context":"## Context and Problem Statement\\nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input\/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.\\n","Decision":"Chosen option: \"Xlib\", because it was used already for the additional tooling and there's lots of documentations and examples.\\n### Positive Consequences <!-- optional -->\\n* Direct compatibility with additional tool libraries, e.g. for XPM handling\\n* Lots of resources and with Openbox a very complete WM (in sense of ICCCM and EWMH) as possible template\\n### Negative Consequences <!-- optional -->\\n* Refactoring code\\n* Wayland might be more modern\\n* XCB is faster at times due to asynchronous\\n","tokens":66,"id":142,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nThe decision records must be stored in a plain text format:\n* This works well with version control systems.\n* It allows the tool to modify the status of records and insert\nhyperlinks when one decision supercedes another.\n* Decisions can be read in the terminal, IDE, version control\nbrowser, etc.\nPeople will want to use some formatting: lists, code examples,\nand so on.\nPeople will want to view the decision records in a more readable\nformat than plain text, and maybe print them out.\n\n## Decision\nbrowser, etc.\nPeople will want to use some formatting: lists, code examples,\nand so on.\nPeople will want to view the decision records in a more readable\nformat than plain text, and maybe print them out.\nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\nDecisions will be formatted nicely and hyperlinked by the\nbrowsers of project hosting sites like GitHub and Bitbucket.\nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convert\nthe decision records into HTML or PDF.\n\n\n## Context\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\n- MovementX\/Y is a clean browser only method for determining distance without having to track previous coordinates.\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\n\n## Decision\nAll pointer event interactions have a mouse event fallback.\n\n\n\nProvide a decision given the context below:\n## Context and Problem Statement\nThe window manager needs to use some kind of display management system to get notified about opening and closing windows and generally handle functions on the display and input\/output. XCB was used initially but some additional functionality was needed, which is covered by Xlib libraries. Mixing the two systems became a hazard.\n<end_of_turn>\n<start_of_turn>model\nChosen option: \"Xlib\", because it was used already for the additional tooling and there's lots of documentations and examples.\n### Positive Consequences <!-- optional -->\n* Direct compatibility with additional tool libraries, e.g. for XPM handling\n* Lots of resources and with Openbox a very complete WM (in sense of ICCCM and EWMH) as possible template\n### Negative Consequences <!-- optional -->\n* Refactoring code\n* Wayland might be more modern\n* XCB is faster at times due to asynchronous\n<end_of_turn>\n"}
{"File Name":"fixcity\/0003-sys-use-modular-monolith.md","Context":"## Context\\n- Constraints\\n- Greenfield project\\n- Quality attributes\\n- Expected fast load\\n","Decision":"We will not separate components in to separate deployment units,\\nwe will use modular monolith approach with single database.\\n","tokens":20,"id":1797,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context and Problem Statement\nWe want to use a component to graphically model nodes and edges in a web application.\n## Decision Drivers <!-- optional -->\n* usable with Angular\n* number of dependecies\n* features\n+ create Nodes\n+ create Connections\n* usability\n\n## Decision\n* usable with Angular\n* number of dependecies\n* features\n+ create Nodes\n+ create Connections\n* usability\nChosen option: Choosen a Combination of ngx-graph and plain d3.js, because ngx-graph might miss some usefull features but has good usability which is worth to have a look at\n### Positive Consequences <!-- optional -->\n* easy to use graph modeler\n* only a few dependencies\n* full control over features and behavior\n### Negative consequences <!-- optional -->\n* needs to be implemented\n\n\n## Context and Problem Statement\nPeople want to include time taken for voice overs to estimate the time of the rough cut.\n## Decision Drivers <!-- optional -->\n* Ease of implementation\n* Accuracy of time estimated\n* Extensibility\n\n## Decision\n* Ease of implementation\n* Accuracy of time estimated\n* Extensibility\nChosen option: 1, because it's the simplest implementation and a standardised way of calculating voice over estimations within the company.\n### Positive Consequences <!-- optional -->\n* A fairly easy implementation.\n### Negative consequences <!-- optional -->\n* Less accurate than other options.\n* Would require us to change the icon and description of the Voice over, such that it's features are not misunderstood. (This was a point raised in [UX investigations: how to better signpost Voice over\/link](https:\/\/github.com\/bbc\/digital-paper-edit-client\/issues\/63))\n\n\n\nProvide a decision given the context below:\n## Context\n- Constraints\n- Greenfield project\n- Quality attributes\n- Expected fast load\n<end_of_turn>\n<start_of_turn>model\nWe will not separate components in to separate deployment units,\nwe will use modular monolith approach with single database.\n<end_of_turn>\n"}
{"File Name":"remultiform\/0008-support-indexeddb-as-a-data-store.md","Context":"## Context\\nWe need to persist the user data to the backend. Rather than building our\\nfrontend to be tightly coupled to an API, we want a layer that can sit between\\nthe application and the API.\\nWe know that we will need to support offline use of these forms. We also want\\nusers to be able to recover unsubmitted forms if they haven't gone online\\nbetween sessions.\\nWe also know that some forms will require the upload of images.\\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\\ndatabase that persists data across sessions. It supports storing blob data (such\\nas images), and has decent storage limits.\\n","Decision":"We will support IndexedDB as a data store layer, sitting between the application\\nand the API.\\n","tokens":144,"id":5257,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nThe main data source of the API catalogue website was originally a collection of\nmarkdown files. The build process uses the Middleman static site generator\nconfigured by the [Tech Docs Gem](https:\/\/github.com\/alphagov\/tech-docs-gem)\n('TDG').\nThe TDG provides additional functionality including search, sidebar\nnavigation ('Table of Contents'), the layout, and styling.\nThe TDG is not necessarily a good fit for the API catalogue because the project\nisn't purely documentation, and our data source is now a CSV.\nIn particular it is difficult to override templates inherited from the gem, to\nadjust the layout on a particular page or add page-specific JavaScript for\nexample.\nUsing TDG to render the Table of Contents is slow for our site because\nby design every page is re-rendered multiple times to pull out the headings\n(adding over a minute to build times).\nThe TDG also requires specific dependency versions. These version\nrestrictions prevent us being in control of version upgrades which are necessary\nto remain on support versions and receive security patches.\n\n## Decision\nRemove the TDG as a dependency by vendoring the code relevant to\nthe API catalogue directly into the project itself.\n\n\n## Context\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\nserver.\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\nObject Storage technologies.\nObject Storage is known to be cheap and yet offers guarantee about durability.\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\nIt also contribute negatively to the project build time.\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\nour Swift code without dropping Swift support at the same time.\n\n## Decision\n* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\n\n\n\nProvide a decision given the context below:\n## Context\nWe need to persist the user data to the backend. Rather than building our\nfrontend to be tightly coupled to an API, we want a layer that can sit between\nthe application and the API.\nWe know that we will need to support offline use of these forms. We also want\nusers to be able to recover unsubmitted forms if they haven't gone online\nbetween sessions.\nWe also know that some forms will require the upload of images.\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\ndatabase that persists data across sessions. It supports storing blob data (such\nas images), and has decent storage limits.\n<end_of_turn>\n<start_of_turn>model\nWe will support IndexedDB as a data store layer, sitting between the application\nand the API.\n<end_of_turn>\n"}
{"File Name":"TOSCAna\/0018-cloudfoundry-no-deployment-in-runtime.md","Context":"## Problem\\nMost of the credentials\/information of the environment (like a service address) are only available as soon the application is deployed.\\n","Decision":"* Chosen Alternative: creating scripts\\n","tokens":29,"id":1745,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\nand the server side code (which will be a dropwizard app).\nDepending on how the users want to run the service provider we may need\ndifferent security solutions.\n\n## Decision\nIf possible users can talk to the service provider on the loopback (127.0.0.1)\nIf that doesn't work for some reason then they can use the dropwizard config\nto set up basic auth or tls or something.\nSee http:\/\/www.dropwizard.io\/1.1.0\/docs\/manual\/configuration.html#connectors\n\n\n## Context\nNeed of using Infrastructure as code to, track what resources are running in the AWS account\n\n## Decision\nUse Terraform\n\n\n\nProvide a decision given the context below:\n## Problem\nMost of the credentials\/information of the environment (like a service address) are only available as soon the application is deployed.\n<end_of_turn>\n<start_of_turn>model\n* Chosen Alternative: creating scripts\n<end_of_turn>\n"}
{"File Name":"celestia-core\/adr-026-general-merkle-proof.md","Context":"## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding\/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n","Decision":"### ProofOperator\\n`type ProofOperator` is an interface for Merkle proofs. The definition is:\\n```go\\ntype ProofOperator interface {\\nRun([][]byte) ([][]byte, error)\\nGetKey() []byte\\nProofOp() ProofOp\\n}\\n```\\nSince a proof can treat various data type, `Run()` takes `[][]byte` as the argument, not `[]byte`. For example, a range proof's `Run()` can take multiple key-values as its argument. It will then return the root of the tree for the further process, calculated with the input value.\\n`ProofOperator` does not have to be a Merkle proof - it can be a function that transforms the argument for intermediate process e.g. prepending the length to the `[]byte`.\\n### ProofOp\\n`type ProofOp` is a protobuf message which is a triple of `Type string`, `Key []byte`, and `Data []byte`. `ProofOperator` and `ProofOp`are interconvertible, using `ProofOperator.ProofOp()` and `OpDecoder()`, where `OpDecoder` is a function that each proof type can register for their own encoding scheme. For example, we can add an byte for encoding scheme before the serialized proof, supporting JSON decoding.\\n","tokens":99,"id":4130,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nABCI-server could expose its own RPC-server and act as a proxy to Tendermint.\nThe idea was for the Tendermint RPC to just be a transparent proxy to the app.\nClients need to talk to Tendermint for proofs, unless we burden all app devs\nwith exposing Tendermint proof stuff. Also seems less complex to lock down one\nserver than two, but granted it makes querying a bit more kludgy since it needs\nto be passed as a `Query`. Also, **having a very standard rpc interface means\nthe light-client can work with all apps and handle proofs**. The only\napp-specific logic is decoding the binary data to a more readable form (eg.\njson). This is a huge advantage for code-reuse and standardization.\n\n## Decision\nWe dont expose an RPC server on any of our ABCI-apps.\n\n\n# Context and Problem Statement\nUsers want to add custom metadata to items. For example, for invoices\nfields like `invoice-number` or `total`\/`amount` make sense. When\nusing a pagination stamp, every item gets a pagination number.\nThis is currently not possible to realize in docspell. But it is an\nessential part when organizing and archiving documents. It should be\nsupported.\n# Considered Options\n\n## Decision\n- values are strings at the database\n- values are strings when transported from\/to server\n- client must provide the correct formatted strings per type\n- numeric: some decimal number\n- money: decimal number\n- text: no restrictions\n- date: a local date as iso string, e.g. `2011-10-09`\n- bool: either `\"true\"` or `\"false\"`, case insensitive\n\n\n\nProvide a decision given the context below:\n## Context\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\nGoals:\n- Layer Merkle proofs without decoding\/reencoding\n- Provide general way to chain proofs\n- Make the proof format extensible, allowing thirdparty proof types\n<end_of_turn>\n<start_of_turn>model\n### ProofOperator\n`type ProofOperator` is an interface for Merkle proofs. The definition is:\n```go\ntype ProofOperator interface {\nRun([][]byte) ([][]byte, error)\nGetKey() []byte\nProofOp() ProofOp\n}\n```\nSince a proof can treat various data type, `Run()` takes `[][]byte` as the argument, not `[]byte`. For example, a range proof's `Run()` can take multiple key-values as its argument. It will then return the root of the tree for the further process, calculated with the input value.\n`ProofOperator` does not have to be a Merkle proof - it can be a function that transforms the argument for intermediate process e.g. prepending the length to the `[]byte`.\n### ProofOp\n`type ProofOp` is a protobuf message which is a triple of `Type string`, `Key []byte`, and `Data []byte`. `ProofOperator` and `ProofOp`are interconvertible, using `ProofOperator.ProofOp()` and `OpDecoder()`, where `OpDecoder` is a function that each proof type can register for their own encoding scheme. For example, we can add an byte for encoding scheme before the serialized proof, supporting JSON decoding.\n<end_of_turn>\n"}
{"File Name":"corona-hackathon\/0011-postgres.md","Context":"## Context\\nWe have to make a decision on how\/where to host the DB.\\n","Decision":"We use a freely managed [Postgres on Heroku](https:\/\/www.heroku.com\/postgres)\\n","tokens":19,"id":3657,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe have to make a decision on the branching strategy for development.\n\n## Decision\n[Git Flow](https:\/\/danielkummer.github.io\/git-flow-cheatsheet\/) it will be.\n\n\n## Context\nWe need to record the future architectural decisions made.\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\n\n\n\nProvide a decision given the context below:\n## Context\nWe have to make a decision on how\/where to host the DB.\n<end_of_turn>\n<start_of_turn>model\nWe use a freely managed [Postgres on Heroku](https:\/\/www.heroku.com\/postgres)\n<end_of_turn>\n"}
{"File Name":"monocle\/0011-search-query-language.md","Context":"## Context and Problem Statement\\nTo build custom dashboards we need to define queries that are too complex for the existing filter box form.\\nWe would like to use flexible search expressions based on a proper query language.\\n","Decision":"Chosen option: \"Monocle Query Language\", because it comes out best (see below).\\n### Positive Consequences\\n- We improve the user experience by replacing the clunky filter box with a simpler search bar.\\n- We create a re-usable component.\\n### Negative Consequences\\n- We need to maintain a language toolchain.\\n","tokens":44,"id":4637,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe would like to have an easy-to-use system for consistent styles that doesn't\ntake much tinkering. We propose using the `bootstrap` gem.\n\n## Decision\nPending\n\n\n## Context and Problem Statement\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\nWhich query syntax should be used for the abstract queries?\nWhich features should the syntax support?\n\n## Decision\nChosen option: \"Use a syntax that is derived of the lucene query syntax\", because only option that is already known, and easy to implement.\nFurthermore parsers for lucene already exist and are tested.\nFor simplicity, and lack of universal capabilities across fetchers, only basic query features and therefor syntax is supported:\n* All terms in the query are whitespace separated and will be ANDed\n* Default and certain fielded terms are supported\n* Fielded Terms:\n* `author`\n* `title`\n* `journal`\n* `year` (for single year)\n* `year-range` (for range e.g. `year-range:2012-2015`)\n* The `journal`, `year`, and `year-range` fields should only be populated once in each query\n* The `year` and `year-range` fields are mutually exclusive\n* Example:\n* `author:\"Igor Steinmacher\" author:\"Christoph Treude\" year:2017` will be converted to\n* `author:\"Igor Steinmacher\" AND author:\"Christoph Treude\" AND year:2017`\nThe supported syntax can be expressed in EBNF as follows:\nQuery := {Clause} \\\nClause:= \\[Field\\] Term \\\nField := author: | title: | journal: | year: | year-range: | default:\\\nTerm  := Word | Phrase \\\nWord can be derived to any series of non-whitespace characters.\nPhrases are multiple words wrapped in quotes and may contain white-space characters within the quotes.\\\nNote: Even though this EBNF syntactically allows the creation of queries with year and year-range fields,\nsuch a query does not make sense semantically and therefore will not be executed.\n### Positive Consequences\n* Already tested\n* Well known\n* Easy to implement\n* Can use an existing parser\n\n\n\nProvide a decision given the context below:\n## Context and Problem Statement\nTo build custom dashboards we need to define queries that are too complex for the existing filter box form.\nWe would like to use flexible search expressions based on a proper query language.\n<end_of_turn>\n<start_of_turn>model\nChosen option: \"Monocle Query Language\", because it comes out best (see below).\n### Positive Consequences\n- We improve the user experience by replacing the clunky filter box with a simpler search bar.\n- We create a re-usable component.\n### Negative Consequences\n- We need to maintain a language toolchain.\n<end_of_turn>\n"}
{"File Name":"GoStudy\/0002-use-hexagonal-architecture.md","Context":"## Context\\nThe SDARS application consists of 3 independent components that can be communicated in various ways.\\nTo enable different communication ways we need to apply a proper architectural style.\\n","Decision":"Adopt Hexagonal Architecture for project.\\n","tokens":37,"id":428,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context and Problem Statement\nMultiple projects need to show data visually using charts and graphs. In order to provide unified look and feel across island.is we should commit to a single approach to implementing charts, i.e. choose one library for the whole repository.\n## Decision Drivers\n- Meet all requirements listed above\n- API quality\n- Pricing\n- Bundle size\n- Typescript support\n\n## Decision\n- Meet all requirements listed above\n- API quality\n- Pricing\n- Bundle size\n- Typescript support\nChosen option: \"Recharts\", because it meets all requirements, and overall has a very nice, dev-friendly API. It is the most popular (downloads per week) react charting library on github, and recommended across the community. We can customize how it looks, and start using it quickly without much groundwork.\n### Positive Consequences\n- We can start implementing charts and graphs as needed in island.is\n### Negative Consequences\n- It is a big dependency, but almost all chart libraries are big due to their nature. We will minimize the impact of this by enforcing charts to be lazy loaded in our codebase.\n\n\n# Context and Problem Statement\nHow should the application be structured into its main components? The\ngoal is to be able to have multiple rest servers\/webapps and multiple\ndocument processor components working togehter.\n# Decision Outcome\nThe following are the \"main\" modules. There may be more helper modules\nand libraries that support implementing a feature.\n\n## Decision\nThe following are the \"main\" modules. There may be more helper modules\nand libraries that support implementing a feature.\n\n\n\nProvide a decision given the context below:\n## Context\nThe SDARS application consists of 3 independent components that can be communicated in various ways.\nTo enable different communication ways we need to apply a proper architectural style.\n<end_of_turn>\n<start_of_turn>model\nAdopt Hexagonal Architecture for project.\n<end_of_turn>\n"}
{"File Name":"form-builder\/0010-testing-the-editor-pipeline.md","Context":"## Context\\nOur pipeline follows a continuous delivery method with the manual gate to release to production.\\n![alt text](0010-release-pipeline-context.png \"Diagram of how the editor is released through the pipeline.\")\\nWe have one Editor in test which causes issues when the rest of the team want to test a feature. Also, while the Editor is tied up in testing the pipeline is blocked for deploying to live-production.\\n","Decision":"![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")\\nThe main problem to solve is having a feature that is being developed available to the wider team to test. Running locally was not considered as the MOJ Macbooks are locked down to those outside of the technical disciplines.\\nChanging the pipeline to create an Editor (all plugged in to the dev and test environments). This will involve, building and deploying the editor code into a pod and attaching to a unique URL. Not every branch will need to create a new test editor, the developer will need to add \"testable-\" to the beginning of the branch name (i.e. `testable-my-really-awesome-feature`).\\nThe Editor will use the current auth mechanism unless the effort outweighs the investment. At a mimimum, basic auth of a set username and password must be added.\\nTest Editor must be cleanly removed after it has fullfilled it's purpose. This should ideally be automated either triggered by merging the branch or via commandline.\\n","tokens":87,"id":5102,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nEngineers in the team want to have their code changes built as real environments in the Development AWS account so that they can have their work evaluated.\nEvaluation includes automated and user acceptance testing.\nWhen evaluation is finished it should be possible to destroy the environment, while retaining important artifacts and data (logs for example).\nIt will be possible to host multiple PR environments in the Development AWS account\n\n## Decision\n### Terraform Configuration\nWe will identify and manage separately, resources at an Account level and resources at an Environment level.\nAccount level resources, such as S3 buckets for logs, the default VPC and networking will be used Environment level resources, such as ECS clusters, DNS routes and Load Balancers.\nThis will enable us to use Terraform Workspaces to contain an Environment completely for it to be built and destroyed simply.\nAccount level resources will remain in place after an Environment is destroyed.\n![split terraform configuration diagram](..\/diagrams\/Use-An-LPA_split_Terraform_configurations.png)\n### Name Spacing\nWe will use the github PR number and the first few characters of the branch name to namespace the environment and it's resources\nFor example, DNS records will be created for each environment using the namespace `https:\/\/view.025-UML-93.use-an-lpa.opg.service.justice.gov.uk`\nwhile an ECS cluster will be named `025-UML-93-use-an-lpa`\n### Environment lifecycle\nAn Environment will be built when a PR is opened\nOnce evaluation is complete, the Envirnoment can be destroyed.\nTo begin with the pipeline will print instructions for destroying an Environment.\nLater, destroying an Environment will be automated and scheduled with a maximum life span of a day or two.\n### CI\/CD Pipeline\nThe Terraform steps in the pipeline will always first provision the Account level shared resources, then provision the Environment level resources.\n![pipeline terraform steps](..\/diagrams\/Use-An-LPA_Terraform_pipeline.png)\n\n\n# ADR 0276: Problem Matchers\n**Date** 2019-06-05\n**Status** Accepted\n## Context\nCompilation failures during a CI build should surface good error messages.\nFor example, the actual compile errors from the typescript compiler should bubble as issues in the UI. And not simply \"tsc exited with exit code 1\".\nVSCode has an extensible model for solving this type of problem. VSCode allows users to configure which [problems matchers](https:\/\/code.visualstudio.com\/docs\/editor\/tasks#_defining-a-problem-matcher) to use, when scanning output. For example, a user can apply the `tsc` problem matcher to receive a rich error output experience in VSCode, when compiling their typescript project.\nThe problem-matcher concept fits well with \"setup\" actions. For example, the `setup-nodejs` action will download node.js, add it to the PATH, and register the `tsc` problem matcher. For the duration of the job, the `tsc` problem matcher will be applied against the output.\n\n## Decision\n### Registration\n#### Using `::` command\n`::add-matcher::path-to-problem-matcher-config.json`\nUsing a `::` command allows for flexibility:\n- Ad hoc scripts can register problem matchers\n- Allows problem matchers to be conditionally registered\nNote, if a matcher with the same name is registered a second time, it will clobber the first instance.\n\n\n\nProvide a decision given the context below:\n## Context\nOur pipeline follows a continuous delivery method with the manual gate to release to production.\n![alt text](0010-release-pipeline-context.png \"Diagram of how the editor is released through the pipeline.\")\nWe have one Editor in test which causes issues when the rest of the team want to test a feature. Also, while the Editor is tied up in testing the pipeline is blocked for deploying to live-production.\n<end_of_turn>\n<start_of_turn>model\n![alt text](0010-release-pipeline-proposed.png \"Diagram of how the editor is released through the pipeline to have a testable editor.\")\nThe main problem to solve is having a feature that is being developed available to the wider team to test. Running locally was not considered as the MOJ Macbooks are locked down to those outside of the technical disciplines.\nChanging the pipeline to create an Editor (all plugged in to the dev and test environments). This will involve, building and deploying the editor code into a pod and attaching to a unique URL. Not every branch will need to create a new test editor, the developer will need to add \"testable-\" to the beginning of the branch name (i.e. `testable-my-really-awesome-feature`).\nThe Editor will use the current auth mechanism unless the effort outweighs the investment. At a mimimum, basic auth of a set username and password must be added.\nTest Editor must be cleanly removed after it has fullfilled it's purpose. This should ideally be automated either triggered by merging the branch or via commandline.\n<end_of_turn>\n"}
{"File Name":"iampeterbanjo.com\/0008-use-ava-test-runner.md","Context":"## Context\\n[Lab][hapi-lab] is misreporting the code coverage stats because I have not set it up to [work with Typescript][lab-ts]. The other issue with Lab is that its community is quite small meaning less plug-and-play with other tools. I thought about using [Ava][ava-typescript] but [this review][dodds-jest] of Ava's performance issues doesn't sound great.\\n","Decision":"Jest seems like the way to go:\\n- Typescript support\\n- Large community\\n- Familiar\\n","tokens":91,"id":1594,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context and Problem Statement\nAfter experimenting with the previous MVP goal defined by [ADR-0002](0002-define-goal-for-an-mvp.md), I've decided it's not a good approach.\nMy main issue with is that in a way, it is re-creating Anki's \"Add Card\" screen, albeit a custom one which adds RTK cards.\nHere's a screenshot of the UI created and filled with placeholder text.\n![](..\/assets\/rtk-browser-screenshot.png)\nWhile I still think it'd be cool to have an RTK browser in Anki and working with QT is ....interesting, I thought\na simpler approach should be sought.\nSo the problem stands, what should be done as an MVP and what should it contain?\n\n## Decision\nAfter some experimenting and research, I've decided an MVP should augment the `Add Card`.\nHere's the vision of how a user interacts with the MVP.\n1. User opens the `Add Card` window which has a \"RTK\" button, and a \"Stories\" button.\n1. User types in their desired Heisig Kanji number is the \"Number\" field.\n1. User clicks the RTK button and the character, and stroke order get auto filled.\n1. User clicks \"Stories\" button, which opens an embedded browser window to user stories.\n### Positive Consequences <!-- optional -->\n- The approach is simpler.\n- We keep Building UI elements with `QT` to a minimum under this approach.\n### Negative Consequences <!-- optional -->\n- We leave out Additional keyword meanings and the radicals for the Kanji\n\n\n## Context\nAfter analysing performances of the extension on a big JavaScript file, [we noticed][comment] the \"Convert to Pure Component\" Quick Fix took most of the time:\n![][flame-chart]\nThis refactoring can only be applied under specific circumstances. It doesn't worth the toll for everyone, on every trigger.\nAlso, there's nothing we can do to improve the performance of this refactoring. The code is implemented by [react-codemod](https:\/\/github.com\/reactjs\/react-codemod). It was originally meant to be run through a CLI command.\n\n## Decision\nWe will stop proposing a Quick Fix for this refactoring.\n\n\n\nProvide a decision given the context below:\n## Context\n[Lab][hapi-lab] is misreporting the code coverage stats because I have not set it up to [work with Typescript][lab-ts]. The other issue with Lab is that its community is quite small meaning less plug-and-play with other tools. I thought about using [Ava][ava-typescript] but [this review][dodds-jest] of Ava's performance issues doesn't sound great.\n<end_of_turn>\n<start_of_turn>model\nJest seems like the way to go:\n- Typescript support\n- Large community\n- Familiar\n<end_of_turn>\n"}
{"File Name":"handbook\/0003-css.md","Context":"## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n","Decision":"- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript jsx\\n\/\/ Good:\\n<Box padding\"small\" \/>\\n<Box padding={{xs: 'small', md: 'medium'}} \/>\\n<Input large \/>\\n<Text preset=\"heading3\" as=\"p\" \/>\\n\/\/ Bad:\\n<Box className={customLayout} \/>\\n<Input style={{ height: 50, padding: 16 }} \/>\\n<Text className={styles.heading} \/>\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n","tokens":126,"id":1992,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context and Problem Statement\nWe want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI\/CD.\n## Decision Drivers\n- Low complexity and overhead in development.\n- Fit for our stack.\n- Optimize CI\/CD with dependency graphs and\/or caching.\n- Flexible.\n\n## Decision\n- Low complexity and overhead in development.\n- Fit for our stack.\n- Optimize CI\/CD with dependency graphs and\/or caching.\n- Flexible.\nChosen option: \"Nx\", because:\n- It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).\n- It's relatively easy to learn with focused documentation.\n- It has schematics to generate apps, libraries and components that includes all of our tools.\n- It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.\n\n\n## Context and Problem Statement\nHow to guarantee reproducibility of Jupyter Notebooks?\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\nNotebooks should be treated as component\/service that use their own dependencies, therefore when storing notebooks,\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\n## Decision Drivers <!-- optional -->\n* user prospective\n* reproducibility\n* traecability\n\n## Decision\n* user prospective\n* reproducibility\n* traecability\nThe option selected is 3. because:\n* enforce reproducibility\n* enforce traceability between notebook\n### Positive Consequences <!-- optional -->\n* Satisfy reproducibility, traecability, shareability.\n* Notebooks are coupled with dependencies in their metadata.\n* If more notebooks are present, a common Pipfile can be created with a button that can automatically extract from all notebook dependencies and new common Pipfile.lock will be created. This would allow creation of an image that can run the notebooks.\n\n\n\nProvide a decision given the context below:\n## Context and Problem Statement\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\n## Decision Drivers\n- Should be performant, with code splitting, caching and minimal runtime overhead.\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\n- Should be type-safe to catch issues when refactoring.\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\n- We want a pattern for responsive props with atomic layout components.\n<end_of_turn>\n<start_of_turn>model\n- Should be performant, with code splitting, caching and minimal runtime overhead.\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\n- Should be type-safe to catch issues when refactoring.\n- Reusable components should be closed, not accepting arbitrary styles\/classes.\n- We want a pattern for responsive props with atomic layout components.\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\nExample:\n```typescript jsx\n\/\/ Good:\n<Box padding\"small\" \/>\n<Box padding={{xs: 'small', md: 'medium'}} \/>\n<Input large \/>\n<Text preset=\"heading3\" as=\"p\" \/>\n\/\/ Bad:\n<Box className={customLayout} \/>\n<Input style={{ height: 50, padding: 16 }} \/>\n<Text className={styles.heading} \/>\n```\n### Positive Consequences\n- Treat is statically extracted at build time, so it has minimal runtime.\n- Styles load in parallel with JS, also when code splitting.\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\n### Negative Consequences\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\n<end_of_turn>\n"}
{"File Name":"hello\/005-test-framework.md","Context":"## Context\\nWe want our code to be well tested. What tools or frameworks can we\\nleverage?\\n","Decision":"We will use the [pytest] framework to test our [Python] code. In\\naddition, we will use the [mock] library to prevent our tests\\nfrom interacting with external services.\\n","tokens":23,"id":4359,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe need a workflow to build our appliation and components.\n\n## Decision\nWe use Storybook only for building new self contained components.\n\n\n## Context\nWe need a way to deploy our application to AWS.\n\n## Decision\nWe will use [Terraform] to deploy our application. More specifically,\nwe will use [Chalice] to create the build artifact for the source\ncode (.zip) and then use [Terraform] to manage the infrastructure\nresources.\n\ud83d\udc49 **Note:** Ideally, we would publish the build artifacts to S3 before\ndeployment. That way the deployment process could happen independently\nof the build process.\n\ud83d\udc49 **Note:** To begin with, we'll just convert the JSON data created\nby [Chalice] to [HCL] and add any other resources needed. Going forward,\nwe will use modules to create reusable components.\n\n\n\nProvide a decision given the context below:\n## Context\nWe want our code to be well tested. What tools or frameworks can we\nleverage?\n<end_of_turn>\n<start_of_turn>model\nWe will use the [pytest] framework to test our [Python] code. In\naddition, we will use the [mock] library to prevent our tests\nfrom interacting with external services.\n<end_of_turn>\n"}
{"File Name":"lbh-adrs\/Event-Driven-Architecture.md","Context":"## **Context**\\nWhen the microservices need to interact with each other (e.g. to sync common data), the preferable approach is to adopt an event-driven architecture. This approach guarantees loosely coupled services which can be run and deployed in isolation.\\nIn order to implement an events-driven architecture we need to use a message broker which should be responsible for sending the messages.\\nIn AWS, two options can be considered:\\n1. SNS (Simple Notification Service)\\nSNS implements pub\/sub messaging which allow applications to send messages to multiple subscribers through push mechanisms.\\n2. SNS (Simple Notification Service) + SQS (Simple Queue Service)\\nSQS is a message queue service used by distributed applications to exchange messages through a polling model and can be used to decouple sending and receiving components. Using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event, and also persisted in an Amazon SQS queue for other applications to process at a later time.\\n","Decision":"**SNS (Simple Notification Service) + SQS (Simple Queue Service)**\\nBy coupling SQS with SNS, the subscriber can receive messages at \"their peace\". It allows subscribers to be offline, tolerant to network and host failures. Although SNS has got a 4-phase retry policy, the message can be lost if the consumer is not available. Instead if the subscriber uses a queue, we are able to achieve guaranteed delivery.\\n","tokens":209,"id":2309,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context and Problem Statement\nTo implement the EAI patterns we use a combination of a generic component\nwhich handles the communication with Kafka and a FaaS solution. The business logic of the EAI patterns (message splitting\/aggregation or transformation) is provided via functions which are hosted on the FaaS solution. The generic component communicates with Kafka and de\/serializes the messages. We need a means to wire the instances of the generic component with the functions. E.g. A user wants to insert a message splitter between two message-based components. To realize this an instance of the generic component in combination with a splitting FaaS function will be used. The generic component needs the address of the FaaS gateway and the function name (e.g. http:\/\/address:8080\/function\/msg-payload-splitter) to call the function. To provide the necessary information to instances of the generic component we considered the following techniques.\n## Decision Drivers <!-- optional -->\n* MUST be supported by the language\/technology which is used to implement the generic component\n* MUST be easy to integrate into MICO\n* SHOULD be a well known and proven solution\n\n## Decision\n* MUST be supported by the language\/technology which is used to implement the generic component\n* MUST be easy to integrate into MICO\n* SHOULD be a well known and proven solution\nChosen option: \"Environment variables\", because MICO already supports this and it is easy to implement in the generic component.\n\n\n## Context\nAuthentication and access control for FlowKit has previously been very limited, making use of only of postgres usernames and passwords. WIth the introduction of the API, we can be much more granular in allowing access through the use of [JSON Web Tokens](https:\/\/jwt.io).\nThese are an encoded and cryptographically signed string, which permit access to some set of API functionality. The string specifies the identity of the user, exactly what they can access, and the time period for which the token is valid.\nTampering with the token will result in the signature not matching, an event which can be logged an subsequently investigated.\nA key advantage of JWTs is that they can be centrally managed, even where the service they will be used with is not accessible to the internet. Another significant advantage is that the tokens are inherently ephemeral - should a token be stolen, the time window for it to lead to a data breach will (given appropriate usage of the system) be small.\nJWTs are also advantageous in that the add relatively minimal overhead to the day to day experience of analysts, and have broad support across many languages.\nBut perhaps the most important advantage is the granularity of access they afford - for example it is possible to allow access to only one specific result with this method. This is useful where developing dashboards and the like, or where access to outputs but not data must be provided to other parties.\nOther alternatives would be to use a more traditional login system, backed by a database, integrate with the Docker host's authentication system, or to use public key based authentication. All of these require more and local administration, or necessitate the use of either a JWT equivalent to actually provide API access, or the use of cookies to avoid a need to log in for every communication with the API.\n\n## Decision\nAPI authentication and access control will use JWTs.\n\n\n\nProvide a decision given the context below:\n## **Context**\nWhen the microservices need to interact with each other (e.g. to sync common data), the preferable approach is to adopt an event-driven architecture. This approach guarantees loosely coupled services which can be run and deployed in isolation.\nIn order to implement an events-driven architecture we need to use a message broker which should be responsible for sending the messages.\nIn AWS, two options can be considered:\n1. SNS (Simple Notification Service)\nSNS implements pub\/sub messaging which allow applications to send messages to multiple subscribers through push mechanisms.\n2. SNS (Simple Notification Service) + SQS (Simple Queue Service)\nSQS is a message queue service used by distributed applications to exchange messages through a polling model and can be used to decouple sending and receiving components. Using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event, and also persisted in an Amazon SQS queue for other applications to process at a later time.\n<end_of_turn>\n<start_of_turn>model\n**SNS (Simple Notification Service) + SQS (Simple Queue Service)**\nBy coupling SQS with SNS, the subscriber can receive messages at \"their peace\". It allows subscribers to be offline, tolerant to network and host failures. Although SNS has got a 4-phase retry policy, the message can be lost if the consumer is not available. Instead if the subscriber uses a queue, we are able to achieve guaranteed delivery.\n<end_of_turn>\n"}
{"File Name":"CICD-pipeline\/002-usage-and-import-of-shared-library.md","Context":"## Context\\nWe need to decide how the shared library should be loaded for usage.\\n- A library version can be a tag, a branch or a commit SHA.\\n- A library can be loaded explicitly or implicitly\\n- A library default version can be configured globally or not\\n### Loading implicitly\\n````groovy\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- a default version of the library needs to be defined globally\\n- there is no need to import the library, as it will be loaded automatically\\n- you can override the version by importing it manually: `@Library('my-shared-library@my-custom-version') _`\\n### Loading explicitly\\n````groovy\\n@Library('my-shared-library@1.0.0') _\\npipeline {\\nagent any\\nstages {\\nstage {\\nsteps {\\nsomeMagicallyAvailableLibraryFunction()\\n}\\n}\\n}\\n}\\n````\\n- an import statement is always needed\\n- a default version could be defined globally, then the version can be left out when importing\\n### Using a global predefined version\\n- can be overriden in the pipeline\\n- does not force users to select a version\\n### No globel predefined version\\n- import will not work without version\\n- consumers are aware of the version that they use\\n- can be dynamic (auto updating) when using release channels with branches\\n","Decision":"We will use explicit loading without a predefined version and release channels for releasing based on branches.\\n- reduces the magic of the used version\\n- can still be auto updating\\n- consumers know that a library is used and what the version is\\n","tokens":314,"id":2766,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n### Context\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\n```php\nfunction test_something(): void {\n\/\/ ...\ntry {\n\/\/ the statement that fail\n$this->fail();\n} catch (Exception $e) {\n\/\/ ...\n}\n}\n```\nOr:\n```php\nfunction test_something(): void {\n\/\/ ...\n$this->expectException($exception)\n\/\/ the statement that fail\n}\n```\n### Decision\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\nA pull request to fix this practice in the whole codebase may be done but has not been made\nmandatory. New pull requests though should stick to this practice.\n### Status\nAccepted ([#1090][1090])\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\n\n## Decision\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\nA pull request to fix this practice in the whole codebase may be done but has not been made\nmandatory. New pull requests though should stick to this practice.\n### Status\nAccepted ([#1090][1090])\n[sebastian-bergmann]: https:\/\/thephp.cc\/company\/consultants\/sebastian-bergmann\n[phpunit-exception-best-practices]: https:\/\/thephp.cc\/news\/2016\/02\/questioning-phpunit-best-practices\n[1090]: https:\/\/github.com\/infection\/infection\/pull\/1061\n\n\n## Context\nWe use foreign keys to help enforce database integrity e.g.\n```\n# ensure creator_id on a document matches a row in the users table\n# when we try to delete a user, raise an error if its the creator of a document\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :restrict\n```\nThere are other `on_delete` behaviours instead of `restrict`.\n```\n# when we delete a user, set creator_id to NULL on all of its documents\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :nullify\n# when we delete a user, also delete all of its document\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :cascade\n```\nDifferent options are appropriate depending on the foreign key.\n* `restrict` is a good choice in a child\/parent scenario, where deleting a child should be restricted if it would make the parent invalid. For example, we should not be able to delete an ActiveStorage blob if its parent image still exists.\n* `nullify` works well if the relationship is optional. For example, our code doesn't currently require a document to have a creator, so it might be reasonable to `creator_id` on document to NULL if we delete a user for some reason.\n* `cascade` is a good choice in a parent\/child scenario, where it makes sense to delete the child when we delete the parent. For example, if we delete a document for some reason, it also makes sense to delete the editions.\n\n## Decision\nCurrently, we don't have any real-world scenarios for deleting stuff from the database. Therefore, we should do the simplest thing and `restrict` on all foreign key delete actions. `restrict` is actually the default, but we should continue to specify it explicitly to indicate a conscious decision.\n\n\n\nProvide a decision given the context below:\n## Context\nWe need to decide how the shared library should be loaded for usage.\n- A library version can be a tag, a branch or a commit SHA.\n- A library can be loaded explicitly or implicitly\n- A library default version can be configured globally or not\n### Loading implicitly\n````groovy\npipeline {\nagent any\nstages {\nstage {\nsteps {\nsomeMagicallyAvailableLibraryFunction()\n}\n}\n}\n}\n````\n- a default version of the library needs to be defined globally\n- there is no need to import the library, as it will be loaded automatically\n- you can override the version by importing it manually: `@Library('my-shared-library@my-custom-version') _`\n### Loading explicitly\n````groovy\n@Library('my-shared-library@1.0.0') _\npipeline {\nagent any\nstages {\nstage {\nsteps {\nsomeMagicallyAvailableLibraryFunction()\n}\n}\n}\n}\n````\n- an import statement is always needed\n- a default version could be defined globally, then the version can be left out when importing\n### Using a global predefined version\n- can be overriden in the pipeline\n- does not force users to select a version\n### No globel predefined version\n- import will not work without version\n- consumers are aware of the version that they use\n- can be dynamic (auto updating) when using release channels with branches\n<end_of_turn>\n<start_of_turn>model\nWe will use explicit loading without a predefined version and release channels for releasing based on branches.\n- reduces the magic of the used version\n- can still be auto updating\n- consumers know that a library is used and what the version is\n<end_of_turn>\n"}
{"File Name":"ios-architecture-decision-logs\/0006-inject-parameters-asArguments-toPresenter.md","Context":"## Context\\nWe faced a problem while injecting new parameters to presenters' initializer. The list of init params are getting much longer.\\n","Decision":"Each module has it's own `createModule` or `setupModule` static method in its router. If we have 3 or more properties to be injected to any of these methods, we must create `XYZArguments` structure that contains those properties and pass the argument object to router.\\nWe must also inject the same arguments variable to the presenters' initializer and use the variables inside through the arguments object, and not copy each of variables inside arguments to the presenter one by one.\\n**Note**: Argument object must only include the values to be injected from outer module to initialize the target module (e.g. any kind of `Config\/AB flags`, `Managers` and `Helpers` must not be stored inside arguments, and these variables must be injected to the presenters' initializer directly)\\n","tokens":28,"id":4983,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe want to be able to write log entries (to file, or STDOUT), without them being interleaved.\nBut also, we want logging to perform well.\n\n## Decision\n_Unlike_ the Ruby standard `Logger`, GreenLog will use a [lock-free logging](https:\/\/www.jstorimer.com\/blogs\/workingwithcode\/7982047-is-lock-free-logging-safe) approach. That is, we will:\n- avoid using of mutexes to serialise output\n- perform atomic writes to `IO` streams (using `<<`)\n\n\n## Context\nRight now, we can query the present validator set, but there is no history.\nIf you were offline for a long time, there is no way to reconstruct past validators. This is needed for the light client and we agreed needs enhancement of the API.\n\n## Decision\nFor every block, store a new structure that contains either the latest validator set,\nor the height of the last block for which the validator set changed. Note this is not\nthe height of the block which returned the validator set change itself, but the next block,\nie. the first block it comes into effect for.\nStoring the validators will be handled by the `state` package.\nAt some point in the future, we may consider more efficient storage in the case where the validators\nare updated frequently - for instance by only saving the diffs, rather than the whole set.\nAn alternative approach suggested keeping the validator set, or diffs of it, in a merkle IAVL tree.\nWhile it might afford cheaper proofs that a validator set has not changed, it would be more complex,\nand likely less efficient.\n\n\n\nProvide a decision given the context below:\n## Context\nWe faced a problem while injecting new parameters to presenters' initializer. The list of init params are getting much longer.\n<end_of_turn>\n<start_of_turn>model\nEach module has it's own `createModule` or `setupModule` static method in its router. If we have 3 or more properties to be injected to any of these methods, we must create `XYZArguments` structure that contains those properties and pass the argument object to router.\nWe must also inject the same arguments variable to the presenters' initializer and use the variables inside through the arguments object, and not copy each of variables inside arguments to the presenter one by one.\n**Note**: Argument object must only include the values to be injected from outer module to initialize the target module (e.g. any kind of `Config\/AB flags`, `Managers` and `Helpers` must not be stored inside arguments, and these variables must be injected to the presenters' initializer directly)\n<end_of_turn>\n"}
{"File Name":"form-design-system\/components_ignore_classnames.md","Context":"## Context and Problem Statement\\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\\narbitrary style overrides on the component.\\nWe want all components in FDS to follow a similar prop interface, so this decision should\\napply to _all_ components.\\n","Decision":"Chosen option: \"No components accept a `className`\", because...\\n- It's too risky to introduce style overrides to low-level layout components.\\n- Style customization should happen via props, where the component can be responsible for\\nstyle changes and variations.\\n### Positive Consequences <!-- optional -->\\n- Prop API consistency across all components.\\n### Negative Consequences <!-- optional -->\\n- Consumers of FDS components must handle the breaking change. Any custom `className`\\nprops on FDS components must be moved to a direct parent or direct child of the\\ncomponent.\\n","tokens":80,"id":802,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe have an existing userbase that relies on `tamr_unify_client` and cannot painlessly make backwards-incompatible changes.\nBut, we want to rearchitect this codebase as a [library of composable functions](\/contributor-guide\/adr\/0005-composable-functions).\n\n## Decision\nImplement rearchitected design as a new package named `tamr_client`.\nRequire the `TAMR_CLIENT_BETA=1` feature flag for `tamr_client` package usage.\nWarn users who attempt to use `tamr_client` package to opt-in if they want to beta test the new design.\n\n\n## Problem\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\n(as they are designed to do so). Limitations of other storage plugins are\ne.g., that not every structure of configuration is allowed.\nSome of these limitations were documented `infos\/status`, others were not.\n\n## Decision\nAdd `infos\/features\/storage` to document limitations of storage plugins.\nIdeally, storage plugins should throw an error in `kdbSet` for\nunrepresentable KeySets.\nElektra cannot guarantee that any configuration file format can\nbe mounted anywhere.\nDevelopers, maintainers and administrators are responsible for what\nthey mount. They need to test the setup.\n- [Base Name](base_name.md)\n\n\n\nProvide a decision given the context below:\n## Context and Problem Statement\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\narbitrary style overrides on the component.\nWe want all components in FDS to follow a similar prop interface, so this decision should\napply to _all_ components.\n<end_of_turn>\n<start_of_turn>model\nChosen option: \"No components accept a `className`\", because...\n- It's too risky to introduce style overrides to low-level layout components.\n- Style customization should happen via props, where the component can be responsible for\nstyle changes and variations.\n### Positive Consequences <!-- optional -->\n- Prop API consistency across all components.\n### Negative Consequences <!-- optional -->\n- Consumers of FDS components must handle the breaking change. Any custom `className`\nprops on FDS components must be moved to a direct parent or direct child of the\ncomponent.\n<end_of_turn>\n"}
{"File Name":"green_log\/0003-decouple-generation-and-handling.md","Context":"## Context\\nWe want the logging API used by applications to be consistent, while allowing for logs to be filed, forwarded, filtered and formatted in a variety of ways.\\n","Decision":"De-couple generation of log message\/entries from how they are handled.\\n* A `logger` object provides an API that can be used to generate log entries.\\n* Log \"entries\" are strongly typed structures.\\n* Log entry \"handlers\" provide a simple, consistent interface.\\n```mermaid\\nsequenceDiagram\\nApp ->> Logger:       info(\"Message\")\\nLogger ->> Handler:   <<(entry)\\n```\\n","tokens":35,"id":1924,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [Stylelint](https:\/\/stylelint.io\/) is one of the more popular CSS\nlinters with support for SASS, and is easily configurable for our purposes.\n\n## Decision\nWe will check SASS syntax using Stylelint.\nWe will use the recommended configuration for plugins where possible.\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\n\n\n## Context\nWe want to be able to write log entries (to file, or STDOUT), without them being interleaved.\nBut also, we want logging to perform well.\n\n## Decision\n_Unlike_ the Ruby standard `Logger`, GreenLog will use a [lock-free logging](https:\/\/www.jstorimer.com\/blogs\/workingwithcode\/7982047-is-lock-free-logging-safe) approach. That is, we will:\n- avoid using of mutexes to serialise output\n- perform atomic writes to `IO` streams (using `<<`)\n\n\n\nProvide a decision given the context below:\n## Context\nWe want the logging API used by applications to be consistent, while allowing for logs to be filed, forwarded, filtered and formatted in a variety of ways.\n<end_of_turn>\n<start_of_turn>model\nDe-couple generation of log message\/entries from how they are handled.\n* A `logger` object provides an API that can be used to generate log entries.\n* Log \"entries\" are strongly typed structures.\n* Log entry \"handlers\" provide a simple, consistent interface.\n```mermaid\nsequenceDiagram\nApp ->> Logger:       info(\"Message\")\nLogger ->> Handler:   <<(entry)\n```\n<end_of_turn>\n"}
{"File Name":"publicservice-registry\/0013-handle-eventual-consistency-with-etags.md","Context":"## Context\\nThe UI might request a resource after it's creation or alteration, while the async projections are not yet up to date with this change.\\nIn that case, the user would see stale data without knowing why, perhaps thinking its changes were not accepted by the application.\\nTo prevent this, the client needs a mechanism to check whether or not its latest update has already been processed by the projections.\\n","Decision":"We will handle eventual consistency concerns using ETags. On succesful POST or PUT to the API, the client will receive a response containing an ETag header. This header will contain the highest event number caused by the client's request.\\nAll Projections will update their ProjectionState with this number.\\nIf a client wants to know if the resource is up to date with their changes, they will have to include the ETag in a `If-Match` header. Depending on whether the Projections is up to date or not, the API will return a `200 OK` or `412 PRECONDITION FAILED`.\\n","tokens":83,"id":3127,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nMany projections produce data that is only required for a limited period of\ntime.\nAn application developer needs to consider how long projection data will be\nretained and ideally implement measures to remove any unnecessary data.\nOften such data can be removed when some future event occurs. However, in some\ncases there is no future event that appropriately indicates the data is no\nlonger required.\nWe have encountered a genuine example of this when implementing an SQL\nprojection that inserts a row whenever a user performs a specific action. The\ndata is queried in order to enforce a \"cool down\" that prevents the user from\nrepeating that action again within a specific time frame.\nThe insert is triggered by the occurrence of an event, but the data becomes\nunnecessary whenever the \"cool down\" time has elapsed.\nIn this particular use case the \"cool down\" was not part of the business logic,\nbut rather an API level restriction. Hence, processes\/timeout messages were not\nthe appropriate solution.\n\n## Decision\nWe have decided to add a `Compact()` method to `ProjectionMessageHandler`.\nThe implementation of `Compact()` can modify the projection's data by whatever\nmeans is appropriate such that unnecessary data is removed but the projection\nstill serves its purpose.\n\n\n**Context**: operations that change state in the server may complete with an\nindeterminate status code. For example: an error in the 500 range may be\nproduced by a middle-tier server after the operation was started by the backend.\nRe-issuing the request may not work as expected for the application developer.\n**Decision**: by default the library will retry all requests, including\nnon-idempotent ones. The library will allow application developers to override\nthe definition of what operations are automatically retried.\n**Consequences**: most operations become easier to use for application\ndevelopers. In very rare cases the operation will result in double uploads, or\nin a new generation of the object or metadata being created. In even more rare\ncases the operation may fail, for example, an operation to create an object with\n`IfGenerationMatch(0)` would fail on the second attempt.\n\n## Decision\nnon-idempotent ones. The library will allow application developers to override\nthe definition of what operations are automatically retried.\n**Consequences**: most operations become easier to use for application\ndevelopers. In very rare cases the operation will result in double uploads, or\nin a new generation of the object or metadata being created. In even more rare\ncases the operation may fail, for example, an operation to create an object with\n`IfGenerationMatch(0)` would fail on the second attempt.\n\n\n\nProvide a decision given the context below:\n## Context\nThe UI might request a resource after it's creation or alteration, while the async projections are not yet up to date with this change.\nIn that case, the user would see stale data without knowing why, perhaps thinking its changes were not accepted by the application.\nTo prevent this, the client needs a mechanism to check whether or not its latest update has already been processed by the projections.\n<end_of_turn>\n<start_of_turn>model\nWe will handle eventual consistency concerns using ETags. On succesful POST or PUT to the API, the client will receive a response containing an ETag header. This header will contain the highest event number caused by the client's request.\nAll Projections will update their ProjectionState with this number.\nIf a client wants to know if the resource is up to date with their changes, they will have to include the ETag in a `If-Match` header. Depending on whether the Projections is up to date or not, the API will return a `200 OK` or `412 PRECONDITION FAILED`.\n<end_of_turn>\n"}
{"File Name":"libelektra\/holes.md","Context":"## Problem\\nConfig files ideally do not copy any structure if they only want to\\nset a single key.\\n","Decision":"Support holes and values for non-leaves in a KeySet if the underlying format allows it.\\nIf the underlying format does not support it and there is also not an obvious\\nway how to circumvent it -- e.g., JSON which does not have comments -- holes and\\nvalues in non-leaves can be supported with key names starting with \u00aeelektra.\\n","tokens":23,"id":1307,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe want to be able to write log entries (to file, or STDOUT), without them being interleaved.\nBut also, we want logging to perform well.\n\n## Decision\n_Unlike_ the Ruby standard `Logger`, GreenLog will use a [lock-free logging](https:\/\/www.jstorimer.com\/blogs\/workingwithcode\/7982047-is-lock-free-logging-safe) approach. That is, we will:\n- avoid using of mutexes to serialise output\n- perform atomic writes to `IO` streams (using `<<`)\n\n\n## Problem\nOn structures like maps or [arrays](array.md) there are different\npossibilities which keys are looked up in the KeySet and which\nare simply iterated.\nWithout any guidelines, applications would provide arbitrary inconsistent\nbehavior.\n\n## Decision\nEvery key that an application wants to use, must be looked up\nwith `ksLookup` using a cascading lookup key.\n- [Arrays](array.md)\n\n\n\nProvide a decision given the context below:\n## Problem\nConfig files ideally do not copy any structure if they only want to\nset a single key.\n<end_of_turn>\n<start_of_turn>model\nSupport holes and values for non-leaves in a KeySet if the underlying format allows it.\nIf the underlying format does not support it and there is also not an obvious\nway how to circumvent it -- e.g., JSON which does not have comments -- holes and\nvalues in non-leaves can be supported with key names starting with \u00aeelektra.\n<end_of_turn>\n"}
{"File Name":"Nosedive\/0005-sqlite-database.md","Context":"## Context\\nOne of the final stack canditate for persistence is [cockroachdb](https:\/\/www.cockroachlabs.com\/), but the windows version\\n","Decision":"We remove all complexity at the beginnign using sqlite. As cockorachdb is SQL compatible, we not expect difficults to migrate\\n","tokens":33,"id":109,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe wish to build a RESTful API, and to be technically RESTful, we must implement HATEOAS. This gets us to stage 3 of the [Richardson Maturity Model](https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html)\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\n\n## Decision\nOur API is to be as RESTful as it can be.\nWe recognise that we must implement Hypermedia as the Engine of Application State.\nThe content structure already discussed in [0005-content-structure.md](0005-content-structure.md) implements links within our resource object... this ADR is to explicitly state our aim is to implement HATEOAS.\nFurthermore, for every endpoint on the API we will implement an OPTIONS verb, returning every action possible at that endpoint.\nFor full context \/ a HATEOAS primer, [read the supporting notes](..\/supporting-notes\/hateoas.md)\n\n\n## Context\n[Starting with macOS Catalina, zsh will be the default shell](https:\/\/support.apple.com\/en-us\/HT208050).\n\n## Decision\nzsh should be used in place of bash.\n\n\n\nProvide a decision given the context below:\n## Context\nOne of the final stack canditate for persistence is [cockroachdb](https:\/\/www.cockroachlabs.com\/), but the windows version\n<end_of_turn>\n<start_of_turn>model\nWe remove all complexity at the beginnign using sqlite. As cockorachdb is SQL compatible, we not expect difficults to migrate\n<end_of_turn>\n"}
{"File Name":"content-publisher\/0008-restricted-deletion.md","Context":"## Context\\nWe use foreign keys to help enforce database integrity e.g.\\n```\\n# ensure creator_id on a document matches a row in the users table\\n# when we try to delete a user, raise an error if its the creator of a document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :restrict\\n```\\nThere are other `on_delete` behaviours instead of `restrict`.\\n```\\n# when we delete a user, set creator_id to NULL on all of its documents\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :nullify\\n# when we delete a user, also delete all of its document\\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :cascade\\n```\\nDifferent options are appropriate depending on the foreign key.\\n* `restrict` is a good choice in a child\/parent scenario, where deleting a child should be restricted if it would make the parent invalid. For example, we should not be able to delete an ActiveStorage blob if its parent image still exists.\\n* `nullify` works well if the relationship is optional. For example, our code doesn't currently require a document to have a creator, so it might be reasonable to `creator_id` on document to NULL if we delete a user for some reason.\\n* `cascade` is a good choice in a parent\/child scenario, where it makes sense to delete the child when we delete the parent. For example, if we delete a document for some reason, it also makes sense to delete the editions.\\n","Decision":"Currently, we don't have any real-world scenarios for deleting stuff from the database. Therefore, we should do the simplest thing and `restrict` on all foreign key delete actions. `restrict` is actually the default, but we should continue to specify it explicitly to indicate a conscious decision.\\n","tokens":341,"id":4872,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nAs a result of running the feature-compare process a bunch of files are produced. These files are essentially a _diff_ between what the operator started out with, that is the dump \/ extract he \/ she started working from, and what they ended up with, after having edited shapes and associated data in their editor. Some of those files have a `_ALL.DBF|_ALL.SHP` suffix. These files are the files we base our logic on to validate and translate to an _internal_ change request. Most notably, these files have an extra column called `RECORDTYPE` which, as the name suggests, classifies a record as either being `IDENTICAL`, `ADDED`, `MODIFIED` or `REMOVED`. Next to that most of the files contain, per record, the equivalent of a primary key that identifies a row. This primary key is used in other files as a foreign key to reference a record sitting in another file. Records with a `RECORDTYPE` of `ADDED` will use a temporary identifier (a really big number that we assume is free to use) to make this work (see [004-temporary-and-permanent-identifiers.md](004-temporary-and-permanent-identifiers.md) for how that works). One gotcha is that the primary keys are not always unique, that is, they can appear multiple times in the `*_ALL.DBF|*_ALL.SHP` files, once for each record type. A common scenario is a modification represented as a removal and an addition record.\nFor the `WEGSEGMENT_ALL.DBF` file, things are more complicated ... next to having a `WS_OIDN` column act as primary key it has a `EVENTIDN` column acting as an alternative primary key in some cases. In case the `RECORDTYPE` is `ADDED` and the `EVENTIDN` has a value differing from `0`, the `WS_OIDN` column refers to an existing road segment and the `EVENTIDN` column refers to its new representation. In such a case, other files refer to a road segment by the value found in the `EVENTIDN`, not by the value in `WS_OIDN`. Alas, such is life ...\n\n## Decision\nModifying a road segment involves data from `WEGSEGMENT_ALL.DBF`, `WEGSEGMENT_ALL.SHP` and `ATTRIJSTROKEN_ALL.DBF`, `ATTWEGBREEDTE_ALL.DBF`, `ATTWEGVERHARDING_ALL.DBF` - that is, when it is represented as an _internal_ change request command. Each of those `.DBF` files contains a `RECORDTYPE` column. As such, a road segment could be marked as identical in `WEGSEGMENT_ALL.DB`, yet it's lanes, width and \/ or surfaces could be marked as a mixture of modified, removed, added, identical. This is the reason why a road segment that is identical is appended as a `provisional` change to the list of translated changes. Because we're not sure, just yet, that it is an actual change. The order in which these files are translated causes a `provisional` change, if warranted, to be promoted to an actual change.\nWhy go thru all this trouble? Well, lanes, widths, and surfaces are tightly coupled to the geometry of a road segment such that it makes sense to capture them as a holistic change rather than as individual fragmented changes. There's still a bigger debate to be had about which pieces of data change together ...\n\n\n## Context\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\nOnly the owner of a table can alter\/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\nWe attempted to work around the PostgreSQL permissions system in the following ways:\n* Using [`ALTER DEFAULT PRIVILEGES`](https:\/\/www.postgresql.org\/docs\/9.5\/static\/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https:\/\/www.postgresql.org\/docs\/9.5\/static\/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\n\n## Decision\nWe decided the simplest solution was to issue the same credentials to all applications bound to the database. This means that the same user will be used by each application, and they will not suffer ownership problems when issuing `DROP` or `ALTER` commands.\nRather than deterministically deriving the credentials we decided to store them in the backing database itself meaning they can be changed at any time in case of a breach. To protect against database breaches that may leak stored credentials we also decided to encrypt the credentials using a symmetric key held by the broker.\nWe accepted that in the future we may wish to extend this by allowing the broker to issue read-only credentials, or credentials that are suitable for `SELECT`, `INSERT`, and `DELETE` operations, but not schema modifications.\n\n\n\nProvide a decision given the context below:\n## Context\nWe use foreign keys to help enforce database integrity e.g.\n```\n# ensure creator_id on a document matches a row in the users table\n# when we try to delete a user, raise an error if its the creator of a document\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :restrict\n```\nThere are other `on_delete` behaviours instead of `restrict`.\n```\n# when we delete a user, set creator_id to NULL on all of its documents\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :nullify\n# when we delete a user, also delete all of its document\nadd_foreign_key \"documents\", \"users\", column: \"creator_id\", on_delete: :cascade\n```\nDifferent options are appropriate depending on the foreign key.\n* `restrict` is a good choice in a child\/parent scenario, where deleting a child should be restricted if it would make the parent invalid. For example, we should not be able to delete an ActiveStorage blob if its parent image still exists.\n* `nullify` works well if the relationship is optional. For example, our code doesn't currently require a document to have a creator, so it might be reasonable to `creator_id` on document to NULL if we delete a user for some reason.\n* `cascade` is a good choice in a parent\/child scenario, where it makes sense to delete the child when we delete the parent. For example, if we delete a document for some reason, it also makes sense to delete the editions.\n<end_of_turn>\n<start_of_turn>model\nCurrently, we don't have any real-world scenarios for deleting stuff from the database. Therefore, we should do the simplest thing and `restrict` on all foreign key delete actions. `restrict` is actually the default, but we should continue to specify it explicitly to indicate a conscious decision.\n<end_of_turn>\n"}
{"File Name":"amf\/0003-new-annotation-removal-stage-present-in-all-webapi-pipelines.md","Context":"## Context\\nWhen referencing external files there are certain cases where AMF inlines the parsed content without creating a links (this is because the targeted elements are not present in the references of the base unit).\\nFor these cases, when a emitting an unresolved model these references are being emitted inlined.\\n","Decision":"In order to avoid emitting these references inlined for an unresolved model, we must make use of annotation to save the original reference.\\nWhen saving this reference, we must make sure that if the model is resolved this annotation is no longer present so that the emitter does not render references for a flattened model.\\nThis leads to the creation of a new resolution stage that removes specific annotations from the model that must not be present in a resolved base unit.\\n","tokens":61,"id":1393,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n# 5 Utility Context Lazily Initialized\n## Context\nThe [synchronous utils](..\/..\/src\/adr_util_sync.js) are intended to be used by commands to work on ADRs, with the given context of the project, e.g. the [adr marker file](1-ADR_directory_identified_by_marker_file.md).\nSince the context is mandatory (the ADR directory, existing ADRs), it is initialized at the beginning, in the object's constructor.\nSeveral commands (scripts) already include that script, and initialize the object on the script loading, as part of the `require` statement.\nThis resulted in loading the context whenever a command is loaded.\nThis might have performance implications, but the more immediate functional lacuna is that this then assumes that the context *can* be loaded every time.\nOf course, this assumption breaks - when running the `init` command, where by definition there is no context to load - the `init` command creates that.\nThis resulted in trying to run the `init` command, on an existing directory, in an error.\n\n## Decision\nThe ADR project context will be loaded lazily, only when needed.\nThis assumes the `init` command doesn't require it, and hence will not invoke any commands that require that context (e.g. the existing files).\nSo any command that is loaded can still create the context object and create an instance of, but the actual context will be loaded easily.\nThis assumption seems safe enough since `init` command indeed creates the context (the `.adr` file).\nAnd this will probably hold for similar commands.\nCommands that will require the ADR context, will continue to use it as today, and it will be loaded lazily.\nWe need of course to make sure that the context isn't created\/calculated every time it is called; e.g. searching for the ADR dir every time one of the utility functions is called. In other words - memoize the context-creating functions.\n\n\n## Context\nIdentifiers (the names and keys used to identify applications and handlers) must\nbe compared by engines to determine if two such entities are to be considered\nequivalent.\nThe documentation specifies that such keys must be non-empty UTF-8 strings\nconsisting of printable characters without whitespace, but it did not previously\nspecify how such strings would be compared.\nThese identifiers are either mostly or entirely immutable and generated as part\nof the source code. They do not need to be parsed and validated from user input.\n\n## Decision\nIn keeping with current behavior, we've decided to specify byte-wise comparison\nsemantics for identifiers.\n\n\n\nProvide a decision given the context below:\n## Context\nWhen referencing external files there are certain cases where AMF inlines the parsed content without creating a links (this is because the targeted elements are not present in the references of the base unit).\nFor these cases, when a emitting an unresolved model these references are being emitted inlined.\n<end_of_turn>\n<start_of_turn>model\nIn order to avoid emitting these references inlined for an unresolved model, we must make use of annotation to save the original reference.\nWhen saving this reference, we must make sure that if the model is resolved this annotation is no longer present so that the emitter does not render references for a flattened model.\nThis leads to the creation of a new resolution stage that removes specific annotations from the model that must not be present in a resolved base unit.\n<end_of_turn>\n"}
{"File Name":"pace-developers\/0008-pace-syntax-guiding-principles.md","Context":"## Context\\nWhilst the core framework of PACE is the Horace (and Herbert) projects,\\nPACE involves many other codes, including [Euphonic](https:\/\/github.com\/pace-neutrons\/Euphonic),\\n[Brille](https:\/\/github.com\/brille\/brille) and [SpinW](https:\/\/spinw.org).\\nThe way in which these programs interact with each other is presently unclear.\\nFurthermore the programs are written in several different languages.\\nIn this ADR we propose some guiding principles in the design of the interfaces\\nbetween the programs or components which make up PACE.\\nThis was discussed at a meeting whose\\n[minutes are here.](https:\/\/stfc365.sharepoint.com\/:w:\/r\/sites\/PACEProject\/Shared%20Documents\/Meetings\/PACE-General\/pace_syntax_meeting1_minutes.docx))\\n","Decision":"The guiding principle for PACE-supported programs is **commonality**.\\nThat is, interacting with the different programs of PACE should be **seamless**\\nto the user, and the user interface presented to them should be **uniform**.\\nThe uniform interface means that function names and how users specify parameters\\nor arguments to functions in Matlab and Python flavours of PACE should align\\nas closely as possible.\\n","tokens":183,"id":5178,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## [Context](https:\/\/docs.google.com\/document\/d\/1zRy8OCZ_JJpbs6scwrvx5PYkWwaWbR-5DLuq3AujhBA\/edit)\nLibero products need automated builds for pull requests and release candidates.\nTravis CI has been [acquired](https:\/\/techcrunch.com\/2019\/01\/23\/idera-acquires-travis-ci\/) by a private equity firm and has an [uncertain future](https:\/\/twitter.com\/ReinH\/status\/1098663375985229825). We also have run into performance bottlenecks of [5 concurrent jobs](https:\/\/travis-ci.com\/plans) in the open source offer.\nGithub Actions is a Github-native general workflow system that can provide CI\/CD capabilities; it has been in general availability [since November](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/). It offers fully managed, sandboxed environments and a per-repository limit to concurrency.\n\n## Decision\nWe will create Github Actions builds for all new projects.\nWe will port existing Travis CI builds of maintained projects onto Github Actions, on an as-needed basis.\n\n\n## Context\nWe want to ensure we're all using one code style, that is familiar across\nprojects. [Prettier](https:\/\/prettier.io\/) is an opinionated code formatter with\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\nof writing, it is used by over\n[1 million repositories](https:\/\/github.com\/prettier\/prettier\/network\/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\non GitHub, including React itself, and has become a standard.\n\n## Decision\nWe will enforce that everything supported by Prettier has its style enforced by\nit.\nWe will set up Git hooks to automatically run the formatter before committing.\nWe will set continuous integration up to reject commits that are not correctly\nformatted.\n\n\n\nProvide a decision given the context below:\n## Context\nWhilst the core framework of PACE is the Horace (and Herbert) projects,\nPACE involves many other codes, including [Euphonic](https:\/\/github.com\/pace-neutrons\/Euphonic),\n[Brille](https:\/\/github.com\/brille\/brille) and [SpinW](https:\/\/spinw.org).\nThe way in which these programs interact with each other is presently unclear.\nFurthermore the programs are written in several different languages.\nIn this ADR we propose some guiding principles in the design of the interfaces\nbetween the programs or components which make up PACE.\nThis was discussed at a meeting whose\n[minutes are here.](https:\/\/stfc365.sharepoint.com\/:w:\/r\/sites\/PACEProject\/Shared%20Documents\/Meetings\/PACE-General\/pace_syntax_meeting1_minutes.docx))\n<end_of_turn>\n<start_of_turn>model\nThe guiding principle for PACE-supported programs is **commonality**.\nThat is, interacting with the different programs of PACE should be **seamless**\nto the user, and the user interface presented to them should be **uniform**.\nThe uniform interface means that function names and how users specify parameters\nor arguments to functions in Matlab and Python flavours of PACE should align\nas closely as possible.\n<end_of_turn>\n"}
{"File Name":"TANF-app\/012-antivirus-strategy.md","Context":"## Context\\nOur original plan was to upload files to S3 using signed URLs from S3, and checking the files' headers to determine whether or not it was, in fact, a text file. Further research has revealed that there is no fool proof way of doing this and a malicious user would be able to spoof a virus as a text file.\\n","Decision":"Instead of using a signed URL and sending the file directly to S3, we will instead send the file to the backend and scan it with Clam AV before sending it to S3. In the event there is a virus, we will destroy the file on the backend immediately and return an error to the frontend.\\nBy using the [ClamAV REST server](https:\/\/github.com\/ajilaag\/clamav-rest) implementation we are able to scan files for viruses and malicious behavior. Additionally, Anti-Virus definitions are kept up to date automatically by use of the included [freshclam](https:\/\/www.clamav.net\/documents\/signature-testing-and-management#freshclam) tool which automatically downloads and updates an internal database of virus signatures using the official ClamAV source.\\nIn addition to this, the frontend is able to reliably determine if a file is a binary file. The client will check submitted files for this and immediately return an error to the user. These files will not be sent to the backend.\\n","tokens":72,"id":3797,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nThe verify-service-provider will be deployed to a number of relying parties.\nWhich relying party is using which version could potentially become difficult\nto keep track of.\nTo make it as easy as possible for us to manage this we'd like the\nverify-service-provider to report its version in some way.\nBecause the verify-service-provider is not intended to be accessible to the\ninternet we can't simply expose an endpoint that reports the version number.\nAlso, since the SAML messages go via the browser we can't use a custom HTTP\nheader.\nThere's also a concern about the security implications of reporting a version\nnumber in cleartext.\nWe considered a couple of options:\n- Requesting metadata from Verify with a custom user-agent string\n- Sending the version in an unencrypted saml extension\n- Sending the version in an encrypted saml extension\n\n## Decision\nWe decided to send the version number in the SAML AuthnRequests as an encrypted\nSAML extension. The XML will look roughly like this:\n```\n<saml:AuthnRequest>\n<saml:Issuer>...<\/saml:Issuer>\n<saml:Signature>...<\/saml:Signature>\n<saml:Extensions>\n<saml:EncryptedAttribute>...<\/saml:EncryptedAttribute>\n<\/saml:Extensions>\n<\/saml:AuthnRequest>\n```\nOnce decrypted, the Attribute in the Extensions will look like:\n```\n<saml:Attribute Name=\"Versions\">\n<saml:AttributeValue xsi:type=\"metrics:VersionsType\">\n<metrics:ApplicationVersion>3.4.1<\/metrics:ApplicationVersion>\n<\/saml:AttributeValue>\n<\/saml:Attribute>\n```\n\n\n## Context\nEvery environment the application is available in, be that development, review,\nstaging or public are openly available to anybody. There is no access control\nat all.  This isn't suitable for a site that could be misinterpreted as the\n'real' version either by people or by search engines and other bots. There\nneeds to be some form of hurdle to overcome in order to prompt people to\nconsider whether this is the correct site. There should also be a barrier to\nprevent access by bots.\n\n## Decision\nWe have decided to use basic authentication on the service in all environments\napart from the public facing one. This will only be applicable to the\nenvironments hosted within the Rancher environment. The username and password\nwill not be secret and will be included within the `README` of the application.\n\n\n\nProvide a decision given the context below:\n## Context\nOur original plan was to upload files to S3 using signed URLs from S3, and checking the files' headers to determine whether or not it was, in fact, a text file. Further research has revealed that there is no fool proof way of doing this and a malicious user would be able to spoof a virus as a text file.\n<end_of_turn>\n<start_of_turn>model\nInstead of using a signed URL and sending the file directly to S3, we will instead send the file to the backend and scan it with Clam AV before sending it to S3. In the event there is a virus, we will destroy the file on the backend immediately and return an error to the frontend.\nBy using the [ClamAV REST server](https:\/\/github.com\/ajilaag\/clamav-rest) implementation we are able to scan files for viruses and malicious behavior. Additionally, Anti-Virus definitions are kept up to date automatically by use of the included [freshclam](https:\/\/www.clamav.net\/documents\/signature-testing-and-management#freshclam) tool which automatically downloads and updates an internal database of virus signatures using the official ClamAV source.\nIn addition to this, the frontend is able to reliably determine if a file is a binary file. The client will check submitted files for this and immediately return an error to the user. These files will not be sent to the backend.\n<end_of_turn>\n"}
{"File Name":"dos-server\/adr-2a-java.md","Context":"## Context\\nAs DOS will be an important component of our collections platform,\\nthe software stack (web server, application container, programming language, database)\\nDOS uses to realize core functionality and to fulfill non-functional requirements\\nof scalability, performance, reliability, security, and backward compatibility needs to be robust. Java can help meet various implicit non-functional requirements out of the box. Similarly, the use of proven and\\nmature ecosystem libraries can help meet the functional requirements easily. The Spring\\nframework also makes it relatively easy to create RESTful web services.\\n","Decision":"The core service will rely on Java and Spring framework as the basic stack for implementing core services.\\n","tokens":114,"id":3374,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\n[Hexagonal architecture](0003-adopt-hexagonal-architecture.md) requires inversion of control to inject infrastructure\nservices implementations dependencies into the services defined in the core hexagon.\nREST API implementation requires a dedicated library to define incoming adapters handling the HTTP resources.\nSpring is a well established framework for Java. It is non-invasive and provides multiple features such as IoC, AOP,\nREST services implementation, security that will help speed up implementation in a cohesive way. The author has also used\nSpring for many years and masters many of the provided features.\n\n## Decision\nSpring framework will be the backbone for `menu-generation` application.\n\n\n## Context and Problem Statement\n* The current API implementations is spread across multiple modules (such as webapp, config, db and query) without a formal definition of the inputs\/outputs. This induces inconsistencies and we foresee technical difficulties to maintain and evole the service.\n* We would like to use an interface description language to define the API between the various component (such as between the webapi and webclients, worker and database).\n* How do we define and maintain the interfaces?\n\n## Decision\nChosen option: \"Protobuf\", because it comes out best (see below).\n### Positive Consequences\n- We need to start defining interfaces outside of their implementations.\n- We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.\nFor example, use `POST \/get_projects BODY { \"index\": \"name\" }` instead of `GET \/projects?index=name`.\n### Negative Consequences\n- The team needs to learn a new language.\n- Follow-up decisions required for the actual implementation.\n\n\n\nProvide a decision given the context below:\n## Context\nAs DOS will be an important component of our collections platform,\nthe software stack (web server, application container, programming language, database)\nDOS uses to realize core functionality and to fulfill non-functional requirements\nof scalability, performance, reliability, security, and backward compatibility needs to be robust. Java can help meet various implicit non-functional requirements out of the box. Similarly, the use of proven and\nmature ecosystem libraries can help meet the functional requirements easily. The Spring\nframework also makes it relatively easy to create RESTful web services.\n<end_of_turn>\n<start_of_turn>model\nThe core service will rely on Java and Spring framework as the basic stack for implementing core services.\n<end_of_turn>\n"}
{"File Name":"front-end-monorepo\/adr-21.md","Context":"## Context\\nAuthentication is currently handled by the existing auth client, which is bundled up as part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/).\\nWhile working on [#1306](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306), I ran into a few issues with it:\\n- Environment variables (env vars) aren't available on the client side. Next.js does have a method for sharing config on both the server and client, but it's academic since:\\n- The only way to configure `panoptes-javascript-client` is _directly_ via env vars, or by passing in query parameters to the URL. We can't get env vars on the client, so that's out, and query parameters become unwieldy very quickly.\\nThis hasn't been an issue when working with Single Page Apps, since env vars are baked in at transpilation time by the build tools. At the moment, we just avoid the problem by having the staging build of the project use the production API.\\nHowever, for the Next.js-based apps we're building right now, we need a different approach, and being able to configure the auth client from a single source of truth is required. Ideally, this would be source-agnostic: the client should be able to be configured from a config file, [env vars](https:\/\/12factor.net\/config), or whatever you want, but that's up to the consuming app to decide.\\n","Decision":"Rewrite the existing auth client as a separate package. The new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.\\nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.\\nThe [existing config](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/blob\/master\/lib\/config.js) will be turned into a separate package. Config settings can be imported wholesale from that for convenience.\\nFor use in the rebuild apps, we would create a `ConfigStore` which we populate with the relevant config settings. The `ConfigStore` snapshot is then used to rehydrate the client, so we get a single source of truth on both client and server.\\n","tokens":315,"id":527,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.\nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse\/Panoptes-Front-End#4992] (https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/issues\/4992)).\nThe default behavior in existing libraries like [JSON8](https:\/\/github.com\/sonnyp\/JSON8\/tree\/master\/packages\/json8#ooserialize) or MobX's [`toJS`](https:\/\/mobx.js.org\/refguide\/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object. However, for us, we are using maps for workflow steps because a key requirement is the ordering of key-value pairs, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.\n\n## Decision\nWe decided to instead implement our own [type conversion utility function](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/store\/utils\/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:\n``` js\nconst workflow = {\nid: '1',\nsteps: [['S1', { taskKeys: ['T1', 'T2'] }], ['S2', { taskKeys: ['T3'] }]] \/\/ How they will be stored on Panoptes\n}\n```\nAnd when a workflow request is received by the classifier store, it is converted by Mobx-State-Tree into an observable map when added to the store.\n**A note about the use of arrays for the key-value pairs**\nSubject locations are an array of objects. It would make sense to do an array of objects here too, however the array of two values is closest to the format expected by maps when you instantiate them: https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Map\nSo there's less type conversion happening if we store it this way. MobX will take objects too when setting an observable map, but if we ever want to instantiate an ES6 map independent of the store we would have to do another conversion from object to array of the key-value pair.\n\n\n## Context and Problem Statement\nThe [Settings Redesign project](https:\/\/jira.mozilla.com\/browse\/FXA-840) will be created [as a new React application](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) and in turn, has opened the door to assess certain pieces of our technology stack.\n[GraphQL](https:\/\/graphql.org\/), or GQL, is not a database query language, but is instead a query language for APIs. It _describes_ data requirements, and is a powerful alternative to REST. Some benefits can be gained by using it on top of existing REST architecture.\n\"Apollo\" in this document refers to the \"Apollo client\" and \"Apollo server\" pieces of the Apollo platform\u00b9, which can be described as a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\nThis ADR serves to lay out pros and cons of using GraphQL and Apollo in the Settings Redesign project as an alternative to hitting our conventional REST endpoints.\n\u00b9Apollo also offers Apollo Graph Manager and Apollo Federation which are paid services, [read more from their docs](https:\/\/www.apollographql.com\/docs\/intro\/platform\/). We do not need to use these to use GQL with Apollo server or Apollo client.\n## Decision Drivers\n- Performance implications; consideration around the number of network requests and data transferred\n- Ease of setup for client-side API calls\n- Clarity around expected data, React integration, and developer tooling\n- Development speed, both around initial setup and as new features roll out\n\n## Decision\n- Performance implications; consideration around the number of network requests and data transferred\n- Ease of setup for client-side API calls\n- Clarity around expected data, React integration, and developer tooling\n- Development speed, both around initial setup and as new features roll out\nChosen option: \"B - Layer GraphQL on top of our REST architecture\", because:\n- GQL offers performance optimizations by allowing us to consolidate our network requests by sending data requirements up in a single request, asking for only what is needed on the client, shifting the burden of figuring out how and where to gather this data from the client onto the server. The server then compensates for overfetching by only sending back what is requested.\n- Allowing developers to query for and expect only exactly what is needed with end-to-end typing and a more \"declarative\" way of thinking towards data requirements, along with keeping data requirement schemas close to where they're consumed, makes it painfully clear what's being sent and received between client and server.\n- Does not preclude option C, as we can replace or supplement direct calls to the FxA auth-server down the line. This is faster for initial development, and this option over option C will also help mitigate risk around using a relatively novel piece of technology for FxA.\n- Nice side-effects include the GQL playground, managing only a single API endpoint, and the ability to store both local state and network data in the Apollo cache.\n\n\n\nProvide a decision given the context below:\n## Context\nAuthentication is currently handled by the existing auth client, which is bundled up as part of [panoptes-javascript-client](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/).\nWhile working on [#1306](https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1306), I ran into a few issues with it:\n- Environment variables (env vars) aren't available on the client side. Next.js does have a method for sharing config on both the server and client, but it's academic since:\n- The only way to configure `panoptes-javascript-client` is _directly_ via env vars, or by passing in query parameters to the URL. We can't get env vars on the client, so that's out, and query parameters become unwieldy very quickly.\nThis hasn't been an issue when working with Single Page Apps, since env vars are baked in at transpilation time by the build tools. At the moment, we just avoid the problem by having the staging build of the project use the production API.\nHowever, for the Next.js-based apps we're building right now, we need a different approach, and being able to configure the auth client from a single source of truth is required. Ideally, this would be source-agnostic: the client should be able to be configured from a config file, [env vars](https:\/\/12factor.net\/config), or whatever you want, but that's up to the consuming app to decide.\n<end_of_turn>\n<start_of_turn>model\nRewrite the existing auth client as a separate package. The new client will follow a pattern that's well established in the wider ecosystem: the package exposes a function which accepts a config object as an argument and returns a configured client.\nThe client is purely for authentication, and will expose the methods for registration, sign in, sign out, and getting bearer tokens.\nThe [existing config](https:\/\/github.com\/zooniverse\/panoptes-javascript-client\/blob\/master\/lib\/config.js) will be turned into a separate package. Config settings can be imported wholesale from that for convenience.\nFor use in the rebuild apps, we would create a `ConfigStore` which we populate with the relevant config settings. The `ConfigStore` snapshot is then used to rehydrate the client, so we get a single source of truth on both client and server.\n<end_of_turn>\n"}
{"File Name":"bfi-discovery\/0002-choice-of-auth-provider.md","Context":"## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\n","Decision":"* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno \/ minimal costs.\\nAuth0 is selected as the solution, due to its managed nature requiring\\nzero deployment and effort. After an evaluation of features compared\\nagainst the project requirements, it has been determined that Auth0's\\nfree tier will suffice and as a result there are no ongoing costs\\neither.\\n### Positive Consequences\\n* As a managed solution, there are no initial deployment or ongoing\\nhosting \/ infrastructure costs.\\n* Patching and maintenance of the solution is provided by Auth0 at no\\nadditional cost \/ interruption to the customer.\\n* It is expected that (at least initially) the free tier will suffice to\\nmeet the requirements of the project.\\n### Negative Consequences\\n* The features and customisations available are limited without\\nupgrading to a paid tier:\\n* Email templates cannot be customised on the free plan.\\n* A custom domain cannot be used to serve the Auth0 tenant.\\n* Enterprise integrations (AD, LDAP, etc) are not available.\\n* Multifactor authentication is not available.\\n* Auth0 hosted UI's can be customised but only to a limited extent.\\n* Limit of 7000 monthly active users.\\n","tokens":96,"id":4857,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context and Problem Statement\nIn order to simplify the tracking consent build and deploy process and\nmake integrating with tracking consent less surprising, should\nthe configuration of the GTM container used by tracking consent be via\ndata attributes rather than separate bundles?\n## Decision Drivers\n* The need to keep things simple for service developers\n* The need to improve the operability of tracking consent\n\n## Decision\n* The need to keep things simple for service developers\n* The need to improve the operability of tracking consent\nChosen option: \"Use a data attribute\", because based on the benefits listed below the team\nbelieves this is the best way forward.\n### Positive Consequences\n* The Javascript bundle creation process is simplified.\n* The Scala Play routing is simplified\n* Only one endpoint needs to be managed in production\n* Future additional containers can be supported more easily\n* Service developers only have to configure a single URL to tracking consent and use\na data attribute to configure the container.\n* The central common configuration repo (owned by a separate team) only requires a single URL to\ntracking consent defining for each environment, rather than one for each GTM container.\n### Negative Consequences\n* Service developers need to add id=\"tracking-consent-script-tag\" to the SCRIPT\ntag when integrating.\n\n\n## Context\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage\/SEPA users).\n## Decision Drivers\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\n\n## Decision\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\n[Option 1] SEPA should look to integrate Azure Active Directory with Dynamics to provide a shared identity 'federation', where backstage (SEPA) users are able to access Dynamics data via the WebAPI & perform CRUD operations using their existing single sign on login.\nDynamics users would therefore match the users within the Active Directory, and SEPA would require a Dynamics license for each SEPA user.\nIf SEPA's Active Directory is managed on premises, Azure AD connect can be used to auto sync existing Active Directory accounts to the Azure setup.\n### Positive Consequences\n* Reuse of existing SEPA logins\n* Better backstage user experience\n### Negative Consequences\n* If Azure Active Directory connect is used to sync accounts, this would need to be monitored & managed by SEPA.\n\n\n\nProvide a decision given the context below:\n## Context and Problem Statement\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\nidentity and access management solution must be provisioned which\nsupports user creation and registration, user management, and\nauthentication and authorisation.\n## Decision Drivers\n* Ease of initial deployment and configuration of the solution.\n* Ongoing hosting and maintenance costs of the solution.\n* Availability of core features to satisfy the project requirements with\nno \/ minimal costs.\n<end_of_turn>\n<start_of_turn>model\n* Ease of initial deployment and configuration of the solution.\n* Ongoing hosting and maintenance costs of the solution.\n* Availability of core features to satisfy the project requirements with\nno \/ minimal costs.\nAuth0 is selected as the solution, due to its managed nature requiring\nzero deployment and effort. After an evaluation of features compared\nagainst the project requirements, it has been determined that Auth0's\nfree tier will suffice and as a result there are no ongoing costs\neither.\n### Positive Consequences\n* As a managed solution, there are no initial deployment or ongoing\nhosting \/ infrastructure costs.\n* Patching and maintenance of the solution is provided by Auth0 at no\nadditional cost \/ interruption to the customer.\n* It is expected that (at least initially) the free tier will suffice to\nmeet the requirements of the project.\n### Negative Consequences\n* The features and customisations available are limited without\nupgrading to a paid tier:\n* Email templates cannot be customised on the free plan.\n* A custom domain cannot be used to serve the Auth0 tenant.\n* Enterprise integrations (AD, LDAP, etc) are not available.\n* Multifactor authentication is not available.\n* Auth0 hosted UI's can be customised but only to a limited extent.\n* Limit of 7000 monthly active users.\n<end_of_turn>\n"}
{"File Name":"copilot\/0003-use-mcp-api-for-pilot-config.md","Context":"## Context\\nMesh Configuration Protocol (MCP) is a [protocol](https:\/\/github.com\/istio\/api\/tree\/master\/mcp) for transferring configuration among Istio components during runtime. MCP is meant to defer all the logics and complexities back to the server (copilot) as oppose to the original design which all the logic was embeded in the client (Pilot). Another goal of MCP is to create a unified contract for all the Custom Resource Definitions and Service Discovery and the way they are communicated with Pilot.\\n","Decision":"Copilot will implement a MCP server to send configuration to Pilot. We will be sending definitions for Gateways, VirtualServices and DestinationRules over bi-directional GRPC.\\n","tokens":106,"id":49,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nJames servers offer a command-line interface in order to interact with the server. However, it relies on the JMX protocol, which is known to be insecure. The JMX server embedded in Apache James, also used by the command line client is exposed to a java de-serialization issue according to [NVD-CVE-2017-12628 Detail](https:\/\/nvd.nist.gov\/vuln\/detail\/CVE-2017-12628), and thus can be used to execute arbitrary commands.\nBesides, the current CLI interface is also not optimal for users. It places actions in front of entities with contiguous syntax, making it harder for the user to remember the command (for example, which entity the GET action command can interact with). If we design to place the entity first and the outgoing actions can interact with that entity afterward, the user will easily imagine what he\/she can do with each entity. This creates an intuitive interface that is easier to remember.\nWebadmin APIs use HTTP protocol, which is more secure than JMX protocol to interact with James servers.\nWebadmin command-line interface is an upcoming replacement for the outdated, security-vulnerable JMX command-line interface.\n\n## Decision\nWe decided to write a new CLI client, running on top of the JVM, communicating with James via the webadmin protocol, using http.\n* What libraries will we use?\n* http client: ***Feign library***. We used it as an http client in other parts of James so we continue to use it.\n* CLI: ***Picocli library***. Picocli is a one-file command line parsing framework written in Java that allows us to create command line applications with almost no code. It allows mixing Options with positional Parameters (Eg: no need to the follow order Options then Parameters), [automatic type conversion](https:\/\/picocli.info\/#_strongly_typed_everything) of command line arguments to the type of the annotated field, provide Automatic Help and better Subcommand Support, easily handle Exceptions.\n* How will we limit breaking changes this new CLI will cause?\n* Work on a wrapper to adapt the old CLI API.\n* Where will we locate this cli code?\n* server\/protocols\/webadmin-cli\n* Write a man page.\n* Picocli generates beautiful documentation for our CLI (HTML, PDF and Unix man pages).\n* We decided to adopt a more modern, modular CLI syntax:\n```\n$ .\/james-cli [OPTION] ENTITY ACTION {ARGUMENT}\n```\nwhere\nOPTION: optional parameter when running the command line,\nENTITY: represents the entity to perform action on,\nACTION: name of the action to perform,\nARGUMENT: arguments needed for the action.\n#### Examples\nAdd a domain to the domain list.\n```\n$ .\/james-cli --url http:\/\/127.0.0.1:9999 domain create domainNameToBeCreated\n```\nIn above command-line\nOPTION: --url http:\/\/127.0.0.1:9999\nENTITY: domain\nACTION: create\nARGUMENT: domainNameToBeCreated\n\n\n## **Context**\nAccording to the [Microfrontend-Architecture ADR](https:\/\/github.com\/LBHackney-IT\/lbh-adrs\/blob\/feature\/create-ADRs\/Platform\/Accepted\/Microfrontend-Architecture.md), we are going to define micro-frontends by using a vertical splitting.\nIn this way each micro-frontends has got its repository and each team manages the development and the deployment of it.\nAbout the client-side micro-frontends compositions, there are two options to implement that:\n1. Writing our own boilerplate code\nEach micro frontend is included in the html page using a \\<script> tag and each of those applications exposes a global function as its entry-point. The container application determines which frontend should be mounted and calls the relevant function to tell a micro frontend when and where to render itself.\n2. Using a framework as Single SPA (https:\/\/single-spa.js.org\/ )\nSingle SPA is a Javascript framework for frontend microservices. In practice, it applies a lifecycle to every application. Each app can respond to url routing events and must know how to bootstrap, mount and unmount itself from the DOM.\n\n## Decision\n**Using Single SPA framework**\nThis framework adopts the same principle as we would implement our own boilerplate code, but the advantage is that we don\u2019t have to build and document our own logic but instead we use this light framework supported by an open source community.\nThe other advantage is that the framework allows to compose also applications written in Angualar and Vue.js\n\n\n\nProvide a decision given the context below:\n## Context\nMesh Configuration Protocol (MCP) is a [protocol](https:\/\/github.com\/istio\/api\/tree\/master\/mcp) for transferring configuration among Istio components during runtime. MCP is meant to defer all the logics and complexities back to the server (copilot) as oppose to the original design which all the logic was embeded in the client (Pilot). Another goal of MCP is to create a unified contract for all the Custom Resource Definitions and Service Discovery and the way they are communicated with Pilot.\n<end_of_turn>\n<start_of_turn>model\nCopilot will implement a MCP server to send configuration to Pilot. We will be sending definitions for Gateways, VirtualServices and DestinationRules over bi-directional GRPC.\n<end_of_turn>\n"}
{"File Name":"content-data-api\/adr-006-track-metrics-via-time-dimension.md","Context":"## Context\\nWe would benefit from having a central repository of integrated data from multiple sources that stores current and historical information, and use this data to create analytical reports and performance indicators to support the publishing workflow within GOV.UK.\\nThis is actually difficult to achieve as we have the information dispersed across different applications that are currently designed to support transactional operations and not analytical reporting of integrated data.\\n","Decision":"Build a data warehouse(*) that maintains a copy of the information of the transactional systems.\\n(*) We will be using a PostgreSQL database in the first iteration, we will be exploring other existing solutions for data warehouses in future iterations once we have validated this approach.\\n","tokens":80,"id":1863,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\n\n## Decision\nWe will use [Quay.io](https:\/\/quay.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and\/or enterprise artifacts with trail licenses).\nWe\u2019d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we'd prefer a SaaS solution.\nThe REST API of Quay.io allows our organization to potentially automate user provisioning\/invitation and user\/group management which is not available for Docker Cloud at this time.\nAdditionally, Quay \/ CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product.\nThe summary [![report of the comparison](https:\/\/img.shields.io\/badge\/report%20of%20the%20comparison-PRIVATE-red.svg)](https:\/\/ts.alfresco.com\/share\/s\/mVAV1sGIReC_iqgMN0GGnQ) also contains reference links to the full investigation.\n\n\n## Context and Problem Statement\nManaging GitHub organizations via UI is restricted to few individuals only and does not offer any review mechanisms. The same apply to repository creation, organization membership management and label management. This ADR focuses on bringing transparency into this aspect of housekeeping within a community and aims to provide a way to individual contributors, so they can influence and propose changes themselves.\nADR seeks a way to manage GitHub organization as a code, declaratively via GitOps as a natural extension of the core paradigm of Operate First movement.\n## Decision Drivers\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\n\n## Decision\nImprove transparency, provide auditing and reviews and allow individual contributors to propose changes.\nChosen option: _\"1. Prow's Peribolos\"_, because it's a declarative solution that is widely embraced in upstream communities like Kubernetes.\n### Positive Consequences\n* Repositories can be created declaratively.\n* Even external users can request themselves to be added to the organization via a pull request.\n* Teams can be created and maintained declaratively.\n* Labels can be centrally managed in a unified way as well as updated per repository bases.\n* Solution can be easily automated via post-submit Prow jobs.\n### Negative Consequences\n* Using a declarative configuration may not be as straightforward as clicking buttons in the UI.\n* Some users can still do manual changes to repositories they own, these changes are undone by the automation.\n\n\n\nProvide a decision given the context below:\n## Context\nWe would benefit from having a central repository of integrated data from multiple sources that stores current and historical information, and use this data to create analytical reports and performance indicators to support the publishing workflow within GOV.UK.\nThis is actually difficult to achieve as we have the information dispersed across different applications that are currently designed to support transactional operations and not analytical reporting of integrated data.\n<end_of_turn>\n<start_of_turn>model\nBuild a data warehouse(*) that maintains a copy of the information of the transactional systems.\n(*) We will be using a PostgreSQL database in the first iteration, we will be exploring other existing solutions for data warehouses in future iterations once we have validated this approach.\n<end_of_turn>\n"}
{"File Name":"adr\/ADR-36-kernel-repository-separation.md","Context":"## Context and Problem Statement\\nAs of today, we already decoupled the `renderer` (decentraland\/unity-renderer) from the `explorer` repository. And we are in the process of decoupling `website` (decentraland\/explorer-website). The ECS and building tools are also in the path of being migrated to their own repositories (decentraland\/js-sdk-toolchain).\\nThe context of the problem is around the tactical\/execution aspect. Since the changes affect directly the publishing cycles and processes of every package. We must optimize not blocking development and also not breaking things in a way that prevent us from releasing hotfixes.\\n","Decision":"We choose to create a separated repository to not block the releases and development of explorer during this migration stage.\\nAlso having separated repositories, will foster external contributions, ensuring clear scopes and responsibilities for each repository.\\n","tokens":135,"id":4609,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nOpenHAB uses OSGi as a runtime.\nThis runtime promotes clear separation between implementation and contract (API\/implementation\/SPI) packages.\nOnce program gets launched OSGi framework such Apache Felix or Eclipse Equinox makes sure that \"implementation\" packages stay hidden.\nOn the build tool side we do not have such strong separation because many parts of project are co-developed.\nInternal packages and API are in the same source root, and often functionally different elements of code are included in the same bundle.\nFor example, this means that the `org.openhab.core.items` package is in the same module as `org.openhab.core.items.internal`.\nAs a result, during compile time we have all of the dependencies together - ones which are required by `core.items` and ones used by `core.items.internal` package.\nWhile it might not cause major issues for this module, it might have devastating influence over callers who depend on public parts of the API.\nDuring compilation phase they will get polluted by internal package dependencies and quite often use them.\nSuch approach promotes tight coupling between contract and implementation.\nMore over, it also promotes exposure of specific implementation classes via public API.\nThe natural way to deal with such things is to address them with a build tool that includes an appropriate includes\/excludes mechanism for dependencies.\nIt would work properly, but openHAB core is a single jar which makes things even harder.\nThis means that quite many dependencies get unnecessarily propagated to all callers of public APIs.\nopenHAB utilizes Apache Karaf for provisioning of the application.\nKaraf provisioning itself is capable of verifying its \"features\" based on declared modules, bundles, JAR files, etc.\nCurrently, most of the project features depend on one of two root features, `openhab-core-base` or `openhab-runtime-base`, making no distinction on how particular parts of the framework interact with each other.\nA tiny extension (SPI) bundle that is targeted at a specific framework feature would need to resolve one of the above features, which would then lead to the activation of the entire openhab framework.\nPresent structure of modules \/ bundles is as follows:\n```\n[openhab thing core] <--- [openhab rest core]\n```\n\n## Decision\nSince openHAB core is a framework, we need to keep strong and clear separation of API and implementation packages.\nThis will clarify module responsibilities and streamline development of extensions through smaller and easier to maintain modules.\nThere are also significant opportunities to speed up the build processes and make better use of resolution and graph analysis between framework parts.\nWe decided that a clear separation between API and implementation packages should be made.\nThis means that each service intended to be interacted with through an OSGi services API should form its own contract bundle.\nNew framework modules will separate `internal` package under a `core.<area>.core` bundle and have a distinct interface in a separate module.\nAdditionally, functionally separate components should be placed in separate bundles to ensure that unnecessary dependencies are not pulled in to a build.\nThis is a big change which involves a lot of code relocations and also cleans up the project dependencies which were built up over time.\nBy preserving public package names we will be able to keep backward compatibility at an affordable level.\n```\n[openhab thing api] <--- [openhab rest thing]\n^\n|\n[openhab thing core]\n```\nIdeally, such a pattern should evolve further to include a common structure used for keeping contracts and implementations separate.\n\n\n## Context\nOne of the primary problems encountered in deployments is drift between different environments.  An important element that creates drift is deltas between the deployment of different environments.  We are already mitigating that by storing our configurations as code, and doing automated deployments of those configurations.  An extension of these practices is to practice code reuse, or DRY (don't repeat yourself).\nGiven that we have existing kubernetes deployments, and that our primary supported services (bedrock\/www, snippets, basket) are already deployed there, it seems reasonable to invest further in the kubernetes eco-system.  Our current pattern is to define a separate set of yml files which are deployed via `kubectl apply -f` per region\/cloud. For example, if we have gcp iowa-a and an aws frankfurt cluster, if we wish to deploy to both we'd have two copies of nearly identical files to define the deployments to those two clusters.\n\n## Decision\nUse helm3 in order to define the 'template' of our services.  Helm calls these templates 'charts', templates can have injected 'values'. The deployment, and associated kubernetes objects (such as services, and scaling policies) should be defined once, with sensible defaults chosen for the primary chart (these should be the 'prod' values).  Secrets should be referenced, but not included in the charts (paths to secrets, not the secrets themselves). Then environments that need different values should have an override file in their repo, which can be combined with defaults at deploy time.\nThere should be a single mozmeao helm repo, that contains all of our custom written charts. We would expect there to be a single chart per service, where bedrock\/www is a service.  There should be a pipeline for that helm repo (that includes testing).  The pipeline for each service would then reference and deploy those charts for the dev\/staging\/prod versions of the service.\nOne advantage of undertaking this work is making it easier to read and understand our deployments.  Answering 'what's different between dev and prod' is difficult when the full configuration is repeated.  It's much easier to answer when dev is defaults + a small override file, and prod is the same.  We should also end up with fewer differences, since each difference is clearly visible in the charts, and we can seek to reduce that count.\nThe other primary advantage is reducing the class of errors where some new feature worked in dev, but doesn't in prod because you forgot to do X. Where X is likely adding an environment variable, or creating a secret.  Having a template means we should be able to fail the deployment earlier in each environment if that configuration is not present.\n\n\n\nProvide a decision given the context below:\n## Context and Problem Statement\nAs of today, we already decoupled the `renderer` (decentraland\/unity-renderer) from the `explorer` repository. And we are in the process of decoupling `website` (decentraland\/explorer-website). The ECS and building tools are also in the path of being migrated to their own repositories (decentraland\/js-sdk-toolchain).\nThe context of the problem is around the tactical\/execution aspect. Since the changes affect directly the publishing cycles and processes of every package. We must optimize not blocking development and also not breaking things in a way that prevent us from releasing hotfixes.\n<end_of_turn>\n<start_of_turn>model\nWe choose to create a separated repository to not block the releases and development of explorer during this migration stage.\nAlso having separated repositories, will foster external contributions, ensuring clear scopes and responsibilities for each repository.\n<end_of_turn>\n"}
{"File Name":"gsp\/ADR011-build-artefacts.md","Context":"## Context\\nAs part of our pipelines we will be building artefacts that will be used to test\\nand deploy our applications. We will be deploying applications to Kubernetes. We\\nwill need to build a container image of some kind.\\nThere are some competing container image formats, namely:\\n* [OCI]\\n* [ACI]\\nThe OCI image format is [based on the Docker v2][oci-standard] image format.\\nThe Kubernetes project appears to [prefer Docker\/OCI][k8s-preferance] images\\nover ACI.\\n[rkt is moving to OCI][rkt-oci] and away from ACI. OCI will become the preferred\\nimage format.\\nDocker has wide industry adoption and appears to have wide understanding within\\nGDS.\\nDocker is the default container runtime for Kubernetes.\\n","Decision":"We will build and store OCI images built using Docker.\\n","tokens":170,"id":3920,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nSources are currently minified by JSMin producing a bundle bigger than\nnecessary. We want to produce a JS bundle as small as possible to ship less\ncode to clients.\nGiven we have a pre-compilation step in Popups, we can leverage node based\ntooling without penalty, so using UglifyJS to minify the code is an option.\nWe performed some analysis to see the differences in asset size between the two\ntools. See:\n* [Minifying assets with uglifyjs][1]\nResults with the current codebase as of today 12th of May 2017 are that uglify\nproduces a 40% smaller bundle, and a 25% smaller bundle when using gzip.\nThis results are stable through the last 3 months, the same test has been\nperformed multiple times. See [Compressed JS comparison][2]\n\n## Decision\nWebpack's production mode has been enabled for `npm run build`, and thus the\ncompiled bundle will be minified in production with UglifyJS.\nSee [Enable production settings for the production bundle][3]\n\n\n## Context\nKitsune supports many locales,\nand has content which we want to be searchable in those locales.\nElasticsearch has support for many language-specific analyzers:\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/7.9\/analysis-lang-analyzer.html\nSearch v1 used per-document analyzers,\nthat is to say, within the same index:\n```\ndoc_1: { \"content\": \"Hello world\" }\ndoc_2: { \"content\": \"Hallo Welt\" }\n```\n`doc_1.content` could be analyzed using an english analyzer,\nand `doc_2.content` could be analyzed using a german analyzer.\nWell before version 7 ES removed this feature,\nand now all fields of the same name across an index must be analyzed the same,\nso we must take a different approach with the current Search implementation.\nWe can either place separate locales in their own index,\nand set up locale-specific analyzers for the same field name across indices.\nOr we can keep separate locales within the same index,\nand define unique field names for each field which needs to be analyzed under a specific locale.\n\n## Decision\nHeavily influenced by: https:\/\/www.elastic.co\/blog\/multilingual-search-using-language-identification-in-elasticsearch\nWe will store all documents within the same index and use an Object field for fields which need to use locale-specific analyzers.\nWe will call this field `SumoLocaleAwareTextField` and will have a key for each locale,\nwith the appropriate analyzer defined on that key,\nsuch that:\n```\ndoc_1: { \"content\": { \"en-US\": \"Hello world\" }}\ndoc_2: { \"content\": { \"de\": \"Hallo Welt\" }}\n```\n`doc_1.content.en-US` is analyzed using an english analyzer,\nand `doc_2.content.de` is analyzed using a german analyzer.\n\n\n\nProvide a decision given the context below:\n## Context\nAs part of our pipelines we will be building artefacts that will be used to test\nand deploy our applications. We will be deploying applications to Kubernetes. We\nwill need to build a container image of some kind.\nThere are some competing container image formats, namely:\n* [OCI]\n* [ACI]\nThe OCI image format is [based on the Docker v2][oci-standard] image format.\nThe Kubernetes project appears to [prefer Docker\/OCI][k8s-preferance] images\nover ACI.\n[rkt is moving to OCI][rkt-oci] and away from ACI. OCI will become the preferred\nimage format.\nDocker has wide industry adoption and appears to have wide understanding within\nGDS.\nDocker is the default container runtime for Kubernetes.\n<end_of_turn>\n<start_of_turn>model\nWe will build and store OCI images built using Docker.\n<end_of_turn>\n"}
{"File Name":"clone_difitalcitizenship\/0003-follow-a-cloud-native-design-principle.md","Context":"## Context\\nWe need to design the building blocks of the Digital Citizenship platform. We will make decisions about the overall architecture and the building blocks we're going to use to satisfy the functional and non-functional requirements.\\nWhen deciding what technologies to use and how to satisfy those requirements, we're going to evaluate whether we want to build certain components from scratch, use open-source solutions managed by us or rely on existing public cloud components, managed by 3rd party providers.\\nAs the world of cloud technologies continues to accelerate, we should absorb new developments into how we work. Leading technology organisations are rapidly embracing new tools like \"serverless\" computing.\\nAt the infrastructure and application level we should expect our applications to be resilient, flexible and API-driven. We should have the tools and practices in place to manage and secure a distributed range of tools accessed over the internet.\\nDecision criteria will also include:\\n* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\n","Decision":"* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\nWe decide to build the components of the Digital Citizenship initiative on top of scalable, managed and cost-effective components provided by the leading public cloud providers.\\n","tokens":227,"id":1182,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\n\n## Decision\nWe will use [Quay.io](https:\/\/quay.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and\/or enterprise artifacts with trail licenses).\nWe\u2019d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we'd prefer a SaaS solution.\nThe REST API of Quay.io allows our organization to potentially automate user provisioning\/invitation and user\/group management which is not available for Docker Cloud at this time.\nAdditionally, Quay \/ CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product.\nThe summary [![report of the comparison](https:\/\/img.shields.io\/badge\/report%20of%20the%20comparison-PRIVATE-red.svg)](https:\/\/ts.alfresco.com\/share\/s\/mVAV1sGIReC_iqgMN0GGnQ) also contains reference links to the full investigation.\n\n\n## Context\nWhen developing a new system, there are many possible language choices available.\n-CCS uses a mixture of different languages for existing systems including:\n- PHP for the Supplier Registration Service and Contracts Finder\n- C# for MISO\n- Java for the CCS ID and DigiTS services\n- Python for the Digital Marketplace\nOther services in government use a mixture of PHP, Java, Python, Ruby, Scala and GO.\nPicking one standard language for all CCS services could restrict the pool of potential suppliers we could interact with when building new services. Conversely, allowing an unlimited selection of languages can make future management and support of services more complex and expensive.\nAgreeing a selection of acceptable languages will allow the greatest flexibility in supplier choice, but also helps minimise future ongoing support costs.\nAny decision we make about languages and frameworks should consider:\n- Skills available in the current team(s) - what are the current team(s) comfortable using? What do they have experience in?\n- Skills of a future team - is there a large enough pool of suppliers and contractors who could support and maintain the services in the future?\n- External toolkits and modules - are there useful toolkits and modules which could reduce development effort (eg the GOV.UK Frontend Toolkit)\n- Cost - what is the cost of developing in this way? Are there licence costs?\n- Hosting - would picking a language restrict hosting options?\nWe should also consider the Service Manual guide on [choosing technology](https:\/\/www.gov.uk\/service-manual\/technology\/choosing-technology-an-introduction).\n\n## Decision\nNew services developed by CCS should use the following languages:\n- Java\n- Python\n- Ruby\n- Javascript\nDigital services should also use HTML5, CSS3 and JSON as required.\nSalesforce work may continue to use the Salesforce Apex language, but where possible, we should use external web service components to reduce the volume of Apex in use.\nNew services which do not use these languages will require an Architectural Decision to explain why.\nAs the software development industry changes quickly, we will re-evaluate this decision in 6 months, taking account of current market trends and industry moves.\nThis decision does not apply to services comprised entirely of Software as a Service (SaaS) solutions, and does not affect systems which already exist. It also does not cover the development of native mobile applications.\n\n\n\nProvide a decision given the context below:\n## Context\nWe need to design the building blocks of the Digital Citizenship platform. We will make decisions about the overall architecture and the building blocks we're going to use to satisfy the functional and non-functional requirements.\nWhen deciding what technologies to use and how to satisfy those requirements, we're going to evaluate whether we want to build certain components from scratch, use open-source solutions managed by us or rely on existing public cloud components, managed by 3rd party providers.\nAs the world of cloud technologies continues to accelerate, we should absorb new developments into how we work. Leading technology organisations are rapidly embracing new tools like \"serverless\" computing.\nAt the infrastructure and application level we should expect our applications to be resilient, flexible and API-driven. We should have the tools and practices in place to manage and secure a distributed range of tools accessed over the internet.\nDecision criteria will also include:\n* elasticity and resilience\n* pay-as-you-go pricing\n* exploiting economies of scale\n* exploiting falling costs of infrastructure over time\n* quality of management tools\n* best of breed security\n* flexibility and opportunity costs\n<end_of_turn>\n<start_of_turn>model\n* elasticity and resilience\n* pay-as-you-go pricing\n* exploiting economies of scale\n* exploiting falling costs of infrastructure over time\n* quality of management tools\n* best of breed security\n* flexibility and opportunity costs\nWe decide to build the components of the Digital Citizenship initiative on top of scalable, managed and cost-effective components provided by the leading public cloud providers.\n<end_of_turn>\n"}
{"File Name":"openjdk-api-v3\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4699,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n\n\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n\n\n\nProvide a decision given the context below:\n## Context\nWe need to record the architectural decisions made on this project.\n<end_of_turn>\n<start_of_turn>model\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n<end_of_turn>\n"}
{"File Name":"openfido-app-service\/0002-project-structure.md","Context":"## Context\\nThe OpenFIDO project will mediate access to a workflow service that runs\\nGridLabD jobs. The frontend React application will need access to these\\nresources, but only if they have permission to access via an organization.\\n","Decision":"Create a Flask Rest service - since the other microservices in this suite of\\nservices are also Flask based, keep this using the same kind of infrastructure\\nso that common utilities can be shared (openfido-utils) and the maintenance will\\nbe simplified.\\nOrganize the database logic into a simplified CQRS-inspired style code\\nstructure. Since we anticipate many conceptual resources, each resource will\\nhave its own module:\\n* app\/RESOURCE\/models.py - contains all models for RESOURCE.\\n* app\/RESOURCE\/routes.py - contains all Rest routes specific to RESOURCE.\\n* app\/RESOURCE\/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\\n* app\/RESOURCE\/services.py - all db commands that modify database state.\\n* app\/RESOURCE\/queries.py - all db queries to the database.\\nAdditional libraries we anticipate using:\\n* [marshmallow](https:\/\/marshmallow.readthedocs.io\/en\/stable\/) will be used since there are many rest api endpoints that will take nested bodies.\\n* [alembic](https:\/\/alembic.sqlalchemy.org\/en\/latest\/) to manage database schema migrations.\\nOther thoughts on design:\\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\\nUse the structure of the [openfido-workflow-service](https:\/\/github.com\/slacgismo\/openfido-workflow-service) project as a reference.\\n","tokens":49,"id":2470,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe need to persist the user data to the backend. Rather than building our\nfrontend to be tightly coupled to an API, we want a layer that can sit between\nthe application and the API.\nWe know that we will need to support offline use of these forms. We also want\nusers to be able to recover unsubmitted forms if they haven't gone online\nbetween sessions.\nWe also know that some forms will require the upload of images.\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\ndatabase that persists data across sessions. It supports storing blob data (such\nas images), and has decent storage limits.\n\n## Decision\nWe will support IndexedDB as a data store layer, sitting between the application\nand the API.\n\n\n# Context and Problem Statement\nHow should the application be structured into its main components? The\ngoal is to be able to have multiple rest servers\/webapps and multiple\ndocument processor components working togehter.\n# Decision Outcome\nThe following are the \"main\" modules. There may be more helper modules\nand libraries that support implementing a feature.\n\n## Decision\nThe following are the \"main\" modules. There may be more helper modules\nand libraries that support implementing a feature.\n\n\n\nProvide a decision given the context below:\n## Context\nThe OpenFIDO project will mediate access to a workflow service that runs\nGridLabD jobs. The frontend React application will need access to these\nresources, but only if they have permission to access via an organization.\n<end_of_turn>\n<start_of_turn>model\nCreate a Flask Rest service - since the other microservices in this suite of\nservices are also Flask based, keep this using the same kind of infrastructure\nso that common utilities can be shared (openfido-utils) and the maintenance will\nbe simplified.\nOrganize the database logic into a simplified CQRS-inspired style code\nstructure. Since we anticipate many conceptual resources, each resource will\nhave its own module:\n* app\/RESOURCE\/models.py - contains all models for RESOURCE.\n* app\/RESOURCE\/routes.py - contains all Rest routes specific to RESOURCE.\n* app\/RESOURCE\/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\n* app\/RESOURCE\/services.py - all db commands that modify database state.\n* app\/RESOURCE\/queries.py - all db queries to the database.\nAdditional libraries we anticipate using:\n* [marshmallow](https:\/\/marshmallow.readthedocs.io\/en\/stable\/) will be used since there are many rest api endpoints that will take nested bodies.\n* [alembic](https:\/\/alembic.sqlalchemy.org\/en\/latest\/) to manage database schema migrations.\nOther thoughts on design:\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\nUse the structure of the [openfido-workflow-service](https:\/\/github.com\/slacgismo\/openfido-workflow-service) project as a reference.\n<end_of_turn>\n"}
{"File Name":"buildit-all\/0008-database.md","Context":"## Context\\nBookit needs to persist the locations, bookables, and bookings so that the data survives multiple instances and deployments over time.\\n","Decision":"* Use SQL approach as opposed to NoSQL solution - the model is simple and ACID transactions keep multiple users separate\\n* Use H2 for unit testing & local development - speeds up execution time and reduces external dependencies\\n* Use AWS RDS Aurora (MySQL) for integration\/staging\/production - better HA & continuous snapshots (enabled for production)\\n* Use MariaDB JDBC driver - has native Aurora support for failover\\n","tokens":29,"id":4415,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nDeploying hosted 3Bot takes a lot of time because of cloning and installation time.\nIt can make use of the new flist build by the CI to be always up to date and reduce the installation time.\n\n## Decision\nUpdate the flist with the up to date one\n\n\n## Context\nWe need a process to bootstrap a new Amazon environment. This should be both heavily automated\nand include set places where the operator can evaluate the status of the provisioning.\nThis bootstrapping process assumes the following:\n* The required repositories are available\n* An Amazon Web Services admin account is available\n* Backups of our data can be retrieved (for the restore process)\n\n## Decision\nThe chosen process has a few, early bootstrap steps, that differ from normal operations. These\naim to quickly provide the usual level of self-service to allow teams to independently\nrestore services without a dependency on a central operations team. An early draft of\nthis process, which will be updated as changes are required, will be:\n1. Clone all the relevant repositories\n1. Build the S3 bucket for Terraform state\n1. Provision the new DNS zones for this environment\n1. Build the Puppet master\n1. Deploy the puppet code and secrets\n1. Build the deploy Jenkins\n* Rebuild everything else in the usual deployment ways\nThis document will be written so that one of the non-infrastructure members of the team\nwill be able to provision a complete environment within our allowed recovery time.\n\n\n\nProvide a decision given the context below:\n## Context\nBookit needs to persist the locations, bookables, and bookings so that the data survives multiple instances and deployments over time.\n<end_of_turn>\n<start_of_turn>model\n* Use SQL approach as opposed to NoSQL solution - the model is simple and ACID transactions keep multiple users separate\n* Use H2 for unit testing & local development - speeds up execution time and reduces external dependencies\n* Use AWS RDS Aurora (MySQL) for integration\/staging\/production - better HA & continuous snapshots (enabled for production)\n* Use MariaDB JDBC driver - has native Aurora support for failover\n<end_of_turn>\n"}
{"File Name":"deeplearning4j\/0003-dealing_with_inconsistencies_in_java_naming.md","Context":"## Context\\nThere are slight inconsistencies in naming between existing op class definitions and factory methods. For example a\\nfactory method called `bernoulli` in the `random` namespace with a corresponding op class called\\n`BernoulliDistribution`.\\nTwo possible solutions where suggested:\\n1. Add an additional property that provides us with the correct class name\\n2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses\\n","Decision":"For now we will introduce a `javaOpClass` property which in cases of inconsistency provides us with the correct class\\nname.\\n","tokens":92,"id":2934,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context and Problem Statement\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\ndefaults that differ between schematic types.\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\nand directories.\n## Decision Drivers\n* Provide consistency when navigating the codebase\n* The earlier we decide on this, the better\n\n## Decision\n* Provide consistency when navigating the codebase\n* The earlier we decide on this, the better\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\nNaming directories should follow these guidelines:\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\n`import { Box } from '@island.is\/island-ui\/core'`\nUse PascalCase for directories only containing React components:\n````\ncomponents\/CtaButton\/index.ts\nimport 'components\/CtaButton'\n````\nor:\n````\ncomponents\/CtaButton\/CtaButton.tsx\nimport 'components\/CtaButton\/CtaButton'\n````\nrather than\n````\ncomponents\/cta-button\/CtaButton.tsx\n````\nIn all other cases, use camelCase.\n### Positive Consequences\n* Easier to navigate the codebase\n* File names are more readable, and developers know what to expect\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\n\n\n## Context and Problem Statement\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\ndefaults that differ between schematic types.\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\nand directories.\n## Decision Drivers\n* Provide consistency when navigating the codebase\n* The earlier we decide on this, the better\n\n## Decision\n* Provide consistency when navigating the codebase\n* The earlier we decide on this, the better\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\nNaming directories should follow these guidelines:\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\n`import { Box } from '@island.is\/island-ui\/core'`\nUse PascalCase for directories only containing React components:\n````\ncomponents\/CtaButton\/index.ts\nimport 'components\/CtaButton'\n````\nor:\n````\ncomponents\/CtaButton\/CtaButton.tsx\nimport 'components\/CtaButton\/CtaButton'\n````\nrather than\n````\ncomponents\/cta-button\/CtaButton.tsx\n````\nIn all other cases, use camelCase.\n### Positive Consequences\n* Easier to navigate the codebase\n* File names are more readable, and developers know what to expect\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\n\n\n\nProvide a decision given the context below:\n## Context\nThere are slight inconsistencies in naming between existing op class definitions and factory methods. For example a\nfactory method called `bernoulli` in the `random` namespace with a corresponding op class called\n`BernoulliDistribution`.\nTwo possible solutions where suggested:\n1. Add an additional property that provides us with the correct class name\n2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses\n<end_of_turn>\n<start_of_turn>model\nFor now we will introduce a `javaOpClass` property which in cases of inconsistency provides us with the correct class\nname.\n<end_of_turn>\n"}
{"File Name":"dotfiles\/0002-replace-travis-ci-with-github-actions.md","Context":"## Context\\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\\ngoing to be generally available on\\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\\nThey are a thing that will have a long term future. It is likely GitHub Actions\\nwill become the default CI mechanism (and possibly more) for projects hosted on\\nGitHub. Using them in this repository which has a very basic use case will\\nprovide some exposure to the service.\\n","Decision":"The decision is to replace Travis CI with GitHub Actions.\\n","tokens":116,"id":118,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\ngoing to be generally available on\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\nGitHub Actions will have a long term future. It is likely GitHub Actions\nwill become the default CI mechanism (and possibly more) for projects hosted on\nGitHub. Using them in this repo, which has a basic use case will provide some\nexposure to the service.\n\n## Decision\nThe decision is to replace Travis CI with GitHub Actions.\n\n\n## Context\nIstio's service mesh capabilites are facilitated via sidecars injected into\nworkload pods. These sidecars run an Istio-patched version of Envoy that is tied\nto the version of Istio that injects them.\nTypically when new versions of Istio are released, new versions of the sidecars\nare released as well. Istio has been good so far about supporting older versions\nof sidecars that were deployed before Istio was upgraded, but it is still\n[documented best practice](https:\/\/istio.io\/latest\/docs\/setup\/upgrade\/) to roll\nall the pods after an Istio upgrade.\nAs an additional constraint, the operators of cf-for-k8s clusters expect to be\nable to perform upgrades in one `kapp deploy`, with no post-install hooks or\nother bash scripts. This limits our options considerably. See this [Slack\nthread](https:\/\/cloudfoundry.slack.com\/archives\/CH9LF6V1P\/p1592521879117400) on\nthat constraint.\n\n## Decision\nWe will use the kubernetes\n[Job](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/job\/) resource\nto run the kubectl command needed to roll workload pods, after waiting for the\nnew Istio control plane to be up and healthy.\nTo that end, we will add the necessary minimal `ServiceAccounts` and `Roles`\nneeded to list resources in the `istio-system` namespace, and restart resources\nin the configured workload namespace. We will also build and maintain a\ncontainer image that contains the Job's logic.\nAll istio components will be tagged with their Istio version so that the job can\npositively determine that the correct version of control plane components are\nalive and healthy. We will also name the job according to it's Istio version, so\nthat we can take advantage of `Jobs` inherent immutability in cases where a\ncf-for-k8s upgrade does not contain a new Istio version (pushing the same job\nagain will not cause it to rerun, preventing workloads from rolling\nunnecessarily). Subsequent jobs will clean up previous ones.\n\n\n\nProvide a decision given the context below:\n## Context\nGitHub Actions are the new shiny and have been in Beta for sometime. They are\ngoing to be generally available on\n[2019-11-13](https:\/\/github.blog\/2019-08-08-github-actions-now-supports-ci-cd\/).\nThey are a thing that will have a long term future. It is likely GitHub Actions\nwill become the default CI mechanism (and possibly more) for projects hosted on\nGitHub. Using them in this repository which has a very basic use case will\nprovide some exposure to the service.\n<end_of_turn>\n<start_of_turn>model\nThe decision is to replace Travis CI with GitHub Actions.\n<end_of_turn>\n"}
{"File Name":"signals-frontend\/0009-stop-mocking-fetch-in-tests.md","Context":"## Context\\nFor mocking unittests we are using at this moment the `jest-fetch-mock` package. This works but there are new develoments on the market. As the creator of the [Testing Library](https:\/\/testing-library.com\/) suggests in this [article](https:\/\/kentcdodds.com\/blog\/stop-mocking-fetch), there is a new strategy available that would simplify testing of components that are making fetch calls.\\n# Technical description\\nThe `msw` server intercepts the fetch calls and returns the configured response. This is done outside the test file and only overrides of the standard responses from the default configuration need to be added to the respective test file. [Here is the documentation](https:\/\/github.com\/mswjs\/msw).\\n","Decision":"The mocking will be done as much as possible by using the the `msw` package.\\n","tokens":156,"id":3050,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nIn [ADR-5](adr-5.md), we decided on a format of a new workflow resource attribute, steps. We decided that the attribute would use ES6 Maps. When stored in the Mobx-State-Tree store, the map is wrapped by a MobX Observable. In order to work with Panoptes, these data structures has to be converted into a format that is supported by JSON API.\nIn the consequences section of ADR-5, we described two possible solutions for the type conversion: using an existing library or doing it ourselves. We initially attempted to use an existing library, but several concerns were raised by fellow team members about this ([zooniverse\/Panoptes-Front-End#4992] (https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/issues\/4992)).\nThe default behavior in existing libraries like [JSON8](https:\/\/github.com\/sonnyp\/JSON8\/tree\/master\/packages\/json8#ooserialize) or MobX's [`toJS`](https:\/\/mobx.js.org\/refguide\/tojson.html) method is to convert maps into objects. In javascript, maps are a kind of object. However, for us, we are using maps for workflow steps because a key requirement is the ordering of key-value pairs, so converting to an object would lose the guaranteed ordering of steps. Using a library also obscures the method of map type conversion, so it will not be clear to other Zooniverse devs for other client libraries in ruby or python how to handle this case.\n\n## Decision\nWe decided to instead implement our own [type conversion utility function](https:\/\/github.com\/zooniverse\/front-end-monorepo\/blob\/master\/packages\/lib-classifier\/src\/store\/utils\/convertMapToArray.js) for the workflow steps map. The steps map will be converted into an array of pairs:\n``` js\nconst workflow = {\nid: '1',\nsteps: [['S1', { taskKeys: ['T1', 'T2'] }], ['S2', { taskKeys: ['T3'] }]] \/\/ How they will be stored on Panoptes\n}\n```\nAnd when a workflow request is received by the classifier store, it is converted by Mobx-State-Tree into an observable map when added to the store.\n**A note about the use of arrays for the key-value pairs**\nSubject locations are an array of objects. It would make sense to do an array of objects here too, however the array of two values is closest to the format expected by maps when you instantiate them: https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Map\nSo there's less type conversion happening if we store it this way. MobX will take objects too when setting an observable map, but if we ever want to instantiate an ES6 map independent of the store we would have to do another conversion from object to array of the key-value pair.\n\n\n## Context\nThe PACE projects will evolve over time and breaking changes will be introduced. Users will need to be able to easily finding the correct documentation for their build.\n[Semantic versioning](https:\/\/semver.org\/) defines a schema in which releases are given `major.minor.patch` version numbers where increments are made to the:\n- `major` version with incompatible API changes,\n- `minor` version when functionality is added in a backwards compatible manner, and\n- `patch` version for backwards compatible bug fixes.\nSphinx plugins are available that support multiple documentation versions ([sphinx-multiversion](https:\/\/pypi.org\/project\/sphinx-multiversion\/)) - this will not work correctly for Brille where documentation is extracted from the build artifacts.\n\n## Decision\nProjects will be semantically versioned and documentation will be retained and be available for users to access for each major or minor version released.\nThere is no requirement to have patch-release specific documentation.\n\n\n\nProvide a decision given the context below:\n## Context\nFor mocking unittests we are using at this moment the `jest-fetch-mock` package. This works but there are new develoments on the market. As the creator of the [Testing Library](https:\/\/testing-library.com\/) suggests in this [article](https:\/\/kentcdodds.com\/blog\/stop-mocking-fetch), there is a new strategy available that would simplify testing of components that are making fetch calls.\n# Technical description\nThe `msw` server intercepts the fetch calls and returns the configured response. This is done outside the test file and only overrides of the standard responses from the default configuration need to be added to the respective test file. [Here is the documentation](https:\/\/github.com\/mswjs\/msw).\n<end_of_turn>\n<start_of_turn>model\nThe mocking will be done as much as possible by using the the `msw` package.\n<end_of_turn>\n"}
{"File Name":"tracking-consent-frontend\/0005-use-data-attribute-for-language.md","Context":"## Context and Problem Statement\\nTracking consent reads the PLAY_LANG cookie to determine whether to render the banner in Welsh. PLAY_LANG is the cookie\\nused by the Play Framework to persist a user's language preferences. It will be set to 'cy'\\nwhen a user has selected Welsh using the language toggle in MDTP services using the Play Framework.\\nTeams are increasingly setting PLAY_LANG to HttpOnly in an attempt to get green ZAP tests, even though there are no\\nknown security concerns around keeping PLAY_LANG as a normal cookie. Setting a cookie to\\nHttpOnly makes it unreadable within the client-side Javascript code that renders the tracking consent banner. The result\\nof this is that the banner will not be translated into Welsh for these services.\\nA related issue is that PLAY_LANG is not set for classic services written in Java, which means a Welsh version of the banner is not\\ncurrently available for classic services.\\nIt is worth noting that the only other known instance of our reading PLAY_LANG using Javascript is in the assets-frontend\\n[timeout dialog](https:\/\/github.com\/hmrc\/assets-frontend\/blob\/97c638289e23bee255ac30724a8572c6efa96817\/assets\/patterns\/help-users-when-we-time-them-out-of-a-service\/timeoutDialog.js#L14) timeout dialog. All the new govuk-frontend and hmrc-frontend components use data attributes instead.\\nShould we remove the reading of PLAY_LANG in tracking consent and accept a data-language attribute instead?\\n## Decision Drivers\\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\n","Decision":"* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\nChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'\\ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG into Zap tests. We also agreed that our frontend\\ncomponents should be consistent in their treatment of language until such time as we are able to provide an\\nalternative approach that works for all components.\\n### Positive Consequences\\n* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner\\n* Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings\/errors\\n* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etc\\n### Negative Consequences\\n* We will need to make a small change to tracking consent and communicate this change to teams\\n* Teams will need to upgrade to the latest version of play-ui\/play-frontend-hmrc to get language translations\\n* Teams not using the tracking consent helpers will need to add a data-attribute for the language\\n","tokens":409,"id":3526,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context and Problem Statement\nTax users are sometimes, without warning, unnecessarily signed out when accessing\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\nsuccess criterion 2.1.1 (Timing adjustable)](https:\/\/www.w3.org\/WAI\/WCAG21\/Understanding\/timing-adjustable.html).\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https:\/\/github.com\/hmrc\/play-frontend-hmrc#warning-users-before-timing-them-out)\ncurrently has no way of knowing about this activity following initial page load.\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\nremaining on the user's active session via an endpoint that is itself excluded from\nsession management.\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\nknowledge and avoiding introducing additional coupling between frontend microservices?\n## Decision Drivers\n* The need to minimise code changes for service teams other than a library upgrade.\n* The avoidance of requiring service teams to add any additional routing rules.\n* The avoidance of requiring service teams to add any additional configuration.\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\nany other library or service.\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\nof failure between frontend microservices.\n* The need for the endpoint used to interrogate the session to not itself affect the session.\n\n## Decision\n* The need to minimise code changes for service teams other than a library upgrade.\n* The avoidance of requiring service teams to add any additional routing rules.\n* The avoidance of requiring service teams to add any additional configuration.\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\nany other library or service.\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\nof failure between frontend microservices.\n* The need for the endpoint used to interrogate the session to not itself affect the session.\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\naround knowledge duplication.\n\n\n**Context**: We know there will be users of these C++ libraries who want to use\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\nfor all of our users, regardless of their ability to use exceptions.\n**Decision**: None of our APIs will throw exceptions to indicate errors.\nInstead, our APIs will typically report errors to callers by returning a\n`Status` or `StatusOr<T>` object, unless the library we're using has another\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\nI\/O library).\n**Consequences**: This decision will result in a single set of APIs and a\nconsistent vocabulary for all users, whether or not they choose to compile with\nexceptions. This decision does not prevent callers from using exceptions in\ntheir own code.\nA downside of this decision is that our APIs will not be natural or idiomatic\nfor the [50+%][survey-link] of users who might prefer exceptions for error\nreporting.\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\nbreaking change. As of this writing (Jan 2019), this project has a\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\ntimeline to change this API in a separate document.\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\n\n## Decision\nInstead, our APIs will typically report errors to callers by returning a\n`Status` or `StatusOr<T>` object, unless the library we're using has another\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\nI\/O library).\n**Consequences**: This decision will result in a single set of APIs and a\nconsistent vocabulary for all users, whether or not they choose to compile with\nexceptions. This decision does not prevent callers from using exceptions in\ntheir own code.\nA downside of this decision is that our APIs will not be natural or idiomatic\nfor the [50+%][survey-link] of users who might prefer exceptions for error\nreporting.\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\nbreaking change. As of this writing (Jan 2019), this project has a\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\ntimeline to change this API in a separate document.\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\n\n\n\nProvide a decision given the context below:\n## Context and Problem Statement\nTracking consent reads the PLAY_LANG cookie to determine whether to render the banner in Welsh. PLAY_LANG is the cookie\nused by the Play Framework to persist a user's language preferences. It will be set to 'cy'\nwhen a user has selected Welsh using the language toggle in MDTP services using the Play Framework.\nTeams are increasingly setting PLAY_LANG to HttpOnly in an attempt to get green ZAP tests, even though there are no\nknown security concerns around keeping PLAY_LANG as a normal cookie. Setting a cookie to\nHttpOnly makes it unreadable within the client-side Javascript code that renders the tracking consent banner. The result\nof this is that the banner will not be translated into Welsh for these services.\nA related issue is that PLAY_LANG is not set for classic services written in Java, which means a Welsh version of the banner is not\ncurrently available for classic services.\nIt is worth noting that the only other known instance of our reading PLAY_LANG using Javascript is in the assets-frontend\n[timeout dialog](https:\/\/github.com\/hmrc\/assets-frontend\/blob\/97c638289e23bee255ac30724a8572c6efa96817\/assets\/patterns\/help-users-when-we-time-them-out-of-a-service\/timeoutDialog.js#L14) timeout dialog. All the new govuk-frontend and hmrc-frontend components use data attributes instead.\nShould we remove the reading of PLAY_LANG in tracking consent and accept a data-language attribute instead?\n## Decision Drivers\n* The need to support classic services\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\navoid services having to upgrade a second time.\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\n* The preference for consistency in language settings across our frontend components\n<end_of_turn>\n<start_of_turn>model\n* The need to support classic services\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\navoid services having to upgrade a second time.\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\n* The preference for consistency in language settings across our frontend components\nChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'\ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG into Zap tests. We also agreed that our frontend\ncomponents should be consistent in their treatment of language until such time as we are able to provide an\nalternative approach that works for all components.\n### Positive Consequences\n* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner\n* Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings\/errors\n* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etc\n### Negative Consequences\n* We will need to make a small change to tracking consent and communicate this change to teams\n* Teams will need to upgrade to the latest version of play-ui\/play-frontend-hmrc to get language translations\n* Teams not using the tracking consent helpers will need to add a data-attribute for the language\n<end_of_turn>\n"}
{"File Name":"compliantkubernetes\/0013-configure-alerts-in-omt.md","Context":"## Context and Problem Statement\\nAlerts are some noteworthy IT event, like a Node becoming un-ready, login failure or a disk getting full.\\nTerminology differs across tooling and organizations, but one generally cares about:\\n* P1 (critical) alerts, which require immediate human attention -- the person on-call needs to be notified immediately -- and;\\n* P2 (high) alerts which require human attention with 24 hours -- the person on-call needs to be notified next morning;\\n* P3 (moderate) alerts which do not require immediate human attention, but should be regularly reviewed.\\nOther priorities (e.g., P4 and below) are generally used for informational purposes.\\nDealing with alerts correctly entails prioritizing them (e.g., P1, P2, P3), deciding if someone should be notified, who should be notified, how they should be notified (e.g., SMS or email) and when.\\n\"Who\", \"how\" and \"when\" should include escalation, if the previous notification was not acknowledged within a pre-configured time interval, then the same person if notified via a different channel or a new person is notified.\\nUnder-alerting -- e.g., notifying an on-call person too late -- may lead to Service Level Agreement (SLA) violations and a general feeling of administrator anxiety: \"Is everything okay, or is alerting not working?\".\\nOver-alerting -- e.g., notifying a person too often about low-priority alerts -- leads to alert fatigue and \"crying wolf\" where even important alerts are eventually ignored.\\nHence, configuring the right level of alerting -- in particular notifications -- is extremely important both for SLA fulfillment and a happy on-call team.\\nWhere should alerting be configured, so as to quickly converge to the optimal alerting level?\\n## Decision Drivers\\n* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\n","Decision":"* Allow to quickly silence, un-silence and re-prioritize alerts.\\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\\n* Leverage existing tools and processes.\\nChosen option: Compliant Kubernetes \u201cover-alerts\u201d, i.e., forwards all alerts and all relevant information to an On-Call Management Tool (OMT, e.g., Opsgenie).\\nConfiguration of alerts happens in the OMT.\\n### Positive Consequences\\n* Clear separation of concerns.\\n* Alerting does not require per-customer configuration of Compliant Kubernetes.\\n* Leverages existing tools and processes.\\n* We do not need to implement complex alert filtering in Compliant Kubernetes, e.g., silence alerts during maintenance windows, silence alerts during Swedish holidays, etc.\\n### Negative Consequences\\n* Does not capture alerting know-how in Compliant Kubernetes.\\n* Migration to a new OMT means all alerting configuration needs to be migrated to the new tool. Fortunately, this can be done incrementally.\\n","tokens":445,"id":3110,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nMost public-facing digital services have a need to notify its users about something at a critical business process, or just to manage their expectations to prevent incoming contact.\nThe notifications can, for example, be:\n* Telling a supplier they have a task to complete,\n* Telling a supplier that a deadline is approaching,\n* Telling a supplier that a buyer has been shortlisted for a procurement,\n* Providing a receipt,\n* Providing status updates,\n* Etc...\nGenerally, the notifications will be email, but may also need to be SMS in some cases.\nTo reduce the need for each organisation or service team to build its own infrastructure for sending email and SMS based messages, the Government Digital Service (GDS) has built GOV.UK Notify.\nGOV.UK Notify can send notifications either via an API call, or by uploading a CSV to the website. Sending email is free, and the service provides feedback on what messages have failed to be delivered.\nThe service is already used in various parts of CCS, including MISO, Report MI, DigiTS etc.\nUsing GOV.UK Notify instead of other solutions (eg using the BCC field in an email) also reduces the potential for an accidental data breach.\n\n## Decision\nWe will use GOV.UK Notify to send notifications to users of any newly built CCS digital services.\nThis will make sure Notify offers the same user experience across all CCS digital services, and it is currently the most cost-effective (having no-cost for emails) bulk messaging 3rd party solution on the market.\nThe dashboard to create new Notify templates and send bulk emails is very easy, and new Service Teams at CCS can easily be onboarded to use it, with minimal\/no training.\nThis ADR does not apply to generic Email Newsletter communication, for which other, more market specific tools may be more appropriate.\n\n\n## **Context**\nAlongside the decision to adopt an event driven architecture, there is a need to define what an event will look like. There are several options for events:\n- **Thin Events**\nA thin event consists of the minimum amount of data that is required that will allow a subscriber to retrieve everything it needs. This normally consists of an ID with which to make an API call back to the source publisher to gather the data it needs.\nThe benefits of thin events are:\n- The payload is small in size\n- Data is always up to date as it is retrieved at the point of consumption\n- If calls to APIs fail due to unavailability of APIs, the message can be replayed\n- Very little need for event versioning\nThe downsides are:\n- Consumers need to make API calls to gather the data they need\n- **Fat Events**\nA fat event contains all the data necessary for any subscriber to be able to perform its job.\nThe benefits of fat events are:\n- all the data needed for consumer processing is present in the event\n- no need to make any API calls to retrieve data\nThe downsides are:\n- Event payload could grow to be quite big\n- Data present in the payload may no longer be required by any consumer\n- It is difficult to version events easily (and multiple versions of the same event may need to be sent for backwards compatibility)\n**Hybrid Approach**\nIdeally, we should use thin events wherever possible,as this reduces the complexity around sharing events, updating consumers with new versions of events, etc. However, there are some instances where a thin event might not be possible - notably when updating an activity audit log with details of what has changed. Therefore, the best solution would be to have a thin event that contains an optional message body to hold a specific payload.\n\n## Decision\n**Hybrid Approach**\nThe easiest solution is to use a hybrid approach, with consumers gathering the data they need using API calls. This has the benefit of reducing the need for event versioning, and event payloads that grow in size over time.\nThe event payload will be:\n```\n{\n\"id\": \"8e648f3d-9556-4896-8400-211cb1c5451b\",\n\"eventType\": \"personCreated\",\n\"sourceDomain\": \"person\",\n\"sourceSystem\": \"personAPI\",\n\"version\": \"v1\",\n\"correlationId\": \"f4d541d0-7c07-4524-8296-2d0d50cb58f4\",\n\"dateTime\": \"2021-05-17T11:59:57.25Z\",\n\"user\": {\n\"id\": \"ac703d87-c100-40ec-90a0-dabf183e7377\",\n\"name\": \"Joe Bloggs\",\n\"email\": \"joe.bloggs@hackney.gov.uk\"\n},\n\"entityId\": \"45c76564-2e38-48f3-bb31-6bab2fef8623\",\n\"eventBody\": {\n\"oldData\": {\n\"optionalProperty1\": \"Property value\",\n\"optionalProperty2\": \"Property value\",\n\"optionalProperty3\": \"Property value\"\n}\n}\n}\n```\n\n\n\nProvide a decision given the context below:\n## Context and Problem Statement\nAlerts are some noteworthy IT event, like a Node becoming un-ready, login failure or a disk getting full.\nTerminology differs across tooling and organizations, but one generally cares about:\n* P1 (critical) alerts, which require immediate human attention -- the person on-call needs to be notified immediately -- and;\n* P2 (high) alerts which require human attention with 24 hours -- the person on-call needs to be notified next morning;\n* P3 (moderate) alerts which do not require immediate human attention, but should be regularly reviewed.\nOther priorities (e.g., P4 and below) are generally used for informational purposes.\nDealing with alerts correctly entails prioritizing them (e.g., P1, P2, P3), deciding if someone should be notified, who should be notified, how they should be notified (e.g., SMS or email) and when.\n\"Who\", \"how\" and \"when\" should include escalation, if the previous notification was not acknowledged within a pre-configured time interval, then the same person if notified via a different channel or a new person is notified.\nUnder-alerting -- e.g., notifying an on-call person too late -- may lead to Service Level Agreement (SLA) violations and a general feeling of administrator anxiety: \"Is everything okay, or is alerting not working?\".\nOver-alerting -- e.g., notifying a person too often about low-priority alerts -- leads to alert fatigue and \"crying wolf\" where even important alerts are eventually ignored.\nHence, configuring the right level of alerting -- in particular notifications -- is extremely important both for SLA fulfillment and a happy on-call team.\nWhere should alerting be configured, so as to quickly converge to the optimal alerting level?\n## Decision Drivers\n* Allow to quickly silence, un-silence and re-prioritize alerts.\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\n* Leverage existing tools and processes.\n<end_of_turn>\n<start_of_turn>model\n* Allow to quickly silence, un-silence and re-prioritize alerts.\n* Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.\n* Leverage existing tools and processes.\nChosen option: Compliant Kubernetes \u201cover-alerts\u201d, i.e., forwards all alerts and all relevant information to an On-Call Management Tool (OMT, e.g., Opsgenie).\nConfiguration of alerts happens in the OMT.\n### Positive Consequences\n* Clear separation of concerns.\n* Alerting does not require per-customer configuration of Compliant Kubernetes.\n* Leverages existing tools and processes.\n* We do not need to implement complex alert filtering in Compliant Kubernetes, e.g., silence alerts during maintenance windows, silence alerts during Swedish holidays, etc.\n### Negative Consequences\n* Does not capture alerting know-how in Compliant Kubernetes.\n* Migration to a new OMT means all alerting configuration needs to be migrated to the new tool. Fortunately, this can be done incrementally.\n<end_of_turn>\n"}
{"File Name":"adr\/ADR-5-how-to-organize-adr-files.md","Context":"## Context and Problem Statement\\n- How do we call the folder for assets?\\n- What is the pattern of the filename where an ADR is stored?\\n- How do we write titles in the ADR documents?\\n","Decision":"### Asset folder\\nChosen option: `resources`\\n### Filename\\nChosen option: `docs\/ADR-NNNN-title-using-dashes.md` and `docs\/resources\/ADR-NNNN\/{filename}`, because\\n- Adding `arch` or `adr` folders is redundant.\\n- `NNNN` provides a unique number, which can be used for referencing using an immutable number (and generating permalinks if we render the repository to a static site).\\n- The creation time of an ADR is of historical interest only, if it gets updated somehow. It can be embeded in the file or extracted from git history.\\n- Having no spaces in filenames eases working in the command line.\\n- Prefixing with `docs\/ADR-` enables future new kinds of documents.\\n- Calling a folder `docs\/resources\/ADR-NNNN\/{filename}` (omiting the `title-with-dashes`) helps in refactors or changes in the file names.\\n### Title (markdown)\\nChosen option: `# Plain title` because:\\n- At the moment of writing the ADR the writer may not know the final ADR number.\\n- It can be embeded by rendering tools\\n- It would duplicate data from the filename\\n","tokens":46,"id":4619,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context and Problem Statement\nWe needed to clarify the relationship between the DPE Client repository, where the components we are using to populate the Storybook repo have already been written, and the Storybook repository.\n- From which repo would components be published?\n- Which repos would consume components from NPM?\n- Should the Storybook live inside the Client repo?\n## Decision Drivers\nN\/A\n\n## Decision\nN\/A\nChosen option: Option 2, because this allows us to refactor components' code and preview changes within the Storybook locally \u2014 before publishing the component to the hosted Storybook and NPM.\nThis means that our workflow for populating the Storybook and refactoring the Client code is as follows:\n1. Duplicate component code to Storybook repo\n2. Publish completed components to NPM\n3. Remove the original component code from the Client and import via NPM\n### Positive Consequences\n### Negative consequences\nCaveat: If more than one person is working on the Storybook and DPE Client, they'll need to sync up to ensure that details in code refactors are not lost due to overlapping work.\nIf possible, also avoid having people working simultaneously on a component that consumes \/ is consumed by another component (i.e., one person working on a card component and another person working on a list component that consumes card components).\n\n\n## Context or problem to solve\n* How should git histories look like?\n* What format should we use to write commit messages?\n\n## Decision\nWe agreed on normalizing commit messages to master branches to avoid things like `Merge pull request #15 from client\/menduz-patch-10` in persuit of more semantic messages like `fix: commit style guide, closes #15`. That is particularly helpful in repositories with several contributors and fosters professionalism in open source repositories.\n### Branches\nWhen you work on a branch on a specific issue, we keep the spirit of [semantic branch naming](https:\/\/medium.com\/@hanuman_95739\/how-to-integrate-branch-naming-commit-message-and-push-rules-in-gitlab-fe9cd642cc1a). Think of this as writing what is and what you are doing in a three word sentence The first one must be oune of the list. For instance:\n```\nfix\/wrong_host\n^  ^^------------^\n|  ||\n|  |+----> Summary in present tense.\n|  +-----> Slash\n+--------> Type: chore, docs, feat, fix, refactor, style, or test.\n```\nOther examples are:\n```\ndocs\/update_readme\nrefactor\/new_welcome_message\n```\nLook for the *Examples* in section *Commit messages* for a description of the allowed branch types.\nIt's OK to use hyphens (`-`) or underscores (`_`) to replace spaces. Avoid any other special characters, like `#` or `$`, as they might lead to problems, for example, when deploying the content using the branch name as part of the URL. The branch name should match this regexp: `(chore|docs|feat|fix|refactor|style|test)\/[0-9a-zA-Z_-]+`\n#### Exceptions:\nThis rules are ignored when you work on *environment* branches like `master`, `development`, `staging` and so on.\n\n\n\nProvide a decision given the context below:\n## Context and Problem Statement\n- How do we call the folder for assets?\n- What is the pattern of the filename where an ADR is stored?\n- How do we write titles in the ADR documents?\n<end_of_turn>\n<start_of_turn>model\n### Asset folder\nChosen option: `resources`\n### Filename\nChosen option: `docs\/ADR-NNNN-title-using-dashes.md` and `docs\/resources\/ADR-NNNN\/{filename}`, because\n- Adding `arch` or `adr` folders is redundant.\n- `NNNN` provides a unique number, which can be used for referencing using an immutable number (and generating permalinks if we render the repository to a static site).\n- The creation time of an ADR is of historical interest only, if it gets updated somehow. It can be embeded in the file or extracted from git history.\n- Having no spaces in filenames eases working in the command line.\n- Prefixing with `docs\/ADR-` enables future new kinds of documents.\n- Calling a folder `docs\/resources\/ADR-NNNN\/{filename}` (omiting the `title-with-dashes`) helps in refactors or changes in the file names.\n### Title (markdown)\nChosen option: `# Plain title` because:\n- At the moment of writing the ADR the writer may not know the final ADR number.\n- It can be embeded by rendering tools\n- It would duplicate data from the filename\n<end_of_turn>\n"}
{"File Name":"james-project\/0025-cassandra-blob-store-cache.md","Context":"## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n","Decision":"Implement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n","tokens":160,"id":2896,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\nAs an example:\n- Mailbox message headers are expected to be small and frequently accessed\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\n\n## Decision\nImplement a write through cache to have better read latency for smaller objects.\nSuch a cache needs to be distributed in order to be more efficient.\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\nand be queried with a consistency level of ONE.\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\nFailure to read the cache, or cache miss will result in a read in the object storage.\n\n\n## Context\nA given mail is often written to the blob store by different components. And mail traffic is heavily duplicated (several recipients receiving similar email, same attachments). This causes a given blob to often be persisted several times.\nCassandra was the first implementation of the blobStore. Cassandra is a heavily write optimized NoSQL database. One can assume writes to be fast on top of Cassandra. Thus we assumed we could always overwrite blobs.\nThis usage pattern was also adopted for BlobStore on top of ObjectStorage.\nHowever writing in Object storage:\n- Takes time\n- Is billed by most cloud providers\nThus choosing a right strategy to avoid writing blob twice is desirable.\nHowever, ObjectStorage (OpenStack Swift) `exist` method was not efficient enough to be a real cost and performance saver.\n\n## Decision\nRely on a StoredBlobIdsList API to know which blob is persisted or not in object storage. Provide a Cassandra implementation of it.\nLocated in blob-api for convenience, this it not a top level API. It is intended to be used by some blobStore implementations\n(here only ObjectStorage). We will provide a CassandraStoredBlobIdsList in blob-cassandra project so that guice products combining\nobject storage and Cassandra can define a binding to it.\n- When saving a blob with precomputed blobId, we can check the existence of the blob in storage, avoiding possibly the expensive \"save\".\n- When saving a blob too big to precompute its blobId, once the blob had been streamed using a temporary random blobId, copy operation can be avoided and the temporary blob could be directly removed.\nCassandra is probably faster doing \"write every time\" rather than \"read before write\" so we should not use the stored blob projection for it\nSome performance tests will be run in order to evaluate the improvements.\n\n\n\nProvide a decision given the context below:\n## Context\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\nAs an example:\n- Mailbox message headers are expected to be small and frequently accessed\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\n<end_of_turn>\n<start_of_turn>model\nImplement a write through cache to have better read latency for smaller objects.\nSuch a cache needs to be distributed in order to be more efficient.\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\nand be queried with a consistency level of ONE.\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\nFailure to read the cache, or cache miss will result in a read in the object storage.\n<end_of_turn>\n"}
{"File Name":"datalab\/0036-replace-kong-with-nginx-ingress-controller.md","Context":"## Context\\nWe have reached a point where we need to secure third party web applications that provide\\nno security of their own. The Kong proxy does not offer a mechanism for this in the open\\nsource version and we haven't had and response from our efforts to contact them.\\nWe believe that the Nginx Ingress controller that has been available since Kubernetes 1.7\\nwas released provides a Kubernetes native route for the same functionality.\\n","Decision":"We have decided the replace the Kong proxy with an Nginx Ingress Controller in the\\nKubernetes cluster and an Nginx load balancer running on the proxy servers.\\nThis should provide all of the same functionality as Kong and in addition should provide\\na mechanism for token and cookie authentication using the `auth_url` annotation that\\nwraps the underlying Nginx `auth_request` module.\\n","tokens":91,"id":733,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\nserver.\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\nObject Storage technologies.\nObject Storage is known to be cheap and yet offers guarantee about durability.\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\nIt also contribute negatively to the project build time.\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\nour Swift code without dropping Swift support at the same time.\n\n## Decision\n* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\n\n\n## Context\nWe need to persist the user data to the backend. Rather than building our\nfrontend to be tightly coupled to an API, we want a layer that can sit between\nthe application and the API.\nWe know that we will need to support offline use of these forms. We also want\nusers to be able to recover unsubmitted forms if they haven't gone online\nbetween sessions.\nWe also know that some forms will require the upload of images.\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\ndatabase that persists data across sessions. It supports storing blob data (such\nas images), and has decent storage limits.\n\n## Decision\nWe will support IndexedDB as a data store layer, sitting between the application\nand the API.\n\n\n\nProvide a decision given the context below:\n## Context\nWe have reached a point where we need to secure third party web applications that provide\nno security of their own. The Kong proxy does not offer a mechanism for this in the open\nsource version and we haven't had and response from our efforts to contact them.\nWe believe that the Nginx Ingress controller that has been available since Kubernetes 1.7\nwas released provides a Kubernetes native route for the same functionality.\n<end_of_turn>\n<start_of_turn>model\nWe have decided the replace the Kong proxy with an Nginx Ingress Controller in the\nKubernetes cluster and an Nginx load balancer running on the proxy servers.\nThis should provide all of the same functionality as Kong and in addition should provide\na mechanism for token and cookie authentication using the `auth_url` annotation that\nwraps the underlying Nginx `auth_request` module.\n<end_of_turn>\n"}
{"File Name":"buildit-all\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4428,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n\n\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n\n\n\nProvide a decision given the context below:\n## Context\nWe need to record the architectural decisions made on this project.\n<end_of_turn>\n<start_of_turn>model\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\n<end_of_turn>\n"}
{"File Name":"dl4j-dev-tools\/0006-op_specific_enums.md","Context":"## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n","Decision":"We allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values\\nfor this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from\\n`0`.\\nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default\\nvalues match one of the possible values (if applicable).\\nOn code generation, an appropriate representation of this enum will be generated in the target language. The name of\\nthe generated enum will be derived from the name of the arg.\\n### Example\\n```kotlin\\nArg(ENUM, \"padMode\"){\\npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")\\ndescription = \"padding mode\"\\n}\\n```\\n","tokens":31,"id":994,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n# Context and Problem Statement\nUsers want to add custom metadata to items. For example, for invoices\nfields like `invoice-number` or `total`\/`amount` make sense. When\nusing a pagination stamp, every item gets a pagination number.\nThis is currently not possible to realize in docspell. But it is an\nessential part when organizing and archiving documents. It should be\nsupported.\n# Considered Options\n\n## Decision\n- values are strings at the database\n- values are strings when transported from\/to server\n- client must provide the correct formatted strings per type\n- numeric: some decimal number\n- money: decimal number\n- text: no restrictions\n- date: a local date as iso string, e.g. `2011-10-09`\n- bool: either `\"true\"` or `\"false\"`, case insensitive\n\n\n## Context and Problem Statement\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\ndefaults that differ between schematic types.\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\nand directories.\n## Decision Drivers\n* Provide consistency when navigating the codebase\n* The earlier we decide on this, the better\n\n## Decision\n* Provide consistency when navigating the codebase\n* The earlier we decide on this, the better\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\nNaming directories should follow these guidelines:\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\n`import { Box } from '@island.is\/island-ui\/core'`\nUse PascalCase for directories only containing React components:\n````\ncomponents\/CtaButton\/index.ts\nimport 'components\/CtaButton'\n````\nor:\n````\ncomponents\/CtaButton\/CtaButton.tsx\nimport 'components\/CtaButton\/CtaButton'\n````\nrather than\n````\ncomponents\/cta-button\/CtaButton.tsx\n````\nIn all other cases, use camelCase.\n### Positive Consequences\n* Easier to navigate the codebase\n* File names are more readable, and developers know what to expect\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\n\n\n\nProvide a decision given the context below:\n## Context\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\nmakes usage and documentation easier.\n<end_of_turn>\n<start_of_turn>model\nWe allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values\nfor this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from\n`0`.\nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default\nvalues match one of the possible values (if applicable).\nOn code generation, an appropriate representation of this enum will be generated in the target language. The name of\nthe generated enum will be derived from the name of the arg.\n### Example\n```kotlin\nArg(ENUM, \"padMode\"){\npossibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")\ndescription = \"padding mode\"\n}\n```\n<end_of_turn>\n"}
{"File Name":"govuk-design-system-architecture\/001-javascript-for-less-capable-browsers.md","Context":"## Context\\nBefore GOV.UK Frontend, our projects used jQuery for DOM interactions, events and data manipulation.\\nWe\u2019re taking a step back from jQuery due to its lack of support for the browsers we support, its large file size, lack of security updates and from conversations with the community.\\n","Decision":"We\u2019re now writing standard ES5 JavaScript instead, that we polyfill where necessary.\\nThis means that in places where we would have previously used [`$.each`](http:\/\/api.jquery.com\/jquery.each\/) we\u2019re using [`.forEach`](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Array\/forEach) instead, and polyfilling the missing gaps.\\nWe use polyfills provided by the Financial Times\u2019 [Polyfill service](https:\/\/polyfill.io).\\nThis approach ensures that multiple polyfills can be sourced from this service with greater confidence that they\u2019ll work without conflicting with each other.\\nThe Polyfill service does not do runtime detection in browsers and instead opts to do this on the server via user-agent sniffing. It only ships the code needed for that browser, which means newer browsers don\u2019t have to run anything. We may investigate lazy-loading in the future, but for now we\u2019re using a bundled approach based on the lowest common denominator.\\nWe are vendoring these polyfills to avoid any [single point of failure](https:\/\/en.wikipedia.org\/wiki\/Single_point_of_failure) issues that could arise from relying on a CDN. By doing this, we can detect if polyfills are needed at runtime, which results in all browsers getting the same polyfill bundle.\\nWe hope that our approach can be automated or moved into a reusable npm package, based on the Financial Times [npm package](https:\/\/github.com\/Financial-Times\/polyfill-service#library).\\nHere is an [example of polyfilling `addEventListener`](https:\/\/github.com\/alphagov\/govuk-frontend\/blob\/master\/docs\/polyfilling.md).\\nAny polyfills included in GOV.UK Frontend will be tested to work in supported browsers and devices, including assistive technology. Any community discussion and documentation around potential bugs or downsides will also be considered before deciding to include a polyfill in GOV.UK Frontend.\\n","tokens":62,"id":3806,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\nserver.\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\nObject Storage technologies.\nObject Storage is known to be cheap and yet offers guarantee about durability.\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\nIt also contribute negatively to the project build time.\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\nour Swift code without dropping Swift support at the same time.\n\n## Decision\n* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\n\n\n## Context\nWe need to persist the user data to the backend. Rather than building our\nfrontend to be tightly coupled to an API, we want a layer that can sit between\nthe application and the API.\nWe know that we will need to support offline use of these forms. We also want\nusers to be able to recover unsubmitted forms if they haven't gone online\nbetween sessions.\nWe also know that some forms will require the upload of images.\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\ndatabase that persists data across sessions. It supports storing blob data (such\nas images), and has decent storage limits.\n\n## Decision\nWe will support IndexedDB as a data store layer, sitting between the application\nand the API.\n\n\n\nProvide a decision given the context below:\n## Context\nBefore GOV.UK Frontend, our projects used jQuery for DOM interactions, events and data manipulation.\nWe\u2019re taking a step back from jQuery due to its lack of support for the browsers we support, its large file size, lack of security updates and from conversations with the community.\n<end_of_turn>\n<start_of_turn>model\nWe\u2019re now writing standard ES5 JavaScript instead, that we polyfill where necessary.\nThis means that in places where we would have previously used [`$.each`](http:\/\/api.jquery.com\/jquery.each\/) we\u2019re using [`.forEach`](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Array\/forEach) instead, and polyfilling the missing gaps.\nWe use polyfills provided by the Financial Times\u2019 [Polyfill service](https:\/\/polyfill.io).\nThis approach ensures that multiple polyfills can be sourced from this service with greater confidence that they\u2019ll work without conflicting with each other.\nThe Polyfill service does not do runtime detection in browsers and instead opts to do this on the server via user-agent sniffing. It only ships the code needed for that browser, which means newer browsers don\u2019t have to run anything. We may investigate lazy-loading in the future, but for now we\u2019re using a bundled approach based on the lowest common denominator.\nWe are vendoring these polyfills to avoid any [single point of failure](https:\/\/en.wikipedia.org\/wiki\/Single_point_of_failure) issues that could arise from relying on a CDN. By doing this, we can detect if polyfills are needed at runtime, which results in all browsers getting the same polyfill bundle.\nWe hope that our approach can be automated or moved into a reusable npm package, based on the Financial Times [npm package](https:\/\/github.com\/Financial-Times\/polyfill-service#library).\nHere is an [example of polyfilling `addEventListener`](https:\/\/github.com\/alphagov\/govuk-frontend\/blob\/master\/docs\/polyfilling.md).\nAny polyfills included in GOV.UK Frontend will be tested to work in supported browsers and devices, including assistive technology. Any community discussion and documentation around potential bugs or downsides will also be considered before deciding to include a polyfill in GOV.UK Frontend.\n<end_of_turn>\n"}
{"File Name":"service-stac\/2020_10_21_static_asset.md","Context":"## Context\\n`service-stac` needs to serve some static assets for the admin pages (css, images, icons, ...). Django is not appropriate to serve static files on production environment. Currently Django is served directly by `gunicorn`. As a good practice to avoid issue with slow client and to avoid Denial of Service attacks, `gunicorn` should be served behind a Reversed proxy (e.g. Apache or Nginx).\\n","Decision":"Because it is to us not clear yet if a Reverse Proxy is really necessary for our Architecture (CloudFront with Kubernetes Ingress), we decided to use WhiteNoise for static assets. This middleware seems to performs well with CDN (like CloudFront) therefore we will use it to serve static files as it is very simple to uses and take care of compressing and settings corrects Headers for caching.\\n","tokens":91,"id":3173,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nA given mail is often written to the blob store by different components. And mail traffic is heavily duplicated (several recipients receiving similar email, same attachments). This causes a given blob to often be persisted several times.\nCassandra was the first implementation of the blobStore. Cassandra is a heavily write optimized NoSQL database. One can assume writes to be fast on top of Cassandra. Thus we assumed we could always overwrite blobs.\nThis usage pattern was also adopted for BlobStore on top of ObjectStorage.\nHowever writing in Object storage:\n- Takes time\n- Is billed by most cloud providers\nThus choosing a right strategy to avoid writing blob twice is desirable.\nHowever, ObjectStorage (OpenStack Swift) `exist` method was not efficient enough to be a real cost and performance saver.\n\n## Decision\nRely on a StoredBlobIdsList API to know which blob is persisted or not in object storage. Provide a Cassandra implementation of it.\nLocated in blob-api for convenience, this it not a top level API. It is intended to be used by some blobStore implementations\n(here only ObjectStorage). We will provide a CassandraStoredBlobIdsList in blob-cassandra project so that guice products combining\nobject storage and Cassandra can define a binding to it.\n- When saving a blob with precomputed blobId, we can check the existence of the blob in storage, avoiding possibly the expensive \"save\".\n- When saving a blob too big to precompute its blobId, once the blob had been streamed using a temporary random blobId, copy operation can be avoided and the temporary blob could be directly removed.\nCassandra is probably faster doing \"write every time\" rather than \"read before write\" so we should not use the stored blob projection for it\nSome performance tests will be run in order to evaluate the improvements.\n\n\n## Context\nWe need to decide whether to run PostgreSQL on the Puppetmaster itself or use\nRDS from the off.\nCurrently our Puppetmaster uses a local PostgreSQL instance as its database. In moving to AWS we have the opportunity to replace this with an RDS instance.\nThis may remove a reasonable amount of puppet code used to configure the database and associated tasks (e.g. backups). It would also require an amount of Terraform work to provision the RDS instance and some updates to the Puppetmaster code to use that instance.\n\n## Decision\nThe Puppetmaster will continue to use a local instance of PostgreSQL.\n\n\n\nProvide a decision given the context below:\n## Context\n`service-stac` needs to serve some static assets for the admin pages (css, images, icons, ...). Django is not appropriate to serve static files on production environment. Currently Django is served directly by `gunicorn`. As a good practice to avoid issue with slow client and to avoid Denial of Service attacks, `gunicorn` should be served behind a Reversed proxy (e.g. Apache or Nginx).\n<end_of_turn>\n<start_of_turn>model\nBecause it is to us not clear yet if a Reverse Proxy is really necessary for our Architecture (CloudFront with Kubernetes Ingress), we decided to use WhiteNoise for static assets. This middleware seems to performs well with CDN (like CloudFront) therefore we will use it to serve static files as it is very simple to uses and take care of compressing and settings corrects Headers for caching.\n<end_of_turn>\n"}
{"File Name":"new-orbit\/0001-use-openshift-as-cloud-provider.md","Context":"## Context and Problem Statement\\nWe want to deploy our application in docker containers that can be easily updated\\n","Decision":"Chosen option: \"OpenShift\", because\\n* Built on Kubernetes.\\nThe bank has experience on it.\\nProvides a lot of added value tools for CI\/CD, automated builds.\\nIs supported by RedHat and we have a great support contract for it.\\n","tokens":21,"id":4096,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context and Problem Statement\nHow do we serve the API using a production ready server?\n## Decision Drivers\n* use production ready server.\n* good integration with reverse proxy servers like Nginx and Apache.\n* support Flask in a simple and performant way\n\n## Decision\n* use production ready server.\n* good integration with reverse proxy servers like Nginx and Apache.\n* support Flask in a simple and performant way\nChosen option: \"[option 1]\", because it is well known in the\nindustry. It is not the most performant but it has good performances\nto serve WSGI app like Flask thanks to the uwsgi protocol used between\nNginx\/Apache.\n### Negative Consequences\n* complexify the architecture.\n\n\n## Context\nWe need a way to deploy our application to AWS.\n\n## Decision\nWe will use [Terraform] to deploy our application. More specifically,\nwe will use [Chalice] to create the build artifact for the source\ncode (.zip) and then use [Terraform] to manage the infrastructure\nresources.\n\ud83d\udc49 **Note:** Ideally, we would publish the build artifacts to S3 before\ndeployment. That way the deployment process could happen independently\nof the build process.\n\ud83d\udc49 **Note:** To begin with, we'll just convert the JSON data created\nby [Chalice] to [HCL] and add any other resources needed. Going forward,\nwe will use modules to create reusable components.\n\n\n\nProvide a decision given the context below:\n## Context and Problem Statement\nWe want to deploy our application in docker containers that can be easily updated\n<end_of_turn>\n<start_of_turn>model\nChosen option: \"OpenShift\", because\n* Built on Kubernetes.\nThe bank has experience on it.\nProvides a lot of added value tools for CI\/CD, automated builds.\nIs supported by RedHat and we have a great support contract for it.\n<end_of_turn>\n"}
{"File Name":"google-cloud-cpp\/2019-01-04-error-reporting-with-statusor.md","Context":"**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\\nfor all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","Decision":"Instead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","tokens":436,"id":2467,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n**Context**: Compiling google-cloud-cpp from source is not always possible nor\ndesired by all users. Some users may want to install a binary package that\ncontains our public headers and pre-compiled libraries so that they can link\nagainst our library. There is no single standard binary package manager in C++,\ninstead there are a variety of others that users might want to use (e.g., dpkg,\nrpm, vcpkg).\n**Decision**: We will not directly support nor provide any binary packages. We\nwill not test binary distributions of our code. And we will not host configs\nfor binary distributions, since that would involve hosting files which we do\nnot test. However, we will do our best to make our code easily packageable by\nothers in a wide variety of formats. For more context, see\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\n**Consequences**: This decision will shield from endorsing any particular\nbinary package management system. Other individuals who want to build and\nmaintain a binary distribution of our code may do so without our permission or\nknowledge. If they need small and reasonable tweaks from us to support their\nbinary distributions, they may file an issue and we'll do our best to\naccommodate them within reason.\n\n## Decision\nwill not test binary distributions of our code. And we will not host configs\nfor binary distributions, since that would involve hosting files which we do\nnot test. However, we will do our best to make our code easily packageable by\nothers in a wide variety of formats. For more context, see\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\n**Consequences**: This decision will shield from endorsing any particular\nbinary package management system. Other individuals who want to build and\nmaintain a binary distribution of our code may do so without our permission or\nknowledge. If they need small and reasonable tweaks from us to support their\nbinary distributions, they may file an issue and we'll do our best to\naccommodate them within reason.\n\n\n**Context**: Compiling google-cloud-cpp from source is not always possible nor\ndesired by all users. Some users may want to install a binary package that\ncontains our public headers and pre-compiled libraries so that they can link\nagainst our library. There is no single standard binary package manager in C++,\ninstead there are a variety of others that users might want to use (e.g., dpkg,\nrpm, vcpkg).\n**Decision**: We will not directly support nor provide any binary packages. We\nwill not test binary distributions of our code. And we will not host configs for\nbinary distributions, since that would involve hosting files which we do not\ntest. However, we will do our best to make our code easily packageable by others\nin a wide variety of formats. For more context, see\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\n**Consequences**: This decision will shield from endorsing any particular binary\npackage management system. Other individuals who want to build and maintain a\nbinary distribution of our code may do so without our permission or knowledge.\nIf they need small and reasonable tweaks from us to support their binary\ndistributions, they may file an issue and we'll do our best to accommodate them\nwithin reason.\n\n## Decision\nwill not test binary distributions of our code. And we will not host configs for\nbinary distributions, since that would involve hosting files which we do not\ntest. However, we will do our best to make our code easily packageable by others\nin a wide variety of formats. For more context, see\n[#333](https:\/\/github.com\/googleapis\/google-cloud-cpp\/issues\/333).\n**Consequences**: This decision will shield from endorsing any particular binary\npackage management system. Other individuals who want to build and maintain a\nbinary distribution of our code may do so without our permission or knowledge.\nIf they need small and reasonable tweaks from us to support their binary\ndistributions, they may file an issue and we'll do our best to accommodate them\nwithin reason.\n\n\n\nProvide a decision given the context below:\n**Context**: We know there will be users of these C++ libraries who want to use\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\nfor all of our users, regardless of their ability to use exceptions.\n**Decision**: None of our APIs will throw exceptions to indicate errors.\nInstead, our APIs will typically report errors to callers by returning a\n`Status` or `StatusOr<T>` object, unless the library we're using has another\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\nI\/O library).\n**Consequences**: This decision will result in a single set of APIs and a\nconsistent vocabulary for all users, whether or not they choose to compile with\nexceptions. This decision does not prevent callers from using exceptions in\ntheir own code.\nA downside of this decision is that our APIs will not be natural or idiomatic\nfor the [50+%][survey-link] of users who might prefer exceptions for error\nreporting.\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\nbreaking change. As of this writing (Jan 2019), this project has a\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\ntimeline to change this API in a separate document.\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\n<end_of_turn>\n<start_of_turn>model\nInstead, our APIs will typically report errors to callers by returning a\n`Status` or `StatusOr<T>` object, unless the library we're using has another\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\nI\/O library).\n**Consequences**: This decision will result in a single set of APIs and a\nconsistent vocabulary for all users, whether or not they choose to compile with\nexceptions. This decision does not prevent callers from using exceptions in\ntheir own code.\nA downside of this decision is that our APIs will not be natural or idiomatic\nfor the [50+%][survey-link] of users who might prefer exceptions for error\nreporting.\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\nbreaking change. As of this writing (Jan 2019), this project has a\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\ntimeline to change this API in a separate document.\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/bigtable\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/main\/google\/cloud\/storage\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\n<end_of_turn>\n"}
{"File Name":"beis-report-official-development-assistance\/0005-use-travis-for-ci.md","Context":"## Context\\nTravis is actively used for many other projects within the BEIS GitHub account.\\nWe know we will want to be using containers to run this application, so the tool we use must support Docker too.\\n","Decision":"Use Travis to build and test the application.\\nUse Travis to automatically deploy the applications to live.\\n","tokens":45,"id":2393,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [Stylelint](https:\/\/stylelint.io\/) is one of the more popular CSS\nlinters with support for SASS, and is easily configurable for our purposes.\n\n## Decision\nWe will check SASS syntax using Stylelint.\nWe will use the recommended configuration for plugins where possible.\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\n\n\n## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\nJavaScript, and has good support for TypeScript though plugins.\n\n## Decision\nWe will check code style using ESLint.\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\nstyles.\nWe will use the recommended configuration for plugins where possible.\nWe will run ESLint as part of the test suite.\n\n\n\nProvide a decision given the context below:\n## Context\nTravis is actively used for many other projects within the BEIS GitHub account.\nWe know we will want to be using containers to run this application, so the tool we use must support Docker too.\n<end_of_turn>\n<start_of_turn>model\nUse Travis to build and test the application.\nUse Travis to automatically deploy the applications to live.\n<end_of_turn>\n"}
{"File Name":"cloud-on-k8s\/0006-sidecar-health.md","Context":"## Context and Problem Statement\\nThis proposal aims to outline possible approaches to report the Elasticsearch sidecar health in combination with the cluster state for cluster-wide monitoring and alerting.\\nAs for now an Elasticsearch pod is composed of 2 containers:\\n- a main container for Elasticsearch\\n- a sidecar container for running the keystore-updater\\nWhat is the keystore-updater in the sidecar doing?\\nIt calls the Elasticsearch endpoint `\/_nodes\/reload_secure_settings` to decrypt and re-read the entire keystore used by the snapshotter job.\\nTo connect to ES it depends on:\\n- an environment variable for the username\\n- secrets mounted as readonly files for the password and the CA certificate\\n- the Elasticsearch readiness\\nCurrently there is no health check based on the state of the sidecar. The sidecar can error without anyone ever noticing this state.\\nSo there is a need to check that everything is correctly setup in the sidecar container and the call to the ES API succeeds.\\nIf the sidecar container is not ready, the Elasticsearch container is impacted because the pod is considered not ready and\\nKubernetes stops to send traffic to the pod. We must accept that the two containers are intimately linked. A sidecar failure\\ncan impact the Elasticsearch availability by design.\\nHowever Go binaries that do simple things are very fast to start and very reliable.\\nFrom that we could admit that the probability to have a failure in the sidecar that runs a simple go binary is very low\\ncompared to have an Elasticsearch failure.\\nAnother challenge is to take into account that some sidecar errors are to be expected when ES is not ready yet.\\nThis can be mitigated by considering a start-up delay during which it is accepted that ES is not ready and\\ndo not report errors during this period. Then how to detect that ES has never started?\\nThe ES readiness probe will fail if ES never becomes ready.\\n## Decision Drivers\\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\n","Decision":"* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\\n* Promote reliability and simplicity because health-checking is a critical part of the system\\nChosen option: option 4, because it gives us more flexibility to take decisions in case of failure, it does not depend on Kubernetes probes\/kubelet and it does not depend on external components.\\n### Positive Consequences\\n* Collecting the sidecar health from the operator side gives us more options to react to failures\\n* Having an HTTP server in the sidecar brings more flexibility to interact with the pod\\n* Does not depend on the Kubernetes probes or the Kubelet\\n* Minimize external dependencies\\n### Negative Consequences\\n* Increase a little the failure domain of the sidecar with the presence of the HTTP server\\n* Add complexity and responsibility to the operator\\n","tokens":455,"id":4705,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nSupporting a new feature, the so called [signal enrichment](https:\/\/github.com\/eclipse-ditto\/ditto\/issues\/561), raises a few\nquestions towards throughput and scalability impact of that new feature.\nIn the current architecture, Ditto internally publishes events (as part of the applied \"event sourcing\" pattern) for\neach change which was done to a `Thing`. This event is the same as the persisted one only containing the actually\nchanged fields.\nThe \"signal enrichment\" feature shall support defining `extraFields` to be sent out to external event subscribers, e.g.\nbeing notified about changes via WebSocket, Server Sent Events (SSEs) or connections (AMQP, MQTT, Kafka, ...).\nThe following alternatives were considered on how to implement that feature:\n1. Sending along the complete `Thing` state in each event in the cluster\n* upside: \"tell, don't ask\" principle -> would lead to a minimum of required cluster remoting \/ roundtrips\n* downside: bigger payload sent around\n* downside: a lot of deserialization effort for all event consuming services\n* downside: policy filtering would have to be additionally done somewhere only included data which the `authSubject` is allowed to READ\n* downside: overall a lot of overhead for probably only few consumers\n2. Enriching the data for sessions\/connections which selected `extraFields` for each incoming event\n* upside: no additional payload for existing events\n* upside: data is only enriched for sessions\/connections really using that feature\n* upside: policy enforcement\/filtering is done by default concierge mechanism for each single request, so is always up-to-date with policy\n* downside: additional 4 remoting (e.g.: gateway-concierge-things-concierge-gateway) calls for each to be enriched event\n* delayed event publishing\n* additional deserialization efforts\n* potentially asking for the same static values each time\n3. Cache based enriching of the data for sessions\/connections which selected `extraFields` for each incoming event\n* upsides: all upsides of approach 2 except that policy is always up-to-date\n* upside: mitigating downsides of approach 2 (because of cache the additional roundtrips are reduced or even completely skipped)\n* downside: cached data as well as policy information might be outdated a configurable amount of time (e.g. 2 minutes)\n\n## Decision\nWe provide 2 different facade implementations providing approach 2 and 3:\n* `org.eclipse.ditto.internal.models.signalenrichment.ByRoundTripSignalEnrichmentFacade`:\nRound-trip for each to-be-enriched event resulting in a guaranteed up-to-dateness of data and applied policy.\n* `org.eclipse.ditto.internal.models.signalenrichment.DittoCachingSignalEnrichmentFacade`:\nUsing cache for each to-be-enriched event resulting in reduced remoting effort and a time interval where the cache might be out of sync with the current data or policy information.\n* the implementation uses a cluster-instance wide cache using a cache key consisting of: `thingId, authSubjects, jsonFieldSelector`\n* the overall size of this cache is configured, by default to `20,000` entries\n* there is an additional \"smart-update\" mechanism for cache entries related to enrichment of twin events:\nin the absence of skipped events, the cache entry can be completely deduced from the twin events triggering\nenrichment and will stay up-to-date with thing changes (but not with policy changes).\nThe configured default in Ditto is the `CachingSignalEnrichmentFacade` but may be configured via\n* connectivity service: environment variable `CONNECTIVITY_SIGNAL_ENRICHMENT_PROVIDER`\n* gateway service: environment variable `GATEWAY_SIGNAL_ENRICHMENT_PROVIDER`\n\n\n## Context and Problem Statement\nTax users are sometimes, without warning, unnecessarily signed out when accessing\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\nsuccess criterion 2.1.1 (Timing adjustable)](https:\/\/www.w3.org\/WAI\/WCAG21\/Understanding\/timing-adjustable.html).\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https:\/\/github.com\/hmrc\/play-frontend-hmrc#warning-users-before-timing-them-out)\ncurrently has no way of knowing about this activity following initial page load.\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\nremaining on the user's active session via an endpoint that is itself excluded from\nsession management.\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\nknowledge and avoiding introducing additional coupling between frontend microservices?\n## Decision Drivers\n* The need to minimise code changes for service teams other than a library upgrade.\n* The avoidance of requiring service teams to add any additional routing rules.\n* The avoidance of requiring service teams to add any additional configuration.\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\nany other library or service.\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\nof failure between frontend microservices.\n* The need for the endpoint used to interrogate the session to not itself affect the session.\n\n## Decision\n* The need to minimise code changes for service teams other than a library upgrade.\n* The avoidance of requiring service teams to add any additional routing rules.\n* The avoidance of requiring service teams to add any additional configuration.\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\nany other library or service.\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\nof failure between frontend microservices.\n* The need for the endpoint used to interrogate the session to not itself affect the session.\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\naround knowledge duplication.\n\n\n\nProvide a decision given the context below:\n## Context and Problem Statement\nThis proposal aims to outline possible approaches to report the Elasticsearch sidecar health in combination with the cluster state for cluster-wide monitoring and alerting.\nAs for now an Elasticsearch pod is composed of 2 containers:\n- a main container for Elasticsearch\n- a sidecar container for running the keystore-updater\nWhat is the keystore-updater in the sidecar doing?\nIt calls the Elasticsearch endpoint `\/_nodes\/reload_secure_settings` to decrypt and re-read the entire keystore used by the snapshotter job.\nTo connect to ES it depends on:\n- an environment variable for the username\n- secrets mounted as readonly files for the password and the CA certificate\n- the Elasticsearch readiness\nCurrently there is no health check based on the state of the sidecar. The sidecar can error without anyone ever noticing this state.\nSo there is a need to check that everything is correctly setup in the sidecar container and the call to the ES API succeeds.\nIf the sidecar container is not ready, the Elasticsearch container is impacted because the pod is considered not ready and\nKubernetes stops to send traffic to the pod. We must accept that the two containers are intimately linked. A sidecar failure\ncan impact the Elasticsearch availability by design.\nHowever Go binaries that do simple things are very fast to start and very reliable.\nFrom that we could admit that the probability to have a failure in the sidecar that runs a simple go binary is very low\ncompared to have an Elasticsearch failure.\nAnother challenge is to take into account that some sidecar errors are to be expected when ES is not ready yet.\nThis can be mitigated by considering a start-up delay during which it is accepted that ES is not ready and\ndo not report errors during this period. Then how to detect that ES has never started?\nThe ES readiness probe will fail if ES never becomes ready.\n## Decision Drivers\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\n* Promote reliability and simplicity because health-checking is a critical part of the system\n<end_of_turn>\n<start_of_turn>model\n* Error distinction: a sidecar failure should be easily identified from an Elasticsearch failure\n* Error side effect: a sidecar failure should not increase the unavailability of Elasticsearch compared to the current situation\n* Promote reliability and simplicity because health-checking is a critical part of the system\nChosen option: option 4, because it gives us more flexibility to take decisions in case of failure, it does not depend on Kubernetes probes\/kubelet and it does not depend on external components.\n### Positive Consequences\n* Collecting the sidecar health from the operator side gives us more options to react to failures\n* Having an HTTP server in the sidecar brings more flexibility to interact with the pod\n* Does not depend on the Kubernetes probes or the Kubelet\n* Minimize external dependencies\n### Negative Consequences\n* Increase a little the failure domain of the sidecar with the presence of the HTTP server\n* Add complexity and responsibility to the operator\n<end_of_turn>\n"}
{"File Name":"terraform-aws-dynamodb\/0005-feature-toggles.md","Context":"## Context\\nToggles are a useful aspect of Terraform modules that provide the ability to\\nturn on\/off functionality.\\nCapital One have written about\\n[Building Feature Toggles into\\nTerraform](https:\/\/medium.com\/capital-one-tech\/building-feature-toggles-into-terraform-d75806217647)\\n. This build on top of the use of the count and ternary to provide a more robust\\nfeature toggle.\\nAs this module is envisioned to be part of a larger Data Storage Module there is\\na concept where no DynamoDB table may need to be provisioned. In addition, when\\nusing DynamoDB Autoscaling can be a useful feature. Providing the capabilities\\nto turn this on and off based on requirements is also a useful requirement.\\n","Decision":"Feature toggles will be leveraged to determine:\\n* If any resources should be created\\n* If a DynamoDB resource should be created\\n* If the Autoscaler should be enabled\\n","tokens":160,"id":2538,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nThe main data source of the API catalogue website was originally a collection of\nmarkdown files. The build process uses the Middleman static site generator\nconfigured by the [Tech Docs Gem](https:\/\/github.com\/alphagov\/tech-docs-gem)\n('TDG').\nThe TDG provides additional functionality including search, sidebar\nnavigation ('Table of Contents'), the layout, and styling.\nThe TDG is not necessarily a good fit for the API catalogue because the project\nisn't purely documentation, and our data source is now a CSV.\nIn particular it is difficult to override templates inherited from the gem, to\nadjust the layout on a particular page or add page-specific JavaScript for\nexample.\nUsing TDG to render the Table of Contents is slow for our site because\nby design every page is re-rendered multiple times to pull out the headings\n(adding over a minute to build times).\nThe TDG also requires specific dependency versions. These version\nrestrictions prevent us being in control of version upgrades which are necessary\nto remain on support versions and receive security patches.\n\n## Decision\nRemove the TDG as a dependency by vendoring the code relevant to\nthe API catalogue directly into the project itself.\n\n\n## Context\nIstio's service mesh capabilites are facilitated via sidecars injected into\nworkload pods. These sidecars run an Istio-patched version of Envoy that is tied\nto the version of Istio that injects them.\nTypically when new versions of Istio are released, new versions of the sidecars\nare released as well. Istio has been good so far about supporting older versions\nof sidecars that were deployed before Istio was upgraded, but it is still\n[documented best practice](https:\/\/istio.io\/latest\/docs\/setup\/upgrade\/) to roll\nall the pods after an Istio upgrade.\nAs an additional constraint, the operators of cf-for-k8s clusters expect to be\nable to perform upgrades in one `kapp deploy`, with no post-install hooks or\nother bash scripts. This limits our options considerably. See this [Slack\nthread](https:\/\/cloudfoundry.slack.com\/archives\/CH9LF6V1P\/p1592521879117400) on\nthat constraint.\n\n## Decision\nWe will use the kubernetes\n[Job](https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/job\/) resource\nto run the kubectl command needed to roll workload pods, after waiting for the\nnew Istio control plane to be up and healthy.\nTo that end, we will add the necessary minimal `ServiceAccounts` and `Roles`\nneeded to list resources in the `istio-system` namespace, and restart resources\nin the configured workload namespace. We will also build and maintain a\ncontainer image that contains the Job's logic.\nAll istio components will be tagged with their Istio version so that the job can\npositively determine that the correct version of control plane components are\nalive and healthy. We will also name the job according to it's Istio version, so\nthat we can take advantage of `Jobs` inherent immutability in cases where a\ncf-for-k8s upgrade does not contain a new Istio version (pushing the same job\nagain will not cause it to rerun, preventing workloads from rolling\nunnecessarily). Subsequent jobs will clean up previous ones.\n\n\n\nProvide a decision given the context below:\n## Context\nToggles are a useful aspect of Terraform modules that provide the ability to\nturn on\/off functionality.\nCapital One have written about\n[Building Feature Toggles into\nTerraform](https:\/\/medium.com\/capital-one-tech\/building-feature-toggles-into-terraform-d75806217647)\n. This build on top of the use of the count and ternary to provide a more robust\nfeature toggle.\nAs this module is envisioned to be part of a larger Data Storage Module there is\na concept where no DynamoDB table may need to be provisioned. In addition, when\nusing DynamoDB Autoscaling can be a useful feature. Providing the capabilities\nto turn this on and off based on requirements is also a useful requirement.\n<end_of_turn>\n<start_of_turn>model\nFeature toggles will be leveraged to determine:\n* If any resources should be created\n* If a DynamoDB resource should be created\n* If the Autoscaler should be enabled\n<end_of_turn>\n"}
{"File Name":"elasticsearch-updater\/0002-store-configurations-in-repository.md","Context":"## Context\\nThe application loads data from a JSON file into an Elasticsearch instance. Along with the raw data the import\\nalso needs to create mappings and transform data to improve search rankings or provide geolocation searches.\\nThese are rich complex JSON objects or functions that cannot be passed in as environment variables.\\n","Decision":"Given the small number of databases (currently only GP Profiles data is held in Elasticsearch) it is pragmatic to co-locate the Elasticsearch configuration alongside the `elasticsearch-updater` code, rather than creating a new repository and file hosting for each mappings and transform.\\n","tokens":61,"id":485,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nThe decision records must be stored in a plain text format:\n* This works well with version control systems.\n* It allows the tool to modify the status of records and insert\nhyperlinks when one decision supercedes another.\n* Decisions can be read in the terminal, IDE, version control\nbrowser, etc.\nPeople will want to use some formatting: lists, code examples,\nand so on.\nPeople will want to view the decision records in a more readable\nformat than plain text, and maybe print them out.\n\n## Decision\nbrowser, etc.\nPeople will want to use some formatting: lists, code examples,\nand so on.\nPeople will want to view the decision records in a more readable\nformat than plain text, and maybe print them out.\nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\nDecisions will be formatted nicely and hyperlinked by the\nbrowsers of project hosting sites like GitHub and Bitbucket.\nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convert\nthe decision records into HTML or PDF.\n\n\n## Context\nCurrently help text is generated by extracting specially formatted comments from the top of the command script.\nThis makes it easy for developers of the tool:  documentation and code is all in one place.\nBut, it means that help text cannot include calculated values, such as the location of files.\n\n## Decision\nWhere necessary, help text can be generated by a script.\nThe script will be called _adr_help_<command>_<subcommand>\n\n\n\nProvide a decision given the context below:\n## Context\nThe application loads data from a JSON file into an Elasticsearch instance. Along with the raw data the import\nalso needs to create mappings and transform data to improve search rankings or provide geolocation searches.\nThese are rich complex JSON objects or functions that cannot be passed in as environment variables.\n<end_of_turn>\n<start_of_turn>model\nGiven the small number of databases (currently only GP Profiles data is held in Elasticsearch) it is pragmatic to co-locate the Elasticsearch configuration alongside the `elasticsearch-updater` code, rather than creating a new repository and file hosting for each mappings and transform.\n<end_of_turn>\n"}
{"File Name":"klokwrk-project\/0002-strategic-project-structure.md","Context":"## Context\\nExcluding the simplest hello-world-like cases, any useful project typically contains several modules. The traditional way to organize project modules is just to put them under the project root.\\nWe can call that structure simply **flat structure**.\\nWhile the flat structure is appropriate and sufficient for simpler projects, when the project grows and the number of modules increases, the flat structure starts suffering from many drawbacks:\\n* Flat structure does not scale well when the number of modules grows.\\n* Flat structure is difficult and confusing to navigate with numerous modules at the same hierarchy level.\\n* Flat structure does not suggest a direction of dependencies between modules.\\n* Flat structure does not suggest abstraction levels of modules.\\n* Flat structure does not suggest where are the system's entry points.\\n* Flat structure can use only module names to provide hints about relations between modules. Unfortunately, even that possibility is rarely leveraged.\\n* Flat structure does not use any high-level constructs that may suggest how modules are organized and related.\\n* Negative usage aspects are getting worse and worse as we add additional modules.\\n* Flat structure often requires extracting modules in separate repositories just because confusion becomes unbearable with a larger number of modules.\\n* When using microservices, the flat structure practically forces us to use one project per microservice.\\n> Note: Terms **flat structure** and **strategic structure** (see below) are ad-hoc terms introduced just for this document. However, in the `klokwrk-project`, we may use them in other places for\\n> convenience.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n","Decision":"**We'll organize project modules around strategic DDD (Domain Driven Design) constructs of bounded context and subdomains.**\\nOur project organization will follow principles and recommendations of **strategic structure** as defined below.\\n### Decision Details\\nWe'll start with a concrete example of the strategic structure used in the klokwrk at the time of writing this document. As a follow-up, we'll present a general scheme for creating the strategic\\nstructure focusing on the differences to the given concrete example.\\n#### Strategic structure in klokwrk\\nThe current project layout in the klokwrk looks like this:\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 asd\\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 booking\\n\u2502   \u2502       \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-commandside\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-view\\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-rdbms-management\\n\u2502   \u2502       \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-boundary-web\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-out-customer\\n\u2502   \u2502       \u2502               cargotracking-booking-lib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502               cargotracking-booking-test-component\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-queryside\\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-testcontainers\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502               cargotracking-lib-axon-cqrs\\n\u2502   \u2502               cargotracking-lib-axon-logging\\n\u2502   \u2502               cargotracking-lib-boundary-api\\n\u2502   \u2502               cargotracking-lib-boundary-query-api\\n\u2502   \u2502               cargotracking-lib-domain-model-command\\n\u2502   \u2502               cargotracking-lib-domain-model-event\\n\u2502   \u2502               cargotracking-lib-web\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-datasourceproxy-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-jackson-springboot\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-context\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-data-jpa\\n\u2502   \u2502   \u2502       klokwrk-lib-hi-validation-springboot\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-archunit\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-datasourceproxy\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-hibernate\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-jackson\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-uom\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-constraint\\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-validator\\n\u2502   \u2502   \u2502\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-base\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-match\\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-simple\\n\u2502   \u2502\\n\u2502   \u2514\u2500\u2500 other\\n\u2502       \u251c\u2500\u2500 platform\\n\u2502       \u2502       klokwrk-platform-base\\n\u2502       \u2502       klokwrk-platform-micronaut\\n\u2502       \u2502       klokwrk-platform-spring-boot\\n\u2502       \u2502\\n\u2502       \u2514\u2500\u2500 tool\\n\u2502               klokwrk-tool-gradle-source-repack\\n\u251c\u2500\u2500 support\\n\u2502   \u2514\u2500\u2500 ... (other files or directories)\\n\u2514\u2500\u2500 ... (other files or directories)\\nAt the top of the hierarchy, we have a project folder  - `klokwrk-project`. It is the equivalent of the whole system. In the strategic structure, the system name appears in the names of artifacts\\nconsidered to be conceptually at the level of a system.\\nRight below the root, we have `modules` and `support` folders. These should be the area of 99% of everyday work, with the `modules` folder taking a vast majority of that percentage.\\nThe `support` folder houses all kinds of supportive files like scripts, documentation, git hooks, etc. The `support` folder is free-form, and the strategic structure does not impose any\\nrecommendations or rules on its content. On the contrary, the strategic structure is applied to the content of the `modules` directory - the home of all source code modules in the system.\\nAt the 1st level of strategic structure - the system level, we have the content of the `modules` directory. It is divided into three subdirectories: `bc` (bounded context modules),\\n`lib` (system-level libraries), and `other` (miscellaneous helper modules).\\nAt the 2nd level - the bounded context level, we have the content of the `modules\/bc` directory that is further organized into three parts, `asd` (asd stands for **A** **S**ub**D**omain),\\n`domain-model` (bounded context domain model), and `lib` (bounded context libraries).\\nAt the 3rd level of a hierarchy, we have the content of the `modules\/bc\/[bounded-context-name]\/asd` directory that holds all bounded context's subdomains. The modules for each subdomain are further\\ndivided into `app` and `lib`. The `modules\/bc\/[bounded-context-name]\/asd\/[subdomain-name]\/app` directory contains the **subdomain applications** responsible for implementing concrete subdomain\\nscenarios. From the abstraction level and dependency perspectives, subdomain applications are at the top of the hierarchy. Subdomain applications speak the language of domain - the bounded context's\\nubiquitous language. They even contribute to it through the naming and meaning of use cases.\\nThe first thing that **subdomain libraries** (`modules\/bc\/[bounded-context-name]\/asd\/subdomain-name\/lib)` can hold is infrastructural code related to the technological choices made for that\\nparticular subdomain and are not reusable outside the subdomain. However, they can temporarily have infrastructural modules intended to be more reusable (either on the bounded context or system\\nlevels) at the end. Still, for whatever reason, it was more convenient to hold them at the subdomain level for a limited time.\\nThe second thing that can be found in subdomain libraries are business-related reusable modules that connect technological choices with the domain model. One characteristic example is the\\n`cargotracking-booking-lib-queryside-model-rdbms-jpa` module. Those kinds of modules do speak bounded context's ubiquitous language.\\nThe bounded context's **domain model** is implemented in `modules\/bc\/[bounded-context-name]\/domain-model`. Those modules contain the essence of the bounded context business logic. Implementation of\\nthe domain model should be free of technology as much as possible and practical. Adding external libraries is not strictly forbidden, but each addition should be conscious and must be carefully\\nevaluated. It is best to have tests that monitor and control the dependencies of a domain model. The domain model implements the majority of code-level representation of the bounded context's\\nubiquitous language and must be consistent across all bounded context's subdomains.\\nBy default, the directory `modules\/bc\/[bounded-context-name]\/lib` is the home of shareable **bounded context infrastructural libraries**. It contains modules with infrastructural code that is\\nreusable across the bounded context. Those modules are at a lower abstraction level than subdomain libraries. Bounded context infrastructural libraries do not speak domain language. However, they can\\nsupport the implementation of the domain model and other module groups higher in the hierarchy. Domain model should not generally depend on bounded context infrastructural libraries. Exceptions are\\nallowed but should be conscious and carefully managed.\\nDo note that another variant of bounded context libraries is also possible. It is a variant supporting the sharing of business logic at the bounded context level when necessary. In that case, instead\\nof a single `lib` directory, we would have `blib` and `ilib` directories. The `blib` directory would contain business-related modules that can depend on a domain model. On the contrary, the `ilib`\\ndirectory cannot use the domain model because it should contain infrastructural code only. The `ilib` directory role is the same as the role of `lib` directory from the default variant of bounded\\ncontext libraries.\\nLet's return to the `modules\/lib` directory containing general **system-level libraries**. It is divided into `hi`, `lo`, and `xlang` subdirectories. All system-level libraries are at lower\\ndependency and abstraction levels than any bounded context module.\\nAlthough separation on the high (`hi`) and low-level (`lo`) system libraries is somewhat arbitrary, it is helpful in practice. The `hi` directory is intended to contain\\n**high-level system libraries**, which are general infrastructural modules closer to the high-level technological frameworks (something like Spring, Spring Boot, or Axon frameworks) used in the\\nsystem. They could contain some specifics of our system, but usually, they do not. In that later case, they are general enough to be reused even outside of our system.\\nThe **low-level system libraries** from the `lo` directory deal with the customizations and extensions of widely used 3rd party libraries like Hibernate, Jackson, Java Bean validations, and similar.\\nBoth types of system-level libraries should not be, in general, dependencies of a domain model.\\nAt the lowest abstraction level, we have the **language extensions** (`modules\/lib\/xlang`). They focus on adding features to the programming language itself or its accompanying SDK (JDK in our case).\\nLanguage extensions can be used from everywhere, even from the domain model, without restrictions. Some of them are often written to ease the implementation of the domain model by making it more\\nexpressive and concise.\\n#### Characteristics of strategic structure\\nThe most important thing about strategic structure is not the structure itself but rather the distinguishing characteristics that it provides.\\nWe already mentioned abstraction levels and dependencies between groups of modules. If you look again at the example, you will notice that both of them are constantly flowing top to bottom through\\nthe strategic structure. For instance, subdomain applications depend on subdomain libraries. They both can depend on the domain model, which can depend on bounded context libraries and language\\nextensions. At the level of system libraries, high-level modules can depend on low-level modules, and they both can depend on the language extensions. However, none of the dependencies can come the\\nother way around. Dependencies are not allowed to flow from the bottom to the top.\\nWe have managed to do this because we applied strategic DDD concepts of bounded context and subdomains to the project structure. They provide sense and meaningfulness by connecting our code to the\\nbusiness. Without that business context, we will be left exclusively to the technical aspects, which are just insufficient. Technical aspects know nothing about the purpose of our system. They do not\\nknow anything about the business context.\\nDescribed characteristics bring important benefits when trying to understand or navigate through the system's code. Finding the desired functionality is much easier because we usually know, at least\\napproximately, where we should look for it. This can greatly reduce cognitive load while exploring unfamiliar (or even familiar) codebases.\\nIn addition, if you follow the proposed naming conventions for modules and their packages (see below), the same easy orientation can be applied at the package level or even if you pull out all\\nmodules into the flat structure. You will always know where to look for.\\n#### Naming conventions\\nYou have probably noticed that modules have very particular names reflecting their position in the strategic structure. The following table summarizes them as used in the example:\\n| Module group    | Naming scheme                                            | Example                                  |\\n|-----------------|----------------------------------------------------------|------------------------------------------|\\n| subdomain apps  | `[bounded-context-name]-[subdomain-name]-app-[app-name]` | `cargotracking-booking-app-commandside`  |\\n| subdomain libs  | `[bounded-context-name]-[subdomain-name]-lib-[lib-name]` | `cargotracking-booking-lib-boundary-web` |\\n| domain model    | `[bounded-context-name]-domain-model-[model-part-name]`  | `cargotracking-domain-model-aggregate`   |\\n| bc libs         | `[bounded-context-name]-lib-[lib-name]`                  | `cargotracking-lib-boundary-api`         |\\n| sys hi libs     | `[system-name]-lib-hi-[lib-name]`                        | `klokwrk-lib-hi-spring-context`          |\\n| sys lo libs     | `[system-name]-lib-lo-[lib-name]`                        | `klokwrk-lib-lo-jackson`                 |\\n| lang extensions | `[system-name]-lib-xlang-[lib-name]`                     | `klokwrk-lib-xlang-groovy-base`          |\\nModule naming conventions are essential because our modules are not always presented (i.e., try the Packages view in the IntelliJ IDEA's Project tool window) or used as a part of the hierarchy (think\\nof JAR names put in the same directory). For those reasons, our naming scheme closely follows the strategic structure hierarchy where parts of module names are directly pulled from corresponding\\nsubdirectory names. That way, we can keep the match between alphabetical order and the direction of dependencies.\\n> Note: When you have multiple bounded contexts and\/or multiple subdomains in the project, to get the exact match between alphabetical order and the direction of dependencies, you can use the `bc-`\\n> prefix in front of bounded context names and the `asd-` prefix for subdomain names.\\nThe same naming principles should also be applied to packages. Here are a few examples of package names:\\norg.klokwrk.cargotracking.booking.app.commandside.*\\norg.klokwrk.cargotracking.booking.lib.boundary.web.*\\norg.klokwrk.cargotracking.domain.model.aggregate.*\\norg.klokwrk.cargotracking.lib.boundary.api.*\\norg.klokwrk.lib.hi.spring.context.*\\norg.klokwrk.lib.lo.jackson.*\\norg.klokwrk.lib.xlang.groovy.base.*\\nWith those naming conventions, we should be able to avoid naming collisions on the module and package levels.\\n#### The general scheme of strategic structure\\nIn some circumstances, we may need additional elements in the strategic structure to deal with shared libraries at different levels. Examples of those, with sparse explanations, are given in the\\ngeneral scheme of strategic structure below:\\nmodules\\n\u251c\u2500\u2500 bc\\n\u2502   \u251c\u2500\u2500 my_food\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 restaurant\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 menu_management\\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 zshared         \/\/ sharing code between subdomains if necessary\\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib                 \/\/ bounded context libraries - default variant\\n\u2502   \u2502           ... *           \/\/ Can be split into \"blib\" and \"ilib\" directories when the sharing of\\n\u2502   \u2502                           \/\/ business logic is necessary at the level of a single bounded context\\n\u2502   \u251c\u2500\u2500 my_carrier\\n\u2502   \u2502   \u251c\u2500\u2500 asd\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 app\\n\u2502   \u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502   \u2502           ... *\\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 lib\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 zshared                 \/\/ shared code between multiple bounded contexts (if necessary).\\n\u2502       \u2502                       \/\/ \"z\" prefix - funny reference to \"zee Germans\" from Snatch movie.\\n\u2502       \u2502                       \/\/ Moves \"zshared\" at the last place alphabetically, which matches\\n\u2502       \u2502                       \/\/ the proper place in terms of dependencies and abstraction levels.\\n\u2502       \u251c\u2500\u2500 domain-model\\n\u2502       \u2502       ... *\\n\u2502       \u2514\u2500\u2500 lib\\n\u2502               ... *\\n\u251c\u2500\u2500 lib\\n\u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502       ... *\\n\u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502       ... *\\n\u2502   \u2514\u2500\u2500 xlang\\n\u2502           ... *\\n\u2514\u2500\u2500 other            \/\/ supportive project's code for various \"other\" purposes\\n\u251c\u2500\u2500 build\\n\u2502       ... *\\n\u251c\u2500\u2500 tool\\n\u2502       ... *\\n\u2514\u2500\u2500 ...\\n#### Simplification - the case of bounded context boundaries matching 1:1 with subdomain\\nThe one-to-one match between bounded context boundaries and corresponding subdomain is considered to be the \"ideal\" case, and it is relatively common in practice. When we know how a fully expanded\\nstrategic structure works and looks like, it is relatively easy to come up with simplification for this particular case.\\nHere are \"refactoring\" steps and the example based on our concrete example from the beginning of this document:\\n- move subdomain applications to the bounded context level\\n- merge subdomain libraries with bounded context libraries\\n- split bounded context libraries into `blib` and `ilib` directories if necessary\\n- rename corresponding modules and packages\\nklokwrk-project\\n\u251c\u2500\u2500 ... (other files or directories)\\n\u251c\u2500\u2500 modules\\n\u2502   \u251c\u2500\u2500 bc\\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\\n\u2502   \u2502       \u251c\u2500\u2500 app\\n\u2502   \u2502       \u2502       cargotracking-app-commandside\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-projection-rdbms\\n\u2502   \u2502       \u2502       cargotracking-app-queryside-view\\n\u2502   \u2502       \u2502       cargotracking-app-rdbms-management\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 blib\\n\u2502   \u2502       \u2502       cargotracking-blib-out-customer\\n\u2502   \u2502       \u2502       cargotracking-blib-queryside-model-rdbms-jpa\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\\n\u2502   \u2502       \u2502\\n\u2502   \u2502       \u2514\u2500\u2500 ilib\\n\u2502   \u2502               cargotracking-ilib-axon-cqrs\\n\u2502   \u2502               cargotracking-ilib-axon-logging\\n\u2502   \u2502               cargotracking-ilib-boundary-api\\n\u2502   \u2502               cargotracking-ilib-boundary-query-api\\n\u2502   \u2502               cargotracking-ilib-boundary-web\\n\u2502   \u2502               cargotracking-ilib-domain-model-command\\n\u2502   \u2502               cargotracking-ilib-domain-model-event\\n\u2502   \u2502               cargotracking-ilib-web\\n\u2502   \u2502               cargotracking-test-component\\n\u2502   \u2502               cargotracking-test-support\\n\u2502   \u2502               cargotracking-test-support-queryside\\n\u2502   \u2502               cargotracking-test-support-testcontainers\\n\u2502   \u2502\\n\u2502   \u251c\u2500\u2500 lib\\n\u2502   \u2502   \u251c\u2500\u2500 hi\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u251c\u2500\u2500 lo\\n\u2502   \u2502   \u2502       ... *\\n\u2502   \u2502   \u2514\u2500\u2500 xlang\\n\u2502   \u2502           ... *\\n\u2502   \u2514\u2500\u2500 other\\n\u2502           ... *\\n\u251c\u2500\u2500 support\\n\u2502       ... *\\n\u2514\u2500\u2500 ... *\\n","tokens":335,"id":4889,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context and Problem Statement\nWe frequently write wrapper scripts. They bring the following value:\n1. They bind together several tools and make them work together as a whole, e.g., `sops` and `kubectl`.\n2. They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files.\n3. They enforce best practices, e.g., encrypt secrets consumed or produced by tools.\nUnfortunately, wrapper scripts can also bring disadvantages:\n1. They make usages that are deviating from the \"good way\" difficult.\n2. They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked.\n3. They add overhead when adding new features or supporting new use-cases.\n4. They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the [Law of Leaky Abstractions](https:\/\/www.joelonsoftware.com\/2002\/11\/11\/the-law-of-leaky-abstractions\/).\n## Decision Drivers\n* We want to make operations simple, predictable, resilient to human error and scalable.\n* We want to have some predictability in how an environment is set up.\n* We want to make Compliant Kubernetes flexible and agile.\n\n## Decision\n* We want to make operations simple, predictable, resilient to human error and scalable.\n* We want to have some predictability in how an environment is set up.\n* We want to make Compliant Kubernetes flexible and agile.\nWe have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value.\nThis decision applies for new wrapper scripts. We will not rework old wrapper scripts.\n### Positive Consequences\n* The operations team can encode standard operating procedures and scale ways of working.\n* Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort.\n* Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability.\n### Negative Consequences\n* There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.\n\n\n## Context\ncoc-markmap \/ markmap-cli has an incredibly large size in `node_modules`. (~30MB)\n### Redundant dependencies\nIn markmap-lib@0.9.x transforming and rendering share the same package, but they have quite different dependencies and are used in quite different scenarios. In other words many of the dependencies are useless most of the time.\nFor example, transforming usually occurs locally when creating a markmap, while rendering happens in the browser when the markmap is opened. So if we are trying to create a markmap, it's likely that we don't need d3 and its friends. If we are exploring a markmap it's likely that we don't need all those transforming tools like remarkable and katex.\n### Extraneous bundles\nSome packages build multiple bundles to support different types of import. For example, `index.js` for `cjs`, and `index.min.js` for minimized `cjs` bundle, and several more. As a result, each of the bundle contains a copy of code, leading to a huge size.\nAfter bundling into a single package, we get a single copy of the code, and optionally minimized.\n\n## Decision\n- Split rendering code from `markmap-lib` into a new package `markmap-view`.\nBy splitting the packages we can manage dependencies more independently, and get rid of unused ones.\nIn most cases, `markmap-view` can be used as a prebuilt bundle, even without installation by leveraging CDNs.\n- Use rollup to bundle code in `markmap-cli`.\nUnder the hood, `coc-markmap` depends on `markmap-cli`, and `markmap-cli` depends on `markmap-lib`.\n`markmap-lib` is supposed to be used in applications with bundlers, so it's better to keep simple and externalize the dependencies.\n`markmap-cli` is a standalone command-line tool. It should be installed without dependency bloat. So it is a good idea to bundle everything in it and get rid of additional dependency packages.\n`coc-markmap` is a thin layer on top of `markmap-cli` so we can just leave it as is.\n\n\n\nProvide a decision given the context below:\n## Context\nExcluding the simplest hello-world-like cases, any useful project typically contains several modules. The traditional way to organize project modules is just to put them under the project root.\nWe can call that structure simply **flat structure**.\nWhile the flat structure is appropriate and sufficient for simpler projects, when the project grows and the number of modules increases, the flat structure starts suffering from many drawbacks:\n* Flat structure does not scale well when the number of modules grows.\n* Flat structure is difficult and confusing to navigate with numerous modules at the same hierarchy level.\n* Flat structure does not suggest a direction of dependencies between modules.\n* Flat structure does not suggest abstraction levels of modules.\n* Flat structure does not suggest where are the system's entry points.\n* Flat structure can use only module names to provide hints about relations between modules. Unfortunately, even that possibility is rarely leveraged.\n* Flat structure does not use any high-level constructs that may suggest how modules are organized and related.\n* Negative usage aspects are getting worse and worse as we add additional modules.\n* Flat structure often requires extracting modules in separate repositories just because confusion becomes unbearable with a larger number of modules.\n* When using microservices, the flat structure practically forces us to use one project per microservice.\n> Note: Terms **flat structure** and **strategic structure** (see below) are ad-hoc terms introduced just for this document. However, in the `klokwrk-project`, we may use them in other places for\n> convenience.\n### Architectural Context\n* System (`klokwrk-project`)\n<end_of_turn>\n<start_of_turn>model\n**We'll organize project modules around strategic DDD (Domain Driven Design) constructs of bounded context and subdomains.**\nOur project organization will follow principles and recommendations of **strategic structure** as defined below.\n### Decision Details\nWe'll start with a concrete example of the strategic structure used in the klokwrk at the time of writing this document. As a follow-up, we'll present a general scheme for creating the strategic\nstructure focusing on the differences to the given concrete example.\n#### Strategic structure in klokwrk\nThe current project layout in the klokwrk looks like this:\nklokwrk-project\n\u251c\u2500\u2500 ... (other files or directories)\n\u251c\u2500\u2500 modules\n\u2502   \u251c\u2500\u2500 bc\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\n\u2502   \u2502       \u251c\u2500\u2500 asd\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 booking\n\u2502   \u2502       \u2502       \u251c\u2500\u2500 app\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-commandside\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-projection-rdbms\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-queryside-view\n\u2502   \u2502       \u2502       \u2502       cargotracking-booking-app-rdbms-management\n\u2502   \u2502       \u2502       \u2514\u2500\u2500 lib\n\u2502   \u2502       \u2502               cargotracking-booking-lib-boundary-web\n\u2502   \u2502       \u2502               cargotracking-booking-lib-out-customer\n\u2502   \u2502       \u2502               cargotracking-booking-lib-queryside-model-rdbms-jpa\n\u2502   \u2502       \u2502               cargotracking-booking-test-component\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-queryside\n\u2502   \u2502       \u2502               cargotracking-booking-test-support-testcontainers\n\u2502   \u2502       \u2502\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\n\u2502   \u2502       \u2502\n\u2502   \u2502       \u2514\u2500\u2500 lib\n\u2502   \u2502               cargotracking-lib-axon-cqrs\n\u2502   \u2502               cargotracking-lib-axon-logging\n\u2502   \u2502               cargotracking-lib-boundary-api\n\u2502   \u2502               cargotracking-lib-boundary-query-api\n\u2502   \u2502               cargotracking-lib-domain-model-command\n\u2502   \u2502               cargotracking-lib-domain-model-event\n\u2502   \u2502               cargotracking-lib-web\n\u2502   \u2502               cargotracking-test-support\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 lib\n\u2502   \u2502   \u251c\u2500\u2500 hi\n\u2502   \u2502   \u2502       klokwrk-lib-hi-datasourceproxy-springboot\n\u2502   \u2502   \u2502       klokwrk-lib-hi-jackson-springboot\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-context\n\u2502   \u2502   \u2502       klokwrk-lib-hi-spring-data-jpa\n\u2502   \u2502   \u2502       klokwrk-lib-hi-validation-springboot\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 lo\n\u2502   \u2502   \u2502       klokwrk-lib-lo-archunit\n\u2502   \u2502   \u2502       klokwrk-lib-lo-datasourceproxy\n\u2502   \u2502   \u2502       klokwrk-lib-lo-hibernate\n\u2502   \u2502   \u2502       klokwrk-lib-lo-jackson\n\u2502   \u2502   \u2502       klokwrk-lib-lo-uom\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-constraint\n\u2502   \u2502   \u2502       klokwrk-lib-lo-validation-validator\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 xlang\n\u2502   \u2502           klokwrk-lib-xlang-groovy-base\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-match\n\u2502   \u2502           klokwrk-lib-xlang-groovy-contracts-simple\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 other\n\u2502       \u251c\u2500\u2500 platform\n\u2502       \u2502       klokwrk-platform-base\n\u2502       \u2502       klokwrk-platform-micronaut\n\u2502       \u2502       klokwrk-platform-spring-boot\n\u2502       \u2502\n\u2502       \u2514\u2500\u2500 tool\n\u2502               klokwrk-tool-gradle-source-repack\n\u251c\u2500\u2500 support\n\u2502   \u2514\u2500\u2500 ... (other files or directories)\n\u2514\u2500\u2500 ... (other files or directories)\nAt the top of the hierarchy, we have a project folder  - `klokwrk-project`. It is the equivalent of the whole system. In the strategic structure, the system name appears in the names of artifacts\nconsidered to be conceptually at the level of a system.\nRight below the root, we have `modules` and `support` folders. These should be the area of 99% of everyday work, with the `modules` folder taking a vast majority of that percentage.\nThe `support` folder houses all kinds of supportive files like scripts, documentation, git hooks, etc. The `support` folder is free-form, and the strategic structure does not impose any\nrecommendations or rules on its content. On the contrary, the strategic structure is applied to the content of the `modules` directory - the home of all source code modules in the system.\nAt the 1st level of strategic structure - the system level, we have the content of the `modules` directory. It is divided into three subdirectories: `bc` (bounded context modules),\n`lib` (system-level libraries), and `other` (miscellaneous helper modules).\nAt the 2nd level - the bounded context level, we have the content of the `modules\/bc` directory that is further organized into three parts, `asd` (asd stands for **A** **S**ub**D**omain),\n`domain-model` (bounded context domain model), and `lib` (bounded context libraries).\nAt the 3rd level of a hierarchy, we have the content of the `modules\/bc\/[bounded-context-name]\/asd` directory that holds all bounded context's subdomains. The modules for each subdomain are further\ndivided into `app` and `lib`. The `modules\/bc\/[bounded-context-name]\/asd\/[subdomain-name]\/app` directory contains the **subdomain applications** responsible for implementing concrete subdomain\nscenarios. From the abstraction level and dependency perspectives, subdomain applications are at the top of the hierarchy. Subdomain applications speak the language of domain - the bounded context's\nubiquitous language. They even contribute to it through the naming and meaning of use cases.\nThe first thing that **subdomain libraries** (`modules\/bc\/[bounded-context-name]\/asd\/subdomain-name\/lib)` can hold is infrastructural code related to the technological choices made for that\nparticular subdomain and are not reusable outside the subdomain. However, they can temporarily have infrastructural modules intended to be more reusable (either on the bounded context or system\nlevels) at the end. Still, for whatever reason, it was more convenient to hold them at the subdomain level for a limited time.\nThe second thing that can be found in subdomain libraries are business-related reusable modules that connect technological choices with the domain model. One characteristic example is the\n`cargotracking-booking-lib-queryside-model-rdbms-jpa` module. Those kinds of modules do speak bounded context's ubiquitous language.\nThe bounded context's **domain model** is implemented in `modules\/bc\/[bounded-context-name]\/domain-model`. Those modules contain the essence of the bounded context business logic. Implementation of\nthe domain model should be free of technology as much as possible and practical. Adding external libraries is not strictly forbidden, but each addition should be conscious and must be carefully\nevaluated. It is best to have tests that monitor and control the dependencies of a domain model. The domain model implements the majority of code-level representation of the bounded context's\nubiquitous language and must be consistent across all bounded context's subdomains.\nBy default, the directory `modules\/bc\/[bounded-context-name]\/lib` is the home of shareable **bounded context infrastructural libraries**. It contains modules with infrastructural code that is\nreusable across the bounded context. Those modules are at a lower abstraction level than subdomain libraries. Bounded context infrastructural libraries do not speak domain language. However, they can\nsupport the implementation of the domain model and other module groups higher in the hierarchy. Domain model should not generally depend on bounded context infrastructural libraries. Exceptions are\nallowed but should be conscious and carefully managed.\nDo note that another variant of bounded context libraries is also possible. It is a variant supporting the sharing of business logic at the bounded context level when necessary. In that case, instead\nof a single `lib` directory, we would have `blib` and `ilib` directories. The `blib` directory would contain business-related modules that can depend on a domain model. On the contrary, the `ilib`\ndirectory cannot use the domain model because it should contain infrastructural code only. The `ilib` directory role is the same as the role of `lib` directory from the default variant of bounded\ncontext libraries.\nLet's return to the `modules\/lib` directory containing general **system-level libraries**. It is divided into `hi`, `lo`, and `xlang` subdirectories. All system-level libraries are at lower\ndependency and abstraction levels than any bounded context module.\nAlthough separation on the high (`hi`) and low-level (`lo`) system libraries is somewhat arbitrary, it is helpful in practice. The `hi` directory is intended to contain\n**high-level system libraries**, which are general infrastructural modules closer to the high-level technological frameworks (something like Spring, Spring Boot, or Axon frameworks) used in the\nsystem. They could contain some specifics of our system, but usually, they do not. In that later case, they are general enough to be reused even outside of our system.\nThe **low-level system libraries** from the `lo` directory deal with the customizations and extensions of widely used 3rd party libraries like Hibernate, Jackson, Java Bean validations, and similar.\nBoth types of system-level libraries should not be, in general, dependencies of a domain model.\nAt the lowest abstraction level, we have the **language extensions** (`modules\/lib\/xlang`). They focus on adding features to the programming language itself or its accompanying SDK (JDK in our case).\nLanguage extensions can be used from everywhere, even from the domain model, without restrictions. Some of them are often written to ease the implementation of the domain model by making it more\nexpressive and concise.\n#### Characteristics of strategic structure\nThe most important thing about strategic structure is not the structure itself but rather the distinguishing characteristics that it provides.\nWe already mentioned abstraction levels and dependencies between groups of modules. If you look again at the example, you will notice that both of them are constantly flowing top to bottom through\nthe strategic structure. For instance, subdomain applications depend on subdomain libraries. They both can depend on the domain model, which can depend on bounded context libraries and language\nextensions. At the level of system libraries, high-level modules can depend on low-level modules, and they both can depend on the language extensions. However, none of the dependencies can come the\nother way around. Dependencies are not allowed to flow from the bottom to the top.\nWe have managed to do this because we applied strategic DDD concepts of bounded context and subdomains to the project structure. They provide sense and meaningfulness by connecting our code to the\nbusiness. Without that business context, we will be left exclusively to the technical aspects, which are just insufficient. Technical aspects know nothing about the purpose of our system. They do not\nknow anything about the business context.\nDescribed characteristics bring important benefits when trying to understand or navigate through the system's code. Finding the desired functionality is much easier because we usually know, at least\napproximately, where we should look for it. This can greatly reduce cognitive load while exploring unfamiliar (or even familiar) codebases.\nIn addition, if you follow the proposed naming conventions for modules and their packages (see below), the same easy orientation can be applied at the package level or even if you pull out all\nmodules into the flat structure. You will always know where to look for.\n#### Naming conventions\nYou have probably noticed that modules have very particular names reflecting their position in the strategic structure. The following table summarizes them as used in the example:\n| Module group    | Naming scheme                                            | Example                                  |\n|-----------------|----------------------------------------------------------|------------------------------------------|\n| subdomain apps  | `[bounded-context-name]-[subdomain-name]-app-[app-name]` | `cargotracking-booking-app-commandside`  |\n| subdomain libs  | `[bounded-context-name]-[subdomain-name]-lib-[lib-name]` | `cargotracking-booking-lib-boundary-web` |\n| domain model    | `[bounded-context-name]-domain-model-[model-part-name]`  | `cargotracking-domain-model-aggregate`   |\n| bc libs         | `[bounded-context-name]-lib-[lib-name]`                  | `cargotracking-lib-boundary-api`         |\n| sys hi libs     | `[system-name]-lib-hi-[lib-name]`                        | `klokwrk-lib-hi-spring-context`          |\n| sys lo libs     | `[system-name]-lib-lo-[lib-name]`                        | `klokwrk-lib-lo-jackson`                 |\n| lang extensions | `[system-name]-lib-xlang-[lib-name]`                     | `klokwrk-lib-xlang-groovy-base`          |\nModule naming conventions are essential because our modules are not always presented (i.e., try the Packages view in the IntelliJ IDEA's Project tool window) or used as a part of the hierarchy (think\nof JAR names put in the same directory). For those reasons, our naming scheme closely follows the strategic structure hierarchy where parts of module names are directly pulled from corresponding\nsubdirectory names. That way, we can keep the match between alphabetical order and the direction of dependencies.\n> Note: When you have multiple bounded contexts and\/or multiple subdomains in the project, to get the exact match between alphabetical order and the direction of dependencies, you can use the `bc-`\n> prefix in front of bounded context names and the `asd-` prefix for subdomain names.\nThe same naming principles should also be applied to packages. Here are a few examples of package names:\norg.klokwrk.cargotracking.booking.app.commandside.*\norg.klokwrk.cargotracking.booking.lib.boundary.web.*\norg.klokwrk.cargotracking.domain.model.aggregate.*\norg.klokwrk.cargotracking.lib.boundary.api.*\norg.klokwrk.lib.hi.spring.context.*\norg.klokwrk.lib.lo.jackson.*\norg.klokwrk.lib.xlang.groovy.base.*\nWith those naming conventions, we should be able to avoid naming collisions on the module and package levels.\n#### The general scheme of strategic structure\nIn some circumstances, we may need additional elements in the strategic structure to deal with shared libraries at different levels. Examples of those, with sparse explanations, are given in the\ngeneral scheme of strategic structure below:\nmodules\n\u251c\u2500\u2500 bc\n\u2502   \u251c\u2500\u2500 my_food\n\u2502   \u2502   \u251c\u2500\u2500 asd\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 restaurant\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\n\u2502   \u2502   \u2502   \u2502           ... *\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 menu_management\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 app\n\u2502   \u2502   \u2502   \u2502   \u2502       ... *\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\n\u2502   \u2502   \u2502   \u2502           ... *\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 zshared         \/\/ sharing code between subdomains if necessary\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 lib\n\u2502   \u2502   \u2502           ... *\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\n\u2502   \u2502   \u2502       ... *\n\u2502   \u2502   \u2514\u2500\u2500 lib                 \/\/ bounded context libraries - default variant\n\u2502   \u2502           ... *           \/\/ Can be split into \"blib\" and \"ilib\" directories when the sharing of\n\u2502   \u2502                           \/\/ business logic is necessary at the level of a single bounded context\n\u2502   \u251c\u2500\u2500 my_carrier\n\u2502   \u2502   \u251c\u2500\u2500 asd\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 app\n\u2502   \u2502   \u2502   \u2502       ... *\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 lib\n\u2502   \u2502   \u2502           ... *\n\u2502   \u2502   \u251c\u2500\u2500 domain-model\n\u2502   \u2502   \u2502       ... *\n\u2502   \u2502   \u2514\u2500\u2500 lib\n\u2502   \u2502           ... *\n\u2502   \u2514\u2500\u2500 zshared                 \/\/ shared code between multiple bounded contexts (if necessary).\n\u2502       \u2502                       \/\/ \"z\" prefix - funny reference to \"zee Germans\" from Snatch movie.\n\u2502       \u2502                       \/\/ Moves \"zshared\" at the last place alphabetically, which matches\n\u2502       \u2502                       \/\/ the proper place in terms of dependencies and abstraction levels.\n\u2502       \u251c\u2500\u2500 domain-model\n\u2502       \u2502       ... *\n\u2502       \u2514\u2500\u2500 lib\n\u2502               ... *\n\u251c\u2500\u2500 lib\n\u2502   \u251c\u2500\u2500 hi\n\u2502   \u2502       ... *\n\u2502   \u251c\u2500\u2500 lo\n\u2502   \u2502       ... *\n\u2502   \u2514\u2500\u2500 xlang\n\u2502           ... *\n\u2514\u2500\u2500 other            \/\/ supportive project's code for various \"other\" purposes\n\u251c\u2500\u2500 build\n\u2502       ... *\n\u251c\u2500\u2500 tool\n\u2502       ... *\n\u2514\u2500\u2500 ...\n#### Simplification - the case of bounded context boundaries matching 1:1 with subdomain\nThe one-to-one match between bounded context boundaries and corresponding subdomain is considered to be the \"ideal\" case, and it is relatively common in practice. When we know how a fully expanded\nstrategic structure works and looks like, it is relatively easy to come up with simplification for this particular case.\nHere are \"refactoring\" steps and the example based on our concrete example from the beginning of this document:\n- move subdomain applications to the bounded context level\n- merge subdomain libraries with bounded context libraries\n- split bounded context libraries into `blib` and `ilib` directories if necessary\n- rename corresponding modules and packages\nklokwrk-project\n\u251c\u2500\u2500 ... (other files or directories)\n\u251c\u2500\u2500 modules\n\u2502   \u251c\u2500\u2500 bc\n\u2502   \u2502   \u2514\u2500\u2500 cargotracking\n\u2502   \u2502       \u251c\u2500\u2500 app\n\u2502   \u2502       \u2502       cargotracking-app-commandside\n\u2502   \u2502       \u2502       cargotracking-app-queryside-projection-rdbms\n\u2502   \u2502       \u2502       cargotracking-app-queryside-view\n\u2502   \u2502       \u2502       cargotracking-app-rdbms-management\n\u2502   \u2502       \u2502\n\u2502   \u2502       \u251c\u2500\u2500 blib\n\u2502   \u2502       \u2502       cargotracking-blib-out-customer\n\u2502   \u2502       \u2502       cargotracking-blib-queryside-model-rdbms-jpa\n\u2502   \u2502       \u2502\n\u2502   \u2502       \u251c\u2500\u2500 domain-model\n\u2502   \u2502       \u2502       cargotracking-domain-model-aggregate\n\u2502   \u2502       \u2502       cargotracking-domain-model-command\n\u2502   \u2502       \u2502       cargotracking-domain-model-event\n\u2502   \u2502       \u2502       cargotracking-domain-model-service\n\u2502   \u2502       \u2502       cargotracking-domain-model-value\n\u2502   \u2502       \u2502\n\u2502   \u2502       \u2514\u2500\u2500 ilib\n\u2502   \u2502               cargotracking-ilib-axon-cqrs\n\u2502   \u2502               cargotracking-ilib-axon-logging\n\u2502   \u2502               cargotracking-ilib-boundary-api\n\u2502   \u2502               cargotracking-ilib-boundary-query-api\n\u2502   \u2502               cargotracking-ilib-boundary-web\n\u2502   \u2502               cargotracking-ilib-domain-model-command\n\u2502   \u2502               cargotracking-ilib-domain-model-event\n\u2502   \u2502               cargotracking-ilib-web\n\u2502   \u2502               cargotracking-test-component\n\u2502   \u2502               cargotracking-test-support\n\u2502   \u2502               cargotracking-test-support-queryside\n\u2502   \u2502               cargotracking-test-support-testcontainers\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 lib\n\u2502   \u2502   \u251c\u2500\u2500 hi\n\u2502   \u2502   \u2502       ... *\n\u2502   \u2502   \u251c\u2500\u2500 lo\n\u2502   \u2502   \u2502       ... *\n\u2502   \u2502   \u2514\u2500\u2500 xlang\n\u2502   \u2502           ... *\n\u2502   \u2514\u2500\u2500 other\n\u2502           ... *\n\u251c\u2500\u2500 support\n\u2502       ... *\n\u2514\u2500\u2500 ... *\n<end_of_turn>\n"}
{"File Name":"james-project\/0041-replace-jcloud-with-s3.md","Context":"## Context\\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\\nserver.\\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\\nObject Storage technologies.\\nObject Storage is known to be cheap and yet offers guarantee about durability.\\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\\nIt also contribute negatively to the project build time.\\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\\nour Swift code without dropping Swift support at the same time.\\n","Decision":"* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\\n","tokens":209,"id":2897,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\n\n## Decision\nUse Rollbar to collect and manage our application errors.\n\n\n## Context\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\n\n## Decision\nUse Rollbar to collect and manage our application errors.\n\n\n\nProvide a decision given the context below:\n## Context\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\nserver.\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\nObject Storage technologies.\nObject Storage is known to be cheap and yet offers guarantee about durability.\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\nIt also contribute negatively to the project build time.\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\nour Swift code without dropping Swift support at the same time.\n<end_of_turn>\n<start_of_turn>model\n* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\n<end_of_turn>\n"}
{"File Name":"tracking-consent-frontend\/0007-use-eslint-for-linting-and-formatting.md","Context":"## Context and Problem Statement\\nWe found that without a standardised format our javascript files ended up with different\\nformats in different files or even multiple formats in the same file.  We also found that\\nour IDEs had different configurations which meant that using an autoformat tool would give\\ndifferent results when each of us do it.\\n## Decision Drivers\\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\n","Decision":"* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\nChosen option: ESLint + Airbnb\\nWe decided to use ESLint with Airbnb because the Airbnb approach is [well documented](https:\/\/github.com\/airbnb\/javascript)\\nand adopting this meant we wouldn't have to make each decision ourselves.  We decided not to use Prettier as well, there\\nwas a discussion about the possibility of conflicting format changes between the two tools - we encountered one of these,\\nit was overcome with config updates.\\nESLint is compatible with both Javascript and Typescript projects which made it seem particularly suitable for us as we\\nchose to use Typescript for Tracking Consent but we usually use Javascript.\\nEach of the current team members use JetBrains IDEs and that has a built-in plugin which supports auto formatting on save,\\nthis is also a common feature which most IDEs have.\\nWe decided to add pre-commit and pre-push hooks which ensure that the style rules have been met but do not mutate code.\\nWe discussed the option of formatting the code on commit but we felt that this introduced an uncertainty about what we\\nwere committing.\\n### Positive Consequences\\n* Code style within a file will be consistent\\n* Files within a project will be consistent with each other\\n* When widely applied this Javascript\/Typescript files between projects will be consistent\\n* IDE will reformat for us which takes away the need for us to do it ourselves\\n* Code reviews will focus less on code format and more on the details of the changes made\\n* Builds will fail if format isn't followed which guarentees consistency\\n### Negative Consequences\\n* Builds will fail if format isn't followed which can disrupt focus\/workflow (mitigated by pre-commit hooks)\\n","tokens":133,"id":3521,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nHistorically we have had \"two ways\" of configuring Verify Service Provider:\n- Using environment variables\n- Using a YAML file\nWhen using environment variables the application used the verify-service-provider-env.yml\nfile from the resources directory (so inside the jar). When using the YAML file you would\npass the path to a different file as a command line parameter - usually people\nwould use the example one that's contained in the repo.\nThere were a couple of reasons for the extra complexity of managing two files, both due to\nrestrictions with the java buildpack used by cloudfoundry:\n- It's not possible to specify command line arguments through the java buildpack,\nso you can't specify a path to your config file\n- We weren't confident in the way cloudfoundry manages static files, so we didn't want\nto rely on one.\nThere was also a philosophical point that 12 factor applications should be configured through\ntheir environment. This made the \"hide the configuration in the .jar and do everything through\nenv vars\" way appealing.\n\n## Decision\nWe will remove the verify-service-provider-env.yml file from src\/main\/resources\nThe application will default to the verify-service-provider.yml\nfile that's included in the .zip if no command line arguments are provided.\nIf the application is started without command line arguments specifying a yml file\nAND no environment variables have been set, startup should error gracefully and tell\nthe user that the configuration fields have not been specified for example:\n\"ERROR - no configuration fields found, either set environment variables or specify\na configuration file using command line arguments ```server <path\/to\/verify-service-provider.yml>```\"\nWe will establish the path to verify-service-provider.yml by asking java for the\npath to the .jar file containing the Application class and looking in the parent\nfolder.\n\n\n## Context\nWe maintain a lot of [terraform modules] which teams use to manage AWS\nresources for their namespaces.\nIn the past, lettings different namespaces use different versions of terraform\nmodules has caused problems because there has not always been a clear upgrade\npath from a module that is several versions old, to the latest version.\nIn these cases, it would have been easier to have upgraded every namespace when\nthe significant change was made to the module, because at that point in time\nthe relevant knowledge and understanding are fresh. Leaving it until later\nmakes the whole process much more difficult.\n\n## Decision\nWe decided to ensure that all namespaces are always using the latest version of\nevery module.\n\n\n\nProvide a decision given the context below:\n## Context and Problem Statement\nWe found that without a standardised format our javascript files ended up with different\nformats in different files or even multiple formats in the same file.  We also found that\nour IDEs had different configurations which meant that using an autoformat tool would give\ndifferent results when each of us do it.\n## Decision Drivers\n* We wanted to spend less time doing manual formatting\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\n* We wanted to see easily which lines had actually changed when reviewing PRs\n* We wanted to avoid discussions about individual's preferences for particular\n<end_of_turn>\n<start_of_turn>model\n* We wanted to spend less time doing manual formatting\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\n* We wanted to see easily which lines had actually changed when reviewing PRs\n* We wanted to avoid discussions about individual's preferences for particular\nChosen option: ESLint + Airbnb\nWe decided to use ESLint with Airbnb because the Airbnb approach is [well documented](https:\/\/github.com\/airbnb\/javascript)\nand adopting this meant we wouldn't have to make each decision ourselves.  We decided not to use Prettier as well, there\nwas a discussion about the possibility of conflicting format changes between the two tools - we encountered one of these,\nit was overcome with config updates.\nESLint is compatible with both Javascript and Typescript projects which made it seem particularly suitable for us as we\nchose to use Typescript for Tracking Consent but we usually use Javascript.\nEach of the current team members use JetBrains IDEs and that has a built-in plugin which supports auto formatting on save,\nthis is also a common feature which most IDEs have.\nWe decided to add pre-commit and pre-push hooks which ensure that the style rules have been met but do not mutate code.\nWe discussed the option of formatting the code on commit but we felt that this introduced an uncertainty about what we\nwere committing.\n### Positive Consequences\n* Code style within a file will be consistent\n* Files within a project will be consistent with each other\n* When widely applied this Javascript\/Typescript files between projects will be consistent\n* IDE will reformat for us which takes away the need for us to do it ourselves\n* Code reviews will focus less on code format and more on the details of the changes made\n* Builds will fail if format isn't followed which guarentees consistency\n### Negative Consequences\n* Builds will fail if format isn't followed which can disrupt focus\/workflow (mitigated by pre-commit hooks)\n<end_of_turn>\n"}
{"File Name":"Data-Platform-Playbook\/002-ingest-google-sheets-data.md","Context":"## Context\\nHackney currently have datasets distributed over a number of different Google Sheets Documents due their data recovery\\nefforts. We need to get this information pulled into the data platform for processing.\\n","Decision":"We will use a python based AWS Glue Job in conjunction with the gspread python library to pull the data onto platform\\n","tokens":41,"id":1016,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nAs [per this Trello card][1], we want to populate the Content Items' dimension with the latest changes that result of editing content.\n\n## Decision\nAddressing the ETL process for Content Items this way:\n1. The first time, we load all the content items via [publishing-api][3]. We retrieve all the content-ids and paths of all the content items that are live.\n2. For each Content Item we will get the JSON from the [Content Store][2], and we will extract the attributes that we need to fulfil the immediate needs. We will also persist all the JSON to be able to extract other attributes in the future.\n3. On a daily basis, we will be listening to [publishing-api][3] events, in the same way that [Rummager][4] or [Email Alert Service][5] do. Once we receive a change for the content item, we will automatically update the content items dimension with the new approach.\n### Benefits:\n1. This is more aligned with GOV.UK architecture.\n1. This is very light and efficient. It also embrace simple code as the ETL process for Content Items is almost trivial.\n\n\n## Context\nWe need to persist the user data to the backend. Rather than building our\nfrontend to be tightly coupled to an API, we want a layer that can sit between\nthe application and the API.\nWe know that we will need to support offline use of these forms. We also want\nusers to be able to recover unsubmitted forms if they haven't gone online\nbetween sessions.\nWe also know that some forms will require the upload of images.\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\ndatabase that persists data across sessions. It supports storing blob data (such\nas images), and has decent storage limits.\n\n## Decision\nWe will support IndexedDB as a data store layer, sitting between the application\nand the API.\n\n\n\nProvide a decision given the context below:\n## Context\nHackney currently have datasets distributed over a number of different Google Sheets Documents due their data recovery\nefforts. We need to get this information pulled into the data platform for processing.\n<end_of_turn>\n<start_of_turn>model\nWe will use a python based AWS Glue Job in conjunction with the gspread python library to pull the data onto platform\n<end_of_turn>\n"}
{"File Name":"runner\/0274-step-outcome-and-conclusion.md","Context":"## Context\\nThis ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.\\nThis allows downstream a step to run based on whether a previous step succeeded or failed.\\nReminder, currently the steps contains `steps.<id>.outputs`.\\n","Decision":"For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:\\n- `success`\\n- `failure`\\n- `cancelled`\\n- `skipped`\\nWhen a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`.\\n### Example\\n```yaml\\nsteps:\\n- id: experimental\\ncontinue-on-error: true\\nrun: .\/build.sh experimental\\n- if: ${{ steps.experimental.outcome == 'success' }}\\nrun: .\/publish.sh experimental\\n```\\n### Terminology\\nThe runs API uses the term `conclusion`.\\nTherefore we use a different term `outcome` for the value prior to continue-on-error.\\nThe following is a snippet from the runs API response payload:\\n```json\\n\"steps\": [\\n{\\n\"name\": \"Set up job\",\\n\"status\": \"completed\",\\n\"conclusion\": \"success\",\\n\"number\": 1,\\n\"started_at\": \"2020-01-09T11:06:16.000-05:00\",\\n\"completed_at\": \"2020-01-09T11:06:18.000-05:00\"\\n},\\n```\\n","tokens":63,"id":3738,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n# 5 Utility Context Lazily Initialized\n## Context\nThe [synchronous utils](..\/..\/src\/adr_util_sync.js) are intended to be used by commands to work on ADRs, with the given context of the project, e.g. the [adr marker file](1-ADR_directory_identified_by_marker_file.md).\nSince the context is mandatory (the ADR directory, existing ADRs), it is initialized at the beginning, in the object's constructor.\nSeveral commands (scripts) already include that script, and initialize the object on the script loading, as part of the `require` statement.\nThis resulted in loading the context whenever a command is loaded.\nThis might have performance implications, but the more immediate functional lacuna is that this then assumes that the context *can* be loaded every time.\nOf course, this assumption breaks - when running the `init` command, where by definition there is no context to load - the `init` command creates that.\nThis resulted in trying to run the `init` command, on an existing directory, in an error.\n\n## Decision\nThe ADR project context will be loaded lazily, only when needed.\nThis assumes the `init` command doesn't require it, and hence will not invoke any commands that require that context (e.g. the existing files).\nSo any command that is loaded can still create the context object and create an instance of, but the actual context will be loaded easily.\nThis assumption seems safe enough since `init` command indeed creates the context (the `.adr` file).\nAnd this will probably hold for similar commands.\nCommands that will require the ADR context, will continue to use it as today, and it will be loaded lazily.\nWe need of course to make sure that the context isn't created\/calculated every time it is called; e.g. searching for the ADR dir every time one of the utility functions is called. In other words - memoize the context-creating functions.\n\n\n## Context and Problem Statement\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`\/`PUT` HTTP methods),\nRead(`GET` HTTP method), Update(`PUT`\/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\nworkflow in a state machine) or command execution.\n## Decision Drivers\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\ntwo different state machines.\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\n\n## Decision\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\ntwo different state machines.\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\nThe \"Taking advantage of the `Controller` REST archetype\" should be considered as a recommended solution. All Sylius\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\nor new schema should be introduced for the JSON-LD structure.\nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing is\ndelivered with vanilla Sylius installation.\n\n\n\nProvide a decision given the context below:\n## Context\nThis ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.\nThis allows downstream a step to run based on whether a previous step succeeded or failed.\nReminder, currently the steps contains `steps.<id>.outputs`.\n<end_of_turn>\n<start_of_turn>model\nFor steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:\n- `success`\n- `failure`\n- `cancelled`\n- `skipped`\nWhen a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`.\n### Example\n```yaml\nsteps:\n- id: experimental\ncontinue-on-error: true\nrun: .\/build.sh experimental\n- if: ${{ steps.experimental.outcome == 'success' }}\nrun: .\/publish.sh experimental\n```\n### Terminology\nThe runs API uses the term `conclusion`.\nTherefore we use a different term `outcome` for the value prior to continue-on-error.\nThe following is a snippet from the runs API response payload:\n```json\n\"steps\": [\n{\n\"name\": \"Set up job\",\n\"status\": \"completed\",\n\"conclusion\": \"success\",\n\"number\": 1,\n\"started_at\": \"2020-01-09T11:06:16.000-05:00\",\n\"completed_at\": \"2020-01-09T11:06:18.000-05:00\"\n},\n```\n<end_of_turn>\n"}
{"File Name":"editions\/07-\u2705-ophan-analytics.md","Context":"## Context\\nWe need a way to send analytics data to Ophan (internal analytics analysis tool). This includes at its MVP level, screen views and events. This needs to work across iOS and Android.\\n","Decision":"```sequence\\nOphan Analytics Library->Multiplatform Ophan: Thrift interface\\nMultiplatform Ophan->Native React Native Layer: Native iOS and Android modules\\nNative React Native Layer->Typescript Layer: Type definitions to mimic modules\\nTypescript Layer->App Code: Consume! \ud83c\udf54\\nApp Code-->Ophan Analytics Library: Sends events\\n```\\n### Key\\n**Ophan Analytics Library** - This is the current library that provides an interface to log events in Ophan.\\n**Multplatform Ophran** - Written in Kotlin, this uses the Ophan Analytics Library via its Thrift interface and exposes a subset of methods required by the apps.\\n**Native React Native Layer** - Takes the module created by Multiplatform Ophan and makes it \"React Nativey\" to expose methods to the JS.\\n**Typescript Layer** - Takes the React Native JS methods exposed and provides type definitions for them. This is what the App will use and it means we can capture errors at build time.\\n**App Code** - Consumes the typescript library where its needed.\\n","tokens":42,"id":669,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe need to persist the user data to the backend. Rather than building our\nfrontend to be tightly coupled to an API, we want a layer that can sit between\nthe application and the API.\nWe know that we will need to support offline use of these forms. We also want\nusers to be able to recover unsubmitted forms if they haven't gone online\nbetween sessions.\nWe also know that some forms will require the upload of images.\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\ndatabase that persists data across sessions. It supports storing blob data (such\nas images), and has decent storage limits.\n\n## Decision\nWe will support IndexedDB as a data store layer, sitting between the application\nand the API.\n\n\n## Context\nWe need to allow a number of users to sign in to the service in order to use it.\nIn order to implement this quickly, we'll use Auth0 to manage this.\nAs Auth0's authentication uses OAuth2, it should be straightforward to migrate\nto another service, if BEIS have a preference for something else.\nAuth0 provides views for authentication that we can use in our user journeys.\nThere are two versions of these views 'Classic' and 'New'.\nClassic uses JavaScript and is not progressively enhanced. New uses language\nthat results in a poor user experience.\n\n## Decision\nWe will use the free tier and 'Classic' views of Auth0 for the private beta.\n\n\n\nProvide a decision given the context below:\n## Context\nWe need a way to send analytics data to Ophan (internal analytics analysis tool). This includes at its MVP level, screen views and events. This needs to work across iOS and Android.\n<end_of_turn>\n<start_of_turn>model\n```sequence\nOphan Analytics Library->Multiplatform Ophan: Thrift interface\nMultiplatform Ophan->Native React Native Layer: Native iOS and Android modules\nNative React Native Layer->Typescript Layer: Type definitions to mimic modules\nTypescript Layer->App Code: Consume! \ud83c\udf54\nApp Code-->Ophan Analytics Library: Sends events\n```\n### Key\n**Ophan Analytics Library** - This is the current library that provides an interface to log events in Ophan.\n**Multplatform Ophran** - Written in Kotlin, this uses the Ophan Analytics Library via its Thrift interface and exposes a subset of methods required by the apps.\n**Native React Native Layer** - Takes the module created by Multiplatform Ophan and makes it \"React Nativey\" to expose methods to the JS.\n**Typescript Layer** - Takes the React Native JS methods exposed and provides type definitions for them. This is what the App will use and it means we can capture errors at build time.\n**App Code** - Consumes the typescript library where its needed.\n<end_of_turn>\n"}
{"File Name":"libelektra\/internal_cache.md","Context":"## Problem\\nWhen doing kdbGet() possible more keys are returned which might be\\nconfusing. When doing a second kdbGet() with a new keyset\\nno keys might be returned, because it is up-to-date.\\nWhen doing kdbSet() a deep duplication is needed.\\nIdea: keep a duplicated keyset internally. Return (non-deep?)\\nduplications exactly of the parts as requested.\\n","Decision":"Not yet decided.\\n- [Global Validation](global_validation.md)\\n","tokens":90,"id":1289,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n**Context**: operations that change state in the server may complete with an\nindeterminate status code. For example: an error in the 500 range may be\nproduced by a middle-tier server after the operation was started by the backend.\nRe-issuing the request may not work as expected for the application developer.\n**Decision**: by default the library will retry all requests, including\nnon-idempotent ones. The library will allow application developers to override\nthe definition of what operations are automatically retried.\n**Consequences**: most operations become easier to use for application\ndevelopers. In very rare cases the operation will result in double uploads, or\nin a new generation of the object or metadata being created. In even more rare\ncases the operation may fail, for example, an operation to create an object with\n`IfGenerationMatch(0)` would fail on the second attempt.\n\n## Decision\nnon-idempotent ones. The library will allow application developers to override\nthe definition of what operations are automatically retried.\n**Consequences**: most operations become easier to use for application\ndevelopers. In very rare cases the operation will result in double uploads, or\nin a new generation of the object or metadata being created. In even more rare\ncases the operation may fail, for example, an operation to create an object with\n`IfGenerationMatch(0)` would fail on the second attempt.\n\n\n## Context\nThe raw HES data is large, complex, unweildly and potentially dirty. A way to\nclean, store and later easily access the data for analysis required, which means\nthat repeated cleaning and prep of the raw data is avoided.\nThere are multiple datasets (with different variables), which once cleaned, will\nbe accompanied by additional derived variables, and additional derived datasets.\n\n## Decision\nThe raw data will be read into a SQLite database through R. R is The Health\nFoundation's open-source language of choice, and is well equiped to\nhandle later analysis of said data. An SQLite database is not stored in memory,\navoiding holding very large datasets an R session. It is a standard database\nformat which can be easily queried with SQL which can in turn be written within\nmost other common languagses, including R.\nIn an SQLite database the database consistents of all the data, and within that\nthere are tables; here each table corresponds to a HES dataset or a derived\ndataset.\n\n\n\nProvide a decision given the context below:\n## Problem\nWhen doing kdbGet() possible more keys are returned which might be\nconfusing. When doing a second kdbGet() with a new keyset\nno keys might be returned, because it is up-to-date.\nWhen doing kdbSet() a deep duplication is needed.\nIdea: keep a duplicated keyset internally. Return (non-deep?)\nduplications exactly of the parts as requested.\n<end_of_turn>\n<start_of_turn>model\nNot yet decided.\n- [Global Validation](global_validation.md)\n<end_of_turn>\n"}
{"File Name":"git-en-boite\/0009-prefer-dugite-over-raw-git-commands-in-tests.md","Context":"## Context\\nThe build failed because the version of git used in CircleCI was not what we expected.\\n","Decision":"Use `GitProcess.exec` even in tests that need to lean on the git process.\\n","tokens":22,"id":3504,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe were unable to address a security vulnerability in jQuery because we couldn't update Bootstrap. This was due to the\nfact that it was present as both a gem and an npm package.\n\n## Decision\nWe removed asset pipeline completely and moved all css and image assets to webpacker. This allowed us to update jQuery\nvia yarn.\n\n\n## Context\nDRTv1 tried to normalise all ports into a single model. We frequently had requests for port specific features\nor structures. Also it\n\n## Decision\nUse a jvm per airport\n\n\n\nProvide a decision given the context below:\n## Context\nThe build failed because the version of git used in CircleCI was not what we expected.\n<end_of_turn>\n<start_of_turn>model\nUse `GitProcess.exec` even in tests that need to lean on the git process.\n<end_of_turn>\n"}
{"File Name":"Conduit\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4408,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n\n\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n\n\n\nProvide a decision given the context below:\n## Context\nWe need to record the architectural decisions made on this project.\n<end_of_turn>\n<start_of_turn>model\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\n<end_of_turn>\n"}
{"File Name":"beis-report-official-development-assistance\/0006-use-auth0-for-authentication.md","Context":"## Context\\nWe need to allow a number of users to sign in to the service in order to use it.\\nIn order to implement this quickly, we'll use Auth0 to manage this.\\nAs Auth0's authentication uses OAuth2, it should be straightforward to migrate\\nto another service, if BEIS have a preference for something else.\\nAuth0 provides views for authentication that we can use in our user journeys.\\nThere are two versions of these views 'Classic' and 'New'.\\nClassic uses JavaScript and is not progressively enhanced. New uses language\\nthat results in a poor user experience.\\n","Decision":"We will use the free tier and 'Classic' views of Auth0 for the private beta.\\n","tokens":127,"id":2405,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nOur user facing applications will need to authenticate users.\nIn the long-term, we want to use the standard user sign-on services in CCS for\neach appropriate user group.\nFor suppliers, this is the [Supplier Registration Service (SRS)][service-srs].\nFor CCS staff it is their G-Suite account.\nUsing a single authentication service will reduce administration overheads\nbecause we won't have to manage a JML (Joiners Movers Leavers) process. For\nusers it will also mean they don't have to remember yet another username and\npassword combination.\nHowever, we want to get moving quickly, and integration with SRS\/CCS G-Suite\nwill take time. For now, we want a simple sign-in service that we can integrate\nwith quickly, to enable us to test and iterate the application.\n\n## Decision\nWe will use [Auth0][service-auth0] for authentication on a temporary basis.\nAuth0 is an authentication service that uses the OAuth protocol. It provides\nand simple integration path, and is free for several thousand active users.\nWe acknowledge that we are incurring technical debt by not integrating\nimmediately with SRS and G-Suite, however we believe that this will let us move\nmore quickly during development.\nThis decision will also allow us to gain a better understanding of the user\nneeds around authentication before a decision around the final authentication\nsolution is made.\nWe will replace Auth0 with the proper authentication service later in the\ndevelopment process.\n\n\n## Context\nWe need to persist the user data to the backend. Rather than building our\nfrontend to be tightly coupled to an API, we want a layer that can sit between\nthe application and the API.\nWe know that we will need to support offline use of these forms. We also want\nusers to be able to recover unsubmitted forms if they haven't gone online\nbetween sessions.\nWe also know that some forms will require the upload of images.\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\ndatabase that persists data across sessions. It supports storing blob data (such\nas images), and has decent storage limits.\n\n## Decision\nWe will support IndexedDB as a data store layer, sitting between the application\nand the API.\n\n\n\nProvide a decision given the context below:\n## Context\nWe need to allow a number of users to sign in to the service in order to use it.\nIn order to implement this quickly, we'll use Auth0 to manage this.\nAs Auth0's authentication uses OAuth2, it should be straightforward to migrate\nto another service, if BEIS have a preference for something else.\nAuth0 provides views for authentication that we can use in our user journeys.\nThere are two versions of these views 'Classic' and 'New'.\nClassic uses JavaScript and is not progressively enhanced. New uses language\nthat results in a poor user experience.\n<end_of_turn>\n<start_of_turn>model\nWe will use the free tier and 'Classic' views of Auth0 for the private beta.\n<end_of_turn>\n"}
{"File Name":"report-a-defect\/0011-use-auth0-for-authentication.md","Context":"## Context\\nAt the moment and for the short term future Hackney do not have a clear candidate for a single sign-on provider. They are reviewing the situation and will decide on a future solution before the end of 2019.\\nWe have so far been using Basic Authentication.\\nAs with other decisions we have attempted to follow the path set by the Repairs Hub which is another Hackney Rails app that is more mature. We asked what they were using and the answer was Microsoft Azure Active Directory using OpenID Connect (OAuth2). We believed we could get the exact same provisioning when the time came for Report a Defect to integrate with a single sign-on provider, however when the time came we learnt that it wasn't supported by Hackney and should be thought of as experimental.\\nAs we had 2 weeks left we discussed with Hackney Architect Keith Gatt the possibility of using Auth0 as a temporary option to enable us to meet the user need of providing agent names within the service, as well as security needs through the use of a logged authentication service that provided one set of credentials per person, rather than a single set for basic auth.\\n","Decision":"Use Auth0 as the single sign-on provider, and remove basic auth from production.\\n","tokens":234,"id":5211,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL\/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted\/PaaS.\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor\/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny\/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\n\n## Decision\nUse AWS RDS Aurora MySql implemntation for our database persistence mechansim.  Create via Riglet CloudFormation stacks.  Production will run with 1 primary and 1 replica for HA.  Staging & Integration will run with only 1 Primary to keep costs down.\n\n\n## Context\nAs the code rapidly grows we need a common structure and approach that will keep the code tidy and coherent.\nThere are various aspects, the initial concern was the data access separation. The issue #140 initially talked about how this could be separated in terms of an ORM (Object-Relational Mapping) or a DAL (Data Access Layer).\nHowever, this discussion also soon covered the structure of the code in general and how to structure it for ease of development and best practise.\n### Rejected Options\nWe considered the following options:\n* The Pubsweet Way - This was to define a base class \"Model\" and extend this for each entity. This was rejected as its another self made ORM but and has validations that we don't want.\n* Other ORMs : Waterline, sequelize, bookshelf - General purpose ORMs were rejected based on the experience that the time spent going up the learning curve and sifting through the docs when things go wrong is greater then simply rolling your own.\n* Query Builders : Knex.js -These were not entirely ruled out, and were deemed compatible with a good code structure that would allow the database access to be encapsulated in a way that would allow a query builder to be used where necessary.\n\n## Decision\nIn summary the server-side code will be structured below the `server` folder as follows:\n* **entities** - This folder will contain subfolders relating to named entities in the system. See [\"Data Model Specification\"](https:\/\/docs.google.com\/document\/d\/1KU-DLMNhPxjQF2j8HVlJvenvttPLxgtbTHo8Sy_PNRc\/).\n* **entities\/\\<entity\\>** - The example below shows `Mansuscript` as an example entity, this folder will contain a common set of files that describe the entity's behaviour. Each of these have a particular purpose explained below:\n* index.js - The main business logic, e.g. for `Manuscript` this could contain `getAuthor()`\n* typedefs.js - Contains the GraphQL types pertinent to this entity.\n* resolvers.js - Contains the GraphQL interface specified in terms of `Query` and `Mutation` 's for this entity. These in general should map onto the exposed functions from index.js. For example, `Manuscript` may contain a `Query` for `allManuscripts` and a `Mutation` for `createManuscript`.\n* data-access.js - Contains the functions for saving to and loading from the database. All references to the database and SQL that is written should remain private to this file.\n* **manuscript-state-machine** - It is anticipated that this is where the most orchestration of the entites will take place. This has been created as a separate folder so that it can expand to implement the state transitions currently defined for the manuscript.\n### Example\n```\nserver\n|\n\u251c\u2500\u2500 entities\n|   |\n\u2502   \u251c\u2500\u2500 manuscript\n\u2502   \u2502   \u251c\u2500\u2500 typedefs.js\n\u2502   \u2502   \u251c\u2500\u2500 resolvers.js\n\u2502   \u2502   \u251c\u2500\u2500 data-access.js\n\u2502   \u2502   \u2514\u2500\u2500 index.js\n|   |\n\u2502   \u2514\u2500\u2500 <entity>\n\u2502       \u251c\u2500\u2500 typedefs.js\n\u2502       \u251c\u2500\u2500 resolvers.js\n\u2502       \u251c\u2500\u2500 data-access.js\n\u2502       \u2514\u2500\u2500 index.js\n|\n\u2514\u2500\u2500 manuscript-state-machine\n\u2514\u2500\u2500 index.js\n```\n\n\n\nProvide a decision given the context below:\n## Context\nAt the moment and for the short term future Hackney do not have a clear candidate for a single sign-on provider. They are reviewing the situation and will decide on a future solution before the end of 2019.\nWe have so far been using Basic Authentication.\nAs with other decisions we have attempted to follow the path set by the Repairs Hub which is another Hackney Rails app that is more mature. We asked what they were using and the answer was Microsoft Azure Active Directory using OpenID Connect (OAuth2). We believed we could get the exact same provisioning when the time came for Report a Defect to integrate with a single sign-on provider, however when the time came we learnt that it wasn't supported by Hackney and should be thought of as experimental.\nAs we had 2 weeks left we discussed with Hackney Architect Keith Gatt the possibility of using Auth0 as a temporary option to enable us to meet the user need of providing agent names within the service, as well as security needs through the use of a logged authentication service that provided one set of credentials per person, rather than a single set for basic auth.\n<end_of_turn>\n<start_of_turn>model\nUse Auth0 as the single sign-on provider, and remove basic auth from production.\n<end_of_turn>\n"}
{"File Name":"publicservice-registry\/0010-do-not-use-clr-type-names-for-event-types.md","Context":"## Context\\nLooking at the SqlStreamStore code, we noticed a warning against using the CLR type name as the event type in your event store.\\nThe reason behind this is that your message types will outlive your .net CLR types. Moving events along namespaces will break stuff.\\n","Decision":"Use a dictionary\/map between your message types and the CLR type you want to deserialize your message into.\\n","tokens":57,"id":3129,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nPreviously, we did not have any rules for governing Automate UI route\nlifecycle. In the past, we likely broke backwards compatibility when\nre-naming or moving routes.\nAdditionally, the top nav highlighting in Automate UI was driven off the\nroot route (e.g. everything under the `Settings` tab must live under the `\/settings`\nroute or it will not be highlighted properly for pages that live under it).\nThis means that if you want to move a page to a new tab, it will require a new route.\n\n## Decision\nEvery route that originally linked to a page that is still in use in the webapp\nmust still work and backwards compatibility must be maintained. Only when the webpage\nthat the original route linked to is being removed from the app can a route be removed.\nThis is so that deep linking from our own sources (blog posts, etc.) as well as links\ncustomers use internally will continue to work until they no longer make sense at all\nand the original resource no longer exists.\nTo maintain backwards compatibility, the original route must redirect to the new route\n(achievable in `app-routing.module.ts`) and an e2e test must be added to verify\nthat redirection in `deprecated-routes.e2e-spec.ts`. See\n[this commit](https:\/\/github.com\/chef\/automate\/commit\/38779c5aabb1af8d6c22e4b6e1b07eaf9c8fa06c)\nfor an example of routes being renamed with proper backwards compatibility, what the e2e\ntests should look like, and how a page can move between topnav tabs.\n\n\n## Context\nThis set of Docker images are opinionated and meant to run within a Docker orchestrator, kubernetes for instance.\nSince most (if not all) the orchestrators have resource management built-in there are certain PHP settings which can be tweaked to make use of them, if PHP has memory limits itself, it'll die as a fatal error, in which case the orchestrator would be unaware that it's actually a out of memory situation.\n\n## Decision\nSet php ini configuration to have `memory_limit = -1`, this will affect both fpm and cli processes since it's added in the `default.ini` file of this repository.\n\n\n\nProvide a decision given the context below:\n## Context\nLooking at the SqlStreamStore code, we noticed a warning against using the CLR type name as the event type in your event store.\nThe reason behind this is that your message types will outlive your .net CLR types. Moving events along namespaces will break stuff.\n<end_of_turn>\n<start_of_turn>model\nUse a dictionary\/map between your message types and the CLR type you want to deserialize your message into.\n<end_of_turn>\n"}
{"File Name":"react-library-template\/0006-use-jest.md","Context":"## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\\napps.\\n","Decision":"We will use Jest as our testing framework.\\n","tokens":40,"id":4223,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe want a test framework that has good support for React and TypeScript.\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\napps.\n\n## Decision\nWe will use Jest as our testing framework.\n\n\n## Context\nWe want to be able to distribute this library to me ingested by TypeScript or\nplain JavaScript (both commonJS and module) applications.\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\nsupport for TypeScript and simple configuration.\n\n## Decision\nWe will build distributables using Rollup.js.\n\n\n\nProvide a decision given the context below:\n## Context\nWe want a test framework that has good support for React and TypeScript.\n[Jest](https:\/\/jestjs.io) is the standard, recommended test framework for React\napps.\n<end_of_turn>\n<start_of_turn>model\nWe will use Jest as our testing framework.\n<end_of_turn>\n"}
{"File Name":"paas-team-manual\/ADR016-end-to-end-encryption.md","Context":"Context\\n=======\\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\\nThere are 3 main network sections between the user and the application:\\n* User to ELB\\n* ELB to router\\n* Router to cells\\nDecision\\n========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","Decision":"========\\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nThe traffic is encrypted end-to-end between the user and the applications.\\n","tokens":247,"id":216,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nThis application needs user sessions to allow users (candidates, providers, referees, support staff) to sign in. Rails offers a number of options for this. Each have a trade-off.\n### Cookie sessions\nAll session information is saved in a cookie. The cookie is encrypted to avoid the user changing or reading the data.\nPro:\n- It does not interact with other infrastructure\n- It's the Rails default, well understood by developers\nCon:\n- When the user signs out, we do not invalidate the session. This means that if the user has made a copy of the cookie, they (or an attacker) can sign themselves back in.\n- Users cannot sign out sessions on other devices (remote sign out)\n### Storage based cookies\nThis mechanism relies on a session ID being saved in a cookie. The session ID corresponds to a record either in a traditional database (PostgreSQL in our case) or in a caching service (Memcached, Redis).\nPro:\n- On sign out, the session is deleted and cannot be revived\n- Sessions can be invalidated \"remotely\", to allow sign out of other devices\nCon:\n- Uses other infrastructure - slight performance overhead, risk of services being unavailable\n- Sensitive data is stored in a database\n\n## Decision\nUse session cookies.\n\n\n## Context\nOur service-operator allows service teams to provision various AWS services by\ndeclaratively defining resources and submitting them via the kubernetes api.\nSome of these resources require IAM to authorise how the provisioned service\ncan be used. The types of actions that can be performed on.\n#### Example\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\n```\n---\napiVersion: storage.govsvc.uk\/v1beta1\nkind: S3Bucket\nmetadata:\nname: s3-bucket-sample\nspec:\naws:\nLifecycleRules:\n- Expiration: 90days\nVersioning:\nEnabled: true\n```\nIn order to access a provisioned bucket via the the AWS SDK users will require\nan IAM role\/policy that allows access.\nWe want things like bucket ACL, versioning configuration and lifecycle policy\nto be defined declaratively via the resource manifest (see example above), and continuously managed\nby the service operator.\nWe want users of the provisioned bucket to be able to read back all\nconfiguration, and be able to fully utilise the specific bucket for reading,\nwriting and managing their objects within the provisioned bucket, but we want\nto avoid giving permissions to users that could cause conflicts with the\nproperties that are managed by the service operator's reconcile loop.\nFor example, given the example manifest above, we would like to avoid giving\npermissions that would allow a user to alter the Expiration LifeCycleRules,\nsince any changes the user made would be periodically overwritten by the\nservice operator's reconciliation.\n\n## Decision\n* We will provision policy that gives full access for users to _use_ the\nprovisioned service.\n* We will avoid provisioning policy that allows users to create, destroy or\nconfigure the provisioned service, so that this can remain the declarative\ndomain of the service-operator.\n\n\n\nProvide a decision given the context below:\nContext\n=======\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\nThere are 3 main network sections between the user and the application:\n* User to ELB\n* ELB to router\n* Router to cells\nDecision\n========\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\nStatus\n======\nAccepted\nConsequences\n============\nThe traffic is encrypted end-to-end between the user and the applications.\n<end_of_turn>\n<start_of_turn>model\n========\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\nStatus\n======\nAccepted\nConsequences\n============\nThe traffic is encrypted end-to-end between the user and the applications.\n<end_of_turn>\n"}
{"File Name":"datalab\/0011-cluster-authentication-by-ssh-key.md","Context":"## Context\\nWe need to configure access to the servers in the cluster and need to decide between\\npassword authentication and SSH key pair authentication.\\n","Decision":"We have decided that all server log on authentication will use SSH key pair\\nauthentication. The public keys will be provisioned onto the server at server creation\\nfor the default deploy user. A password will be required for sudo.\\n","tokens":29,"id":731,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\nand the server side code (which will be a dropwizard app).\nDepending on how the users want to run the service provider we may need\ndifferent security solutions.\n\n## Decision\nIf possible users can talk to the service provider on the loopback (127.0.0.1)\nIf that doesn't work for some reason then they can use the dropwizard config\nto set up basic auth or tls or something.\nSee http:\/\/www.dropwizard.io\/1.1.0\/docs\/manual\/configuration.html#connectors\n\n\n## Problem\nThe crypto plugin applies cryptographic operations to Keys and KeySets.\nIn order to do that it needs keys and initialization vectors (IV).\nThe problem is how to retrieve or derivate those keys in a safe way and how\nto pass them on to the underlying crypto libraries (OpenSSL and libgcrypt\nat the time of writing).\n\n## Decision\n### General Approach\nThe introduction of a GPG interface enables the user to utilize her existing key-pairs for cryptographic operations in Elektra.\nThe private key is used for encrypting a random sequence, which serves as seed for a key derivation function (KDF).\nThis way we can safely derivate cryptographic keys for symmetric value encryption.\nBoth OpenSSL and libgcrypt have built-in support for the PBKDF2 (see RFC 2898).\nThe PBKDF2 needs an iteration number and a salt in order to work.\nThose values will be stored per Key as MetaKey.\n### Implementation Details\nDuring the **mount phase** a random master password _r_ is being generated. _r_ is sent to the gpg binary for encryption. The resulting encrypted master password _m_ is stored in the plugin configuration at `config\/masterChallenge`.\nDuring the **set phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be encrypted. A random salt _s(k)_ is generated. By applying the PBKDF2 (mentioned earlier) with _r_ and _s(k)_, the cryptographic key _e(k)_ and the initialization vector _i(k)_ is being derived. The value of _k_ will be encrypted using _e(k)_ and _i(k)_. The seed _s(k)_ will be encoded as prefix into the encrypted value.\nDuring the **get phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be decrypted. The salt _s(k)_ is read from the encrypted message. By applying the PBKDF2 with _r_ and _s(k)_ the values of _e(k)_ and _i(k)_ are restored. Then the encrypted message can be decrypted.\n\n\n\nProvide a decision given the context below:\n## Context\nWe need to configure access to the servers in the cluster and need to decide between\npassword authentication and SSH key pair authentication.\n<end_of_turn>\n<start_of_turn>model\nWe have decided that all server log on authentication will use SSH key pair\nauthentication. The public keys will be provisioned onto the server at server creation\nfor the default deploy user. A password will be required for sudo.\n<end_of_turn>\n"}
{"File Name":"teaching-vacancies\/0011_use_devcontainers.md","Context":"## Context and Problem Statement\\nSetting up the Teaching Vacancies app for local development is a frustrating process involving many\\nsteps and multiple dependencies driven by manually updated setup documentation. A workflow based on\\n[devcontainers](https:\/\/code.visualstudio.com\/docs\/remote\/create-dev-container) would alleviate\\nmuch of this setup pain, and provide a trivially reproducible environment for local development,\\nbenefitting both developers and non-developers on the team.\\n## Decision Drivers\\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\n","Decision":"- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\\nenable users on restricted organisation-managed devices to contribute to the application\\nAdd devcontainers as an option for now, with a view to iterate on it and improve it to the point\\nwhere we can consider it the \"official\" default way of running Teaching Vacancies (while still\\nallowing other development workflows for developers who prefer different ways of working).\\n### Positive Consequences\\n- Drastically easier onboarding and \"re-boarding\" (e.g. on a new device or after an OS upgrade\\ncausing developer tooling issues)\\n- Dependencies reduced to just Git, Docker, and VS Code\\n- A fully functioning development environment is ready in 10 minutes from scratch, with no user\\ninteraction beyond opening the repository in VS Code and selecting \"Reopen in container\"\\n- Moving entirety of development experience into a container fixes past Docker development workflow\\nissues experienced on the team (where tasks and services where executed from the host instead of\\ninteracting with a shell and an editor from inside the container itself)\\n- Developers and other team members can develop on any host OS (macOS\/Linux\/Windows) but we only\\nneed to support one single consistent environment\\n- Does away with all the Mac vs Linux vs WSL setup steps in our current documentation\\n- Reduces likelihood of \"works on my machine\" development environment issues\\n- \"Leave no trace\" on the host machine and complete isolation from other projects\\n- Removes possibility of \"dependency hell\" when working on multiple projects\\n- Removes need to clutter local environment with applications and dependencies that need to be\\nkept up to date and in sync (e.g. Google Chrome and `chromedriver`)\\n- Removes need for language version managers (`rbenv`, `nvm`)\\n- Provides _executable documentation_ of project setup and dependencies\\n- Removes need for manually updated setup documentation that can become stale\\n- Experienced developers who have a different preferred workflow can get a clear, in-code view\\nof setup steps and dependencies\\n- Good workflow for everyone, but excellent additional integration with Visual Studio Code\\n- Automatic passthrough of SSH and GPG keys\\n- Language extensions run within the container itself, and can be specified in the devcontainer\\nconfiguration file for instant setup of useful extensions for new users\\n- Automatic bootstrapping of personal dotfiles\\n- Ability to easily move to cloud-based workflows in the future\\n- Ability to easily propagate new tools and improved configuration to all developers on the team\\n- Trivial rebuilds to a known good state when performing \"dangerous\" operations in the container\\n### Negative Consequences\\n- Slightly reduced performance on some host OSs (non-Linux) due to Docker being Linux-native\\ntechnology (overhead of containers running in an intermediate VM)\\n- Somewhat mitigated by use of volumes for IO-intensive cache directories\\n- Can be worked around entirely by moving workspace root into the container, and we will continue\\nto investigate before we fully agree on devcontainers as our default workflow\\n- Container layers need occasional pruning on the host as Docker can fill up disk space quickly\\n- Some duplication of Docker configuration between production and development configuration (but\\nthat is to be expected given that use cases are very different)\\n","tokens":224,"id":4370,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nThere is likely to  be a common requirement across Modernisation Platform consumers to utilise the benefits of using customised AMIs, such as managed consistency of configuration and speed of EC2 instance deployment.\nSome options to tackle AMI generation and management were considered\n* A custom approach - based on a reference architecture (see <https:\/\/aws.amazon.com\/blogs\/awsmarketplace\/announcing-the-golden-ami-pipeline\/>) that brings together a number of tools and techniques to manage AMIs through their lifecycle. This approach brings with it a fair amount of complexity and arguably extra operational overhead.\n* [Packer](https:\/\/www.packer.io\/) - a common approach across private and public cloud platforms, using this Hashicorp tool to programmatically build AMIs. Such a solution would also need to manage the distribution of AMIs across accounts and lifecycle management of AMIs\n* [AWS Image Builder](https:\/\/docs.aws.amazon.com\/imagebuilder\/latest\/userguide\/what-is-image-builder.html) - a managed AWS service incorporating concepts such as pipelines, recipes and components, and even a marketplace of build and test components. Image builder is based on the use of AWS Systems Manager (so no ssh connections and exposed ports). A solution based on this (at the time of writing) would need to also handle the lifecycle management of AMIs (as it the case with Packer)\n\n## Decision\nWe will create an AMI generation and management capability based on AWS Image Builder.\n\n\n## Context and Problem Statement\nTax users are sometimes, without warning, unnecessarily signed out when accessing\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\nsuccess criterion 2.1.1 (Timing adjustable)](https:\/\/www.w3.org\/WAI\/WCAG21\/Understanding\/timing-adjustable.html).\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https:\/\/github.com\/hmrc\/play-frontend-hmrc#warning-users-before-timing-them-out)\ncurrently has no way of knowing about this activity following initial page load.\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\nremaining on the user's active session via an endpoint that is itself excluded from\nsession management.\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\nknowledge and avoiding introducing additional coupling between frontend microservices?\n## Decision Drivers\n* The need to minimise code changes for service teams other than a library upgrade.\n* The avoidance of requiring service teams to add any additional routing rules.\n* The avoidance of requiring service teams to add any additional configuration.\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\nany other library or service.\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\nof failure between frontend microservices.\n* The need for the endpoint used to interrogate the session to not itself affect the session.\n\n## Decision\n* The need to minimise code changes for service teams other than a library upgrade.\n* The avoidance of requiring service teams to add any additional routing rules.\n* The avoidance of requiring service teams to add any additional configuration.\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\nany other library or service.\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\nof failure between frontend microservices.\n* The need for the endpoint used to interrogate the session to not itself affect the session.\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\naround knowledge duplication.\n\n\n\nProvide a decision given the context below:\n## Context and Problem Statement\nSetting up the Teaching Vacancies app for local development is a frustrating process involving many\nsteps and multiple dependencies driven by manually updated setup documentation. A workflow based on\n[devcontainers](https:\/\/code.visualstudio.com\/docs\/remote\/create-dev-container) would alleviate\nmuch of this setup pain, and provide a trivially reproducible environment for local development,\nbenefitting both developers and non-developers on the team.\n## Decision Drivers\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\nenable users on restricted organisation-managed devices to contribute to the application\n<end_of_turn>\n<start_of_turn>model\n- Complex and time-consuming onboarding and \"re-boarding\" experience of the application\n- Difficulties experiences by non-developers in getting the app set up locally, and getting it\nrunning again after major dependency changes (e.g. our recent addition of PostGIS)\n- Increasing adoption of devcontainers as a de-facto standard in the wider development community\nincluding [Ruby on Rails](https:\/\/github.com\/rails\/rails\/tree\/main\/.devcontainer)\n- Possible use of cloud-based development environments such as Github Codespaces in the future to\nenable users on restricted organisation-managed devices to contribute to the application\nAdd devcontainers as an option for now, with a view to iterate on it and improve it to the point\nwhere we can consider it the \"official\" default way of running Teaching Vacancies (while still\nallowing other development workflows for developers who prefer different ways of working).\n### Positive Consequences\n- Drastically easier onboarding and \"re-boarding\" (e.g. on a new device or after an OS upgrade\ncausing developer tooling issues)\n- Dependencies reduced to just Git, Docker, and VS Code\n- A fully functioning development environment is ready in 10 minutes from scratch, with no user\ninteraction beyond opening the repository in VS Code and selecting \"Reopen in container\"\n- Moving entirety of development experience into a container fixes past Docker development workflow\nissues experienced on the team (where tasks and services where executed from the host instead of\ninteracting with a shell and an editor from inside the container itself)\n- Developers and other team members can develop on any host OS (macOS\/Linux\/Windows) but we only\nneed to support one single consistent environment\n- Does away with all the Mac vs Linux vs WSL setup steps in our current documentation\n- Reduces likelihood of \"works on my machine\" development environment issues\n- \"Leave no trace\" on the host machine and complete isolation from other projects\n- Removes possibility of \"dependency hell\" when working on multiple projects\n- Removes need to clutter local environment with applications and dependencies that need to be\nkept up to date and in sync (e.g. Google Chrome and `chromedriver`)\n- Removes need for language version managers (`rbenv`, `nvm`)\n- Provides _executable documentation_ of project setup and dependencies\n- Removes need for manually updated setup documentation that can become stale\n- Experienced developers who have a different preferred workflow can get a clear, in-code view\nof setup steps and dependencies\n- Good workflow for everyone, but excellent additional integration with Visual Studio Code\n- Automatic passthrough of SSH and GPG keys\n- Language extensions run within the container itself, and can be specified in the devcontainer\nconfiguration file for instant setup of useful extensions for new users\n- Automatic bootstrapping of personal dotfiles\n- Ability to easily move to cloud-based workflows in the future\n- Ability to easily propagate new tools and improved configuration to all developers on the team\n- Trivial rebuilds to a known good state when performing \"dangerous\" operations in the container\n### Negative Consequences\n- Slightly reduced performance on some host OSs (non-Linux) due to Docker being Linux-native\ntechnology (overhead of containers running in an intermediate VM)\n- Somewhat mitigated by use of volumes for IO-intensive cache directories\n- Can be worked around entirely by moving workspace root into the container, and we will continue\nto investigate before we fully agree on devcontainers as our default workflow\n- Container layers need occasional pruning on the host as Docker can fill up disk space quickly\n- Some duplication of Docker configuration between production and development configuration (but\nthat is to be expected given that use cases are very different)\n<end_of_turn>\n"}
{"File Name":"copilot\/0006-do-something-for-one-way-tls.md","Context":"## Context\\nEnable one way TLS between front-end and envoy per host+domain that is\\nspecified via gateway config.\\n","Decision":"#### Gateway Configuration\\nThis is achieved by sending the following config from copilot.\\n```\\napiVersion: networking.istio.io\/v1alpha3\\nkind: Gateway\\nmetadata:\\nname: mygateway\\nspec:\\nselector:\\nistio: ingressgateway # use istio default ingress gateway\\nservers:\\n- port:\\nnumber: 443\\nname: https-httpbin\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-certs\/tls.key\\nhosts:\\n- \"httpbin.example.com\"\\n- port:\\nnumber: 443\\nname: https-bookinfo\\nprotocol: HTTPS\\ntls:\\nmode: SIMPLE\\nserverCertificate: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.crt\\nprivateKey: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.key\\nhosts:\\n- \"bookinfo.com\"\\n```\\nIn the config above each cert and key in the array of servers represent a\\nhost+domain and the path to each cert and the key is arbitrarily chosen.\\nCopilot extracts the domain information from the cert chains provided in the bosh spec properties:\\n```\\nfrontend_tls_keypairs:\\nexample:\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n- cert_chain: |\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\n-----BEGIN CERTIFICATE-----\\n-----END CERTIFICATE-----\\nprivate_key: |\\n-----BEGIN RSA PRIVATE KEY-----\\n-----END RSA PRIVATE KEY-----\\n```\\n#### Cert Storage\\nThe placement of the certs and keys on the envoy VM is done using a separate\\nprocess specific to this purpose. This process will be in charge of knowing\\nwhere the certs are located and placing the certs on the correct paths. It is\\nimportant for the envoy VM and copilot to agree on a path where the cert and the keys\\nare stored, and having a specific process to manage this will reduce duplication\\nand mitigate skew.\\n","tokens":25,"id":50,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe need to configure access to the servers in the cluster and need to decide between\npassword authentication and SSH key pair authentication.\n\n## Decision\nWe have decided that all server log on authentication will use SSH key pair\nauthentication. The public keys will be provisioned onto the server at server creation\nfor the default deploy user. A password will be required for sudo.\n\n\n## Context\nIn order to interact with a remote workspace, for example, at structurizr.com.\nThe remote or local workspace has to be (de-)serialized from or to JSON.\n\n## Decision\nIn order to perform these operations we choose\n[pydantic](https:\/\/pydantic-docs.helpmanual.io\/) which has a nice API, active\ncommunity, good data validation, helpful documentation, and good performance.\n\n\n\nProvide a decision given the context below:\n## Context\nEnable one way TLS between front-end and envoy per host+domain that is\nspecified via gateway config.\n<end_of_turn>\n<start_of_turn>model\n#### Gateway Configuration\nThis is achieved by sending the following config from copilot.\n```\napiVersion: networking.istio.io\/v1alpha3\nkind: Gateway\nmetadata:\nname: mygateway\nspec:\nselector:\nistio: ingressgateway # use istio default ingress gateway\nservers:\n- port:\nnumber: 443\nname: https-httpbin\nprotocol: HTTPS\ntls:\nmode: SIMPLE\nserverCertificate: \/etc\/istio\/ingressgateway-certs\/tls.crt\nprivateKey: \/etc\/istio\/ingressgateway-certs\/tls.key\nhosts:\n- \"httpbin.example.com\"\n- port:\nnumber: 443\nname: https-bookinfo\nprotocol: HTTPS\ntls:\nmode: SIMPLE\nserverCertificate: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.crt\nprivateKey: \/etc\/istio\/ingressgateway-bookinfo-certs\/tls.key\nhosts:\n- \"bookinfo.com\"\n```\nIn the config above each cert and key in the array of servers represent a\nhost+domain and the path to each cert and the key is arbitrarily chosen.\nCopilot extracts the domain information from the cert chains provided in the bosh spec properties:\n```\nfrontend_tls_keypairs:\nexample:\n- cert_chain: |\n-----BEGIN CERTIFICATE-----\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n-----END CERTIFICATE-----\nprivate_key: |\n-----BEGIN RSA PRIVATE KEY-----\n-----END RSA PRIVATE KEY-----\n- cert_chain: |\n-----BEGIN CERTIFICATE-----\n-----END CERTIFICATE-----\n-----BEGIN CERTIFICATE-----\n-----END CERTIFICATE-----\nprivate_key: |\n-----BEGIN RSA PRIVATE KEY-----\n-----END RSA PRIVATE KEY-----\n```\n#### Cert Storage\nThe placement of the certs and keys on the envoy VM is done using a separate\nprocess specific to this purpose. This process will be in charge of knowing\nwhere the certs are located and placing the certs on the correct paths. It is\nimportant for the envoy VM and copilot to agree on a path where the cert and the keys\nare stored, and having a specific process to manage this will reduce duplication\nand mitigate skew.\n<end_of_turn>\n"}
{"File Name":"libelektra\/reference_counting.md","Context":"## Problem\\n- locking is not reset when ref counting again gets 0 (adding to keyset and\\npop again) #2202\\n- C++ API for KeySet and Key has unexpected differences: also use ref counting\\nfor KeySets (also suggested in #1332)\\n","Decision":"- add second counter to Key\\n- One counter is for references, the other one is for locking the keyname. The keyname is thereby protected with a re-entrant lock.\\n- introduce reference counter for KeySets (for external keyset references, e.g. in bindings)\\n- limit number of references to `UINT16_MAX - 1` and use `UINT16_MAX` as an error value\\n- return error on reference count overflow\\n- no error on underflow (decrement when zero), instead stay at zero\\n- use fixed sized types (`uint16_t`) for reference counters\\n- increment\/decrement references before\/after passing instances to plugins\\n","tokens":59,"id":1309,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Problem\nWhen doing kdbGet() possible more keys are returned which might be\nconfusing. When doing a second kdbGet() with a new keyset\nno keys might be returned, because it is up-to-date.\nWhen doing kdbSet() a deep duplication is needed.\nIdea: keep a duplicated keyset internally. Return (non-deep?)\nduplications exactly of the parts as requested.\n\n## Decision\nNot yet decided.\n- [Global Validation](global_validation.md)\n\n\n## Problem\nThe crypto plugin applies cryptographic operations to Keys and KeySets.\nIn order to do that it needs keys and initialization vectors (IV).\nThe problem is how to retrieve or derivate those keys in a safe way and how\nto pass them on to the underlying crypto libraries (OpenSSL and libgcrypt\nat the time of writing).\n\n## Decision\n### General Approach\nThe introduction of a GPG interface enables the user to utilize her existing key-pairs for cryptographic operations in Elektra.\nThe private key is used for encrypting a random sequence, which serves as seed for a key derivation function (KDF).\nThis way we can safely derivate cryptographic keys for symmetric value encryption.\nBoth OpenSSL and libgcrypt have built-in support for the PBKDF2 (see RFC 2898).\nThe PBKDF2 needs an iteration number and a salt in order to work.\nThose values will be stored per Key as MetaKey.\n### Implementation Details\nDuring the **mount phase** a random master password _r_ is being generated. _r_ is sent to the gpg binary for encryption. The resulting encrypted master password _m_ is stored in the plugin configuration at `config\/masterChallenge`.\nDuring the **set phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be encrypted. A random salt _s(k)_ is generated. By applying the PBKDF2 (mentioned earlier) with _r_ and _s(k)_, the cryptographic key _e(k)_ and the initialization vector _i(k)_ is being derived. The value of _k_ will be encrypted using _e(k)_ and _i(k)_. The seed _s(k)_ will be encoded as prefix into the encrypted value.\nDuring the **get phase** the master password _m_ is sent to the gpg binary for decryption in order to retrieve _r_. The following steps will be repeated for every Key _k_, that is supposed to be decrypted. The salt _s(k)_ is read from the encrypted message. By applying the PBKDF2 with _r_ and _s(k)_ the values of _e(k)_ and _i(k)_ are restored. Then the encrypted message can be decrypted.\n\n\n\nProvide a decision given the context below:\n## Problem\n- locking is not reset when ref counting again gets 0 (adding to keyset and\npop again) #2202\n- C++ API for KeySet and Key has unexpected differences: also use ref counting\nfor KeySets (also suggested in #1332)\n<end_of_turn>\n<start_of_turn>model\n- add second counter to Key\n- One counter is for references, the other one is for locking the keyname. The keyname is thereby protected with a re-entrant lock.\n- introduce reference counter for KeySets (for external keyset references, e.g. in bindings)\n- limit number of references to `UINT16_MAX - 1` and use `UINT16_MAX` as an error value\n- return error on reference count overflow\n- no error on underflow (decrement when zero), instead stay at zero\n- use fixed sized types (`uint16_t`) for reference counters\n- increment\/decrement references before\/after passing instances to plugins\n<end_of_turn>\n"}
{"File Name":"modernisation-platform\/0011-use-vpc-flow-logs-to-gain-insight-into-network-state.md","Context":"## Context\\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence.\\n","Decision":"[VPC flow flows](https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/flow-logs.html) contain information about data coming into and out of attached network interfaces. As such, flow log data could be collected, aggregated, analysed and visualised in order to provide insights into the traffic flowing (or not) through VPCs. VPC flow logs are already collected in environment accounts and at the platform-level in the core-logging account, within CloudWatch log groups.\\n","tokens":65,"id":3150,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe need to design the building blocks of the Digital Citizenship platform. We will make decisions about the overall architecture and the building blocks we're going to use to satisfy the functional and non-functional requirements.\nWhen deciding what technologies to use and how to satisfy those requirements, we're going to evaluate whether we want to build certain components from scratch, use open-source solutions managed by us or rely on existing public cloud components, managed by 3rd party providers.\nAs the world of cloud technologies continues to accelerate, we should absorb new developments into how we work. Leading technology organisations are rapidly embracing new tools like \"serverless\" computing.\nAt the infrastructure and application level we should expect our applications to be resilient, flexible and API-driven. We should have the tools and practices in place to manage and secure a distributed range of tools accessed over the internet.\nDecision criteria will also include:\n* elasticity and resilience\n* pay-as-you-go pricing\n* exploiting economies of scale\n* exploiting falling costs of infrastructure over time\n* quality of management tools\n* best of breed security\n* flexibility and opportunity costs\n\n## Decision\n* elasticity and resilience\n* pay-as-you-go pricing\n* exploiting economies of scale\n* exploiting falling costs of infrastructure over time\n* quality of management tools\n* best of breed security\n* flexibility and opportunity costs\nWe decide to build the components of the Digital Citizenship initiative on top of scalable, managed and cost-effective components provided by the leading public cloud providers.\n\n\n## Context\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\n\n## Decision\nWe will use [Quay.io](https:\/\/quay.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and\/or enterprise artifacts with trail licenses).\nWe\u2019d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we'd prefer a SaaS solution.\nThe REST API of Quay.io allows our organization to potentially automate user provisioning\/invitation and user\/group management which is not available for Docker Cloud at this time.\nAdditionally, Quay \/ CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product.\nThe summary [![report of the comparison](https:\/\/img.shields.io\/badge\/report%20of%20the%20comparison-PRIVATE-red.svg)](https:\/\/ts.alfresco.com\/share\/s\/mVAV1sGIReC_iqgMN0GGnQ) also contains reference links to the full investigation.\n\n\n\nProvide a decision given the context below:\n## Context\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence.\n<end_of_turn>\n<start_of_turn>model\n[VPC flow flows](https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/flow-logs.html) contain information about data coming into and out of attached network interfaces. As such, flow log data could be collected, aggregated, analysed and visualised in order to provide insights into the traffic flowing (or not) through VPCs. VPC flow logs are already collected in environment accounts and at the platform-level in the core-logging account, within CloudWatch log groups.\n<end_of_turn>\n"}
{"File Name":"old-web-frontend\/0002-use-aws-s3-as-host-solution.md","Context":"## Context and Problem Statement\\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\\n## Decision Drivers\\n* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\n","Decision":"* Easy to update\\n* High availability\\n* Easy to configure HTTPS\\n* Observability\\nChosen option: \"AWS\", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\\n","tokens":53,"id":583,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context and Problem Statement\nHow do we serve the API using a production ready server?\n## Decision Drivers\n* use production ready server.\n* good integration with reverse proxy servers like Nginx and Apache.\n* support Flask in a simple and performant way\n\n## Decision\n* use production ready server.\n* good integration with reverse proxy servers like Nginx and Apache.\n* support Flask in a simple and performant way\nChosen option: \"[option 1]\", because it is well known in the\nindustry. It is not the most performant but it has good performances\nto serve WSGI app like Flask thanks to the uwsgi protocol used between\nNginx\/Apache.\n### Negative Consequences\n* complexify the architecture.\n\n\n## Context and Problem Statement\nWe need to store data internally in a structured way to gain performance.\n## Decision Drivers\n* Easy to integrate\n* Easy to use\n* Common technology\n\n## Decision\n* Easy to integrate\n* Easy to use\n* Common technology\nChosen option: \"H2 Database Engine\", because it was straight-forward to use.\n\n\n\nProvide a decision given the context below:\n## Context and Problem Statement\nWe need to host our web application so clients can access it. The solution must be easy to manage and update.\n## Decision Drivers\n* Easy to update\n* High availability\n* Easy to configure HTTPS\n* Observability\n<end_of_turn>\n<start_of_turn>model\n* Easy to update\n* High availability\n* Easy to configure HTTPS\n* Observability\nChosen option: \"AWS\", because it's the one we have the most experience. The 3 solutions analyzed are pretty similar in regard to the drivers considered. Given that, we made our decision based on our previous experience.\n<end_of_turn>\n"}
{"File Name":"js-sdk\/0008-add-kwargs-to-3bot-start.md","Context":"## Context\\nAdding packages with kwargs has some limitations and hence kwargs are needed every time start is called not only once when adding package.\\n","Decision":"Add kwargs passed to the package instance that will be saved locally, and can be retrieved everytime the threebot server restarts and starts the package.\\n","tokens":28,"id":5188,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context and Problem Statement\nTests including Prolog queries with SWI Prolog can not be executed with CircleCI because SWI Prolog is not\nrunning at the executing system.\n\n## Decision\nChosen option: extra test class to reduce the effort and to still enable testing SWI Prolog locally. For this the extra class is\nannotated with @Ignore. All other tests run with CircleCI. These tests includes the core functionalities of the developed tool.\n\n\n## Context\nIn ES5, implement interceptor will modify existing class and this operation is not reversible.\nIt may cause compatibility issue with other frameworks.\n\n## Decision\nWe will add ES6 Proxy for a transparent layer which not modifying user's class\/object.\n\n\n\nProvide a decision given the context below:\n## Context\nAdding packages with kwargs has some limitations and hence kwargs are needed every time start is called not only once when adding package.\n<end_of_turn>\n<start_of_turn>model\nAdd kwargs passed to the package instance that will be saved locally, and can be retrieved everytime the threebot server restarts and starts the package.\n<end_of_turn>\n"}
{"File Name":"front-end-monorepo\/adr-19.md","Context":"## Context\\nAs part of the lib-classifier, we are developing a component for displaying multi-frame subjects in the classification interface. Initially, we referred to this viewer as the PagesViewer, but changed the name to multiFrame Viewer to reflect more general use cases. In the first iteration, we will focus on multiImage features necessary for transcription projects. As projects with different requirements, such as ecology projects, move to the new classifier, we can add new features to the filmstrip component (see decisions 6 an 7).\\nSee the [InVision document for the Pages Viewer](https:\/\/projects.invisionapp.com\/d\/main#\/console\/12924056\/393421254\/preview) and Issue #1142.\\n","Decision":"1. **When to Render:** There will be an explicit workflow configuration (i.e., `{ subject_viewer: 'multiFrame' }`) for the multiFrame Viewer. However, as a fallback, the multiFrame subject viewer will render if (1) the subject delivered to the client has multiple frames and (2) the workflow is not configured to display all frames at once.\\n2. **Number of Frames:** The multiFrame Viewer will display up to ten frames. All frames after the tenth frame will be ignored.\\n3. **Frame Display**: For every frame that is an image, the filmstrip will render a thumbnail of the image. The fallback for non image frames will be a dot. Later, we will need to think about subjects with mixed media frames. HTML Inputs will be used instead of HTML buttons to display each frame.\\n4. **Selected Image Display**: The selected image will be displayed using the SingleImageViewer.\\n5. **Location of Filmstrip:** The filmstrip will be located to the left of the displayed image. This will help for images that are tall as well as make the interface easier to use for volunteers navigating by keyboard or screen reader (see https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1262).\\n6. **Playing Slideshow**: Although adding a slideshow play button will be necessary for non-transcription projects, at this time, the play button is a necessary enhancement\/future feature.\\n7. **Collapsing Filmstrip**: Adding a button to collapse the image previews into pagination dots is an enhancement\/future feature.\\n","tokens":147,"id":498,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\n*This section describes the forces at play, including technological, political, social, and project local.\nThese forces are probably in tension, and should be called out as such. The language in this section is value-neutral.\nIt is simply describing facts.*\nThe Office of Development Transformation (ODT) is focused on helping CAD learn and adopt lean, agile, and devOps techniques and\nprocesses. ODT will be initialy focus on Green Field Developent. The Online Bank is a sample app used in the ODT University classes.  The Purpose of theapplication is to provide a\nsample app as a context for the training courses.  We believe most people are familiar with online\nbanking, and won't need to spend a lot of time learning the problem domain.\nThe purpose of this document is to determine the technology stack for the sample application.\n**NOTE:** While we are initially focused on a Java Web stack, we may elect to develop out an example in the Microsoft Stack.\nIf you would like to help, please contact us.\n\n## Decision\n*This section describes our response to these forces. It is stated in full sentences, with active voice.\n\"We will ...\"*\nWe will be focusing on the modern web java stack.\n### UI Technology Stack:\n- Angular 2\n- Bootstrap\n- protractor\n- jasmine\n### Server Side Technology Stack:\n- Java 1.8+\n- SpringBoot 1.4+\n- Restful\n- MySQL\n- Spring Data JPA (Hibernate)\n- JUnit 5 \/ Spock\n### Development Technology Stack:\n- Local Development\n- SCM: GIT\n- Intellij\n- Favorite Editor\n- Gradle\n- Angular Quickstart\n- Concourse CI (CI\/CD)\n- Jira\n- Sonar\n#### Deployment:\n- Pivotal Cloud Foundry (Java and Static Build Packs)\n\n\n## Context and Problem Statement\nThe [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.\nHistorically, the Firefox Accounts codebase has not adhered to a formal CSS structure. This ADR serves to determine how we'll approach our CSS architecture in the Settings Redesign project, evaluating libraries and frameworks to determine which if any will be the best option for the FxA ecosystem. It is part 2 of two [Settings Redesign CSS ADRs](https:\/\/github.com\/mozilla\/fxa\/issues\/5087); part 1, detailing how we'll approach build conventions and variables, [can be found here](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0015-use-css-variables-and-scss.md).\nConsiderations around class naming conventions, color and measurement standards, interoperability across shared components, and custom configuration options offered by each library to meet Settings Redesign design standards are taken into account. Notably, the new design uses space measurements in increments of 8px and [colors](https:\/\/protocol.mozilla.org\/fundamentals\/color.html) are based in Mozilla Protocol's design system, where a hue's brightness scales in increments of 10.\n## Decision Drivers\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\n\n## Decision\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\nChosen options: \"Option B\" with Tailwind CSS for majority styling, and implementation details from \"Option D\" when utility classes don't meet the entire need, because:\n- Of the options set forth, a utility library provides us with the most flexible yet durable set of tools.\n- Single-purpose classes are performant and reduce the possibility of overly-complex or convoluted stylesheets.\n- A utility library is leaner and less opinionated compared to a set of UI components and other options, allowing greater flexibility and reusability across various projects.\n- Our team has prior experience with Tailwind in particular and newcomers should ramp up quickly with a utility pattern.\n- Tailwind is highly configurable without being cumbersome, allowing us to modify type and spacing scales, define color ranges, and set up media queries to meet our exact needs.\n- For cases when we do need to write custom SCSS we will structure our React components to initially rely on utility classes, but allow additional custom styles to be written in an adjacent SCSS file when needed. This is also applicable to components in `fxa-components` where the component can accept a `classes` prop with a list of needed utility classes, and any additional styling can be done in an external SCSS file located where the component was composed as needed (e.g., outside of `fxa-components`). CSS variables can be shared across the Tailwind configuration and in custom SCSS.\n- Note: class name conventions for the custom SCSS will be declared when the library configuration is setup, as we'd like them to make sense together. Examples will be provided in the `fxa-settings` README at this time as well.\n\n\n\nProvide a decision given the context below:\n## Context\nAs part of the lib-classifier, we are developing a component for displaying multi-frame subjects in the classification interface. Initially, we referred to this viewer as the PagesViewer, but changed the name to multiFrame Viewer to reflect more general use cases. In the first iteration, we will focus on multiImage features necessary for transcription projects. As projects with different requirements, such as ecology projects, move to the new classifier, we can add new features to the filmstrip component (see decisions 6 an 7).\nSee the [InVision document for the Pages Viewer](https:\/\/projects.invisionapp.com\/d\/main#\/console\/12924056\/393421254\/preview) and Issue #1142.\n<end_of_turn>\n<start_of_turn>model\n1. **When to Render:** There will be an explicit workflow configuration (i.e., `{ subject_viewer: 'multiFrame' }`) for the multiFrame Viewer. However, as a fallback, the multiFrame subject viewer will render if (1) the subject delivered to the client has multiple frames and (2) the workflow is not configured to display all frames at once.\n2. **Number of Frames:** The multiFrame Viewer will display up to ten frames. All frames after the tenth frame will be ignored.\n3. **Frame Display**: For every frame that is an image, the filmstrip will render a thumbnail of the image. The fallback for non image frames will be a dot. Later, we will need to think about subjects with mixed media frames. HTML Inputs will be used instead of HTML buttons to display each frame.\n4. **Selected Image Display**: The selected image will be displayed using the SingleImageViewer.\n5. **Location of Filmstrip:** The filmstrip will be located to the left of the displayed image. This will help for images that are tall as well as make the interface easier to use for volunteers navigating by keyboard or screen reader (see https:\/\/github.com\/zooniverse\/front-end-monorepo\/issues\/1262).\n6. **Playing Slideshow**: Although adding a slideshow play button will be necessary for non-transcription projects, at this time, the play button is a necessary enhancement\/future feature.\n7. **Collapsing Filmstrip**: Adding a button to collapse the image previews into pagination dots is an enhancement\/future feature.\n<end_of_turn>\n"}
{"File Name":"govuk-docker\/0004-use-versioned-database-services.md","Context":"## Context\\nGOV.UK Docker has services defined for various databases, such as PostgreSQL,\\nMongoDB and Elasticsearch. These have their data persisted in\\n[Docker volumes][volume] defined [for each service][defined-volumes].\\nWhen GOV.UK Docker updated from [MongoDB 2.4 to 3.6][mongodb-update] users\\nbegan receiving cryptic errors when trying to run applications that depended\\non MongoDB. This was because they had a volume with data structured for MongoDB\\n2.4 whereas MongoDB was expecting this to be structured for 3.6.\\nWe felt that this upgrade path was more confusing and difficult than it needed\\nbe and wanted to improve it for the future.\\n[volume]: https:\/\/docs.docker.com\/storage\/volumes\/\\n[defined-volumes]: https:\/\/github.com\/alphagov\/govuk-docker\/blob\/cb124a3a1d3353e777d4f777d77f03f93415d415\/docker-compose.yml#L5-L10\\n[mongodb-update]: https:\/\/github.com\/alphagov\/govuk-docker\/pull\/356\\n","Decision":"We decided that we would rename database services, and their respective\\nvolumes, to reflect the software version number. For example the `mongo`\\nservice, and namesake volume, have been renamed to `mongo-3.6`.\\nThe precision of the version number is chosen based on the backwards\\ncompatibility between versions of the software. For example to upgrade from\\nMySQL 5.5 to 5.6 requires an upgrade script and thus would cause problems for\\nGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not. For\\nservices that aren't sensitive to minor versions, such as Elasticsearch, we\\nhave specified the service and volume with respective to a major version:\\n`elasticsearch-6`.\\n","tokens":238,"id":1666,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\nAdditionally Geocoder can be used to verify addresses that are manually entered.\n- [Geocoder](https:\/\/www2.gov.bc.ca\/gov\/content\/data\/geographic-data-services\/location-services\/geocoder)\n- [Data BC](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service)\n- [API Swagger](https:\/\/catalogue.data.gov.bc.ca\/dataset\/bc-address-geocoder-web-service\/resource\/40d6411e-ab98-4df9-a24e-67f81c45f6fa\/view\/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\n- [Developer API Keys](https:\/\/github.com\/bcgov\/gwa\/wiki\/Developer-Guide#developer-api-keys)\n- API Host = `https:\/\/geocoder.api.gov.bc.ca`\n\n## Decision\nIntegrate with Data BC Geocoder API.\nWhen a user types an address a list of viable matches will be displayed.\nIf the user selects one of the matches it will be used to set the address and GIS coordinates,.\n\n\n## Context and Problem Statement\nTracking consent reads the PLAY_LANG cookie to determine whether to render the banner in Welsh. PLAY_LANG is the cookie\nused by the Play Framework to persist a user's language preferences. It will be set to 'cy'\nwhen a user has selected Welsh using the language toggle in MDTP services using the Play Framework.\nTeams are increasingly setting PLAY_LANG to HttpOnly in an attempt to get green ZAP tests, even though there are no\nknown security concerns around keeping PLAY_LANG as a normal cookie. Setting a cookie to\nHttpOnly makes it unreadable within the client-side Javascript code that renders the tracking consent banner. The result\nof this is that the banner will not be translated into Welsh for these services.\nA related issue is that PLAY_LANG is not set for classic services written in Java, which means a Welsh version of the banner is not\ncurrently available for classic services.\nIt is worth noting that the only other known instance of our reading PLAY_LANG using Javascript is in the assets-frontend\n[timeout dialog](https:\/\/github.com\/hmrc\/assets-frontend\/blob\/97c638289e23bee255ac30724a8572c6efa96817\/assets\/patterns\/help-users-when-we-time-them-out-of-a-service\/timeoutDialog.js#L14) timeout dialog. All the new govuk-frontend and hmrc-frontend components use data attributes instead.\nShould we remove the reading of PLAY_LANG in tracking consent and accept a data-language attribute instead?\n## Decision Drivers\n* The need to support classic services\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\navoid services having to upgrade a second time.\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\n* The preference for consistency in language settings across our frontend components\n\n## Decision\n* The need to support classic services\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\navoid services having to upgrade a second time.\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\n* The preference for consistency in language settings across our frontend components\nChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'\ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG into Zap tests. We also agreed that our frontend\ncomponents should be consistent in their treatment of language until such time as we are able to provide an\nalternative approach that works for all components.\n### Positive Consequences\n* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner\n* Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings\/errors\n* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etc\n### Negative Consequences\n* We will need to make a small change to tracking consent and communicate this change to teams\n* Teams will need to upgrade to the latest version of play-ui\/play-frontend-hmrc to get language translations\n* Teams not using the tracking consent helpers will need to add a data-attribute for the language\n\n\n\nProvide a decision given the context below:\n## Context\nGOV.UK Docker has services defined for various databases, such as PostgreSQL,\nMongoDB and Elasticsearch. These have their data persisted in\n[Docker volumes][volume] defined [for each service][defined-volumes].\nWhen GOV.UK Docker updated from [MongoDB 2.4 to 3.6][mongodb-update] users\nbegan receiving cryptic errors when trying to run applications that depended\non MongoDB. This was because they had a volume with data structured for MongoDB\n2.4 whereas MongoDB was expecting this to be structured for 3.6.\nWe felt that this upgrade path was more confusing and difficult than it needed\nbe and wanted to improve it for the future.\n[volume]: https:\/\/docs.docker.com\/storage\/volumes\/\n[defined-volumes]: https:\/\/github.com\/alphagov\/govuk-docker\/blob\/cb124a3a1d3353e777d4f777d77f03f93415d415\/docker-compose.yml#L5-L10\n[mongodb-update]: https:\/\/github.com\/alphagov\/govuk-docker\/pull\/356\n<end_of_turn>\n<start_of_turn>model\nWe decided that we would rename database services, and their respective\nvolumes, to reflect the software version number. For example the `mongo`\nservice, and namesake volume, have been renamed to `mongo-3.6`.\nThe precision of the version number is chosen based on the backwards\ncompatibility between versions of the software. For example to upgrade from\nMySQL 5.5 to 5.6 requires an upgrade script and thus would cause problems for\nGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not. For\nservices that aren't sensitive to minor versions, such as Elasticsearch, we\nhave specified the service and volume with respective to a major version:\n`elasticsearch-6`.\n<end_of_turn>\n"}
{"File Name":"james\/0004-distributed-tasks-listing.md","Context":"## Context\\nBy switching the task manager to a distributed implementation, we need to be able to `list` all `Task`s running on the cluster.\\n","Decision":"* Read a Cassandra projection to get all `Task`s and their `Status`\\n","tokens":32,"id":2140,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\n\n## Decision\n* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\n\n\n## Context and Problem Statement\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\narbitrary style overrides on the component.\nWe want all components in FDS to follow a similar prop interface, so this decision should\napply to _all_ components.\n\n## Decision\nChosen option: \"No components accept a `className`\", because...\n- It's too risky to introduce style overrides to low-level layout components.\n- Style customization should happen via props, where the component can be responsible for\nstyle changes and variations.\n### Positive Consequences <!-- optional -->\n- Prop API consistency across all components.\n### Negative Consequences <!-- optional -->\n- Consumers of FDS components must handle the breaking change. Any custom `className`\nprops on FDS components must be moved to a direct parent or direct child of the\ncomponent.\n\n\n\nProvide a decision given the context below:\n## Context\nBy switching the task manager to a distributed implementation, we need to be able to `list` all `Task`s running on the cluster.\n<end_of_turn>\n<start_of_turn>model\n* Read a Cassandra projection to get all `Task`s and their `Status`\n<end_of_turn>\n"}
{"File Name":"cosmos-sdk\/adr-043-nft-module.md","Context":"## Context\\nNFTs are more than just crypto art, which is very helpful for accruing value to the Cosmos ecosystem. As a result, Cosmos Hub should implement NFT functions and enable a unified mechanism for storing and sending the ownership representative of NFTs as discussed in https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065.\\nAs discussed in [#9065](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065), several potential solutions can be considered:\\n* irismod\/nft and modules\/incubator\/nft\\n* CW721\\n* DID NFTs\\n* interNFT\\nSince functions\/use cases of NFTs are tightly connected with their logic, it is almost impossible to support all the NFTs' use cases in one Cosmos SDK module by defining and implementing different transaction types.\\nConsidering generic usage and compatibility of interchain protocols including IBC and Gravity Bridge, it is preferred to have a generic NFT module design which handles the generic NFTs logic.\\nThis design idea can enable composability that application-specific functions should be managed by other modules on Cosmos Hub or on other Zones by importing the NFT module.\\nThe current design is based on the work done by [IRISnet team](https:\/\/github.com\/irisnet\/irismod\/tree\/master\/modules\/nft) and an older implementation in the [Cosmos repository](https:\/\/github.com\/cosmos\/modules\/tree\/master\/incubator\/nft).\\n","Decision":"We create a `x\/nft` module, which contains the following functionality:\\n* Store NFTs and track their ownership.\\n* Expose `Keeper` interface for composing modules to transfer, mint and burn NFTs.\\n* Expose external `Message` interface for users to transfer ownership of their NFTs.\\n* Query NFTs and their supply information.\\nThe proposed module is a base module for NFT app logic. It's goal it to provide a common layer for storage, basic transfer functionality and IBC. The module should not be used as a standalone.\\nInstead an app should create a specialized module to handle app specific logic (eg: NFT ID construction, royalty), user level minting and burning. Moreover an app specialized module should handle auxiliary data to support the app logic (eg indexes, ORM, business data).\\nAll data carried over IBC must be part of the `NFT` or `Class` type described below. The app specific NFT data should be encoded in `NFT.data` for cross-chain integrity. Other objects related to NFT, which are not important for integrity can be part of the app specific module.\\n### Types\\nWe propose two main types:\\n* `Class` -- describes NFT class. We can think about it as a smart contract address.\\n* `NFT` -- object representing unique, non fungible asset. Each NFT is associated with a Class.\\n#### Class\\nNFT **Class** is comparable to an ERC-721 smart contract (provides description of a smart contract), under which a collection of NFTs can be created and managed.\\n```protobuf\\nmessage Class {\\nstring id          = 1;\\nstring name        = 2;\\nstring symbol      = 3;\\nstring description = 4;\\nstring uri         = 5;\\nstring uri_hash    = 6;\\ngoogle.protobuf.Any data = 7;\\n}\\n```\\n* `id` is used as the primary index for storing the class; _required_\\n* `name` is a descriptive name of the NFT class; _optional_\\n* `symbol` is the symbol usually shown on exchanges for the NFT class; _optional_\\n* `description` is a detailed description of the NFT class; _optional_\\n* `uri` is a URI for the class metadata stored off chain. It should be a JSON file that contains metadata about the NFT class and NFT data schema ([OpenSea example](https:\/\/docs.opensea.io\/docs\/contract-level-metadata)); _optional_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is app specific metadata of the class; _optional_\\n#### NFT\\nWe define a general model for `NFT` as follows.\\n```protobuf\\nmessage NFT {\\nstring class_id           = 1;\\nstring id                 = 2;\\nstring uri                = 3;\\nstring uri_hash           = 4;\\ngoogle.protobuf.Any data  = 10;\\n}\\n```\\n* `class_id` is the identifier of the NFT class where the NFT belongs; _required_\\n* `id` is an identifier of the NFT, unique within the scope of its class. It is specified by the creator of the NFT and may be expanded to use DID in the future. `class_id` combined with `id` uniquely identifies an NFT and is used as the primary index for storing the NFT; _required_\\n```text\\n{class_id}\/{id} --> NFT (bytes)\\n```\\n* `uri` is a URI for the NFT metadata stored off chain. Should point to a JSON file that contains metadata about this NFT (Ref: [ERC721 standard and OpenSea extension](https:\/\/docs.opensea.io\/docs\/metadata-standards)); _required_\\n* `uri_hash` is a hash of the document pointed by uri; _optional_\\n* `data` is an app specific data of the NFT. CAN be used by composing modules to specify additional properties of the NFT; _optional_\\nThis ADR doesn't specify values that `data` can take; however, best practices recommend upper-level NFT modules clearly specify their contents.  Although the value of this field doesn't provide the additional context required to manage NFT records, which means that the field can technically be removed from the specification, the field's existence allows basic informational\/UI functionality.\\n### `Keeper` Interface\\n```go\\ntype Keeper interface {\\nNewClass(ctx sdk.Context,class Class)\\nUpdateClass(ctx sdk.Context,class Class)\\nMint(ctx sdk.Context,nft NFT\uff0creceiver sdk.AccAddress)   \/\/ updates totalSupply\\nBatchMint(ctx sdk.Context, tokens []NFT,receiver sdk.AccAddress) error\\nBurn(ctx sdk.Context, classId string, nftId string)    \/\/ updates totalSupply\\nBatchBurn(ctx sdk.Context, classID string, nftIDs []string) error\\nUpdate(ctx sdk.Context, nft NFT)\\nBatchUpdate(ctx sdk.Context, tokens []NFT) error\\nTransfer(ctx sdk.Context, classId string, nftId string, receiver sdk.AccAddress)\\nBatchTransfer(ctx sdk.Context, classID string, nftIDs []string, receiver sdk.AccAddress) error\\nGetClass(ctx sdk.Context, classId string) Class\\nGetClasses(ctx sdk.Context) []Class\\nGetNFT(ctx sdk.Context, classId string, nftId string) NFT\\nGetNFTsOfClassByOwner(ctx sdk.Context, classId string, owner sdk.AccAddress) []NFT\\nGetNFTsOfClass(ctx sdk.Context, classId string) []NFT\\nGetOwner(ctx sdk.Context, classId string, nftId string) sdk.AccAddress\\nGetBalance(ctx sdk.Context, classId string, owner sdk.AccAddress) uint64\\nGetTotalSupply(ctx sdk.Context, classId string) uint64\\n}\\n```\\nOther business logic implementations should be defined in composing modules that import `x\/nft` and use its `Keeper`.\\n### `Msg` Service\\n```protobuf\\nservice Msg {\\nrpc Send(MsgSend)         returns (MsgSendResponse);\\n}\\nmessage MsgSend {\\nstring class_id = 1;\\nstring id       = 2;\\nstring sender   = 3;\\nstring reveiver = 4;\\n}\\nmessage MsgSendResponse {}\\n```\\n`MsgSend` can be used to transfer the ownership of an NFT to another address.\\nThe implementation outline of the server is as follows:\\n```go\\ntype msgServer struct{\\nk Keeper\\n}\\nfunc (m msgServer) Send(ctx context.Context, msg *types.MsgSend) (*types.MsgSendResponse, error) {\\n\/\/ check current ownership\\nassertEqual(msg.Sender, m.k.GetOwner(msg.ClassId, msg.Id))\\n\/\/ transfer ownership\\nm.k.Transfer(msg.ClassId, msg.Id, msg.Receiver)\\nreturn &types.MsgSendResponse{}, nil\\n}\\n```\\nThe query service methods for the `x\/nft` module are:\\n```protobuf\\nservice Query {\\n\/\/ Balance queries the number of NFTs of a given class owned by the owner, same as balanceOf in ERC721\\nrpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/balance\/{owner}\/{class_id}\";\\n}\\n\/\/ Owner queries the owner of the NFT based on its class and id, same as ownerOf in ERC721\\nrpc Owner(QueryOwnerRequest) returns (QueryOwnerResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/owner\/{class_id}\/{id}\";\\n}\\n\/\/ Supply queries the number of NFTs from the given class, same as totalSupply of ERC721.\\nrpc Supply(QuerySupplyRequest) returns (QuerySupplyResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/supply\/{class_id}\";\\n}\\n\/\/ NFTs queries all NFTs of a given class or owner,choose at least one of the two, similar to tokenByIndex in ERC721Enumerable\\nrpc NFTs(QueryNFTsRequest) returns (QueryNFTsResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\";\\n}\\n\/\/ NFT queries an NFT based on its class and id.\\nrpc NFT(QueryNFTRequest) returns (QueryNFTResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\/{class_id}\/{id}\";\\n}\\n\/\/ Class queries an NFT class based on its id\\nrpc Class(QueryClassRequest) returns (QueryClassResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\/{class_id}\";\\n}\\n\/\/ Classes queries all NFT classes\\nrpc Classes(QueryClassesRequest) returns (QueryClassesResponse) {\\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\";\\n}\\n}\\n\/\/ QueryBalanceRequest is the request type for the Query\/Balance RPC method\\nmessage QueryBalanceRequest {\\nstring class_id = 1;\\nstring owner    = 2;\\n}\\n\/\/ QueryBalanceResponse is the response type for the Query\/Balance RPC method\\nmessage QueryBalanceResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryOwnerRequest is the request type for the Query\/Owner RPC method\\nmessage QueryOwnerRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryOwnerResponse is the response type for the Query\/Owner RPC method\\nmessage QueryOwnerResponse {\\nstring owner = 1;\\n}\\n\/\/ QuerySupplyRequest is the request type for the Query\/Supply RPC method\\nmessage QuerySupplyRequest {\\nstring class_id = 1;\\n}\\n\/\/ QuerySupplyResponse is the response type for the Query\/Supply RPC method\\nmessage QuerySupplyResponse {\\nuint64 amount = 1;\\n}\\n\/\/ QueryNFTstRequest is the request type for the Query\/NFTs RPC method\\nmessage QueryNFTsRequest {\\nstring                                class_id   = 1;\\nstring                                owner      = 2;\\ncosmos.base.query.v1beta1.PageRequest pagination = 3;\\n}\\n\/\/ QueryNFTsResponse is the response type for the Query\/NFTs RPC methods\\nmessage QueryNFTsResponse {\\nrepeated cosmos.nft.v1beta1.NFT        nfts       = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n\/\/ QueryNFTRequest is the request type for the Query\/NFT RPC method\\nmessage QueryNFTRequest {\\nstring class_id = 1;\\nstring id       = 2;\\n}\\n\/\/ QueryNFTResponse is the response type for the Query\/NFT RPC method\\nmessage QueryNFTResponse {\\ncosmos.nft.v1beta1.NFT nft = 1;\\n}\\n\/\/ QueryClassRequest is the request type for the Query\/Class RPC method\\nmessage QueryClassRequest {\\nstring class_id = 1;\\n}\\n\/\/ QueryClassResponse is the response type for the Query\/Class RPC method\\nmessage QueryClassResponse {\\ncosmos.nft.v1beta1.Class class = 1;\\n}\\n\/\/ QueryClassesRequest is the request type for the Query\/Classes RPC method\\nmessage QueryClassesRequest {\\n\/\/ pagination defines an optional pagination for the request.\\ncosmos.base.query.v1beta1.PageRequest pagination = 1;\\n}\\n\/\/ QueryClassesResponse is the response type for the Query\/Classes RPC method\\nmessage QueryClassesResponse {\\nrepeated cosmos.nft.v1beta1.Class      classes    = 1;\\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\\n}\\n```\\n### Interoperability\\nInteroperability is all about reusing assets between modules and chains. The former one is achieved by ADR-33: Protobuf client - server communication. At the time of writing ADR-33 is not finalized. The latter is achieved by IBC. Here we will focus on the IBC side.\\nIBC is implemented per module. Here, we aligned that NFTs will be recorded and managed in the x\/nft. This requires creation of a new IBC standard and implementation of it.\\nFor IBC interoperability, NFT custom modules MUST use the NFT object type understood by the IBC client. So, for x\/nft interoperability, custom NFT implementations (example: x\/cryptokitty) should use the canonical x\/nft module and proxy all NFT balance keeping functionality to x\/nft or else re-implement all functionality using the NFT object type understood by the IBC client. In other words: x\/nft becomes the standard NFT registry for all Cosmos NFTs (example: x\/cryptokitty will register a kitty NFT in x\/nft and use x\/nft for book keeping). This was [discussed](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065#discussioncomment-873206) in the context of using x\/bank as a general asset balance book. Not using x\/nft will require implementing another module for IBC.\\n","tokens":307,"id":827,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context and Problem Statement\nThe [Settings Redesign project](https:\/\/jira.mozilla.com\/browse\/FXA-840) will be created [as a new React application](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) and in turn, has opened the door to assess certain pieces of our technology stack.\n[GraphQL](https:\/\/graphql.org\/), or GQL, is not a database query language, but is instead a query language for APIs. It _describes_ data requirements, and is a powerful alternative to REST. Some benefits can be gained by using it on top of existing REST architecture.\n\"Apollo\" in this document refers to the \"Apollo client\" and \"Apollo server\" pieces of the Apollo platform\u00b9, which can be described as a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\nThis ADR serves to lay out pros and cons of using GraphQL and Apollo in the Settings Redesign project as an alternative to hitting our conventional REST endpoints.\n\u00b9Apollo also offers Apollo Graph Manager and Apollo Federation which are paid services, [read more from their docs](https:\/\/www.apollographql.com\/docs\/intro\/platform\/). We do not need to use these to use GQL with Apollo server or Apollo client.\n## Decision Drivers\n- Performance implications; consideration around the number of network requests and data transferred\n- Ease of setup for client-side API calls\n- Clarity around expected data, React integration, and developer tooling\n- Development speed, both around initial setup and as new features roll out\n\n## Decision\n- Performance implications; consideration around the number of network requests and data transferred\n- Ease of setup for client-side API calls\n- Clarity around expected data, React integration, and developer tooling\n- Development speed, both around initial setup and as new features roll out\nChosen option: \"B - Layer GraphQL on top of our REST architecture\", because:\n- GQL offers performance optimizations by allowing us to consolidate our network requests by sending data requirements up in a single request, asking for only what is needed on the client, shifting the burden of figuring out how and where to gather this data from the client onto the server. The server then compensates for overfetching by only sending back what is requested.\n- Allowing developers to query for and expect only exactly what is needed with end-to-end typing and a more \"declarative\" way of thinking towards data requirements, along with keeping data requirement schemas close to where they're consumed, makes it painfully clear what's being sent and received between client and server.\n- Does not preclude option C, as we can replace or supplement direct calls to the FxA auth-server down the line. This is faster for initial development, and this option over option C will also help mitigate risk around using a relatively novel piece of technology for FxA.\n- Nice side-effects include the GQL playground, managing only a single API endpoint, and the ability to store both local state and network data in the Apollo cache.\n\n\n## Context and Problem Statement\nMany regulations require traffic to be encrypted over public Internet. Compliant Kubernetes solves this problem via an [Ingress controller](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/ingress-controllers\/) and [cert-manager](https:\/\/github.com\/jetstack\/cert-manager). As of February 2021, Compliant Kubernetes comes by default with [nginx-ingress](https:\/\/kubernetes.github.io\/ingress-nginx\/), but [Ambassador](https:\/\/www.getambassador.io\/docs\/latest\/topics\/running\/ingress-controller\/) is planned as an alternative. The question is, how does traffic arrive at the Ingress controller?\n## Decision Drivers\n* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\n* We want to keep things simple.\n\n## Decision\n* We want to obey the [Principle of Least Astonishment](https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_astonishment).\n* We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for [Kubernetes-controlled load balancer](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#loadbalancer).\n* Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.\n* We want to keep things simple.\nChosen options:\n1. Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker nodes with a manual or Terraform-controlled load-balancer. This includes:\n* Where load-balancing does not add value, e.g., if a deployment is planned to have only a single-node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead.\n* Exoscale currently falls in this category, due to its Kubernetes integration being rather recent.\n* SafeSpring falls in this category, since it is missing load balancers.\n* If the cloud provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing.\n2. Use Service Type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud.\nAdditional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until *after* we apply Compliant Kubernetes Apps. There is a risk for \"the Internet\" -- LetsEncrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.:\n```\n*.$BASE_DOMAIN     60s A 203.0.113.123\n*.ops.$BASE_DOMAIN 60s A 203.0.113.123\n```\n203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by [kops](https:\/\/github.com\/kubernetes\/kops\/blob\/d5d08a43034dd4c7242cf1faa020cf9a8c3965e2\/upup\/pkg\/fi\/cloudup\/dns.go#L41) and should not feel astonishing.\n### Positive Consequences\n* We make the best of each cloud provider.\n* Obeys principle of least astonishment.\n* We do not add a load balancer \"just because\".\n### Negative Consequences\n* Complexity is a bit increased, however, this feels like essential complexity.\n\n\n\nProvide a decision given the context below:\n## Context\nNFTs are more than just crypto art, which is very helpful for accruing value to the Cosmos ecosystem. As a result, Cosmos Hub should implement NFT functions and enable a unified mechanism for storing and sending the ownership representative of NFTs as discussed in https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065.\nAs discussed in [#9065](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065), several potential solutions can be considered:\n* irismod\/nft and modules\/incubator\/nft\n* CW721\n* DID NFTs\n* interNFT\nSince functions\/use cases of NFTs are tightly connected with their logic, it is almost impossible to support all the NFTs' use cases in one Cosmos SDK module by defining and implementing different transaction types.\nConsidering generic usage and compatibility of interchain protocols including IBC and Gravity Bridge, it is preferred to have a generic NFT module design which handles the generic NFTs logic.\nThis design idea can enable composability that application-specific functions should be managed by other modules on Cosmos Hub or on other Zones by importing the NFT module.\nThe current design is based on the work done by [IRISnet team](https:\/\/github.com\/irisnet\/irismod\/tree\/master\/modules\/nft) and an older implementation in the [Cosmos repository](https:\/\/github.com\/cosmos\/modules\/tree\/master\/incubator\/nft).\n<end_of_turn>\n<start_of_turn>model\nWe create a `x\/nft` module, which contains the following functionality:\n* Store NFTs and track their ownership.\n* Expose `Keeper` interface for composing modules to transfer, mint and burn NFTs.\n* Expose external `Message` interface for users to transfer ownership of their NFTs.\n* Query NFTs and their supply information.\nThe proposed module is a base module for NFT app logic. It's goal it to provide a common layer for storage, basic transfer functionality and IBC. The module should not be used as a standalone.\nInstead an app should create a specialized module to handle app specific logic (eg: NFT ID construction, royalty), user level minting and burning. Moreover an app specialized module should handle auxiliary data to support the app logic (eg indexes, ORM, business data).\nAll data carried over IBC must be part of the `NFT` or `Class` type described below. The app specific NFT data should be encoded in `NFT.data` for cross-chain integrity. Other objects related to NFT, which are not important for integrity can be part of the app specific module.\n### Types\nWe propose two main types:\n* `Class` -- describes NFT class. We can think about it as a smart contract address.\n* `NFT` -- object representing unique, non fungible asset. Each NFT is associated with a Class.\n#### Class\nNFT **Class** is comparable to an ERC-721 smart contract (provides description of a smart contract), under which a collection of NFTs can be created and managed.\n```protobuf\nmessage Class {\nstring id          = 1;\nstring name        = 2;\nstring symbol      = 3;\nstring description = 4;\nstring uri         = 5;\nstring uri_hash    = 6;\ngoogle.protobuf.Any data = 7;\n}\n```\n* `id` is used as the primary index for storing the class; _required_\n* `name` is a descriptive name of the NFT class; _optional_\n* `symbol` is the symbol usually shown on exchanges for the NFT class; _optional_\n* `description` is a detailed description of the NFT class; _optional_\n* `uri` is a URI for the class metadata stored off chain. It should be a JSON file that contains metadata about the NFT class and NFT data schema ([OpenSea example](https:\/\/docs.opensea.io\/docs\/contract-level-metadata)); _optional_\n* `uri_hash` is a hash of the document pointed by uri; _optional_\n* `data` is app specific metadata of the class; _optional_\n#### NFT\nWe define a general model for `NFT` as follows.\n```protobuf\nmessage NFT {\nstring class_id           = 1;\nstring id                 = 2;\nstring uri                = 3;\nstring uri_hash           = 4;\ngoogle.protobuf.Any data  = 10;\n}\n```\n* `class_id` is the identifier of the NFT class where the NFT belongs; _required_\n* `id` is an identifier of the NFT, unique within the scope of its class. It is specified by the creator of the NFT and may be expanded to use DID in the future. `class_id` combined with `id` uniquely identifies an NFT and is used as the primary index for storing the NFT; _required_\n```text\n{class_id}\/{id} --> NFT (bytes)\n```\n* `uri` is a URI for the NFT metadata stored off chain. Should point to a JSON file that contains metadata about this NFT (Ref: [ERC721 standard and OpenSea extension](https:\/\/docs.opensea.io\/docs\/metadata-standards)); _required_\n* `uri_hash` is a hash of the document pointed by uri; _optional_\n* `data` is an app specific data of the NFT. CAN be used by composing modules to specify additional properties of the NFT; _optional_\nThis ADR doesn't specify values that `data` can take; however, best practices recommend upper-level NFT modules clearly specify their contents.  Although the value of this field doesn't provide the additional context required to manage NFT records, which means that the field can technically be removed from the specification, the field's existence allows basic informational\/UI functionality.\n### `Keeper` Interface\n```go\ntype Keeper interface {\nNewClass(ctx sdk.Context,class Class)\nUpdateClass(ctx sdk.Context,class Class)\nMint(ctx sdk.Context,nft NFT\uff0creceiver sdk.AccAddress)   \/\/ updates totalSupply\nBatchMint(ctx sdk.Context, tokens []NFT,receiver sdk.AccAddress) error\nBurn(ctx sdk.Context, classId string, nftId string)    \/\/ updates totalSupply\nBatchBurn(ctx sdk.Context, classID string, nftIDs []string) error\nUpdate(ctx sdk.Context, nft NFT)\nBatchUpdate(ctx sdk.Context, tokens []NFT) error\nTransfer(ctx sdk.Context, classId string, nftId string, receiver sdk.AccAddress)\nBatchTransfer(ctx sdk.Context, classID string, nftIDs []string, receiver sdk.AccAddress) error\nGetClass(ctx sdk.Context, classId string) Class\nGetClasses(ctx sdk.Context) []Class\nGetNFT(ctx sdk.Context, classId string, nftId string) NFT\nGetNFTsOfClassByOwner(ctx sdk.Context, classId string, owner sdk.AccAddress) []NFT\nGetNFTsOfClass(ctx sdk.Context, classId string) []NFT\nGetOwner(ctx sdk.Context, classId string, nftId string) sdk.AccAddress\nGetBalance(ctx sdk.Context, classId string, owner sdk.AccAddress) uint64\nGetTotalSupply(ctx sdk.Context, classId string) uint64\n}\n```\nOther business logic implementations should be defined in composing modules that import `x\/nft` and use its `Keeper`.\n### `Msg` Service\n```protobuf\nservice Msg {\nrpc Send(MsgSend)         returns (MsgSendResponse);\n}\nmessage MsgSend {\nstring class_id = 1;\nstring id       = 2;\nstring sender   = 3;\nstring reveiver = 4;\n}\nmessage MsgSendResponse {}\n```\n`MsgSend` can be used to transfer the ownership of an NFT to another address.\nThe implementation outline of the server is as follows:\n```go\ntype msgServer struct{\nk Keeper\n}\nfunc (m msgServer) Send(ctx context.Context, msg *types.MsgSend) (*types.MsgSendResponse, error) {\n\/\/ check current ownership\nassertEqual(msg.Sender, m.k.GetOwner(msg.ClassId, msg.Id))\n\/\/ transfer ownership\nm.k.Transfer(msg.ClassId, msg.Id, msg.Receiver)\nreturn &types.MsgSendResponse{}, nil\n}\n```\nThe query service methods for the `x\/nft` module are:\n```protobuf\nservice Query {\n\/\/ Balance queries the number of NFTs of a given class owned by the owner, same as balanceOf in ERC721\nrpc Balance(QueryBalanceRequest) returns (QueryBalanceResponse) {\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/balance\/{owner}\/{class_id}\";\n}\n\/\/ Owner queries the owner of the NFT based on its class and id, same as ownerOf in ERC721\nrpc Owner(QueryOwnerRequest) returns (QueryOwnerResponse) {\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/owner\/{class_id}\/{id}\";\n}\n\/\/ Supply queries the number of NFTs from the given class, same as totalSupply of ERC721.\nrpc Supply(QuerySupplyRequest) returns (QuerySupplyResponse) {\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/supply\/{class_id}\";\n}\n\/\/ NFTs queries all NFTs of a given class or owner,choose at least one of the two, similar to tokenByIndex in ERC721Enumerable\nrpc NFTs(QueryNFTsRequest) returns (QueryNFTsResponse) {\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\";\n}\n\/\/ NFT queries an NFT based on its class and id.\nrpc NFT(QueryNFTRequest) returns (QueryNFTResponse) {\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/nfts\/{class_id}\/{id}\";\n}\n\/\/ Class queries an NFT class based on its id\nrpc Class(QueryClassRequest) returns (QueryClassResponse) {\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\/{class_id}\";\n}\n\/\/ Classes queries all NFT classes\nrpc Classes(QueryClassesRequest) returns (QueryClassesResponse) {\noption (google.api.http).get = \"\/cosmos\/nft\/v1beta1\/classes\";\n}\n}\n\/\/ QueryBalanceRequest is the request type for the Query\/Balance RPC method\nmessage QueryBalanceRequest {\nstring class_id = 1;\nstring owner    = 2;\n}\n\/\/ QueryBalanceResponse is the response type for the Query\/Balance RPC method\nmessage QueryBalanceResponse {\nuint64 amount = 1;\n}\n\/\/ QueryOwnerRequest is the request type for the Query\/Owner RPC method\nmessage QueryOwnerRequest {\nstring class_id = 1;\nstring id       = 2;\n}\n\/\/ QueryOwnerResponse is the response type for the Query\/Owner RPC method\nmessage QueryOwnerResponse {\nstring owner = 1;\n}\n\/\/ QuerySupplyRequest is the request type for the Query\/Supply RPC method\nmessage QuerySupplyRequest {\nstring class_id = 1;\n}\n\/\/ QuerySupplyResponse is the response type for the Query\/Supply RPC method\nmessage QuerySupplyResponse {\nuint64 amount = 1;\n}\n\/\/ QueryNFTstRequest is the request type for the Query\/NFTs RPC method\nmessage QueryNFTsRequest {\nstring                                class_id   = 1;\nstring                                owner      = 2;\ncosmos.base.query.v1beta1.PageRequest pagination = 3;\n}\n\/\/ QueryNFTsResponse is the response type for the Query\/NFTs RPC methods\nmessage QueryNFTsResponse {\nrepeated cosmos.nft.v1beta1.NFT        nfts       = 1;\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\n}\n\/\/ QueryNFTRequest is the request type for the Query\/NFT RPC method\nmessage QueryNFTRequest {\nstring class_id = 1;\nstring id       = 2;\n}\n\/\/ QueryNFTResponse is the response type for the Query\/NFT RPC method\nmessage QueryNFTResponse {\ncosmos.nft.v1beta1.NFT nft = 1;\n}\n\/\/ QueryClassRequest is the request type for the Query\/Class RPC method\nmessage QueryClassRequest {\nstring class_id = 1;\n}\n\/\/ QueryClassResponse is the response type for the Query\/Class RPC method\nmessage QueryClassResponse {\ncosmos.nft.v1beta1.Class class = 1;\n}\n\/\/ QueryClassesRequest is the request type for the Query\/Classes RPC method\nmessage QueryClassesRequest {\n\/\/ pagination defines an optional pagination for the request.\ncosmos.base.query.v1beta1.PageRequest pagination = 1;\n}\n\/\/ QueryClassesResponse is the response type for the Query\/Classes RPC method\nmessage QueryClassesResponse {\nrepeated cosmos.nft.v1beta1.Class      classes    = 1;\ncosmos.base.query.v1beta1.PageResponse pagination = 2;\n}\n```\n### Interoperability\nInteroperability is all about reusing assets between modules and chains. The former one is achieved by ADR-33: Protobuf client - server communication. At the time of writing ADR-33 is not finalized. The latter is achieved by IBC. Here we will focus on the IBC side.\nIBC is implemented per module. Here, we aligned that NFTs will be recorded and managed in the x\/nft. This requires creation of a new IBC standard and implementation of it.\nFor IBC interoperability, NFT custom modules MUST use the NFT object type understood by the IBC client. So, for x\/nft interoperability, custom NFT implementations (example: x\/cryptokitty) should use the canonical x\/nft module and proxy all NFT balance keeping functionality to x\/nft or else re-implement all functionality using the NFT object type understood by the IBC client. In other words: x\/nft becomes the standard NFT registry for all Cosmos NFTs (example: x\/cryptokitty will register a kitty NFT in x\/nft and use x\/nft for book keeping). This was [discussed](https:\/\/github.com\/cosmos\/cosmos-sdk\/discussions\/9065#discussioncomment-873206) in the context of using x\/bank as a general asset balance book. Not using x\/nft will require implementing another module for IBC.\n<end_of_turn>\n"}
{"File Name":"ReportMI-service-manual\/0018-submission-state-machine.md","Context":"## Context\\nAs outlined in [ADR-0016][adr-0016], the Data Submission Service will use\\n\"submissions\" to describe something that a supplier has submitted to us in order\\nto complete a \"task\".\\nSubmissions will exist in a state machine that outlines what is happening with\\nthem.\\nCurrently, we expect there to be 6 states:\\n* **pending** - a blank submission that is awaiting data\\n* **processing** - a submission where data is being processed\\n(eg a file is being ingested or data is being validated)\\n* **validation_failed** - the submitted data has failed the validation process\\nand needs to be corrected\\n* **validation_passed** - the submitted data has passed the validation process,\\nand the supplier now needs to review the results\\n* **supplier_rejected** - the supplier has reviewed the result of the data\\nprocessing and has rejected it (eg they have realised that their data needs to\\nbe amended)\\n* **supplier_accepted** - the supplier has reviewed the result of the data\\nprocessing and are happy that it is accurate\\nOther states may added in future to cover approval processes, and fixing\\nmistakes in returns.\\n","Decision":"The system will model the 6 states highlighted above.\\nSubmissions containing data will proceed through the states from `pending` to\\n`processing` to either `validation_failed` or `validation_passed`. If the\\nvalidation has passed, the supplier can reject the submission (move to\\n`supplier_rejected`) or accept the submission (move to `supplier_accepted`).\\nA 'no business' submission will proceed straight to `supplier_accepted` once\\nthe supplier has confirmed they wish to make this submission.\\nThis is shown in the following diagram.\\n![Submission state machine diagram](..\/diagrams\/0018-submission-states.jpg)\\n","tokens":250,"id":2039,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Problem Statement\n### Why Distributed Tracing?\n> Observability is defined as the ability of the internal states of a system to\nbe determined by its external outputs.\nThere are several established techniques for gaining observability into complex\nsoftware systems.\n* Logging\n* helpful for known knowns\n* e.g. \"I want to know when a specific condition is reached\"\n* Metrics\n* helpful for assessing known unknowns\n* e.g. \"How many requests per second was the system handling last Tuesday?\"\n* Distributed Tracing\n* helpful for learning about unknown unknowns\n* e.g. \"What was the execution context for User X that caused their\ninteraction to timeout last Tuesday?\"\nSome of the benefits of distributed tracing, as outlined in\n[this](https:\/\/petabridge.com\/blog\/why-use-distributed-tracing\/) article are:\n* radically improves developer productivity and output\n* works across multiple applications, programming languages, and transports\n* improve time to market\n* facilitates excellent cross-team communication and cooperation\nHere are several example scenarios or questions that distributed tracing can\nhelp answer.\n* As a new engineer on the team, I want to understand how many separate systems\nare involved when a certain user type logs in and the first page is rendered.\n* As an operations engineer, I want to know how many SQL queries are executed\nfor a given endpoint or interaction.\n* As a product manager, I want to know if a new feature is being used by a\ncertain cohort of users on a regular basis.\n* As an engineer, I want to prove that an optimization I wrote is effective\nin a production environment.\n* As a load tester, after I have shown that a problem exists, I want to\nunderstand how the system is interacting so I can debug and fix the issue.\n### ADR Goals and Anti-goals\n* Goal: Choose which set of libraries to use at callsites (across programming\nlanguages) within the MilMove codebase, which will be used to generate\ndistributed tracing data\n* Anti-goal: Committing to a specific \"backend\", i.e. platform or service for\ngathering, exploring, and displaying trace information\n* Sub-goal: Leaving open as many options as possible for the backend\n\n## Decision\n* Chosen Alternative: _Use OpenTelemetry_\n* OpenTelemetry is an emerging industry standard\n* vendors find benefit of being in the OpenTelemetry ecosystem because they\nno longer have to create or support instrumentation libraries in an ever\ngrowing array of languages, i.e. as soon as language library exists for\nOpenTelemetry, the vendors automatically become available to support that\ngiven language.\n* OpenTelemetry is vendor agnostic\n* tracing information can be sent to hosted services (e.g. Honeycomb.io, AWS\nX-Ray, etc) or self-hosted Open Source implementations (e.g. Zipkin, Jaeger,\netc)\n* if left unconfigured, OpenTelemetry instrumentation calls default to\nlightweight\/noop executions\n* OpenTelemetry has well-maintained libraries for the languages used in the\nlayers of the MilMove project\n* i.e. Go (back-end); JavaScript (front-end); Python (load testing); etc\n* Easily swappable back-ends\n* e.g. could choose a local Docker version of OpenZipkin for an all-local\ndevelopment environment\n* e.g. can use Honeycomb.io in the experimental commercial-cloud hosted\nenvironment\n* e.g. can swap in AWS X-Ray for use in GovCloud hosted environments\n* Cons\n* as an abstraction layer, OpenTelemetry may prohibit usage of vendor-\nspecific capabilities\n* some OpenTelemetry libraries and tools may trail their vendor-supported\ncounterparts\n* instrumentation for tracing may be a vector for performance overhead\n\n\n# Context\nThe way the Government Digital Service (GDS) [makes technology choices is\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which technology will to use to provide\npersistence for the Performance Platform.\nGDS has experience in running MongoDB and MySQL in production.\nWe envisage the Performance Platform as taking in unstructured data from a\nvariety of data sources (spreadsheets, analytics, logs, other databases and\napplications) and allowing people to collect this data in a single place. This\nshould enable service managers to:\n- make comparisons\n- see how well their service is performing\n- see how the performance changes over time, as they iterate the service\nSo we want a persistent data store that will store unstructured data, and\nallow us to apply a structure either by post-processing the data, or at query\ntime.\nThe volume of the data that we are envisaging at this stage is pretty small.\nWe will be building a small thing to start; as we learn more about the\nuser needs and problem space, then we will revisit this decision. Since the\nvolume is small, it does not seem likely that we need Hadoop \/ HDFS or\nCassandra.\nWe are not the canonical source of this data. We are an aggregator; the\ncanonical source remains the data sources which will be providing feeds or\npushing the data into the Performance Platform.\nBecause of this position, we do not need ACID properties for this data, nor\nneed worry about the CAP theorem in any detail.\n# Decision\nWe will use MongoDB. We are comfortable operating it in production,\nit will allow unstructured data (in the form of JSON documents) and we can\napply structure at query time.\n# Status\nAccepted.\n# Consequences\nUse MongoDB with an appropriate replica-set configuration.\n\n## Decision\nWe will use MongoDB. We are comfortable operating it in production,\nit will allow unstructured data (in the form of JSON documents) and we can\napply structure at query time.\n# Status\nAccepted.\n# Consequences\nUse MongoDB with an appropriate replica-set configuration.\n\n\n\nProvide a decision given the context below:\n## Context\nAs outlined in [ADR-0016][adr-0016], the Data Submission Service will use\n\"submissions\" to describe something that a supplier has submitted to us in order\nto complete a \"task\".\nSubmissions will exist in a state machine that outlines what is happening with\nthem.\nCurrently, we expect there to be 6 states:\n* **pending** - a blank submission that is awaiting data\n* **processing** - a submission where data is being processed\n(eg a file is being ingested or data is being validated)\n* **validation_failed** - the submitted data has failed the validation process\nand needs to be corrected\n* **validation_passed** - the submitted data has passed the validation process,\nand the supplier now needs to review the results\n* **supplier_rejected** - the supplier has reviewed the result of the data\nprocessing and has rejected it (eg they have realised that their data needs to\nbe amended)\n* **supplier_accepted** - the supplier has reviewed the result of the data\nprocessing and are happy that it is accurate\nOther states may added in future to cover approval processes, and fixing\nmistakes in returns.\n<end_of_turn>\n<start_of_turn>model\nThe system will model the 6 states highlighted above.\nSubmissions containing data will proceed through the states from `pending` to\n`processing` to either `validation_failed` or `validation_passed`. If the\nvalidation has passed, the supplier can reject the submission (move to\n`supplier_rejected`) or accept the submission (move to `supplier_accepted`).\nA 'no business' submission will proceed straight to `supplier_accepted` once\nthe supplier has confirmed they wish to make this submission.\nThis is shown in the following diagram.\n![Submission state machine diagram](..\/diagrams\/0018-submission-states.jpg)\n<end_of_turn>\n"}
{"File Name":"operational-data-hub\/0005-build-open-source-solutions.md","Context":"## Context\\nOpen source software is software with source code that anyone can inspect, modify, and enhance.\\nBy design, open source software licenses promote collaboration and sharing because they permit other people to make modifications to source code and incorporate those changes into their own projects. They encourage computer programmers to access, view, and modify open source software whenever they like, as long as they let others do the same when they share their work.\\nOpen source software development furthers several principles, described below.\\nTransparency. Whether we're developing software or solving a business problem, we all have access to the information and materials necessary for doing our best work. And when these materials are accessible, we can build upon each other's ideas and discoveries. We can make more effective decisions and understand how decisions affect us.\\nCollaboration. When we're free to participate, we can enhance each other's work in unanticipated ways. When we can modify what others have shared, we unlock new possibilities. By initiating new projects together, we can solve problems that no one can solve alone. And when we implement open standards, we enable others to contribute in the future.\\nRelease early and often. Rapid prototypes can lead to rapid discoveries. An iterative approach leads to better solutions faster. When you're free to experiment, you can look at problems in new ways and seek answers in new places. You can learn by doing.\\nInclusive meritocracy. Good ideas can come from anywhere, and the best ideas should win. Only by including diverse perspectives in our conversations can we be certain we've identified the best ideas, and decision-makers continually seek those perspectives. We may not operate by consensus, but successful work determines which projects gather support and effort from the community.\\nCommunity. Communities form when different people unite around a common purpose. Shared values guide decision making, and community goals supersede individual interests and agendas.\\n","Decision":"We make our code available as open source.\\n","tokens":382,"id":2759,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nLike so many other accessibility tools, such as the [Accessibility Developer Tools by Google](https:\/\/github.com\/GoogleChrome\/accessibility-developer-tools), [aXe by Deque](https:\/\/github.com\/dequelabs\/axe-core), and [HTML_CodeSniffer by Squiz](https:\/\/github.com\/squizlabs\/HTML_CodeSniffer) to name a few, our proprietary accessibility conformance testing engine at Siteimprove runs within the context of a browser. The reason why this seems to be the de facto way of implementing an accessibility tool is obvious: The browser is the tool used to consume your website, so why not test directly within that very tool? Through the APIs exposed by the browser, we get access to all the information needed in order to assess the accessibility of a website; the structure we can access and inspect through the DOM, information about styling can be gained through the CSSOM, and soon we also get our hands on a standardised accessibility tree through the [AOM](https:\/\/wicg.github.io\/aom\/).\nHowever, not all is good in the land of browsers. Rendering a website is an inherently non-deterministic process and the timing of network requests, script execution, the content of request headers, and much more, all play a role in what the final result will look like. In most cases, this will directly affect the assessment of a tool that runs within the browser and will become very apparent at scale. At Siteimprove, we feel the effect of this on a daily basis; a customer asking us why we came up with a certain result and us having little to no clue because we cannot replicate the exact circumstances that led to that result. This is a frustrating experience for both our customers and ourselves as it makes it difficult to reason about our tool.\nWe want to fix this and we want to fix it for good. To do so, we must ensure that we have the ability to exactly replicate the results of a given accessibility assessment. Ideally, as many unknown browser variables as possible should be taken out of the equation and the browser only be used for what is absolutely necessary.\n\n## Decision\nWe will abandon any sort of dynamic analysis within the context of a browser. The input to Alfa will be static data and any assessment must be made based on that data alone. A browser may or may not be involved in the construction of the data, but the browser will not be required for any further assessment thereof.\nIf additional data is needed by a given accessibility rule, we will adjust the data format to meet the needs of the rule. We will also carefully consider the extent of the data format as to not bloat it with information that could otherwise be inferred from existing data. Ideally, the size of the data when serialised and stored on disk will not be much larger than the size of the original source code on which the data is based.\n\n\n## Context\nWe need to design the building blocks of the Digital Citizenship platform. We will make decisions about the overall architecture and the building blocks we're going to use to satisfy the functional and non-functional requirements.\nWhen deciding what technologies to use and how to satisfy those requirements, we're going to evaluate whether we want to build certain components from scratch, use open-source solutions managed by us or rely on existing public cloud components, managed by 3rd party providers.\nAs the world of cloud technologies continues to accelerate, we should absorb new developments into how we work. Leading technology organisations are rapidly embracing new tools like \"serverless\" computing.\nAt the infrastructure and application level we should expect our applications to be resilient, flexible and API-driven. We should have the tools and practices in place to manage and secure a distributed range of tools accessed over the internet.\nDecision criteria will also include:\n* elasticity and resilience\n* pay-as-you-go pricing\n* exploiting economies of scale\n* exploiting falling costs of infrastructure over time\n* quality of management tools\n* best of breed security\n* flexibility and opportunity costs\n\n## Decision\n* elasticity and resilience\n* pay-as-you-go pricing\n* exploiting economies of scale\n* exploiting falling costs of infrastructure over time\n* quality of management tools\n* best of breed security\n* flexibility and opportunity costs\nWe decide to build the components of the Digital Citizenship initiative on top of scalable, managed and cost-effective components provided by the leading public cloud providers.\n\n\n\nProvide a decision given the context below:\n## Context\nOpen source software is software with source code that anyone can inspect, modify, and enhance.\nBy design, open source software licenses promote collaboration and sharing because they permit other people to make modifications to source code and incorporate those changes into their own projects. They encourage computer programmers to access, view, and modify open source software whenever they like, as long as they let others do the same when they share their work.\nOpen source software development furthers several principles, described below.\nTransparency. Whether we're developing software or solving a business problem, we all have access to the information and materials necessary for doing our best work. And when these materials are accessible, we can build upon each other's ideas and discoveries. We can make more effective decisions and understand how decisions affect us.\nCollaboration. When we're free to participate, we can enhance each other's work in unanticipated ways. When we can modify what others have shared, we unlock new possibilities. By initiating new projects together, we can solve problems that no one can solve alone. And when we implement open standards, we enable others to contribute in the future.\nRelease early and often. Rapid prototypes can lead to rapid discoveries. An iterative approach leads to better solutions faster. When you're free to experiment, you can look at problems in new ways and seek answers in new places. You can learn by doing.\nInclusive meritocracy. Good ideas can come from anywhere, and the best ideas should win. Only by including diverse perspectives in our conversations can we be certain we've identified the best ideas, and decision-makers continually seek those perspectives. We may not operate by consensus, but successful work determines which projects gather support and effort from the community.\nCommunity. Communities form when different people unite around a common purpose. Shared values guide decision making, and community goals supersede individual interests and agendas.\n<end_of_turn>\n<start_of_turn>model\nWe make our code available as open source.\n<end_of_turn>\n"}
{"File Name":"TANF-app\/008-deployment-flow.md","Context":"## Context\\nOur Cloud.gov organization currently has three Spaces -- `tanf-dev`, `tanf-staging`, and `tanf-prod`. The vendor team currently has access to the tanf-dev space only.\\nSince the recent changes to our [Git workflow](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/009-git-workflow.md) we believe our current deploy strategy should be updated to more closely match the workflow. Previously, since we had approvals on two different repositories we decided that it made sense to maintain [two separate staging sites](https:\/\/github.com\/HHS\/TANF-app\/blob\/837574415af7c57e182684a75bbcf4d942d3b62a\/docs\/Architecture%20Decision%20Record\/008-deployment-flow.md). We would deploy to one with approval in the raft-tech repository, and another with approval to HHS. Since we now have all approvals made in raft-tech, the deploy after approval serves the same purpose as deploying to the Government staging site would have.\\nAdditionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\n","Decision":"Additionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\\nDeploy Environment | Cloud.gov Space | Cloud.gov Dev Access | Role                                             | Deploys when ...                                  |\\n-------------------|-----------------|----------------------|--------------------------------------------------|---------------------------------------------------|\\nDev                | Tanf-Dev        | Vendor & Gov      | Deploy code submitted for gov review                | Relevant github label assigned as shown below     |\\nDevelop            | Tanf-Staging    | Vendor & Gov      | Deploy code once gov-approved                       | Code merged to `raft-tech\/TANF-app:develop` |\\nStaging            | Tanf-Staging    | Gov               | Deploy code once gov-approved                       | Code merged to `HHS\/TANF-app:main` |\\nProduction         | Tanf-Prod       | Gov               | Deploy code tested in staging & ready for prod      | Code merged to `HHS\/TANF-app:master`                |\\n### Gitflow and Deployments\\nWe will be following the Gitflow process which is an industry standard. You can read more about it [in our ADR](.\/018-versioning-and-releases.md). I will just highlight the parts relevant for our deployment strategy. Release branches will be merged to `HHS\/TANF-app:master` which will deploy to our production sites. Code merged to `raft-tech\/TANF-app:develop` will be deployed to our staging sites.\\n### Dev deployments\\nWithin the dev space, there is no correlation for branch to environment as these feature or bugfix branches will constantly vary:\\n| Dev Site | Frontend URL | Backend URL | Purpose                                          |\\n| -------- | -------- | -------- |--------------------------------------------------|\\n| A11y | https:\/\/tdp-frontend-a11y.app.cloud.gov | https:\/\/tdp-backend-a11y.app.cloud.gov\/admin\/ | Space for accessibility testing                  |\\n| QASP | https:\/\/tdp-frontend-qasp.app.cloud.gov | https:\/\/tdp-backend-qasp.app.cloud.gov\/admin\/ | Space for QASP review                            |\\n| raft | https:\/\/tdp-frontend-raft.app.cloud.gov | https:\/\/tdp-backend-raft.app.cloud.gov\/admin\/ | Space for Raft review                          |\\n","tokens":396,"id":3789,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context and Problem Statement\nThe [Settings Redesign project](https:\/\/github.com\/mozilla\/fxa\/issues\/3740) provides us with an opportunity to review how FxA approaches and employs CSS, both while building out new components for this project and for FxA going forward.\nHistorically, the Firefox Accounts codebase has not adhered to a formal CSS structure. This ADR serves to determine how we'll approach our CSS architecture in the Settings Redesign project, evaluating libraries and frameworks to determine which if any will be the best option for the FxA ecosystem. It is part 2 of two [Settings Redesign CSS ADRs](https:\/\/github.com\/mozilla\/fxa\/issues\/5087); part 1, detailing how we'll approach build conventions and variables, [can be found here](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0015-use-css-variables-and-scss.md).\nConsiderations around class naming conventions, color and measurement standards, interoperability across shared components, and custom configuration options offered by each library to meet Settings Redesign design standards are taken into account. Notably, the new design uses space measurements in increments of 8px and [colors](https:\/\/protocol.mozilla.org\/fundamentals\/color.html) are based in Mozilla Protocol's design system, where a hue's brightness scales in increments of 10.\n## Decision Drivers\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\n\n## Decision\n- **Reusability** - does the approach yield DRY, lean code that can be reused and repurposed?\n- **Longevity** - will the approach be supported in upcoming years and will it provide a stable platform for years of revolving HTML through added features and bug fixes?\n- **Developer experience** - are some team members already familiar with the approach, making the transition easier than an unfamiliar one?\n- **Ease of use** - will the choice result in a large learning curve or be easy for engineers to pick up?\nChosen options: \"Option B\" with Tailwind CSS for majority styling, and implementation details from \"Option D\" when utility classes don't meet the entire need, because:\n- Of the options set forth, a utility library provides us with the most flexible yet durable set of tools.\n- Single-purpose classes are performant and reduce the possibility of overly-complex or convoluted stylesheets.\n- A utility library is leaner and less opinionated compared to a set of UI components and other options, allowing greater flexibility and reusability across various projects.\n- Our team has prior experience with Tailwind in particular and newcomers should ramp up quickly with a utility pattern.\n- Tailwind is highly configurable without being cumbersome, allowing us to modify type and spacing scales, define color ranges, and set up media queries to meet our exact needs.\n- For cases when we do need to write custom SCSS we will structure our React components to initially rely on utility classes, but allow additional custom styles to be written in an adjacent SCSS file when needed. This is also applicable to components in `fxa-components` where the component can accept a `classes` prop with a list of needed utility classes, and any additional styling can be done in an external SCSS file located where the component was composed as needed (e.g., outside of `fxa-components`). CSS variables can be shared across the Tailwind configuration and in custom SCSS.\n- Note: class name conventions for the custom SCSS will be declared when the library configuration is setup, as we'd like them to make sense together. Examples will be provided in the `fxa-settings` README at this time as well.\n\n\n## Context\nThe value of tidy and [semi-linear commit history](https:\/\/fangpenlin.com\/images\/2013-09-30-keep-a-readable-git-history\/source_tree_new_branch_rebase_merge.png) is often overlooked in many Git-based\nprojects. This is unfortunate since non-linear git commit history might be a [horrible mess](https:\/\/tugberkugurlu.blob.core.windows.net\/bloggyimages\/d773c1fe-4db8-4d2f-a994-c60f3f8cb6f0.png) that\ndoes not provide any useful information. We want to use as simple as possible git workflow that promotes and ensures a semi-linear history.\n> * **Semi-linear** commit history usually refers to a history that uses merge commits (git \"no-fast-forward\" merge option) to clearly denote which commits are meant to be together and represent a\n>   coherent whole.\n> * **Linear** commit history usually refers to completely flat history (git default \"fast-forward\") where it is impossible to tell at first glance which commits belong together.\nWhen working on individual features, related git commits can be organized either as \"work log\" or as a \"recipe\". When working in a team, it is crucial that team members and\/or reviewers can easily\ncomprehend what is going on in a particular feature. For this reason, we prefer features to be organized as \"recipes\".\n> * **Work log** style of organizing feature commits refers to the style without any organization. Commits are added solely as they are developed through time.\n> * **Recipe** style of organizing feature commits refers to the style where commits have a sensible organization where peer developers can clearly see and learn how the feature is created. This\n>   style requires some additional work as its primary goal is communication, instead of just implementing a feature.\nVery often, in bigger teams, common git workflows have a problem of broken continuous integration builds. We want to embrace and use as simple as possible workflow that resolves that problem.\nChosen git workflow should seamlessly support release versioning and, if needed, related work on release branches.\n### Architectural Context\n* System (`klokwrk-project`)\n\n## Decision\n**We will use a [stable mainline branching model for Git](https:\/\/www.bitsnbites.eu\/a-stable-mainline-branching-model-for-git\/).** It\n[supports semi-linear Git history](https:\/\/www.bitsnbites.eu\/a-tidy-linear-git-history\/) and helps to resolve the problem of broken continuous integration builds.\n**We will, however, introduce several tweaks to the \"stable mainline branching model\":**\n* We will use the following naming pattern for feature branches: **`feature_<name>`** instead of `feature\/name`.\n* We will not use new branches when remote feature branches need rebasing. Instead, we will just inform all collaborators that rebasing is pending for a feature branch. It is important\nto communicate with collaborators **before** rebasing and force-pushing.\n* We will not create a release branch for each release. In general, we will just tag a release. However, we will create a release branch when a particular release needs fixing.\nWe will organize our [feature commits as recipes](https:\/\/www.bitsnbites.eu\/git-history-work-log-vs-recipe\/) because we want to promote the team's learning and communication.\nWe will also use specific commit message format as described in [ADR-0014 - Commit Message Format](.\/0014-commit-message-format.md)\n\n\n\nProvide a decision given the context below:\n## Context\nOur Cloud.gov organization currently has three Spaces -- `tanf-dev`, `tanf-staging`, and `tanf-prod`. The vendor team currently has access to the tanf-dev space only.\nSince the recent changes to our [Git workflow](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/009-git-workflow.md) we believe our current deploy strategy should be updated to more closely match the workflow. Previously, since we had approvals on two different repositories we decided that it made sense to maintain [two separate staging sites](https:\/\/github.com\/HHS\/TANF-app\/blob\/837574415af7c57e182684a75bbcf4d942d3b62a\/docs\/Architecture%20Decision%20Record\/008-deployment-flow.md). We would deploy to one with approval in the raft-tech repository, and another with approval to HHS. Since we now have all approvals made in raft-tech, the deploy after approval serves the same purpose as deploying to the Government staging site would have.\nAdditionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\n<end_of_turn>\n<start_of_turn>model\nAdditionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.\nAs of Spring 2022, following [ADR 018](https:\/\/github.com\/HHS\/TANF-app\/blob\/main\/docs\/Technical-Documentation\/Architecture-Decision-Record\/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.\nDeploy Environment | Cloud.gov Space | Cloud.gov Dev Access | Role                                             | Deploys when ...                                  |\n-------------------|-----------------|----------------------|--------------------------------------------------|---------------------------------------------------|\nDev                | Tanf-Dev        | Vendor & Gov      | Deploy code submitted for gov review                | Relevant github label assigned as shown below     |\nDevelop            | Tanf-Staging    | Vendor & Gov      | Deploy code once gov-approved                       | Code merged to `raft-tech\/TANF-app:develop` |\nStaging            | Tanf-Staging    | Gov               | Deploy code once gov-approved                       | Code merged to `HHS\/TANF-app:main` |\nProduction         | Tanf-Prod       | Gov               | Deploy code tested in staging & ready for prod      | Code merged to `HHS\/TANF-app:master`                |\n### Gitflow and Deployments\nWe will be following the Gitflow process which is an industry standard. You can read more about it [in our ADR](.\/018-versioning-and-releases.md). I will just highlight the parts relevant for our deployment strategy. Release branches will be merged to `HHS\/TANF-app:master` which will deploy to our production sites. Code merged to `raft-tech\/TANF-app:develop` will be deployed to our staging sites.\n### Dev deployments\nWithin the dev space, there is no correlation for branch to environment as these feature or bugfix branches will constantly vary:\n| Dev Site | Frontend URL | Backend URL | Purpose                                          |\n| -------- | -------- | -------- |--------------------------------------------------|\n| A11y | https:\/\/tdp-frontend-a11y.app.cloud.gov | https:\/\/tdp-backend-a11y.app.cloud.gov\/admin\/ | Space for accessibility testing                  |\n| QASP | https:\/\/tdp-frontend-qasp.app.cloud.gov | https:\/\/tdp-backend-qasp.app.cloud.gov\/admin\/ | Space for QASP review                            |\n| raft | https:\/\/tdp-frontend-raft.app.cloud.gov | https:\/\/tdp-backend-raft.app.cloud.gov\/admin\/ | Space for Raft review                          |\n<end_of_turn>\n"}
{"File Name":"re-build-systems\/0002-use-containers-and-multiple-boxes.md","Context":"## Context\\nThe aim of this project is to allow teams to quickly create their own Jenkins\\nplatform, running on AWS.\\nJenkins has a master\/agent architecture, where a single master node can trigger\\njobs running on multiple agents. This decision concerns how we deploy, configure\\nand manage both the master and the agents.\\n","Decision":"We will make both the master and the agents Docker containers, running on\\nseparate managed EC2 instances. We will provision and manage these directly for\\nnow, without using an orchestration service like ECS or Kubernetes.\\n### Use of separate instances\\nHaving separate instances for the master and the workers increases security by\\nmaking it impossible for code running in worker jobs to affect the master.\\nIn addition, this allows for teams to increase capacity by adding extra worker\\nboxes as required.\\n### Use of Docker for master\\nRunning the master in Docker makes it easy to deploy and upgrade. This improves\\nteams' abilities to quickly respond when new Jenkins versions are released in\\nresponse to security vulnerabilities, for example.\\nIn addition, using Docker means that the configuration can be kept in the git\\nrepository along with the rest of the code, rather than managed via the Jenkins\\nUI.\\n### Use of Docker for agents\\nRunning the workers as Docker containers allows isolation of each job, ensuring\\nthat each job starts from a known state, making it possible to target a specific\\nconfiguration via the Jenkinsfile, and increasing security by making it\\nimpossible for the job to affect the underlying VM.\\n### Not using orchestration frameworks\\nAlthough some teams at GDS are experimenting with ECS, the Jenkins service is\\nsimple enough that it is not worth introducing the added complexity for this\\nproject.\\n","tokens":68,"id":319,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nOur user facing applications will need to authenticate users.\nIn the long-term, we want to use the standard user sign-on services in CCS for\neach appropriate user group.\nFor suppliers, this is the [Supplier Registration Service (SRS)][service-srs].\nFor CCS staff it is their G-Suite account.\nUsing a single authentication service will reduce administration overheads\nbecause we won't have to manage a JML (Joiners Movers Leavers) process. For\nusers it will also mean they don't have to remember yet another username and\npassword combination.\nHowever, we want to get moving quickly, and integration with SRS\/CCS G-Suite\nwill take time. For now, we want a simple sign-in service that we can integrate\nwith quickly, to enable us to test and iterate the application.\n\n## Decision\nWe will use [Auth0][service-auth0] for authentication on a temporary basis.\nAuth0 is an authentication service that uses the OAuth protocol. It provides\nand simple integration path, and is free for several thousand active users.\nWe acknowledge that we are incurring technical debt by not integrating\nimmediately with SRS and G-Suite, however we believe that this will let us move\nmore quickly during development.\nThis decision will also allow us to gain a better understanding of the user\nneeds around authentication before a decision around the final authentication\nsolution is made.\nWe will replace Auth0 with the proper authentication service later in the\ndevelopment process.\n\n\n## Context\nThe key proposition of Cloud Platform is to do the \"hosting\" of services, and we choose [Kubernetes for container management](004-use-kubernetes-for-container-management.md).\nIn agreeing a good interface for service teams, there several concerns:\n* Definitions - teams should be able to specify the workloads and infrastructure they want running.\n* Control - teams should be able to use a default hosting configuration, getting things running as simply as with a PaaS. However teams should also have full control over their Kubernetes resources, including pod configuration, lifecycle, network connectivity, etc.\n* Multi-tenancy - Service teams' workloads need isolation between their dev and prod environments, and from other service teams' workloads.\n\n## Decision\n1. Teams are offered 'namespaces'. A namespace is the concept of an isolated environment for workloads\/resources.\n2. A CP namespace is implemented as a Kubernetes namespace and AWS resources (e.g. RDS instance, S3 bucket).\n3. Isolation in Kubernetes namespaces is implemented using RBAC and NetworkPolicy:\n* RBAC - teams can only administer k8s resources in their own namespaces\n* NetworkPolicy - containers can only receive traffic from its ingresses and other containers in the same namespace (implemented with a NetworkPolicy, which teams can edit if needed)\n4. Isolation between AWS resources is achieved using access control.\nEach ECR repo, or S3 bucket, RDS bucket is made accessible to an IAM User, and the team are provided access key credentials for it.\n5. A user defines a namespace in files: YAML (Kubernetes) and Terraform (AWS resources).\nThe YAML includes by default: a Namespace and various default limits on resources, pods and networking.\nFor deploying a simple workload, teams can include a YAML Deployment etc, so that these get applied automatically by CP's pipeline. Alternatively teams get more control by managing app resources using their namespace credentials - see below.\nThe Terraform can specify any AWS resources like S3 buckets, RDS databases, Elasticache. Typically teams specify an ECR repo, so they have somewhere to deploy their images to.\n6. The namespace definition is held in GitHub.\nGitHub provides a mechanism for peer-review, automated checks and versioning.\nOther options considered for configuring a namespace do not come with these advantages, for example:\n* a console \/ web form, implemented as a custom web app (click ops)\n* commands via a CLI or API\nNamespace definitions are stored in the [environments repo](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments)\n7. Namespace changes are checked by both a bot and a human from the CP team\nIn Kubernetes, cluster-wide privileges are required to apply changes to a Kubernetes Namespace, as well as associated resources: LimitRange, NetworkPolicy and ServiceAccount. These privileges mean that the blast radius is large when applying changes.\nIn terms of AWS resources, for common ones like S3 and RDS we provide terraform modules - to abstract away detail and promote best practice (for example, setting default encryption for S3 buckets). However Terraform can specify a huge range of AWS resources, each with multitude options. There are likely ways that one team can disrupt or get access to other teams' AWS services, that we can't anticipate, which is a risk to manage.\nTo mitigate these concerns:\n* [automated checks](https:\/\/github.com\/ministryofjustice\/cloud-platform-environments\/tree\/main\/.github\/workflows) are used to validate against common problems\n* Human review (by an engineer on the CP team) is also required on PRs, to check against unanticipated problems\n8. Pipeline to deploy namespace automatically.\nThe \"deploy pipeline\" is a CI\/CD pipeline that applies teams' namespace definitions in the clusters and AWS account. It triggers when the reviewed PR is merged to master.\n9. Teams have full control within their Kubernetes namespace\nUsers are given access to Kubernetes user credentials (kubecfg) with admin rights to their namespace. This gives them full control over their pods etc. They can deploy with 'kubectl apply' or Helm. They can debug problems with pod starting up, see logs etc.\nUsers are also invited to create a ServiceAccount (using their environment YAML), and provide the creds to their CI\/CD, for deploying their app.\n\n\n\nProvide a decision given the context below:\n## Context\nThe aim of this project is to allow teams to quickly create their own Jenkins\nplatform, running on AWS.\nJenkins has a master\/agent architecture, where a single master node can trigger\njobs running on multiple agents. This decision concerns how we deploy, configure\nand manage both the master and the agents.\n<end_of_turn>\n<start_of_turn>model\nWe will make both the master and the agents Docker containers, running on\nseparate managed EC2 instances. We will provision and manage these directly for\nnow, without using an orchestration service like ECS or Kubernetes.\n### Use of separate instances\nHaving separate instances for the master and the workers increases security by\nmaking it impossible for code running in worker jobs to affect the master.\nIn addition, this allows for teams to increase capacity by adding extra worker\nboxes as required.\n### Use of Docker for master\nRunning the master in Docker makes it easy to deploy and upgrade. This improves\nteams' abilities to quickly respond when new Jenkins versions are released in\nresponse to security vulnerabilities, for example.\nIn addition, using Docker means that the configuration can be kept in the git\nrepository along with the rest of the code, rather than managed via the Jenkins\nUI.\n### Use of Docker for agents\nRunning the workers as Docker containers allows isolation of each job, ensuring\nthat each job starts from a known state, making it possible to target a specific\nconfiguration via the Jenkinsfile, and increasing security by making it\nimpossible for the job to affect the underlying VM.\n### Not using orchestration frameworks\nAlthough some teams at GDS are experimenting with ECS, the Jenkins service is\nsimple enough that it is not worth introducing the added complexity for this\nproject.\n<end_of_turn>\n"}
{"File Name":"jfluentvalidation\/0001-primitive-array-constraints.md","Context":"## Context and Problem Statement\\nMy first pass at building out array constraint was to use a generic parameter `A` with `java.lang.reflect.Array` to obtain\\nthe length of the property representing `A`.\\nI was curious what the cost of using `java.lang.reflect.Array` compared to grabbing the `length` property from a known type was.\\nAnything with a name like `reflect*` gives me nightmares about terrible performance.\\nI decided to write a JMH benchmark to determine the performance impact `java.lang.reflect.Array` to assist in determining\\nwhich implementation to use.\\n## Decision Drivers\\n1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\n","Decision":"1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\\nI decided to choose option 1 as I prioritized performance above the overhead to maintain additional classes and having\\nduplicate logic.\\nWhile it might be premature optimization and such a small impact (1 - 2 ns) the benchmark results below still convinced me.\\nI'm sure someone can convince me that the overhead is insignificant or that I simply messed up the bencharmark at which point\\nit should be easier refactor to option 2.\\nI've included a rough [implementation of option 2](#option-2-implementation) just in case.\\n```java\\npackage jfluentvalidation.constraints.array;\\nimport jfluentvalidation.constraints.array.length.ArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraint;\\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraintAlternative;\\nimport jfluentvalidation.rules.PropertyRule;\\nimport jfluentvalidation.validators.RuleContext;\\nimport jfluentvalidation.validators.ValidationContext;\\nimport org.openjdk.jmh.annotations.*;\\nimport org.openjdk.jmh.runner.Runner;\\nimport org.openjdk.jmh.runner.options.OptionsBuilder;\\nimport java.util.concurrent.TimeUnit;\\n@BenchmarkMode(Mode.AverageTime)\\n@OutputTimeUnit(TimeUnit.NANOSECONDS)\\n@State(Scope.Benchmark)\\npublic class LengthBenchmark {\\npublic static class Foo {\\nprivate boolean[] bar;\\npublic Foo(boolean[] bar) {\\nthis.bar = bar;\\n}\\n}\\nRuleContext<Foo, boolean[]> ruleContext;\\nBooleanArrayExactLengthConstraintAlternative booleanArrayExactLengthConstraintAlternative;\\nArrayExactLengthConstraint arrayExactLengthConstraint;\\nBooleanArrayExactLengthConstraint booleanArrayExactLengthConstraint;\\n@Setup\\npublic void prepare() {\\nFoo f = new Foo(new boolean[5]);\\nPropertyRule propertyRule = new PropertyRule(foo -> f.bar, \"bar\");\\nruleContext = new RuleContext<>(new ValidationContext(f), propertyRule);\\nbooleanArrayExactLengthConstraintAlternative = new BooleanArrayExactLengthConstraintAlternative(5);\\narrayExactLengthConstraint = new ArrayExactLengthConstraint(5);\\n}\\n@Benchmark\\npublic void booleanArrayExactLengthConstraintAlternative() {\\nbooleanArrayExactLengthConstraintAlternative.isValid(ruleContext);\\n}\\n@Benchmark\\npublic void arrayExactLengthConstraint() {\\narrayExactLengthConstraint.isValid(ruleContext);\\n}\\npublic static void main(String[] args) throws Exception {\\nnew Runner(new OptionsBuilder()\\n.include(LengthBenchmark.class.getSimpleName())\\n.forks(1)\\n.warmupIterations(2)\\n.measurementIterations(5)\\n.build())\\n.run();\\n}\\n}\\n```\\nRun 1\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.504 | \u00b1 0.143 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.099 | \u00b1 0.022 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 2\\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.436 | \u00b1 0.049 | ns\/op |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.041 | \u00b1 0.013 | ns\/op |\\nobject cast has a performance impact of roughly ~19%\\nRun 3\\n| Benchmark                                                     | Mode  | Cnt | Score     | Error       | Units  |\\n|---------------------------------------------------------------|-------|-----|-----------|-------------|--------|\\n| LengthBenchmark.arrayExactLengthConstraint                    | thrpt | 25  | 0.424     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | thrpt | 25  | 0.626     | \u00b1 0.001     | ops\/ns |\\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt  | 25  | 2.362     | \u00b1 0.022     | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt  | 25  | 1.599     | \u00b1 0.007     | ns\/op  |\\n| LengthBenchmark.arrayExactLengthConstraint                    | ss    | 5   | 48261.200 | \u00b1 4493.186  | ns\/op  |\\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | ss    | 5   | 18690.000 | \u00b1 23530.434 | ns\/op  |\\nA ~20% performance impact on throughput and even larger on average time is a bit too much for me to ignore.\\n### Positive Consequences\\n- Gain roughly a 20% performance instead in average time and throughput.\\n### Negative Consequences\\n- We need 36 classes (8 primitive types + 1 object type and 4 constraint classes for each type) compared to 4 constraint classes\\n(ExactLength, BetweenLength, MinimumLength, MaximumLength) to implement array length constraints for primitive arrays.\\n- Duplicate logic in constraint classes across types.\\n","tokens":164,"id":2591,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nAccessing ENV directly without a wrapper is limited and can introduce problems.\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\n\n## Decision\nUse DotEnv to load our environment variables.\n\n\n## Context\nAccessing ENV directly without a wrapper is limited and can introduce problems.\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\n\n## Decision\nUse DotEnv to load our environment variables.\n\n\n\nProvide a decision given the context below:\n## Context and Problem Statement\nMy first pass at building out array constraint was to use a generic parameter `A` with `java.lang.reflect.Array` to obtain\nthe length of the property representing `A`.\nI was curious what the cost of using `java.lang.reflect.Array` compared to grabbing the `length` property from a known type was.\nAnything with a name like `reflect*` gives me nightmares about terrible performance.\nI decided to write a JMH benchmark to determine the performance impact `java.lang.reflect.Array` to assist in determining\nwhich implementation to use.\n## Decision Drivers\n1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\n<end_of_turn>\n<start_of_turn>model\n1. We want to keep performance in mind and attempt to be as performant as possible across all constraints.\n2. Avoid adding Additional classes and limit duplicating logic across constraints when possible.\nI decided to choose option 1 as I prioritized performance above the overhead to maintain additional classes and having\nduplicate logic.\nWhile it might be premature optimization and such a small impact (1 - 2 ns) the benchmark results below still convinced me.\nI'm sure someone can convince me that the overhead is insignificant or that I simply messed up the bencharmark at which point\nit should be easier refactor to option 2.\nI've included a rough [implementation of option 2](#option-2-implementation) just in case.\n```java\npackage jfluentvalidation.constraints.array;\nimport jfluentvalidation.constraints.array.length.ArrayExactLengthConstraint;\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraint;\nimport jfluentvalidation.constraints.array.length.BooleanArrayExactLengthConstraintAlternative;\nimport jfluentvalidation.rules.PropertyRule;\nimport jfluentvalidation.validators.RuleContext;\nimport jfluentvalidation.validators.ValidationContext;\nimport org.openjdk.jmh.annotations.*;\nimport org.openjdk.jmh.runner.Runner;\nimport org.openjdk.jmh.runner.options.OptionsBuilder;\nimport java.util.concurrent.TimeUnit;\n@BenchmarkMode(Mode.AverageTime)\n@OutputTimeUnit(TimeUnit.NANOSECONDS)\n@State(Scope.Benchmark)\npublic class LengthBenchmark {\npublic static class Foo {\nprivate boolean[] bar;\npublic Foo(boolean[] bar) {\nthis.bar = bar;\n}\n}\nRuleContext<Foo, boolean[]> ruleContext;\nBooleanArrayExactLengthConstraintAlternative booleanArrayExactLengthConstraintAlternative;\nArrayExactLengthConstraint arrayExactLengthConstraint;\nBooleanArrayExactLengthConstraint booleanArrayExactLengthConstraint;\n@Setup\npublic void prepare() {\nFoo f = new Foo(new boolean[5]);\nPropertyRule propertyRule = new PropertyRule(foo -> f.bar, \"bar\");\nruleContext = new RuleContext<>(new ValidationContext(f), propertyRule);\nbooleanArrayExactLengthConstraintAlternative = new BooleanArrayExactLengthConstraintAlternative(5);\narrayExactLengthConstraint = new ArrayExactLengthConstraint(5);\n}\n@Benchmark\npublic void booleanArrayExactLengthConstraintAlternative() {\nbooleanArrayExactLengthConstraintAlternative.isValid(ruleContext);\n}\n@Benchmark\npublic void arrayExactLengthConstraint() {\narrayExactLengthConstraint.isValid(ruleContext);\n}\npublic static void main(String[] args) throws Exception {\nnew Runner(new OptionsBuilder()\n.include(LengthBenchmark.class.getSimpleName())\n.forks(1)\n.warmupIterations(2)\n.measurementIterations(5)\n.build())\n.run();\n}\n}\n```\nRun 1\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.504 | \u00b1 0.143 | ns\/op |\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.099 | \u00b1 0.022 | ns\/op |\nobject cast has a performance impact of roughly ~19%\nRun 2\n| Benchmark                                                     | Mode | Cnt | Score | Error   | Units |\n|---------------------------------------------------------------|------|-----|-------|---------|-------|\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt | 25  | 2.436 | \u00b1 0.049 | ns\/op |\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt | 25  | 2.041 | \u00b1 0.013 | ns\/op |\nobject cast has a performance impact of roughly ~19%\nRun 3\n| Benchmark                                                     | Mode  | Cnt | Score     | Error       | Units  |\n|---------------------------------------------------------------|-------|-----|-----------|-------------|--------|\n| LengthBenchmark.arrayExactLengthConstraint                    | thrpt | 25  | 0.424     | \u00b1 0.001     | ops\/ns |\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | thrpt | 25  | 0.626     | \u00b1 0.001     | ops\/ns |\n| LengthBenchmark.arrayExactLengthConstraint                    | avgt  | 25  | 2.362     | \u00b1 0.022     | ns\/op  |\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | avgt  | 25  | 1.599     | \u00b1 0.007     | ns\/op  |\n| LengthBenchmark.arrayExactLengthConstraint                    | ss    | 5   | 48261.200 | \u00b1 4493.186  | ns\/op  |\n| LengthBenchmark.booleanArrayExactLengthConstraintAlternative  | ss    | 5   | 18690.000 | \u00b1 23530.434 | ns\/op  |\nA ~20% performance impact on throughput and even larger on average time is a bit too much for me to ignore.\n### Positive Consequences\n- Gain roughly a 20% performance instead in average time and throughput.\n### Negative Consequences\n- We need 36 classes (8 primitive types + 1 object type and 4 constraint classes for each type) compared to 4 constraint classes\n(ExactLength, BetweenLength, MinimumLength, MaximumLength) to implement array length constraints for primitive arrays.\n- Duplicate logic in constraint classes across types.\n<end_of_turn>\n"}
{"File Name":"simple-server\/001-synchronization.md","Context":"## Context\\nNetwork connectivity on phones in rural areas can be low, and\\npatchy. So, the app needs to work offline, as much as possible. The\\nsync mechanism exists to allow sharing records of patients, blood\\npressures, etc across devices.\\nWe need to accommodate cases where patients, and nurses move across\\nfacilities multiple times during a week.\\n","Decision":"### Mechanism\\n1. Send records from device to server\\nThe device needs to keep track of records that need to be\\nsynced. These can be new records, or records that have one or more\\nfields updated. These records need to be formatted into the payload\\nschemas as defined in the individual APIs below. The API will not\\ndifferentiate between new and updated records; this is handled by the\\nserver.  These payloads then need to be sent in batches, where each\\nbatch is inherently independent of the previous one. However, it is\\nimportant to _send_ all records that need syncing, before fetching\\nrecords from the server.\\n2. Fetch records from server to device\\nWhen fetching records for the first time, the `first_time` query\\nparameter should be set. The server will then send back a number of\\nrecords as defined by the `number_of_records` query parameter. This is\\nessentially the \\\"batch_size\\\". The first response also includes a\\n`latest_record_timestamp`, that needs to be sent with the next\\nrequest, in order to retrieve more records from the server. If there\\nare no more records to be sent from the server, it returns an empty\\nlist.\\n3. Merge records on device\\nAfter receiving records from the server, the device will match the\\nrecords in the local database using the ID field of the entity, and\\nupdate them to the server's version. If the local record in the\\ndatabase is pending a sync, it will not update it. The merging of\\nrecords will be handled only by the server.\\nThe server may send back the same record multiple times, so updating\\nrecords on the device needs to be idempotent.\\n### Levers\\nPhysical resource constraints such as battery life, or network are not\\nunder our control. In order for the app to adapt well to the\\nconstraints, we need to have knobs on:\\n1. Batch size: how many records to send or fetch\\n2. Sync frequency: how often sending or fetching happens\\nPreferably, these levers are modifiable at run-time, per device.\\n","tokens":77,"id":1712,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nFrequency Capping allows Content Managers to limit the number of\nimpressions or interactions users have with content. Is a widely\navailable tool in Publishing Platforms.\nIt's usually developed on the server side where the system can decide\nhow many times to serve the content to the requesting users which we\ncall \"Global Frequency Capping\". Additionally the system may be able\nto limit the number of impressions per user which we call \"Local\" or\n\"User Frequency Capping\".\nFor example a Content Piece can be set to 1,000,000 Global Impressions\nand 1 Impression per User, thus indirectly driving 1,000,000 different\nusers to this Content.\nThis functionality has been lacking from the Snippet Service due to\ntechnical limitations imposed by the way metrics were collected and\ncontent selection was handled on the client side. The latest\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\nPipeline unblock this capability. [0]\n\n## Decision\nWe decide to implement the Frequency Capping functionality into our\nplatform to allow Content Managers to limit the number of Impressions,\nClicks and Blocks per Job.\nLocal or User Frequency Capping will be handled on the Browser level\nby the Firefox Messaging Platform. The later supports only Impression\nFrequency Capping.\nThe Snippets Service will provide an interface (UI) for the Content\nManagers to set upper limits on the number of Impressions a Job gets\nper Hour, Day, Week, Fortnight, Month or for the complete Browser\nProfile Lifetime. This information is included in the JSON generated\nfor each Job.\nFor Global Frequency Capping the Snippets Service will provide an\ninterface (UI) for the Content Managers to set the limits on total\nworldwide number of Impressions, Clicks and Blocks per Job.\nSnippets Service will query Mozilla's Redash for Telemetry data every\nten minutes and will fetch current impressions, clicks, blocks for\neach Job with set limits.\nWhen the reported numbers exceed the set limits then, the Job will be\nmarked COMPLETE and will be pulled out of the Bundles on the next run\nof `update_jobs` cron job.\nThe Frequency Capping functionality is additional to the Date\nPublishing controls, therefore a Job can end on a specific Date and\nTime or when its Global Frequency Capping Limits are met.\n### Monitoring and Handling of Errors\nSince Global Frequency Capping depends on an external system for\nMetrics (Redash \/ Telemetry) it is possible that the latest numbers are\nnot always available to the Snippets Service to make a decision. Such\ncases include scheduled or unplanned service interruptions or network\nerrors.\nIn co-ordination with Snippet Content Owner we decided that for cases\nwhere the Snippets Service cannot get the latest numbers for more than\n24 hours, Jobs with Global Frequency Capping will get canceled. The\ncancellation reason will state that the Jobs where prematurely\nterminated due to missing metrics.\nThe cron job responsible for fetching the Data from Telemetry is\nmonitored by a Dead Man's Snitch.\n\n\n## Context\nOur user facing applications will need to authenticate users.\nIn the long-term, we want to use the standard user sign-on services in CCS for\neach appropriate user group.\nFor suppliers, this is the [Supplier Registration Service (SRS)][service-srs].\nFor CCS staff it is their G-Suite account.\nUsing a single authentication service will reduce administration overheads\nbecause we won't have to manage a JML (Joiners Movers Leavers) process. For\nusers it will also mean they don't have to remember yet another username and\npassword combination.\nHowever, we want to get moving quickly, and integration with SRS\/CCS G-Suite\nwill take time. For now, we want a simple sign-in service that we can integrate\nwith quickly, to enable us to test and iterate the application.\n\n## Decision\nWe will use [Auth0][service-auth0] for authentication on a temporary basis.\nAuth0 is an authentication service that uses the OAuth protocol. It provides\nand simple integration path, and is free for several thousand active users.\nWe acknowledge that we are incurring technical debt by not integrating\nimmediately with SRS and G-Suite, however we believe that this will let us move\nmore quickly during development.\nThis decision will also allow us to gain a better understanding of the user\nneeds around authentication before a decision around the final authentication\nsolution is made.\nWe will replace Auth0 with the proper authentication service later in the\ndevelopment process.\n\n\n\nProvide a decision given the context below:\n## Context\nNetwork connectivity on phones in rural areas can be low, and\npatchy. So, the app needs to work offline, as much as possible. The\nsync mechanism exists to allow sharing records of patients, blood\npressures, etc across devices.\nWe need to accommodate cases where patients, and nurses move across\nfacilities multiple times during a week.\n<end_of_turn>\n<start_of_turn>model\n### Mechanism\n1. Send records from device to server\nThe device needs to keep track of records that need to be\nsynced. These can be new records, or records that have one or more\nfields updated. These records need to be formatted into the payload\nschemas as defined in the individual APIs below. The API will not\ndifferentiate between new and updated records; this is handled by the\nserver.  These payloads then need to be sent in batches, where each\nbatch is inherently independent of the previous one. However, it is\nimportant to _send_ all records that need syncing, before fetching\nrecords from the server.\n2. Fetch records from server to device\nWhen fetching records for the first time, the `first_time` query\nparameter should be set. The server will then send back a number of\nrecords as defined by the `number_of_records` query parameter. This is\nessentially the \\\"batch_size\\\". The first response also includes a\n`latest_record_timestamp`, that needs to be sent with the next\nrequest, in order to retrieve more records from the server. If there\nare no more records to be sent from the server, it returns an empty\nlist.\n3. Merge records on device\nAfter receiving records from the server, the device will match the\nrecords in the local database using the ID field of the entity, and\nupdate them to the server's version. If the local record in the\ndatabase is pending a sync, it will not update it. The merging of\nrecords will be handled only by the server.\nThe server may send back the same record multiple times, so updating\nrecords on the device needs to be idempotent.\n### Levers\nPhysical resource constraints such as battery life, or network are not\nunder our control. In order for the app to adapt well to the\nconstraints, we need to have knobs on:\n1. Batch size: how many records to send or fetch\n2. Sync frequency: how often sending or fetching happens\nPreferably, these levers are modifiable at run-time, per device.\n<end_of_turn>\n"}
{"File Name":"adr-demo\/0003-use-cassandra-as-storage-backend.md","Context":"## Context\\nAs the application offers uploads capabilities to our users, we need to be able to store uploaded images to some\\nkind of storage. The storage backend should be able to do the following things:\\n* Store raw images\/gifs\\n* Store thumbnails of images\/gifs\\n* Offer a basic search capability (search by title, search by tag)\\n","Decision":"After a quick comparative study, we decide to go with a Cassandra storage backend. The reasons are the following:\\n* Cassandra is quick and easy to setup\\n* It should answer our needs without much custom configuration, even when scaling up\\n* Cassandra allows us to do simple search operations on the fields we're interested in\\n* The team is already familiar with Cassandra usage\\nHere is a quick picture of our architecture including Cassandra as the storage backend:\\n![](microservices-architecture-with-cassandra.png)\\n**Note**: Many services seem to point to the same Cassandra, however we need to setup multiple tables so we can respect\\nCQRS principles (command and query separation):\\n* One table will be used to store thumbnails\\n* One table will be used to store raw images\\n* Thumbnail and upload services are write services (but to different tables)\\n* Search service and download service are read-only services\\n### Other databases\\nIn our quest to the holy storage backend we also studied MongoDb but it didn't seem to be a good fit for our use case : its Json-like structure does not fill particularly well with what we are trying to achieve, and is not much appropriate for very large storage volumes.\\n","tokens":73,"id":2515,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe need to persist the user data to the backend. Rather than building our\nfrontend to be tightly coupled to an API, we want a layer that can sit between\nthe application and the API.\nWe know that we will need to support offline use of these forms. We also want\nusers to be able to recover unsubmitted forms if they haven't gone online\nbetween sessions.\nWe also know that some forms will require the upload of images.\nIndexedDB is a [well supported](https:\/\/caniuse.com\/#feat=indexeddb) browser\ndatabase that persists data across sessions. It supports storing blob data (such\nas images), and has decent storage limits.\n\n## Decision\nWe will support IndexedDB as a data store layer, sitting between the application\nand the API.\n\n\n## Context\nRight now this cli tool comes with a builtin set of commands that is hardcoded\nto a gist I own somewhere. This is not ideal.\nWe want this tool to easily access \"repositories of commands\" that are either\nremote or local; as well as a way to easily add\/remove repositories.\nSome ideas of repositories:\n- HTTP urls pointing to yaml manifests with commands definitions\n- Github repositories\n- Local files or directories\n\n## Decision\n- We won't have built-in commands or repositories\n- We will support three types of repositories:\n- Remote http resources pointing to yaml files\n- Local manifest yaml files\n- Local directories containing .mcd files\nBecause http resources are now supported, we could make it so that the\nrepositories with commands has CI\/CD via Jenkins, which generates a repo and\npublishes it to artifactory.\nThe macondo file will be simplified to only include, for now, a top-level\n`repositories` array, which would point to the repositories.\nA macondo update command will be added that refreshes the list of commands\nprovided by the repositories. This is only done for http repos, whose\ncontents could change anytime.\nA new set of commands to be implemented:\n- `macondo repo add` to add repositories\n- `macondo repo remove` to remove them\n- `macondo repo update` to update them\n- `macondo repo list` to list them\n\n\n\nProvide a decision given the context below:\n## Context\nAs the application offers uploads capabilities to our users, we need to be able to store uploaded images to some\nkind of storage. The storage backend should be able to do the following things:\n* Store raw images\/gifs\n* Store thumbnails of images\/gifs\n* Offer a basic search capability (search by title, search by tag)\n<end_of_turn>\n<start_of_turn>model\nAfter a quick comparative study, we decide to go with a Cassandra storage backend. The reasons are the following:\n* Cassandra is quick and easy to setup\n* It should answer our needs without much custom configuration, even when scaling up\n* Cassandra allows us to do simple search operations on the fields we're interested in\n* The team is already familiar with Cassandra usage\nHere is a quick picture of our architecture including Cassandra as the storage backend:\n![](microservices-architecture-with-cassandra.png)\n**Note**: Many services seem to point to the same Cassandra, however we need to setup multiple tables so we can respect\nCQRS principles (command and query separation):\n* One table will be used to store thumbnails\n* One table will be used to store raw images\n* Thumbnail and upload services are write services (but to different tables)\n* Search service and download service are read-only services\n### Other databases\nIn our quest to the holy storage backend we also studied MongoDb but it didn't seem to be a good fit for our use case : its Json-like structure does not fill particularly well with what we are trying to achieve, and is not much appropriate for very large storage volumes.\n<end_of_turn>\n"}
{"File Name":"exercise3-group-6\/0003-create-two-observation-comparators.md","Context":"## Context\\nWhale observations have a variety of qualities and may require sorting for research purposes. Two\\ndistinct sorting methods proposed for sorting these observations will require two unique comparison methods.\\n","Decision":"Implement `Comparable<Observation>`, create default field `compareTo` method for `ObservationId` (long) and nested\\ncomparator class for field `sightingTime` (Date) in Observation.\\n","tokens":39,"id":2775,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nResearchers may require methods to search for a particular Whale Observation or set of Whale Observations during their\nstudies. For the program to accomplish this task two search methods may require implementation.\n\n## Decision\nCreate `getByDate` and `getById` methods in ObservationRepository.\n`getByDate` uses the Observation iterator to compare Observation objects `sightingTime` (Date) to the date passed to the\nmethod and creates an array list of matching objects. This function uses an iterator rather than `Collections.binarySearch()`\nto reduce the runtime given that there may exist multiple objects of a similar date.\n`getById` uses `Collections.binarySearch()` to search and possibly return the object with the `ObservationId` matching\nthe long passed to the method.\nCollection.sort()` used in `getById` implements the abstract strategy of the `Comparator` interface using the concrete\nstrategy `compareByDate` and returns an integer referring to the order of the objects.\n\n\n## Context\nDatasets are made up of observations and dimensions. Each of the options for a\ndimension have wider definitions and metadata which apply to that option in all\nscenarios, across multiple datasets. These code definitions live in code-lists.\nWhen storing both datasets and code-lists in a graph database, there is some\noverlap between the information stored on a code and on a dimension option. A\ndesign decision was needed as to whether the dimension option and code nodes\nin the graph database should be collapsed into a single node, with the relevant\nrelationships for each coming off that single node.\n\n## Decision\nDimension options and codes should not be collapsed into a single node. Dimension\noptions have relationships to every observation within an imported dataset, and\nadding those relationships to the code node would result in relationships to\nevery observation across all datasets - a significant and unnecessary burden.\n\n\n\nProvide a decision given the context below:\n## Context\nWhale observations have a variety of qualities and may require sorting for research purposes. Two\ndistinct sorting methods proposed for sorting these observations will require two unique comparison methods.\n<end_of_turn>\n<start_of_turn>model\nImplement `Comparable<Observation>`, create default field `compareTo` method for `ObservationId` (long) and nested\ncomparator class for field `sightingTime` (Date) in Observation.\n<end_of_turn>\n"}
{"File Name":"nr-arch\/2020-03-12-ARCH-record-architecture-decisions-simple-template.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4804,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n\n## Decision\nThe change that we're proposing or have agreed to implement.\n\n\n## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n\n## Decision\nThe change that we're proposing or have agreed to implement.\n\n\n\nProvide a decision given the context below:\n## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n<end_of_turn>\n<start_of_turn>model\nThe change that we're proposing or have agreed to implement.\n<end_of_turn>\n"}
{"File Name":"racing-tips\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":3119,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n\n\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\n\n\n\nProvide a decision given the context below:\n## Context\nWe need to record the architectural decisions made on this project.\n<end_of_turn>\n<start_of_turn>model\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n<end_of_turn>\n"}
{"File Name":"architecture\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1436,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n\n\n## Context\nWe need to record the architectural decisions made on this project.\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\n\n\n\nProvide a decision given the context below:\n## Context\nWe need to record the architectural decisions made on this project.\n<end_of_turn>\n<start_of_turn>model\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\n<end_of_turn>\n"}
{"File Name":"hmpps-interventions-ui\/0010-use-stylelint-for-linting-styles.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https:\/\/stylelint.io\/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n","Decision":"We will check SASS syntax using Stylelint.\\nWe will use the recommended configuration for plugins where possible.\\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\\n","tokens":75,"id":458,"text":"<bos><start_of_turn>user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\nJavaScript, and has good support for TypeScript though plugins.\n\n## Decision\nWe will check code style using ESLint.\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\nstyles.\nWe will use the recommended configuration for plugins where possible.\nWe will run ESLint as part of the test suite.\n\n\n## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\nJavaScript, and has good support for TypeScript though plugins.\n\n## Decision\nWe will check code style using ESLint.\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\nstyles.\nWe will use the recommended configuration for plugins where possible.\nWe will run ESLint as part of the test suite.\n\n\n\nProvide a decision given the context below:\n## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [Stylelint](https:\/\/stylelint.io\/) is one of the more popular CSS\nlinters with support for SASS, and is easily configurable for our purposes.\n<end_of_turn>\n<start_of_turn>model\nWe will check SASS syntax using Stylelint.\nWe will use the recommended configuration for plugins where possible.\nWe will use Stylelint to automatically fix linting errors in a pre-commit hook.\n<end_of_turn>\n"}
