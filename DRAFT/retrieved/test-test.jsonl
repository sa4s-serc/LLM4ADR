{"File Name":"libelektra\/ingroup_removal.md","Context":"## Problem\\nCurrently, an error or warning message in elektra causes the following line to be shown:\\n```\\nIngroup: <group>\\n```\\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\\n","Decision":"The `ingroup` message will be removed as it does not yield any notable benefit.\\nSee [Error concept](error_codes.md)\\n","tokens":83,"id":1308,"text":"user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nWhen HapiJS plugins are registered they expect an object like this:\n```JavaScript\n{\nname: 'plugin-name',\nversion: '1.0.0',\nregister: (server, options) => {}\n}\n```\nThe problem with `export default` is that it exports an object whose property `default` is the value of the exported object. This makes my convention of having the `index.ts` register the plugin not work because HapiJS still uses CommonJs and won't get the `default` value E.g.\n```JavaScript\nimport plugin from '.\/plugin';\nexport default {\nplugin,\n}\n```\nSwitching between `module.exports` and `export default` will make using my packages tricky. It won't be clear when to use CommonJS or ES6 modules so I'll pick one and stick with it.\nSo let's use `module.exports` instead of `export default`.\n\n## Decision\nRejected because if I try and import a module that uses `module.exports` Typescript shows an error that the file `is not a module`. The implication of this is that I have to replace [Glue][hapi-glue] to compose the API.\n\n\n## Context\nThere no agreed-upon way of handling application configuration. There are two issues this ADR aims to address.\n### Issue 1: Nested keys\nIn `config.exs`, configuration keys can be nested by using a Keyword as value. Unfortunately, there is no built-in support for this nesting. For example:\n```\nApplication.fetch_env!(:rig, :required_key)\n```\nwill show a nice error, while\n```\nApplication.fetch_env!(:rig, :required_key)[:required_subkey]\n```\nwill simply return nil in case `:required_key` is present but `:required_subkey` is not.\n### Issue 2: Default values\nSome default values are defined where they are needed in the code, which is a problem once a key is used more than once. Also, defaults are no easily inspectable.\n\n## Decision\n* We prefer not to nest configuration keys; instead, we prefix them where it makes sense. For example, instead of `config :rig, :kafka, client_id: :rig_brod_client` we write `config :rig, kafka_client_id: :rig_brod_client`. This allows us to leverage the built-in methods (e.g., `Application.fetch_env!`), which produce sensible error messages in case required values are not defined.\n* We always set default values in config.exs (in turn this means that we prefer `Application.fetch_env!` over `Application.get_env`). This way, it is easy to reason about default values, and using a default value in more than one place is not an issue.\n\n\n\nProvide a decision given the context below:\n## Problem\nCurrently, an error or warning message in elektra causes the following line to be shown:\n```\nIngroup: <group>\n```\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\n\nmodel\nThe `ingroup` message will be removed as it does not yield any notable benefit.\nSee [Error concept](error_codes.md)\n\n"}
{"File Name":"knot-documentation\/ARC-003-amqp-exc-name.md","Context":"## Context\\nAs defined by CloudAMQP Blog, \"exchanges are message routing agents, defined by the virtual host within RabbitMQ. An exchange is responsible for routing the messages to different queues with the help of header attributes, bindings, and routing keys.\"\\nThe Exchange name needs to be defined taking into account the architecture defined for the system. Thus, there will be a client that communicates with BabelTower, which has the Connector as one of its services (in this case BabelTower is the Connector client) or the client communicates directly with the Connector.\\nThe goal is to define names that make it clear what this Exchange is used for.\\n","Decision":"* The communication between two components will be done with two queues, where one component is consumer in one and producer in another and the other component follows the opposite.\\n* The reference for choosing the queue name will be the client.\\n* The queue where BabelTower publishes messages will be called 'fogOut'.\\n* The queue where BabelTower consumes messages will be called 'fogIn'.\\n* The queue where Connector publishes messages will be called 'connOut'.\\n* The queue where Connector consumes messages will be called 'connIn'.\\n","tokens":131,"id":3339,"text":"user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context and Problem Statement\nTo implement the EAI patterns we use a combination of a generic component\nwhich handles the communication with Kafka and a FaaS solution. The business logic of the EAI patterns (message splitting\/aggregation or transformation) is provided via functions which are hosted on the FaaS solution. The generic component communicates with Kafka and de\/serializes the messages. We need a means to wire the instances of the generic component with the functions. E.g. A user wants to insert a message splitter between two message-based components. To realize this an instance of the generic component in combination with a splitting FaaS function will be used. The generic component needs the address of the FaaS gateway and the function name (e.g. http:\/\/address:8080\/function\/msg-payload-splitter) to call the function. To provide the necessary information to instances of the generic component we considered the following techniques.\n## Decision Drivers <!-- optional -->\n* MUST be supported by the language\/technology which is used to implement the generic component\n* MUST be easy to integrate into MICO\n* SHOULD be a well known and proven solution\n\n## Decision\n* MUST be supported by the language\/technology which is used to implement the generic component\n* MUST be easy to integrate into MICO\n* SHOULD be a well known and proven solution\nChosen option: \"Environment variables\", because MICO already supports this and it is easy to implement in the generic component.\n\n\n## **Context**\nWhen the microservices need to interact with each other (e.g. to sync common data), the preferable approach is to adopt an event-driven architecture. This approach guarantees loosely coupled services which can be run and deployed in isolation.\nIn order to implement an events-driven architecture we need to use a message broker which should be responsible for sending the messages.\nIn AWS, two options can be considered:\n1. SNS (Simple Notification Service)\nSNS implements pub\/sub messaging which allow applications to send messages to multiple subscribers through push mechanisms.\n2. SNS (Simple Notification Service) + SQS (Simple Queue Service)\nSQS is a message queue service used by distributed applications to exchange messages through a polling model and can be used to decouple sending and receiving components. Using Amazon SNS and Amazon SQS together, messages can be delivered to applications that require immediate notification of an event, and also persisted in an Amazon SQS queue for other applications to process at a later time.\n\n## Decision\n**SNS (Simple Notification Service) + SQS (Simple Queue Service)**\nBy coupling SQS with SNS, the subscriber can receive messages at \"their peace\". It allows subscribers to be offline, tolerant to network and host failures. Although SNS has got a 4-phase retry policy, the message can be lost if the consumer is not available. Instead if the subscriber uses a queue, we are able to achieve guaranteed delivery.\n\n\n\nProvide a decision given the context below:\n## Context\nAs defined by CloudAMQP Blog, \"exchanges are message routing agents, defined by the virtual host within RabbitMQ. An exchange is responsible for routing the messages to different queues with the help of header attributes, bindings, and routing keys.\"\nThe Exchange name needs to be defined taking into account the architecture defined for the system. Thus, there will be a client that communicates with BabelTower, which has the Connector as one of its services (in this case BabelTower is the Connector client) or the client communicates directly with the Connector.\nThe goal is to define names that make it clear what this Exchange is used for.\n\nmodel\n* The communication between two components will be done with two queues, where one component is consumer in one and producer in another and the other component follows the opposite.\n* The reference for choosing the queue name will be the client.\n* The queue where BabelTower publishes messages will be called 'fogOut'.\n* The queue where BabelTower consumes messages will be called 'fogIn'.\n* The queue where Connector publishes messages will be called 'connOut'.\n* The queue where Connector consumes messages will be called 'connIn'.\n\n"}
{"File Name":"decodeweb\/0003-store-configuration-into-local-storage.md","Context":"## Context\\nTo provide the required functionality for the pilot we need to store some\\nconfiguration in a persistent way while just operating within a browser\\nenvironment. This state could be written to a backend database, or we could\\njust store into localstorage.\\n","Decision":"We will store the user's configuration into localstorage protected by a PIN\\ncode.\\n### Proposed storage structure\\n```json\\n{\\n\"1234\": {\\n\"keypair\": {},\\n\"blindSignature\": {},\\n\"devices\": {\\n\"abc123\": {\\n\"longitude\": 12.2,\\n\"latitude\": 52.4,\\n\"exposure\": \"INDOOR\",\\n\"communities\": {\\n\"2a745dac-015a-4cde-a348-75bccc21502b\": {\\n\"authorizable_attribute_id\": \"6a1ba9b1-3f20-4e12-ae65-154c5eb7e00f\",\\n\"credential\": {},\\n\"blindproofCredential\": {}\\n}\\n}\\n}\\n}\\n}\\n}\\n```\\nPlease see `.\/localstorage.json` for a complete example including zenroom credentials.\\n","tokens":52,"id":2221,"text":"user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nIn order to interact with a remote workspace, for example, at structurizr.com.\nThe remote or local workspace has to be (de-)serialized from or to JSON.\n\n## Decision\nIn order to perform these operations we choose\n[pydantic](https:\/\/pydantic-docs.helpmanual.io\/) which has a nice API, active\ncommunity, good data validation, helpful documentation, and good performance.\n\n\n## Context\nThe OpenFIDO project will mediate access to a workflow service that runs\nGridLabD jobs. The frontend React application will need access to these\nresources, but only if they have permission to access via an organization.\n\n## Decision\nCreate a Flask Rest service - since the other microservices in this suite of\nservices are also Flask based, keep this using the same kind of infrastructure\nso that common utilities can be shared (openfido-utils) and the maintenance will\nbe simplified.\nOrganize the database logic into a simplified CQRS-inspired style code\nstructure. Since we anticipate many conceptual resources, each resource will\nhave its own module:\n* app\/RESOURCE\/models.py - contains all models for RESOURCE.\n* app\/RESOURCE\/routes.py - contains all Rest routes specific to RESOURCE.\n* app\/RESOURCE\/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\n* app\/RESOURCE\/services.py - all db commands that modify database state.\n* app\/RESOURCE\/queries.py - all db queries to the database.\nAdditional libraries we anticipate using:\n* [marshmallow](https:\/\/marshmallow.readthedocs.io\/en\/stable\/) will be used since there are many rest api endpoints that will take nested bodies.\n* [alembic](https:\/\/alembic.sqlalchemy.org\/en\/latest\/) to manage database schema migrations.\nOther thoughts on design:\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\nUse the structure of the [openfido-workflow-service](https:\/\/github.com\/slacgismo\/openfido-workflow-service) project as a reference.\n\n\n\nProvide a decision given the context below:\n## Context\nTo provide the required functionality for the pilot we need to store some\nconfiguration in a persistent way while just operating within a browser\nenvironment. This state could be written to a backend database, or we could\njust store into localstorage.\n\nmodel\nWe will store the user's configuration into localstorage protected by a PIN\ncode.\n### Proposed storage structure\n```json\n{\n\"1234\": {\n\"keypair\": {},\n\"blindSignature\": {},\n\"devices\": {\n\"abc123\": {\n\"longitude\": 12.2,\n\"latitude\": 52.4,\n\"exposure\": \"INDOOR\",\n\"communities\": {\n\"2a745dac-015a-4cde-a348-75bccc21502b\": {\n\"authorizable_attribute_id\": \"6a1ba9b1-3f20-4e12-ae65-154c5eb7e00f\",\n\"credential\": {},\n\"blindproofCredential\": {}\n}\n}\n}\n}\n}\n}\n```\nPlease see `.\/localstorage.json` for a complete example including zenroom credentials.\n\n"}
{"File Name":"holochain-rust\/0006-splitting-agent-into-front-house-back-house-or-not.md","Context":"## Context\\nFor Holo, we need to have user agent's keys and source chain on the client machine and the rest of the Holochain agent (the DHT shard etc.) be held by HoloPorts.\\nIn February 2018, Arthur, Eric and Nico discussed this during a co-creative session in London and made the assumption to have the Holochain agent be split up into two pieces, called front-house and back-house (prior left and right hemisphere). The front-house was meant to entail the source chain and private key management as well as a ribosome to run the app and provide interfaces for the UI and in the non-Holo case for bridging - everything that is user\/agent facing. The back-house should have been the DHT shard, routing table, metrics, etc. Basically everything that is network facing.\\nWith this separation, the reasoning in February was to rewrite (only) the front-house in JS to be able to run this piece in a web browser, as needed for Holo.\\nEric and Nico continued to map out the specifics of how these two modules would interface. In that process it became apparent that dividing an agent in these two pieces and have them run on different remote machines has a huge problem:\\n* every network communication (including the world model which happens on an ongoing basis) that the back-house is doing has to be signed with the agent's keys\\n* the agent's keys are by definition part of the front-house\\n* **-> the back-house can't live up to its main accountability without communicating with the front-house and requesting a signature for every packet of communication that might be even triggered from the network\/DHT while the user is offline and keys not accessible**\\nFurther conversation including Arthur on May 17th 2018 makes it clear that thinking in terms of two different kinds of agency seems appropriate. We discussed separating the authoring or creative agency from the part that runs validations and holds DHT shards and world model information, and allows the later to proxy for the former, **with separate keys**.\\n","Decision":"We decide to **not** emulate a single agency (as represented by one key) across two remote devices. In other words: we decide to not try to implement distributed agents.\\nInstead, we solve the initial Holo front-end problem by establishing **two sub-agencies with distinct keys**, where the Holo client's authoring agency explicitly grants proxy rights to a HoloPort's DHT agency.\\nIn other words: the Holo user uses their local key to sign a statement that grants another agent (the HoloPort) to act on their behalf for all the cases needed to have the HoloPort carry the DHT weight for this agent. But technically, it is another agent with its own key.\\n","tokens":423,"id":1500,"text":"user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n# Context\nThe way the Government Digital Service (GDS) [makes technology choices is\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which technology will to use to provide\npersistence for the Performance Platform.\nGDS has experience in running MongoDB and MySQL in production.\nWe envisage the Performance Platform as taking in unstructured data from a\nvariety of data sources (spreadsheets, analytics, logs, other databases and\napplications) and allowing people to collect this data in a single place. This\nshould enable service managers to:\n- make comparisons\n- see how well their service is performing\n- see how the performance changes over time, as they iterate the service\nSo we want a persistent data store that will store unstructured data, and\nallow us to apply a structure either by post-processing the data, or at query\ntime.\nThe volume of the data that we are envisaging at this stage is pretty small.\nWe will be building a small thing to start; as we learn more about the\nuser needs and problem space, then we will revisit this decision. Since the\nvolume is small, it does not seem likely that we need Hadoop \/ HDFS or\nCassandra.\nWe are not the canonical source of this data. We are an aggregator; the\ncanonical source remains the data sources which will be providing feeds or\npushing the data into the Performance Platform.\nBecause of this position, we do not need ACID properties for this data, nor\nneed worry about the CAP theorem in any detail.\n# Decision\nWe will use MongoDB. We are comfortable operating it in production,\nit will allow unstructured data (in the form of JSON documents) and we can\napply structure at query time.\n# Status\nAccepted.\n# Consequences\nUse MongoDB with an appropriate replica-set configuration.\n\n## Decision\nWe will use MongoDB. We are comfortable operating it in production,\nit will allow unstructured data (in the form of JSON documents) and we can\napply structure at query time.\n# Status\nAccepted.\n# Consequences\nUse MongoDB with an appropriate replica-set configuration.\n\n\n## Context and Problem Statement\nWithin contact-frontend, for the standalone pages, two routes into the page exist. The first requires a tax service\nuser to be logged in, and redirects to login if the service user is not logged in. The second, on a different URL\nsuffixed with \"-unauthenticated\", does not require login, but serves the same page. After discussion, the PlatUI team\ndecided that the requirement for a user to be logged in was making a worse experience for the service end user, adding\nthe requirement of login,when all the same functionality is also available without logging in via a different URL.\n## Decision Drivers\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\navailable without login\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\npersisted to Deskpro agents\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\nthe codebase, making our development process slower and our testing time longer\n\n## Decision\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\navailable without login\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\npersisted to Deskpro agents\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\nthe codebase, making our development process slower and our testing time longer\nChosen option: \"Remove login redirect from all pages and keep existing endpoints for now\", because:\n* The benefits to end users in terms of accessibility feel significant enough to warrant the change\n* Information about logged in users will still be persisted to Deskpro but there will not be a redirect for users\nwho have been logged out in the background\n* Codebase is significantly simplified for developers working on contact-frontend\n* However, deleting the \"-unauthenticated\" routes will be a major breaking change across the Platform, and should be\ndecoupled from this\n### Positive Consequences\n* Capturing information about all logged in users, not just the ones who have clicked on a contact-frontend route not\nending in \"-unauthenticated\"\n* More accessible experience for tax platform end users who are not forced to log in to report technical problems\n* Significant simplification of codebase for developers\n### Negative Consequences\n* There is a risk that without forcing users to log in to particular implementations of the contact-frontend forms,\nthere will be less information sent to DCST in certain cases. On the other hand, for certain users, the login\nrequirement might lock them out of the contact-frontend forms completely, e.g. if they are trying to report a problem\nwith login\n\n\n\nProvide a decision given the context below:\n## Context\nFor Holo, we need to have user agent's keys and source chain on the client machine and the rest of the Holochain agent (the DHT shard etc.) be held by HoloPorts.\nIn February 2018, Arthur, Eric and Nico discussed this during a co-creative session in London and made the assumption to have the Holochain agent be split up into two pieces, called front-house and back-house (prior left and right hemisphere). The front-house was meant to entail the source chain and private key management as well as a ribosome to run the app and provide interfaces for the UI and in the non-Holo case for bridging - everything that is user\/agent facing. The back-house should have been the DHT shard, routing table, metrics, etc. Basically everything that is network facing.\nWith this separation, the reasoning in February was to rewrite (only) the front-house in JS to be able to run this piece in a web browser, as needed for Holo.\nEric and Nico continued to map out the specifics of how these two modules would interface. In that process it became apparent that dividing an agent in these two pieces and have them run on different remote machines has a huge problem:\n* every network communication (including the world model which happens on an ongoing basis) that the back-house is doing has to be signed with the agent's keys\n* the agent's keys are by definition part of the front-house\n* **-> the back-house can't live up to its main accountability without communicating with the front-house and requesting a signature for every packet of communication that might be even triggered from the network\/DHT while the user is offline and keys not accessible**\nFurther conversation including Arthur on May 17th 2018 makes it clear that thinking in terms of two different kinds of agency seems appropriate. We discussed separating the authoring or creative agency from the part that runs validations and holds DHT shards and world model information, and allows the later to proxy for the former, **with separate keys**.\n\nmodel\nWe decide to **not** emulate a single agency (as represented by one key) across two remote devices. In other words: we decide to not try to implement distributed agents.\nInstead, we solve the initial Holo front-end problem by establishing **two sub-agencies with distinct keys**, where the Holo client's authoring agency explicitly grants proxy rights to a HoloPort's DHT agency.\nIn other words: the Holo user uses their local key to sign a statement that grants another agent (the HoloPort) to act on their behalf for all the cases needed to have the HoloPort carry the DHT weight for this agent. But technically, it is another agent with its own key.\n\n"}
{"File Name":"raster-foundry\/adr-0006-workflow-manager.md","Context":"## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n","Decision":"Raster Foundry will use Airflow as a task manager. There are a number of advantages over some of the\\nalternatives. First, Airflow has a large, active, user base that have used it in production. The\\nproject itself is in the Apache incubator, providing a strong signal that the project is of high quality\\nSecond, Airflow's UI for monitoring task and workflow progress is great. It provides\\nhigh-level relevant information that will enable us to diagnose issues quickly. Additionally, it\\nprovides a means to view log output of tasks directly in the admin interface. Third, Airflow\\nsupports both scheduled and ad hoc tasks. Lastly, Airflow's architecture would re-use many\\ncomponents that will already be a part of Raster Foundry's infrastructure - including a Postgres\\ndatabase and a redis cache.\\n","tokens":353,"id":1688,"text":"user\nYou are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\n\n## Context\nI wanted to create a website blog using [Markdown][1], a [static site generator][2] and [HapiJS][3] to serve the files. Static site generators I tried, in order, were -\n### [11ty][4]\nPros\n- Easy setup\n- Predictable static file output\n- Relatively small codebase\n- Supporst different template engines\nCons\n- The frontmatter parser [didn't work as I expected][7]\n- Template errors were not reported and would only fail on build\n- Needs some attention to make production ready e.g. SEO, images, favicons etc\n### [Gatsby][5]\nPros\n- Modern [JAMstack][8] project with React\n- Batteries included by default - includes helpers for favicons, image loading, SEO etc.\n- Opportunity to use GraphQL\nCons\n- Doesn't output static files - [totally depends on clientside JavaScript][9]\n- It relies on service workers to cache responses which causes flakey e2e tests and unpredictable behaviour\n- Developing static files based on API was difficult because of caching and unexpected error handling\n- Relatively complex project with separate build, develop steps, config, plugins etc\n### [Hugo][6]\nPros\n- Fast and mature static site generator\n- Small footprint\nCons\n- Written in Go - I prefer fullstack JavaScript for this site\n\n## Decision\nIn the context of using several static site generators each of them had with their own downsides. And facing the concern of having blog that I can easily maintain and customise I decided to build my blog with HapiJS to achieve a smaller blog footprint, predictable behaviour and an opportunity to learn something new. I accept that I'm re-inventing the wheel.\n\n\n## Context and Problem Statement\nThe FxA back-end service stack contains an application architecture that is ad-hoc, not documented, and missing modern features (such as Dependency Injection) which results in the following problems:\n- New developers struggle to get up to speed as they must learn the architecture by reading the code as we have no documentation on the application structure, why they're structured the way they are, or how new components should be added to fit in. Each back-end service may vary in its ad-hoc architecture as well.\n- Adding new objects needed in a route handler can be time-consuming as the object must be plumbed through the entire initialization chain vs. more elegant methods like Dependency Injection (DI).\n- Not clear where\/how to add new components and takes time to study\/understand how things are currently setup in an attempt to mimic the structure for the new component.\n- Time consuming to setup boiler-plate for components, as we have no tooling to work with the current ad-hoc application architectures.\n- Our ad-hoc architecture frequently mixes concerns such as having business logic mixed in with request handling logic, and has other warts from its evolution over time vs. being planned up front.\n- New back-end services evolve differently resulting in more ad-hoc application architectures to learn.\n- Shared components in `fxa-shared` can't rely on basic object lifecycles or setup approaches as they may be used in multiple different ad-hoc application architectures.\nNot choosing an application framework means that we have choosen to make ad-hoc application architectures which will continue to exhibit the problems above.\nIt is assumed that the four newest FxA back-end services (admin-server, support-panel, event-broker, gql-api) will be switched to the chosen approach for consistency.\n## Decision Drivers\n- Documented application architecture.\n- Tooling that reduces boiler-plate and creates consistent code architecture.\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\n- Training materials so that new developers can understand and work on application features easily.\n- Ability to migrate legacy applications to similar conventions\/setup.\n- Handles situations we need such as:\n- Exception handling\n- Validation\n- Service instantiation (DI, etc)\n- Authentication\n- RESTful API's (Swagger\/OpenAPI a bonus)\n- GraphQL\n- Use of components\/services easily in a script\n\n## Decision\n- Documented application architecture.\n- Tooling that reduces boiler-plate and creates consistent code architecture.\n- Modern paradigms to ease creation of global objects and their testability, such as DI.\n- Training materials so that new developers can understand and work on application features easily.\n- Ability to migrate legacy applications to similar conventions\/setup.\n- Handles situations we need such as:\n- Exception handling\n- Validation\n- Service instantiation (DI, etc)\n- Authentication\n- RESTful API's (Swagger\/OpenAPI a bonus)\n- GraphQL\n- Use of components\/services easily in a script\nChosen Option: Use an existing framework: NestJS\nNestJS and LoopBack are the two most compelling options, however NestJS has substantially better GraphQL support and a much larger user-base. LoopBack has its user-base divided between two versions with substantial changes between them. We will use NestJS for new projects and update the newest services (admin-server, support-panel, event-broker, gql-api) to NestJS.\nTo reduce documentation needs and address that auth-server will not be reasonable to migrate, we will backport code organization schemes and directory\/filename conventions from NestJS and use `typedi` as a DI system to ease configuration\/testing in auth-server and possibly other legacy packages. This will allow developers familiar with NestJS projects to still locate and easily work on FxA legacy packages.\n### Positive Consequences\n- Less documentation to create, as we only document differences for legacy services.\n- NestJS GraphQL support matches existing typegraphql paradigm closely for minimal effort switch.\n### Negative Consequences\n- There will be some effort involved to migrate the 4 packages to NestJS.\n- We will need to document the differences between NestJS and other legacy services.\n\n\n\nProvide a decision given the context below:\n## Context\nThis ADR has been superseded by `ADR-0019`.\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\ntools it seemed prudent to explore additional options as well.\n| Workflow Tool   | Pros | Cons |\n|-----------------|------|------|\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\n\nmodel\nRaster Foundry will use Airflow as a task manager. There are a number of advantages over some of the\nalternatives. First, Airflow has a large, active, user base that have used it in production. The\nproject itself is in the Apache incubator, providing a strong signal that the project is of high quality\nSecond, Airflow's UI for monitoring task and workflow progress is great. It provides\nhigh-level relevant information that will enable us to diagnose issues quickly. Additionally, it\nprovides a means to view log output of tasks directly in the admin interface. Third, Airflow\nsupports both scheduled and ad hoc tasks. Lastly, Airflow's architecture would re-use many\ncomponents that will already be a part of Raster Foundry's infrastructure - including a Postgres\ndatabase and a redis cache.\n\n"}
