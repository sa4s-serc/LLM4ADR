{"File Name":"tacboard\/001-Selectors-and-Reselect.md","Context":"## Context\\nUsing [selectors](https:\/\/medium.com\/@matthew.holman\/what-is-a-redux-selector-a517acee1fe8) is a best practice for Redux-applications to derive information from a minimal state while keeping the `mapStateToProps` and  `mapDispatchToProps` functions small.\\nI will use this pattern, because I made good experience with them on our work-project.\\nI am considering to use the [Reselect](https:\/\/github.com\/reduxjs\/reselect) for this, because it seems to be the \"industry-standard\".\\nArguments for using Rselect:\\n- Reselect is very popular and has a lot of documentation on the official pages and on StackOverflow\\n- Reselect handles memoisation\\n- Adding reselect now is not much effort, because there are no selectors yet. Adding it later will require touching all selectors again.\\nArguments against using Reselect:\\n- Selectors can also be written as plain functions to achieve the same separation of state and derived data\\n- An additional library adds to the size of the bundle\\n- The state of the tacboard is not that complicate, I don't expect memoisation to become a problem soon (but who knows?)\\n- Many of the selectors can not be cachedefficiently, because they depend on component-props (e.g. whether a field is empty or not requires the field-index), see [Reselect-Docs](https:\/\/github.com\/reduxjs\/reselect#accessing-react-props-in-selectors)\\n","Decision":"I will use Reselect to rewrite the selectors. The only real downside seems to be the increased package-size, but that is not a big concern at this point.\\nIt seems more important to avoid having to rewrite everything later. There also is a way to get memoization for selectors that use component props (see [Reselect-Docs](https:\/\/github.com\/reduxjs\/reselect#sharing-selectors-with-props-across-multiple-component-instances)).\\n","tokens":313,"id":2592,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nAs part of the process of rewriting the admin components for link platform, we need to decide on what strategy we want\\nto use for including styling in our application.\\nThere were two main choices:\\n* CSS stylesheets\\n* CSS-in-JS\\n### Why CSS-in-JS?\\nThere are many benefits to CSS-in-JS which are nicely outlined in\\n[this article](\\'https:\/\/medium.com\/seek-blog\/a-unified-styling-language-d0c208de2660\\'):\\n* Enforces fundamental scoping without relying on pure convention (automatically generated classes).\\n* Renders critical CSS baked in, as components bring their styles with them automatically.\\n* Removes the necessity to hard code class names.\\n* Increases reusability of styles.\\n* Improves maintainability and dead code detection\\n### Why Fela?\\n[Fela](https:\/\/github.com\/rofrischmann\/fela) is a high performance, framework agnostic\\ntool to handle state-driven styling in Javascript. It also uses Atomic Class design to\\ngenerate class names, and creates stylesheets with those class names, which allows for\\nmodular style reuse. Fela performs about\\n[four times faster](https:\/\/github.com\/A-gambit\/CSS-IN-JS-Benchmarks\/blob\/master\/RESULT.md)\\nthan Glamorous. Fela has also been used by Zendesk engineers in the past and so comes\\nwith the added bonus of having some internal knowledge already in place to hit the\\nground running.\\n\\n## Decision\\nWe will use CSS-in-JS, in the form of Fela and React-Fela.\\n\\n\\n## Context\\nWe are rewriting our client-side from scratch. That gives us the opportunity to restructure and rethink our css classes.\\nWe discussed two CSS schemas `naming classes as components` vs `naming classes according to visual properties`.\\nNaming classes as components - This will make it more clear where the css classes are used, but will duplicate what we already reflect in our markup. A variation of this would be to use the [Block-Element-Modifier (BEM)](http:\/\/getbem.com\/introduction\/) pattern.\\nNaming classes as styling - This will make the CSS classes reusable and fits well with the naming schema of our CSS framework, [Bulma](https:\/\/bulma.io\/). Examples of this schema are the [Atomic CSS](https:\/\/github.com\/nemophrost\/atomic-css) schema and [Tailwind CSS](https:\/\/tailwindcss.com\/docs\/what-is-tailwind\/). There were several small concerns with this style:\\n* The style makes our markup less performant: because each element has several classes, the resulting size will be bigger. Since the markup will be compressed anyway and the compression algorithm is efficient for reducing repeating patterns, this is not an issue.\\n* We might end up with \"lying\" class names, e.g. `.is-red { color: blue}`. This will be mitigated by the style names having a slightly higher and more purpose-oriented schema, e.g. `.primary-color`. We strive to achieve a sweet spot between the extremes of BEM and Atomic CSS.\\n* The added classes make the HTML source less readable. \"Readability\", especially when compared to BEM, is kind of subjective.\\n\\n## Decision\\nWe will name CSS classes according to the styling they produce instead of the component they are used in. Adding component class names to Vue components feels like a violation of the [DRY principle](https:\/\/en.wikipedia.org\/wiki\/Don%27t_repeat_yourself).\\nWe will share as much of those styling classes between components as possible, instead of each component defining their own styles.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nUsing [selectors](https:\/\/medium.com\/@matthew.holman\/what-is-a-redux-selector-a517acee1fe8) is a best practice for Redux-applications to derive information from a minimal state while keeping the `mapStateToProps` and  `mapDispatchToProps` functions small.\\\\nI will use this pattern, because I made good experience with them on our work-project.\\\\nI am considering to use the [Reselect](https:\/\/github.com\/reduxjs\/reselect) for this, because it seems to be the \"industry-standard\".\\\\nArguments for using Rselect:\\\\n- Reselect is very popular and has a lot of documentation on the official pages and on StackOverflow\\\\n- Reselect handles memoisation\\\\n- Adding reselect now is not much effort, because there are no selectors yet. Adding it later will require touching all selectors again.\\\\nArguments against using Reselect:\\\\n- Selectors can also be written as plain functions to achieve the same separation of state and derived data\\\\n- An additional library adds to the size of the bundle\\\\n- The state of the tacboard is not that complicate, I don\\'t expect memoisation to become a problem soon (but who knows?)\\\\n- Many of the selectors can not be cachedefficiently, because they depend on component-props (e.g. whether a field is empty or not requires the field-index), see [Reselect-Docs](https:\/\/github.com\/reduxjs\/reselect#accessing-react-props-in-selectors)\\\\n'}, {'role': 'model', 'content': 'I will use Reselect to rewrite the selectors. The only real downside seems to be the increased package-size, but that is not a big concern at this point.\\\\nIt seems more important to avoid having to rewrite everything later. There also is a way to get memoization for selectors that use component props (see [Reselect-Docs](https:\/\/github.com\/reduxjs\/reselect#sharing-selectors-with-props-across-multiple-component-instances)).\\\\n'}]"}
{"File Name":"iampeterbanjo.com\/0004-yarn-npm-pnpm-and-docker.md","Context":"## Context\\nI started off using `yarn` workspaces because it was convenient for running scripts inside packages using `--cwd`. But it has no `audit fix` and deployments started failing because of missing packages.\\n```shell\\n2019-08-09T07:25:06+01:00 code: 'MODULE_NOT_FOUND',\\n2019-08-09T07:25:06+01:00 at Module.require (internal\/modules\/cjs\/loader.js:683:19) {\\n2019-08-09T07:25:06+01:00 at Module.load (internal\/modules\/cjs\/loader.js:643:32)\\n```\\nI tried [changing node engines][node-issue] which failed. I switched to `pnpm` because it had a reputation for solving dependency issues but I ran into problems with the way Hapi and its dependencies get [dynamically imported][hapi-issue].\\nIn order to use `pnpm` I had to use Docker deployments because [Clever cloud][clever-cloud] only support `npm` and `yarn` package managers. I could not get `pnpm` to [work with Docker][docker-issue] (recursive installs kept failing) but `npm` worked fine. I decided to keep Docker for learning and portablity purposes.\\n","Decision":"In the context of deployments failing because of missing packages. And facing the concern of wanting to have this projects' dependencies reliably installed in production, I've adopted `npm` as the default package manager. I accept that I will have to write more scripts to maintain the monorepo structure. Docker is no longer necessary but is convenient for portability and learning.\\n","tokens":277,"id":1592,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nOpenShift likes to shift (pun intended) the UID -- i.e., assign arbitrary UIDs -- to containers. They do this as an additional security feature, given that OpenShift is a multi-tentant Kubernetes solution. Each OpenShift project received a non-overlapping UID range. Hence, in case an attacker escapes a container, it will be more difficult to interfere with other processes.\\nHowever, this shifting of UIDs introduces an additional complexity: What if a process wants to write to the filesystem? What uid, gid and permissions should the files and folders have? To solve this problem, the OpenShift documentation (see [\"Support arbitrary user ids\"][openshift-docs]) recommends setting gid=0 on those files and folders. Specifically, the Dockerfiles of the container images should contain:\\n```Dockerfile\\nRUN chgrp -R 0 \/some\/directory && chmod -R g=u \/some\/directory\\n```\\nDuring execution, OpenShift assigns `gid=0` as a supplementary group to containers, so as to give them access to the required files.\\nIn contrast to OpenShift, Compliant Kubernetes is not a multi-tenant solution. Given previous vulnerabilities in Kubernetes that affected tenant isolation (e.g., [CVE-2020-8554][cve]\\n), we believe that non-trusting users should not share a workload cluster. Hence, we do not assign arbitrary UIDs to containers and do not need to assign `gid=0` as a supplementary group.\\nThe `gid=0` practice above seems to have made its way in [quite a few Dockerfiles][github-search], however, it is far from being the default outside OpenShift.\\nWhat should Compliant Kubernetes do with the `gid=0` practice?\\n## Decision Drivers\\n* For user expectations, we want to make it easy to start with Compliant Kubernetes.\\n* For better security and easier audits, we do not want to add unnecessary permissions.\\n* [ID mapping in mounts][idmapping] has landed in Linux 5.12. Once this feature is used in container runtimes and Kubernetes, the `gid=0` problem will go away.\\n\\n## Decision\\n* For user expectations, we want to make it easy to start with Compliant Kubernetes.\\n* For better security and easier audits, we do not want to add unnecessary permissions.\\n* [ID mapping in mounts][idmapping] has landed in Linux 5.12. Once this feature is used in container runtimes and Kubernetes, the `gid=0` problem will go away.\\nChosen option: \"disallow `gid=0` by default\". Enabling it on a case-by-case basis is okay.\\n### Positive Consequences\\n* We do not unnecessarily add a permission to containers.\\n### Negative Consequences\\n* Some users will complain about their container images not starting, and we will need to add a less restricted PodSecurityPolicy in their cluster.\\n\\n\\n## Context\\nAs per [ADR-0003](adr-0003-nix-for-dependency-management.md) we are using nix to manage our\\ndependencies.\\nGood nix development involves switching into a `nix-shell` which contains the appropriate libraries\\n*and* binaries required to build your program. This is handy in a terminal but is tricky for an\\neditor as editors often require access to the same binaries to enable fancy features like\\nauto-completion and jump-to-function.\\nCurrently I\\'m using emacs. I _could_ open emacs from within a `nix-shell` but I want to be able\\nto switch between buffers from other projects without breaking everything.\\nOne option which I\\'ve used in the past is to leverage [direnv](https:\/\/github.com\/direnv\/direnv).\\nDirenv is a tool that can modify your environment variables on a per-directory basis. Crucially it supports nix and has an emacs plugin. We just need to create an `.envrc` containing `use nix`.\\nAnother option is to leverage [nix-buffer](https:\/\/github.com\/shlevy\/nix-buffer) which seems\\nto require the creation of a `.dir-locals.el`. This approach seems promising but I haven\\'t\\nbeen able to make sense of the fairly sparse documentation and as a result I\\'ll be discarding\\nthis option for now.\\n\\n## Decision\\nI will use direnv to integrate editors with nix\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nI started off using `yarn` workspaces because it was convenient for running scripts inside packages using `--cwd`. But it has no `audit fix` and deployments started failing because of missing packages.\\\\n```shell\\\\n2019-08-09T07:25:06+01:00 code: \\'MODULE_NOT_FOUND\\',\\\\n2019-08-09T07:25:06+01:00 at Module.require (internal\/modules\/cjs\/loader.js:683:19) {\\\\n2019-08-09T07:25:06+01:00 at Module.load (internal\/modules\/cjs\/loader.js:643:32)\\\\n```\\\\nI tried [changing node engines][node-issue] which failed. I switched to `pnpm` because it had a reputation for solving dependency issues but I ran into problems with the way Hapi and its dependencies get [dynamically imported][hapi-issue].\\\\nIn order to use `pnpm` I had to use Docker deployments because [Clever cloud][clever-cloud] only support `npm` and `yarn` package managers. I could not get `pnpm` to [work with Docker][docker-issue] (recursive installs kept failing) but `npm` worked fine. I decided to keep Docker for learning and portablity purposes.\\\\n'}, {'role': 'model', 'content': \"In the context of deployments failing because of missing packages. And facing the concern of wanting to have this projects' dependencies reliably installed in production, I've adopted `npm` as the default package manager. I accept that I will have to write more scripts to maintain the monorepo structure. Docker is no longer necessary but is convenient for portability and learning.\\\\n\"}]"}
{"File Name":"pay-connector\/adr-002-decrypting-google-pay-token-not-required.md","Context":"## Context\\nThere are two possible tokenization methods when making a payment request to Google Pay: [PAYMENT_GATEWAY and DIRECT](https:\/\/developers.google.com\/pay\/api\/web\/guides\/tutorial#tokenization).\\nFeatures of PAYMENT_GATEWAY tokenization:\\n- Merchant id identifying GOV.UK Pay with Google Pay needs to be set up\\n- We simply pass the encrypted response from Google Pay through to the payment gateway as is\\n- No need of the operational overhead of key management as is already handled between Google Pay and the payment gateway\\nFeatures of DIRECT tokenization:\\n- No need for a Merchant id between GOV.UK Pay and Google Pay\\n- Need to be PCI DSS compliant (which we already are)\\n- Need to decrypt the response to get information to send to the payment gateway\\n- Need the operational overhead of key management as we need to [roll keys once a year and register these with Google Pay](https:\/\/developers.google.com\/pay\/api\/web\/guides\/resources\/payment-data-cryptography#key-management)\\nWith either tokenization method, we get a [number of data](https:\/\/developers.google.com\/pay\/api\/web\/reference\/object#response-objects), but the ones relevant to us are:\\n- Last 4 card digits\\n- Billing address (which includes name)\\n- Email address\\n- Card Network\\nWith DIRECT tokenization we decrypt to get:\\n- PAN\\n- expiration month and year\\n[and other things](https:\/\/developers.google.com\/pay\/api\/web\/guides\/resources\/payment-data-cryptography#encrypted-message).\\nChoosing whether to decrypt the payload from Google Pay or not is informed by the following two deal breakers:\\n- We need to be able to retrieve the cardholder name, email, and last 4 digits of a card\\n- We don't want services to have to manage any sort of keys\/certificates\\n","Decision":"Unlike the current implementation of Apple Pay, we have chosen to go with not decrypting the payload for the following reasons:\\n- We get all the information we require (Last 4 card digits, billing address name, email address, card network) in unencrypted form\\n- Services won't need to manage any keys\/certificates. This is because when a payment request is made to Google Pay we specify the payment gateway, e.g. \"Worldpay\".\\nGoogle Pay will return a response encrypted with the payment gateway's public key.\\nThis response is decryptable by the payment gateway as they have their private encryption key.\\nSo there is already a key management process between Google Pay and the payment gateway.\\nOur initial assumption that Google Pay encrypts with the merchant id's public key was incorrect.\\n- No operational process is needed on our part to manage keys\/certs.\\n","tokens":384,"id":2685,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe cloud platforms team currently use Docker Registry for storage of docker images. This solution is self-hosted, needs regular patching, and occasionally has downtime.\\nExample of an issue [CPT-274](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-274).\\nWe want to update the container registry to avoid some of the problems we have been seeing with it. The container registry will also be a key part of our new [Kubernetes based infrastructure](https:\/\/github.com\/ministryofjustice\/cloud-platform\/blob\/master\/architecture-decision-record\/004-use-kubernetes-for-container-management.md).\\nThe criteria for selecting a new solution included:\\n* Finding a solution that would work with well GitHub based identity which is one of our [major architectural choices](https:\/\/github.com\/ministryofjustice\/cloud-platform\/blob\/master\/architecture-decision-record\/006-Use-github-as-user-directory.md)\\n* Decreasing the amount of day to day ops work, perhaps by using a managed service\\n* Good value for money\\n* Preference for open source tooling\\n* Can or is able to run addons i.e. security scanners\\n* We can make images that we host public by default\\nFrom this process we tentatively chose ECR. Unfortunately none of the SaaS registries in common use support Github authentication, but as ECR uses IAM for authentication, and IAM supports federated identity with Github via Auth0, it will in theory be possible to define ECR access policies that reference Github teams and roles.\\n\\n## Decision\\nWe will replace our self hosted Docker Registry to managed Elastic Container Registry (ECR).\\n\\n\\n## Context and Problem Statement\\nThe current reference implementation will ignore unrecognized fields in a\\nmetadata file when loading it.\\nThis leads to the side effect that if you read a metadata file with unrecognized\\nfields and immediately write it back to the disk, this file will be modified.\\nFurthermore, some TAPs like:\\n- [TAP 6](https:\/\/github.com\/theupdateframework\/taps\/blob\/master\/tap6.md)\\n- [TAP 10](https:\/\/github.com\/theupdateframework\/taps\/blob\/master\/tap10.md)\\n- [TAP 14](https:\/\/github.com\/theupdateframework\/taps\/blob\/master\/tap14.md)\\n- [TAP 15](https:\/\/github.com\/theupdateframework\/taps\/blob\/master\/tap15.md)\\n- [TAP 16](https:\/\/github.com\/theupdateframework\/taps\/blob\/master\/tap16.md)\\nare relying on that unrecognized fields will be accepted to introduce new fields\\nto the specification without making the metadata invalid for older clients who\\ndon\\'t recognize the field.\\n## Decision Drivers\\n- The TUF specification implies support for unrecognized attribute-value fields,\\nsee [Document formats](https:\/\/theupdateframework.github.io\/specification\/latest\/#document-formats)\\n- If we perform the following operations on a metadata file with no\\nintermediate operations:\\n1. read the metadata file\\n2. write the metadata file back to the disk\\nthen, the checksum (the content) of the file must not be changed.\\n- Flexibility to add new fields in the spec without adding breaking changes.\\n- Don\\'t store unrecognized fields when it is not allowed by the specification.\\n\\n## Decision\\n- The TUF specification implies support for unrecognized attribute-value fields,\\nsee [Document formats](https:\/\/theupdateframework.github.io\/specification\/latest\/#document-formats)\\n- If we perform the following operations on a metadata file with no\\nintermediate operations:\\n1. read the metadata file\\n2. write the metadata file back to the disk\\nthen, the checksum (the content) of the file must not be changed.\\n- Flexibility to add new fields in the spec without adding breaking changes.\\n- Don\\'t store unrecognized fields when it is not allowed by the specification.\\nChosen option: \"Ignore, but store unrecognized fields as an additional attribute\\nexcept for a couple of places where it\\'s not allowed by the specification.\"\\nThe motivation for this decision is that the TUF specification already implies\\nthat we should accept unrecognized fields for backward compatibility and easier\\nfuture extensibility.\\nAdditionally, it seems unacceptable to change a metadata file content just by\\nreading and writing it back.\\nThere are exceptions however for places in the metadata format when it is not\\nallowed by specification: keys, roles, meta, hashes, and targets are\\nactual dictionaries (vs JSON objects that most structures in the format are)\\nwhere `unrecognized field` is not a meaningful concept.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThere are two possible tokenization methods when making a payment request to Google Pay: [PAYMENT_GATEWAY and DIRECT](https:\/\/developers.google.com\/pay\/api\/web\/guides\/tutorial#tokenization).\\\\nFeatures of PAYMENT_GATEWAY tokenization:\\\\n- Merchant id identifying GOV.UK Pay with Google Pay needs to be set up\\\\n- We simply pass the encrypted response from Google Pay through to the payment gateway as is\\\\n- No need of the operational overhead of key management as is already handled between Google Pay and the payment gateway\\\\nFeatures of DIRECT tokenization:\\\\n- No need for a Merchant id between GOV.UK Pay and Google Pay\\\\n- Need to be PCI DSS compliant (which we already are)\\\\n- Need to decrypt the response to get information to send to the payment gateway\\\\n- Need the operational overhead of key management as we need to [roll keys once a year and register these with Google Pay](https:\/\/developers.google.com\/pay\/api\/web\/guides\/resources\/payment-data-cryptography#key-management)\\\\nWith either tokenization method, we get a [number of data](https:\/\/developers.google.com\/pay\/api\/web\/reference\/object#response-objects), but the ones relevant to us are:\\\\n- Last 4 card digits\\\\n- Billing address (which includes name)\\\\n- Email address\\\\n- Card Network\\\\nWith DIRECT tokenization we decrypt to get:\\\\n- PAN\\\\n- expiration month and year\\\\n[and other things](https:\/\/developers.google.com\/pay\/api\/web\/guides\/resources\/payment-data-cryptography#encrypted-message).\\\\nChoosing whether to decrypt the payload from Google Pay or not is informed by the following two deal breakers:\\\\n- We need to be able to retrieve the cardholder name, email, and last 4 digits of a card\\\\n- We don\\'t want services to have to manage any sort of keys\/certificates\\\\n'}, {'role': 'model', 'content': 'Unlike the current implementation of Apple Pay, we have chosen to go with not decrypting the payload for the following reasons:\\\\n- We get all the information we require (Last 4 card digits, billing address name, email address, card network) in unencrypted form\\\\n- Services won\\'t need to manage any keys\/certificates. This is because when a payment request is made to Google Pay we specify the payment gateway, e.g. \"Worldpay\".\\\\nGoogle Pay will return a response encrypted with the payment gateway\\'s public key.\\\\nThis response is decryptable by the payment gateway as they have their private encryption key.\\\\nSo there is already a key management process between Google Pay and the payment gateway.\\\\nOur initial assumption that Google Pay encrypts with the merchant id\\'s public key was incorrect.\\\\n- No operational process is needed on our part to manage keys\/certs.\\\\n'}]"}
{"File Name":"super-eks\/0003-use-yarn-workspaces.md","Context":"## Context\\nWe need to organize our code. We also want to run integration tests, and code examples that use the code as a 3rd party library.\\n","Decision":"We want a monorepo style setup and settle for yarn workspaces. Lerna seems to complicated at the moment.\\n","tokens":34,"id":2030,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe want to be confident about the code we write, and for it to be\\nself-documenting as much as possible. TypeScript is a compiled language with\\noptional typing. It's a superset of JavaScript, so is familiar to developers\\nwho know JavaScript. It has wide editor support.\\n\\n## Decision\\nWe will use TypeScript by default.\\n\\n\\n## Context\\nWe want automated ETE tests on build to keep things consistent. We also want this to be free - because free is good. CircleCI provides a free tier and solid docker support.\\n\\n## Decision\\nUse CircleCI for automated ETE tests\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need to organize our code. We also want to run integration tests, and code examples that use the code as a 3rd party library.\\\\n\"}, {'role': 'model', 'content': 'We want a monorepo style setup and settle for yarn workspaces. Lerna seems to complicated at the moment.\\\\n'}]"}
{"File Name":"superwerker\/living-documentation.md","Context":"## Context\\nA dashboard with more information and deep-links to resources, e.g. setting up SSO with existing identity providers, GuardDuty\/Security Hub dashboards.\\n","Decision":"- Create a CloudWatch Dashboard called `superwerker` in the AWS management account. The CW dashboard a) ensures a deep link which can be used to link from the README.md and b) ensures the user is authorized to access the information.\\n- Display DNS delegation state and setup instructions\\n- Refresh dashboard with scheduler every minute since this removes the compexity to deal with event-based dashboard generation. Lambda invocations are completely covered by free-tier.\\n","tokens":36,"id":3399,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nGLAM requires authentication to protect sensitive metrics, make some metrics and\\nviews visible to non-Mozillians, and in the future to potentially add\\nuser-oriented features. Currently we handle the authentication in the app layer,\\nalbeit with some issues. This ADR attempts to resolve these issues.\\n## Decision Drivers\\n- Authentication should work without issue, including infinite redirects or\\nconflicts with enhanced tracking protection and 3rd party cookies.\\n- Authentication should allow some areas of the site to be public via URL\\nmatching rules.\\n- Authentication should pass information about the authenticated user to the\\nbackend for future use-cases.\\n\\n## Decision\\n- Authentication should work without issue, including infinite redirects or\\nconflicts with enhanced tracking protection and 3rd party cookies.\\n- Authentication should allow some areas of the site to be public via URL\\nmatching rules.\\n- Authentication should pass information about the authenticated user to the\\nbackend for future use-cases.\\nIn the GLAM meeting, nginx was chosen as the preferred option for the following\\nreasons:\\n- It removes code complexity and maintenance from the app layer.\\n- It would remove a necessary security review by using something that has\\nalready been reviewed.\\n- It offers the quickest path to fix the authentication issues we are facing.\\n- In the future it satifies the requirement of offering user-facing features.\\n- In the future it satifies the requirement of optional non-authenticated\\nendpoints.\\n\\n\\n## Context\\nKubernetes services can be exposed to the public using Kubernetes managed resources\\nthat can integrate with Cloud environments, such as the Loadbalancer service resource,\\nor with Cloud native services, such as AWS ELBs.\\nWhich solution to use will affect how we manage associated resources and processes, for\\ninstance EIPs and DNSs, HTTPS certificates, green-blue deployments, etc.\\nInitially we want to find a solution that meets some basic requirements:\\n- HTTP and HTTPS support\\n- Internal-only and Internet facing services\\n- Probably we want to route traffic to services based on request URI\\n\\n## Decision\\nInitially we tested a Loadbalancer Kubernetes service on AWS. This service configures a\\nclassic ELB that connects with the pod service port. It requires uses Kubernetes secrets\\nto read the HTTPS certificate but doesn't support request URI routing. It also needs a\\nfixed public IP to start the ELB and a DNS entry managed externally.\\nKubernetes Ingress resources allow us to define rules to route traffic to Kubernetes\\nservices. Google Cloud has native support for Ingress resources, but on AWS we need to deploy\\na controller service. On non-cloud environments (for instance, a local Minikube VM),\\nit is also possible to deploy a Ingress controller that can replicate the behaviour\\nexpected in the cloud. The catalogue of Ingress controllers can be found here:\\nhttps:\/\/github.com\/kubernetes\/ingress\/blob\/master\/docs\/catalog.md\\nWe tested Traefik and CoreOS ALB Ingress controller and finally decided to use the CoreOS\\nsolution:\\n- Traefik feedback?\\n- CoreOS ALB ingress controller https:\/\/github.com\/coreos\/alb-ingress-controller\\n- AWS managed certificates, with AWS Certificate Manager\\n- Uses AWS ALB loadbalancers\\n- Adds Route53 entries\\n- Can optionally access services only on specific namespaces\\nWe decided to use the CoreOS ALB controller because it uses AWS native solutions that meet our requirements.\\nAlso, it's supported by CoreOS.\\nOther links and discussions about loadbalancer support for Kubernetes on cloud platforms:\\n- http:\/\/www.sandtable.com\/a-single-aws-elastic-load-balancer-for-several-kubernetes-services-using-kubernetes-ingress\/\\n- Kubernetes discussion around Ingress\/AWS LBs support: https:\/\/github.com\/kubernetes\/kubernetes\/issues\/30518\\n- Internal ELB, useful for services that don't need to be open to the Internet: https:\/\/github.com\/kubernetes\/kops\/issues\/2011\\n- ELB and HTTPS: https:\/\/github.com\/kubernetes\/kubernetes\/issues\/22854\\n- Traefik working branch: https:\/\/github.com\/alphagov\/govuk-hosting-discovery\/compare\/traefik-ingress\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nA dashboard with more information and deep-links to resources, e.g. setting up SSO with existing identity providers, GuardDuty\/Security Hub dashboards.\\\\n\"}, {'role': 'model', 'content': '- Create a CloudWatch Dashboard called `superwerker` in the AWS management account. The CW dashboard a) ensures a deep link which can be used to link from the README.md and b) ensures the user is authorized to access the information.\\\\n- Display DNS delegation state and setup instructions\\\\n- Refresh dashboard with scheduler every minute since this removes the compexity to deal with event-based dashboard generation. Lambda invocations are completely covered by free-tier.\\\\n'}]"}
{"File Name":"gsp\/ADR030-aws-service-operator.md","Context":"## Context\\n[Amazon announced](https:\/\/aws.amazon.com\/blogs\/opensource\/aws-service-operator-kubernetes-available\/) an AWS Service Operator for Kubernetes in October 2018.\\nThere is a need for one or more users to provision SQS queues for their application to interact with.\\nThe AWS Service Operator consists of a container that sits in the Kubernetes cluster and monitors for custom resource types which it maps to the appropriate CloudFormation resources and deploys.\\nIt supports the following resources:\\n* CloudFormation Template\\n* DynamoDB\\n* ECR Repository\\n* ElastiCache\\n* S3 Bucket\\n* SNS Subscription\\n* SNS Topic\\n* SQS Queue\\n","Decision":"We have included AWS Service Operator as part of GSP so that users can provision SQS queues.\\n","tokens":137,"id":3881,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\n### Audit events\\nThe GOV.UK PaaS has components which expose events which can be used for\\nauditing. For example:\\n- BOSH director kernel audit logs\\n- BOSH director Credhub security events\\n- BOSH director UAA events\\n- Cloud Foundry UAA events\\n- Cloud Foundry Cloud Controller security events\\nThe BOSH director and BOSH managed instances store these logs in\\n`\/var\/vcap\/sys\/log\/`.\\n### Logging service\\nThe Cyber Security team run a centralised log ingestion system called the\\nCentral Security Logging Service (CSLS).\\nThis service runs in AWS and uses [CloudWatch log group subscription\\nfilters](https:\/\/docs.aws.amazon.com\/AmazonCloudWatch\/latest\/logs\/SubscriptionFilters.html)\\nto ingest logs, which are then sent to Splunk for indexing.\\n\\n## Decision\\nWe will use Terraform to create log groups in our existing pipelines:\\n- `create-bosh-concourse`\\n- `create-cloudfoundry`\\nWe will store audit logs in CloudWatch for 18 months (545 days).\\nWe will use Terraform to create log group subscription filters which will send\\nlogs to CSLS.\\nTerraform failing to create log group subscription filters should not block the\\nexecution of the pipeline, so our pipelines are not coupled to CSLS.\\nWe will run a fork of the\\n[awslogs-boshrelease](https:\/\/github.com\/alphagov\/paas-awslogs-boshrelease),\\non all instances that have relevant audit and security event logs, to send logs\\nfrom the instances to CloudWatch.\\nThe CloudWatch log groups will have subscription filters that will send logs\\nto CSLS so that logs are indexed in Splunk.\\n\\n\\n## Context\\nMy first instinct for the tool was to use the\\nNPM [Serverless](https:\/\/serverless.com\/) framework.\\nThe main advantage of serverless is that it is cloud\\nagnostic. However JavaScript is not widely used at GDS.\\nAWS [Chalice](https:\/\/chalice.readthedocs.io) is a similar\\nframework which has a multi-stage deploy process to\\nautomate provisioning Lambda and Api Gateway but written\\nin Python.\\nBoth languages have frameworks for accessing the AWS API:\\n* The [SDK](https:\/\/docs.aws.amazon.com\/sdk-for-javascript)\\nfor JavasSript and\\n* [Boto3](https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/index.html)\\nfor Python.\\n\\n## Decision\\nGiven that we had already made the decision that cloud\\nagnostic was not important for this service and that\\nPython was more in keeping with GDS common practice we\\ndecided to use Chalice.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\n[Amazon announced](https:\/\/aws.amazon.com\/blogs\/opensource\/aws-service-operator-kubernetes-available\/) an AWS Service Operator for Kubernetes in October 2018.\\\\nThere is a need for one or more users to provision SQS queues for their application to interact with.\\\\nThe AWS Service Operator consists of a container that sits in the Kubernetes cluster and monitors for custom resource types which it maps to the appropriate CloudFormation resources and deploys.\\\\nIt supports the following resources:\\\\n* CloudFormation Template\\\\n* DynamoDB\\\\n* ECR Repository\\\\n* ElastiCache\\\\n* S3 Bucket\\\\n* SNS Subscription\\\\n* SNS Topic\\\\n* SQS Queue\\\\n'}, {'role': 'model', 'content': 'We have included AWS Service Operator as part of GSP so that users can provision SQS queues.\\\\n'}]"}
{"File Name":"govuk-kubernetes-discovery\/0004-structure-for-terraform-projects.md","Context":"## Context\\nWe wanted to agree on our Terraform code organisation to manage resources in different stacks and\\navoid having to recreate things every time we refactor code.\\n","Decision":"- We want to separate code from data, so in the future we can opensource the code without disclosing our implementation details\\n- We want to be able to encrypt sensitive data in the repository: we want to support sensitive data encryption as part of the same\\nprocess, without having to manage secrets in a different repository, with different scripts, etc.\\n- We want to create Terraform modules to reuse code\\n- We want to separate Terraform code into different projects (stacks, tiers), each one representing a logical tier. This is specially\\nimportant to separate resources between GOV.UK applications.\\nThe initial solution presents three directories: data, modules and projects:\\n- The data directory contains a subdirectory per Terraform project, to store variable values that can be customised per environment.\\n- The data directory also contains \\_secrets files with sensitive data encrypted with 'sops'\\n- The modules directory contains a subdirectory per Terraform provider\\n- The projects directory contains the Terraform stacks\/tiers\\n```\\n\u251c\u2500\u2500 data\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gke-base\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gke-cluster\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 my-application\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 integration_secrets.json\\n\u251c\u2500\u2500 modules\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 google\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 container_cluster\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 dns_managed_zone\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 mysql_database_instance\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 mysql.tf\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 network\\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 network.tf\\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 public_subnetwork\\n\u2502\u00a0\u00a0             \u2514\u2500\u2500 main.tf\\n\u2514\u2500\u2500 projects\\n\u251c\u2500\u2500 gke-base\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 integration.backend\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u251c\u2500\u2500 gke-cluster\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 integration.backend\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u2514\u2500\u2500 my-application\\n\u251c\u2500\u2500 integration.backend\\n\u251c\u2500\u2500 main.tf\\n\u2514\u2500\u2500 variables.tf\\n```\\n","tokens":33,"id":2794,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe needed to make sure we're not overloading the servers with excessive\\nrequests. We wanted to find a way to serve fresh resources while keeping\\nthe back-end happy.\\n\\n## Decision\\nPage Previews will leverage the browser's cache rather than maintaining its own.\\nWe rely on Grade A browsers implementing HTTP caching correctly and their\\nvendors making accessing them as efficient as possible in order to avoid\\nincurring the incidental complexity of writing our own cache in JavaScript.\\nWe'll set appropriate `Cache-Control` HTTP headers for both the MediaWiki API,\\nvia [the `maxage` and `smaxage` main module parameters][0], and the RESTBase page\\nsummary endpoint with the help of the Services team.\\n\\n\\n## Context\\nThe bulk of the application is a processing pipeline for data feeds. We need a way to process streams of information.\\nWe considered apache-spark but we didn't think we  have enough data yet to justify the overhead of a spark installation.\\nConsidered Java\/ScalaRX, but we're not quite reactive - also due to the processing overhead of the crunch, the backpressure which\\nyou can get with Akka Streams seemed like a win\\n\\n## Decision\\nUse akka streams\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe wanted to agree on our Terraform code organisation to manage resources in different stacks and\\\\navoid having to recreate things every time we refactor code.\\\\n\"}, {'role': 'model', 'content': \"- We want to separate code from data, so in the future we can opensource the code without disclosing our implementation details\\\\n- We want to be able to encrypt sensitive data in the repository: we want to support sensitive data encryption as part of the same\\\\nprocess, without having to manage secrets in a different repository, with different scripts, etc.\\\\n- We want to create Terraform modules to reuse code\\\\n- We want to separate Terraform code into different projects (stacks, tiers), each one representing a logical tier. This is specially\\\\nimportant to separate resources between GOV.UK applications.\\\\nThe initial solution presents three directories: data, modules and projects:\\\\n- The data directory contains a subdirectory per Terraform project, to store variable values that can be customised per environment.\\\\n- The data directory also contains \\\\_secrets files with sensitive data encrypted with 'sops'\\\\n- The modules directory contains a subdirectory per Terraform provider\\\\n- The projects directory contains the Terraform stacks\/tiers\\\\n```\\\\n\u251c\u2500\u2500 data\\\\n\u2502\\xa0\\xa0 \u251c\u2500\u2500 gke-base\\\\n\u2502\\xa0\\xa0 \u2502\\xa0\\xa0 \u251c\u2500\u2500 common.tfvars\\\\n\u2502\\xa0\\xa0 \u2502\\xa0\\xa0 \u2514\u2500\u2500 integration.tfvars\\\\n\u2502\\xa0\\xa0 \u251c\u2500\u2500 gke-cluster\\\\n\u2502\\xa0\\xa0 \u2502\\xa0\\xa0 \u251c\u2500\u2500 common.tfvars\\\\n\u2502\\xa0\\xa0 \u2502\\xa0\\xa0 \u2514\u2500\u2500 integration.tfvars\\\\n\u2502\\xa0\\xa0 \u2514\u2500\u2500 my-application\\\\n\u2502\\xa0\\xa0     \u251c\u2500\u2500 common.tfvars\\\\n\u2502\\xa0\\xa0     \u251c\u2500\u2500 integration.tfvars\\\\n\u2502\\xa0\\xa0     \u2514\u2500\u2500 integration_secrets.json\\\\n\u251c\u2500\u2500 modules\\\\n\u2502\\xa0\\xa0 \u2514\u2500\u2500 google\\\\n\u2502\\xa0\\xa0     \u251c\u2500\u2500 container_cluster\\\\n\u2502\\xa0\\xa0     \u2502\\xa0\\xa0 \u251c\u2500\u2500 main.tf\\\\n\u2502\\xa0\\xa0     \u2502\\xa0\\xa0 \u2514\u2500\u2500 variables.tf\\\\n\u2502\\xa0\\xa0     \u251c\u2500\u2500 dns_managed_zone\\\\n\u2502\\xa0\\xa0     \u2502\\xa0\\xa0 \u2514\u2500\u2500 main.tf\\\\n\u2502\\xa0\\xa0     \u251c\u2500\u2500 mysql_database_instance\\\\n\u2502\\xa0\\xa0     \u2502\\xa0\\xa0 \u251c\u2500\u2500 mysql.tf\\\\n\u2502\\xa0\\xa0     \u2502\\xa0\\xa0 \u2514\u2500\u2500 variables.tf\\\\n\u2502\\xa0\\xa0     \u2514\u2500\u2500 network\\\\n\u2502\\xa0\\xa0         \u251c\u2500\u2500 network.tf\\\\n\u2502\\xa0\\xa0         \u2514\u2500\u2500 public_subnetwork\\\\n\u2502\\xa0\\xa0             \u2514\u2500\u2500 main.tf\\\\n\u2514\u2500\u2500 projects\\\\n\u251c\u2500\u2500 gke-base\\\\n\u2502\\xa0\\xa0 \u251c\u2500\u2500 integration.backend\\\\n\u2502\\xa0\\xa0 \u251c\u2500\u2500 main.tf\\\\n\u2502\\xa0\\xa0 \u2514\u2500\u2500 variables.tf\\\\n\u251c\u2500\u2500 gke-cluster\\\\n\u2502\\xa0\\xa0 \u251c\u2500\u2500 integration.backend\\\\n\u2502\\xa0\\xa0 \u251c\u2500\u2500 main.tf\\\\n\u2502\\xa0\\xa0 \u2514\u2500\u2500 variables.tf\\\\n\u2514\u2500\u2500 my-application\\\\n\u251c\u2500\u2500 integration.backend\\\\n\u251c\u2500\u2500 main.tf\\\\n\u2514\u2500\u2500 variables.tf\\\\n```\\\\n\"}]"}
{"File Name":"operational-data-hub\/0045-code-config-separation.md","Context":"## Context\\nBoth Code and Configuration reside in source code control (Github in our case). This makes it very easy to mix-up code and configuration. However, these 2 should be clearly separated. Where possible code can be reused, but configuration is most of the times instance specific.\\n","Decision":"Code and Configuration is clearly separated. At deployment time the CI\/CD tools are responsible for bringin code and config together and deploy the code together with the correct configuration.\\n","tokens":58,"id":2741,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe ADR flow tool is a command line tool for managing ADR files. It is aimed to be used in command line, in MS-Windows primarily, but also other platforms.\\nAs a guiding principle, and in order to preserve simplicity, we don't want to introduce a separate data store. Data is stored and managed in files. This includes configuration, which should be explicit and easily identifiable and editable.\\nGiven this, in [ADR 1](1-ADR_directory_identified_by_marker_file.md), we decided that the ADR directory is identified by a file called `.adr`. This file will also contain configuration of the tool. This is to reduce the number of moving parts; so the marker file (`.adr`) is used also as a configuration file.\\nPart of the value statement is for these files to be available in source control, and easily shared and collaborated on by a team. This means the marker file needs to be version-controlled as well.\\nAt the same time, the configuration may change on a per-developer basis. For example, the location of the editor used to edit ADRs.\\n### Alternative 1: Introduce a Separate Configuration File\\nThis means that we, by definition, and always will have a separate configuraton file, on top of an empty marker file.\\nThis makes sharing configuration slightly harder, and makes it hard to share some of the configuration and customize others.\\n### Alternative 2: Use Environment Variables\\nWith this approach we configure the system using environment variables.\\nThis is OS-dependenet, and makes the configuration somewhat less explicit. It's also harder to share across a team where necessary.\\n\\n## Decision\\nWe will keep the `.adr` file, with its current dual role - a marker file, and a *shared* configuration file.\\nIn addition, we'll add the option to supply a `local.adr` file, with the same format as the `.adr` file, and allow a developer to specify properties to override there.\\nProperties that are specified only in `.adr` will have their values taken from there.\\n\\n\\n## Context\\nMenes supports C# 8.0's nullable references feature. In most cases, libraries need to use some of the attributes from the `System.Diagnostics.CodeAnalysis` namespace that enable to you provide sufficient information for the compiler's null analysis to do a good job.\\nThese attributes are not available in `netstandard2.0`. However, there is a standard workaround: define your own copies of these attributes and use those. We are using the `Nullable` NuGet package to do this for us. This works nicely, enabling applications targeting older runtimes still to enable nullable references.\\nThe problem is that you don't want to use this workaround unless you have to. Newer versions of .NET Core and .NET Standard have these attributes, so it's just a waste of space to define your own.\\n\\n## Decision\\nMenes will target both .NET Standard 2.0 and .NET Standard 2.1. The .NET Standard 2.0 version brings its own copies of the attributes, the .NET Standard 2.1 version relies on the ones built into the framework.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nBoth Code and Configuration reside in source code control (Github in our case). This makes it very easy to mix-up code and configuration. However, these 2 should be clearly separated. Where possible code can be reused, but configuration is most of the times instance specific.\\\\n\"}, {'role': 'model', 'content': 'Code and Configuration is clearly separated. At deployment time the CI\/CD tools are responsible for bringin code and config together and deploy the code together with the correct configuration.\\\\n'}]"}
{"File Name":"event-routing-backends\/0004-transformers-architecture.md","Context":"Context\\n-------\\nWe can develop event transformers either using the \u201cBackend\u201d architecture\\nor \u201cProcessor\u201d architecture. Making the transformers \u201cbackends\u201d will result\\nin relatively more nesting in the configurations as this \u201cbackend\u201d will have\\nits own configurations.\\nIf we decide to develop the event transformers as \u201cprocessors\u201d, it will result\\nin less complexity in the code since the transformer can be easily appended in\\nany backend\u2019s (router or logger) processors\u2019 pipeline.\\nDecision\\n--------\\nTransformers will be developed as event processors that can be added in\\nany backend\u2019s pipeline. Then the transformed events can be used for any purpose,\\neither for simply logging using the LoggerBackend or to route events using\\nEventRoutingBackend.\\nConsequences\\n------------\\nDeveloping transformers as processors will result in relatively less complex\\nconfigurations and it would provide us wider range of use cases for the transformers.\\n","Decision":"--------\\nTransformers will be developed as event processors that can be added in\\nany backend\u2019s pipeline. Then the transformed events can be used for any purpose,\\neither for simply logging using the LoggerBackend or to route events using\\nEventRoutingBackend.\\nConsequences\\n------------\\nDeveloping transformers as processors will result in relatively less complex\\nconfigurations and it would provide us wider range of use cases for the transformers.\\n","tokens":188,"id":4478,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\nContext\\n-------\\nThe purpose of this library is to collect together data from one or more sensor devices, save that data to a database and periodically create and send batches of data for processing.\\nShould the library be setup to handle multiple devices, checking and logging data from each device in turn or should that be left to the application using the library?\\nPros for Multi device\\n-------\\n1. easier for application developer - just provide the access details of each device and away you go.\\nPros for Single device\\n-------\\n1. Each device may well have different timings and manage data in different ways. At two extremes:\\n- data is stored on the device and collected daily\\n- device requires sampling at 10 second intervals\\nThis level of complexity is better handled by the application.\\n2. If there are delays in collecting data from one device, this could have a knock on effect in the collection interval of another.\\n3. If one goes down they all go down.\\nDecision\\n----------\\nKeep it Simple - Library written for one device.  In future may add settings to accomodate threading if this proves useful.\\nConsequences\\n--------------\\n\\n## Decision\\n----------\\nKeep it Simple - Library written for one device.  In future may add settings to accomodate threading if this proves useful.\\nConsequences\\n--------------\\n\\n\\nContext\\n-------\\nWe have a number of applications and servers that need numerous Perl libraries\\ninstalled. We have historically managed the libraries at a system level and\\ncoordinated versions with application teams. While there are now compelling\\noptions like [Carton](https:\/\/metacpan.org\/pod\/Carton), which allow an\\nappliction to manage and isolate its dependencies, most of our apps would\\nrequire changes to be ready to use something like this.\\nTo aid in keeping the versions stable and ensuring that underlying system\\nlibraries like OpenSSL are in sync, Debian packages many CPAN libraries at\\nspecific versions for a release. These are still at a system level, but we\\nhave an already-running process that would catch updates in the case of a\\nmajor bug fix or security release.\\nIn service of completing OS and hardware upgrades, we are seeking primarily to\\nidentify the dependencies and to be able to reproduce our application\\nenvironments. In the long term, applications are likely to be containerized or\\notherwise placed under PSGI\/Plack, where many of these concerns could be\\nisolated more readily. This decision is a record of current realities, rather\\nthan a long-term position.\\nDecision\\n--------\\nWhere possible, when installing Perl libraries at the system level, we will\\nuse the Debian-released package. Where practical, we will use\\n[dh-make-perl](https:\/\/manpages.debian.org\/stretch\/dh-make-perl\/dh-make-perl.1p.en.html)\\nto create local .deb packages from those in CPAN, but not released by Debian.\\nWe will use `nebula::cpan` as a last resort.\\nConseqeuences\\n-------------\\nUsing Debian-released libraries means that we are limited to the versions\\nselected in their process. This could be a stabilizing or limiting force.\\nProducing our own packages requires additional maintenance and monitoring.\\nUsing bare CPAN installs could result in different versions on different\\nsystems, depending on when the installation happens.\\n\\n## Decision\\n--------\\nWhere possible, when installing Perl libraries at the system level, we will\\nuse the Debian-released package. Where practical, we will use\\n[dh-make-perl](https:\/\/manpages.debian.org\/stretch\/dh-make-perl\/dh-make-perl.1p.en.html)\\nto create local .deb packages from those in CPAN, but not released by Debian.\\nWe will use `nebula::cpan` as a last resort.\\nConseqeuences\\n-------------\\nUsing Debian-released libraries means that we are limited to the versions\\nselected in their process. This could be a stabilizing or limiting force.\\nProducing our own packages requires additional maintenance and monitoring.\\nUsing bare CPAN installs could result in different versions on different\\nsystems, depending on when the installation happens.\\n\\n\\n\\nProvide a decision given the context below:\\nContext\\\\n-------\\\\nWe can develop event transformers either using the \u201cBackend\u201d architecture\\\\nor \u201cProcessor\u201d architecture. Making the transformers \u201cbackends\u201d will result\\\\nin relatively more nesting in the configurations as this \u201cbackend\u201d will have\\\\nits own configurations.\\\\nIf we decide to develop the event transformers as \u201cprocessors\u201d, it will result\\\\nin less complexity in the code since the transformer can be easily appended in\\\\nany backend\u2019s (router or logger) processors\u2019 pipeline.\\\\nDecision\\\\n--------\\\\nTransformers will be developed as event processors that can be added in\\\\nany backend\u2019s pipeline. Then the transformed events can be used for any purpose,\\\\neither for simply logging using the LoggerBackend or to route events using\\\\nEventRoutingBackend.\\\\nConsequences\\\\n------------\\\\nDeveloping transformers as processors will result in relatively less complex\\\\nconfigurations and it would provide us wider range of use cases for the transformers.\\\\n'}, {'role': 'model', 'content': '--------\\\\nTransformers will be developed as event processors that can be added in\\\\nany backend\u2019s pipeline. Then the transformed events can be used for any purpose,\\\\neither for simply logging using the LoggerBackend or to route events using\\\\nEventRoutingBackend.\\\\nConsequences\\\\n------------\\\\nDeveloping transformers as processors will result in relatively less complex\\\\nconfigurations and it would provide us wider range of use cases for the transformers.\\\\n'}]"}
{"File Name":"CICD-pipeline\/008-backwards-compatibility.md","Context":"## Context\\n1) We have commonly used existing Jenkins libraries\\n1) We want to be able to migrate a large number of projects with minimal effort\\n","Decision":"We provide a compatibility layer for the current library\\n","tokens":31,"id":2770,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nWe want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI\/CD.\\n## Decision Drivers\\n* Low complexity and overhead in development.\\n* Fit for our stack.\\n* Optimize CI\/CD with dependency graphs and\/or caching.\\n* Flexible.\\n\\n## Decision\\n* Low complexity and overhead in development.\\n* Fit for our stack.\\n* Optimize CI\/CD with dependency graphs and\/or caching.\\n* Flexible.\\nChosen option: \"Nx\", because:\\n* It\\'s specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).\\n* It\\'s relatively easy to learn with focused documentation.\\n* It has schematics to generate apps, libraries and components that includes all of our tools.\\n* It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.\\n\\n\\n## Context and Problem Statement\\nWe want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI\/CD.\\n## Decision Drivers\\n* Low complexity and overhead in development.\\n* Fit for our stack.\\n* Optimize CI\/CD with dependency graphs and\/or caching.\\n* Flexible.\\n\\n## Decision\\n* Low complexity and overhead in development.\\n* Fit for our stack.\\n* Optimize CI\/CD with dependency graphs and\/or caching.\\n* Flexible.\\nChosen option: \"Nx\", because:\\n* It\\'s specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).\\n* It\\'s relatively easy to learn with focused documentation.\\n* It has schematics to generate apps, libraries and components that includes all of our tools.\\n* It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\n1) We have commonly used existing Jenkins libraries\\\\n1) We want to be able to migrate a large number of projects with minimal effort\\\\n'}, {'role': 'model', 'content': 'We provide a compatibility layer for the current library\\\\n'}]"}
{"File Name":"amf-core\/0007-no-spec-mediatypes-in-amf-operations.md","Context":"## Context\\nOn AMF 5 Beta's first iteration \"domain+syntax\" mediatypes like \"application\/swagger20+yaml\" were\\nused to decide how the syntax and the domain were to be parsed. This was especially useful to\\nvalidate, transform and render units using compound configurationts (API, WebAPI, RAML, OAS)\\nThis is controversial as:\\n- Resulting mediatypes are strange to the end-user and are not standard. Besides we could\\nonly handle a specific ordering in domain and syntax. A mediatype formed by syntax+domain\\ncouldn't be parsed.\\n- Clients that used a specific configuration like RAML10 or OAS30 had to specify the mediatype\\nalthough the configuration they used already specified their intended domain and syntax.\\n","Decision":"Remove those compound mediatypes and instead only keep them for syntax purposes when needed.\\n","tokens":165,"id":408,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe OpenAPI generator has to generate names for APIs, their operations and schemas.\\nWe initially used the file name to generate an API name, but found that for bigger APIs this is would result in very complex APIs, therefore we need a more sensible default.\\nIn addition API owners whish to be able to customize names of APIs and operations.\\nTo follow naming conventions in JavaScript\/TypeScript we planned on formatting API names as pascal case, operation names as camel case and property and parameter names as camel case.\\nFor properties and parameters we would have to resolve the original names to use them in the requests.\\nWe found that for most cases this is straight forward, but there are corner cases that require a different\/more complex architecture.\\n### Issues with properties and parameters\\nGiven the following two types and their original properties, if those were used in combination, e. g. `allOf`, there is no possibility to set both values.\\n```ts\\ntype A {\\nsomeProperty: string; \/\/ original name: \\'some_property`\\n}\\ntype A {\\nsomeProperty: string; \/\/ original name: \\'some-property`\\n}\\n```\\nThis could be solved by making names unique throughout the whole service.\\nHowever this again would lead to less convenient names, e.g. `name`, `name1`, `name2`.\\n\\n## Decision\\n### API naming\\nAPIs are named based on the first tag, if there are any.\\nIf there is no tag, a \"default\" tag is used.\\nTo allow customizing of APIs, we introduced the `x-sap-cloud-sdk-api-name` extension, which takes precedence over the default name generation.\\nThis extension can be set on on multiple levels:\\n- operation, causing this operation to be treated as part of the given API.\\n- path, causing all operations under this path to be treated as part of the given API.\\n- document root, causing all operations in the document to be treated as part of the given API.\\nAll names are transformed to pascal case (`SomeApi`).\\nAPI names are formed by appending an \"Api\" suffix.\\nIf the original name ends with an \"api\" suffix (case independent), the original suffix is removed prior to appending \"Api\".\\nExamples:\\n- `my-api` => `MyApi`\\n- `my` => `MyApi`\\n- `my-api-api` => `MyApiApi`\\n### Operation naming\\nOperations are named based on their given `operationId`.\\nIf no `operationId` is given the name is parsed from the method and the path under which the operation occurred.\\nExample:\\n- path: `entity\/{id}`\\n- method: `get`\\n- resulting name: `getEntityById`\\nUsers can set specific operation names using the `x-sap-cloud-sdk-operation-name` extension.\\nThis extension can be set on the operation only.\\nNames will be transformed to camel case, duplicate names will cause generation to fail.\\n### Property and query parameter naming\\nKeep the names as provided per the specification.\\nNo camel case renaming is done to avoid the original name corner cases discussed above.\\n\\n\\n## Context\\nABCI errors should provide an abstraction between application details\\nand the client interface responsible for formatting & displaying errors to the user.\\nCurrently, this abstraction consists of a single integer (the `code`), where any\\n`code > 0` is considered an error (ie. invalid transaction) and all type\\ninformation about the error is contained in the code. This integer is\\nexpected to be decoded by the client into a known error string, where any\\nmore specific data is contained in the `data`.\\nIn a [previous conversation](https:\/\/github.com\/tendermint\/abci\/issues\/165#issuecomment-353704015),\\nit was suggested that not all non-zero codes need to be errors, hence why it\\'s called `code` and not `error code`.\\nIt is unclear exactly how the semantics of the `code` field will evolve, though\\nbetter lite-client proofs (like discussed for tags\\n[here](https:\/\/github.com\/tendermint\/tendermint\/issues\/1007#issuecomment-413917763))\\nmay play a role.\\nNote that having all type information in a single integer\\nprecludes an easy coordination method between \"module implementers\" and \"client\\nimplementers\", especially for apps with many \"modules\". With an unbounded error domain (such as a string), module\\nimplementers can pick a globally unique prefix & error code set, so client\\nimplementers could easily implement support for \"module A\" regardless of which\\nparticular blockchain network it was running in and which other modules were running with it. With\\nonly error codes, globally unique codes are difficult\/impossible, as the space\\nis finite and collisions are likely without an easy way to coordinate.\\nFor instance, while trying to build an ecosystem of modules that can be composed into a single\\nABCI application, the Cosmos-SDK had to hack a higher level \"codespace\" into the\\nsingle integer so that each module could have its own space to express its\\nerrors.\\n\\n## Decision\\nInclude a `string code_space` in all ABCI messages that have a `code`.\\nThis allows applications to namespace the codes so they can experiment with\\ntheir own code schemes.\\nIt is the responsibility of applications to limit the size of the `code_space`\\nstring.\\nHow the codespace is hashed into block headers (ie. so it can be queried\\nefficiently by lite clients) is left for a separate ADR.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nOn AMF 5 Beta\\'s first iteration \"domain+syntax\" mediatypes like \"application\/swagger20+yaml\" were\\\\nused to decide how the syntax and the domain were to be parsed. This was especially useful to\\\\nvalidate, transform and render units using compound configurationts (API, WebAPI, RAML, OAS)\\\\nThis is controversial as:\\\\n- Resulting mediatypes are strange to the end-user and are not standard. Besides we could\\\\nonly handle a specific ordering in domain and syntax. A mediatype formed by syntax+domain\\\\ncouldn\\'t be parsed.\\\\n- Clients that used a specific configuration like RAML10 or OAS30 had to specify the mediatype\\\\nalthough the configuration they used already specified their intended domain and syntax.\\\\n'}, {'role': 'model', 'content': 'Remove those compound mediatypes and instead only keep them for syntax purposes when needed.\\\\n'}]"}
{"File Name":"verify-frontend\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":1450,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need to record the architectural decisions made on this project.\\\\n'}, {'role': 'model', 'content': 'We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\\\n'}]"}
{"File Name":"forkhandles\/0002-monorepo-and-bom.md","Context":"## Context\\nDo we have lots of little xxx4k libraries in their own repositories?  Or have a monorepo: one big project with each xxx4k library in a subdirectory.\\nThe former decouples release cadences.\\nThe latter makes it easier to maintain a single BOM for publishing to Maven Central, perform integration testing when libraries depend on one another, and use a consistent version number across all libraries.\\n","Decision":"We will have a monorepo.\\n","tokens":86,"id":4542,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe `bus` is designed to decoupled projects from each other, rather than adding a many-to-many web of API calls.\\nThe set of SNS topics should be very much stable as it contains published content types from the API.\\nQueue listeners are deployed with each interested project, and are the responsibility of each project to empty.\\n\\n## Decision\\nEach project should own queues for the content types it consumes, while the topics should be owned by a global `bus` project.\\n\\n\\n## Context and Problem Statement\\nThe Open Data Hub itself consists of many components all having their own release cycles. ODH users might decide to\\nupdate individual components such as container images used by JupyterHub. Nevertheless it is required to create\\nreleases of ODH as a whole.\\n\\n## Decision\\nChosen option: we do a monolithic, coordinated release, because it will enable us to have a release at the\\nproject\/product level while maintianing freedom of others to update.\\n### Positive Consequences <!-- optional -->\\n* Operators of ODH have a clear base line of versions, these versions have been tested with each other and have\\nundergone ODH integration testing.\\n* Operators of ODH can update individual components, they could maintain a repository analog to odh-manifests declaring\\nthe exact versions (container image tags, git repository tags) of components they want to deploy.\\n* Operators can mix in their builds of container images following the method mentioned above.\\n### Negative Consequences <!-- optional -->\\n* An ODH release (a tag in the odh-manifests repository) might not contain the latest versions of components, for example\\nsecurity updates might have forced the build of a S2I image used with JupyterHub component of ODH.\\n<!-- markdownlint-disable-file MD013 -->\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nDo we have lots of little xxx4k libraries in their own repositories?  Or have a monorepo: one big project with each xxx4k library in a subdirectory.\\\\nThe former decouples release cadences.\\\\nThe latter makes it easier to maintain a single BOM for publishing to Maven Central, perform integration testing when libraries depend on one another, and use a consistent version number across all libraries.\\\\n'}, {'role': 'model', 'content': 'We will have a monorepo.\\\\n'}]"}
{"File Name":"content-data-api\/adr-009-consume-from-publishing-api.md","Context":"## Context\\nWhen we created the ETL pipeline we originally used the publishing API messages just to flag\\nitems as updated. Then we would fetch items from the content store overnight.\\nWe now grow the `Item` dimension every time there is an update, instead of limiting it\\nto one record per day. Now that we do this, we rely on two sources of truth for this data:\\nthe publishing API and the content store. We should not need to make requests to the content\\nstore at all, because the content store is intended to support the frontend architecture, not ETL\/analytics.\\nWe are addressing this now because we want to change the data warehouse to store individual parts\\nof guides and travel advice, and this is difficult to implement due to the complexity of the message\\nprocessing.\\n","Decision":"We will use the publishing API messages to extract all content, rather than issuing an additional request to the content store.\\n### Data flow\\nWe are refactoring the publishing API message hander, so that it first transforms a message into a `ContentItem` class, which stores all the information we need.\\nThis is the input we use to present the content for that item, calculate a hash of that content, and (if neccessary) calculate quality metrics for that content. We have decided to do all this processing synchronously for the time being so the pipeline is more predictable and easy to debug.\\n### Growing the item dimension\\nWe subscribe to these kinds of updates from the publishing API:\\n- major\\n- minor\\n- links\\n- republish\\n- unpublish\\nA new item record is created if:\\n- we've never seen that content item before\\n- the content hash changes\\n- the links change (if links appear in a different order they should still be considered the same)\\nOtherwise we don't insert new records.\\nWhen creating a new item we \"promote\" it by setting `latest=false` on the existing latest item with that content ID, and setting `latest=true` on the new item.\\n### Impact of multi-part items\\nFor a multi-part item, we will transform a single message into multiple items, each with a unique base path but a shared content ID.\\nThe base path of a part-item is the `base_path` attribute of the publishing API content item plus the `slug` of the part in the `details` hash.\\nWhen we grow the dimension, we mark *all* parts with the same `content_id` with `latest=false`, and create new item records for *all* the part items from the new message.\\n![](images\/content_etl.png)\\n### Benefits:\\n- We can implement multi-part documents very easily\\n- The data warehouse doesn't depend on frontend infrastructure.\\n","tokens":165,"id":1860,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https:\/\/www.gov.uk\/service-manual\/making-software\/choosing-technology). We are selecting which web framework to use for the data\\nAPI for the Performance Platform.\\nWe are writing the API in Python. We need to support accepting unstructured\\ndata, and returning structured data for reads.\\nThe API will not be RESTful initially. We want to optimise for easy writing,\\nand do the hard work to return structured data at read time. We do not\\nunderstand the model or resources that we will be using, so for now, we think\\nwe will allow data sets to be created and written to, and do some\\ntransformations at query time on those data sets.\\nThis means that we think we need to allow simple POST\/PUT to a data set, and\\nthen a more complex read query to produce structured data.\\nOver time, we expect to see common data sets \/ types and we can then look to\\nuse a more structured approach, defining resources and relationships. At that\\npoint, we might also want to consider using RESTful features, or switching\\nto a framework that has more support for that approach.\\nOptions considered include Django and Flask, since we are familar with those.\\n# Decision\\nWe will write the data API using Flask.\\nWe don't need the ORM for Django, and Flask seems simpler in terms of starting\\nwith a small API.\\n# Status\\nAccepted.\\n# Consequences\\nDevs and operations need to be comfortable with Flask apps.\\n\\n## Decision\\nWe will write the data API using Flask.\\nWe don't need the ORM for Django, and Flask seems simpler in terms of starting\\nwith a small API.\\n# Status\\nAccepted.\\n# Consequences\\nDevs and operations need to be comfortable with Flask apps.\\n\\n\\n## Context\\nWe need a way to modify existing data on live environments. A good example of this is the new field we made called `geography`. We made is a required field but missed the fact that existing data needed to be migrated based on the value of `recipient_region`. We missed this and needed to take action to correct the mistake on each environment.\\nWe want to avoid having unstructured access to rails consoles on live environments to de-risk the process where possible.\\nWe want to avoid the fact we have to remember to run a manual action before or after a deployment to snure they aren't forgotten.\\nUsing conventional rails migrations as the mechanism for only change data is an option. Though we may wish to change the schema without changing the data migrations. Tieing these 2 responsibilities into the same mechanism seems like it could back us into a corner later.\\nWe would like to document the actions we perform through our code. This would help us understand and debug the state of the service.\\n\\n## Decision\\nUse the data-migrate gem[1] when we need to make changes to data without touching the schema.\\nChanges to the schema can still include changes relevant to data within the db:migrate. If we were to add the geography feature again, we would add the new column and set the value as a single transaction.\\nInclude the `rake data:migrate` step in the deployment process so it is automatically run after `rake db:migrate`.\\n[1] https:\/\/github.com\/ilyakatz\/data-migrate\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWhen we created the ETL pipeline we originally used the publishing API messages just to flag\\\\nitems as updated. Then we would fetch items from the content store overnight.\\\\nWe now grow the `Item` dimension every time there is an update, instead of limiting it\\\\nto one record per day. Now that we do this, we rely on two sources of truth for this data:\\\\nthe publishing API and the content store. We should not need to make requests to the content\\\\nstore at all, because the content store is intended to support the frontend architecture, not ETL\/analytics.\\\\nWe are addressing this now because we want to change the data warehouse to store individual parts\\\\nof guides and travel advice, and this is difficult to implement due to the complexity of the message\\\\nprocessing.\\\\n\"}, {'role': 'model', 'content': 'We will use the publishing API messages to extract all content, rather than issuing an additional request to the content store.\\\\n### Data flow\\\\nWe are refactoring the publishing API message hander, so that it first transforms a message into a `ContentItem` class, which stores all the information we need.\\\\nThis is the input we use to present the content for that item, calculate a hash of that content, and (if neccessary) calculate quality metrics for that content. We have decided to do all this processing synchronously for the time being so the pipeline is more predictable and easy to debug.\\\\n### Growing the item dimension\\\\nWe subscribe to these kinds of updates from the publishing API:\\\\n- major\\\\n- minor\\\\n- links\\\\n- republish\\\\n- unpublish\\\\nA new item record is created if:\\\\n- we\\'ve never seen that content item before\\\\n- the content hash changes\\\\n- the links change (if links appear in a different order they should still be considered the same)\\\\nOtherwise we don\\'t insert new records.\\\\nWhen creating a new item we \"promote\" it by setting `latest=false` on the existing latest item with that content ID, and setting `latest=true` on the new item.\\\\n### Impact of multi-part items\\\\nFor a multi-part item, we will transform a single message into multiple items, each with a unique base path but a shared content ID.\\\\nThe base path of a part-item is the `base_path` attribute of the publishing API content item plus the `slug` of the part in the `details` hash.\\\\nWhen we grow the dimension, we mark *all* parts with the same `content_id` with `latest=false`, and create new item records for *all* the part items from the new message.\\\\n![](images\/content_etl.png)\\\\n### Benefits:\\\\n- We can implement multi-part documents very easily\\\\n- The data warehouse doesn\\'t depend on frontend infrastructure.\\\\n'}]"}
{"File Name":"gp-redirect\/0003-use-prometheus-for-exposing-metrics.md","Context":"## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n","Decision":"We will use Prometheus to monitor and alert on the state of the application.\\n","tokens":86,"id":4400,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n\\n## Decision\\nWe will use Prometheus to monitor and alert on the state of the application.\\n\\n\\n## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n\\n## Decision\\nWe will use Prometheus to monitor and alert on the state of the application.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need to know what the application is doing in a more light weight way than\\\\nscraping logs. We need to be able to monitor KPIs of the application in order\\\\nto understand the health of the application. This will allow us to react and\\\\npotentially pro-actively initiate measures as to ensure the application's\\\\nhealth if sound. Ultimately providing a better service for our users.\\\\n\"}, {'role': 'model', 'content': 'We will use Prometheus to monitor and alert on the state of the application.\\\\n'}]"}
{"File Name":"voroni-postals\/0003-switch-to-spring-boot.md","Context":"## Context\\nRapidoid was an interesting framework to try back in 2016, but since then I have wanted to switch this project back to Spring Boot for demonstration purposes.\\n","Decision":"This project will be switched to Spring Boot.\\n","tokens":37,"id":3268,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThis project is being developed as a technology demonstrator. During my day job, we use Spring Boot for almost all of our REST services. I have been exploring several micro frameworks, and for this project I chose to look at [Rapidoid](https:\/\/www.rapidoid.org\/).\\n\\n## Decision\\nUse Rapidoid for this experiment.\\n\\n\\n## Context\\nThis project was first implemented in plain JavaScript. Its only third-party dependency was on SoundCloud's JavaScript SDK.\\nI had recently discovered frontend web frameworks and had been learning about AngularJS. I was interested in applying it to a larger project.\\n\\n## Decision\\nThe application will be rewritten in AngularJS 1.x.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nRapidoid was an interesting framework to try back in 2016, but since then I have wanted to switch this project back to Spring Boot for demonstration purposes.\\\\n\"}, {'role': 'model', 'content': 'This project will be switched to Spring Boot.\\\\n'}]"}
{"File Name":"gsp\/ADR033-nlb-for-mtls.md","Context":"## Context\\nVerify's [doc-checking service](https:\/\/github.com\/alphagov\/doc-checking) is\\nsecured in part using mTLS. Currently, our clusters are fronted by ALBs which\\ncannot provide mTLS.\\nThe doc-checking service currently runs an nginx that provides the mTLS\\nfunctionality. In order for GSP to be able to allow something within the\\ncluster to perform mTLS we must run a load balancer that forwards unaltered TCP\\npackets in addition to, or instead of an ALB.\\n","Decision":"We will optionally create and run an NLB in addition to the current ALB for\\nclusters that have a requirement to terminate their own TLS. This NLB will be\\navailable at `nlb.$CLUSTER_DOMAIN`.\\n","tokens":115,"id":3888,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe use both MySQL and PostgreSQL in our stack for different applications. These\\nhave been traditionally provisioned as VM instances and managed with Puppet.\\nPuppet manages every aspect of these instances:\\n- Server configuration and tuning\\n- CRUD actions on databases\\n- Users and permissions\\n- Replication\/clustering\\n- Backups\\nAWS offers the [Relational Database Service](https:\/\/aws.amazon.com\/rds\/) (RDS).\\nUsing RDS would remove the following away from being managed by Puppet:\\n- Server configuration and tuning\\n- Replication\/clustering\\n- Backups\\nTerraform code would be required to ensure the above is configured correctly.\\n\\n## Decision\\nWe are going to use RDS to remove a significant portion of our Puppet code that\\ntraditionally managed both PostgreSQL and MySQL.\\nTo manage databases, users, data loading and long-term offsite backups, we will\\ncreate a new type of node class named \"db_admin\". This will be able to connect\\nand configure each RDS instance in a stack.\\n\\n\\n## Context\\n[govuk-frontend](https:\/\/github.com\/alphagov\/govuk-frontend) doesn\\'t do a good\\njob of detecting if it\\'s in a browser environment or not, and therefore, whether\\n`document` and friends are available or not. It also performs a number of\\npolyfills that rely on the presence of those browser globals. This means that it\\nthrows exceptions when attempting to import some JavaScript modules from that\\nlibrary.\\nThe way `govuk-frontend` JavaScript is generally written is under the assumption\\nthat you\\'ll want, and be able, to pass the raw DOM nodes to the module\\nconstructors. React doesn\\'t work that way, and we could dig into the prototypes\\nof the objects created by those constructors, but given that these methods are\\nnot intended to be used directly, it would be compliant with\\n[Semantic Versioning](https:\/\/semver.org\/spec\/v2.0.0.html) to completely change\\ntheir implementation without releasing a new major version. This makes relying\\non their behaviour being fixed unreliable.\\n\\n## Decision\\nWe will re-implement JavaScript features from `govuk-frontend` in more idiomatic\\nReact in this library.\\nWe will link to the version and the source of the original implementation\\nalongside our implementation for reference, linking to the code in the specific\\ncommit we were referring to, not `master`.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nVerify\\'s [doc-checking service](https:\/\/github.com\/alphagov\/doc-checking) is\\\\nsecured in part using mTLS. Currently, our clusters are fronted by ALBs which\\\\ncannot provide mTLS.\\\\nThe doc-checking service currently runs an nginx that provides the mTLS\\\\nfunctionality. In order for GSP to be able to allow something within the\\\\ncluster to perform mTLS we must run a load balancer that forwards unaltered TCP\\\\npackets in addition to, or instead of an ALB.\\\\n'}, {'role': 'model', 'content': 'We will optionally create and run an NLB in addition to the current ALB for\\\\nclusters that have a requirement to terminate their own TLS. This NLB will be\\\\navailable at `nlb.$CLUSTER_DOMAIN`.\\\\n'}]"}
{"File Name":"docs\/0026-implementation-of-complex-eai-patterns-with-faas.md","Context":"## Context and Problem Statement\\nSome [Enterprise Integration Patterns](https:\/\/www.enterpriseintegrationpatterns.com) have a complex structure where parts of the behaviour can be implemented generically while some parts need to be modifiable by the end user (in our case the system admin using MICO).\\nWe have already [decided to use a FaaS platform](0023-faas.md) to provide this modifiability in form of code as configuration.\\nWhile this works well for most patterns, for some of the more complex patterns it is not easy to allow modifiability via FaaS.\\nThis is especially the case if the user want to write as little code as possible meaning the [generic part](0025-generic-component.md) of the component has to be implemented by the MICO team.\\n## Decision Drivers\\n* Modifiability of the patterns must be provided via a FaaS function\\n* The function should only have to contain as little code as possible\\n* Generic code for the pattern should be provided by the MICO platform either as a library to import into the FaaS function or in the component that calls said function\\n","Decision":"* Modifiability of the patterns must be provided via a FaaS function\\n* The function should only have to contain as little code as possible\\n* Generic code for the pattern should be provided by the MICO platform either as a library to import into the FaaS function or in the component that calls said function\\n**Where to implement logic**: To be decided\\n**State of configuration channels**: To be decided\\n**Stateful functions**: To be decided\\n**Routing**: We will support custom routing decisions in the FaaS function by always interpreting a routing slip if it is present.\\nThe routing slip has to support multiple destinations for one routing step.\\nThis will also allow us to make more patterns possible (everything that is not stateful) with a single generic kafka to FaaS connector.\\n**Intermediate requests**: To be decided\\n","tokens":231,"id":4669,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n# Context\\nIt is not possible to list and up front define all the different\\ntransformations a user might want to apply to a dataset. User defined\\ntransformations is therefor an important feature in Lumen.\\nWe must decide what kind of data transformation language we want to\\nuse for these transformations. There are a couple of options:\\nWe could create a custom DSL perhaps inspired by Excel formulas. A\\ncustom DSL could be made less powerful (non Turing complete) which\\ncould mean that it\\'s easier to learn for the user and easier to\\nsandbox by the system.\\nWe can also use an existing scripting language such as Clojure or\\nJavaScript. The obvious advantage is that the language is already\\ndeveloped\/tested\/documented\/optimized etc. It can be more challenging\\nto securely sandbox the execution and the language isn\\'t tailor made\\nfor our particular use case.\\n# Decision\\nWe\\'ll use JavaScript running in a sandboxed\\n[Nashorn](https:\/\/docs.oracle.com\/javase\/8\/docs\/technotes\/guides\/scripting\/nashorn\/intro.html)\\njs environment as the transformation language in Lumen.\\n# Status\\nProposed\\n# Consequences\\n* User code must run in a sandboxed environment for security reasons\\n* We must guard against non-termination via some timeout mechanism.\\n* Lumen relies on typed columns but JavaScript is a dynamic language.\\nWe need to somehow ensure that the result of a user defined\\ntransformation doesn\\'t result in inconsistent column types.\\n\\n## Decision\\nWe\\'ll use JavaScript running in a sandboxed\\n[Nashorn](https:\/\/docs.oracle.com\/javase\/8\/docs\/technotes\/guides\/scripting\/nashorn\/intro.html)\\njs environment as the transformation language in Lumen.\\n# Status\\nProposed\\n# Consequences\\n* User code must run in a sandboxed environment for security reasons\\n* We must guard against non-termination via some timeout mechanism.\\n* Lumen relies on typed columns but JavaScript is a dynamic language.\\nWe need to somehow ensure that the result of a user defined\\ntransformation doesn\\'t result in inconsistent column types.\\n\\n\\n## Context\\nDevelopers consider dealing with local (runtime) exceptions in JVM quite convenient and easy. As a consequence, exceptions are often used as a means for an alternative execution flow. One typical\\nexample is reporting structural and business validation violations.\\nIn the local JVM environment, expected execution flow exceptions are usually created with a full stack trace included. However, as these exceptions represent \"normal\" conditions, creating a full\\nstack trace is wasteful in terms of performance and resources.\\nThe full stack trace problem is additionally emphasized in a distributed JVM environment. Not only the stack trace creation is wasteful, but it will take much more bandwidth to transfer it on the\\nother side.\\nAnd finally, in a distributed JVM environment, we cannot safely assume that our custom exception classes are present in the classpath of both JVMs. Therefore, we can not assume the exception created\\non the server-side can be deserialized on the client.\\nTo remedy the situation, we can use a small number of stack-less exception classes shared between JVMs included in the communication. They belong to the boundary layer and are part of the API used\\nfor distributed communication. To read more about boundary layer, take a look near the end of \"Domain libraries\" section in\\n\"[Organizing modules and packages](..\/..\/article\/modules-and-packages\/modulesAndPackages.md#domain-libraries)\" article.\\nFor communicating various custom exceptions, we can use violation (or error) codes. The violation code defines its severity and contains several additional properties for describing violation details.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\\n## Decision\\n**For distributed error reporting, we will use stack-less exceptions for communicating custom error codes between JVMs in a distributed Axon environment.**\\nIn a distributed Axon environment, the dispatching side (let\\'s call it a client) sends commands and queries over Axon Server to the handling side (let\\'s call it a server). When the handling side\\ndetects broken business invariants, it can raise an exception to report the details to the dispatching side.\\nAxon framework already contains base infrastructure for reporting such distributed errors via so-called exception details. Exception details can be any DTO-like class shared between communicating\\nparties, including all necessary data describing error conditions.\\n### Decision details\\n#### Business exception handling\\nIn our case, as exception details DTO, we will use the `DomainException` class and its descendants. It is a stack-less exception that carries `ViolationInfo` property. `ViolationInfo` contains\\n`Severity` and `ViolationCode` properties. `ViolationCode` contains `code` and `codeMessage` in English. There are also a `resolvableMessageKey` and `resolvableMessageParameters` that support error\\ncode resolving through a resource bundle for internationalization purposes.\\nIt is worth noting that `DomainException` can be used on the dispatching side (client) too. Exception (un)wrapping on the handling side and centralized exception handling on the dispatching side are\\nhidden in infrastructural code. From the developer\\'s perspective, he works with `CommandException` or `QueryException` only (both are extended from `DomainException`). This is a nice addition as\\nbusiness invariants are handled the same way on handling and dispatching sides, without depending on classes from the Axon framework.\\nAs a usage example on the handling side, take a look at `BookingOfferAggregate` or `BookingOfferSummaryQueryHandlerService` classes. An example for the dispatching side can be found in\\n`BookingOfferFactoryService` class.\\n#### Unexpected exception handling\\nWhen handling business exceptions, we are not interested in stack traces. Since business exceptions are just a form of the alternate execution flow, there isn\\'t much benefit in logging their\\nstacktrace.\\nContrary, we want to log the stack trace when an unexpected exception (i.e., NullPointerException) occurs at the remote handler. However, communication constraints still hold, and we still have to\\nuse error codes for communicating exceptions. Further, to correlate exceptions on handling and dispatching sides, we must use an exception identifier and put it in the log messages on both sides.\\nThe logic of handling unexpected exceptions on the handling side can be seen in the `CommandHandlerExceptionInterceptor` and `QueryHandlerExceptionInterceptor` classes.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\nSome [Enterprise Integration Patterns](https:\/\/www.enterpriseintegrationpatterns.com) have a complex structure where parts of the behaviour can be implemented generically while some parts need to be modifiable by the end user (in our case the system admin using MICO).\\\\nWe have already [decided to use a FaaS platform](0023-faas.md) to provide this modifiability in form of code as configuration.\\\\nWhile this works well for most patterns, for some of the more complex patterns it is not easy to allow modifiability via FaaS.\\\\nThis is especially the case if the user want to write as little code as possible meaning the [generic part](0025-generic-component.md) of the component has to be implemented by the MICO team.\\\\n## Decision Drivers\\\\n* Modifiability of the patterns must be provided via a FaaS function\\\\n* The function should only have to contain as little code as possible\\\\n* Generic code for the pattern should be provided by the MICO platform either as a library to import into the FaaS function or in the component that calls said function\\\\n'}, {'role': 'model', 'content': '* Modifiability of the patterns must be provided via a FaaS function\\\\n* The function should only have to contain as little code as possible\\\\n* Generic code for the pattern should be provided by the MICO platform either as a library to import into the FaaS function or in the component that calls said function\\\\n**Where to implement logic**: To be decided\\\\n**State of configuration channels**: To be decided\\\\n**Stateful functions**: To be decided\\\\n**Routing**: We will support custom routing decisions in the FaaS function by always interpreting a routing slip if it is present.\\\\nThe routing slip has to support multiple destinations for one routing step.\\\\nThis will also allow us to make more patterns possible (everything that is not stateful) with a single generic kafka to FaaS connector.\\\\n**Intermediate requests**: To be decided\\\\n'}]"}
{"File Name":"lcarsde\/identification-of-special-tool-windows.md","Context":"## Context and Problem Statement\\nSome tool windows like the status bar and the side bar menu need special placement in the UI and therefore special treatment by the window manager. The window manager must be able to identify them to treat them accordingly.\\n","Decision":"Chosen option: \"Client Properties \/ Atoms\", because it is the most reliable and side-effect free way to identify the tool windows.\\n","tokens":48,"id":145,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nThe application menu needs to know the currently active windows, shall display the name and enable the user to select or close them. Therefor it is necessary to exchange the corresponding information with the window manager.\\n## Decision Drivers <!-- optional -->\\n* Fast, the application menu entry information should be updated \"instantly\" when windows open, close or get renamed\\n* No message should be lost\\n\\n## Decision\\n* Fast, the application menu entry information should be updated \"instantly\" when windows open, close or get renamed\\n* No message should be lost\\nChosen option: \"Message queues\", because this approach is a faster than shared storage approaches and doesn\\'t need to handle locking. Message queues are asynchronous (the producer doesn\\'t need to wait for the consumer) and messages are queued and not lost.\\n### Positive Consequences <!-- optional -->\\n* Easy to use and flexible\\n* Library support in Python\\n### Negative Consequences <!-- optional -->\\n* Manual implementation in Kotlin necessary (but straight forward)\\n\\n\\n## Context and Problem Statement\\nWhen cutting release branches, a set of rc artifacts are not automatically created. The user must disable `--diffcheck` to generate a set of rc artifacts, or manually publish the last dev artifacts to the rc feed, or update the rc tags in the case of single registry.\\nThis process might be made simpler for the end-user, so that intervention is not required when cutting a release branch.\\n\\n## Decision\\nWe move away from recommending multiple feeds, rather to use a single feed originating from the trunk, and release definitions will utilize the LATEST_TAG or LATEST_GIT_TAG to do a release from the from the release branch. Users are expected to follow semantic versioning to prevent conflicts. In the case of parallel development, it is still possible to use semantic versioning in a single artifact repository.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\nSome tool windows like the status bar and the side bar menu need special placement in the UI and therefore special treatment by the window manager. The window manager must be able to identify them to treat them accordingly.\\\\n'}, {'role': 'model', 'content': 'Chosen option: \"Client Properties \/ Atoms\", because it is the most reliable and side-effect free way to identify the tool windows.\\\\n'}]"}
{"File Name":"react-template\/0007-use-eslint.md","Context":"## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript and React though plugins.\\n","Decision":"We will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n","tokens":71,"id":3330,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n\\n## Decision\\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n\\n\\n## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n\\n## Decision\\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe want to enforce consistency in our code, and catch as many errors\\\\nautomatically as we are able to. Linting the code is good practice to achieve\\\\nthese aims. [ESLint](https:\/\/eslint.org\/) is the standard linter for modern\\\\nJavaScript, and has good support for TypeScript and React though plugins.\\\\n'}, {'role': 'model', 'content': 'We will check code style using ESLint.\\\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\\\nstyles.\\\\nWe will use the recommended configuration for plugins where possible.\\\\nWe will run ESLint as part of the test suite.\\\\n'}]"}
{"File Name":"platform\/2021-09-14-technical-concept-custom-entities.md","Context":"## Context\\nIt should be possible for apps to define their entities. Furthermore, it should be possible, if desired, that these entities are available via Store API.\\nLater, it should also be possible for store operators to create such entities. The concept is to consider that Apps can not add PHP code into the system under current circumstances. Also, a store operator is, seen from our point of view, not able to write PHP code himself to guarantee logic for his custom entities.\\nTherefore, purely through the definition of a custom entity, certain business logic should be automatically guaranteed.\\n","Decision":"### Schema\\n* Definition\\n* An app can include a `config\/custom_entity.xml` file.\\n* Multiple custom entities can be defined in the XML file.\\n* Each custom entity, is registered with the prefix `custom_entity_` or the `ce_` shorthand.\\n* App developers can then define that they would like to have `custom_entity_swag_blog` as an entity\\n* To prevent naming collisions, app developers should always add their developer prefix to the entity name\\n* We then create the `custom_entity_swag_blog` table\\n* Tables \/ Properties \/ Columns:\\n* A proper MySQL table is created for each custom entity.\\n* For each custom entity field we create a real MySQL table column.\\n* We support the following field data types:\\n* All scalar fields (int, string, text, float, date, boolean)\\n* All JSON fields (JSON, list, price, etc.)\\n* All \"linking\" associations (many-to-one and many-to-many)\\n* A bi-directional association will be left out for now.\\n* one-to-one and one-to-many will be not supported for now.\\n* Install & Update\\n* When installing and updating an app, the core automatically performs a schema update.\\n* Consider running a `dal:validate` on the schema when installing and updating an app.\\n* New fields on a custom entity must always be nullable or have a default\\n* Changing a field\/property data type is not allowed\\n* If a field is no longer defined in the .xml file, it will be deleted from the database.\\n* Identification and representation\\n* Each custom entity gets a `IdField(id)`, which serves as primary key\\n* Each custom entity gets a field `TranslatedField(label)`, which is required and serves as display name for the admin\\n### Bootstrapping\\n* At kernel boot we load all custom entities from the database and register them in the registry and di-container.\\n* For each custom entity, an entity definition is registered\\n* A generic entity definition is used, which gets the property\/column schema injected\\n* It must be checked how performant this is in case of bad performance we must put a cache in front of it (serialized properties\/columns e.g.)\\n* If no database connection exists, a kernel boot should still be possible\\n* The loading of the custom entities for the kernel boot should be outsourced to a CustomEntityKernelLoader\\n### Api availability\\nFor routing, we have to trick a bit, because currently for each entity in the system the routes defined exactly. This is not possible because the API route loader is triggered before the custom entities are registered. Therefore...\\n* We always register `\/api\/custom-entity-{entity}` as an API route and point to a custom controller that derives from ApiController.\\n* A request `\/api\/custom-entity-swag-blog`, then runs into our controller, and we get for the parameter `entity` the value `swag-blog`. We then pass this value to the parent method and prefetch it\\n* If the entity was defined with the `ce_` shorthand the API endpoints also use that shorthand, which means the route would be `\/api\/ce-{entity}`.\\n### Store api integration\\n* On the schema of the entity, the developer can define if this is `store_api_aware`.\\n* Entities which are not marked as `store_api_aware` will be removed from the response\\n* We will provide no automatic generated endpoint for the entities.\\n* Store api logics will be realized with the app-scripting epic\\n","tokens":118,"id":4501,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe need to customize different features in the tool raises the question of how to specify the customization.\\nOn the one hand, there\\'s a need to offer flexibility and expressive power. Assuming the average user is a competent developer, the most obvious way to achieve this  is to enable\\nsome kind of a programming interface\/definition.\\nGiven that the tool is written in JS, it\\'s reasonable to assume a definition that will be done in JS as well.\\nThis might be too much power, resulting in potential performance\/bugs\/security issues.\\nOn the other hand, there\\'s the motivation to keep the tool simple enough, so configuration is minimal, but still expressive.\\nWe can always start with a limited definition, but with an eye for further extensions. In any case, we\\'d like customizations to happen in a consistent and uniform manner.\\nThis in a sense extends\/generalizes the [decision to allow more permissive file names](3-Allow_More_Permissive_File_Names.md) by simply allowing any kind of customized file names, when working with the file name customization.\\n\\n## Decision\\nIn order to enable future extensions, we\\'ll choose a customization method based on JS-based extensions.\\nWe\\'ll define a file called `adr_custom.js` which will contain customization done to the tool. This file will need to reside in the ADR directory (the directory with the `.adr` file).\\nThe customization code will be simple JS code, w\/ predefined objects, depending on the API as defined by the tool. To be documented separately.\\nFor example, for defining the file name pattern, we\\'ll probably require some JS object, with a predefined name, to be in that file.\\nThe customization file (`adr_custom.js`) will be in a module form (using `module.export` etc.), so it can be easily consumed in the tool\\'s code.\\nWe assume that a user customizing the tool is competent enough to handle simple scripting.\\nLimitations on customizations, resulting in easier customization, are left to specific features.\\n\\n\\n## Context\\nOur initial design for tenancy in Corvus (which necessarily affected Marain.Tenancy) comingled read and write behaviour. The model was similar to the .NET Entity Framework: if you wanted to modify a tenant, you would first fetch an object representing that tenant, then make changes to that object, and then invoke an operation indicating that you wanted those changes to be written back.\\nWe made various changes to the Property Bag system that tenancy uses to store tenant properties to disassociate the API from any particular JSON serialization framework. We had previously forced a dependency on Json.NET, but we wanted to be able to move onto `System.Text.Json`, so we wanted to introduce a Property Bag abstraction that was independent of serialization mechanism (although still with a presumption that it must be possible for the properties to be serialized as JSON).\\nOne of the basic principles of efficient JSON parsing in the new world is that you don\\'t build an object model representing the JSON unless you really need to. Ideally, you leave the JSON in its raw UTF-8 state, referred to via one or more `IMemory<byte>` values, and extract what data you need only as you need it. This can dramatically reduce GC pressure, particularly in cases where most of the data in question is not used most of the time. However, this model does not fit well with the \"modifiable entities\" approach to updates. If anything is free to modify the properties at any time, this implies an ability to edit or regenerate the JSON.\\nIn practice, modification of tenant properties is the exception, not the rule. Most Marain services will only ever fetch tenant properties. Only the Marain.Tenancy service should normally directly edit these properties. So the \"modifiable entities\" approach is not really necessary, and causes problems for migration to allocation-efficient strategies.\\n\\n## Decision\\nSince `Corvus.Json.Abstractions` separates out read and update operations for `IPropertyBag`, and `Corvus.Tenancy` therefore does the same (since it uses property bags), Marain.Tenancy will follow suit.\\nThe web API presented by Marain.Tenancy for modifying tenants uses JSON Patch. So instead of this procedure:\\n* fetch a serialized representation of an ITenant from the web API\\n* modify that representation to reflect the changes you wish to make\\n* PUT that serialized representation of an ITenant back to the web API\\nwe now use this procedure instead:\\n* send a PATCH request in describing the changes required in JSON Patch format\\nFor example, to rename a tenant, you would send this PATCH to the Marain.Tenancy service, using the URL representing the tenant (the same URL from which you would fetch the tenant if reading) with an `application\/json-patch+json` content type:\\n```json\\n[{\\n\"path\": \"\/name\",\\n\"op\": \"replace\",\\n\"value\": \"NewTenantName\"\\n}]\\n```\\nJSON Patch supports multiple changes in a single request, e.g.:\\n```json\\n[\\n{\\n\"op\": \"add\",\\n\"path\": \"\/properties\/StorageConfiguration__corvustenancy\",\\n\"value\": {\\n\"AccountName\": \"mardevtenancy\",\\n\"Container\": null,\\n\"KeyVaultName\": \"mardevkv\",\\n\"AccountKeySecretName\": \"mardevtenancystore\",\\n\"DisableTenantIdPrefix\": false\\n}\\n},\\n{\\n\"op\": \"add\",\\n\"path\": \"\/properties\/Foo__bar\",\\n\"value\": \"Some string\"\\n},\\n{\\n\"op\": \"add\",\\n\"path\": \"\/properties\/Foo__spong\",\\n\"value\": 42\\n}\\n]\\n```\\nThe `op` can be set to `remove` to delete properties.\\nClients will not typically build these PATCH requests themselves, because the `ClientTenantStore` type contains the relevant code. `ClientTenantStore` provides an implementation of `ITenantStore` that works by using the web API provided by Marain.Tenancy. So in practice, updating the name of a tenant is as simple as:\\n```csharp\\nawait tenantStore.UpdateTenantAsync(tenantId, name: \"NewTenantName\");\\n```\\nAdding or changing a property looks like this:\\nawait tenantStore.UpdateTenantAsync(\\ntenantId,\\npropertiesToSetOrAdd: new Dictionary<string, object>()\\n{\\n{ \"StorageConfiguration__corvustenancy\", myStorageConfig },\\n{ \"SomeOtherSetting\": 42 },\\n});\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nIt should be possible for apps to define their entities. Furthermore, it should be possible, if desired, that these entities are available via Store API.\\\\nLater, it should also be possible for store operators to create such entities. The concept is to consider that Apps can not add PHP code into the system under current circumstances. Also, a store operator is, seen from our point of view, not able to write PHP code himself to guarantee logic for his custom entities.\\\\nTherefore, purely through the definition of a custom entity, certain business logic should be automatically guaranteed.\\\\n'}, {'role': 'model', 'content': '### Schema\\\\n* Definition\\\\n* An app can include a `config\/custom_entity.xml` file.\\\\n* Multiple custom entities can be defined in the XML file.\\\\n* Each custom entity, is registered with the prefix `custom_entity_` or the `ce_` shorthand.\\\\n* App developers can then define that they would like to have `custom_entity_swag_blog` as an entity\\\\n* To prevent naming collisions, app developers should always add their developer prefix to the entity name\\\\n* We then create the `custom_entity_swag_blog` table\\\\n* Tables \/ Properties \/ Columns:\\\\n* A proper MySQL table is created for each custom entity.\\\\n* For each custom entity field we create a real MySQL table column.\\\\n* We support the following field data types:\\\\n* All scalar fields (int, string, text, float, date, boolean)\\\\n* All JSON fields (JSON, list, price, etc.)\\\\n* All \"linking\" associations (many-to-one and many-to-many)\\\\n* A bi-directional association will be left out for now.\\\\n* one-to-one and one-to-many will be not supported for now.\\\\n* Install & Update\\\\n* When installing and updating an app, the core automatically performs a schema update.\\\\n* Consider running a `dal:validate` on the schema when installing and updating an app.\\\\n* New fields on a custom entity must always be nullable or have a default\\\\n* Changing a field\/property data type is not allowed\\\\n* If a field is no longer defined in the .xml file, it will be deleted from the database.\\\\n* Identification and representation\\\\n* Each custom entity gets a `IdField(id)`, which serves as primary key\\\\n* Each custom entity gets a field `TranslatedField(label)`, which is required and serves as display name for the admin\\\\n### Bootstrapping\\\\n* At kernel boot we load all custom entities from the database and register them in the registry and di-container.\\\\n* For each custom entity, an entity definition is registered\\\\n* A generic entity definition is used, which gets the property\/column schema injected\\\\n* It must be checked how performant this is in case of bad performance we must put a cache in front of it (serialized properties\/columns e.g.)\\\\n* If no database connection exists, a kernel boot should still be possible\\\\n* The loading of the custom entities for the kernel boot should be outsourced to a CustomEntityKernelLoader\\\\n### Api availability\\\\nFor routing, we have to trick a bit, because currently for each entity in the system the routes defined exactly. This is not possible because the API route loader is triggered before the custom entities are registered. Therefore...\\\\n* We always register `\/api\/custom-entity-{entity}` as an API route and point to a custom controller that derives from ApiController.\\\\n* A request `\/api\/custom-entity-swag-blog`, then runs into our controller, and we get for the parameter `entity` the value `swag-blog`. We then pass this value to the parent method and prefetch it\\\\n* If the entity was defined with the `ce_` shorthand the API endpoints also use that shorthand, which means the route would be `\/api\/ce-{entity}`.\\\\n### Store api integration\\\\n* On the schema of the entity, the developer can define if this is `store_api_aware`.\\\\n* Entities which are not marked as `store_api_aware` will be removed from the response\\\\n* We will provide no automatic generated endpoint for the entities.\\\\n* Store api logics will be realized with the app-scripting epic\\\\n'}]"}
{"File Name":"unit-e-project\/ADR-0016.md","Context":"## Context\\nFor an open source project being able to distribute its code under the chosen\\nlicense it's important to make sure that all contributions are done under this\\nlicense and that contributors have the legal right to do so. There are three\\nways how this is usually handled:\\n* Rely on the implicit agreement by contributors when they submit code to a\\nproject with a known license.\\n* Require contributors to sign a contributor license agreement (CLA) which\\nexplicitly assigns rights to the project and gives guarantees from the\\ncontributor. There are a variety of CLAs used by open source projects which\\ndiffer in details such as what rights are given to the organization governing\\nthe project. For contributions being done by a company it's required that the\\ncompany has signed a corporate CLA.\\n* The [Developer Certificate of Origin\\n(DCO)](https:\/\/developercertificate.org\/), established by the Linux\\nFoundation, which is an explicit statement by contributors in each commit that\\nthey have the legal rights to contribute the code and are contributing the\\ncode under the license the project has chosen. This doesn't require signing\\nformal documents between project and contributor or their employer.\\nThere are two dimensions to consider in choosing which way to use, one is the\\nclarity about the terms of the contribution and the other is how much of a\\nbarrier is created by adding requirements for contributions.\\nThe DCO is an increasingly popular way of providing legal clarity while not\\nadding the overhead and barrier of a CLA. It works in a symmetric way, not\\ngiving the project additional rights such as to relicense the code under a\\nproprietary license which often is the case in CLAs. It doesn't require signing\\nan agreement which can be difficult if the employer of the developer has to do\\nthat. It is standardized and used by many open source projects such as the Linux\\nKernel, Docker, Chef, or GitLab, so that many developers are already familiar\\nwith it and know how to use it.\\nUsing the DCO sends a clear signal that the open source project is serious about\\nthe integrity of its code not only from a technical perspective but also from a\\nlegal one.\\n","Decision":"UnitE adopts the DCO. That means that all commits MUST be signed off by the\\nauthor by adding a statement at the end of the commit message in the form:\\nSigned-off-by: Jane Doe <jd@example.com>\\nThis statement declares that the author has the right to contribute the code and\\ndoes so under the license of the open source project, which in the case of UnitE\\nis MIT. The statement formally declares acceptance of the Developer Certificate\\nof Origin which is hosted at https:\/\/developercertificate.org\/:\\nDeveloper Certificate of Origin\\nVersion 1.1\\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\\n1 Letterman Drive\\nSuite D4700\\nSan Francisco, CA, 94129\\nEveryone is permitted to copy and distribute verbatim copies of this\\nlicense document, but changing it is not allowed.\\nDeveloper's Certificate of Origin 1.1\\nBy making a contribution to this project, I certify that:\\n(a) The contribution was created in whole or in part by me and I\\nhave the right to submit it under the open source license\\nindicated in the file; or\\n(b) The contribution is based upon previous work that, to the best\\nof my knowledge, is covered under an appropriate open source\\nlicense and I have the right under that license to submit that\\nwork with modifications, whether created in whole or in part\\nby me, under the same open source license (unless I am\\npermitted to submit under a different license), as indicated\\nin the file; or\\n(c) The contribution was provided directly to me by some other\\nperson who certified (a), (b) or (c) and I have not modified\\nit.\\n(d) I understand and agree that this project and the contribution\\nare public and that a record of the contribution (including all\\npersonal information I submit with it, including my sign-off) is\\nmaintained indefinitely and may be redistributed consistent with\\nthis project or the open source license(s) involved.\\nOther people involved with creating the submission, such as co-authors or the\\ncommitter, SHOULD add their own `Signed-off-by` statement so that you might get\\nmultiple of these statements in a commit message.\\n","tokens":455,"id":3773,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nIn an effort to support [Go-API-Stability](.\/adr-060-go-api-stability.md),\\nduring the 0.35 development cycle, we have attempted to reduce the the API\\nsurface area by moving most of the interface of the `node` package into\\nunexported functions, as well as moving the reactors to an `internal`\\npackage. Having this coincide with the 0.35 release made a lot of sense\\nbecause these interfaces were _already_ changing as a result of the `p2p`\\n[refactor](.\/adr-061-p2p-refactor-scope.md), so it made sense to think a bit\\nmore about how tendermint exposes this API.\\nWhile the interfaces of the P2P layer and most of the node package are already\\ninternalized, this precludes some operational patterns that are important to\\nusers who use tendermint as a library. Specifically, introspecting the\\ntendermint node service and replacing components is not supported in the latest\\nversion of the code, and some of these use cases would require maintaining a\\nvendor copy of the code. Adding these features requires rather extensive\\n(internal\/implementation) changes to the `node` and `rpc` packages, and this\\nADR describes a model for changing the way that tendermint nodes initialize, in\\nservice of providing this kind of functionality.\\nWe consider node initialization, because the current implemention\\nprovides strong connections between all components, as well as between\\nthe components of the node and the RPC layer, and being able to think\\nabout the interactions of these components will help enable these\\nfeatures and help define the requirements of the node package.\\n\\n## Decision\\n- To the greatest extent possible, factor the code base so that\\npackages are responsible for their own initialization, and minimize\\nthe amount of code in the `node` package itself.\\n- As a design goal, reduce direct coupling and dependencies between\\ncomponents in the implementation of `node`.\\n- Begin iterating on a more-flexible internal framework for\\ninitializing tendermint nodes to make the initatilization process\\nless hard-coded by the implementation of the node objects.\\n- Reactors should not need to expose their interfaces *within* the\\nimplementation of the node type\\n- This refactoring should be entirely opaque to users.\\n- These node initialization changes should not require a\\nreevaluation of the `service.Service` or a generic initialization\\norchestration framework.\\n- Do not proactively provide a system for injecting\\ncomponents\/services within a tendtermint node, though make it\\npossible to retrofit this kind of plugability in the future if\\nneeded.\\n- Prioritize implementation of p2p-based statesync reactor to obviate\\nneed for users to inject a custom state-sync provider.\\n\\n\\n## Context\\nIn an effort to support [Go-API-Stability](.\/adr-060-go-api-stability.md),\\nduring the 0.35 development cycle, we have attempted to reduce the the API\\nsurface area by moving most of the interface of the `node` package into\\nunexported functions, as well as moving the reactors to an `internal`\\npackage. Having this coincide with the 0.35 release made a lot of sense\\nbecause these interfaces were _already_ changing as a result of the `p2p`\\n[refactor](.\/adr-061-p2p-refactor-scope.md), so it made sense to think a bit\\nmore about how tendermint exposes this API.\\nWhile the interfaces of the P2P layer and most of the node package are already\\ninternalized, this precludes some operational patterns that are important to\\nusers who use tendermint as a library. Specifically, introspecting the\\ntendermint node service and replacing components is not supported in the latest\\nversion of the code, and some of these use cases would require maintaining a\\nvendor copy of the code. Adding these features requires rather extensive\\n(internal\/implementation) changes to the `node` and `rpc` packages, and this\\nADR describes a model for changing the way that tendermint nodes initialize, in\\nservice of providing this kind of functionality.\\nWe consider node initialization, because the current implemention\\nprovides strong connections between all components, as well as between\\nthe components of the node and the RPC layer, and being able to think\\nabout the interactions of these components will help enable these\\nfeatures and help define the requirements of the node package.\\n\\n## Decision\\n- To the greatest extent possible, factor the code base so that\\npackages are responsible for their own initialization, and minimize\\nthe amount of code in the `node` package itself.\\n- As a design goal, reduce direct coupling and dependencies between\\ncomponents in the implementation of `node`.\\n- Begin iterating on a more-flexible internal framework for\\ninitializing tendermint nodes to make the initatilization process\\nless hard-coded by the implementation of the node objects.\\n- Reactors should not need to expose their interfaces *within* the\\nimplementation of the node type\\n- This refactoring should be entirely opaque to users.\\n- These node initialization changes should not require a\\nreevaluation of the `service.Service` or a generic initialization\\norchestration framework.\\n- Do not proactively provide a system for injecting\\ncomponents\/services within a tendtermint node, though make it\\npossible to retrofit this kind of plugability in the future if\\nneeded.\\n- Prioritize implementation of p2p-based statesync reactor to obviate\\nneed for users to inject a custom state-sync provider.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nFor an open source project being able to distribute its code under the chosen\\\\nlicense it's important to make sure that all contributions are done under this\\\\nlicense and that contributors have the legal right to do so. There are three\\\\nways how this is usually handled:\\\\n* Rely on the implicit agreement by contributors when they submit code to a\\\\nproject with a known license.\\\\n* Require contributors to sign a contributor license agreement (CLA) which\\\\nexplicitly assigns rights to the project and gives guarantees from the\\\\ncontributor. There are a variety of CLAs used by open source projects which\\\\ndiffer in details such as what rights are given to the organization governing\\\\nthe project. For contributions being done by a company it's required that the\\\\ncompany has signed a corporate CLA.\\\\n* The [Developer Certificate of Origin\\\\n(DCO)](https:\/\/developercertificate.org\/), established by the Linux\\\\nFoundation, which is an explicit statement by contributors in each commit that\\\\nthey have the legal rights to contribute the code and are contributing the\\\\ncode under the license the project has chosen. This doesn't require signing\\\\nformal documents between project and contributor or their employer.\\\\nThere are two dimensions to consider in choosing which way to use, one is the\\\\nclarity about the terms of the contribution and the other is how much of a\\\\nbarrier is created by adding requirements for contributions.\\\\nThe DCO is an increasingly popular way of providing legal clarity while not\\\\nadding the overhead and barrier of a CLA. It works in a symmetric way, not\\\\ngiving the project additional rights such as to relicense the code under a\\\\nproprietary license which often is the case in CLAs. It doesn't require signing\\\\nan agreement which can be difficult if the employer of the developer has to do\\\\nthat. It is standardized and used by many open source projects such as the Linux\\\\nKernel, Docker, Chef, or GitLab, so that many developers are already familiar\\\\nwith it and know how to use it.\\\\nUsing the DCO sends a clear signal that the open source project is serious about\\\\nthe integrity of its code not only from a technical perspective but also from a\\\\nlegal one.\\\\n\"}, {'role': 'model', 'content': \"UnitE adopts the DCO. That means that all commits MUST be signed off by the\\\\nauthor by adding a statement at the end of the commit message in the form:\\\\nSigned-off-by: Jane Doe <jd@example.com>\\\\nThis statement declares that the author has the right to contribute the code and\\\\ndoes so under the license of the open source project, which in the case of UnitE\\\\nis MIT. The statement formally declares acceptance of the Developer Certificate\\\\nof Origin which is hosted at https:\/\/developercertificate.org\/:\\\\nDeveloper Certificate of Origin\\\\nVersion 1.1\\\\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\\\\n1 Letterman Drive\\\\nSuite D4700\\\\nSan Francisco, CA, 94129\\\\nEveryone is permitted to copy and distribute verbatim copies of this\\\\nlicense document, but changing it is not allowed.\\\\nDeveloper's Certificate of Origin 1.1\\\\nBy making a contribution to this project, I certify that:\\\\n(a) The contribution was created in whole or in part by me and I\\\\nhave the right to submit it under the open source license\\\\nindicated in the file; or\\\\n(b) The contribution is based upon previous work that, to the best\\\\nof my knowledge, is covered under an appropriate open source\\\\nlicense and I have the right under that license to submit that\\\\nwork with modifications, whether created in whole or in part\\\\nby me, under the same open source license (unless I am\\\\npermitted to submit under a different license), as indicated\\\\nin the file; or\\\\n(c) The contribution was provided directly to me by some other\\\\nperson who certified (a), (b) or (c) and I have not modified\\\\nit.\\\\n(d) I understand and agree that this project and the contribution\\\\nare public and that a record of the contribution (including all\\\\npersonal information I submit with it, including my sign-off) is\\\\nmaintained indefinitely and may be redistributed consistent with\\\\nthis project or the open source license(s) involved.\\\\nOther people involved with creating the submission, such as co-authors or the\\\\ncommitter, SHOULD add their own `Signed-off-by` statement so that you might get\\\\nmultiple of these statements in a commit message.\\\\n\"}]"}
{"File Name":"ELEN4010\/ADR Testing.md","Context":"## Context\\nIn order to holistically test the core functionality of the website, a combination of unit testing, end-to-end testing, and manual testing is used.\\nUnit tests are used on back-end models and database-related code in order to validate the functionality of each essential unit of the code (which, in most cases, are functions).\\nOn the front-end, various user actions are performed by automated testing software. During that process, key aspects relating to the front-end side of the website are tested.\\nHigh-level functionality is exclusively assessed and confirmed via manual user testing. This includes testing the following aspects of the website:\\n- Marker placement on maps\\n- Destinations being correctly added and drawn\\n- Trips being correctly written to and received from session storage\\n","Decision":"The testing framework chosen for automated testing is Jest. This framework is used because:\\n- It has a simple installation and configuration process for Node.js\\n- Due to its popularity as a javascript testing framework, it has a large developer-community which produces many articles, documents and forum threads (amongst many other sources of documentation and support)\\n- It has a wide variety of built-in assertion abilities (which means that there is no need for the installation of a third-party assertion library)\\nIn order to simulate in-browser user-interactions with the website, Selenium WebDriver is used. Front-end testing is performed on the https:\/\/testawaywego.azurewebsites.net website since it is the website used for development.\\nUltimately, it was decided that all automated front-end user testing will be performed using Google Chrome as the browser. The reason for this is due to the fact that Google Chrome has the highest browser market share (more than 60%) globally - meaning that a majrity of the website's users will be using Google Chrome.\\nAt multiple stages throughout the development process, manual testing on other major browsers (i.e. FireFox, Safari and Microsoft Edge) was also performed in order to ensure the cross-browser compatibility of the website. Manual testing was also used to ensure that the website is mobile-friendly.\\n","tokens":156,"id":56,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe accessibility monitoring team require a means of:\\n* Picking websites to test (from a random list weighted by a variety of criteria, from complaints passed to us, from previously-tested sites (to test changes) and sites specifically chosen by guiding bodies)\\n* Collecting and maintaining a list of public sector websites from which to pick\\n* Triaging each site (check it is live, check for accessibility statement(s), measure its size in pages etc)\\n* Prioritising sites for testing\\n* Tracking the progress of testing on each website\\n* Creating a report for the completed test\\n* Sending the report to the site\\'s owner\\n* Managing and recording interaction with the site\\'s owner\\nWhilst there are a number of accessibility testing solutions available, they are all geared towards testing of one\\'s own site, and require (or at least, assume) access to the source code and deployment system. Testing of multiple sites by a 3rd party from outside the sites\\' domains is not a common requirement for vendors!\\nSimilarly, whilst the reports generated by some of the testing tools are very good, they are not up to GDS standards for content and, ironically, accessibility. Furthermore, GDS have no control over the content of the generated reports.\\nWe therefore need a combination of database administration, semi-automatic accessibility testing and a Customer Relationship \/ Support Ticket Management system, all able to move data between them in a usable fashion.\\n\\n## Decision\\nWe will use 3rd-party tools where they provide part of the functionality that is required and can be made to communicate easily with each other.\\nWe will write whatever code is necessary to form the \"glue\" that binds them together and to automate whatever can be (and is worth) automating.\\n\\n\\n## Context\\nFrom time to time the Fundraising team wants to try out changes in the fundraising frontend to see if they improve the amount of donors, the donation sum, the membership application rate and so on.\\nIn the past, the developers implemented those experiments in an ad-hoc fashion, with different places in the code implementing the branching, and with different places in the code and database to store the outcomes.\\nFrom a developer perspective, the new implementation of bucket testing must not affect the code quality and spread throughout the code base. To achieve that, we should collect the relevant code in one agreed-upon central location. Also, there should be some mechanism in place to determine, if experiments are still ongoing. Ideally, our CI should help us to prune dead code related to bucket tests which have completed.\\nWhen our system places a visitor in a bucket, it must store that assignment persistently in the database. When querying the database, we must be able to link the record to donations\/memberships if needed. No need to build an analysis software yet: The FUN team will provide the results of each bucket testing campaign as raw\/aggregated data obtained though querying the database.\\n\\n## Decision\\n* Create campaign configuration which describes the experiments in a human-readable format (YAML) with bucket names, start and end dates and contextual information.\\n* For better performance, the configuration is validated in CI, at runtime it\\'s assumed to be correct.\\n* Create CLI-based campaign validator for logical and technical errors.\\n* Create CLI-based validator which verifies that each campaign has the necessary code logic implemented and ensures that removed campaigns have had their code properly pruned to prevent dead code from accumulating\\n* Add code that translates the configuration into feature toggles. Use [Doorkeeper](https:\/\/github.com\/remotelyliving\/doorkeeper) as a feature toggle library. We [evaluated different feature toggle libraries](https:\/\/gist.github.com\/gbirke\/ab53316c69341718a9dd5cb79ed32642) and chose Doorkeeper because\\n* It has the most modern code (PHP 7), 100% test coverage and the code looks most SOLID of the considered options.\\n* It already implements toggle conditions we need (check date range, check user bucket).\\n* All branching based on feature toggles *must* take place at creation time, i.e. in the central factory that creates the use cases and their dependencies. We created the `ChoiceFactory` for that purpose.\\nRemoving the old code that stores the result of old A\/B tests (but was not used in the 2016 and 2017 campaigns and stores empty default values) is not as part of the acceptance criteria.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nIn order to holistically test the core functionality of the website, a combination of unit testing, end-to-end testing, and manual testing is used.\\\\nUnit tests are used on back-end models and database-related code in order to validate the functionality of each essential unit of the code (which, in most cases, are functions).\\\\nOn the front-end, various user actions are performed by automated testing software. During that process, key aspects relating to the front-end side of the website are tested.\\\\nHigh-level functionality is exclusively assessed and confirmed via manual user testing. This includes testing the following aspects of the website:\\\\n- Marker placement on maps\\\\n- Destinations being correctly added and drawn\\\\n- Trips being correctly written to and received from session storage\\\\n'}, {'role': 'model', 'content': \"The testing framework chosen for automated testing is Jest. This framework is used because:\\\\n- It has a simple installation and configuration process for Node.js\\\\n- Due to its popularity as a javascript testing framework, it has a large developer-community which produces many articles, documents and forum threads (amongst many other sources of documentation and support)\\\\n- It has a wide variety of built-in assertion abilities (which means that there is no need for the installation of a third-party assertion library)\\\\nIn order to simulate in-browser user-interactions with the website, Selenium WebDriver is used. Front-end testing is performed on the https:\/\/testawaywego.azurewebsites.net website since it is the website used for development.\\\\nUltimately, it was decided that all automated front-end user testing will be performed using Google Chrome as the browser. The reason for this is due to the fact that Google Chrome has the highest browser market share (more than 60%) globally - meaning that a majrity of the website's users will be using Google Chrome.\\\\nAt multiple stages throughout the development process, manual testing on other major browsers (i.e. FireFox, Safari and Microsoft Edge) was also performed in order to ensure the cross-browser compatibility of the website. Manual testing was also used to ensure that the website is mobile-friendly.\\\\n\"}]"}
{"File Name":"granary\/0002-api-documentation.md","Context":"# Context\\nGranary will be a standalone deployable open source application. Users' primary\\nway of interacting with Granary will be through its REST API, rather than\\nthrough a UI, at least for the foreseeable future. This focus puts a larger\\nburden than usual on the quality of our API documentation.\\nWe'd like to evaluate different ways to keep API documentation up-to-date. For\\neach strategy, we want to know:\\n- What are the general pros and cons?\\n- How do we find out that our docs have drifted from the API?\\n- How can we document different versions of the API at the same time?\\n","Decision":"We should use `tapir` for automatically generating API documentation. The ADT\\nsupport and straightforward API (inputs use `.in`, outputs use `.out`, auth\\nextractors use `auth`) will flatten out the learning curve, and we'll have a\\nstable and correct reference point for API documentation that users setting up\\ntheir own deployments can refer to. We can call this out at the beginning of\\nthe README and hopefully save ourselves from having to answer an entire\\ncategory of questions.\\n# Consequences\\n- The first routes added to the API will be slightly more difficult, because\\nthey'll include writing API routes with a new library for the first time.\\n- The README should be updated to point to the location of the API\\ndocumentation.\\n","tokens":134,"id":3549,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nRegardless of how well planned an API is, it\\'s a sure bet that business requirements WILL eventually dictate that backwards-incompatible changes will be made to the API. Versioning the API is a necessity.\\nThe OPG-Data API must implement versioning. Must be RESTful, and offer as much flexibility with regard to future hosting\/infrastructure as possible.\\nSadly there is no real consesus on the best approach to versioning an API, with several of the \\'big players\\' opting for different strategies. There are pros and cons to each approach.\\nAn exploration of various API versioning strategies may be found here [versioning-strategy.md](..\/supporting-notes\/versioning-strategy.md)\\n\\n## Decision\\n* We will be using semantic versioning [https:\/\/semver.org](https:\/\/semver.org) see [versioning-strategy.md#semver](..\/supporting-notes\/versioning-strategy.md#semver) for more\\n* We will implement versioning via the Content Negotiation using the Accept header, as per [versioning-strategy.md#options-5](..\/supporting-notes\/versioning-strategy.md#options-5). This seems the most future proof, most RESTful solution.\\nThis necessitates our own vendor content type. Examples:\\n* `application\/vnd.opg-data.v1+json` (v1 presented as JSON)\\n* `application\/vnd.opg-data.v1+yml` (v1 presented as YAML)\\n* `application\/vnd.opg-data.v1` (v1 presented as JSON)\\n* `application\/vnd.opg-data` (latest version, presented as JSON)\\n* `application\/json` (latest version, as JSON)\\nThe final two are dangerous in that the version presented will change over time without warning.\\nSee also: [4. Content Negotiation](0004-content-negotiation.md)\\n* At any given time, the API will allow requests from two major versions: The latest version and the previous, deprecated version.\\n* All API responses will contain a x-current-api-version which will be set to the current version of the API in addition to an x-api-warn header when calling any deprecated API endpoints.\\n* A Versions Timeline Document kept up to date with all changes and presented at an endpoint `api\/release-info` from within the API itself\\n* When using a version-non-specific content-type as with the final two examples of the preceding list, an x-api-warn header will be returned, warning the client to specify a version\\n\\n\\n## Context and Problem Statement\\nWhen [deciding to build this tool](.\/0000-whats-the-big-idea.md), the main risk identified was that we\\'d spend too\\nmuch time on an ultimately unworkable or unmaintainable idea. What early design decisions can we make to mitigate\\nthis risk? What things are an existential risk to the success of this project that must be included in the first\\nversion, and what things can we safely defer to future work?\\nIn other words: how do we build an MVP of this tool that is both *minimal* and *viable*?\\n## Decision Drivers\\n* Strictly timebox our efforts to \"prove out\" the approach.\\n* Establish whether we can effectively maintain this kind of tool as a team.\\n* Support initial development of a new rust component with externally-imposed, near-term deadlines.\\n\\n## Decision\\n* Strictly timebox our efforts to \"prove out\" the approach.\\n* Establish whether we can effectively maintain this kind of tool as a team.\\n* Support initial development of a new rust component with externally-imposed, near-term deadlines.\\nChosen options:\\n* **Option A: Use an external interface definition file based on WebIDL.**\\n* **Option E: Provide a tool that developers need to run by hand.**\\n* **Option F: Go broad, implementing many data types and API capabilities, even if they\\'re slow or incomplete.**\\nThe set of options chosen here makes an explicit tradeoff, preferring to get something up and running quickly\\nand accepting a certain amount of jank in the developer experience. We don\\'t have to build the perfect tool\\nright away, we only have to build something that\\'s better than doing this work by hand. If we like the result\\nwe can polish it from there.\\nThe MVP tool will read API definitions from an external WebIDL file. This will be a bit weird and inconvenient\\nfor consumers because WebIDL is not a precise fit for our needs, but it avoids us bikeshedding the perfect\\nAPI-definition experience during this first phase.\\nThe MVP developer experience will involve `cargo install`ing the tool onto your system and manually running it\\nor integrating it into your build process. This risks being mildly inconvenient for consumers, but gives us\\nlots of flexibility while we learn about what a better workflow might look like.\\nThe MVP tool may support more features than turn out to be strictly necessary, in the interests of ensuring\\nmultiple team members can be involved in its development at this early stage. As a tradeoff, the MVP generated\\ncode will be allowed to contain inefficiencies and limitations that hand-written code might not,\\non the premise that our first consumers are not very performance-sensitive, and that there is a lot of scope for\\nimproving these implementation details over time.\\nWe are likely to ***revisit every single one of these choices*** if the MVP of the tool proves successful,\\nand will attempt to build it in such a what that they\\'re easy to revisit.\\n\\n\\n\\nProvide a decision given the context below:\\n# Context\\\\nGranary will be a standalone deployable open source application. Users\\' primary\\\\nway of interacting with Granary will be through its REST API, rather than\\\\nthrough a UI, at least for the foreseeable future. This focus puts a larger\\\\nburden than usual on the quality of our API documentation.\\\\nWe\\'d like to evaluate different ways to keep API documentation up-to-date. For\\\\neach strategy, we want to know:\\\\n- What are the general pros and cons?\\\\n- How do we find out that our docs have drifted from the API?\\\\n- How can we document different versions of the API at the same time?\\\\n'}, {'role': 'model', 'content': \"We should use `tapir` for automatically generating API documentation. The ADT\\\\nsupport and straightforward API (inputs use `.in`, outputs use `.out`, auth\\\\nextractors use `auth`) will flatten out the learning curve, and we'll have a\\\\nstable and correct reference point for API documentation that users setting up\\\\ntheir own deployments can refer to. We can call this out at the beginning of\\\\nthe README and hopefully save ourselves from having to answer an entire\\\\ncategory of questions.\\\\n# Consequences\\\\n- The first routes added to the API will be slightly more difficult, because\\\\nthey'll include writing API routes with a new library for the first time.\\\\n- The README should be updated to point to the location of the API\\\\ndocumentation.\\\\n\"}]"}
{"File Name":"oklog\/adr-001-multitenancy.md","Context":"## Context\\nSystem operators often run shared infrastructure for different workloads. For\\nexample, a single Kubernetes cluster may serve multiple departments in an\\norganization, none of whom should need to know about the others.\\nOK Log as it exists today (v0.3.2) assumes all users (both producers and\\nconsumers) are part of the same global namespace, and provides no way to\\nsegment ingestion or querying.\\nWe have at least one interesting use case, with one potential user, where\\nmulti-tenanancy would be a requirement.\\n","Decision":"Motivated by this new use case, we judge that adding multi-tenant features, in\\naddition to a handful of other longstanding feature requests, would push OK Log\\nin a useful direction.\\nThe initial set of issues include:\\n- [Add first-class concept of topics](https:\/\/github.com\/oklog\/oklog\/issues\/113)\\n- Separate indexer layer for faster queries\\n- [Move to length-delimited records](https:\/\/github.com\/oklog\/oklog\/issues\/112)\\n- [Extend record identifier](https:\/\/github.com\/oklog\/oklog\/issues\/114)\\n- [Long-term storage](https:\/\/github.com\/oklog\/oklog\/issues\/115)\\nAdditional issues may be filed, and these issues may be refactored or dropped\\noutright depending on the result of experimentation.\\n","tokens":115,"id":1836,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nThe Identity platform consists of numerous interconnected components,\\neach responsible for generating their own log outputs. Some of these log\\noutputs will be driven by the implementation of logging within a\\ncomponent, and others will be provided \"out-of-the-box\" by a managed\\nservice. In either case, these logs files must be readily accessible,\\nqueryable, and persisted over a long period.\\n## Decision Drivers\\n* Collation of logs to a single, centralised platform utilised by all\\nplatforms in Wellcome Collection\\'s technical estate.\\n* Minimal infrastructure \/ configuration required in components to\\nutilise the centralised logging platform.\\n\\n## Decision\\n* Collation of logs to a single, centralised platform utilised by all\\nplatforms in Wellcome Collection\\'s technical estate.\\n* Minimal infrastructure \/ configuration required in components to\\nutilise the centralised logging platform.\\n- For containers deployed into AWS ECS \/ AWS Fargate, Wellcome\\nCollection\\'s existing ELK stack will be utilised. Logs will be pushed\\nto the stack via a Firelens sidecar containers.\\n- For all log generating services (e.g. Lambda Function and API Gateway\\nlogs), the default AWS CloudWatch configurations will be used.\\n\\n\\n## Context\\nWe needed to test parallel login of users to ensure that\\nsession data isn\\'t accidentally shared between users\\n(see [decision 0002](.\/0002-custom-save-handler-in-service-front.md)).\\nAlso, Make an LPA currently has no load tests. These are useful\\nfor capacity planning and finding bottlenecks which may be\\ncausing errors for users on the site. While we are auto-scaling,\\nwe don\\'t have a way to verify that it is sufficient to manage\\nexpected load, which load testing can provide.\\nLoad testing can also provide a benchmark we can use to ensure\\nthat any new work we do does not degrade performance of the stack\\nas a whole.\\n\\n## Decision\\nUse [locust.io](https:\/\/locust.io\/) to test parallel logins.\\nWhile there are alternatives to locust, it is implemented in Python\\n(our chosen language going forward), has an intuitive API, and some\\nmembers of the team have experience with it already (and can reuse\\npreviously-written code).\\nLoad tests will be added to the tests\/load directory in the project,\\nas they are not component-specific and apply to the whole stack.\\nDoing the above has the happy side effect of opening up the possibility of\\nload testing the whole application stack in future.\\nInitially, this will only run locally and is not integrated into\\nthe CI pipeline. We will consider extending this testing into CI\\nin future.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nSystem operators often run shared infrastructure for different workloads. For\\\\nexample, a single Kubernetes cluster may serve multiple departments in an\\\\norganization, none of whom should need to know about the others.\\\\nOK Log as it exists today (v0.3.2) assumes all users (both producers and\\\\nconsumers) are part of the same global namespace, and provides no way to\\\\nsegment ingestion or querying.\\\\nWe have at least one interesting use case, with one potential user, where\\\\nmulti-tenanancy would be a requirement.\\\\n'}, {'role': 'model', 'content': 'Motivated by this new use case, we judge that adding multi-tenant features, in\\\\naddition to a handful of other longstanding feature requests, would push OK Log\\\\nin a useful direction.\\\\nThe initial set of issues include:\\\\n- [Add first-class concept of topics](https:\/\/github.com\/oklog\/oklog\/issues\/113)\\\\n- Separate indexer layer for faster queries\\\\n- [Move to length-delimited records](https:\/\/github.com\/oklog\/oklog\/issues\/112)\\\\n- [Extend record identifier](https:\/\/github.com\/oklog\/oklog\/issues\/114)\\\\n- [Long-term storage](https:\/\/github.com\/oklog\/oklog\/issues\/115)\\\\nAdditional issues may be filed, and these issues may be refactored or dropped\\\\noutright depending on the result of experimentation.\\\\n'}]"}
{"File Name":"embvm-core\/0009-event-driven-framework-design.md","Context":"## Context\\n* Embedded systems are highly event-driven, as they are responding to external stimuli and reacting in a planned way\\n* Event-driven APIs reduce coupling, as the various objects don't need to know anything about other objects that they work with\\n* We can reduce the number of threads used by relying on event-driven behavior\\n","Decision":"The framework will be defined with interfaces and processing models that support event-driven development (callbacks, events, register\/unregister for events).\\nDispatch queues will be provided to assist with the event driven model.\\nPlatform examples will default to dispatch-based processing models.\\n","tokens":66,"id":3007,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\n* How do we create instances of our classes in the application?\\n* How to pass dependencies to the constructors?\\n* How to ensure that some objects can be shared, while others are created every time they are needed?\\n## Decision Drivers\\n* We want the object creation to be type-safe to be able to use the code intelligence features of the IDE,\\nthe strict type checking of PHP and the static analysis capabilities of PHPStan.\\n* We want the object creation to support adhering to the SOLID principles:\\n* The single responsibility principle will lead to lots of classes, depending on each other in a hierarchy.\\nAlso, our instance creation solution should separate the concerns of instantiation from what the classes are doing.\\n* The Open-Closed principle, when solved with interfaces, makes it necessary to create different instances for the\\nsame interface and support switching out instances, e.g. for testing.\\n* The Dont-Repeat-Yourself principle means we should have one place where classes are instantiated.\\n* The Edge2Edge and integration tests need an environment that resembles the production environment as closely as\\npossible, while still being able to switch out services to isolate from side effects (database contents, filesystem,\\nrandomness, date, etc)\\n* We want to keep the public API of our code (the methods exposed to the web and command line framework code) as small\\nas possible. Ideally, the API exposes only the use cases.\\n\\n## Decision\\n* We want the object creation to be type-safe to be able to use the code intelligence features of the IDE,\\nthe strict type checking of PHP and the static analysis capabilities of PHPStan.\\n* We want the object creation to support adhering to the SOLID principles:\\n* The single responsibility principle will lead to lots of classes, depending on each other in a hierarchy.\\nAlso, our instance creation solution should separate the concerns of instantiation from what the classes are doing.\\n* The Open-Closed principle, when solved with interfaces, makes it necessary to create different instances for the\\nsame interface and support switching out instances, e.g. for testing.\\n* The Dont-Repeat-Yourself principle means we should have one place where classes are instantiated.\\n* The Edge2Edge and integration tests need an environment that resembles the production environment as closely as\\npossible, while still being able to switch out services to isolate from side effects (database contents, filesystem,\\nrandomness, date, etc)\\n* We want to keep the public API of our code (the methods exposed to the web and command line framework code) as small\\nas possible. Ideally, the API exposes only the use cases.\\nWe chose the \"Central Factory\" (a class called `FunFunFactory`) because type-safety and IDE comfort (auto-completion)\\ntrumped the concerns about boilerplate. We also saw not binding to a specific library or framework a plus.\\nWe\\'re still using the Pimple DIC internally to get shared objects. We wrapped the access to instances in Pimple with type-safe in getters.\\nIn hindsight, this use of Pimple as a holder of shared objects turned out to be a bad\\nidea because the creation functions (how we build objects) are too far away from the code that requests them and there\\nis no easy way to jump from a getter to the creation function. When we realized that, we switched from Pimple to the\\n`createSharedObject` function, but did not refactor all creation functions.\\nWe use naming conventions to give the consumers a hint if an object is shared (a singleton) or newly created: methods\\nstarting with `get` return shared objects, methods with `new` return \"fresh\" objects on every call.\\nWe extracted the use cases into bounded contexts at a late stage of the project. We attempted to split the factory\\nfor use cases into  \"ContextFactory\" classes, but that attempt was half-hearted and difficult to do, since some of the\\nunderlying dependencies (db connection, entity manager) only make sense when an application uses a bounded context.\\nThe `FunFunFactory` is has become long and disorganized. Unfortunately, because we did not pay attention to boundaries\\nacross bounded context or abstraction layers. We did not apply the naming conventions consistently.\\n`FunFunFactory` contains setters that make it possible to switch out implementations for testing. They are also used to set\\nup environment-specific services like logging (See the classes in the `WMDE\\\\Fundraising\\\\Frontend\\\\Factories\\\\EnvironmentSetup`\\nnamespace). From an architectural perspective this is bad, since it potentially allows controllers to switch out their\\nservices, which shouldn\\'t be allowed.\\n\\n\\n## Context\\nOur back-end needs to store state somehow.\\nTraditional Object-Relational systems store the current state of the application as the source of truth.\\nWhile these are simple to build, they are less conducive to changes in the way data is stored in the long run.\\nSystems built under our current shareholders have shown to be very prone to change over time, which gives us the need for systems that are conducive to change.\\nOur experience with our previous system under these shareholders, `Wegwijs`, has shown to be very permissive to change. `Wegwijs` had rather simple data manipulations, with the difficulty of the system lying in the presentation of this data.\\nWe built `Wegwijs` using event sourcing, which uses a record of all changes as its source of truth. These events can then be used to create different representations of the data using `projections`.\\nDue to this, we were able to capture the manipulations easily, while still allowing the stakeholders to change their mind about the presentation of this data.\\n\\n## Decision\\nWe will use event sourcing the single source of truth for our application.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\n* Embedded systems are highly event-driven, as they are responding to external stimuli and reacting in a planned way\\\\n* Event-driven APIs reduce coupling, as the various objects don\\'t need to know anything about other objects that they work with\\\\n* We can reduce the number of threads used by relying on event-driven behavior\\\\n'}, {'role': 'model', 'content': 'The framework will be defined with interfaces and processing models that support event-driven development (callbacks, events, register\/unregister for events).\\\\nDispatch queues will be provided to assist with the event driven model.\\\\nPlatform examples will default to dispatch-based processing models.\\\\n'}]"}
{"File Name":"Sylius\/2021_06_15_api_platform_config_customization.md","Context":"## Context and Problem Statement\\nSylius is by design application that will be adjusted to customer needs.\\nTherefore each part of it has to be crafted with replaceability or customizability at its core.\\nNonetheless, the current state of API Platform integration requires the replacement of the whole config of the whole resource.\\nIn terms of the more complicated cases like Order or Customer, this practice may become error-prone and hard to maintain for both maintainers and Sylius users.\\n","Decision":"Chosen option: \"Config merging\", because it allows us easily overwrite any endpoint, without getting deep into api platform resources.\\n","tokens":98,"id":680,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\n1. The framework rationale requires a separation of concern between \"code\\nis code, data is data\" and because of that some libraries were not\\nable to proceed to be used in the application.\\n2. The framework also wants to be a complete solution which means that\\nour users should not be required to learn too many libraries in order\\nto be productive. Instead, the framework should provide layers of\\nindirection to wrap such functionalities and provide centralized\\ncontrol over the features.\\n\\n## Decision\\n- Usage of `yogthos\/config` to handle config files\\n- Usage of `stuartsierra\/components` to handle dependency management\\n- Usage of `honeysql` to handle SQL interactions\\n- Build a framework layer to handle migrations in `honeysql` style\\n- Build framework layer to wrap honeysql, honeysql-plugins, next-jdbc,\\nhikari-cp and possible others lirbaries.\\n\\n\\n## Context and problem statement\\nThe core of Archivematica is a server application that coordinates work across\\nmultiple workers. The collection of related tasks (also known as *workflow\\ndata*) is stored in the application database using a relation model.\\nIntroducing changes to the workflow data is hard because the model is clumsy and\\nconvoluted. Changes to the data are delivered as data and schema migrations\\nwhich are difficult to implement and they add risk to the process of upgrading\\nthe software.\\nAdditionally, we want to add full internationalisation support in order to make\\nthe system more attractive to a wider range of users and increase adoption. We\\nneed a simpler mechanism to deliver translations and other workflow data changes\\nin general.\\n\\n## Decision\\n* Archivematica 1.9 will implement the proposed solution replacing the existing\\ndatabase tables.\\n* We do not currently intend to expose the ability to modify the workflow data\\nto Archivematica users - but this may change in the future as the solution\\nmatures and becomes interoperable.\\n* The visualization and manipulation of the workflow data will be facilitated by\\na new tool that we\\'re calling amflow.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\nSylius is by design application that will be adjusted to customer needs.\\\\nTherefore each part of it has to be crafted with replaceability or customizability at its core.\\\\nNonetheless, the current state of API Platform integration requires the replacement of the whole config of the whole resource.\\\\nIn terms of the more complicated cases like Order or Customer, this practice may become error-prone and hard to maintain for both maintainers and Sylius users.\\\\n'}, {'role': 'model', 'content': 'Chosen option: \"Config merging\", because it allows us easily overwrite any endpoint, without getting deep into api platform resources.\\\\n'}]"}
{"File Name":"wikiindex\/adr-002-routing_library.md","Context":"## Context\\n* We are building a web app that has one official url that uses query params\\n* The url is \/search?q=search-term\\n* We believe that future endpoints will be added, or more complicated search requests\\n","Decision":"* We will use Bidi to parse the routes.\\n","tokens":46,"id":1384,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\n* We need to index and search a 32Mb XML file.\\n* The search is across multiple fields.\\n* Only one search word is used.\\n* We currently don\\'t have any external platform tools like Mongo, ElasticSearch, an RDMS\\n* We do not know how often the XML file will be updated.\\n\\n## Decision\\n* We will parse the file on startup, store it as a clojure collection that is available throughout the system.\\n* We will do this before the web server http port is available.\\n* Searches will be performed against this in memory collection.\\n* The app would need to be restarted to parse a new XML file, unless an additional endpoint is added.\\n\\n\\n## Context and Problem Statement\\nWhen quering for a title of a paper, the title might contain stop words such as \"a\", \"for\", \"and\". Some data providers return 0 results when querying for a stop word. When transforming a query to the lucene syntax, the default Boolean operator `and` is used. When using IEEE, this often leads to zero search results.\\n## Decision Drivers\\n* Consistent to the Google search engine\\n* Allow reproducible searches\\n* Avoid WTFs on the user\\'s side\\n\\n## Decision\\n* Consistent to the Google search engine\\n* Allow reproducible searches\\n* Avoid WTFs on the user\\'s side\\nChosen option: \"Remove stop words from the query\", because comes out best.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\n* We are building a web app that has one official url that uses query params\\\\n* The url is \/search?q=search-term\\\\n* We believe that future endpoints will be added, or more complicated search requests\\\\n'}, {'role': 'model', 'content': '* We will use Bidi to parse the routes.\\\\n'}]"}
{"File Name":"infrastructure-adrs\/0001-migrate-off-fedora-3.md","Context":"## Context and Problem Statement <!-- required -->\\nFedora 3 is unsupported and has been unsupported for four years; it is unlikely to be supported in the foreseeable future. It also requires a difficult-to-support version of the Java Virtual Machine. And yet, Fedora 3 is the cornerstone of our management \"repository,\" in which all SDR content is managed and from which said content flows to access and preservation environments. At the same time, there is a dwindling number of organizations in the cultural heritage community who are still using Fedora 3.\\n## Decision Drivers <!-- optional -->\\n* Fedora 3 is unsupported and unlikely to be supported\\n* Fedora 3 will be harder to install on newer operating systems\\n* The Fedora 3 data model is not inherently validatable\\n* The Fedora 3 community is disappearing, so we are increasingly going it alone\\n* Fedora 3 is a critical piece of SDR infrastructure and represents an enormous risk\\n* Samvera software that supports Fedora 3 is outdated and maintained\/supported only through our own efforts, preventing us from using mainstream Samvera software\\n* We have (unverified) concerns about the scalability of Fedora 3\\n","Decision":"* Fedora 3 is unsupported and unlikely to be supported\\n* Fedora 3 will be harder to install on newer operating systems\\n* The Fedora 3 data model is not inherently validatable\\n* The Fedora 3 community is disappearing, so we are increasingly going it alone\\n* Fedora 3 is a critical piece of SDR infrastructure and represents an enormous risk\\n* Samvera software that supports Fedora 3 is outdated and maintained\/supported only through our own efforts, preventing us from using mainstream Samvera software\\n* We have (unverified) concerns about the scalability of Fedora 3\\nNo decision made yet. See status field above.\\n","tokens":241,"id":796,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\nContext\\n-------\\nWe have used Docker containers, in various ways, on previous projects and our experience has been mixed but overall positive. Containers help to speed up CI builds and deployments, at the cost of added complexity and usage frustrations, especially for non-Linux users (most notably designers). We\\'ve decided that the best way to use Docker containers for local development is inside of a Vagrant VM, which lessens some of their benefits for local development but also improves the experience for non-Linux users.\\nFor the past three years or so, we have used Ubuntu as the basis for every new project which required an operating system.\\nThere are other container specifications, such as Rocket, and obviously, other Linux distributions. However, we do not have major complaints about either Docker containers or Ubuntu.\\nDecision\\n--------\\nWe will continue to use Docker containers to encapsulate running applications, and to use Ubuntu as our host operating system. We have developed quite a bit of experience with both Ubuntu and Docker, and we\\'re generally satisfied with both of them, so there are not compelling reasons to make changes to such foundational components of our software stack at this point. In theory, the isolation introduced by containers should allow us to swap out either the host OS or the container technology without (much) fear of unforeseen consequences, so this decision can be revised later if necessary.\\nIt is possible that some deployment decisions may necessitate or encourage the use of other Linux distributions, either for host machines or as base Docker images. For example, some Linux distributions have base images which are significantly smaller than Ubuntu\\'s base image. Similarly, AWS provides Elastic-Container-Service-optimized AMIs that we may choose to use over Ubuntu base AMIs. We will make decisions on a case-by-case basis whether to use these other distributions but we anticipate continuing to use Ubuntu locally for development.\\nConsequences\\n------------\\nWe will need to develop a Docker-centric system architecture and development environment, and determine how best to integrate Docker and Spark. We will need to continue to be vigilant about maintaining a positive development experience for non-Linux users.\\n\\n## Decision\\n--------\\nWe will continue to use Docker containers to encapsulate running applications, and to use Ubuntu as our host operating system. We have developed quite a bit of experience with both Ubuntu and Docker, and we\\'re generally satisfied with both of them, so there are not compelling reasons to make changes to such foundational components of our software stack at this point. In theory, the isolation introduced by containers should allow us to swap out either the host OS or the container technology without (much) fear of unforeseen consequences, so this decision can be revised later if necessary.\\nIt is possible that some deployment decisions may necessitate or encourage the use of other Linux distributions, either for host machines or as base Docker images. For example, some Linux distributions have base images which are significantly smaller than Ubuntu\\'s base image. Similarly, AWS provides Elastic-Container-Service-optimized AMIs that we may choose to use over Ubuntu base AMIs. We will make decisions on a case-by-case basis whether to use these other distributions but we anticipate continuing to use Ubuntu locally for development.\\nConsequences\\n------------\\nWe will need to develop a Docker-centric system architecture and development environment, and determine how best to integrate Docker and Spark. We will need to continue to be vigilant about maintaining a positive development experience for non-Linux users.\\n\\n\\n## Context and Problem Statement <!-- required -->\\nCurrently the way to create new objects in the SDR is to:\\n1. Register an object (typically using Argo)\\n1. Move files into the `\/dor` mount\\n1. Run the assembly workflow (pre-assembly?)\\nThe Google Books project will grow SDR by millions of new objects. This growth is an opportunity to inject an API into the SDR architecture that serves as the \"face\" of SDR. Creating an external-facing API allows SDR to accept deposits from on-campus stakeholders such as Big Local News and Open Neuro. Fronting SDR with a new API affords us a mechanism to validate the completeness and correctness of deposited data very early in the accessioning process. Furthermore, we might also use such an API to refactor internal projects (such as ETDs) so these projects no longer require intimate knowledge of the internals of SDR, which reduces coupling.\\n## Decision Drivers <!-- optional -->\\n* APIs make it possible to decouple implementations. Our current deposit process is tightly coupled to (Fedora) persistence and we would like to sever this coupling.\\n* We have potential projects on campus that would like to deposit to SDR in self-service mode.\\n* Bad data is occasionally deposited, causing problems in the workflow. Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.\\n\\n## Decision\\n* APIs make it possible to decouple implementations. Our current deposit process is tightly coupled to (Fedora) persistence and we would like to sever this coupling.\\n* We have potential projects on campus that would like to deposit to SDR in self-service mode.\\n* Bad data is occasionally deposited, causing problems in the workflow. Fronting SDR with an API that strictly validates deposits would help us detect these problems earlier.\\nOption 2: we will create a deposit API as part of work on the 2020 Google Books work cycle.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement <!-- required -->\\\\nFedora 3 is unsupported and has been unsupported for four years; it is unlikely to be supported in the foreseeable future. It also requires a difficult-to-support version of the Java Virtual Machine. And yet, Fedora 3 is the cornerstone of our management \"repository,\" in which all SDR content is managed and from which said content flows to access and preservation environments. At the same time, there is a dwindling number of organizations in the cultural heritage community who are still using Fedora 3.\\\\n## Decision Drivers <!-- optional -->\\\\n* Fedora 3 is unsupported and unlikely to be supported\\\\n* Fedora 3 will be harder to install on newer operating systems\\\\n* The Fedora 3 data model is not inherently validatable\\\\n* The Fedora 3 community is disappearing, so we are increasingly going it alone\\\\n* Fedora 3 is a critical piece of SDR infrastructure and represents an enormous risk\\\\n* Samvera software that supports Fedora 3 is outdated and maintained\/supported only through our own efforts, preventing us from using mainstream Samvera software\\\\n* We have (unverified) concerns about the scalability of Fedora 3\\\\n'}, {'role': 'model', 'content': '* Fedora 3 is unsupported and unlikely to be supported\\\\n* Fedora 3 will be harder to install on newer operating systems\\\\n* The Fedora 3 data model is not inherently validatable\\\\n* The Fedora 3 community is disappearing, so we are increasingly going it alone\\\\n* Fedora 3 is a critical piece of SDR infrastructure and represents an enormous risk\\\\n* Samvera software that supports Fedora 3 is outdated and maintained\/supported only through our own efforts, preventing us from using mainstream Samvera software\\\\n* We have (unverified) concerns about the scalability of Fedora 3\\\\nNo decision made yet. See status field above.\\\\n'}]"}
{"File Name":"platform\/2021-10-13-refund-handling.md","Context":"## Context\\nShopware offers no way of unified refund handling. This results in every payment extension either implementing it themselves or not at all.\\n","Decision":"We want to implement the following structure to offer a unified refund handling for all extension types.\\n","tokens":29,"id":4527,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nMultiple projects need to publish or subscribe to messages from the `bus`.\\nProjects are written in a fixed set of supported programming languages.\\nThere is a set of non-functional requirements to cater for in integrations, especially when listening to queues (retries, graceful shutdown, timeouts, etc).\\n\\n## Decision\\nAccess the bus exclusively through the eLife SDKs, either for publishing or subscribing to messages:\\n- [bus-sdk-php](https:\/\/github.com\/elifesciences\/bus-sdk-php)\\n- [bus-sdk-python](https:\/\/github.com\/elifesciences\/bus-sdk-python)\\n\\n\\n## Context\\nIt cannot be assumed that users of this module would manage DNS in AWS or that users may have a cross account design that prevents access to the hosted zone.\\n\\n## Decision\\nProvide a means to create the custom Route53 resource as an option\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nShopware offers no way of unified refund handling. This results in every payment extension either implementing it themselves or not at all.\\\\n'}, {'role': 'model', 'content': 'We want to implement the following structure to offer a unified refund handling for all extension types.\\\\n'}]"}
{"File Name":"figgy\/0006-preservation-tombstones.md","Context":"## Context\\nAfter an item is deleted from the database all traces of it are gone, but if\\nit's been preserved then it's possible to restore it. However, finding the\\nparticular ID of the item to restore is difficult if all that's known are pieces\\nof its metadata (title, source metadata ID, etc.)\\nWe will remedy this by storing a \"tombstone\" of deleted items as a small record\\nof what was deleted.\\n","Decision":"1. When a preserved item is deleted it will create a \"tombstone\" containing the following\\nmetadata:\\n1. ID of deleted resource\\n1. Title of deleted resource\\n1. Original Filename of deleted resource (if a FileSet)\\n1. Embedded PreservationObject that existed at time of deletion\\n1. ID of Parent at time of deletion.\\n2. These tombstones will be displayed in the Figgy UI and used as a way to\\ndiscover material that can be recovered.\\n","tokens":96,"id":4827,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n**Context**: operations that change state in the server may complete with an\\nindeterminate status code. For example: an error in the 500 range may be\\nproduced by a middle-tier server after the operation was started by the backend.\\nRe-issuing the request may not work as expected for the application developer.\\n**Decision**: by default the library will retry all requests, including\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more\\nrare cases the operation may fail, for example, an operation to create an object\\nwith `IfGenerationMatch(0)` would fail on the second attempt.\\n\\n## Decision\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more\\nrare cases the operation may fail, for example, an operation to create an object\\nwith `IfGenerationMatch(0)` would fail on the second attempt.\\n\\n\\n## Context\\nSometimes we make a change to the catalog config set that will break search results if it\\'s deployed without a new index already in place. In these cases the config set in use cannot be updated with this change and we have to deploy a different config set for the reindexing collection, which is then swapped in once it\\'s fully populated.\\n\\n## Decision\\nFor most config changes we will continue updating and reloading the config set in production.\\nIn the cases described above we will copy a new config set from the most recent production config set, incrementing the version suffix. Once the index is created and swapped in, the previously-used config set should be deleted.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nAfter an item is deleted from the database all traces of it are gone, but if\\\\nit\\'s been preserved then it\\'s possible to restore it. However, finding the\\\\nparticular ID of the item to restore is difficult if all that\\'s known are pieces\\\\nof its metadata (title, source metadata ID, etc.)\\\\nWe will remedy this by storing a \"tombstone\" of deleted items as a small record\\\\nof what was deleted.\\\\n'}, {'role': 'model', 'content': '1. When a preserved item is deleted it will create a \"tombstone\" containing the following\\\\nmetadata:\\\\n1. ID of deleted resource\\\\n1. Title of deleted resource\\\\n1. Original Filename of deleted resource (if a FileSet)\\\\n1. Embedded PreservationObject that existed at time of deletion\\\\n1. ID of Parent at time of deletion.\\\\n2. These tombstones will be displayed in the Figgy UI and used as a way to\\\\ndiscover material that can be recovered.\\\\n'}]"}
{"File Name":"react-native-app\/0004-use-amplitude-for-analytics.md","Context":"## Context\\nTo build the product we need to understand users\\n","Decision":"We will use [Amplitude's free plan](https:\/\/amplitude.com\/pricing?ref=nav), it has everything that we need at the beginning.\\n","tokens":13,"id":4212,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe need to choose a language to in which to code this project.\\n\\n## Decision\\nUse Golang.\\n\\n\\n## Context and Problem Statement\\n### Context\\nHow best can we structure the API project to make it easy for developers to navigate and comprehend.\\n\\n## Decision\\n\"Organise by Feature and use MediatR\", because we can use MediatR to encapsulate the various use-cases of the API. Together with feature folders this should allow developers to more easily identify the capabilities of the API and target the section they need to.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nTo build the product we need to understand users\\\\n'}, {'role': 'model', 'content': \"We will use [Amplitude's free plan](https:\/\/amplitude.com\/pricing?ref=nav), it has everything that we need at the beginning.\\\\n\"}]"}
{"File Name":"java-template\/0002-hexagonal-architecture.md","Context":"## Context and Problem Statement\\n- How can we maintain a clean design?\\n- How will we be able to maintain our architecture iteratively in accordance with the TDD practice?\\n## Decision Drivers\\n* We employ TDD, which favors small iterations.\\n* We aim to implement full Continuous Delivery.\\n","Decision":"* We employ TDD, which favors small iterations.\\n* We aim to implement full Continuous Delivery.\\n","tokens":63,"id":4805,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nWe need to measure uptime for at least two reasons:\\n1. To serve as feedback on what needs to be improved next.\\n2. To demonstrate compliance with our SLAs.\\nHow exactly should we measure uptime?\\n## Decision Drivers\\n* We want to reduce tools sprawl.\\n* We want to be mindful about capacity and infrastructure costs.\\n* We want to measure uptime as observed by a consumer -- i.e., application or user -- taking into account business continuity measures, such as redundancy, fail-over time, etc.\\n\\n## Decision\\n* We want to reduce tools sprawl.\\n* We want to be mindful about capacity and infrastructure costs.\\n* We want to measure uptime as observed by a consumer -- i.e., application or user -- taking into account business continuity measures, such as redundancy, fail-over time, etc.\\nChosen option: \"use Probe for measuring uptime of internal Compliant Kubernetes services\", because it measures uptime as observed by a consumer. Although this requires a bit of extra capacity for running Blackbox, the costs are worth the benefits.\\nInstead of configuring Blackbox directly, `Probe` is a cleaner abstraction provided by the Prometheus Operator.\\nThe following is an example for a Probe:\\n```yaml\\napiVersion: monitoring.coreos.com\/v1\\nkind: Probe\\nmetadata:\\nname: google-is-up\\nlabels:\\nprobe: google\\nrelease: kube-prometheus-stack\\nspec:\\ninterval: 60s\\nmodule: http_2xx\\nprober:\\nurl: blackbox-prometheus-blackbox-exporter.monitoring.svc.cluster.local:9115\\ntargets:\\nstaticConfig:\\nstatic:\\n- https:\/\/www.google.com\\n```\\nThis will generate a metric as follows: `probe_success{cluster=\"ckdemo-wc\", instance=\"https:\/\/www.google.com\", job=\"probe\/demo1\/google-is-up\", namespace=\"demo1\"}`.\\n### Positive Consequences\\n* We measure uptime as observed by a consumer.\\n* Increasing redundancy, reducing failure time, etc. will contribute positively to our uptime, as desired.\\n### Negative Consequences\\n* We don\\'t currently run Blackbox in the workload cluster, so we\\'ll need a bit of extra capacity.\\n\\n\\n## Context and Problem Statement\\nWhen [deciding to build this tool](.\/0000-whats-the-big-idea.md), the main risk identified was that we\\'d spend too\\nmuch time on an ultimately unworkable or unmaintainable idea. What early design decisions can we make to mitigate\\nthis risk? What things are an existential risk to the success of this project that must be included in the first\\nversion, and what things can we safely defer to future work?\\nIn other words: how do we build an MVP of this tool that is both *minimal* and *viable*?\\n## Decision Drivers\\n* Strictly timebox our efforts to \"prove out\" the approach.\\n* Establish whether we can effectively maintain this kind of tool as a team.\\n* Support initial development of a new rust component with externally-imposed, near-term deadlines.\\n\\n## Decision\\n* Strictly timebox our efforts to \"prove out\" the approach.\\n* Establish whether we can effectively maintain this kind of tool as a team.\\n* Support initial development of a new rust component with externally-imposed, near-term deadlines.\\nChosen options:\\n* **Option A: Use an external interface definition file based on WebIDL.**\\n* **Option E: Provide a tool that developers need to run by hand.**\\n* **Option F: Go broad, implementing many data types and API capabilities, even if they\\'re slow or incomplete.**\\nThe set of options chosen here makes an explicit tradeoff, preferring to get something up and running quickly\\nand accepting a certain amount of jank in the developer experience. We don\\'t have to build the perfect tool\\nright away, we only have to build something that\\'s better than doing this work by hand. If we like the result\\nwe can polish it from there.\\nThe MVP tool will read API definitions from an external WebIDL file. This will be a bit weird and inconvenient\\nfor consumers because WebIDL is not a precise fit for our needs, but it avoids us bikeshedding the perfect\\nAPI-definition experience during this first phase.\\nThe MVP developer experience will involve `cargo install`ing the tool onto your system and manually running it\\nor integrating it into your build process. This risks being mildly inconvenient for consumers, but gives us\\nlots of flexibility while we learn about what a better workflow might look like.\\nThe MVP tool may support more features than turn out to be strictly necessary, in the interests of ensuring\\nmultiple team members can be involved in its development at this early stage. As a tradeoff, the MVP generated\\ncode will be allowed to contain inefficiencies and limitations that hand-written code might not,\\non the premise that our first consumers are not very performance-sensitive, and that there is a lot of scope for\\nimproving these implementation details over time.\\nWe are likely to ***revisit every single one of these choices*** if the MVP of the tool proves successful,\\nand will attempt to build it in such a what that they\\'re easy to revisit.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\n- How can we maintain a clean design?\\\\n- How will we be able to maintain our architecture iteratively in accordance with the TDD practice?\\\\n## Decision Drivers\\\\n* We employ TDD, which favors small iterations.\\\\n* We aim to implement full Continuous Delivery.\\\\n'}, {'role': 'model', 'content': '* We employ TDD, which favors small iterations.\\\\n* We aim to implement full Continuous Delivery.\\\\n'}]"}
{"File Name":"origin\/0006-rename-assets-to-devices.md","Context":"## Context\\nThere has been confusion on why we call generating and consuming devices Assets in Origin. We have noticed that most Origin users would refer to Assets as Devices and not Assets.\\n","Decision":"We decided to change our terminology to refer to Assets as Devices, to keep in line with the prevailing terminology in the industry.\\n","tokens":37,"id":1555,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nIn the past we were approaching an entity repository pattern for reading and writing HEPTAconnect internal data.\\nThis has been useful for building applications inside various frameworks in the past as their storages often come with a database abstraction layer that follows the same pattern.\\nIn any implementation that follows the entity repository pattern we have the issue that reading the data for different use cases is different and therefore performance varies significantly.\\nWhen we started to extract reading access for mappings in different parts we had much more control in the underlying storage layer.\\n\\n## Decision\\nWe split up the different reading scenarios into separate classes.\\nWriting operations potentially need to read other data first so these need to be extracted as well.\\nAll write operations should be transactional to ensure a known state in case of an exception.\\nWith their transactional behaviour their return value can safely be a fixed collection of values.\\nOther operations that run indefinitely long as they look for and load data should return an iterable of a single entry.\\nAs every operation will have its own class and the intention is not to add future methods into the services, we will use [interfaces](.\/2020-04-30-contracts-and-interfaces.md).\\nNames of the operations are not prescribed and should represent the expected business logic.\\nIngoing payload- and criteria- as well as returned result objects should have verbose property names to prevent ambiguity errors in the future.\\nWith this in mind we found these naming patterns to be useful:\\n* list - is a non-human-used listing of a search based upon a criteria\\n* overview - is a human-used listing of a pageable search with various information\\n* create - creates a batch of entries which should return the primary keys\\n* get - read a list of entries based upon primary keys\\n* find - look for a certain entry by its unique components\\n\\n\\n## Context\\nPreviously my thoughts regarding a \"streamid\" in the `CloudEvent` has been that since it\\'s not visible to the user.\\nThe \"streamid\" extension property was added on write and removed on read to avoid surprises to the user.\\nHowever this leads to problems implementing snapshotting and sagas since then it\\'s highly likely that we want to use the streamId\\nin these cases (and the user probably wants to know the stream id in a saga).\\n\\n## Decision\\nFor this reason the implementors of the `WriteEventStream` api will _add_ a \"streamid\" to each `CloudEvent`.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThere has been confusion on why we call generating and consuming devices Assets in Origin. We have noticed that most Origin users would refer to Assets as Devices and not Assets.\\\\n'}, {'role': 'model', 'content': 'We decided to change our terminology to refer to Assets as Devices, to keep in line with the prevailing terminology in the industry.\\\\n'}]"}
{"File Name":"operational-data-hub\/0046-javascript-framework.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nA Javascript framework is an abstraction in which software providing generic functionality can be selectively changed by additional user-written code. JavaScript framework is an application framework written in JavaScript where the programmers can manipulate the functions and use them for their convenience.\\nFrameworks are more adaptable for the designing of websites, and hence, most of the website developers prefer it. JavaScript frameworks are a type of tool that makes working with JavaScript easier and smoother. These frameworks also make it possible for the programmer to code the application as a device responsive. This responsiveness is yet another reason why the JavaScript frameworks are quite popular when it comes to the question of using a high-level machine language.\\n","Decision":"We have decided to use the Angular Javascript framework\\n","tokens":150,"id":2717,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nA primary component of OpenBounty is a web application. As part of our work on this web application we regularly need to implement new UI elements or flows to support overall product development. This frontend work requires usage of CSS to specify positioning, text styles and many more variables.\\nA common problem with CSS is that developers try to generalize CSS classes so that they can be reused (see e.g. [BEM](http:\/\/getbem.com\/)). Arguably the intention is great but inevitably the time will come when constraints change and so the component's CSS is modified. By that time other people may have used that component in other places relying on the current implementation.\\nIn programming languages breaking a collaborator's expectation like this can be mitigated using assertions or automatic tests but this is less easily done when working with CSS.\\n\\n## Decision\\nIn order to avoid the problems outlined above we will adopt the approach of using atomic, immutable utility classes as promoted by the [Tachyons](http:\/\/tachyons.io\/) library.\\nTachyons provides safe-to-reuse, single-purpose classes that help with achieving consistent scales of whitespace and font-sizes.\\nBy not modifying the definition of CSS classes anymore we can safely build out UI components using those classes without needing to worry if we're breaking someone else's expectations.\\n\\n\\n## Context\\nThis decision is being made during Alpha. It also comes at a time that the Digital organisation within DfE are building developer capability and want the members of the developer community to readily switch between services. Hence a default preferred language and framework is desired, Digital are coalescing around Ruby and the associated Rails framework.\\nRails is a development tool which gives web developers a framework, providing structure for all the code they write. The Rails framework helps developers to build websites and applications, because it abstracts and simplifies common repetitive tasks.\\nRails is written in Ruby, the programming language which is also used alongside Rails. Ruby is to Rails as PHP is to Symfony and Zend, or as Python is to Django. The appeal of Ruby to developers lies in the elegance and terseness of the language.\\n\\n## Decision\\nThe decision is to align with the central Digital preference for Ruby and Rails.\\nKey reasons for choosing Ruby and Rails are:-\\n* Alignment with a large portion of the Government Digital Service sites and services, including cross service development skills within DfE. [Here](https:\/\/dfe-digital.github.io\/technical-guidance\/guides\/default-technology-stack\/#the-ruby-stack) is the DfE Technical Guidance around this.\\n* Optimisation around developer productivity and lower costs.\\n* It was created specifically for building web applications.\\n* Numerous code libraries (Gems) which provide free, open-source code for developers to fulfil specific needs.\\n* A very strong community with a great sense of collaboration and support.\\n* It is robust and high-quality. All Ruby developers are encouraged to follow a set of community-supported coding conventions and standards, which in turn helps produce better code, and thereby high quality digital products and services.\\n* It\u2019s conventions make it much easier for developers to move between Rails services, which will tend to use the same structure and coding practices.\\n* A strong focus on testing, with many good testing frameworks.\\n* It emphasises RESTful application design. REST (REpresentational State Transfer) is a style of software architecture based around a client-server relationship. It encourages a logical structure which can be exposed as an API (Application Programing Interface) which can be used to expose platform functionality (as desired for this service).\\n* Is an intuitive and easy-to-read language. It uses fewer characters than other languages allowing for intuitive, natural language, coding rather than having to use lots of special characters. The flip side is that reading code is made simpler, allowing for quick on-boarding to a service and peer reviewing for example.\\n* Enables rapid development. Due to many of the above points, such as readily available code libraries specifically designed for websites and apps, an active community which promotes high-quality development, and code that is simple to read and write then using Rails can achieve rapid development with high-quality results.\\nReasons why you might not choose Ruby and Rails are:-\\n* Not all hosting can support Rails - but since we are choosing where to host our service we can ensure this isn\u2019t an issue.\\n* Java and PHP are more widely used and there are more developers in these languages. However there is still a strong market in Ruby developers and the very extensive open-source libraries available mean a lot can be achieved by a few developers.\\n* Performance and Scalability. Rails is not as fast as Java or C, but it is fast enough for this service and will horizontally scale out when needed. There are plenty of high-profile organisations relying on Rails, examples include AirBnB, Yellow Pages, Groupon, Channel 5, GitHub, Shopify and Gov.uk.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\\\nA Javascript framework is an abstraction in which software providing generic functionality can be selectively changed by additional user-written code. JavaScript framework is an application framework written in JavaScript where the programmers can manipulate the functions and use them for their convenience.\\\\nFrameworks are more adaptable for the designing of websites, and hence, most of the website developers prefer it. JavaScript frameworks are a type of tool that makes working with JavaScript easier and smoother. These frameworks also make it possible for the programmer to code the application as a device responsive. This responsiveness is yet another reason why the JavaScript frameworks are quite popular when it comes to the question of using a high-level machine language.\\\\n\"}, {'role': 'model', 'content': 'We have decided to use the Angular Javascript framework\\\\n'}]"}
{"File Name":"linshare-mobile-android-app\/0002-image-loading-with-glide.md","Context":"## Context\\nIn the android linshare application, we implement the list file from the space of user. Proposed user interface design includes the thumbnail of file preview when user wants to show\\nIt is necessary to have an library to process first part is image thumbnail and preview.\\nOver the best practice of image processing there are 2 libraries is very commons by android developer community: Glide and Picasso\\nTo compare between them, the commonly usage is the same, but Glide have much more strengthen rather than Picasso.\\nIt process the image source and have method to generate the thumbnail natively, it consume less the memory than Picasso and the library is have smaller packer with much more APIs to help process image, witch could be useful later when we implement more functionality in the application\\n","Decision":"We decided to use Glide instead of Picasso.\\n","tokens":155,"id":1642,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\n* This is Open Source app which may be used by big variety of projects\\n* TypeScript is widely known nowadays, so there is high chance to get contribution from other peple if this technology is used\\n* Project\\'s author and main contibutor (@kopach) is fluent in this technology, so no need in spending extra efort in learning something differrent\\n* TypeScript is type safe, so more secure and potentially should prevent from common mistakes\\n* TypeScript integrates well with JavaScript so all libraries from both ecosystems can be used easilly\\n\\n## Decision\\nUse TypeScript as main and only programming language in this project\\n\\n\\n## Context and Problem Statement\\nWe want to base our user interface on map software and calculations in routing engine.\\nWhich map provider should we use?\\n## Decision Drivers\\n* Since this is research project of using free software, we require that the map usage MUST be free.\\n* Required is that the map has supporting library to generate and draw actual routes between two points.\\n* Required is that the map has supporting library or methods to calculate actual routes and time for driving, cycling and walking between two points.\\n\\n## Decision\\n* Since this is research project of using free software, we require that the map usage MUST be free.\\n* Required is that the map has supporting library to generate and draw actual routes between two points.\\n* Required is that the map has supporting library or methods to calculate actual routes and time for driving, cycling and walking between two points.\\nChosen option: \"[`Openstreetmap`](https:\/\/openstreetmap.org) with service provided by [`OSRM routing engine`](http:\/\/project-osrm.org\/)\", because\\n* Only option that is free for implementatation and does not require registration of any kind.\\n* The product is research only, and not intended to installation to production.\\n* `ja.is` did not provide any documentation about using routing engine and tiles, so it cannot be considered.\\nWe accept that usage of `Google maps` would be lot easier to implement. `Google maps` does host and maintain all required api\\'s and geographical data.\\n`Google directions api` seems to have better geographical data for Iceland than we see in accessable `OSRM routing profiles`\\nIf product was intended for production we must add in our decision conideration about the price of using `Google maps` compared with the cost of hosting and maintaining `OSRM routing engine`.\\n### Positive Consequences\\n* We do not need to pay for usage of map.\\n### Negative Consequences\\n* We need to install, host and maintain `OSRM routing engine`.\\n* We need to update and maintain routing info on local `OSRM routing engine` to avoid outdated map data and route calculations.\\n<hr>\\nContent of decision: Copyright \u00a9 2020 \u00deorleifur Bjarnason, All Rights Reserved.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nIn the android linshare application, we implement the list file from the space of user. Proposed user interface design includes the thumbnail of file preview when user wants to show\\\\nIt is necessary to have an library to process first part is image thumbnail and preview.\\\\nOver the best practice of image processing there are 2 libraries is very commons by android developer community: Glide and Picasso\\\\nTo compare between them, the commonly usage is the same, but Glide have much more strengthen rather than Picasso.\\\\nIt process the image source and have method to generate the thumbnail natively, it consume less the memory than Picasso and the library is have smaller packer with much more APIs to help process image, witch could be useful later when we implement more functionality in the application\\\\n'}, {'role': 'model', 'content': 'We decided to use Glide instead of Picasso.\\\\n'}]"}
{"File Name":"poc-terraform-aws-web-cluster\/0002-infrastructure-layout.md","Context":"## Context\\nWhen creating a piece of infrastructure it is implied that this code would be able to be run in several environments. To illustrate this, there are three environments created, dev, uat, and prod. There will be one and only one\\nterraform module and each environment will feed it with different parameters to create distinct instances.\\n","Decision":"The terraform code is made multi environment and multi account through there are better tools to do that with, like terragrunt, which is outside the scope of the PoS.\\nThe credentials are assumed to be in the credentials file. the author is aware that the are more secure ways of doing this , but as the PoS needs to be easily reproducible the author has chosen to make it easy to use by dropping in the credential files in the home directory.\\nThe credentials are that of an administrator, there has been no attempt made to create assumed roles as would be done in production systems, as the goal in this case is speed.\\n","tokens":70,"id":1023,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe currently build an individual docker image for each deployment (dev, stage, and prod) that contains the\\nproper data for that environment. It would save time and testing if we only built a single image that could\\nbe promoted to each environment and loaded with the proper data at startup.\\n\\n## Decision\\nWe will use a Kubernetes DaemonSet to ensure that a data updater pod is running on each node in a cluster. This\\npod will keep the database and l10n files updated in a volume that will be used by the other bedrock pods to\\naccess the data.\\n[GitHub issue](https:\/\/github.com\/mozmeao\/infra\/issues\/1306)\\n\\n\\n## Context\\nThe existing site (Frontend) includes Javascript in a variety of ways:\\n-   inline\\n-   script tags\\n-   dynamically added script tags\\nThere is a single (universal) boot process which conditionally loads additional\\nJS as required based on page metadata.\\nFor Dotcom Rendering we want to simplify things so that it is easy for a\\ndeveloper to know where to add a script\/functionality. We also want an approach\\nthat helps ensure good performance and, related, which is less monolithic.\\n\\n## Decision\\n-   Load scripts in the head as deferred, or as async at the bottom of the HTML\\nbody section\\n-   All other script loading (other than for commercial third-party code) is\\nprohibited\\n-   We will not use a universal 'boot' JS file, but load scripts which have a\\nunique purpose and which are self-contained aside from a few shared helpers\\nFuller discussion of this approach can be found here:\\nhttps:\/\/docs.google.com\/document\/d\/1EnkowJ7BFumrqkpkRacRl-gSkiJs9fUgRCixQTLxBvo\/edit#.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWhen creating a piece of infrastructure it is implied that this code would be able to be run in several environments. To illustrate this, there are three environments created, dev, uat, and prod. There will be one and only one\\\\nterraform module and each environment will feed it with different parameters to create distinct instances.\\\\n\"}, {'role': 'model', 'content': 'The terraform code is made multi environment and multi account through there are better tools to do that with, like terragrunt, which is outside the scope of the PoS.\\\\nThe credentials are assumed to be in the credentials file. the author is aware that the are more secure ways of doing this , but as the PoS needs to be easily reproducible the author has chosen to make it easy to use by dropping in the credential files in the home directory.\\\\nThe credentials are that of an administrator, there has been no attempt made to create assumed roles as would be done in production systems, as the goal in this case is speed.\\\\n'}]"}
{"File Name":"Horace\/0020-use-c-mex-api.md","Context":"## Context\\nMatlab has two Mex APIs that can be used by C++ code:\\nthe [C API](https:\/\/www.mathworks.com\/help\/matlab\/cc-mx-matrix-library.html)\\nor the\\n[C++ API](https:\/\/www.mathworks.com\/help\/matlab\/cpp-mex-file-applications.html).\\nThe C++ API is relatively new and is therefore only compatible with Matlab releases\\n[R2018a and later](https:\/\/www.mathworks.com\/help\/matlab\/matlab_external\/choosing-mex-applications.html#mw_d3e64706-faf9-486f-ab58-1860c63564d8).\\nThroughout the Herbert and Horace codebases only the C API has been used.\\nThis is primarily due to the fact that Herbert and Horace pre-date the C++ API.\\nThe C++ API contains restrictions on directly accessing Matlab memory,\\nthis provided challenges when writing the Matlab object serializer in C++.\\nThe API does not allow access to the underlying memory of the Matlab objects,\\nand the C++ objects had varying sized headers.\\nThis caused problems when attempting to perform copies and casts:\\nthe underlying data needed to first be copied via the C++ API,\\nand only then could a `memcpy` be performed by the serializer.\\nThis meant there were three copies of the data present at one time,\\nwhich is not desirable for large objects (e.g. pixel data).\\n","Decision":"Herbert\/Horace will continue to use the C API for new Mex functions.\\n","tokens":294,"id":4243,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nAs part of the process of rewriting the admin components for link platform, we need to decide on what strategy we want\\nto use for including styling in our application.\\nThere were two main choices:\\n* CSS stylesheets\\n* CSS-in-JS\\n### Why CSS-in-JS?\\nThere are many benefits to CSS-in-JS which are nicely outlined in\\n[this article](\\'https:\/\/medium.com\/seek-blog\/a-unified-styling-language-d0c208de2660\\'):\\n* Enforces fundamental scoping without relying on pure convention (automatically generated classes).\\n* Renders critical CSS baked in, as components bring their styles with them automatically.\\n* Removes the necessity to hard code class names.\\n* Increases reusability of styles.\\n* Improves maintainability and dead code detection\\n### Why Fela?\\n[Fela](https:\/\/github.com\/rofrischmann\/fela) is a high performance, framework agnostic\\ntool to handle state-driven styling in Javascript. It also uses Atomic Class design to\\ngenerate class names, and creates stylesheets with those class names, which allows for\\nmodular style reuse. Fela performs about\\n[four times faster](https:\/\/github.com\/A-gambit\/CSS-IN-JS-Benchmarks\/blob\/master\/RESULT.md)\\nthan Glamorous. Fela has also been used by Zendesk engineers in the past and so comes\\nwith the added bonus of having some internal knowledge already in place to hit the\\nground running.\\n\\n## Decision\\nWe will use CSS-in-JS, in the form of Fela and React-Fela.\\n\\n\\n## Context\\nData Bridge is a browser-side application that allows users to edit Wikidata from Wikibase Client wikis.\\nThis involves showing users references for the statements they are editing, in a readable format.\\nThe guiding example for this format is the following reference from the [South Pole Telescope] article:\\n> [\"South Pole Telescope eyes birth of first massive galaxies\"](https:\/\/antarcticsun.usap.gov\/science\/contenthandler.cfm?id=2737). United States Antarctic Program. 14 September 2012. Retrieved 11 February 2017.\\nWhich is generated by Lua code from the following Wikidata reference:\\n> - reference URL: https:\/\/antarcticsun.usap.gov\/science\/contenthandler.cfm?id=2737 (`url`)\\n> - retrieved: 11 February 2017 (`time`)\\n> - title: South Pole Telescope eyes birth of first massive galaxies (English) (`monolingualtext`)\\n> - publication date: 14 September 2012 (`time`)\\n> - publisher: [United States Antarctic Program][Q955267] (`wikibase-item`)\\nThis reference already involves the formatting of four different datatypes, none of which are otherwise supported by Data Bridge in its initial iteration.\\nAdditionally, other extensions can register additional datatypes (examples include WikibaseLexeme, Math, and Score), which may appear in references as well.\\nThis makes formatting references browser-side unattractive:\\nwe would have to implement formatting a great number of datatypes, and provide a mechanism for other extensions to hook formatters for their own datatypes into Data Bridge.\\nBy contrast, the server-side code already knows how to format every datatype for presentation,\\nboth for repository and client functionality.\\nWikibase has an API for formatting individual values (`wbformatvalue`),\\nbut this is limited to a single value at a time,\\nwhich means formatting several rich references may require a lot of API requests.\\nAdditionally, some values should be combined in the reference:\\nmost importantly, the reference URL and title should be combined into a link with that target URL and text.\\nIn the future, Data Bridge will also support editing of references ([T245640]),\\nso there is also a need to support displaying references that have not yet been saved, and are not present on the target item.\\n\\n## Decision\\nTo format these references, we will add an action API module to Wikibase Client.\\nThis module will accept a reference JSON blob and return HTML.\\nKnowledge such as combining certain values, or the order of properties, is encoded into this module:\\nData Bridge should be able to use its output unmodified.\\nWe envision that this functionality will also be useful to others:\\nboth as an API module, for other API users,\\nand as a Lua function, for wiki content\\n(especially on wikis that don\u2019t have their own sophisticated reference formatting modules yet).\\nHowever, the initial version will be marked internal, and only serve the needs of Data Bridge.\\nThe action API is chosen because Wikibase does not yet use any other style of API,\\nso it is the most straightforward option.\\nWikibase Client (rather than Wikibase Repository) is chosen\\nbecause the desired snak formatting aligns more closely with that on the client.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nMatlab has two Mex APIs that can be used by C++ code:\\\\nthe [C API](https:\/\/www.mathworks.com\/help\/matlab\/cc-mx-matrix-library.html)\\\\nor the\\\\n[C++ API](https:\/\/www.mathworks.com\/help\/matlab\/cpp-mex-file-applications.html).\\\\nThe C++ API is relatively new and is therefore only compatible with Matlab releases\\\\n[R2018a and later](https:\/\/www.mathworks.com\/help\/matlab\/matlab_external\/choosing-mex-applications.html#mw_d3e64706-faf9-486f-ab58-1860c63564d8).\\\\nThroughout the Herbert and Horace codebases only the C API has been used.\\\\nThis is primarily due to the fact that Herbert and Horace pre-date the C++ API.\\\\nThe C++ API contains restrictions on directly accessing Matlab memory,\\\\nthis provided challenges when writing the Matlab object serializer in C++.\\\\nThe API does not allow access to the underlying memory of the Matlab objects,\\\\nand the C++ objects had varying sized headers.\\\\nThis caused problems when attempting to perform copies and casts:\\\\nthe underlying data needed to first be copied via the C++ API,\\\\nand only then could a `memcpy` be performed by the serializer.\\\\nThis meant there were three copies of the data present at one time,\\\\nwhich is not desirable for large objects (e.g. pixel data).\\\\n'}, {'role': 'model', 'content': 'Herbert\/Horace will continue to use the C API for new Mex functions.\\\\n'}]"}
{"File Name":"tech-team\/0002-use-containers-for-foreign-languages.md","Context":"## Context\\neLife has a small set of supported languages: PHP, Python, JavaScript; in-house developers are present for them. All other languages are defined as **foreign**.\\nThere are tools that are peculiar to our infrastructure, such as [goaws](https:\/\/github.com\/p4tin\/goaws) for AWS simulation, but are written in languages no one is an expert in (Go).\\nThere are also tools that were originally written in another language but are being adopted by us, like [INK](https:\/\/gitlab.coko.foundation\/INK\/ink-api) for document conversion, written in Ruby.\\nThese tools are usually distributed as source code. The operational overhead of writing formulas for the environment to build them is a form of waste.\\nSome tools written in Java instead have a very stable runtime platform (ElasticSearch, even Jenkins), as they are distributed as binaries.\\n","Decision":"We will use existing Docker containers to deploy tools that require building from source in a foreign language, in testing or production environments.\\n","tokens":183,"id":937,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nHaving a repeatable, predictable way to create and change infrastructure is essential to stability and reliability of applications. One leading candidate to allow writing infrastructure as code is [Terraform](https:\/\/www.terraform.io).\\nOther tools to consider might be Ansible, Puppet or Chef, but they are less suited to modern cloud infrastructure than Terraform as they were developed when running code on VMs was the norm. They are good options for Configuration Management, but less appropriate for managing the infrastructure itself.\\nAmazon CloudFormation is a closer a possibility, but it is only usable on the Amazon stack whereas Terraform can be used to manage any Cloud. This flexibility will allow us to manage infrastructure which span clouds which this project may require (such as backend processing on AWS and frontend APIs on Heroku).\\nBoth CloudFormation and Terraform are good choices for Infrastructure Orchestration, but the AWS only restriction of CloudFormation makes it much less compelling to adopt.\\nSee [3. Follow Twelve Factor methodology](0003-follow-twelve-factor-methodology.md)\\n\\n## Decision\\nWe will use Terraform to configure our Infrastructure.\\n\\n\\n## Context\\nStrongly-typed code can provide a number of advantages in a medium-large codebase:\\n- Catch errors much earlier (at build-time). Runtime errors require manual testing or tools such as Selenium to track down\u2014this is more expensive and prone to failure.\\n- Codebase is easier to understand as the code is much more explicit\u2014potentially easier on-boarding of new developers.\\n- Available tooling such as [IntellisenseTM](https:\/\/code.visualstudio.com\/docs\/editor\/intellisense) enhances developer experience.\\nHowever, there are a number of risks:\\n- Increased complexity of code\u2014many developers are unfamiliar with stongly-typed languages.\\n- Increased complexity of tooling needed to compile the code. This is somewhat negated by the fact the existing build setup already transpiles code via Babel.\\n### Flow vs. TypeScript\\nThere are two significant options in the strongly-typed Javascript market\u2014[Flow](https:\/\/flow.org\/) (Facebook) and [TypeScript](https:\/\/www.typescriptlang.org\/) (Microsoft). Whilst Flow is a type-checker, TypeScript is a superset of the JavaScript language itself and both require additional tooling (in our case, via Webpack) to work.\\nFor this exercise, we evaluated both options by setting up the relevant tools and refactoring some existing code to include types.\\n#### Flow\\nAlthough fairly easy to setup initially, integration with third-party libraries was problematic and poorly documented in some cases. [https:\/\/github.com\/flowtype\/flow-typed](flow-typed) was used (as recommended) and things worked briefly, errors due to incompatibilities between versions of software quickly became a problem.\\n#### TypeScript\\nTypescript was again, fairly easy to setup but, again, integrations with third-party libraries was the main cause of problems. The solution to this, installing type definitions via NPM seemed a better option than flow-typed. We were able to refactor some existing code successfully.\\n\\n## Decision\\nTypeScript was slightly easier to set up and we were successful in refactoring existing code and so was the obvious choice. Going forward, it is recommended that we apply first to the [https:\/\/github.com\/ONSdigital\/eq-author-api](eQ Author API) codebase as it is smaller and simpler with less dependencies. This will be a valuable exercise, allowing us to understand the system better and make a decision regarding implementing TypeScript in the Author Application itself.\\nThe [https:\/\/github.com\/ONSdigital\/eq-author\/tree\/spike-typescript](branch used for this spike) is still available on GitHub for reference.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\neLife has a small set of supported languages: PHP, Python, JavaScript; in-house developers are present for them. All other languages are defined as **foreign**.\\\\nThere are tools that are peculiar to our infrastructure, such as [goaws](https:\/\/github.com\/p4tin\/goaws) for AWS simulation, but are written in languages no one is an expert in (Go).\\\\nThere are also tools that were originally written in another language but are being adopted by us, like [INK](https:\/\/gitlab.coko.foundation\/INK\/ink-api) for document conversion, written in Ruby.\\\\nThese tools are usually distributed as source code. The operational overhead of writing formulas for the environment to build them is a form of waste.\\\\nSome tools written in Java instead have a very stable runtime platform (ElasticSearch, even Jenkins), as they are distributed as binaries.\\\\n'}, {'role': 'model', 'content': 'We will use existing Docker containers to deploy tools that require building from source in a foreign language, in testing or production environments.\\\\n'}]"}
{"File Name":"fxa\/0014-auth-for-settings-redesign.md","Context":"## Context and Problem Statement\\nThe Settings Redesign app needs to query & mutate the same protected user data as the existing settings app hosted on content-server. This will require some form of authentication & authorization to manage that data.\\n## Decision Drivers\\n- Smooth UX\\n- Security\\n- Development velocity\\n- Ease of integration\\n","Decision":"- Smooth UX\\n- Security\\n- Development velocity\\n- Ease of integration\\nChosen option: \"Authenticate via OAuth2 to use auth-server APIs\", because:\\n- It requires minimal changes to existing auth-server implementation and infrastructure.\\n- It relies on an authentication mechanism with relatively well-known security properties.\\n### Positive Consequences\\n- The OAuth2 access token mechanism is better suited to this purpose than a novel scheme to share session token credentials.\\n- The new Settings Redesign app can use existing auth-server APIs with minimal modifications to accept scoped OAuth2 access tokens. This can constrain most of the novelty in the project to the redesigned & reimplemented settings UX.\\n### Negative Consequences\\n- We will need to modify auth strategies in auth-server APIs, taking care not to affect existing usage in the production settings app.\\n- We don't have an entirely greenfield project, which could be a bit of a drag. But, rediscovering lessons learned in existing code can also be a drag.\\n","tokens":66,"id":382,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nGiven our application allows making changes to the federation configuration,\\nit is neccessary to secure the service appropriately. The same instance\\nwill be accessed by different users with varying level of permissions.\\nTherefore we need to have a strong authentication and user management in place.\\nOur key authentication requirements are:\\n- Secure\\n- Flexible and extendable\\n- Cost effective\\n- Multi-factor authentication (MFA) support\\n- GDPR compliant\\n\\n## Decision\\nBased on our requirements, we decided to use Amazon Cognito - a service provided by AWS.\\nAmazon Cognito satisfies all the requirements and offers an easy way to implement\\nauthentication in our application.\\nInitially we integrated with Cognito using OAuth. However, we soon realized the\\ncustomisation options are very limited. In the interest of more flexibility and\\na more consistent user journey we have decided to integrate with Cognito using AWS SDK and APIs.\\nIn terms of cost, Cognito is free under 50,000 monthly active user. We never envision hitting the threshold.\\nFurthermore, to ease our implementation using the SDK we have imported Devise to provide\\nthe app with an authentication framework.\\n\\n\\n## Context and Problem Statement\\nGLAM requires authentication to protect sensitive metrics, make some metrics and\\nviews visible to non-Mozillians, and in the future to potentially add\\nuser-oriented features. Currently we handle the authentication in the app layer,\\nalbeit with some issues. This ADR attempts to resolve these issues.\\n## Decision Drivers\\n- Authentication should work without issue, including infinite redirects or\\nconflicts with enhanced tracking protection and 3rd party cookies.\\n- Authentication should allow some areas of the site to be public via URL\\nmatching rules.\\n- Authentication should pass information about the authenticated user to the\\nbackend for future use-cases.\\n\\n## Decision\\n- Authentication should work without issue, including infinite redirects or\\nconflicts with enhanced tracking protection and 3rd party cookies.\\n- Authentication should allow some areas of the site to be public via URL\\nmatching rules.\\n- Authentication should pass information about the authenticated user to the\\nbackend for future use-cases.\\nIn the GLAM meeting, nginx was chosen as the preferred option for the following\\nreasons:\\n- It removes code complexity and maintenance from the app layer.\\n- It would remove a necessary security review by using something that has\\nalready been reviewed.\\n- It offers the quickest path to fix the authentication issues we are facing.\\n- In the future it satifies the requirement of offering user-facing features.\\n- In the future it satifies the requirement of optional non-authenticated\\nendpoints.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\nThe Settings Redesign app needs to query & mutate the same protected user data as the existing settings app hosted on content-server. This will require some form of authentication & authorization to manage that data.\\\\n## Decision Drivers\\\\n- Smooth UX\\\\n- Security\\\\n- Development velocity\\\\n- Ease of integration\\\\n'}, {'role': 'model', 'content': '- Smooth UX\\\\n- Security\\\\n- Development velocity\\\\n- Ease of integration\\\\nChosen option: \"Authenticate via OAuth2 to use auth-server APIs\", because:\\\\n- It requires minimal changes to existing auth-server implementation and infrastructure.\\\\n- It relies on an authentication mechanism with relatively well-known security properties.\\\\n### Positive Consequences\\\\n- The OAuth2 access token mechanism is better suited to this purpose than a novel scheme to share session token credentials.\\\\n- The new Settings Redesign app can use existing auth-server APIs with minimal modifications to accept scoped OAuth2 access tokens. This can constrain most of the novelty in the project to the redesigned & reimplemented settings UX.\\\\n### Negative Consequences\\\\n- We will need to modify auth strategies in auth-server APIs, taking care not to affect existing usage in the production settings app.\\\\n- We don\\'t have an entirely greenfield project, which could be a bit of a drag. But, rediscovering lessons learned in existing code can also be a drag.\\\\n'}]"}
{"File Name":"gsp\/ADR016-code-verification.md","Context":"## Context\\nAll of our deployment processes start with code in git repositories hosted on\\nGithub and continuous delivery pipelines automate the build and deployment of\\napplications out to production. Git supports GPG signing of commits.\\nCode changes should not be able to make it out to a production environment\\nwithout being exposed to at least two authorised code owners.\\nOur aim is to improve on process-based approval methods by being able to\\ndigitally verify that code has been exposed to at least two authorised code\\nowners without adversely affecting developer workflow.\\nSome potential solutions involve:\\n1. Enforcing that two unique developers each an add empty signed commit at the\\ntip of the branch for PRs before being merged, and verifying the existence\\nof these empty signed commits as part of the delivery pipeline.\\n2. Enforcing that two unique developers each sign the commmit at the tip of the\\nbranch for PRs using\\n[git-signatures](https:\/\/github.com\/hashbang\/git-signatures) and verifying\\nthat at least two trusted developer keys are present as part of the delivery\\npipeline.\\n3. Verify that at least one (non-PR-authoring) trusted Github user has approved\\nthe commit from the delivery pipeline using the Github API (rather than the\\nweb UI, which is vulnerable to manipulation by a single Github \"owner\").\\n","Decision":"Solutions (1) and (2) both introduce significant changes to existing developer\\nworkflows. The benefit of either of these solutions is that when combined with\\nan external hardware security device (Yubikey) they can provide greater\\nprotection against developer machine compromise than Github's session duration\\nalone.  However we don't feel this benefit warrants the reduced usability for\\ndevelopers across GDS.\\nWe implement solution (3).\\n","tokens":275,"id":3900,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nUniffi currently uses a very coarse locking strategy for managing concurrent access to object instances,\\nwhich has caused us to accidentally ship code in a product that [blocked the main thread on network I\/O](https:\/\/jira.mozilla.com\/browse\/SDK-157).\\nWe need to enable finer-grained concurrency control in order to provide the desired API for a key consumer.\\nCurrently, every interface has a corresponding [ffi_support::ConcurrentHandleMap](https:\/\/docs.rs\/ffi-support\/0.4.0\/ffi_support\/handle_map\/struct.ConcurrentHandleMap.html) that is responsible for owning all instances of\\nthat interface and for handing out references to them in a mutability-safe and threadsafe manner. This\\nensures that the generated code is safe in the face of concurrent operations, but has a substantial\\nruntime cost: only one method call can be executed on an instance at any time. Any attempt to call an\\nobject method while a concurrent method is already executing, will block until the previous call has completed.\\nThe desired API for Project Nimbus includes methods that will be called synchronously from the main thread,\\nand hence must not block on network or disk I\/O. Such an API cannot be built with uniffi as currently\\nimplemented.\\n## Decision Drivers <!-- optional -->\\n* Enabling consumers to control the potential blocking behaviour of their generated APIs.\\n* Ensure safety of the generated code.\\n* Ship a solution in a timely manner to unblock Project Nimbus.\\n\\n## Decision\\n* Enabling consumers to control the potential blocking behaviour of their generated APIs.\\n* Ensure safety of the generated code.\\n* Ship a solution in a timely manner to unblock Project Nimbus.\\nChosen option:\\n* **Option B: Let consumers mark interface definitions as threadsafe to opt in to a less-locking handlemap.**\\nThe choice here comes down to safety and simplicity. By making a more-concurrency-friendly handlemap\\nwe can maintain the current strict enforcement of Rust\\'s mutability-safety and thread-safety guarantees,\\neven in the face of errors in the generated bindings. It seems to be a relatively small change, and\\nby making it opt-in we avoid creating busywork for other consumers who are not urgently facing this\\nproblem.\\nOne downside is that consumers need to opt-in to the fix, meaning that the default behavior may still\\nbe surprising to new consumers. We\\'ll mitigate this with docs and will consider revisiting the default\\nbehaviour if the majority of consumers adopt the new approach.\\nThis choice does also punt some potential performance improvements to future work, but that seems in keeping\\nwith where we are in the project\\'s lifecycle.\\n\\n\\n## Context\\nIn an effort to support [Go-API-Stability](.\/adr-060-go-api-stability.md),\\nduring the 0.35 development cycle, we have attempted to reduce the the API\\nsurface area by moving most of the interface of the `node` package into\\nunexported functions, as well as moving the reactors to an `internal`\\npackage. Having this coincide with the 0.35 release made a lot of sense\\nbecause these interfaces were _already_ changing as a result of the `p2p`\\n[refactor](.\/adr-061-p2p-refactor-scope.md), so it made sense to think a bit\\nmore about how tendermint exposes this API.\\nWhile the interfaces of the P2P layer and most of the node package are already\\ninternalized, this precludes some operational patterns that are important to\\nusers who use tendermint as a library. Specifically, introspecting the\\ntendermint node service and replacing components is not supported in the latest\\nversion of the code, and some of these use cases would require maintaining a\\nvendor copy of the code. Adding these features requires rather extensive\\n(internal\/implementation) changes to the `node` and `rpc` packages, and this\\nADR describes a model for changing the way that tendermint nodes initialize, in\\nservice of providing this kind of functionality.\\nWe consider node initialization, because the current implemention\\nprovides strong connections between all components, as well as between\\nthe components of the node and the RPC layer, and being able to think\\nabout the interactions of these components will help enable these\\nfeatures and help define the requirements of the node package.\\n\\n## Decision\\n- To the greatest extent possible, factor the code base so that\\npackages are responsible for their own initialization, and minimize\\nthe amount of code in the `node` package itself.\\n- As a design goal, reduce direct coupling and dependencies between\\ncomponents in the implementation of `node`.\\n- Begin iterating on a more-flexible internal framework for\\ninitializing tendermint nodes to make the initatilization process\\nless hard-coded by the implementation of the node objects.\\n- Reactors should not need to expose their interfaces *within* the\\nimplementation of the node type\\n- This refactoring should be entirely opaque to users.\\n- These node initialization changes should not require a\\nreevaluation of the `service.Service` or a generic initialization\\norchestration framework.\\n- Do not proactively provide a system for injecting\\ncomponents\/services within a tendtermint node, though make it\\npossible to retrofit this kind of plugability in the future if\\nneeded.\\n- Prioritize implementation of p2p-based statesync reactor to obviate\\nneed for users to inject a custom state-sync provider.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nAll of our deployment processes start with code in git repositories hosted on\\\\nGithub and continuous delivery pipelines automate the build and deployment of\\\\napplications out to production. Git supports GPG signing of commits.\\\\nCode changes should not be able to make it out to a production environment\\\\nwithout being exposed to at least two authorised code owners.\\\\nOur aim is to improve on process-based approval methods by being able to\\\\ndigitally verify that code has been exposed to at least two authorised code\\\\nowners without adversely affecting developer workflow.\\\\nSome potential solutions involve:\\\\n1. Enforcing that two unique developers each an add empty signed commit at the\\\\ntip of the branch for PRs before being merged, and verifying the existence\\\\nof these empty signed commits as part of the delivery pipeline.\\\\n2. Enforcing that two unique developers each sign the commmit at the tip of the\\\\nbranch for PRs using\\\\n[git-signatures](https:\/\/github.com\/hashbang\/git-signatures) and verifying\\\\nthat at least two trusted developer keys are present as part of the delivery\\\\npipeline.\\\\n3. Verify that at least one (non-PR-authoring) trusted Github user has approved\\\\nthe commit from the delivery pipeline using the Github API (rather than the\\\\nweb UI, which is vulnerable to manipulation by a single Github \"owner\").\\\\n'}, {'role': 'model', 'content': \"Solutions (1) and (2) both introduce significant changes to existing developer\\\\nworkflows. The benefit of either of these solutions is that when combined with\\\\nan external hardware security device (Yubikey) they can provide greater\\\\nprotection against developer machine compromise than Github's session duration\\\\nalone.  However we don't feel this benefit warrants the reduced usability for\\\\ndevelopers across GDS.\\\\nWe implement solution (3).\\\\n\"}]"}
{"File Name":"raster-foundry\/adr-0010-domain-layout.md","Context":"## Context\\nRaster Foundry's web presence needs a landing page for customer acquisition and a blog for curating content relevant to the project's objectives. Given that we are still early in the product development process, there is a desire to keep solutions to the landing page and blog simple, but also flexible so that they can easily be iterated on by all parties involved.\\n","Decision":"Building upon the experience gained by maintaining products like Cicero, HunchLab, and OpenTreeMap, the following decisions have been made regarding Raster Foundry\u2019s domain layout:\\n- Canonical domain is `www.rasterfoundry.com`\\n- Web application resides on `app.rasterfoundry.com`\\n- Blog resides on `blog.rasterfoundry.com`\\nAs far as the platforms that drive the landing page and blog, the former will be a static website built using [Jekyll](https:\/\/jekyllrb.com\/) and hosted via [GitHub Pages](https:\/\/pages.github.com\/) (with support from Amazon CloudFront for HTTPS) and the latter will be based on [Medium](https:\/\/medium.com\/).\\n","tokens":75,"id":1692,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe are an open source project and as a result can possibly accept outside contribution to our codebase. To this end, we need to communicate our requirements for the code that is submitted and our process for acceptance of this code. The end goal for the rewrite is to be more modular which we believe will enable both the team and outside contributors flexibility and ease of adding enhanced features.\\n\\n## Decision\\nWe will add a contributing guide to our documentation files. Once available, Github will show links to this guide automatically to any outside user visiting the front-end-monorepo repository. Any outside contributions must follow this guide to submit code for review.\\n\\n\\n## Context\\nOne of the components of the ASAP Hub is a Web Application. We expect many dynamic elements in that web application, so we want to use a web framework to give structure to the application code and easily cope with the arising complexities. YLD as a consultancy company is most familiar with React, which has overall been an immensely popular choice in recent years. Using React as opposed to other options will reduce the learning curve significantly.\\nWe need to decide in what way we want to use React to render the user interface, and whether we want to use further tools around it that fundamentally influence the technical project setup.\\n\\n## Decision\\n**We will set up the frontend as a single-page application (1).**\\nThere is not enough static content to give value to a static site (2).\\nServer-rendering (3) does not provide enough value for our use cases to warrant the complexity it introduces.\\n**We will use create-react-app (1.1) to set up the frontend.**\\nNext.js (1.2) has its strength around server rendering.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nRaster Foundry's web presence needs a landing page for customer acquisition and a blog for curating content relevant to the project's objectives. Given that we are still early in the product development process, there is a desire to keep solutions to the landing page and blog simple, but also flexible so that they can easily be iterated on by all parties involved.\\\\n\"}, {'role': 'model', 'content': 'Building upon the experience gained by maintaining products like Cicero, HunchLab, and OpenTreeMap, the following decisions have been made regarding Raster Foundry\u2019s domain layout:\\\\n- Canonical domain is `www.rasterfoundry.com`\\\\n- Web application resides on `app.rasterfoundry.com`\\\\n- Blog resides on `blog.rasterfoundry.com`\\\\nAs far as the platforms that drive the landing page and blog, the former will be a static website built using [Jekyll](https:\/\/jekyllrb.com\/) and hosted via [GitHub Pages](https:\/\/pages.github.com\/) (with support from Amazon CloudFront for HTTPS) and the latter will be based on [Medium](https:\/\/medium.com\/).\\\\n'}]"}
{"File Name":"opg-refunds\/0002-default-service-key-for-dynamo-db-sessions.md","Context":"## Context\\nEncryption keys for frontend user sessions are provided in environment variables and cycled during every release.\\nThis has resulted in one incident of losing syncronisation, causing errors for users when services scale up and then scale down.\\n","Decision":"We will use a AWS owned Customer Master Key for the Sessions Dynamo DB tables to encrypt session tokens, and not push encryption keys into containers.\\nTable names                                       |\\n--------------------------------------------------|\\nrefunds-sessions-front-<opg_stackname>            |\\nrefunds-sessions-caseworker-front-<opg_stackname> |\\n","tokens":48,"id":295,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nMultiple projects need to publish or subscribe to messages from the `bus`.\\nProjects are written in a fixed set of supported programming languages.\\nThere is a set of non-functional requirements to cater for in integrations, especially when listening to queues (retries, graceful shutdown, timeouts, etc).\\n\\n## Decision\\nAccess the bus exclusively through the eLife SDKs, either for publishing or subscribing to messages:\\n- [bus-sdk-php](https:\/\/github.com\/elifesciences\/bus-sdk-php)\\n- [bus-sdk-python](https:\/\/github.com\/elifesciences\/bus-sdk-python)\\n\\n\\n## **Context**\\nIn order to be able to release code safely as part of a CI\/CD pipeline, it is necessary to have a feature management capability to externalise the ability to enable features.\\nThis can be achieved by including feature toggles in code, and by versioning APIs to account for breaking changes.\\nThe options available are:\\n- Environment Variables\\n- AWS AppConfig\\n\\n## Decision\\n**AWS AppConfig**\\nWhat is the change that we're proposing and\/or doing?\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nEncryption keys for frontend user sessions are provided in environment variables and cycled during every release.\\\\nThis has resulted in one incident of losing syncronisation, causing errors for users when services scale up and then scale down.\\\\n\"}, {'role': 'model', 'content': 'We will use a AWS owned Customer Master Key for the Sessions Dynamo DB tables to encrypt session tokens, and not push encryption keys into containers.\\\\nTable names                                       |\\\\n--------------------------------------------------|\\\\nrefunds-sessions-front-<opg_stackname>            |\\\\nrefunds-sessions-caseworker-front-<opg_stackname> |\\\\n'}]"}
{"File Name":"interlok\/0010-apply-config-reconnect-loop.md","Context":"## Context and Problem Statement\\nAs recorded in [INTERLOK-2875](https:\/\/adaptris.atlassian.net\/browse\/INTERLOK-2975) when Interlok is attempting to reconnect to an endpoint it becomes very difficult to amend configuration through the UI.\\nThe reconnect loop is dictated by the reconnection settings with each configured connection.  When a connection is initialized if it cannot connect to it's endpoint the reconnect loop begins.  Connections are initialized on startup and during runtime should it lose it's connection, Interlok will shift the connection through the lifecycle phases until it is initialised again.\\nTherefore we have two triggers for this issue;\\n- A connection cannot be established on startup\\n- A connection breaks during runtime.\\nUntil a connection has either established it's connection or has reached the maximum number of configured reconnect attempts, the connection will remain in the initialization phase.\\nEach phase of an Interlok component is dictated by the StateManagedComponent interface that defines the 4 basic stages of __init__, __start__, __stop__ and __close__.  When we __request__ a change of a components state (from __init__ to __start__ for example) we typically synchronize the state change until it is complete.\\nThis means for example when Interlok launches the initialization phase kicks in which will attempt to initialize each connection.  Any connection that cannot be established as mentioned above will stay in the synchronized __init__ phase potentially forever.  Because the phase transition is synchronized we cannot therefore change this state until it's current transition has completed.\\nThere are currently two problems applying configuration from the UI while a connection is attempting reconnection.\\nWhen applying config the UI will request a restart of the running instance, which will attempt to run through the lifecycle of phases mentioned above.\\nBut of course if a connection is stuck in it's synchronized initialization phase trying to reconnect all other state change requests must wait therefore the process hangs.\\nThe second problem when applying config is simply that the UI process will wait until the Interlok process has completed it's full startup, which of course it will not be able to do if a connection cannot be established.  Essentially the process hangs for a period of time.\\n","Decision":"### First issue\\nChosen option: Not yet decided.\\n#### Do Nothing\\nIn this case we accept that most people are not modifying production ready instances and even if they are they are probably not using the UI, therefore they are not going to run into the issues raised here.\\n#### Remove the synchronization\\nThere are a couple of ways to do this.\\nThe first would be to literally remove the synch blocks from implementations like this one;\\n```java\\npublic final void init() throws CoreException {\\nsynchronized (lock) {\\nif (!prepared) {\\nprepare();\\n}\\ninitConnection();\\nLifecycleHelper.init(connectionErrorHandler());\\n}\\n}\\n```\\nThe second way to do it would be to wrap the actual work inside the synch block with a new thread.\\n##### Consequences\\nAllowing a process to request a state change while in the middle of a previous state change could cause undefined behaviour especially around edge cases.\\n#### Interrupt the blocking transition\\nEssentially we would stop the transition currently running, in this case the reconnect inside the initialization phase transition.\\nThis can be done a couple of ways;\\nFirst we could make sure the thread handling the initialization transition is a named and managed thread from our ThreadManagerFactory and then add a new static method to that factory that would send an interrupt to that named thread.\\nOr considering the reconnect code could be refactored into the parent connection impl, we could cover all connections that have reconnect ability, which is all of those that extend AllowsRetriesConnection.  Once we make sure the reconnect code only exists in this parent class we then have a new volatile variable that gets set when a __close__ request is made the reconnect loop could check the value and exit if necessary.\\n### Second issue\\nChosen option: Not yet decided.\\n#### Do Nothing\\nIn this case we deem the current hanging behaviour as expected and correct.\\n#### Non-blocking applying of config\\nIn this case a change would be made to the UI application of config where it would request the full restart but not necessarily wait for the running instance to complete it's full start-up.  There would then either be some kind of notification once Interlok has completed the start-up or users would just be directed to the dashboard.\\n","tokens":448,"id":2346,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\n### Summary\\nAt launch, IBC will be a novel protocol, without an experienced user-base. At the protocol layer, it is not possible to distinguish between client expiry or misbehaviour due to genuine faults (Byzantine behavior) and client expiry or misbehaviour due to user mistakes (failing to update a client, or accidentally double-signing). In the base IBC protocol and ICS 20 fungible token transfer implementation, if a client can no longer be updated, funds in that channel will be permanently locked and can no longer be transferred. To the degree that it is safe to do so, it would be preferable to provide users with a recovery mechanism which can be utilised in these exceptional cases.\\n### Exceptional cases\\nThe state of concern is where a client associated with connection(s) and channel(s) can no longer be updated. This can happen for several reasons:\\n1. The chain which the client is following has halted and is no longer producing blocks\/headers, so no updates can be made to the client\\n1. The chain which the client is following has continued to operate, but no relayer has submitted a new header within the unbonding period, and the client has expired\\n1. This could be due to real misbehaviour (intentional Byzantine behaviour) or merely a mistake by validators, but the client cannot distinguish these two cases\\n1. The chain which the client is following has experienced a misbehaviour event, and the client has been frozen & thus can no longer be updated\\n### Security model\\nTwo-thirds of the validator set (the quorum for governance, module participation) can already sign arbitrary data, so allowing governance to manually force-update a client with a new header after a delay period does not substantially alter the security model.\\n\\n## Decision\\nWe elect not to deal with chains which have actually halted, which is necessarily Byzantine behaviour and in which case token recovery is not likely possible anyways (in-flight packets cannot be timed-out, but the relative impact of that is minor).\\n1. Require Tendermint light clients (ICS 07) to be created with the following additional flags\\n1. `allow_governance_override_after_expiry` (boolean, default false)\\n1. Require Tendermint light clients (ICS 07) to expose the following additional internal query functions\\n1. `Expired() boolean`, which returns whether or not the client has passed the trusting period since the last update (in which case no headers can be validated)\\n1. Require Tendermint light clients (ICS 07) to expose the following additional state mutation functions\\n1. `Unfreeze()`, which unfreezes a light client after misbehaviour and clears any frozen height previously set\\n1. Require Tendermint light clients (ICS 07) & solo machine clients (ICS 06) to be created with the following additional flags\\n1. `allow_governance_override_after_misbehaviour` (boolean, default false)\\n1. Add a new governance proposal type, `ClientUpdateProposal`, in the `x\/ibc` module\\n1. Extend the base `Proposal` with a client identifier (`string`) and a header (`bytes`, encoded in a client-type-specific format)\\n1. If this governance proposal passes, the client is updated with the provided header, if and only if:\\n1. `allow_governance_override_after_expiry` is true and the client has expired (`Expired()` returns true)\\n1. `allow_governance_override_after_misbehaviour` is true and the client has been frozen (`Frozen()` returns true)\\n1. In this case, additionally, the client is unfrozen by calling `Unfreeze()`\\nNote additionally that the header submitted by governance must be new enough that it will be possible to update the light client after the new header is inserted into the client state (which will only happen after the governance proposal has passed).\\nThis ADR does not address planned upgrades, which are handled separately as per the [specification](https:\/\/github.com\/cosmos\/ics\/tree\/master\/spec\/ics-007-tendermint-client#upgrades).\\n\\n\\n## Context\\nIn computing, an **idempotent operation** is one that has no additional effect if it is called more than once with the same input parameters.\\nA vast majority of critical flows of a web services we're running is required to provide this guarantee for their clients.\\nThus, it makes sense to extract this functionality into a library with support for common use-cases.\\nThere a two common approaches in the industry when it comes to implementing support for idempotency:\\n1) distributed **in-memory** data stores (i.e. Redis)\\n2) **database** - master node to avoid any issues caused by possible replication lag\\n### Client\\nTypically, the only additional requirement for the clients to enable idempotency is to add a unique identifier for the request\\nthat is only expected to have a side effect once. Sometimes APIs make this a mandatory property of the request.\\nFrom a security considerations and to lower a chance of collisions it's recommended to use UUIDv4 as a unique request identifier.\\n### Service\\nCritical requirements for the processing service:\\n1) **persisted result** for a successful and failed with non-retriable error request\\n2) **non-blocking** - any concurrent request with a same identifier should be rejected with a transient error\\nResult persistence aims to avoid any heavy computations on a re-try or duplicate requests if original request completed with a **success** or **non-retriable** error.\\nClients should have a freedom of choice on what exactly is being persisted and which format to use.\\nQuick failure in case of the aggressive retry policy on the client side ensures that no server resource is blocked and wasted for such concurrent requests with a same identifier.\\n3) request processing procedure and persistence of its result has to be transactional\\nWe require request execution logic and idempotence status update to happen in a same transaction.\\nThat requirement comes at a cost of the client flexibility. Allowing clients to control transactional context\\nintroduces edge-cases such as a failure to persist idempotency status whereas action has been completed and side effect taken place.\\n\\n## Decision\\nProvide code idempotence service based on a database approach, specific database integration is pluggable.\\nBoth **in-memory** and **database** approaches have their own pros and cons, we're not going to go in depth comparision here\\nmostly due to the lack of adoption of Redis in our current infrastructure. That said we should definitely consider providing such support in the future.\\nWe require request execution logic and idempotence status update to happen in a **same transaction**.\\nThat requirement comes at a cost of the client flexibility, but allowing clients to have full control over transaction context\\nintroduces edge-cases such as a **failure** to update imdepotency action status after **side effect** already took place.\\nThat would require clients to verify if side effect has taken place on each execution - which we consider suboptimal for most common cases.\\nIf that approach proves to be limiting for certain flows we can extend core service interface to allow clients to choose different execution strategies and have better control over transactional context.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\nAs recorded in [INTERLOK-2875](https:\/\/adaptris.atlassian.net\/browse\/INTERLOK-2975) when Interlok is attempting to reconnect to an endpoint it becomes very difficult to amend configuration through the UI.\\\\nThe reconnect loop is dictated by the reconnection settings with each configured connection.  When a connection is initialized if it cannot connect to it's endpoint the reconnect loop begins.  Connections are initialized on startup and during runtime should it lose it's connection, Interlok will shift the connection through the lifecycle phases until it is initialised again.\\\\nTherefore we have two triggers for this issue;\\\\n- A connection cannot be established on startup\\\\n- A connection breaks during runtime.\\\\nUntil a connection has either established it's connection or has reached the maximum number of configured reconnect attempts, the connection will remain in the initialization phase.\\\\nEach phase of an Interlok component is dictated by the StateManagedComponent interface that defines the 4 basic stages of __init__, __start__, __stop__ and __close__.  When we __request__ a change of a components state (from __init__ to __start__ for example) we typically synchronize the state change until it is complete.\\\\nThis means for example when Interlok launches the initialization phase kicks in which will attempt to initialize each connection.  Any connection that cannot be established as mentioned above will stay in the synchronized __init__ phase potentially forever.  Because the phase transition is synchronized we cannot therefore change this state until it's current transition has completed.\\\\nThere are currently two problems applying configuration from the UI while a connection is attempting reconnection.\\\\nWhen applying config the UI will request a restart of the running instance, which will attempt to run through the lifecycle of phases mentioned above.\\\\nBut of course if a connection is stuck in it's synchronized initialization phase trying to reconnect all other state change requests must wait therefore the process hangs.\\\\nThe second problem when applying config is simply that the UI process will wait until the Interlok process has completed it's full startup, which of course it will not be able to do if a connection cannot be established.  Essentially the process hangs for a period of time.\\\\n\"}, {'role': 'model', 'content': \"### First issue\\\\nChosen option: Not yet decided.\\\\n#### Do Nothing\\\\nIn this case we accept that most people are not modifying production ready instances and even if they are they are probably not using the UI, therefore they are not going to run into the issues raised here.\\\\n#### Remove the synchronization\\\\nThere are a couple of ways to do this.\\\\nThe first would be to literally remove the synch blocks from implementations like this one;\\\\n```java\\\\npublic final void init() throws CoreException {\\\\nsynchronized (lock) {\\\\nif (!prepared) {\\\\nprepare();\\\\n}\\\\ninitConnection();\\\\nLifecycleHelper.init(connectionErrorHandler());\\\\n}\\\\n}\\\\n```\\\\nThe second way to do it would be to wrap the actual work inside the synch block with a new thread.\\\\n##### Consequences\\\\nAllowing a process to request a state change while in the middle of a previous state change could cause undefined behaviour especially around edge cases.\\\\n#### Interrupt the blocking transition\\\\nEssentially we would stop the transition currently running, in this case the reconnect inside the initialization phase transition.\\\\nThis can be done a couple of ways;\\\\nFirst we could make sure the thread handling the initialization transition is a named and managed thread from our ThreadManagerFactory and then add a new static method to that factory that would send an interrupt to that named thread.\\\\nOr considering the reconnect code could be refactored into the parent connection impl, we could cover all connections that have reconnect ability, which is all of those that extend AllowsRetriesConnection.  Once we make sure the reconnect code only exists in this parent class we then have a new volatile variable that gets set when a __close__ request is made the reconnect loop could check the value and exit if necessary.\\\\n### Second issue\\\\nChosen option: Not yet decided.\\\\n#### Do Nothing\\\\nIn this case we deem the current hanging behaviour as expected and correct.\\\\n#### Non-blocking applying of config\\\\nIn this case a change would be made to the UI application of config where it would request the full restart but not necessarily wait for the running instance to complete it's full start-up.  There would then either be some kind of notification once Interlok has completed the start-up or users would just be directed to the dashboard.\\\\n\"}]"}
{"File Name":"lightweight-architecture-decision-records\/0001-use-elasticsearch-for-search-api.md","Context":"## Context\\nThere is a need of having an API exposed which can be used to search enterprise wide common data model.\\nThe data currently resides in a RDBMS database, it is difficult to expose micro-services directly querying out of RDBMS databases since the application runs out the same environment.\\nThere are options like ElasticSearch or Solr where data can be replicated.\\n","Decision":"Use ElasticSearch for data indexing\\n","tokens":76,"id":3246,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nCljdoc needs to store ad-hoc data that doesn't directly fit in its data model.\\nFor example: Project's download stats on Clojars, Projects contributors, stars on Github etc.\\nWe need this data to show alongside project's documentation. The source of truth for this kind of data is a service across the network.\\nClojars, Github, Maven Central etc. Making multiple network calls to render documentation for a projects deteriorates performance.\\n\\n## Decision\\nWe decide to cache this data to avoid multiple network calls. We are using `clojure.core.memoize` to provide a layer over this cache.\\nIt enables us to memoize functions that make network calls. `clojure.core.memoize` requires an implementation of `clojure.core.cache\/CacheProtocol`\\nto memoize return values of a function. We provide an implementation of this protocol. Our cache has two important properties.\\nFirst, it is persistent (backed by cljdoc datastore), we don't want to loose cached data on server restart.\\nSecond, it has a TTL functionality. Some data such as Github stars and Clojars download count etc should be refreshed timely.\\nTTL can be used to set expiry time for a cached item.\\n\\n\\n## Context\\nTransfer Digital Records needs a web interface to allow government users to\\nupload files to be transferred to The National Archives.\\nThere are many frameworks to choose from when building user interfaces for the\\nweb. Most modern programming languages have at least one widely-used server-side\\nweb framework, such as Rails (Ruby), Node.js (JavaScript) or Spring (Java). There\\nare also many client-side JavaScript frameworks such as React, Vue or Angular.\\nThese are optimised for presenting dynamic content that can be updated without a\\nfull page reload.\\n\\n## Decision\\nUse [Scala Play][play].\\nWe chose Play over other MVC frameworks because we want to continue developing\\nin Scala.\\nWe chose a server-side MVC framework over Next.Js, which was the other main\\noption that we prototyped, because we want to support progressive enhancement as\\nmuch as possible, and this is easier in a framework like Play whose main purpose\\nis server-side rendering.\\nAlthough Next.Js also supports server-side rendering, it's main purpose is to\\nspeed up the rendering of React sites, so its purpose is more to enhance\\nclient-side JavaScript than to support situations where JS isn't available or\\nfails to load.\\nAgain, see [Front end technology considerations][alpha-considerations] and\\nthe [Play MVC vs Next.js comparison][play-vs-next] for more considerations which\\nfed into the decision.\\n[play]: https:\/\/www.playframework.com\/\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThere is a need of having an API exposed which can be used to search enterprise wide common data model.\\\\nThe data currently resides in a RDBMS database, it is difficult to expose micro-services directly querying out of RDBMS databases since the application runs out the same environment.\\\\nThere are options like ElasticSearch or Solr where data can be replicated.\\\\n\"}, {'role': 'model', 'content': 'Use ElasticSearch for data indexing\\\\n'}]"}
{"File Name":"remultiform\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":5261,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need to record the architectural decisions made on this project.\\\\n'}, {'role': 'model', 'content': 'We will use Architecture Decision Records, as\\\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\\\n'}]"}
{"File Name":"uniprot-website\/0004-state-management.md","Context":"## Context\\nThe state of data and UI will grow in the application as the development goes on. There needs to be good state management mechanism to handle this, particularly when components will require interaction across the component hierarchy.\\n","Decision":"[Redux](https:\/\/redux.js.org\/) will be used for state management. It is a centralised state management container that can handle data and UI state.\\n","tokens":44,"id":2014,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nEven a self-discoverable API needs to be documented to let consumers understand the underlying application features and\\nthe resources attributes significance.\\nA good API documentation will provide requests and responses structure, defining each attribute, giving examples and\\nmixing functional and technical information so that consumers do not need to traverse different documents to get a\\ncomplete overview of the features and technical concerns.\\nThe documentation must also provide links between different parts of the document, so that it is easy to go back and\\nforth between those parts.\\nThe documentation must be kept in sync with the features. Ideally, a documentation is generated from the code.\\n\\n## Decision\\n`menu-generation` will generate its API documentation with [Spring Rest-Docs](https:\/\/spring.io\/projects\/spring-restdocs).\\n\\n\\n## Context\\nThe framework is being designed to provide portability for embedded systems applications. We also want to maximize reuse of software across embedded products to minimize bring-up time and to increase our confidence in the software.\\nIn order to accomplish this, we must think about how to isolate the various platform-specific forces of change that occur within embedded products:\\n* The processor\\n* The layout\/connections on a given board\\n* The memory availability\\n* The component ICs that are included on the board\\nBy isolating these various forces of change and platform dependence, we can improve the possibility of re-use on embedded platforms.\\n\\n## Decision\\nThe architecture is structured in three layers:\\n1. Virtual Processor Layer (Base)\\n2. Virtual Platform Layer\\n3. Software Layer\\nThe virtual processor layer is a standalone layer which manages the chip and HAL.\\nThe virtual platform layer contains platform-specific implementations for the C\/C++ runtime, RTOS, and board\/platforms.\\nThe software layer contains standalone software which is built to use the virtual platform abstractions.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThe state of data and UI will grow in the application as the development goes on. There needs to be good state management mechanism to handle this, particularly when components will require interaction across the component hierarchy.\\\\n'}, {'role': 'model', 'content': '[Redux](https:\/\/redux.js.org\/) will be used for state management. It is a centralised state management container that can handle data and UI state.\\\\n'}]"}
{"File Name":"oasis-core\/0007-improved-random-beacon.md","Context":"## Context\\n> Any one who considers arithmetical methods of producing random digits\\n> is, of course, in a state of sin.\\n>\\n> --Dr. John von  Neumann\\nThe existing random beacon used by Oasis Core, is largely a placeholder\\nimplementation that naively uses the previous block's commit hash as the\\nentropy input.  As such it is clearly insecure as it is subject to\\nmanipulation.\\nA better random beacon which is harder for an adversary to manipulate\\nis required to provide entropy for secure committee elections.\\n","Decision":"At a high level, this ADR proposes implementing an on-chain random beacon\\nbased on \"SCRAPE: Scalabe Randomness Attested by Public Entities\" by\\nCascudo and David.  The new random beacon will use a commit-reveal scheme\\nbacked by a PVSS scheme so that as long as the threshold of participants\\nis met, and one participant is honest, secure entropy will be generated.\\nNote: This document assumes the reader understands SCRAPE. Details\\nregarding the underlying SCRAPE implementation are omitted for brevity.\\n### Node Descriptor\\nThe node descriptor of each node will be extended to include the following\\ndatastructure.\\n```golang\\ntype Node struct {\\n\/\/ ... existing fields omitted ...\\n\/\/ Beacon contains information for this node's participation\\n\/\/ in the random beacon protocol.\\n\/\/\\n\/\/ TODO: This is optional for now, make mandatory once enough\\n\/\/ nodes provide this field.\\nBeacon *BeaconInfo `json:\"beacon,omitempty\"`\\n}\\n\/\/ BeaconInfo contains information for this node's participation in\\n\/\/ the random beacon protocol.\\ntype BeaconInfo struct {\\n\/\/ Point is the elliptic curve point used for the PVSS algorithm.\\nPoint scrape.Point `json:\"point\"`\\n}\\n```\\nEach node will generate and maintain a long term elliptic curve point\\nand scalar pair (public\/private key pair), the point (public key) of\\nwhich will be included in the node descriptor.\\nFor the purposes of the initial implementation, the curve will be P-256.\\n### Consensus Parameters\\nThe beacon module will have the following consensus parameters that\\ncontrol behavior.\\n```golang\\ntype SCRAPEParameters struct {\\nParticipants  uint64 `json:\"participants\"`\\nThreshold     uint64 `json:\"threshold\"`\\nPVSSThreshold uint64 `json:\"pvss_threshold\"`\\nCommitInterval  int64 `json:\"commit_interval\"`\\nRevealInterval  int64 `json:\"reveal_interval\"`\\nTransitionDelay int64 `json:\"transition_delay\"`\\n}\\n```\\nFields:\\n- `Participants` - The number of participants to be selected for each\\nbeacon generation protocol round.\\n- `Threshold` - The minimum number of participants which must\\nsuccessfully contribute entropy for the final output to be\\nconsidered valid.\\n- `PVSSThreshold` - The minimum number of participants that are\\nrequired to reconstruct a PVSS secret from the corresponding\\ndecrypted shares (Note: This usually should just be set to\\n`Threshold`).\\n- `CommitInterval` - The duration of the Commit phase, in blocks.\\n- `RevealInterval` - The duration of the Reveal phase, in blocks.\\n- `TransitionDelay` - The duration of the post Reveal phase delay, in blocks.\\n### Consensus State and Events\\nThe on-chain beacon will maintain and make available the following consensus\\nstate.\\n```golang\\n\/\/ RoundState is a SCRAPE round state.\\ntype RoundState uint8\\nconst (\\nStateInvalid  RoundState = 0\\nStateCommit   RoundState = 1\\nStateReveal   RoundState = 2\\nStateComplete RoundState = 3\\n)\\n\/\/ SCRAPEState is the SCRAPE backend state.\\ntype SCRAPEState struct {\\nHeight int64 `json:\"height,omitempty\"`\\nEpoch EpochTime  `json:\"epoch,omitempty\"`\\nRound uint64     `json:\"round,omitempty\"`\\nState RoundState `json:\"state,omitempty\"`\\nInstance     *scrape.Instance      `json:\"instance,omitempty\"`\\nParticipants []signature.PublicKey `json:\"participants,omitempty\"`\\nEntropy      []byte                `json:\"entropy,omitempty\"`\\nBadParticipants map[signature.PublicKey]bool `json:\"bad_participants,omitempty\"`\\nCommitDeadline   int64 `json:\"commit_deadline,omitempty\"`\\nRevealDeadline   int64 `json:\"reveal_deadline,omitempty\"`\\nTransitionHeight int64 `json:\"transition_height,omitempty\"`\\nRuntimeDisableHeight int64 `json:\"runtime_disable_height,omitempty\"`\\n}\\n```\\nFields:\\n- `Height` - The block height at which the last event was emitted.\\n- `Epoch` - The epoch in which this beacon is being generated.\\n- `Round` - The epoch beacon generation round.\\n- `State` - The beacon generation step (commit\/reveal\/complete).\\n- `Instance` - The SCRAPE protocol state (encrypted\/decrypted shares of\\nall participants).\\n- `Participants` - The node IDs of the nodes selected to participate\\nin this beacon generation round.\\n- `Entropy` - The final raw entropy, if any.\\n- `BadParticipants` - A map of nodes that were selected, but have failed\\nto execute the protocol correctly.\\n- `CommitDeadline` - The height in blocks by which participants must\\nsubmit their encrypted shares.\\n- `RevealDeadline` - The height in blocks by which participants must\\nsubmit their decrypted shares.\\n- `TransitionHeight` - The height at which the epoch will transition\\nassuming this round completes successfully.\\n- `RuntimeDisableHeight` - The height at which, upon protocol failure,\\nruntime transactions will be disabled.  This height will be set to\\nthe transition height of the 0th round.\\nUpon transition to a next step of the protocol, the on-chain beacon will\\nemit the following event.\\n```golang\\n\/\/ SCRAPEEvent is a SCRAPE backend event.\\ntype SCRAPEEvent struct {\\nHeight int64 `json:\"height,omitempty\"`\\nEpoch EpochTime  `json:\"epoch,omitempty\"`\\nRound uint64     `json:\"round,omitempty\"`\\nState RoundState `json:\"state,omitempty\"`\\nParticipants []signature.PublicKey `json:\"participants,omitempty\"`\\n}\\n```\\nField definitions are identical to that of those in the `SCRAPEState`\\ndatastructure.\\n","tokens":116,"id":4351,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe current data model assumes all inputs to arrive via a single\\ncollection of `(e a v)` tuples. Although this is very flexible, we are\\nfacing three major problems:\\n(1) Reading data from sources other than Datomic (e.g. Kafka or files\\non disk) is awkward, as those sources usually keep data in separate\\nrelations (e.g. Kafka topics). Reading across sources also invalidates\\nthe assumption of a single time axis on which all inputs reside.\\n(2) Inflexible arrangements. The hard assumption on a single input\\ncollection is reflected in similarly hardcoded indexing structures\\n(the `DB` and `ImplContext` structs). External sources would therefore\\nnot benefit from any shared arrangements currently. Ultimately, we\\nwant all decisions on what computations to store in arrangements to be\\nmade by some kind of optimiser. The current indexing scheme is merely\\na special case of that problem and should not require special\\nconstructs.\\n(3) Layout inefficiencies. In principle we could do away with tuples\\nentirely and simply rely on `Vec<Value>` everywhere. As most queries\\nusually interact with many attributes directly, this would mean\\nleaving a lot of optimisation potential on the table.\\n\\n## Decision\\nWe will move to a column-oriented data model, by maintaining a\\nseparate collection for each attribute.\\nWe will separate specialized operator implementations from the process\\nof chosing the right operators in every situation. E.g. we will not\\nonly provide a general `Join`, but also something like\\n`JoinColumns`. We will also encode information on existing\\narrangements to use in the plan type, e.g. `MatchE -> MatchArrangedE`.\\n\\n\\n## Context\\nAfter implementing several real business domains we have found that it can be\\ndifficult to use `AggregateCommandScope.Destroy()` effectively.\\nIdeally, `Destroy()` could be called after recording any event that effectively\\n\"resets\" the aggregate\\'s state, from the perspective of the business logic.\\nIn practice, logic that results in a call to `Destroy()` may be followed by some\\nconditional logic that records a new event. In existing implementations this\\nresults in a panic from the engine, though as of ADR-16, which removed\\n`Create()` this is no longer documented as required engine behavior.\\n\\n## Decision\\nRemove the requirement that any call to `Destroy()` be preceeded by a call to\\n`RecordEvent()` within the same scope.\\nCalling `RecordEvent()` *after* `Destroy()` event should \"negate\" the call to\\n`Destroy()`, as though it were never called.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\n> Any one who considers arithmetical methods of producing random digits\\\\n> is, of course, in a state of sin.\\\\n>\\\\n> --Dr. John von  Neumann\\\\nThe existing random beacon used by Oasis Core, is largely a placeholder\\\\nimplementation that naively uses the previous block\\'s commit hash as the\\\\nentropy input.  As such it is clearly insecure as it is subject to\\\\nmanipulation.\\\\nA better random beacon which is harder for an adversary to manipulate\\\\nis required to provide entropy for secure committee elections.\\\\n'}, {'role': 'model', 'content': 'At a high level, this ADR proposes implementing an on-chain random beacon\\\\nbased on \"SCRAPE: Scalabe Randomness Attested by Public Entities\" by\\\\nCascudo and David.  The new random beacon will use a commit-reveal scheme\\\\nbacked by a PVSS scheme so that as long as the threshold of participants\\\\nis met, and one participant is honest, secure entropy will be generated.\\\\nNote: This document assumes the reader understands SCRAPE. Details\\\\nregarding the underlying SCRAPE implementation are omitted for brevity.\\\\n### Node Descriptor\\\\nThe node descriptor of each node will be extended to include the following\\\\ndatastructure.\\\\n```golang\\\\ntype Node struct {\\\\n\/\/ ... existing fields omitted ...\\\\n\/\/ Beacon contains information for this node\\'s participation\\\\n\/\/ in the random beacon protocol.\\\\n\/\/\\\\n\/\/ TODO: This is optional for now, make mandatory once enough\\\\n\/\/ nodes provide this field.\\\\nBeacon *BeaconInfo `json:\"beacon,omitempty\"`\\\\n}\\\\n\/\/ BeaconInfo contains information for this node\\'s participation in\\\\n\/\/ the random beacon protocol.\\\\ntype BeaconInfo struct {\\\\n\/\/ Point is the elliptic curve point used for the PVSS algorithm.\\\\nPoint scrape.Point `json:\"point\"`\\\\n}\\\\n```\\\\nEach node will generate and maintain a long term elliptic curve point\\\\nand scalar pair (public\/private key pair), the point (public key) of\\\\nwhich will be included in the node descriptor.\\\\nFor the purposes of the initial implementation, the curve will be P-256.\\\\n### Consensus Parameters\\\\nThe beacon module will have the following consensus parameters that\\\\ncontrol behavior.\\\\n```golang\\\\ntype SCRAPEParameters struct {\\\\nParticipants  uint64 `json:\"participants\"`\\\\nThreshold     uint64 `json:\"threshold\"`\\\\nPVSSThreshold uint64 `json:\"pvss_threshold\"`\\\\nCommitInterval  int64 `json:\"commit_interval\"`\\\\nRevealInterval  int64 `json:\"reveal_interval\"`\\\\nTransitionDelay int64 `json:\"transition_delay\"`\\\\n}\\\\n```\\\\nFields:\\\\n- `Participants` - The number of participants to be selected for each\\\\nbeacon generation protocol round.\\\\n- `Threshold` - The minimum number of participants which must\\\\nsuccessfully contribute entropy for the final output to be\\\\nconsidered valid.\\\\n- `PVSSThreshold` - The minimum number of participants that are\\\\nrequired to reconstruct a PVSS secret from the corresponding\\\\ndecrypted shares (Note: This usually should just be set to\\\\n`Threshold`).\\\\n- `CommitInterval` - The duration of the Commit phase, in blocks.\\\\n- `RevealInterval` - The duration of the Reveal phase, in blocks.\\\\n- `TransitionDelay` - The duration of the post Reveal phase delay, in blocks.\\\\n### Consensus State and Events\\\\nThe on-chain beacon will maintain and make available the following consensus\\\\nstate.\\\\n```golang\\\\n\/\/ RoundState is a SCRAPE round state.\\\\ntype RoundState uint8\\\\nconst (\\\\nStateInvalid  RoundState = 0\\\\nStateCommit   RoundState = 1\\\\nStateReveal   RoundState = 2\\\\nStateComplete RoundState = 3\\\\n)\\\\n\/\/ SCRAPEState is the SCRAPE backend state.\\\\ntype SCRAPEState struct {\\\\nHeight int64 `json:\"height,omitempty\"`\\\\nEpoch EpochTime  `json:\"epoch,omitempty\"`\\\\nRound uint64     `json:\"round,omitempty\"`\\\\nState RoundState `json:\"state,omitempty\"`\\\\nInstance     *scrape.Instance      `json:\"instance,omitempty\"`\\\\nParticipants []signature.PublicKey `json:\"participants,omitempty\"`\\\\nEntropy      []byte                `json:\"entropy,omitempty\"`\\\\nBadParticipants map[signature.PublicKey]bool `json:\"bad_participants,omitempty\"`\\\\nCommitDeadline   int64 `json:\"commit_deadline,omitempty\"`\\\\nRevealDeadline   int64 `json:\"reveal_deadline,omitempty\"`\\\\nTransitionHeight int64 `json:\"transition_height,omitempty\"`\\\\nRuntimeDisableHeight int64 `json:\"runtime_disable_height,omitempty\"`\\\\n}\\\\n```\\\\nFields:\\\\n- `Height` - The block height at which the last event was emitted.\\\\n- `Epoch` - The epoch in which this beacon is being generated.\\\\n- `Round` - The epoch beacon generation round.\\\\n- `State` - The beacon generation step (commit\/reveal\/complete).\\\\n- `Instance` - The SCRAPE protocol state (encrypted\/decrypted shares of\\\\nall participants).\\\\n- `Participants` - The node IDs of the nodes selected to participate\\\\nin this beacon generation round.\\\\n- `Entropy` - The final raw entropy, if any.\\\\n- `BadParticipants` - A map of nodes that were selected, but have failed\\\\nto execute the protocol correctly.\\\\n- `CommitDeadline` - The height in blocks by which participants must\\\\nsubmit their encrypted shares.\\\\n- `RevealDeadline` - The height in blocks by which participants must\\\\nsubmit their decrypted shares.\\\\n- `TransitionHeight` - The height at which the epoch will transition\\\\nassuming this round completes successfully.\\\\n- `RuntimeDisableHeight` - The height at which, upon protocol failure,\\\\nruntime transactions will be disabled.  This height will be set to\\\\nthe transition height of the 0th round.\\\\nUpon transition to a next step of the protocol, the on-chain beacon will\\\\nemit the following event.\\\\n```golang\\\\n\/\/ SCRAPEEvent is a SCRAPE backend event.\\\\ntype SCRAPEEvent struct {\\\\nHeight int64 `json:\"height,omitempty\"`\\\\nEpoch EpochTime  `json:\"epoch,omitempty\"`\\\\nRound uint64     `json:\"round,omitempty\"`\\\\nState RoundState `json:\"state,omitempty\"`\\\\nParticipants []signature.PublicKey `json:\"participants,omitempty\"`\\\\n}\\\\n```\\\\nField definitions are identical to that of those in the `SCRAPEState`\\\\ndatastructure.\\\\n'}]"}
{"File Name":"js-sdk\/0013-update-cryptpad-flist.md","Context":"## Context\\nCryptpad solution image used in docker was deprecated and flist doesn't make use of volumes\\n","Decision":"- Update flist to make use of volumes\\n- Update base image in docker file to be able to maintain\\n","tokens":22,"id":5202,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nRight now, JabRef does not have support for Biblatex-Software out of the box, users have to add custom entry types.\\nWith citing software becoming fairly common, native support is helpful.\\n## Decision Drivers\\n* None of the existing flows should be impacted\\n\\n## Decision\\n* None of the existing flows should be impacted\\nChosen option: \"Add a new divider\", because comes out best (see below).\\n### Positive Consequences\\n* Inbuilt coverage for a entry type that is getting more and more importance\\n### Negative Consequences\\n* Adds a little bit more clutter to the Add Entry pane\\n\\n\\n## Context\\nWe don\\'t store any user data on back-end yet, so we can do it in AsyncStorage.\\n\\n## Decision\\nWe will use [redux-persist](https:\/\/github.com\/rt2zz\/redux-persist)\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nCryptpad solution image used in docker was deprecated and flist doesn\\'t make use of volumes\\\\n'}, {'role': 'model', 'content': '- Update flist to make use of volumes\\\\n- Update base image in docker file to be able to maintain\\\\n'}]"}
{"File Name":"dogma\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1621,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need to record the architectural decisions made on this project.\\\\n'}, {'role': 'model', 'content': 'We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\\\n'}]"}
{"File Name":"functionaut\/0004-no-type-checking.md","Context":"## Context\\nMaking sure that a function operates with the right input and providing the user with a good debugging experience is not trivial.\\nConsider this contrived example:\\n```javascript\\nfunction inc(x) {\\nif (typeof x != 'number') throw new Error('x is not a number');\\nreturn x + 1;\\n}\\nfunction inc_array(xs) {\\nif (!Array.isArray(xs)) throw new Error('xs is not an array');\\nreturn xs.map(x => inc(x)); \/\/ <- May or may not throw\\n}\\n```\\nWe cannot guarantee that `inc_array` won't throw even if `xs` is an array. If we wanted to give early feedback, we would need to go through each `x` and do the same type checking that `inc` already does. So if the function is invoked correctly we would type check `xs` once and each `x` twice!\\n","Decision":"This project will adopt a *\"Garbage In \u2014 Garbage Out\"* philosophy and deliver a *\"no hand-holding\"* library.\\n","tokens":192,"id":2688,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe recently discovered an issue with one of the classes in\\n`service-api\/module\/Application\/src\/Model\/DataAccess\/Postgres\/`, where\\na search parameter was not correctly escaped. On investigation,\\nwe found that test coverage of the files in that directory was\\npractically zero.\\nWe also found that writing unit tests for these classes was tricky, as\\nthey were acting as factory classes for real database connections. This\\nmade it difficult to mock out result sets etc. to test different\\ncode paths.\\nIn pseudo-code, the old code followed this pattern:\\n```\\nclass AbstractBase\\n{\\n\/\/ $adapter is a Laminas\\\\Db\\\\Adapter\\\\Adapter;\\n\/\/ this constructs AbstractBase with a reference\\n\/\/ to a real database adapter\\npublic __construct($adapter)\\n{\\n$this->adapter = $adapter;\\n}\\npublic getAdapter()\\n{\\nreturn $this->adapter;\\n}\\n}\\nclass UserData\\n{\\npublic getData()\\n{\\n$query = new Sql($this->getAdapter());\\n}\\n}\\n```\\nNote that the `getData()` method is constructing the SQL directly\\ninside the `UserData` class. This means we can't mock it out\\nin a test context as we can't control how it's constructed.\\n\\n## Decision\\nRefactor the `*Data` database access classes so that database-specific\\noperations and SQL construction can be mocked in a test context.\\nWe decided to pull the adapter out of the data access\\nclasses, instead wrapping it in a `DbWrapper` class. We then put some utility\\nmethods on this class, such as a method for creating a `Sql` instance.\\nThe `DbWrapper` then holds the reference to the adapter.\\nIt looks like this:\\n```\\nclass DbWrapper\\n{\\npublic __construct($adapter)\\n{\\n$this->adapter = $adapter;\\n}\\npublic createSql()\\n{\\nreturn new Sql($this->adapter);\\n}\\n}\\nclass AbstractBase\\n{\\npublic __construct($dbWrapper)\\n{\\n$this->dbWrapper = $dbWrapper;\\n}\\n}\\nclass UserData\\n{\\npublic getData()\\n{\\n$query = $this->dbWrapper->createSql();\\n}\\n}\\n```\\nWe are now able to pass a mock `DbWrapper` instance to each data access\\nclass; in turn, we can then return mock `Sql` instances from its\\n`createSql()` method. This will make it possible to unit test classes\\nwhich were previously very difficult to test.\\n\\n\\n## Context\\nIf you make a log like this:\\n```js\\nclass ConsoleLog {\\nconstructor(opts = {}) {\\nthis._opts = { allowTrace: false, ...opts };\\n};\\ninfo(message) {\\nconsole.log(message);\\n}\\ntrace(message) {\\nconsole.log(`ths: ${this}`);\\nif (this._opts.allowTrace) {\\nconsole.log(message);\\n}\\n}\\n}\\n```\\nand then pass a reference to `trace` to another function, `trace` fails because `this` is undefined.\\n\\n## Decision\\nThe change that we're proposing or have agreed to implement.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nMaking sure that a function operates with the right input and providing the user with a good debugging experience is not trivial.\\\\nConsider this contrived example:\\\\n```javascript\\\\nfunction inc(x) {\\\\nif (typeof x != 'number') throw new Error('x is not a number');\\\\nreturn x + 1;\\\\n}\\\\nfunction inc_array(xs) {\\\\nif (!Array.isArray(xs)) throw new Error('xs is not an array');\\\\nreturn xs.map(x => inc(x)); \/\/ <- May or may not throw\\\\n}\\\\n```\\\\nWe cannot guarantee that `inc_array` won't throw even if `xs` is an array. If we wanted to give early feedback, we would need to go through each `x` and do the same type checking that `inc` already does. So if the function is invoked correctly we would type check `xs` once and each `x` twice!\\\\n\"}, {'role': 'model', 'content': 'This project will adopt a *\"Garbage In \u2014 Garbage Out\"* philosophy and deliver a *\"no hand-holding\"* library.\\\\n'}]"}
{"File Name":"paas-team-manual\/ADR029-aiven-project-structure.html.md","Context":"## Context\\nAiven provides hosted Elasticsearch for the Elasticsearch backing service.\\nThe PaaS has several environments which will need to use Aiven. These\\nenvironments should be isolated from each other so that changes made in testing\\nand development environments do do not affect production users.\\nAiven provide a \"Project\" abstraction where a user can be a member of several\\nprojects. API tokens are user specific. By creating one user per project it's\\npossible to scope API tokens to a project.\\n","Decision":"We'll use separate projects for separate environments, initially using the\\nfollowing Aiven projects:\\n* ci-testing (for the CI environment for the elasticsearch broker itself)\\n* paas-cf-dev\\n* paas-cf-staging\\n* paas-cf-prod\\nFor staging and prod we will use separate API tokens within the same project to\\nseparate credentials between the London and Ireland regions.\\nWe will have the following per-project users to hold API tokens:\\nthe-multi-cloud-paas-team+aiven-ci@digital.cabinet-office.gov.uk\\nthe-multi-cloud-paas-team+aiven-dev@digital.cabinet-office.gov.uk\\nthe-multi-cloud-paas-team+aiven-staging@digital.cabinet-office.gov.uk\\nthe-multi-cloud-paas-team+aiven-prod@digital.cabinet-office.gov.uk\\nThe credentials for the ci and dev users will be stored in the\\n`paas-credentials` passwordstore. staging and prod will be stored in\\n`paas-credentials-high`.\\nMembers of the PaaS team will each have their own user which will have access\\nall of the projects for management purposes.\\n","tokens":100,"id":191,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe want a mechanism that allows for the tests to be run whenever changes are made to the project.\\nWe also want the project to be deployed whenever these tests pass.\\nCircleCI allows for builds to run in a Docker container based on a specified \"root\" image, with support for specifying additonal images for dependencies, i.e. database required for integration tests.\\nCircleCI provides open source projects with four free linux containers for running builds.\\nAll of the configuration for a project built with CircleCI is stored in the project, except for environment variables.\\nMicrosoft provides the \"aspnetcore-build\" Docker image, which contains all the dependencies required to build an ASP.NET Core application.\\n\\n## Decision\\nWe will use CircleCI to handle continuous integration builds and deployments.\\n\\n\\n## Context and Problem Statement\\nNimbus Experiment configurations are stored in the Experiment Console (Experimenter), sychronized to\\nclients via Remote Settings and interpreted by the Nimbus SDK, which as of current writing, includes\\nboth a JS implementation in Firefox Desktop and a Rust implementation that will eventually replace\\nit. Jetstream must also call the Experimenter API in order to read metadata about live experiments.\\nWe need some way to validate data across multiple applications and languages (Python, Rust, JS) and\\na process for publishing changes.\\n\\n## Decision\\n### Format and Language\\nWe decided to use [JSON Schema Draft-07](https:\/\/tools.ietf.org\/html\/draft-handrews-json-schema-01)\\nfor cross-platform validation of the experiment schema since validators are available in many\\nlanguages, including Python, Rust, and JS.\\nHowever, because JSON Schema is difficult to read and write directly, we decided to use Typescript\\nas the source for Experiment schemas and generate JSON Schema from them using\\n`ts-json-schema-generator`. This library allows us add validations and annotations outside of what\\nTypescript provides in its type system (e.g. integers, default and min values, description).\\n### Publishing changes across the system\\nThe source code for the schema lives in a cental repository\\n([mozilla\/nimbus-shared](https:\/\/github.com\/mozilla\/nimbus-shared\/blob\/9a74dde37cbfb73dacd49e8daa66b66cb013619b\/types\/experiments.ts)).\\nSchemas are generated on CircleCI and clients can install the `nimbus-shared` package to access them\\n(as well as some utility methods) from [npm](https:\/\/www.npmjs.com\/package\/@mozilla\/nimbus-shared)\\nor [PyPI](https:\/\/pypi.org\/project\/mozilla-nimbus-shared\/).\\n### Process for introducing updates to the schema\\n1. Create a PR on the nimbus-shared repo, cc teach leads from other projects.\\n2. Use the nimbus-shared\\n[release process](https:\/\/github.com\/mozilla\/nimbus-shared\/blob\/main\/.github\/ISSUE_TEMPLATE\/release-checklist.md)\\nto tag and release a new version of the schema to npm\/PyPI.\\n3. Bump the version in clients using package managers (Experimenter, Jetstream,\\nmozilla-inflight-assets) to upgrade to the new version of the schema.\\n4. Manually update Firefox Desktop by copying the latest schema and creating a phabricator patch.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nAiven provides hosted Elasticsearch for the Elasticsearch backing service.\\\\nThe PaaS has several environments which will need to use Aiven. These\\\\nenvironments should be isolated from each other so that changes made in testing\\\\nand development environments do do not affect production users.\\\\nAiven provide a \"Project\" abstraction where a user can be a member of several\\\\nprojects. API tokens are user specific. By creating one user per project it\\'s\\\\npossible to scope API tokens to a project.\\\\n'}, {'role': 'model', 'content': \"We'll use separate projects for separate environments, initially using the\\\\nfollowing Aiven projects:\\\\n* ci-testing (for the CI environment for the elasticsearch broker itself)\\\\n* paas-cf-dev\\\\n* paas-cf-staging\\\\n* paas-cf-prod\\\\nFor staging and prod we will use separate API tokens within the same project to\\\\nseparate credentials between the London and Ireland regions.\\\\nWe will have the following per-project users to hold API tokens:\\\\nthe-multi-cloud-paas-team+aiven-ci@digital.cabinet-office.gov.uk\\\\nthe-multi-cloud-paas-team+aiven-dev@digital.cabinet-office.gov.uk\\\\nthe-multi-cloud-paas-team+aiven-staging@digital.cabinet-office.gov.uk\\\\nthe-multi-cloud-paas-team+aiven-prod@digital.cabinet-office.gov.uk\\\\nThe credentials for the ci and dev users will be stored in the\\\\n`paas-credentials` passwordstore. staging and prod will be stored in\\\\n`paas-credentials-high`.\\\\nMembers of the PaaS team will each have their own user which will have access\\\\nall of the projects for management purposes.\\\\n\"}]"}
{"File Name":"gsp\/ADR017-vendor-provided-container-orchestration.md","Context":"## Context\\nFollowing the [rollout of AWS EKS in London](https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/02\/amazon-eks-available-in-mumbai--london--and-paris-aws-regions\/) it is now an attractive alternative to the hand-rolled kubernetes installation that was created as a result of [ADR003](ADR003-container-orchestration.md). This will bring numerous benefits:\\n* reducing the amount of infrastructure we manage in-house, by offloading it to AWS\\n* better alignment with [Technology & Operations Strategic Principle #3 - \"Use fully managed cloud services by default\"](https:\/\/reliability-engineering.cloudapps.digital\/documentation\/strategy-and-principles\/re-principles.html#3-use-fully-managed-cloud-services-by-default)\\nAs of 1.12, EKS supports what we need (e.g. Istio, kiam etc.).\\n","Decision":"We will host the GDS Supported Platform on AWS EKS.\\n","tokens":194,"id":3911,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe require a method of managing and directing external internet traffic into the cluster. Kubernetes provides [several options for handling inbound traffic](https:\/\/medium.com\/google-cloud\/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0).\\nWe do not want to expose the cluster directly to the internet, and so require an AWS load balancer in front of the cluster. The load balancer must support TLS termination and integration with our DNS provider (AWS Route 53).\\nHistorically Kubernetes has supported provisioning of ALBs and NLBs for `Service` resources of `type=LoadBalancer` via the in-tree (built-in) [AWS cloud provider](https:\/\/github.com\/kubernetes\/cloud-provider-aws), with out-of-tree controllers required for `Ingress` resources. Built-in cloud providers are now [considered deprecated overall, in favour of out-of-tree providers](https:\/\/kubernetes.io\/blog\/2019\/04\/17\/the-future-of-cloud-providers-in-kubernetes\/), so an [Ingress Controller](https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/ingress-controllers\/) with support for `Service` resources is required.\\nThe primary and recommended ingress controller for AWS\/EKS is the [AWS Load Balancer Controller](https:\/\/docs.aws.amazon.com\/eks\/latest\/userguide\/aws-load-balancer-controller.html), which can provision and manage [ALBs for `Ingress` resources and NLBs for `Service` resources](https:\/\/kubernetes-sigs.github.io\/aws-load-balancer-controller\/v2.2\/#aws-load-balancer-controller).\\nWe must also consider how Kubernetes edge services and AWS load balancers will interact with the existing [GOV.UK Router service](https:\/\/github.com\/alphagov\/router), as there is significant overlap in their functionality and responsibilities. This will require further investigation and likely experimentation, and so that end we should ensure that we\\'re able to use both `Ingress` and `Service` Kubernetes resources so that we have the flexibility to support a wide range of use cases in the immediate term - L4 & L7 traffic, name-based routing, HTTP->HTTPS redirection, etc.\\n\\n## Decision\\nUse the [AWS Load Balancer Controller](https:\/\/github.com\/kubernetes-sigs\/aws-load-balancer-controller).\\n\\n\\n## Context\\nWe should document architecture and technical decisions for the shopware platform. The documentation should be easy to understand and easy to follow. The workflow for new decisions should add to our existing workflows and should not block the whole development process. One solution could be the form of architecture decision records (ADR) as described in the following articles:\\n*  [Documenting Architecture Decisions](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions)\\n*  [A Simple but Powerful Tool to Record Your Architectural Decisions](https:\/\/medium.com\/better-programming\/here-is-a-simple-yet-powerful-tool-to-record-your-architectural-decisions-5fb31367a7da)\\n\\n## Decision\\n*  [A Simple but Powerful Tool to Record Your Architectural Decisions](https:\/\/medium.com\/better-programming\/here-is-a-simple-yet-powerful-tool-to-record-your-architectural-decisions-5fb31367a7da)\\nWe will record architecture decisions (ADR) in markdown files directly in the platform repository. The workflow for ADRs will be integrated in the existing merge request workflow. This has the following advantages:\\n*  Decision records are an integral part of the development process\\n*  Decisions remain in sync with the code itself\\n*  The Git history is also the decision history\\n*  Decisions are public available and accessible for every developer\\n*  Also external developers can add new ADRs via GitHub pull requests\\n*  Decision finding can be asynchronous via comments in the corresponding merge request\\n**How can you create new ADRs?**\\nThe ADRs are markdown files inside the platform repository, located in the \"adr\" directory in the root of the repository. So new ADRs can simply be created via merge requests. The merge request is also the approval process for the ADR. Along with the ADR, all necessary code changes have to be added to the merge request, which are needed to implement the new decision. Add the \"ADR\" label to your merge request, so everyone can identify merge requests containing an ADR.\\n**How many people have to approve an ADR?**\\n*  Two additional developers have to review the ADR\\n*  One developer must be a member of the core development team\\n*  One developer must be a member of a team, other than the team of the creator\\n*  One product owner or higher role has to approve an ADR\\n**Should counter decisions also be documented?**\\nNot specific, but if there is more than one possible solution, all options should be outlined.\\n**How does an ADR look like?**\\nYou can use this first ADR as an orientation. The filename of the ADR should contain the date and a meaningful title. The content of the ADR should always use the following template:\\n```\\n# [Date] - [Title]\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nFollowing the [rollout of AWS EKS in London](https:\/\/aws.amazon.com\/about-aws\/whats-new\/2019\/02\/amazon-eks-available-in-mumbai--london--and-paris-aws-regions\/) it is now an attractive alternative to the hand-rolled kubernetes installation that was created as a result of [ADR003](ADR003-container-orchestration.md). This will bring numerous benefits:\\\\n* reducing the amount of infrastructure we manage in-house, by offloading it to AWS\\\\n* better alignment with [Technology & Operations Strategic Principle #3 - \"Use fully managed cloud services by default\"](https:\/\/reliability-engineering.cloudapps.digital\/documentation\/strategy-and-principles\/re-principles.html#3-use-fully-managed-cloud-services-by-default)\\\\nAs of 1.12, EKS supports what we need (e.g. Istio, kiam etc.).\\\\n'}, {'role': 'model', 'content': 'We will host the GDS Supported Platform on AWS EKS.\\\\n'}]"}
{"File Name":"beis-report-official-development-assistance\/0003-use-dotenv-for-managing-environment-variables.md","Context":"## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n","Decision":"Use DotEnv to load our environment variables.\\n","tokens":324,"id":2390,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch(\\'FOO\\', \\'default\\')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with(\\'BOX_ID\\').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return(\\'bar\\')`. We should then consider how we can stub environment variables.\\n\\n## Decision\\nUse DotEnv to load our environment variables.\\n\\n\\n## Context\\nDevelopers consider dealing with local (runtime) exceptions in JVM quite convenient and easy. As a consequence, exceptions are often used as a means for an alternative execution flow. One typical\\nexample is reporting structural and business validation violations.\\nIn the local JVM environment, expected execution flow exceptions are usually created with a full stack trace included. However, as these exceptions represent \"normal\" conditions, creating a full\\nstack trace is wasteful in terms of performance and resources.\\nThe full stack trace problem is additionally emphasized in a distributed JVM environment. Not only the stack trace creation is wasteful, but it will take much more bandwidth to transfer it on the\\nother side.\\nAnd finally, in a distributed JVM environment, we cannot safely assume that our custom exception classes are present in the classpath of both JVMs. Therefore, we can not assume the exception created\\non the server-side can be deserialized on the client.\\nTo remedy the situation, we can use a small number of stack-less exception classes shared between JVMs included in the communication. They belong to the boundary layer and are part of the API used\\nfor distributed communication. To read more about boundary layer, take a look near the end of \"Domain libraries\" section in\\n\"[Organizing modules and packages](..\/..\/article\/modules-and-packages\/modulesAndPackages.md#domain-libraries)\" article.\\nFor communicating various custom exceptions, we can use violation (or error) codes. The violation code defines its severity and contains several additional properties for describing violation details.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\\n## Decision\\n**For distributed error reporting, we will use stack-less exceptions for communicating custom error codes between JVMs in a distributed Axon environment.**\\nIn a distributed Axon environment, the dispatching side (let\\'s call it a client) sends commands and queries over Axon Server to the handling side (let\\'s call it a server). When the handling side\\ndetects broken business invariants, it can raise an exception to report the details to the dispatching side.\\nAxon framework already contains base infrastructure for reporting such distributed errors via so-called exception details. Exception details can be any DTO-like class shared between communicating\\nparties, including all necessary data describing error conditions.\\n### Decision details\\n#### Business exception handling\\nIn our case, as exception details DTO, we will use the `DomainException` class and its descendants. It is a stack-less exception that carries `ViolationInfo` property. `ViolationInfo` contains\\n`Severity` and `ViolationCode` properties. `ViolationCode` contains `code` and `codeMessage` in English. There are also a `resolvableMessageKey` and `resolvableMessageParameters` that support error\\ncode resolving through a resource bundle for internationalization purposes.\\nIt is worth noting that `DomainException` can be used on the dispatching side (client) too. Exception (un)wrapping on the handling side and centralized exception handling on the dispatching side are\\nhidden in infrastructural code. From the developer\\'s perspective, he works with `CommandException` or `QueryException` only (both are extended from `DomainException`). This is a nice addition as\\nbusiness invariants are handled the same way on handling and dispatching sides, without depending on classes from the Axon framework.\\nAs a usage example on the handling side, take a look at `BookingOfferAggregate` or `BookingOfferSummaryQueryHandlerService` classes. An example for the dispatching side can be found in\\n`BookingOfferFactoryService` class.\\n#### Unexpected exception handling\\nWhen handling business exceptions, we are not interested in stack traces. Since business exceptions are just a form of the alternate execution flow, there isn\\'t much benefit in logging their\\nstacktrace.\\nContrary, we want to log the stack trace when an unexpected exception (i.e., NullPointerException) occurs at the remote handler. However, communication constraints still hold, and we still have to\\nuse error codes for communicating exceptions. Further, to correlate exceptions on handling and dispatching sides, we must use an exception identifier and put it in the log messages on both sides.\\nThe logic of handling unexpected exceptions on the handling side can be seen in the `CommandHandlerExceptionInterceptor` and `QueryHandlerExceptionInterceptor` classes.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch(\\'FOO\\', \\'default\\')`, this should be managed centrally.\\\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https:\/\/github.com\/laserlemon\/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with(\\'BOX_ID\\').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return(\\'bar\\')`. We should then consider how we can stub environment variables.\\\\n'}, {'role': 'model', 'content': 'Use DotEnv to load our environment variables.\\\\n'}]"}
{"File Name":"Data-Platform-Playbook\/009-ingesting-data-from-apis.md","Context":"## Context\\nThe Data Platform currently has no capacity to ingest data from external APIs\\n","Decision":"![API ingestion](..\/images\/api-ingestion.png)\\nDeploy a Lambda that will call the the required API and save the returned data\\nobject into S3, once all the records have been downloaded the lambda will\\ntrigger an AWS glue workflow that will convert JSON objects into parquet and\\ncrawl the data enabling users to access that form a data catalogue in AWS Athena.\\n","tokens":17,"id":1015,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe output from the ETL is only available in the container and needs to be exposed to consuming applications.\\n\\n## Decision\\nWhen the ETL has completed the output will be stored in an Azure blob, Azure being the current preferred cloud platform.\\nThe output file will be exposed on a publicly available URL.\\n\\n\\n## Context\\nThe output from the ETL is only available in the container and needs to be exposed to consuming applications.\\n\\n## Decision\\nWhen the ETL has completed the output will be stored in an Azure blob, Azure being the current preferred cloud platform.\\nThe output file will be exposed on a publicly available URL.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThe Data Platform currently has no capacity to ingest data from external APIs\\\\n'}, {'role': 'model', 'content': '![API ingestion](..\/images\/api-ingestion.png)\\\\nDeploy a Lambda that will call the the required API and save the returned data\\\\nobject into S3, once all the records have been downloaded the lambda will\\\\ntrigger an AWS glue workflow that will convert JSON objects into parquet and\\\\ncrawl the data enabling users to access that form a data catalogue in AWS Athena.\\\\n'}]"}
{"File Name":"figgy\/0009-unlinked-files.md","Context":"## Context\\nWhen an ingest fails in the middle of a transaction which is adding files, the\\nFileSets will not get persisted. However, the files will have already been\\ncopied to the repository via `FileAppender`. This results in files in the\\nrepository which have no corresponding database record.\\nFixing this will require development of a transactional disk StorageAdapter\\nwhich moves files at the end of a metadata transaction.\\n","Decision":"1. We don't have time to implement a transactional disk StorageAdapter at this\\ntime.\\n2. Accept this situation, document it here, and know we can free up space in the\\nfuture by looking for unlinked files and deleting them.\\n","tokens":90,"id":4824,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n**Context**: operations that change state in the server may complete with an\\nindeterminate status code. For example: an error in the 500 range may be\\nproduced by a middle-tier server after the operation was started by the backend.\\nRe-issuing the request may not work as expected for the application developer.\\n**Decision**: by default the library will retry all requests, including\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more\\nrare cases the operation may fail, for example, an operation to create an object\\nwith `IfGenerationMatch(0)` would fail on the second attempt.\\n\\n## Decision\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more\\nrare cases the operation may fail, for example, an operation to create an object\\nwith `IfGenerationMatch(0)` would fail on the second attempt.\\n\\n\\n## Context\\nJames enables a user to register additional mailbox listeners.\\nThe distributed James server is handling mailbox event processing (mailboxListener execution) using a RabbitMQ work-queue\\nper listener.\\nThe distributed James server then declares a queue upon start for each one of these user registered listeners, that it\\nbinds to the main event exchange.\\nMore information about this component, and its distributed, RabbitMQ based implementation, can be found in\\n[ADR 0036](0037-eventbus.md).\\nIf the user unconfigures the listener, the queue and the binding are still present but not consumed. This results in\\nunbounded queue growth eventually causing RabbitMQ resource exhaustion and failure.\\n\\n## Decision\\nWe need a clear consensus and auditability across the James cluster about **required groups** (and their changes). Thus\\nEvent sourcing will maintain an aggregate tracking **required groups** (and their changes). Audit will be enabled by\\nadding host and date information upon changes. A subscriber will perform changes (binds and unbinds) in registered groups\\nfollowing the changes of the aggregate.\\nEvent sourcing is desirable as it allows:\\n- Detecting previously removed MailboxListener upon start\\n- Audit of unbind decisions\\n- Enables writing more complex business rules in the future\\nThe event sourcing system will have the following command:\\n- **RequireGroups** the groups that the **EventBus** is starting with.\\nAnd the following events:\\n- **RequiredGroupAdded** a group is added to the required groups.\\n- **RequiredGroupRemoved** a group is removed from the required groups.\\nUpon start the aggregate will be updated if needed and bindings will be adapted accordingly.\\nNote that upon failure, registered groups will diverge from required groups. We will add a health check to diagnose\\nsuch issues. Eventually, we will expose a webadmin task to reset registered groups to required groups.\\nThe queues should not be deleted to prevent message loss.\\nGiven a James topology with a non uniform configuration, the effective RabbitMQ routing will be the one of the latest\\nstarted James server.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWhen an ingest fails in the middle of a transaction which is adding files, the\\\\nFileSets will not get persisted. However, the files will have already been\\\\ncopied to the repository via `FileAppender`. This results in files in the\\\\nrepository which have no corresponding database record.\\\\nFixing this will require development of a transactional disk StorageAdapter\\\\nwhich moves files at the end of a metadata transaction.\\\\n'}, {'role': 'model', 'content': \"1. We don't have time to implement a transactional disk StorageAdapter at this\\\\ntime.\\\\n2. Accept this situation, document it here, and know we can free up space in the\\\\nfuture by looking for unlinked files and deleting them.\\\\n\"}]"}
{"File Name":"terraform-aws-iam-module\/0003-dual-support-for-terraform-version-0-11-and-0-12.md","Context":"## Context\\nTerraform version 0.12 release was a major change with the API. Given the worked required to upgrade, it is envisaged that Terraform 0.11 will remain for quite some time.\\n","Decision":"This module will support both version 0.11 and 0.12 of Terraform. Version 0.11 support will be managed from the 0.11 branch and tagged with a version pattern 0.minor.patch. Version 0.12 support will be managed from the master branch and tagged with a version pattern 1.minor.patch.\\n","tokens":46,"id":1628,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nTerraform version 0.12 release was a major change with the API. Given the worked required to upgrade, it is envisaged that Terraform 0.11 will remain for quite some time.\\n\\n## Decision\\nThis module will support both version 0.11 and 0.12 of Terraform. Version 0.11 support will be managed from the 0.11 branch and tagged with a version pattern 0.minor.patch. Version 0.12 support will be managed from the master branch and tagged with a version pattern 1.minor.patch.\\n\\n\\n## Context\\nAs part of on-going development, it has been noticed (thanks Norris) that the\\ntranpilation is not what was expected and we should use a later version of\\nECMA script to target.\\n\\n## Decision\\nFrom [here](https:\/\/kangax.github.io\/compat-table\/es2016plus\/#node12_11)\\nThe suggestion is to use node v12 and target \"ES2019\" in your `tsconfig.json`.\\nThe minor node version will be updated to match the minor version of the Alpine docker image for node (currently 12.15)\\n### Discussion\\nDone.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nTerraform version 0.12 release was a major change with the API. Given the worked required to upgrade, it is envisaged that Terraform 0.11 will remain for quite some time.\\\\n'}, {'role': 'model', 'content': 'This module will support both version 0.11 and 0.12 of Terraform. Version 0.11 support will be managed from the 0.11 branch and tagged with a version pattern 0.minor.patch. Version 0.12 support will be managed from the master branch and tagged with a version pattern 1.minor.patch.\\\\n'}]"}
{"File Name":"opg-metrics\/0004-use-grafana-for-our-data-visualisation.md","Context":"## Context\\nWe need a way to visualise our time series data for users to view and analyse data.\\nWe need the ability for authenticating users with levels of permissions for creation and viewing. A way to organise dashboards by project or area of interest should also be available from the solution to help navigate to information.\\nIt is not to be used for debugging, we have other solutions that provide and handle this sort of functionality. This should be for analysing metrics across a spectrum of sources and be able to overlay key points on top of each other.\\n","Decision":"We have chosen Grafana for its ease of use, popularity, plugin support and Open Source status.\\n","tokens":114,"id":1543,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nThe newest version of the Marketplace supports various sources for it's data.\\nWe need to agreggate the blockchain, multiple graphs and data from our partners, which themselves are represented as API requests to different servers.\\nWe don't only need to show the data to the user, but we should also provide different ways to interact with it, mainly buying and selling assets. We should also allow the user to filter the data in different ways.\\nTo do this, assets from different sources have their own version of what it means to filter them, and we might also need to interact with different contracts, depending on the operation.\\n\\n## Decision\\nWe choose option 2 because it gives us many advantages over centralizing the logic in the front-end, like:\\n- having a server other parts of Decentraland can access\\n- making the front-end lighter and simpler, to be able to deliver the webpage faster to the user\\n- providing caching for problematic endpoints if necessary\\nbut allowing us to keep parts that are Marketplace-centric in the front end, like the interaction with partners and with contracts.\\n\\n\\n## Context\\nThe _ordering system_ is a dynamically developing system that starts as a modularized monolith. Even in the initial form, it can be split up into several services that will have to be monitored. The system starts small, to save budget for cloud instances, but has the potential to grow towards many services. Based on technical and business metrics there should be a clear way to make a decision about extracting the monolith's modules into services with smaller responsibility scopes and improved scalability.\\nThere should be a set of clear metrics and vision about health statuses of services. It includes not only consumption of hardware resources, but also information about processing user's requests.\\n\\n## Decision\\nThere is a dedicated end-point per service that will provide information about its health status.\\nFor technical health checks we are using a heartbeat pattern for fault detection. It allows us to add services when necessary and the supervisor will passively listen for incoming messages, instead of pushing requests (ping pattern).\\nFor business health checks there are sets of critical path scenarios. Each scenario starts with a dummy message from a dedicated supervisor to measure execution of specific user scenarios. Measurements will includes correctness of execution and time spent (duration).\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need a way to visualise our time series data for users to view and analyse data.\\\\nWe need the ability for authenticating users with levels of permissions for creation and viewing. A way to organise dashboards by project or area of interest should also be available from the solution to help navigate to information.\\\\nIt is not to be used for debugging, we have other solutions that provide and handle this sort of functionality. This should be for analysing metrics across a spectrum of sources and be able to overlay key points on top of each other.\\\\n\"}, {'role': 'model', 'content': 'We have chosen Grafana for its ease of use, popularity, plugin support and Open Source status.\\\\n'}]"}
{"File Name":"scientific-thesis-template\/0004-use-lualatex-for-correct-ligatures.md","Context":"## Context and Problem Statement\\nFor high-quality documents, the [ligatures](https:\/\/en.wikipedia.org\/wiki\/Typographic_ligature) have to be right.\\nSee [english.stackexchange.com](https:\/\/english.stackexchange.com\/q\/50660\/66058) for a long discusisson.\\nSee a German rant on wrong ligatures: <https:\/\/web.archive.org\/web\/20150425155310\/http:\/\/www.mengensatz.de\/blog\/?p=79>.\\nFor instance, in the English word \"offline\", the letters \"f\" and \"l\" must not be joined.\\nIn the German word \"Auflage\", the letters \"f\" and \"l\" must not be joined.\\nSee also the last lines in <https:\/\/tex.stackexchange.com\/a\/64457\/9075>.\\n### More readings\\n* <https:\/\/www.typolexikon.de\/ligatur\/>\\n* Questions on ligatures: <https:\/\/tex.stackexchange.com\/questions\/tagged\/ligatures>\\n* Disable ligatures in headings: <https:\/\/tex.stackexchange.com\/q\/198743\/9075>\\n","Decision":"Chosen option: \"lualatex\", because comes out best (see below).\\n","tokens":237,"id":251,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nA common pitfall in choice of data storage for currency for money is to use floating point which then results in rounding errors due to the inexact representation of base-10 (decimal) fractional numbers in base-2 (binary) floating point storage that can result in a loss of faith in a calculations system.\\nOptions available:\\n1. [BigDecimal](https:\/\/ruby-doc.org\/stdlib-3.0.0\/libdoc\/bigdecimal\/rdoc\/BigDecimal.html) as pounds.pence\\n2. [BigDecimal](https:\/\/ruby-doc.org\/stdlib-3.0.0\/libdoc\/bigdecimal\/rdoc\/BigDecimal.html) as pence\\n3. The [RubyMoney\/Money gem](https:\/\/github.com\/RubyMoney\/money)\\n4. [float](https:\/\/ruby-doc.org\/core-3.0.0\/Float.html) (default for numbers in ruby)\\nI\\'m glad to say rounding errors here are [unlikely to be fatal as has been seen before](https:\/\/www.gao.gov\/products\/imtec-92-26) but we\\'d still like to avoid them.\\n### Research\\n* <https:\/\/www.honeybadger.io\/blog\/ruby-currency\/>\\n\\n## Decision\\n### 1. BigDecimal as pounds.pence - yes\\n* Provides accurate rounding compared to float, producing the results that accountants would expect.\\n* Extra dependency, albeit commonly used.\\n* No mismatch between normal written representation of the value and the stored value (compared to storing as pence).\\nBigDecimal provides [`BigDecimal.round`](https:\/\/ruby-doc.org\/stdlib-3.0.0\/libdoc\/bigdecimal\/rdoc\/BigDecimal.html#method-i-round) that we can use to round up to pennies as needed.\\n### 2. BigDecimal as pence - no\\nSome developers like to store currency in pennies.\\nI\\'ve not seen a compelling reason to do this. I personally think it adds extra risk of confusion compared to using the decimal type in a way that matches the currency (i.e. pounds as the integer part of a decimal and pennies as the fractional part).\\n* <https:\/\/stackoverflow.com\/questions\/18934774\/what-are-the-downsides-to-storing-money-values-as-cents-minor-units>\\n### 3. RubyMoney - no (for now)\\nThe [RubyMoney\/Money gem](https:\/\/github.com\/RubyMoney\/money):\\nThis type provides extra information about the currency in use (e.g. GBP, USD) which is of no use to us as this is an entirely GBP system.\\nUsing a money type would explicitly shows that the numbers stored are financial which would be nice but isn\\'t critical to understanding.\\nIt also provides formatting and truncation methods. We don\\'t yet know if these will prove useful.\\nIt is judged that the tradeoff for introducing an additional relatively complex dependency for the possible benefits is not currently worthwhile.\\n### 4. Floats - no\\nDo not use for money. Ever.\\nStoring currency in floats causes rounding errors so that\\'s out.\\n\\n\\n## Context\\nWe use [Bosh](https:\/\/bosh.io\/) to create and manage our cloudfoundry deployment on AWS.\\nTo deploy software, Bosh needs certain binary dependencies available.\\nThese are known as bosh [releases](https:\/\/bosh.io\/docs\/release.html).\\nBefore this decision, we usually built and uploaded releases to Bosh as part of our [concourse](https:\/\/concourse-ci.org\/) pipeline.\\nOccasionally, we would manually build a release, store it on GitHub, and point Bosh to it there.\\n### Building Bosh Releases\\nWe investigated different approaches to creating bosh releases, in particular\\n* Multiple pipelines created dynamically using [branch manager](https:\/\/github.com\/alphagov\/paas-concourse-branch-manager)\\n* A single pipeline using [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource)\\nThe work on these spikes was recorded in\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/115142265\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/128937731\\n\\n## Decision\\nWe will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\nFor high-quality documents, the [ligatures](https:\/\/en.wikipedia.org\/wiki\/Typographic_ligature) have to be right.\\\\nSee [english.stackexchange.com](https:\/\/english.stackexchange.com\/q\/50660\/66058) for a long discusisson.\\\\nSee a German rant on wrong ligatures: <https:\/\/web.archive.org\/web\/20150425155310\/http:\/\/www.mengensatz.de\/blog\/?p=79>.\\\\nFor instance, in the English word \"offline\", the letters \"f\" and \"l\" must not be joined.\\\\nIn the German word \"Auflage\", the letters \"f\" and \"l\" must not be joined.\\\\nSee also the last lines in <https:\/\/tex.stackexchange.com\/a\/64457\/9075>.\\\\n### More readings\\\\n* <https:\/\/www.typolexikon.de\/ligatur\/>\\\\n* Questions on ligatures: <https:\/\/tex.stackexchange.com\/questions\/tagged\/ligatures>\\\\n* Disable ligatures in headings: <https:\/\/tex.stackexchange.com\/q\/198743\/9075>\\\\n'}, {'role': 'model', 'content': 'Chosen option: \"lualatex\", because comes out best (see below).\\\\n'}]"}
{"File Name":"dalmatian-frontend\/0007-use-coveralls-for-monitoring-test-coverage.md","Context":"## Context\\nWe want to keep our test coverage as high as possible without having to run manual checks as these take time.\\n","Decision":"Use the free tier of Coveralls to give us statistics and to give our pull requests feedback.\\n","tokens":26,"id":2556,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe have started to miss test coverage for a few methods. We noticed this problem where we had delivered a feature with a feature test but had forgotten to add enough unit coverage. When the feature test later changed coverage was lost.\\nWe want to keep our test coverage as high as possible without having to run manual checks as these take time.\\n\\n## Decision\\nUse the free tier of Coveralls to give us statistics and to give our pull requests feedback.\\n\\n\\n## Context\\nWe shouldn't overwhelm the Notifier.  This would happen then we have many orders, surveys and recommendations to send to customers.\\n\\n## Decision\\nThe decision is to introduce a queue for all these messages.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe want to keep our test coverage as high as possible without having to run manual checks as these take time.\\\\n\"}, {'role': 'model', 'content': 'Use the free tier of Coveralls to give us statistics and to give our pull requests feedback.\\\\n'}]"}
{"File Name":"timdex\/0002-use-jwt-for-api-authorization.md","Context":"## Context\\nThe API portion of this application will require authentication.\\nJSON Web Token (JWT) is an open standard described by [RFC 7519]( https:\/\/tools.ietf.org\/html\/rfc7519).\\n[Additional Information](https:\/\/en.wikipedia.org\/wiki\/JSON_Web_Token).\\n","Decision":"We will use JWT for authentication.\\n","tokens":62,"id":2375,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nAfter adopting the last specifications of JMAP (see\\n[new JMAP specifications adoption ADR](https:\/\/github.com\/apache\/james-project\/blob\/master\/src\/adr\/0018-jmap-new-specs.md)),\\nit was agreed that we need to be able to serve both `jmap-draft` and the new `jmap` with a reactive server.\\nThe current outdated implementation of JMAP in James is currently using a non-reactive [Jetty server](https:\/\/www.eclipse.org\/jetty\/).\\nThere are many possible candidates as reactive servers. Among the most popular ones for Java:\\n* [Spring](https:\/\/spring.io)\\n* [Reactor-netty](https:\/\/github.com\/reactor\/reactor-netty)\\n* [Akka HTTP](https:\/\/doc.akka.io\/docs\/akka-http\/current\/introduction.html)\\n* ...\\n\\n## Decision\\nWe decide to use `reactor-netty` for the following reasons:\\n* It's a reactive server\\n* It's using [Reactor](https:\/\/projectreactor.io\/), which is the same technology that we use in the rest of our codebase\\n* Implementing JMAP does not require high level HTTP server features\\n\\n\\n## Context\\nAfter adopting the last specifications of JMAP (see\\n[new JMAP specifications adoption ADR](https:\/\/github.com\/apache\/james-project\/blob\/master\/src\/adr\/0018-jmap-new-specs.md)),\\nit was agreed that we need to be able to serve both `jmap-draft` and the new `jmap` with a reactive server.\\nThe current outdated implementation of JMAP in James is currently using a non-reactive [Jetty server](https:\/\/www.eclipse.org\/jetty\/).\\nThere are many possible candidates as reactive servers. Among the most popular ones for Java:\\n* [Spring](https:\/\/spring.io)\\n* [Reactor-netty](https:\/\/github.com\/reactor\/reactor-netty)\\n* [Akka HTTP](https:\/\/doc.akka.io\/docs\/akka-http\/current\/introduction.html)\\n* ...\\n\\n## Decision\\nWe decide to use `reactor-netty` for the following reasons:\\n* It's a reactive server\\n* It's using [Reactor](https:\/\/projectreactor.io\/), which is the same technology that we use in the rest of our codebase\\n* Implementing JMAP does not require high level HTTP server features\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThe API portion of this application will require authentication.\\\\nJSON Web Token (JWT) is an open standard described by [RFC 7519]( https:\/\/tools.ietf.org\/html\/rfc7519).\\\\n[Additional Information](https:\/\/en.wikipedia.org\/wiki\/JSON_Web_Token).\\\\n\"}, {'role': 'model', 'content': 'We will use JWT for authentication.\\\\n'}]"}
{"File Name":"dotcom-rendering\/019-remove-monorepo.md","Context":"## Context\\nWe used to have a monorepo with multiple packages, including `guui` and `design`. However, once these packages were migrated to `src-foundation` we were left with a monorepo with only one package.\\n","Decision":"Remove the use of Yarn Workspaces which was being used for the monorepo. Restore a basic yarn package, merging dependencies.\\n","tokens":51,"id":2666,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n### Context\\nWe want to use Lrud as a `tap-static` module, which requires converting to `amd` format. To do so we use `esm-2-amd`, which means we need to distribute lrud as ESM. Currently it\\'s not possible as it\\'s only built to CJS.\\n### Decision\\nThe distribution folder structure will change with two subfolders, `cjs` and `esm`. The type definitions will also be distributed in another subfolder, `types`. Rollup will still be used to create the CJS and ESM format.\\n### Status\\nApproved\\n### Consequences\\nThe final distribution size will increase.\\nThis change won\\'t affect how current users consume lrud, as the package `main` is updated to point to the CJS min file.\\n\\n## Decision\\nThe distribution folder structure will change with two subfolders, `cjs` and `esm`. The type definitions will also be distributed in another subfolder, `types`. Rollup will still be used to create the CJS and ESM format.\\n### Status\\nApproved\\n### Consequences\\nThe final distribution size will increase.\\nThis change won\\'t affect how current users consume lrud, as the package `main` is updated to point to the CJS min file.\\n\\n\\n## Context\\nIn `cluster.py` we used to install all the `kube-system` components using a `systemd` unit. This consisted basically in a bash script that deployed all the manifests from `\/srv\/kubernetes\/manifests\/*\/*.yaml` using `kubectl`.\\nWe obviously do not want to update versions manually via kubectl. Furthermore, this approach also meant that we had to launch a new master instance in order to apply the updated manifests.\\n\\n## Decision\\nWe will do the following:\\n- remove entirely the \"install-kube-system\" unit from the master user data.\\n- create a folder with all the manifests for each of the kubernetes artifact\\n- apply all the manifests from the Cluster Lifecycle Manager code\\nSome of the possible alternatives for the folder structures are:\\n1. \/manifests\/APPLICATION_NAME\/deployment.yaml - which uses a folder structure that includes the APPLICATION_NAME\\n2. \/manifests\/APPLICATION_NAME\/KIND\/mate.yaml - which uses a folder structure that includes APPLICATION_NAME and KIND\\n3. \/manifests\/mate-deployment.yaml - where we have a flat structure and the filenames contain the name of the application and the kind\\n4. \/manifests\/mate.yaml - where mate.yaml contains all the artifacts of all kinds related to mate\\nWe choose number 1 as it seems the most compelling alternative.\\nNumber 2 will only introduce an additional folder level that does not provide any benefit. Number 3 will instead rely on a naming convention on the given kind.\\nNumber 4, instead, is a competitive alternative to number 1 and could be adopted, but we prefer to go with number 1 as this is very flexible and probably more readable for the maintainer.\\nFor the file naming convention, we recommend to split in files for kind when is possible and put the name (or just a prefix) in the file name. We will not make any assumption on the file naming scheme in the code.\\nAlso, no assumption will be made on the order of execution of such files.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe used to have a monorepo with multiple packages, including `guui` and `design`. However, once these packages were migrated to `src-foundation` we were left with a monorepo with only one package.\\\\n'}, {'role': 'model', 'content': 'Remove the use of Yarn Workspaces which was being used for the monorepo. Restore a basic yarn package, merging dependencies.\\\\n'}]"}
{"File Name":"biosamples-v4\/0006-submitting-the-same-sample-twice.md","Context":"## Context\\nWhat should we do if a user submitted same sample twice ?\\n1. Update the first one by checking name and domain fields\\n2. Create another sample with an accession without considering the first one\\n3. Send an error message explaining that a sample exist with same name and domain\\n","Decision":"For now we decided to create a new sample with a new accession as this updating a sample could cause accidental data loss.\\n","tokens":61,"id":4725,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nSample should have a release and update date when we store it in database.\\nWhen user submit a sample we could give flexibility to the user by filling release and update date to today if they are missing.\\nBut filling release date can accidentally make a sample public.\\n\\n## Decision\\nWe decided that it is best to send an error message if sample release date is not provided.\\nOn the other hand, we will fill update date to today\\'s date if it is missing.\\n\\n\\n## Context\\n`wbeditentity` summary message for Wikibase Items and Properties provides\\nvery little information on what have been changed on the entity, with the main\\nmessage in the summary being \"Item Changed\".\\nIn [T220696], we want to update those summary\\nmessages generated for Wikibase Items and Properties when using `wbeditentity`\\nto be more detailed and informative.\\nWikibase Lexeme achieved generating detailed summary messages by:\\n1. making its  implementations update the summary object passed to\\nthe change op instance with granular information about the change.\\n2. implementing [SummaryAggregator] class and using it internally in some\\n[ChangeOp] implementations (some of those that are non-leaf nodes and\\ncontain further change ops) in order to combine those summary messages\\ngenerated by leaf nodes (the effective change ops in the tree).\\n\\n## Decision\\nWe decided to go with the second option, the visitor pattern design.\\nIn order to achieve that, we will make [ChangeOp::apply()] return\\n[ChangeOpResult] that is defined by the following IDL:\\n```php\\n\/\/ encapsulates the result of applying a change op to an entity document\\ninterface ChangeOpResult {\\n\/\/ the id of the entity document that the change op was applied to\\nEntityId getEntityId();\\n\/\/ whether the entity document was actually changed in any way\\n\/\/ as a result of applying the change op to it\\nbool isEntityChanged();\\n}\\n```\\n### Next steps\\n1. Change [ChangeOp::apply()] to return [ChangeOpResult]\\n2. Provide needed implementations of [ChangeOpResult] that can capture the result\\nof applying the different [ChangeOp] implementations.\\nExample:\\n[ChangeOpLabel] will probably return something like [ChangeOpLabelResult] that\\nis defined as:\\n```php\\ninterface ChangeOpLabelResult : ChangeOpResult {\\nstring getOldLabel();\\nstring getNewLabel();\\nstring getLanguageCode();\\n}\\n```\\n3. Update implementations of [ChangeOp] in Wikibase and [WikibaseLexeme] to conform\\nwith the changes to the interface.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWhat should we do if a user submitted same sample twice ?\\\\n1. Update the first one by checking name and domain fields\\\\n2. Create another sample with an accession without considering the first one\\\\n3. Send an error message explaining that a sample exist with same name and domain\\\\n'}, {'role': 'model', 'content': 'For now we decided to create a new sample with a new accession as this updating a sample could cause accidental data loss.\\\\n'}]"}
{"File Name":"scholarsphere\/0005-order-files-alphanumerically.md","Context":"## Context\\nFiles within a work should be displayed in a certain order. The ordering can be automatic or arbitrary.\\n","Decision":"Files will be ordered alphanumerically, according to their names. The application can now render them in the same order\\neverytime, without additional metadata.\\n","tokens":24,"id":4558,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nMetadata for works and collections should be publicly viewable. Only the binary content, i.e. the files, within the\\nwork needs to be restricted based on visibility or other permissions.\\n\\n## Decision\\nGrant discovery access to the public on all works and collections by creating the appropriate ACL for each work and\\ncollection. Discovery access stipulates that all metadata is viewable, but that binary content is not downloadable.\\n\\n\\n## Context\\nThe developer should be able to define, if an association has to be considered or skipped during the cloning of an entity.\\n\\n## Decision\\nThe current clone behavior is controlled by the `Shopware\\\\Core\\\\Framework\\\\DataAbstractionLayer\\\\Field\\\\Flag\\\\CascadeDelete` flag.\\nAll associations which are marked with this flag are considered in the clone process.\\nWe will add an optional parameter to the flag constructor to disable this behavior.\\nI already added this flag to the following associations:\\n* `product.productReviews`\\n* This association is already overwritten by the administration\\n* `product.searchKeywords`\\n* Will be indexed by the product indexer, so we can skip this association in the clone process\\n* `product.categoriesRo`\\n* Will be indexed by the product indexer, so we can skip this association in the clone process\\nAn example looks like the following:\\n```\\n(new OneToManyAssociationField('searchKeywords', ProductSearchKeywordDefinition::class, 'product_id'))\\n->addFlags(new CascadeDelete(false)),\\n```\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nFiles within a work should be displayed in a certain order. The ordering can be automatic or arbitrary.\\\\n\"}, {'role': 'model', 'content': 'Files will be ordered alphanumerically, according to their names. The application can now render them in the same order\\\\neverytime, without additional metadata.\\\\n'}]"}
{"File Name":"link_platform\/0012-use-jest-and-enzyme-for-unit-testing.md","Context":"## Context\\nJest is a general JavaScript testing framework. Enzyme is a testing utility that makes it easier to assert, manipulate, and traverse React components. [This](https:\/\/medium.com\/welldone-software\/an-overview-of-javascript-testing-in-2018-f68950900bc3) article goes into detail about various testing alternatives and [this](https:\/\/www.codementor.io\/vijayst\/unit-testing-react-components-jest-or-enzyme-du1087lh8) article details using Jest with Enzyme.\\n","Decision":"Based on the familiarity of our engineers with Jest and Enzyme and the large community of support, use Jest and Enzyme to unit test our React application.\\n","tokens":112,"id":5020,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nAs mentioned in [ADR #3. Use relatively modern tech stack](0003-use-relatively-modern-tech-stack.md) this project is intended to be a [breakable toy](https:\/\/www.oreilly.com\/library\/view\/apprenticeship-patterns\/9780596806842\/ch05.html#breakable_toys) where I can experiment with other tech.\\nThis project was first implemented in JavaScript. We want to use a static typing system so that we can catch more errors at compile time and to document the interfaces between different functions, components and so on.\\n\\n## Decision\\nThis project will be migrated over to TypeScript.\\n\\n\\n## Context\\nWe decided to augment the existing [RPC-based Tendermint light client](https:\/\/github.com\/tendermint\/tendermint\/blob\/bc643b19c48495077e0394d3e21e1d2a52c99548\/light\/doc.go#L2-L126) by adding the possibility to additionally validate blocks by doing Data Availability Sampling (DAS).\\nIn general, DAS gives light clients assurance that the data behind the block header they validated is actually available in the network and hence, that state fraud proofs could be generated.\\nSee [ADR 002](adr-002-ipld-da-sampling.md) for more context on DAS.\\nA great introduction on the Tendermint light client (and light clients in general) can be found in this series of [blog posts](https:\/\/medium.com\/tendermint\/everything-you-need-to-know-about-the-tendermint-light-client-f80d03856f98) as well as this [paper](https:\/\/arxiv.org\/abs\/2010.07031).\\nThis ADR describes the changes necessary to augment the existing Tendermint light client implementation with DAS from a UX as well as from a protocol perspective.\\n\\n## Decision\\nFor our MVP, we [decide](https:\/\/github.com\/celestiaorg\/celestia-core\/issues\/307) to only modify the existing RPC-endpoint based light client.\\nThis is mostly because we want to ship our MVP as quickly as possible but independently of this it makes sense to provide a familiar experience for engineers coming from the Cosmos ecosystem.\\nWe will later implement the above mentioned variants.\\nHow exactly will be described in separate ADRs though.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nJest is a general JavaScript testing framework. Enzyme is a testing utility that makes it easier to assert, manipulate, and traverse React components. [This](https:\/\/medium.com\/welldone-software\/an-overview-of-javascript-testing-in-2018-f68950900bc3) article goes into detail about various testing alternatives and [this](https:\/\/www.codementor.io\/vijayst\/unit-testing-react-components-jest-or-enzyme-du1087lh8) article details using Jest with Enzyme.\\\\n'}, {'role': 'model', 'content': 'Based on the familiarity of our engineers with Jest and Enzyme and the large community of support, use Jest and Enzyme to unit test our React application.\\\\n'}]"}
{"File Name":"platform\/2020-07-02-Implement-sales-channel-context-token-requirement.md","Context":"## Context\\nSome routes for the sales-channel-api and the store-api depend on a sales-channel-context-token to identify the correct context.\\nTo ensure these routes cannot be called accidentally or intentionally without a token, a route parameter is in need to distinguish open routes and those that need a token.\\n","Decision":"Every route that depends on a sales-channel-token will only be callable with such a token provided.\\nTo decide whether a route depends on a token or not the following questions should help:\\n* Will the automatic generation of the token be a security Issue?\\n* Will the automatic generation of the token lead to an abandoned entity? (e.g. the cart)\\n* Can every possible caller create or know the needed token beforehand? (e.g. the asynchronous payment provider cannot)\\n","tokens":60,"id":4505,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nAs well as the SAML AuthnRequest something needs to render an HTML form to handle\\nthe \"SAML Post Binding\". This form contains the SAML request, the relay state, and\\noptionally any \"hints\" the relying party needs to send the hub.\\nThis form could either be generated by the verify-service-provider, the client library\\n(e.g. passport-verify), or the service itself.\\n\\n## Decision\\nThe HTML form will be generated by the client library.\\n\\n\\n## Context\\nThe requested functionality was to synchronize a specified route (either add or replace) from the main route table to the custom route tables, triggered from a log event.\\nAccess to the API endpoint should be restricted since it can modify, custom route tables, but needs to be very simplistic and either accepts or rejects the call on the first request given the nature of the trigger.\\nOptimally, the Lambda proxy function should not be called if the user does not present the proper information to minimize costs.\\n\\n## Decision\\nAPI Gateway API keys will be used to limit access given the constraints.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nSome routes for the sales-channel-api and the store-api depend on a sales-channel-context-token to identify the correct context.\\\\nTo ensure these routes cannot be called accidentally or intentionally without a token, a route parameter is in need to distinguish open routes and those that need a token.\\\\n'}, {'role': 'model', 'content': 'Every route that depends on a sales-channel-token will only be callable with such a token provided.\\\\nTo decide whether a route depends on a token or not the following questions should help:\\\\n* Will the automatic generation of the token be a security Issue?\\\\n* Will the automatic generation of the token lead to an abandoned entity? (e.g. the cart)\\\\n* Can every possible caller create or know the needed token beforehand? (e.g. the asynchronous payment provider cannot)\\\\n'}]"}
{"File Name":"gsp\/ADR040-cluster-stability-node-replacement.md","Context":"## Context\\nSREs sometimes need to change various things about a cluster's worker nodes, and therefore replace all of the nodes:\\n* AMI (e.g. for an EKS update)\\n* instance type\\n* anything else in the launch template in some way (e.g. instance role)\\nIn addition to this, in our Terraform we get the latest AMI ID for our EKS version from AWS - this means that as soon as AWS releases a new AMI for our current EKS version, the next time the cluster's deployer step runs for Terraform it will also replace all the nodes.\\nWe have a CloudFormation UpdatePolicy on our worker node auto-scaling-groups that tells it to replace 1 node at a time in each ASG. This would be fine if we didn't have 3 ASGs with a small number of nodes in each - right now this practice terminates too many instances at once and results in the cluster becoming unstable. This in turn causes outages in the applications running in the cluster.\\nWithout such a policy we'd be able to update the launch template for the ASG and it wouldn't remove any existing nodes, just set up any new future nodes correctly.\\nThe reason we have 3 small ASGs is so the cluster auto-scaler can scale nodes independently in each AZ - it will know that a pod attempting (and failing) to schedule is tied to a persistent volume claim in a particular availability zone and can scale up the ASG for that zone.\\n","Decision":"We will do action 3 (lifecycle hooks) (and therefore not action 2 (some other node rolling process)).  This will mean that the ASG will know how to wait for nodes to be ready in the cluster when launching, and how to wait for nodes to be drained before terminating.  It also means we can choose our method of node rolling with relative freedom.  We will keep our existing CloudFormation UpdatePolicy as our actual mechanism of rolling nodes.\\nWe will do action 4 (turn off autoscaler while rolling nodes).  This will probably need to be a script run before we run terraform (to scale the autoscaler down) and afterwards (to scale the autoscaler back up).\\n","tokens":311,"id":3906,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nThis design proposal is focused on how best to handle Elasticsearch process restart inside a pod, to catch up with configuration changes.\\nThe primary use case for this is switching for one license type to another with network configuration change. For example, switching from a cluster with a Basic license (no TLS) to a cluster with a Gold license (TLS) requires a full cluster restart. It\\'s not possible to \"just\" replace the pods with new ones here: we would need data to be migrated between pods that cannot communicate with each others. Reusing PersistentVolumes could be a solution here, but may not play well with local persistent volumes bound to a particular node: what if the node gets scheduled to another pod in-between the migration?\\nReusing pods may also be useful in other situations, where simply restarting Elasticsearch process can be way faster and more efficient than replacing the pod:\\n- minor configuration changes\\n- plugin changes\\n## Decision Drivers\\n* It must be possible to perform a \"full cluster restart\" and reuse existing pods.\\n* Configuration changes should be propagated through a volume mount, not through the pod spec.\\n* Operator client cache inconsistencies must be taken into consideration (for ex. don\\'t restart a pod 3 times because of a stale cache).\\n* Volume propagation time must be taken into consideration (it can take more than 1 minute, the ES restart must not happen before the new configuration is there).\\n* It should be resilient to operator restart.\\n\\n## Decision\\n* It must be possible to perform a \"full cluster restart\" and reuse existing pods.\\n* Configuration changes should be propagated through a volume mount, not through the pod spec.\\n* Operator client cache inconsistencies must be taken into consideration (for ex. don\\'t restart a pod 3 times because of a stale cache).\\n* Volume propagation time must be taken into consideration (it can take more than 1 minute, the ES restart must not happen before the new configuration is there).\\n* It should be resilient to operator restart.\\nChosen option: option 1, because that\\'s the only one we have here? :)\\n### Positive Consequences\\n* Handles pod and cluster restart, rolling or coordinated\\n* Allows humans to trigger a restart through annotations\\n* Safe from cache inconsistencies, operator restart, reconciliation retries, volume propagation\\n### Negative Consequences\\n* Needs to be implemented!\\n* Additional complexity.\\n* Need to be extra careful about chaining steps in the right order, and make them idempotent.\\n* Once a restart is scheduled, it will go through completion. If the user modifies settings again, we\\'ll wait for the current restart to be done.\\n\\n\\n## Context\\nIdempotency guarantees for an action are normally provided for a **limited period of time**. That creates an opportunity for a service to periodically do a **cleanup** of stored actions and as a result avoid ever growing storage requirements and performance implications.\\nWe\u2019ve considered few approaches to this problem:\\n- partition\\n- periodical purge\\nPerformance wise introducing partitions in idempotent_action table seemed like the best idea. Performance benefits outweighed any complications of facilitating and dropping partitions (i.e. there is no way to automatically create partition in Postgres). The issue comes with an absence of global index across partitions in some databases (i.e. Postgres) that creates a limitation that any idempotent action key has to **uniquely map into a partition**. Since the most natural choice of a partition key is a date, it would mean each `action_id` would need to come along with a date or have a date bit embedded, i.e. **UUIDv2**. So far we\u2019ve tried to avoid imposing any limitations on a key format, hence we put the idea of partitioning on hold.\\nPeriodical purge seemed like the only other feasible approach for a database that is lacking time-to-live (TTL) indexes.\\nIt comes with its own challenges like multi-instance set up and DB performance.\\nClients have to be extremely careful with batch size of purging to avoid any negative effect on database performance.\\nTo avoid any need for co-ordination of purge job execution in a multi instance setup or a dependency on Zookeeper\/etcd for a leader election we opted for a lightweight database scheduler `db-scheduler` that guarantees single instance execution.\\n\\n## Decision\\nAllow clients to configure retention policy for idempotent actions.\\nClients can specify:\\n- retention period (ISO-8601 format)\\n- cron schedule for purge job (spring format)\\n- batch-size for removal operation\\n```yaml\\nidempotence4j:\\nretention:\\nenabled: true\\nperiod: P0Y0M20D\\npurge:\\nschedule: \"*\/15 * * * * ?\"\\nbatchSize: 150\\n```\\nPurge job will only be executed on a single node in a multi instance set up.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nSREs sometimes need to change various things about a cluster\\'s worker nodes, and therefore replace all of the nodes:\\\\n* AMI (e.g. for an EKS update)\\\\n* instance type\\\\n* anything else in the launch template in some way (e.g. instance role)\\\\nIn addition to this, in our Terraform we get the latest AMI ID for our EKS version from AWS - this means that as soon as AWS releases a new AMI for our current EKS version, the next time the cluster\\'s deployer step runs for Terraform it will also replace all the nodes.\\\\nWe have a CloudFormation UpdatePolicy on our worker node auto-scaling-groups that tells it to replace 1 node at a time in each ASG. This would be fine if we didn\\'t have 3 ASGs with a small number of nodes in each - right now this practice terminates too many instances at once and results in the cluster becoming unstable. This in turn causes outages in the applications running in the cluster.\\\\nWithout such a policy we\\'d be able to update the launch template for the ASG and it wouldn\\'t remove any existing nodes, just set up any new future nodes correctly.\\\\nThe reason we have 3 small ASGs is so the cluster auto-scaler can scale nodes independently in each AZ - it will know that a pod attempting (and failing) to schedule is tied to a persistent volume claim in a particular availability zone and can scale up the ASG for that zone.\\\\n'}, {'role': 'model', 'content': 'We will do action 3 (lifecycle hooks) (and therefore not action 2 (some other node rolling process)).  This will mean that the ASG will know how to wait for nodes to be ready in the cluster when launching, and how to wait for nodes to be drained before terminating.  It also means we can choose our method of node rolling with relative freedom.  We will keep our existing CloudFormation UpdatePolicy as our actual mechanism of rolling nodes.\\\\nWe will do action 4 (turn off autoscaler while rolling nodes).  This will probably need to be a script run before we run terraform (to scale the autoscaler down) and afterwards (to scale the autoscaler back up).\\\\n'}]"}
{"File Name":"my-notes\/stylelint.md","Context":"### Context\\n- linting for CSS\\n- After running it, it didn't pick up a single error\\n### Decision\\nDon't start using stylelint\\n","Decision":"Don't start using stylelint\\n","tokens":33,"id":5084,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n### Context\\nIt\\'s annoying when tests break when refactoring.\\n### Decision\\nStart using React Testing Library\\n\\n## Decision\\nStart using React Testing Library\\n\\n\\n## Context and Problem Statement\\n> Dex requires persisting state to perform various tasks such as track refresh tokens, preventing replays, and rotating keys.\\nWhat persistence option should we use?\\n## Decision Drivers\\n* CRDs add complexity\\n* Storage adds complexity\\n\\n## Decision\\n* CRDs add complexity\\n* Storage adds complexity\\nChosen option: \"use memory\", because it simplified operations with little negative impact.\\n### Positive Consequences\\n* Dex brings no additional CRDs, which simplified upgrades.\\n* Dex brings no state, which simplified upgrades.\\n### Negative Consequences\\n* The authentication flow is disrupted, if Dex is rebooted *exactly* during an authentication flow. There is no user impact if Dex is restarted after the [JWT](https:\/\/jwt.io\/) was issued. Cristian tested this with `kubectl` and Grafana. Since we will only reboot Dex during maintenance windows, this is unlikely to be an issue in the foreseeable future.\\n\\n\\n\\nProvide a decision given the context below:\\n### Context\\\\n- linting for CSS\\\\n- After running it, it didn\\'t pick up a single error\\\\n### Decision\\\\nDon\\'t start using stylelint\\\\n'}, {'role': 'model', 'content': \"Don't start using stylelint\\\\n\"}]"}
{"File Name":"external-service-operator\/0006-only-one-probe-of-http-tcp-is-possible.md","Context":"## Context\\nThe Kubernetes Probe Resource Structure would allow adding multiple types of Healthchecks at once. This would be:\\n* exec\\n* httpGet\\n* tcpSocket\\nHowever currently v1.18 v1.20 when one tries to add more than one Type, like:\\n```\\nreadinessProbe:\\nexec:\\ncommand:\\n- \"\/usr\/bin\/sh\"\\n- \"-c\"\\n- \"echo Hello World\"\\nhttpGet:\\nhost: localhost\\nport: 80\\ntcpSocket:\\nhost: localhost\\nport: 80\\n```\\nkubernetes will not validate the Resource with following message:\\n```\\n# pods \"test-probes\" was not valid:\\n# * spec.containers[0].readinessProbe.httpGet: Forbidden: may not specify more than 1 handler type\\n# * spec.containers[0].readinessProbe.tcpSocket: Forbidden: may not specify more than 1 handler type\\n```\\n","Decision":"It is decided to go with the same logic and ensure that only one type will be accepted, so the Operator is not supposed to handle the case more than one Types are given.\\nFurther it follows the same logic and will use and check HTTP first than TCP last. Exec healthchecks don't make sense and will be ignored completly\\n","tokens":202,"id":1734,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nContent Performance Manager is subscribed to events in PublishingAPI via RabbitMQ. At the moment we process the events synchronously:\\n```\\nclass Consumer\\ndef process(rabbit_mq_message)\\n# preprocess\\ndo_process(rabbit_mq_message)\\n# post-process\\nend\\nend\\n```\\nEach item should run within a transaction, so we effectively need to wrap it with:\\n```\\nActiveRecord::Base.transaction do\\ndo_process(rabbit_mq_message)\\nend\\n```\\nWith this configuration we have had unexpected side effects because the database seems to reject the connections via the following message:\\n```\\n`PG::ConnectionBad: PQsocket() can\\'t get socket descriptor: BEGIN\\n```\\nWe debugged the database for many hours, and we got the feeling that the connections might be shared among multiple threads, and this only happens in staging because we have more RabbitMQ workers \/ hosts.\\nIn any case, we had already planned using Sidekiq for processing the messages because:\\n1. It allows us to reuse the existing monitoring capabilities in GOV.UK\\n2. It supports out-of-the-box exponential backoff algorithm for retries.\\n\\n## Decision\\nProcess all the events from Publishing API in Sidekiq Jobs\\n### Consequences\\nDefine a new ActiveJob job to process each message.\\n\\n\\nContext\\n-------\\nWhen using the puppetlabs firewall module, one of the recommended\\nsettings is to purge all rules not managed by puppet:\\n```puppet\\nresources { \\'firewall\\':\\npurge => true,\\n}\\n```\\nSince kubernetes and calico perform networking and load balancing at the\\nkernel level, this means puppet finds itself with hundreds of rules to\\npurge every 30 minutes.\\nThe firewall module also has an undocumented feature that lets you\\nmanage individual netfilter chains:\\n```puppet\\n# This will remove all `-A INPUT` rules unless they\\'re either (a)\\n# managed by puppet or (b) contain `--comment \"you can trust me ;)\"`.\\nfirewallchain { \\'INPUT:filter:IPv4\\':\\nensure => \\'present\\',\\npurge  => true,\\nignore => [\\n\\'--comment \"you can trust me ;)\"\\',\\n],\\n}\\n```\\nThis doesn\\'t work in conjunction with `purge => true` for all firewall\\nresources. If we use `firewallchain` resources instead, then any chains\\nwe don\\'t explicitly define will be ignored by puppet. So, for example,\\nif we define only `INPUT:filter:IPv4`, then any existing `OUTPUT` rules\\nwill be ignored rather than purged by puppet.\\nDecision\\n--------\\nWe will only manage INPUT, OUTPUT, and FORWARD chains on kubernetes\\nmachines. We will specifically whitelist particular lines known to be\\nused in those chains by kubernetes and calico.\\nConsequences\\n------------\\nAs of writing this, it means we now have two firewall profiles\\n(`networking::firewall` and `kubernetes::firewall`), and they share most\\nof their code. We should probably split that `purge => true` line out of\\nthe central firewall profile and come up with a single structure that\\nworks more broadly.\\nAdditionally, docker itself performs its own networking, so we\\'ll want\\nto do something like this for the `docker` profile, but with different\\nexceptions in the chains we manage.\\nSecurity consequences: unknown.\\n\\n## Decision\\n--------\\nWe will only manage INPUT, OUTPUT, and FORWARD chains on kubernetes\\nmachines. We will specifically whitelist particular lines known to be\\nused in those chains by kubernetes and calico.\\nConsequences\\n------------\\nAs of writing this, it means we now have two firewall profiles\\n(`networking::firewall` and `kubernetes::firewall`), and they share most\\nof their code. We should probably split that `purge => true` line out of\\nthe central firewall profile and come up with a single structure that\\nworks more broadly.\\nAdditionally, docker itself performs its own networking, so we\\'ll want\\nto do something like this for the `docker` profile, but with different\\nexceptions in the chains we manage.\\nSecurity consequences: unknown.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThe Kubernetes Probe Resource Structure would allow adding multiple types of Healthchecks at once. This would be:\\\\n* exec\\\\n* httpGet\\\\n* tcpSocket\\\\nHowever currently v1.18 v1.20 when one tries to add more than one Type, like:\\\\n```\\\\nreadinessProbe:\\\\nexec:\\\\ncommand:\\\\n- \"\/usr\/bin\/sh\"\\\\n- \"-c\"\\\\n- \"echo Hello World\"\\\\nhttpGet:\\\\nhost: localhost\\\\nport: 80\\\\ntcpSocket:\\\\nhost: localhost\\\\nport: 80\\\\n```\\\\nkubernetes will not validate the Resource with following message:\\\\n```\\\\n# pods \"test-probes\" was not valid:\\\\n# * spec.containers[0].readinessProbe.httpGet: Forbidden: may not specify more than 1 handler type\\\\n# * spec.containers[0].readinessProbe.tcpSocket: Forbidden: may not specify more than 1 handler type\\\\n```\\\\n'}, {'role': 'model', 'content': \"It is decided to go with the same logic and ensure that only one type will be accepted, so the Operator is not supposed to handle the case more than one Types are given.\\\\nFurther it follows the same logic and will use and check HTTP first than TCP last. Exec healthchecks don't make sense and will be ignored completly\\\\n\"}]"}
{"File Name":"libelektra\/key_string_return_value.md","Context":"## Problem\\nWhen using keyString() on empty \/ binary values the return values are the literal strings (null) \/ (binary). This seems very awkward and unintuitive from a user's perspective.\\n","Decision":"- `key == NULL` return 0, error code via second channel\\n- `key->value == NULL` return 0, error code via second channel\\n- `key == <binary>` return 0, error code via second channel\\n- everything else as is\\n","tokens":41,"id":1295,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Problem\\nCurrently it is inefficient to detect the length of an array and\\nit is impossible to know if a key (without subkeys) should be\\nan array or not.\\n\\n## Decision\\nStore length in metadata `array` of key, or keep metadata `array` empty if empty array.\\nOnly children that have `#` syntax are allowed in a valid array.\\nThe index start with `#0`.\\nBoth `keyAddName(\"#12\")` or `keyAddBaseName(\"#_12\")` is allowed to add the 13th index.\\nFor example (`ni syntax`, sections used for metadata):\\n```\\nmyarray\/#0 = value0\\nmyarray\/#1 = value1\\nmyarray\/#2 = value2\\nmyarray\/#3 = value3\\nmyarray\/#4 = value4\\nmyarray\/#5 = value5\\n[myarray]\\narray = #5\\n```\\nIt is not allowed to have anything else than `#5` in the metadata `array`,\\ne.g. even `#4` would be a malformed array.\\nWith the metadata `array` in `spec:\/` the `spec` plugin will add the\\n`array` marker with the correct length.\\nThis needs to be happening early, so that plugins can rely on having\\ncorrect arrays.\\nFor example:\\n```\\nspec:\/myarray      # <- has array marker\\nuser:\/myarray      # <- either has correct array marker or no array marker\\nuser:\/myarray\/#0\\nuser:\/myarray\/#1\\n```\\nHere, the `spec` plugin would add `array=#1` to `user:\/myarray` if it was not there before.\\nTo look up an array, first do `ksLookupByName (ks, \"\/myarray\", 0)` on the parent.\\nWith the last index you get from its metadata `array`, iterate over the children.\\nA cascading lookup on every individual child is also needed to make sure that overrides on individual\\nelements are respected.\\nFor example:\\n```\\nspec:\/myarray    # <- contains the specification for the array\\nspec:\/myarray\/#  # <- contains the specification for the array elements\\ndir:\/myarray\/#0  # <- not an array, just an override for user:\/myarray\/#\\nuser:\/myarray    # <- with metadata array=#0, this would be the array we get\\nuser:\/myarray\/#0\\nsystem:\/myarray  # <- not found in cascading lookup, as user:\/myarray exists\\n```\\nThe `spec` plugin should check if it is a valid array, i.e.:\\n- that the parent key always contains the metadata `array`,\\n- that the correct length is in `array`,\\n- that the array only contains `#` children, and\\n- that the children are numbered from `#0` to `#n`, without holes.\\n- [Global Plugins](global_plugins.md)\\n- [Global Validation](global_validation.md)\\n- [Base Names](base_name.md)\\n- [Metadata in Spec Namespace](spec_metadata.md)\\n- [Spec Expressiveness](spec_expressiveness.md)\\n\\n\\n## Context and Problem Statement\\nCurrently, JabRef uses some localized preferences. We want to remove the Localization-dependency from the JabRefPreferences and move the Localization to where the String is used.\\nThe problem is how to store the default values.\\n\\n## Decision\\nChosen option: \"_Store the unlocalized String._ Consumers then check the String they got as a preference against the defaults. If it matches, localize it. Otherwise, use it.\", because Achieves goals without requiring too much refactoring and without (known) downsides.\\n\\n\\n\\nProvide a decision given the context below:\\n## Problem\\\\nWhen using keyString() on empty \/ binary values the return values are the literal strings (null) \/ (binary). This seems very awkward and unintuitive from a user\\'s perspective.\\\\n'}, {'role': 'model', 'content': '- `key == NULL` return 0, error code via second channel\\\\n- `key->value == NULL` return 0, error code via second channel\\\\n- `key == <binary>` return 0, error code via second channel\\\\n- everything else as is\\\\n'}]"}
{"File Name":"operational-data-hub\/0040-hpa.md","Context":"## Context\\nHigh privilege access (HPA) limits production access for developers to only the components and period this access is required to investigate issues of check system health. This implements the [principle of least privilege](0039-least-privilege-access.md) for support on production systems.\\n","Decision":"We will use a high privilege access procedure to secure access to production systems for support.\\n","tokens":60,"id":2721,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nDeployment of new software to the production environment should be done in a controlled way. The possibility of unauthorized deployment to production is a security vulnerability. Therefore, deployment is [automated](0049-gitops-deployment.md) in CI\/CD pipelines. Next to that, a pull request with review procedure can be used to implement 4 eyes principle through review, either by peers or other functions.\\n\\n## Decision\\nWe will use pull requests to initiate a deployment of new software to production.\\n\\n\\n## Context\\nThe design proposed in [018](018-Dedicated-Ingress-Controllers.md) proved\\nimpossible to implement. Every ingress controller requires an AWS Network Load\\nBalancer (NLB), and AWS have hard limits on the number of NLBs we can create\\nper VPC and availability zone (AZ).\\n\\n## Decision\\nWe will:\\n* Disable mod-security on the default ingress-controller\\nThis should enable this ingress-controller to comfortably handle thousands of ingresses.\\n* Create and manage a set of ingress controllers with mod-security enabled\\nMost, if not all, production services will want the protection of a web\\napplication firewall, and mod-security is the easiest to enable. We need to\\nensure that each ingress controller only handles as many mod-security-enabled\\ningresses as it can reliably cope with.\\n* Continue to have some dedicated ingress controllers\\nDuring our aborted migration to dedicated ingress controllers for every\\nnamespace, several ingress controllers were created for specific services. We\\nwill leave these in place to avoid additional disruption to these service\\nteams.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nHigh privilege access (HPA) limits production access for developers to only the components and period this access is required to investigate issues of check system health. This implements the [principle of least privilege](0039-least-privilege-access.md) for support on production systems.\\\\n'}, {'role': 'model', 'content': 'We will use a high privilege access procedure to secure access to production systems for support.\\\\n'}]"}
{"File Name":"molgenis-r-armadillo\/0003-use-s3-api.md","Context":"## Context\\n* Minio already has a UI and an S3 API that allow administration of files and buckets.\\n* There are existing client libraries for the S3 API.\\n* It is nontrivial to proxy large file uploads through the armadillo server.\\n","Decision":"The Armadillo client will be written as a high-level library on top of an existing S3 API.\\n","tokens":56,"id":547,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nThe backend needs to serialize the REST resources before sending them via http to the client.\\n## Decision Drivers\\n- the format should be REST\/[HATEOAS](https:\/\/spring.io\/understanding\/HATEOAS) compatible (e.g. allow hyperlinks)\\n- the format should be easy to parse\/serialize for the backend and the client\\n\\n## Decision\\n- the format should be REST\/[HATEOAS](https:\/\/spring.io\/understanding\/HATEOAS) compatible (e.g. allow hyperlinks)\\n- the format should be easy to parse\/serialize for the backend and the client\\nWe will use json+hal (or the spring implementation for json with hyperlinks) without the [`_embedded`](https:\/\/tools.ietf.org\/html\/draft-kelly-json-hal-08#section-4.1.2) attribute (because of its complexity to implement correctly).\\n```eval_rst\\n.. seealso::\\n* :doc:`0010-rest-api-design`\\n* `Richardson Maturity Model <https:\/\/martinfowler.com\/articles\/richardsonMaturityModel.html>`_\\n* `HATEOAS is for Humans <http:\/\/intercoolerjs.org\/2016\/05\/08\/hatoeas-is-for-humans.html>`_\\n```\\n\\n\\n## Context\\nNode Exporter needs to be installed on Verify's infrastructure so that machine metrics can be gathered.\\nVerify runs Ubuntu Trusty which does not have an existing node exporter package.\\nVerify has an existing workflow for packaging binaries which can be leveraged to package node exporter.\\n\\n## Decision\\nNode exporter will be packaged as a deb using FPM following Verify's exiting packaging workflow.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\n* Minio already has a UI and an S3 API that allow administration of files and buckets.\\\\n* There are existing client libraries for the S3 API.\\\\n* It is nontrivial to proxy large file uploads through the armadillo server.\\\\n\"}, {'role': 'model', 'content': 'The Armadillo client will be written as a high-level library on top of an existing S3 API.\\\\n'}]"}
{"File Name":"oasis-core\/0002-go-modules-compatible-git-tags.md","Context":"## Context\\nProjects that depend on [Oasis Core's Go module], i.e.\\n`github.com\/oasisprotocol\/oasis-core\/go`, need a way to depend on its particular\\nversion.\\nGo Modules only allow [Semantic Versioning 2.0.0] for\\n[versioning of the modules][go-mod-ver] which makes it hard to work\\nwith [Oasis Core's CalVer (calendar versioning) scheme].\\nThe currently used scheme for Go Modules compatible Git tags is:\\n```\\ngo\/v0.YY.MINOR[.MICRO]\\n```\\nwhere:\\n- `YY` represents the short year (e.g. `19`, `20`, `21`, ...),\\n- `MINOR` represents the minor version starting with zero (e.g. `0`, `1`, `2`,\\n`3`, ...),\\n- `MICRO` represents the final number in the version (sometimes referred to as\\nthe \"patch\" segment) (e.g. `0`, `1`, `2`, `3`, ...).\\nIf the `MICRO` version is `0`, it is omitted.\\nIt turns out this only works for Oasis Core versions with the `MICRO` version\\nof `0` since the Go Modules compatible Git tag omits the `.MICRO` part and is\\nthus compatible with [Go Modules versioning requirements][go-mod-ver].\\n[Oasis Core's Go module]:\\nhttps:\/\/pkg.go.dev\/mod\/github.com\/oasisprotocol\/oasis-core\/go\\n[Semantic Versioning 2.0.0]:\\nhttps:\/\/semver.org\/spec\/v2.0.0.html\\n[go-mod-ver]:\\nhttps:\/\/golang.org\/ref\/mod#versions\\n[Oasis Core's CalVer (calendar versioning) scheme]: ..\/versioning.md\\n","Decision":"The proposed design is to tag Oasis Core releases with the following Go Modules\\ncompatible Git tags (in addition to the ordinary Git tags):\\n```\\ngo\/v0.YY0MINOR.MICRO\\n```\\nwhere:\\n- `YY` represents the short year (e.g. `19`, `20`, `21`, ...),\\n- `0MINOR` represents the zero-padded minor version starting with zero (e.g.\\n`00`, `01`, `02`, ..., `10`, `11`, ...),\\n- `MICRO` represents the final number in the version (sometimes referred to as\\nthe \"patch\" segment) (e.g. `0`, `1`, `2`, `3`, ...).\\nHere are some examples of how the ordinary and the corresponding Go Modules\\ncompatible Git tags would look like:\\n| Version       | Ordinary Git tag | Go Modules compatible Git tag  |\\n|:-------------:|:----------------:|:------------------------------:|\\n| 20.9          | `v20.9`          | `go\/v0.2009.0`                 |\\n| 20.9.1        | `v20.9.1`        | `go\/v0.2009.1`                 |\\n| 20.9.2        | `v20.9.2`        | `go\/v0.2009.2`                 |\\n| 20.10         | `v20.10`         | `go\/v0.2010.0`                 |\\n| 20.10.1       | `v20.10.1`       | `go\/v0.2010.1`                 |\\n| 20.10.2       | `v20.10.2`       | `go\/v0.2010.2`                 |\\n| ...           | ...              | ...                            |\\n| 21.0          | `v21.0`          | `go\/v0.2100.0`                 |\\n| 21.0.1        | `v21.0.1`        | `go\/v0.2100.1`                 |\\n| 21.0.2        | `v21.0.2`        | `go\/v0.2100.2`                 |\\n| 21.1          | `v21.1`          | `go\/v0.2101.0`                 |\\n| 21.1.1        | `v21.1.1`        | `go\/v0.2101.1`                 |\\n| 21.1.2        | `v21.1.2`        | `go\/v0.2101.2`                 |\\n| ...           | ...              | ...                            |\\nUsing such a scheme makes the version of the Oasis Core Go module fully\\ncompatible with the [Go Modules versioning requirements][go-mod-ver] and thus\\nenables users to use the familiar Go tools to check for new module versions,\\ni.e. `go list -m -u all`, or to obtain and require a module, i.e.\\n`go get github.com\/oasisprotocol\/oasis-core\/go@latest`.\\n","tokens":390,"id":4345,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nI\\'m trying to make changes to this monorepo but I keep encountering dependency problems. Ideally the root package.json contains the devDependencies allow the `packages\/` folder\\'s package.json to contain the dependecies. This works reasonably well with yarn, lerna or pnpm but using npm I see errors like:\\n```shell\\n> site@1.0.0 lint \/home\/iampeterbanjo\/clever-cloud\/iampeterbanjo.com\\n> npx eslint \"**\/*.{ts,js}\" --fix\\nCannot read config file: \/home\/iampeterbanjo\/clever-cloud\/iampeterbanjo.com\/node_modules\/@typescript-eslint\/eslint-plugin\/dist\/index.js\\nError: Cannot find module \\'typescript\\'\\nReferenced from: \/home\/iampeterbanjo\/clever-cloud\/iampeterbanjo.com\/packages\/server\/.eslintrc.json\\n```\\nPackages that could be shared:\\n- Typescript\\n- Eslint\\n- Prettier\\nIt is possible to share configurations from a base config with Typescript. e.g.\\n```json\\n{\\n\"extends\": \"..\/..\/tsconfig.base.json\",\\n\"compilerOptions\": {\\n\"outDir\": \".\/build\"\\n},\\n\"include\": [\"src\", \"__tests__\"]\\n}\\n```\\nFor prettier I can publish my [custom configuration][published-prettier-config] and reference it in `package.json` like:\\n```json\\n{\\n\"name\": \"my-cool-library\",\\n\"version\": \"9000.0.1\",\\n\"prettier\": \"@company\/prettier-config\"\\n}\\n```\\n\\n## Decision\\n- All packages should contain all their dependencies.\\n- The root `package.json` contains only convenience scripts or base configs.\\n- Drop eslint because it\\'s value is not clear: prettier formats, Typescript checks - done.\\n\\n\\n### Context\\nPHPUnit offers a range of `@covers` annotations with the possible to enforce a strict mode or to\\nenforce them. The question is when should those annotations be enforced and\/or if we need to enable\\nanother settings as well?\\n### Decision\\nSince we are using the [`symfony\/phpunit-bridge`][phpunit-bridge], we decide to leverage the\\n[`Symfony\\\\Bridge\\\\PhpUnit\\\\CoverageListener`][code-coverage-listener] in `phpunit.xml.dist` in order to avoid to require the\\n`@covers` annotations whilst still benefit from it.\\nThis however does not allow to completely forgo its usage due to the following cases:\\n- A test testing more than one class, requiring multiple `@covers` annotations\\n- A test case testing a \"test class\", i.e. code reserved for testing purposes\\nFor this reason, the proposal to remove the `@covers` annotations via the [PHP-CS-Fixer][php-cs-fixer]\\nsetting `general_phpdoc_annotation_remove` has been refused.\\nSince no one came up with an easy or acceptable proposal to automate the process of whether a\\n`@covers` annotation is necessary or not, no further action has been voted for automating this\\nprocess.\\n### Status\\nAccepted ([#1060][1060])\\n[code-coverage-listener]: https:\/\/symfony.com\/doc\/current\/components\/phpunit_bridge.html#code-coverage-listener\\n[phpunit-bridge]: https:\/\/packagist.org\/packages\/symfony\/phpunit-bridge\\n[php-cs-fixer]: https:\/\/github.com\/FriendsOfPHP\/PHP-CS-Fixer\\n[1060]: https:\/\/github.com\/infection\/infection\/pull\/1060\\n\\n## Decision\\nSince we are using the [`symfony\/phpunit-bridge`][phpunit-bridge], we decide to leverage the\\n[`Symfony\\\\Bridge\\\\PhpUnit\\\\CoverageListener`][code-coverage-listener] in `phpunit.xml.dist` in order to avoid to require the\\n`@covers` annotations whilst still benefit from it.\\nThis however does not allow to completely forgo its usage due to the following cases:\\n- A test testing more than one class, requiring multiple `@covers` annotations\\n- A test case testing a \"test class\", i.e. code reserved for testing purposes\\nFor this reason, the proposal to remove the `@covers` annotations via the [PHP-CS-Fixer][php-cs-fixer]\\nsetting `general_phpdoc_annotation_remove` has been refused.\\nSince no one came up with an easy or acceptable proposal to automate the process of whether a\\n`@covers` annotation is necessary or not, no further action has been voted for automating this\\nprocess.\\n### Status\\nAccepted ([#1060][1060])\\n[code-coverage-listener]: https:\/\/symfony.com\/doc\/current\/components\/phpunit_bridge.html#code-coverage-listener\\n[phpunit-bridge]: https:\/\/packagist.org\/packages\/symfony\/phpunit-bridge\\n[php-cs-fixer]: https:\/\/github.com\/FriendsOfPHP\/PHP-CS-Fixer\\n[1060]: https:\/\/github.com\/infection\/infection\/pull\/1060\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nProjects that depend on [Oasis Core\\'s Go module], i.e.\\\\n`github.com\/oasisprotocol\/oasis-core\/go`, need a way to depend on its particular\\\\nversion.\\\\nGo Modules only allow [Semantic Versioning 2.0.0] for\\\\n[versioning of the modules][go-mod-ver] which makes it hard to work\\\\nwith [Oasis Core\\'s CalVer (calendar versioning) scheme].\\\\nThe currently used scheme for Go Modules compatible Git tags is:\\\\n```\\\\ngo\/v0.YY.MINOR[.MICRO]\\\\n```\\\\nwhere:\\\\n- `YY` represents the short year (e.g. `19`, `20`, `21`, ...),\\\\n- `MINOR` represents the minor version starting with zero (e.g. `0`, `1`, `2`,\\\\n`3`, ...),\\\\n- `MICRO` represents the final number in the version (sometimes referred to as\\\\nthe \"patch\" segment) (e.g. `0`, `1`, `2`, `3`, ...).\\\\nIf the `MICRO` version is `0`, it is omitted.\\\\nIt turns out this only works for Oasis Core versions with the `MICRO` version\\\\nof `0` since the Go Modules compatible Git tag omits the `.MICRO` part and is\\\\nthus compatible with [Go Modules versioning requirements][go-mod-ver].\\\\n[Oasis Core\\'s Go module]:\\\\nhttps:\/\/pkg.go.dev\/mod\/github.com\/oasisprotocol\/oasis-core\/go\\\\n[Semantic Versioning 2.0.0]:\\\\nhttps:\/\/semver.org\/spec\/v2.0.0.html\\\\n[go-mod-ver]:\\\\nhttps:\/\/golang.org\/ref\/mod#versions\\\\n[Oasis Core\\'s CalVer (calendar versioning) scheme]: ..\/versioning.md\\\\n'}, {'role': 'model', 'content': 'The proposed design is to tag Oasis Core releases with the following Go Modules\\\\ncompatible Git tags (in addition to the ordinary Git tags):\\\\n```\\\\ngo\/v0.YY0MINOR.MICRO\\\\n```\\\\nwhere:\\\\n- `YY` represents the short year (e.g. `19`, `20`, `21`, ...),\\\\n- `0MINOR` represents the zero-padded minor version starting with zero (e.g.\\\\n`00`, `01`, `02`, ..., `10`, `11`, ...),\\\\n- `MICRO` represents the final number in the version (sometimes referred to as\\\\nthe \"patch\" segment) (e.g. `0`, `1`, `2`, `3`, ...).\\\\nHere are some examples of how the ordinary and the corresponding Go Modules\\\\ncompatible Git tags would look like:\\\\n| Version       | Ordinary Git tag | Go Modules compatible Git tag  |\\\\n|:-------------:|:----------------:|:------------------------------:|\\\\n| 20.9          | `v20.9`          | `go\/v0.2009.0`                 |\\\\n| 20.9.1        | `v20.9.1`        | `go\/v0.2009.1`                 |\\\\n| 20.9.2        | `v20.9.2`        | `go\/v0.2009.2`                 |\\\\n| 20.10         | `v20.10`         | `go\/v0.2010.0`                 |\\\\n| 20.10.1       | `v20.10.1`       | `go\/v0.2010.1`                 |\\\\n| 20.10.2       | `v20.10.2`       | `go\/v0.2010.2`                 |\\\\n| ...           | ...              | ...                            |\\\\n| 21.0          | `v21.0`          | `go\/v0.2100.0`                 |\\\\n| 21.0.1        | `v21.0.1`        | `go\/v0.2100.1`                 |\\\\n| 21.0.2        | `v21.0.2`        | `go\/v0.2100.2`                 |\\\\n| 21.1          | `v21.1`          | `go\/v0.2101.0`                 |\\\\n| 21.1.1        | `v21.1.1`        | `go\/v0.2101.1`                 |\\\\n| 21.1.2        | `v21.1.2`        | `go\/v0.2101.2`                 |\\\\n| ...           | ...              | ...                            |\\\\nUsing such a scheme makes the version of the Oasis Core Go module fully\\\\ncompatible with the [Go Modules versioning requirements][go-mod-ver] and thus\\\\nenables users to use the familiar Go tools to check for new module versions,\\\\ni.e. `go list -m -u all`, or to obtain and require a module, i.e.\\\\n`go get github.com\/oasisprotocol\/oasis-core\/go@latest`.\\\\n'}]"}
{"File Name":"easi-app\/0001-separate-infra-repo.md","Context":"## Decision Drivers\\n* Visibility of changes to anyone working in the codebases.\\n* Management of CI\/CD configuration.\\n* Iteration speed.\\n* Coupling of infra and app changes.\\n","Decision":"* Visibility of changes to anyone working in the codebases.\\n* Management of CI\/CD configuration.\\n* Iteration speed.\\n* Coupling of infra and app changes.\\n* *Separate repo for Infrastructure.*\\nDecoupling the infrastructure and application deployments allows iteration to\\nhappen on both of those aspects of the project at separate speeds without\\nhindering one another. We have experience in making this repo separation work\\nand can easily configure CI and deployment tooling to allow for proper\\norchestration to bring these two pieces together.\\nPlease see the full good\/bad list below.\\n","tokens":42,"id":2167,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nWe want to be able to centralise in a single tool all tasks to be called:\\n* During the development cycle.\\n* During the build cycle.\\n* In the CI\/CD pipelines.\\n## Decision Drivers\\n* Must have a significant user base and community.\\n* Must not require significant installation.\\n* Must be sufficiently simple to learn and use.\\n\\n## Decision\\n* Must have a significant user base and community.\\n* Must not require significant installation.\\n* Must be sufficiently simple to learn and use.\\nChosen option: `GNU Make`, because compared to the other evaluated tools (see\\n[Pros and Cons](#pros-and-cons-of-the-options)), it fits the bill where as:\\n* `Invoke` is not well maintained, nor well documented, nor a de facto standard, nor has\\na sufficient community.\\n* `Rake` required to install and learn Ruby.\\n* `SCons` is more a build tool and seems difficult to apprehend\/get to grips with.\\n\\n\\n## Context and Problem Statement\\nThe JabRef repository contains Markdown (`.md`) files documenting the JabRef code.\\nThe documentation contains links to external resources.\\nFor high-quality documentation, external links should be working.\\n## Decision Drivers\\n* Checking external links should not cause issues in the normal workflow\\n\\n## Decision\\n* Checking external links should not cause issues in the normal workflow\\nChosen option: \"\\\\[option 1\\\\]\", because \\\\[justification. e.g., only option, which meets k.o. criterion decision driver \\\\| which resolves force force \\\\| \u2026 \\\\| comes out best \\\\(see below\\\\)\\\\].\\n### Positive Consequences\\n* Automatic notification of broken external links\\n### Negative Consequences\\n* Some external sites need to [be disabled](https:\/\/github.com\/JabRef\/jabref\/pull\/6542\/files). For instance, GitHub.com always returns \"forbidden\". A [filter for status is future work of the used tool](https:\/\/github.com\/tcort\/markdown-link-check\/issues\/94#issuecomment-634947466).\\n\\n\\n\\nProvide a decision given the context below:\\n## Decision Drivers\\\\n* Visibility of changes to anyone working in the codebases.\\\\n* Management of CI\/CD configuration.\\\\n* Iteration speed.\\\\n* Coupling of infra and app changes.\\\\n'}, {'role': 'model', 'content': '* Visibility of changes to anyone working in the codebases.\\\\n* Management of CI\/CD configuration.\\\\n* Iteration speed.\\\\n* Coupling of infra and app changes.\\\\n* *Separate repo for Infrastructure.*\\\\nDecoupling the infrastructure and application deployments allows iteration to\\\\nhappen on both of those aspects of the project at separate speeds without\\\\nhindering one another. We have experience in making this repo separation work\\\\nand can easily configure CI and deployment tooling to allow for proper\\\\norchestration to bring these two pieces together.\\\\nPlease see the full good\/bad list below.\\\\n'}]"}
{"File Name":"io-pagopa-proxy\/0002-use-uuid-v1-for-sequential-requests.md","Context":"## Context\\nWe have to send requests to a server that requires unique Request IDs for each message.\\nSo, we need to generate uuids.\\n","Decision":"We decided to use uuid library and generate unique uuids based on timestamp (Version 1):\\nhttps:\/\/www.npmjs.com\/package\/uuid\\n","tokens":31,"id":634,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\\n\\n## Decision\\n* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\\n\\n\\n## Context\\nRead [Event Bus ADR](0037-eventbus.md) for context.\\nGiven several James servers, we need them to share a common EventBus.\\nThis:\\n- Ensures a better load balancing for `group mailbox listners`.\\n- Is required for correctness of notifications (like IMAP IDLE).\\n\\n## Decision\\nProvide a distributed implementation of the EventBus leveraging RabbitMQ.\\nEvents are emitted to a single Exchange.\\nEach group will have a corresponding queue, bound to the main exchange, with a default routing key. Each eventBus\\nwill consume this queue and execute the relevant listener, ensuring at least once execution at the cluster level.\\nRetries are managed via a dedicated exchange for each group: as we need to count retries, the message headers need to\\nbe altered and we cannot rely on rabbitMQ build in retries. Each time the execution fails locally, a new event is emitted\\nvia the dedicated exchange, and the original event is acknowledged.\\nEach eventBus will have a dedicated exclusive queue, bound to the main exchange with the `registrationKeys` used by local\\nnotification mailboxListeners (to only receive the corresponding subset of events). Errors are not retried for\\nnotifications, failures are not persisted within `DeadLetter`, achieving at most once event delivery.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe have to send requests to a server that requires unique Request IDs for each message.\\\\nSo, we need to generate uuids.\\\\n\"}, {'role': 'model', 'content': 'We decided to use uuid library and generate unique uuids based on timestamp (Version 1):\\\\nhttps:\/\/www.npmjs.com\/package\/uuid\\\\n'}]"}
{"File Name":"zsh\/0002-use-drop-in-pattern-for-extensibility.md","Context":"## Context\\nFacilitate organizing shell customizations into smaller, more readable, more understandable, more maintainable blocks and to provide a clear and common patter for extensibility.\\n","Decision":"Create a $ZDOTDEEDIR directory where extension files can be stored. Add a rooutine to .zprofile or .zshrc which sources all files found in this directory in lexagraphical order by file name.\\n","tokens":37,"id":1102,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nReducing complexity in the `ConsoleUI` object and make it more reusable and testable.\\n\\n## Decision\\n`ConsoleUI` now only knows when to output things, but not the contents of messages, which is now responsibility of a\\n`Formatter`. This object can be replaced by other formatters in the future.\\n\\n\\n## Context\\nAt the moment, sherds are merely a short bit of free text.\\nTo extract useful information, it will be useful to have some standard event types, such as team members joining or leaving.  These event types would have a well defined structure -- maybe still be Markdown, but with constraints on the content and markup used -- and be generated by convenient subcommands of the `pottery` tool.\\n\\n## Decision\\nAdd the event type to the sherd filename, after the date but before the unique ID.  The parts of the filename are separated by underscores.\\nThe event type is a short string that cannot contain an underscore.\\nFree text sherds have the event type \"post\".\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nFacilitate organizing shell customizations into smaller, more readable, more understandable, more maintainable blocks and to provide a clear and common patter for extensibility.\\\\n'}, {'role': 'model', 'content': 'Create a $ZDOTDEEDIR directory where extension files can be stored. Add a rooutine to .zprofile or .zshrc which sources all files found in this directory in lexagraphical order by file name.\\\\n'}]"}
{"File Name":"figgy\/0001-document-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n","tokens":16,"id":4823,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need to record the architectural decisions made on this project.\\\\n'}, {'role': 'model', 'content': 'We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\\\n'}]"}
{"File Name":"accessibility-monitoring\/ADR005-database-public-sector-websites.md","Context":"## Context\\nWe currently have a \"public sector domains\" table (along with an \"organisations\" table containing the owners of the domains).\\nHowever, many organisations have websites that are not on their own domain but in a subfolder of another domain. Moreover, many _services_ reside in subfolders or other logical locations.\\nEach of these needs to be treated as a separate \"website\" in the context of accessibility monitoring.\\nThus, we either change the \"public sector domains\" table to include all websites, not just unique domains, OR we create a new table specifically for public sector websites and initially copy over those domains that we can confirm are definitely websites.\\nThe domains table is useful in and of itself as a repository of registered domains. We should keep and maintain this as-is.\\n","Decision":"We will create and populate a new table in the PubSecDomains schema that contains:\\n* url\\n* name of the service\\n* the site's title from its HTML <head><title> element, where given\\n* the site's description from its <head><meta name=\"description\"> element, where given\\n* last_updated timestamp\\n* the website's sector (foreign key to sectors table)\\n* many-to-many join to the existing Organisations table (one site can, surprisingly, come under the auspices of more than one organisation; obviously one organisation can have multiple websites for their various services)\\n","tokens":162,"id":130,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nPUDL has [collection landing\\npages](http:\/\/pudl.princeton.edu\/collections\/pudl0058) because it\\'s a digital collections site which\\nstaff pushed content into. PUDL required all items be in a single collection,\\nand couldn\\'t provide good search-across. Figgy is a staff back-end, so we need some sort of\\napplication to provide that functionality.\\nCurators also had a history of either requesting or creating ad-hoc websites to showcase\\ntheir material or accompany on-site exhibits. We wanted instead to provide a CMS for\\nthem to create those experiences based on material they curate which wouldn\\'t\\ncreate metadata silos and increased maintenance.\\nFurther, curators often had different use cases about how metadata should\\ndisplay in different contexts. For example, items with the same title in the catalog may need to be differentiated in an exhibit. It was important that certain fields display a certain\\nway, but be cataloged according to best practices.\\nSpotlight is an exhibit building platform that provides controlled CMS\\nfunctionality and is built on Blacklight similar to our new catalog. It allows\\nfor local overrides of fields.\\n\\n## Decision\\nWe will use Spotlight to fulfill both the requirements of a Collection Landing\\nPage as well as exhibits. Curators of collections will generate the collection\\npages. Staff who wish to generate exhibits will be able to manage membership in\\nFiggy but have the tools to create those exhibits in Pomegranate.\\n\\n\\n## Context\\nIn [ADR 6](0006-tagging-governments.md) we discussed our decision to defer\\nthe tagging of governments and political status to content. In December 2019\\nthe political climate determined that we would need to implement this as it\\nwas expected that content published through Content Publisher would have to be\\nable to enter [history mode](..\/history-mode.md).\\nIn order for content to be in history mode it needs to be associated with a\\ngovernment that is no longer the current one and the content itself be flagged\\nas political. This tagging broadly means that the content represents the\\npolicies of the government it is associated with.\\nThe business rules behind this feature dictate that the political status of\\ncontent is determined dynamically based on attributes and associations of the\\ncontent. The government is determined based on the time the content is first\\npublished or backdated to.\\n\\n## Decision\\nTo implement this feature we decided to add a [service][edit-edition-service]\\nthat runs each time content is edited which can determine whether the content\\nis political or not. If the content has been published or is backdated it can\\nalso determine the government. This data is stored in the editions database\\ntable. As this value is dynamically calculated only the current value is\\nstored as it is not considered an aspect of the publications history.\\nThe political value for the content can be overwritten by a publisher. This\\nvalue is known as `editor_political` and supersedes the value stored on an\\nedition, which is known as `system_political`. As this data is set by a\\npublisher it is considered part of a revision of a document and is stored\\nfor prosperity.\\nWhen content has not been published or backdated it is not associated with a\\ngovernment. At the point of content being published the current time is used\\nto determine the government.\\nWhen sent to the Publishing API, these fields are represented in the form of\\na boolean attribute for political and an edition link for the government.\\nIn the time since ADR 6 governments have been added to the Publishing API\\nwhich means that content associated with a government no longer needs to be\\nrepublished when the government is no longer current.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe currently have a \"public sector domains\" table (along with an \"organisations\" table containing the owners of the domains).\\\\nHowever, many organisations have websites that are not on their own domain but in a subfolder of another domain. Moreover, many _services_ reside in subfolders or other logical locations.\\\\nEach of these needs to be treated as a separate \"website\" in the context of accessibility monitoring.\\\\nThus, we either change the \"public sector domains\" table to include all websites, not just unique domains, OR we create a new table specifically for public sector websites and initially copy over those domains that we can confirm are definitely websites.\\\\nThe domains table is useful in and of itself as a repository of registered domains. We should keep and maintain this as-is.\\\\n'}, {'role': 'model', 'content': 'We will create and populate a new table in the PubSecDomains schema that contains:\\\\n* url\\\\n* name of the service\\\\n* the site\\'s title from its HTML <head><title> element, where given\\\\n* the site\\'s description from its <head><meta name=\"description\"> element, where given\\\\n* last_updated timestamp\\\\n* the website\\'s sector (foreign key to sectors table)\\\\n* many-to-many join to the existing Organisations table (one site can, surprisingly, come under the auspices of more than one organisation; obviously one organisation can have multiple websites for their various services)\\\\n'}]"}
{"File Name":"arch\/0005-replace-wechat-mail-sms-with-slack.md","Context":"## Context\\n1. \u5f53\u524d\u4f7f\u7528\u5fae\u4fe1\u3001\u90ae\u4ef6\u4f5c\u4e3a\u5de5\u4f5c\u4e2d\u7684\u6c9f\u901a\u5de5\u5177\uff0c\u5de5\u4f5c\u4e0e\u751f\u6d3b\u6df7\u5408\uff1b\\n2. \u5404\u79cd\u544a\u8b66\u4fe1\u606f\u9700\u5206\u522b\u67e5\u770b\u90ae\u4ef6\u3001\u77ed\u4fe1\u6216\u662f\u76d1\u63a7\u5e73\u53f0\uff1b\\n3. \u968f\u7740\u4efb\u52a1\u7ba1\u7406\u5e73\u53f0\u3001Bug \u7ba1\u7406\u5e73\u53f0\u3001wiki\u3001\u6587\u6863\u5206\u4eab\u7f51\u7ad9\u3001\u804a\u5929\u5de5\u5177\u7684\u6d41\u884c\uff0c\u90ae\u4ef6\u7684\u4f7f\u7528\u573a\u666f\u8d8a\u6765\u8d8a\u5c0f\uff0c\u5e76\u4e14\u90ae\u4ef6\u4fe1\u606f\u5e76\u4e0d\u53ca\u65f6\uff1b\\n4. \u901a\u8fc7\u5fae\u4fe1\u8fdb\u884c\u6c9f\u901a\uff0c\u65b0\u4eba\u65e0\u6cd5\u4e86\u89e3\u5386\u53f2\u4fe1\u606f\uff0c\u7ecf\u5e38\u9700\u8981\u628a\u53d1\u8fc7\u7684\u5185\u5bb9\u91cd\u590d\u53d1\u9001\u3002\\n","Decision":"### Slack \u652f\u6301\u7684\u529f\u80fd\\n1. **\u516c\u5f00\u4e0e\u79c1\u6709\u7684\u9891\u9053\uff0c\u9891\u9053\u53ef\u968f\u65f6\u52a0\u5165\u6216\u9000\u51fa**\uff1b\\n1. \u6309\u9879\u76ee\uff0cproject-crm, project-sms, \u7b49\uff1b\\n2. \u6309\u6280\u672f\u8bdd\u9898\uff0ctech-restful-api, tech-queue \u7b49\uff1b\\n3. \u53ef\u4ee5\u9080\u8bf7\u76f8\u5173\u4eba\uff0c\u4efb\u4f55\u4eba\u4e5f\u53ef\u968f\u610f\u52a0\u5165\u3002\\n2. **\u6d88\u606f\u652f\u6301\u8868\u60c5\u5feb\u901f\u56de\u590d**\uff1b\\n1. \u8868\u60c5\u662f\u9644\u52a0\u5728\u6d88\u606f\u4e0a\u7684\uff0c\u4e0a\u4e0b\u6587\u5185\u805a\u5f88\u9ad8\u3002\\n3. \u6d88\u606f\u652f\u6301\u6536\u85cf\uff1b\\n1. \u4e00\u4e9b\u4e0d\u9519\u7684\u91cd\u8981\u4fe1\u606f\uff0c\u53ef\u4ee5\u6536\u85cf\u8d77\u6765\u968f\u65f6\u67e5\u770b\u3002\\n4. \u652f\u6301\u5404\u79cd\u6587\u4ef6\u7684\u5206\u4eab\uff1b\\n1. pdf \u7b49\u6587\u4ef6\u5747\u53ef\u9884\u89c8\u3002\\n5. **\u5206\u4eab\u7684\u94fe\u63a5\u652f\u6301\u9884\u89c8**\uff1b\\n1. \u5206\u4eab\u7684\u94fe\u63a5\uff0c\u4e0d\u7528\u70b9\u5f00\u4e5f\u77e5\u9053\u5927\u4f53\u5185\u5bb9\u3002\\n6. \u641c\u7d22\u529f\u80fd\u5f3a\u5927\uff0c\u53ef\u901a\u8fc7\u5feb\u6377\u65b9\u5f0f\uff0c\u641c\u7d22\u540c\u4e8b\uff0c\u6d88\u606f\u8bb0\u5f55\uff0c\u6587\u4ef6\u7b49\uff1b\\n7. \u591a\u79cd\u7a0b\u5e8f\u4ee3\u7801\u652f\u6301\u9ad8\u4eae\uff1b\\n1. \u4ee3\u7801\u9ad8\u4eae\u9884\u89c8\uff0c\u81ea\u52a8\u6298\u53e0\uff0c\u4e0d\u5f71\u54cd\u6574\u4f53\u6548\u679c\u3002\\n8. **\u5f3a\u5927\u7684\u7b2c\u4e09\u65b9\u96c6\u6210\uff0c\u5c06\u6240\u6709\u6d88\u606f\u3001\u901a\u77e5\u6c47\u805a\u5728\u4e00\u5904**\uff0c\u4f8b\u5982\uff0cTrello\uff0cGithub\uff0cNewRelic, Sentry\uff0cJenkins \u7b49\uff1b\\n9. **\u65b0\u52a0\u5165\u8005\u53ef\u67e5\u770b\u7fa4\u7ec4\u5386\u53f2\u4fe1\u606f**\u3002\\n1. \u4fe1\u606f\u518d\u4e5f\u4e0d\u7528\u91cd\u590d\u53d1\u4e86\u3002\\n10. \u5f00\u653e\u6027\u975e\u5e38\u597d\\n1. \u4efb\u4f55\u4eba\u90fd\u53ef\u4ee5\u65b9\u4fbf\u7533\u8bf7\u5f00\u53d1\u8005 KEY\uff0c\u5efa\u7acb\u81ea\u5df1\u7684\u673a\u5668\u4eba\u3002\\n### \u7814\u53d1\u90e8\u9891\u9053\u8bbe\u8ba1\\n1. CI\/CD - \u7528\u4e8e\u63a5\u6536\u6d4b\u8bd5\u3001\u90e8\u7f72\u7ed3\u679c\uff0c\u6d4b\u8bd5\u8986\u76d6\u7387\uff0cPR \u7b49\u4fe1\u606f\uff0c\u4e5f\u7528\u4e8e\u53d1\u8d77\u6d4b\u8bd5\u3001\u90e8\u7f72\u7b49\uff1b\\n2. NewRelic - \u7528\u4e8e\u63a5\u6536\u5e94\u7528\u6027\u80fd\u62a5\u8b66\u7b49\u4fe1\u606f\uff1b\\n3. Sentry - \u7ebf\u4e0a\u5b9e\u65f6\u9519\u8bef\u4fe1\u606f\uff0c\u53ef\u6839\u636e\u9879\u76ee\u5355\u72ec\u62c6\u51fa\u6765\uff1b\\n4. Team-X - \u7528\u4e8e\u7ec4\u5185\u6c9f\u901a\uff0c\u4e00\u4e2a Scrum \u7ec4\uff0c\u5305\u62ec\u7814\u53d1\uff0c\u4ea7\u54c1\u53ca\u6d4b\u8bd5\uff1b\\n5. Knowledge - \u7528\u4e8e\u6240\u6709\u7814\u53d1\u4eba\u5458\u8fdb\u884c\u6c9f\u901a\u4e0e\u5206\u4eab\uff1b\\n6. Product - \u7528\u4e8e\u8de8 Team \u7684\u4ea7\u54c1\u8fdb\u884c\u6c9f\u901a\u4e0e\u5206\u4eab\uff1b\\n7. Backend - \u7528\u4e8e\u6240\u6709\u540e\u7aef\u4eba\u5458\u8fdb\u884c\u95ee\u9898\u54a8\u8be2\u53ca\u5206\u4eab\uff1b\\n8. Leads(\u79c1\u5bc6) - \u7528\u4e8e\u6240\u6709 leader \u8fdb\u884c\u6c9f\u901a\u3001\u5b89\u6392\u5468\u4f1a\u7b49\uff1b\\n9. Frontend, UI, Mobile, Devops, QA etc\\n### \u6211\u4eec\u7528\u4e86\u5982\u4e0b\u7b2c\u4e09\u65b9\u8f6f\u4ef6\\n* Sentry\\n* NewRelic\\n* RedMine\\n* Teambition\\n* Confluence\\n* Github\\n* Bugly\\n* FIR.im\\n* Jenkins\\n* etc\\n### Slack vs BearyChat\\n* Slack \u4f18\u7f3a\u70b9\uff1a\\n* \u9f3b\u7956\\n* \u51e0\u4e4e\u6240\u6709\u5e38\u7528\u8f6f\u4ef6(\u56fd\u5916)\u90fd\u6709\u96c6\u6210\\n* \u529f\u80fd\u5b8c\u5584\u5065\u58ee\uff0c\u516c\u53f8\u53ef\u4fe1\\n* \u7f51\u7edc\u4e0d\u7a33\u5b9a\\n* \u56fd\u5185\u5e94\u7528\u96c6\u6210\u5f88\u5c11\\n* BearChat \u4f18\u7f3a\u70b9\uff1a\\n* \u57fa\u672c\u6709 Slack \u7684\u6838\u5fc3\u529f\u80fd\\n* \u6570\u636e\u7684\u9690\u79c1\u65e0\u6cd5\u4fdd\u8bc1\\n* \u670d\u52a1\u7684\u53ef\u7528\u6027\u65e0\u6cd5\u4fdd\u8bc1\\n* \u7b2c\u4e09\u65b9\u96c6\u6210\u592a\u5c11\uff08must-read, Simple Poll, etc \u90fd\u6ca1\u6709\uff09\uff0c\u5012\u662f\u96c6\u6210\u4e86\u56fd\u5185\u7684 Teambiton, \u76d1\u63a7\u5b9d\u7b49\\n\u6700\u7ec8\uff0c\u6211\u4eec\u7ed9\u56fd\u4ea7\u8f6f\u4ef6(\u670d\u52a1)\u4e00\u4e2a\u673a\u4f1a\u3002\\n\u7ecf\u8fc7\u4e24\u5468\u7684 BearyChat \u8bd5\u7528\uff0c\u76ee\u524d\u5df2\u51c6\u5907\u8f6c\u7528 Slack - 2017-08-03\\n1. BearyChat \u7684 Teambition \u96c6\u6210\u5931\u8d25\uff0c\u6c9f\u901a\u540e\u8bf4\u662f TB \u7684\u63a5\u53e3\u8c03\u6574\uff1b\\n2. \u96c6\u6210\u6570\u7684\u9650\u5236\uff0c\u90fd\u662f 10 \u4e2a\uff0c\u4f46\u540c\u4e00\u4e2a\u7b2c\u4e09\u65b9\u591a\u6b21\u96c6\u6210\uff0cSlack \u7b97\u4e00\u4e2a\uff0cBearyChat \u7b97\u591a\u4e2a\uff0c\u6bd4\u5982 Github, Sentry\uff1b\\n3. \u7ecf\u8fc7\u5bf9\u5404\u4e2a\u56fd\u5185\u516c\u53f8\u7684\u4e86\u89e3\uff0c\u53d1\u73b0\u4f7f\u7528\u4e2d Slack \u7684\u56fd\u5185\u7a33\u5b9a\u6027\u8fd8\u597d\u3002\\n","tokens":166,"id":2448,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\n1. \u6211\u4eec\u7684\u4e00\u4e9b\u667a\u80fd\u8bbe\u5907\uff0c\u901a\u8fc7 TCP \u548c\u540e\u7aef\u8fdb\u884c\u901a\u4fe1\uff0c\u4e0a\u62a5\u6570\u636e\u53ca\u63a5\u6536\u6307\u4ee4\uff1b\\n2. \u667a\u80fd\u8bbe\u5907\u7531\u7b2c\u4e09\u65b9\u63d0\u4f9b\uff0c\u6211\u4eec\u5b9e\u73b0\u540e\u7aef\u670d\u52a1\uff0c\u5f53\u524d\u652f\u6301\u57df\u540d\u548c IP \u8bf7\u6c42\uff1b\\n3. \u968f\u7740\u672a\u6765\u8bbe\u5907\u6570\u91cf\u7684\u589e\u957f\uff0c\u6211\u4eec\u7684\u5355\u53f0\u670d\u52a1\u9884\u8ba1\u5c06\u65e0\u6cd5\u6ee1\u8db3\uff1b\\n4. \u539f\u65b9\u6848\u662f\u9488\u5bf9\u4e0d\u540c\u6279\u6b21\u7684\u667a\u80fd\u8bbe\u5907\uff0c\u6211\u4eec\u7ed1\u5b9a\u4e0d\u540c\u7684\u57df\u540d\uff0c\u5df2\u505a\u5ba2\u6237\u7684\u8d1f\u8f7d\uff1b\\n5. \u5b58\u5728\u8bbe\u5907\u4f7f\u7528\u7684\u4e0d\u53ef\u63a7\u5f15\u8d77\u670d\u52a1\u7aef\u7684\u4f7f\u7528\u7387\u4f1a\u5b58\u5728\u95ee\u9898\uff1b\\n6. \u670d\u52a1\u5668\u51fa\u95ee\u9898\u540e\u670d\u52a1\u7684\u6062\u590d\u65f6\u95f4\u53d7\u9650\u4e8e DNS \u66f4\u65b0\u7684\u65f6\u95f4\uff08\u5207\u6362\u65b0\u673a\u5668\uff09\u3002\\n\\n## Decision\\n![][image-1]\\n\\n\\n## Context\\n1. \u6211\u4eec\u7684\u652f\u4ed8\u53ca\u8d22\u52a1\u4e1a\u52a1\uff0c\u9700\u8981\u5411\u7b2c\u4e09\u65b9\u91d1\u878d\u673a\u6784\u53d1\u8d77\u8bf7\u6c42\uff0c\u7b2c\u4e09\u65b9\u673a\u6784\u51fa\u4e8e\u5b89\u5168\u6027\u8003\u8651\uff0c\u9700\u8981\u5c06\u6211\u4eec\u7684\u670d\u52a1\u5668 IP \u5730\u5740\u8fdb\u884c\u62a5\u5907\uff1b\\n2. \u7b2c\u4e09\u65b9\u673a\u6784\u8f83\u591a\uff0c\u9488\u5bf9\u670d\u52a1\u5668\u6269\u5bb9\uff0c\u66f4\u6362\uff0c\u62a5\u5907\u4e00\u6b21\u6bd4\u8f83\u9ebb\u70e6\uff1b\\n3. \u7b2c\u4e09\u65b9\u673a\u6784\u5ba1\u6838\u65f6\u95f4\u4e0d\u5b9a\uff0c1 \u5929\u5230\u591a\u5929\u4e0d\u5b9a\uff0c\u5173\u952e\u65f6\u523b\u5f71\u54cd\u4e1a\u52a1\u3002\\n\\n## Decision\\n\u6b63\u5411\u4ee3\u7406\u662f\u4e00\u4e2a\u4f4d\u4e8e\u5ba2\u6237\u7aef\u548c\u76ee\u6807\u670d\u52a1\u5668\u4e4b\u95f4\u7684\u4ee3\u7406\u670d\u52a1\u5668(\u4e2d\u95f4\u670d\u52a1\u5668)\u3002\u4e3a\u4e86\u4ece\u539f\u59cb\u670d\u52a1\u5668\u53d6\u5f97\u5185\u5bb9\uff0c\u5ba2\u6237\u7aef\u5411\u4ee3\u7406\u670d\u52a1\u5668\u53d1\u9001\u4e00\u4e2a\u8bf7\u6c42\uff0c\u5e76\u4e14\u6307\u5b9a\u76ee\u6807\u670d\u52a1\u5668\uff0c\u4e4b\u540e\u4ee3\u7406\u5411\u76ee\u6807\u670d\u52a1\u5668\u8f6c\u4ea4\u5e76\u4e14\u5c06\u83b7\u5f97\u7684\u5185\u5bb9\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\u3002\u6b63\u5411\u4ee3\u7406\u7684\u60c5\u51b5\u4e0b\u5ba2\u6237\u7aef\u5fc5\u987b\u8981\u8fdb\u884c\u4e00\u4e9b\u7279\u522b\u7684\u8bbe\u7f6e\u624d\u80fd\u4f7f\u7528\u3002\\n\u5e38\u89c1\u573a\u666f\uff1a\\n![][image-1]\\n\u53cd\u5411\u4ee3\u7406\u6b63\u597d\u76f8\u53cd\u3002\u5bf9\u4e8e\u5ba2\u6237\u7aef\u6765\u8bf4\uff0c\u53cd\u5411\u4ee3\u7406\u5c31\u597d\u50cf\u76ee\u6807\u670d\u52a1\u5668\u3002\u5e76\u4e14\u5ba2\u6237\u7aef\u4e0d\u9700\u8981\u8fdb\u884c\u4efb\u4f55\u8bbe\u7f6e\u3002\u5ba2\u6237\u7aef\u5411\u53cd\u5411\u4ee3\u7406\u53d1\u9001\u8bf7\u6c42\uff0c\u63a5\u7740\u53cd\u5411\u4ee3\u7406\u5224\u65ad\u8bf7\u6c42\u8d70\u5411\u4f55\u5904\uff0c\u5e76\u5c06\u8bf7\u6c42\u8f6c\u4ea4\u7ed9\u5ba2\u6237\u7aef\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u5185\u5bb9\u5c31\u597d\u4f3c\u4ed6\u81ea\u5df1\u4e00\u6837\uff0c\u4e00\u6b21\u5ba2\u6237\u7aef\u5e76\u4e0d\u4f1a\u611f\u77e5\u5230\u53cd\u5411\u4ee3\u7406\u540e\u9762\u7684\u670d\u52a1\uff0c\u4e5f\u56e0\u6b64\u4e0d\u9700\u8981\u5ba2\u6237\u7aef\u505a\u4efb\u4f55\u8bbe\u7f6e\uff0c\u53ea\u9700\u8981\u628a\u53cd\u5411\u4ee3\u7406\u670d\u52a1\u5668\u5f53\u6210\u771f\u6b63\u7684\u670d\u52a1\u5668\u5c31\u597d\u4e86\u3002\\n\u5e38\u89c1\u573a\u666f\uff1a\\n![][image-2]\\n\u65b9\u6848\uff1a\\n1. \u4f7f\u7528\u6b63\u5411\u4ee3\u7406\uff1b\\n* \u8fd9\u4e2a\u573a\u666f\u6700\u5bb9\u6613\u60f3\u5230\u7684\u65b9\u6848\uff0c\u4f7f\u7528\u8d77\u6765\u76f4\u89c2\uff0c\u6613\u61c2\uff1b\\n* \u9700\u8981\u6bcf\u4e2a\u5ba2\u6237\u7aef\u8fdb\u884c\u914d\u7f6e\uff0c\u6216\u662f\u5728\u5e94\u7528\u4e2d\u5bf9\u5355\u4e2a\u8bf7\u6c42\u505a\u4ee3\u7406\u914d\u7f6e\uff1b\\n* \u9700\u8981\u7ef4\u62a4\u4e00\u4e2a\u9ad8\u53ef\u7528\u7684\u4ee3\u7406\u670d\u52a1\uff0c\u5e76\u5907\u6848\u6b64\u4ee3\u7406\u670d\u52a1\u5668\u3002\\n2. \u4f7f\u7528\u53cd\u5411\u4ee3\u7406\u3002\\n* \u5728\u6211\u4eec\u7684\u670d\u52a1\u5668\u4e0a\u505a\u5bf9\u65b9\u670d\u52a1\u7684\u53cd\u5411\u4ee3\u7406\uff08\u542c\u8d77\u6765\u6709\u70b9\u7ed5\uff0c\u4e0d\u76f4\u89c2\uff09\\n* \u7ef4\u62a4\u7b80\u5355\uff0c\u5c31\u50cf\u662f\u6211\u4eec\u7528 nginx\/slb \u4e3a\u5bf9\u65b9\u505a\u4e86\u4e2a\u8d1f\u8f7d\u5747\u8861\uff0c\u4f46\u914d\u7f6e\u7a0d\u6709\u4e0d\u540c\uff1b\\n```json\\nserver {\\nserver_name {{ proxy_info.server_name }};\\nlisten {{ proxy_info.ssl_listen }};\\nlocation \/ {\\nproxy_pass_header Server;\\nproxy_pass {{ proxy_info.proxy_url }};\\nproxy_redirect off;\\nproxy_set_header X-Real-IP $remote_addr;\\nproxy_set_header X-Scheme $scheme;\\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\n}\\n}\\n```\\n* \u4f7f\u7528\u7b80\u5355\uff0c\u4f7f\u7528\u65b9\u914d\u7f6e\u672c\u673a hosts \u5373\u53ef\uff0c\u4e3a\u5bf9\u65b9\u57df\u540d\u548c\u4ee3\u7406 IP \u6620\u5c04\u3002\\n3. iptables\\n* \u7c7b\u4f3c\u5c40\u57df\u7f51\u5171\u4eab\u4e0a\u7f51\uff1b\\n* \u5bf9 iptables \u914d\u7f6e\u6709\u8981\u6c42\uff1b\\n* \u76ee\u6807\u57df\u540d\u5bf9\u5e94\u7684 ip \u5730\u5740\u6539\u53d8\uff0c\u9700\u8981\u66f4\u65b0\u914d\u7f6e\u3002\\n\u6700\u7ec8\u6211\u4eec\u901a\u8fc7 aliyun slb \u76844\u5c42\u8d1f\u8f7d\u63a5\u4e24\u53f0\u90e8\u7f72\u4e86 ss5 \u7684\u673a\u5668\u63d0\u4f9b\u9ad8\u53ef\u7528\u7684\u4ee3\u7406\u670d\u52a1\\n* 4\u5c42\uff08TCP\u534f\u8bae\uff09\u670d\u52a1\u4e2d\uff0c\u5f53\u524d\u4e0d\u652f\u6301\u6dfb\u52a0\u8fdb\u540e\u7aef\u4e91\u670d\u52a1\u5668\u6c60\u7684ECS\u65e2\u4f5c\u4e3aReal Server\uff0c\u53c8\u4f5c\u4e3a\u5ba2\u6237\u7aef\u5411\u6240\u5728\u7684\u8d1f\u8f7d\u5747\u8861\u5b9e\u4f8b\u53d1\u9001\u8bf7\u6c42\u3002\\n* ss5 \u542f\u52a8\u4e8e\u5185\u7f51\u5730\u5740\u5373\u53ef\\n* ss5 \u914d\u7f6e\u9700\u5173\u6ce8 AUTHENTICATION \u548c AUTHORIZATION\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\n1. \u5f53\u524d\u4f7f\u7528\u5fae\u4fe1\u3001\u90ae\u4ef6\u4f5c\u4e3a\u5de5\u4f5c\u4e2d\u7684\u6c9f\u901a\u5de5\u5177\uff0c\u5de5\u4f5c\u4e0e\u751f\u6d3b\u6df7\u5408\uff1b\\\\n2. \u5404\u79cd\u544a\u8b66\u4fe1\u606f\u9700\u5206\u522b\u67e5\u770b\u90ae\u4ef6\u3001\u77ed\u4fe1\u6216\u662f\u76d1\u63a7\u5e73\u53f0\uff1b\\\\n3. \u968f\u7740\u4efb\u52a1\u7ba1\u7406\u5e73\u53f0\u3001Bug \u7ba1\u7406\u5e73\u53f0\u3001wiki\u3001\u6587\u6863\u5206\u4eab\u7f51\u7ad9\u3001\u804a\u5929\u5de5\u5177\u7684\u6d41\u884c\uff0c\u90ae\u4ef6\u7684\u4f7f\u7528\u573a\u666f\u8d8a\u6765\u8d8a\u5c0f\uff0c\u5e76\u4e14\u90ae\u4ef6\u4fe1\u606f\u5e76\u4e0d\u53ca\u65f6\uff1b\\\\n4. \u901a\u8fc7\u5fae\u4fe1\u8fdb\u884c\u6c9f\u901a\uff0c\u65b0\u4eba\u65e0\u6cd5\u4e86\u89e3\u5386\u53f2\u4fe1\u606f\uff0c\u7ecf\u5e38\u9700\u8981\u628a\u53d1\u8fc7\u7684\u5185\u5bb9\u91cd\u590d\u53d1\u9001\u3002\\\\n'}, {'role': 'model', 'content': '### Slack \u652f\u6301\u7684\u529f\u80fd\\\\n1. **\u516c\u5f00\u4e0e\u79c1\u6709\u7684\u9891\u9053\uff0c\u9891\u9053\u53ef\u968f\u65f6\u52a0\u5165\u6216\u9000\u51fa**\uff1b\\\\n1. \u6309\u9879\u76ee\uff0cproject-crm, project-sms, \u7b49\uff1b\\\\n2. \u6309\u6280\u672f\u8bdd\u9898\uff0ctech-restful-api, tech-queue \u7b49\uff1b\\\\n3. \u53ef\u4ee5\u9080\u8bf7\u76f8\u5173\u4eba\uff0c\u4efb\u4f55\u4eba\u4e5f\u53ef\u968f\u610f\u52a0\u5165\u3002\\\\n2. **\u6d88\u606f\u652f\u6301\u8868\u60c5\u5feb\u901f\u56de\u590d**\uff1b\\\\n1. \u8868\u60c5\u662f\u9644\u52a0\u5728\u6d88\u606f\u4e0a\u7684\uff0c\u4e0a\u4e0b\u6587\u5185\u805a\u5f88\u9ad8\u3002\\\\n3. \u6d88\u606f\u652f\u6301\u6536\u85cf\uff1b\\\\n1. \u4e00\u4e9b\u4e0d\u9519\u7684\u91cd\u8981\u4fe1\u606f\uff0c\u53ef\u4ee5\u6536\u85cf\u8d77\u6765\u968f\u65f6\u67e5\u770b\u3002\\\\n4. \u652f\u6301\u5404\u79cd\u6587\u4ef6\u7684\u5206\u4eab\uff1b\\\\n1. pdf \u7b49\u6587\u4ef6\u5747\u53ef\u9884\u89c8\u3002\\\\n5. **\u5206\u4eab\u7684\u94fe\u63a5\u652f\u6301\u9884\u89c8**\uff1b\\\\n1. \u5206\u4eab\u7684\u94fe\u63a5\uff0c\u4e0d\u7528\u70b9\u5f00\u4e5f\u77e5\u9053\u5927\u4f53\u5185\u5bb9\u3002\\\\n6. \u641c\u7d22\u529f\u80fd\u5f3a\u5927\uff0c\u53ef\u901a\u8fc7\u5feb\u6377\u65b9\u5f0f\uff0c\u641c\u7d22\u540c\u4e8b\uff0c\u6d88\u606f\u8bb0\u5f55\uff0c\u6587\u4ef6\u7b49\uff1b\\\\n7. \u591a\u79cd\u7a0b\u5e8f\u4ee3\u7801\u652f\u6301\u9ad8\u4eae\uff1b\\\\n1. \u4ee3\u7801\u9ad8\u4eae\u9884\u89c8\uff0c\u81ea\u52a8\u6298\u53e0\uff0c\u4e0d\u5f71\u54cd\u6574\u4f53\u6548\u679c\u3002\\\\n8. **\u5f3a\u5927\u7684\u7b2c\u4e09\u65b9\u96c6\u6210\uff0c\u5c06\u6240\u6709\u6d88\u606f\u3001\u901a\u77e5\u6c47\u805a\u5728\u4e00\u5904**\uff0c\u4f8b\u5982\uff0cTrello\uff0cGithub\uff0cNewRelic, Sentry\uff0cJenkins \u7b49\uff1b\\\\n9. **\u65b0\u52a0\u5165\u8005\u53ef\u67e5\u770b\u7fa4\u7ec4\u5386\u53f2\u4fe1\u606f**\u3002\\\\n1. \u4fe1\u606f\u518d\u4e5f\u4e0d\u7528\u91cd\u590d\u53d1\u4e86\u3002\\\\n10. \u5f00\u653e\u6027\u975e\u5e38\u597d\\\\n1. \u4efb\u4f55\u4eba\u90fd\u53ef\u4ee5\u65b9\u4fbf\u7533\u8bf7\u5f00\u53d1\u8005 KEY\uff0c\u5efa\u7acb\u81ea\u5df1\u7684\u673a\u5668\u4eba\u3002\\\\n### \u7814\u53d1\u90e8\u9891\u9053\u8bbe\u8ba1\\\\n1. CI\/CD - \u7528\u4e8e\u63a5\u6536\u6d4b\u8bd5\u3001\u90e8\u7f72\u7ed3\u679c\uff0c\u6d4b\u8bd5\u8986\u76d6\u7387\uff0cPR \u7b49\u4fe1\u606f\uff0c\u4e5f\u7528\u4e8e\u53d1\u8d77\u6d4b\u8bd5\u3001\u90e8\u7f72\u7b49\uff1b\\\\n2. NewRelic - \u7528\u4e8e\u63a5\u6536\u5e94\u7528\u6027\u80fd\u62a5\u8b66\u7b49\u4fe1\u606f\uff1b\\\\n3. Sentry - \u7ebf\u4e0a\u5b9e\u65f6\u9519\u8bef\u4fe1\u606f\uff0c\u53ef\u6839\u636e\u9879\u76ee\u5355\u72ec\u62c6\u51fa\u6765\uff1b\\\\n4. Team-X - \u7528\u4e8e\u7ec4\u5185\u6c9f\u901a\uff0c\u4e00\u4e2a Scrum \u7ec4\uff0c\u5305\u62ec\u7814\u53d1\uff0c\u4ea7\u54c1\u53ca\u6d4b\u8bd5\uff1b\\\\n5. Knowledge - \u7528\u4e8e\u6240\u6709\u7814\u53d1\u4eba\u5458\u8fdb\u884c\u6c9f\u901a\u4e0e\u5206\u4eab\uff1b\\\\n6. Product - \u7528\u4e8e\u8de8 Team \u7684\u4ea7\u54c1\u8fdb\u884c\u6c9f\u901a\u4e0e\u5206\u4eab\uff1b\\\\n7. Backend - \u7528\u4e8e\u6240\u6709\u540e\u7aef\u4eba\u5458\u8fdb\u884c\u95ee\u9898\u54a8\u8be2\u53ca\u5206\u4eab\uff1b\\\\n8. Leads(\u79c1\u5bc6) - \u7528\u4e8e\u6240\u6709 leader \u8fdb\u884c\u6c9f\u901a\u3001\u5b89\u6392\u5468\u4f1a\u7b49\uff1b\\\\n9. Frontend, UI, Mobile, Devops, QA etc\\\\n### \u6211\u4eec\u7528\u4e86\u5982\u4e0b\u7b2c\u4e09\u65b9\u8f6f\u4ef6\\\\n* Sentry\\\\n* NewRelic\\\\n* RedMine\\\\n* Teambition\\\\n* Confluence\\\\n* Github\\\\n* Bugly\\\\n* FIR.im\\\\n* Jenkins\\\\n* etc\\\\n### Slack vs BearyChat\\\\n* Slack \u4f18\u7f3a\u70b9\uff1a\\\\n* \u9f3b\u7956\\\\n* \u51e0\u4e4e\u6240\u6709\u5e38\u7528\u8f6f\u4ef6(\u56fd\u5916)\u90fd\u6709\u96c6\u6210\\\\n* \u529f\u80fd\u5b8c\u5584\u5065\u58ee\uff0c\u516c\u53f8\u53ef\u4fe1\\\\n* \u7f51\u7edc\u4e0d\u7a33\u5b9a\\\\n* \u56fd\u5185\u5e94\u7528\u96c6\u6210\u5f88\u5c11\\\\n* BearChat \u4f18\u7f3a\u70b9\uff1a\\\\n* \u57fa\u672c\u6709 Slack \u7684\u6838\u5fc3\u529f\u80fd\\\\n* \u6570\u636e\u7684\u9690\u79c1\u65e0\u6cd5\u4fdd\u8bc1\\\\n* \u670d\u52a1\u7684\u53ef\u7528\u6027\u65e0\u6cd5\u4fdd\u8bc1\\\\n* \u7b2c\u4e09\u65b9\u96c6\u6210\u592a\u5c11\uff08must-read, Simple Poll, etc \u90fd\u6ca1\u6709\uff09\uff0c\u5012\u662f\u96c6\u6210\u4e86\u56fd\u5185\u7684 Teambiton, \u76d1\u63a7\u5b9d\u7b49\\\\n\u6700\u7ec8\uff0c\u6211\u4eec\u7ed9\u56fd\u4ea7\u8f6f\u4ef6(\u670d\u52a1)\u4e00\u4e2a\u673a\u4f1a\u3002\\\\n\u7ecf\u8fc7\u4e24\u5468\u7684 BearyChat \u8bd5\u7528\uff0c\u76ee\u524d\u5df2\u51c6\u5907\u8f6c\u7528 Slack - 2017-08-03\\\\n1. BearyChat \u7684 Teambition \u96c6\u6210\u5931\u8d25\uff0c\u6c9f\u901a\u540e\u8bf4\u662f TB \u7684\u63a5\u53e3\u8c03\u6574\uff1b\\\\n2. \u96c6\u6210\u6570\u7684\u9650\u5236\uff0c\u90fd\u662f 10 \u4e2a\uff0c\u4f46\u540c\u4e00\u4e2a\u7b2c\u4e09\u65b9\u591a\u6b21\u96c6\u6210\uff0cSlack \u7b97\u4e00\u4e2a\uff0cBearyChat \u7b97\u591a\u4e2a\uff0c\u6bd4\u5982 Github, Sentry\uff1b\\\\n3. \u7ecf\u8fc7\u5bf9\u5404\u4e2a\u56fd\u5185\u516c\u53f8\u7684\u4e86\u89e3\uff0c\u53d1\u73b0\u4f7f\u7528\u4e2d Slack \u7684\u56fd\u5185\u7a33\u5b9a\u6027\u8fd8\u597d\u3002\\\\n'}]"}
{"File Name":"operational-data-hub\/0042-python-version-3.md","Context":"## Context\\nIn the past, there was a bit of a debate in the coding community about which Python version was the best one to learn: Python 2 vs Python 3 (or, specifically, Python 2.7 vs 3.5). Now, it's more of a no-brainer: Python 3 is the clear winner.\\n","Decision":"We will use python3  exclusively for all our python code.\\nMake sure you use version active maintained version (currently 3.6 or higher)\\n","tokens":72,"id":2713,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n### Context\\nThe app needs tests. This is a must. Although test driven development is best, the app added tests after development began to test logic and encourage future changes do not break functionality. It would be best to test all aspects of the MVVM architecture, but view models are where the majority of the app logic lives.\\n### Decision\\nSeveral testing frameworks exist for .NET apps, with the main three being xUnit, NUnit and MSTest. The app uses xUnit to test view models, but any of the other two frameworks would've also been suitable. I chose xUnit because the testing syntax looked cleaner, the framework was a bit newer, and I found some nice tutorials that recommended xUnit above the other two. Perhaps most importantly, I wanted to get started with testing instead of spending too much time debating the merits of which one to chose.\\nPerhaps just as important as which testing framework to use is which mocking package to use when faking API or other function calls in my tests. For that, Moq seems to be the clear winner. It's easy to find documentation online about using Moq with each testing framework.\\n### Status\\nAccepted\\n### Consequences\\nGetting started with testing is an undertaking, and it is very easy to let tests fall by the wayside when the touch table has a deadline and new features need to be added quickly. However, many xUnit tutorials seem incomplete and only test the view models. I'm curious if this is a sufficient amount of test coverage.\\n_In Retrospect:_ Unfortunately, I added tests later in the development process, and that caused some unnecessary delay while I was restructuring the architecture. This could have been avoided if I was a bit more familiar with how [Moq](https:\/\/www.nuget.org\/packages\/moq\/) worked from the outset of building the app. I also had to explore using [Unity](https:\/\/www.nuget.org\/packages\/Unity\/) to make sure dependency injection was in place for my unit tests. For more information in using Unity, refer to the `ContainerHelper` class in the `Lib` folder of the app.\\n\\n## Decision\\nSeveral testing frameworks exist for .NET apps, with the main three being xUnit, NUnit and MSTest. The app uses xUnit to test view models, but any of the other two frameworks would've also been suitable. I chose xUnit because the testing syntax looked cleaner, the framework was a bit newer, and I found some nice tutorials that recommended xUnit above the other two. Perhaps most importantly, I wanted to get started with testing instead of spending too much time debating the merits of which one to chose.\\nPerhaps just as important as which testing framework to use is which mocking package to use when faking API or other function calls in my tests. For that, Moq seems to be the clear winner. It's easy to find documentation online about using Moq with each testing framework.\\n### Status\\nAccepted\\n### Consequences\\nGetting started with testing is an undertaking, and it is very easy to let tests fall by the wayside when the touch table has a deadline and new features need to be added quickly. However, many xUnit tutorials seem incomplete and only test the view models. I'm curious if this is a sufficient amount of test coverage.\\n_In Retrospect:_ Unfortunately, I added tests later in the development process, and that caused some unnecessary delay while I was restructuring the architecture. This could have been avoided if I was a bit more familiar with how [Moq](https:\/\/www.nuget.org\/packages\/moq\/) worked from the outset of building the app. I also had to explore using [Unity](https:\/\/www.nuget.org\/packages\/Unity\/) to make sure dependency injection was in place for my unit tests. For more information in using Unity, refer to the `ContainerHelper` class in the `Lib` folder of the app.\\n\\n\\n## Context\\nCypress is currently ubiquitous in the end-to-end testing game, but there's a lot of things lacking.\\nPersonally, I've been excited about [Playwright](https:\/\/playwright.dev\/) for quite a while now, and I think it has the potential to unseat Cypress as a de facto e2e testing solution. Previously, the lack of a test runner made Playwright difficult to use as a testing framework \u2014 it was better suited to general-purpose browser automation. However, the addition of `playwright-test` changes this up.\\nI'm using my website as a trial run of what Playwright can do and whether it holds up to Cypress as a general testing solution. \ud83e\udd1e\\n\\n## Decision\\nReplace existing Cypress tests with end-to-end tests written in Playwright.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nIn the past, there was a bit of a debate in the coding community about which Python version was the best one to learn: Python 2 vs Python 3 (or, specifically, Python 2.7 vs 3.5). Now, it's more of a no-brainer: Python 3 is the clear winner.\\\\n\"}, {'role': 'model', 'content': 'We will use python3  exclusively for all our python code.\\\\nMake sure you use version active maintained version (currently 3.6 or higher)\\\\n'}]"}
{"File Name":"beis-report-official-development-assistance\/0003-use-standard-rb.md","Context":"## Context\\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\\n","Decision":"We will use [Standard.rb](https:\/\/github.com\/testdouble\/standard) and run the standard.rb rake task to lint the code as part of the test suite.\\n","tokens":36,"id":2382,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\\n\\n## Decision\\nWe will use [Standard.rb](https:\/\/github.com\/testdouble\/standard) and run the standard.rb rake task to lint the code as part of the test suite.\\n\\n\\n## Context\\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\\n\\n## Decision\\nWe will use [Standard.rb](https:\/\/github.com\/testdouble\/standard) and run the standard.rb rake task to lint the code as part of the test suite.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\\\\n'}, {'role': 'model', 'content': 'We will use [Standard.rb](https:\/\/github.com\/testdouble\/standard) and run the standard.rb rake task to lint the code as part of the test suite.\\\\n'}]"}
{"File Name":"smjs\/2018112801-encapsulate-sma-stack-usage.md","Context":"## Context\\nReal world assembly programming exposes the stack pointer and the base pointer, allowing assembly code to directly mess with them, for example changing the stack pointer regardless of how `push` and `pop` instructions have been used. Despite allowing wild optimizations, this is really error prone and, if used extensively, makes programs very obscure: for these reasons, we don't want to support such things in our SMA architecture.\\n","Decision":"To prevent code from messing up the stack, we'll avoid adding public stack pointer and base pointer registers. Rather, these two concepts will still be used in the implementation of the stack object, but clients won't be able to directly use them, since the only interfaces allowed will be the usual `push` and `pop`, which will take care of properly updating the stack pointer and the base pointer.\\nSince pointers are not directly usable from the code, we need to add a way to abstract the concept of stack frame: this is the role of the `pushFrame` and `popFrame` methods: the first one takes the return address, and takes care of pushing it on the stack at the right moment, while the second one will return the previously stored return address.\\nAnother problem with encapsulating frame handling, is that of passing procedure arguments to the stack. In regular assembly, one first pushes the arguments, then pushes the return address and the base pointer, and the jumps to the procedure. From inside the procedure, to get the passed arguments it's then necessary to first temporarily pop the base pointer and the return address, then pop the arguments, and then push back return address and base pointer.\\nSince we don't want to write all this boilerplate every time we call a procedure, we'll provide an alternative `call` instruction, supporting argument passing: this instruction will take the size of the pushed arguments (in bytes), and will automatically move the pushed arguments over the return address and the base pointer, after having pushed these, so that the procedure code will immediately find the arguments at the top of the stack, without the need to do any boilerplate.\\n","tokens":85,"id":3977,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nMenes supports C# 8.0's nullable references feature. In most cases, libraries need to use some of the attributes from the `System.Diagnostics.CodeAnalysis` namespace that enable to you provide sufficient information for the compiler's null analysis to do a good job.\\nThese attributes are not available in `netstandard2.0`. However, there is a standard workaround: define your own copies of these attributes and use those. We are using the `Nullable` NuGet package to do this for us. This works nicely, enabling applications targeting older runtimes still to enable nullable references.\\nThe problem is that you don't want to use this workaround unless you have to. Newer versions of .NET Core and .NET Standard have these attributes, so it's just a waste of space to define your own.\\n\\n## Decision\\nMenes will target both .NET Standard 2.0 and .NET Standard 2.1. The .NET Standard 2.0 version brings its own copies of the attributes, the .NET Standard 2.1 version relies on the ones built into the framework.\\n\\n\\n## Context\\nThis code generation experiment is meant to be our starting point for both the API unification for ND4J and SameDiff,\\nand the multi-language support. For this reason we have to define ops, or their interface, in a language neutral way.\\nThe initial idea was to use a Language Workbench like MPS. This had to be discarded because of bugs and limitations\\nencountered while trying to define a language that would work for a few simple examples.\\nThe next idea was to use Ops defined in JSON files. This would have allowed us to define Ops as human readable data and\\nread and write those files from any programming language. However, the drawback with this approach is that writing json\\nmanually invites many problems if written manually (e.g. typos, bad structuring, having to look up the proper keys,...).\\nIn order to rectify that drawback, we would have to create custom tooling, that we would have to maintain and that\\ncontributors would have to use.\\nUsing a Java builder pattern based approach is very verbose.\\n\\n## Decision\\nWe use a Kotlin-based DSL to define Ops.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nReal world assembly programming exposes the stack pointer and the base pointer, allowing assembly code to directly mess with them, for example changing the stack pointer regardless of how `push` and `pop` instructions have been used. Despite allowing wild optimizations, this is really error prone and, if used extensively, makes programs very obscure: for these reasons, we don't want to support such things in our SMA architecture.\\\\n\"}, {'role': 'model', 'content': \"To prevent code from messing up the stack, we'll avoid adding public stack pointer and base pointer registers. Rather, these two concepts will still be used in the implementation of the stack object, but clients won't be able to directly use them, since the only interfaces allowed will be the usual `push` and `pop`, which will take care of properly updating the stack pointer and the base pointer.\\\\nSince pointers are not directly usable from the code, we need to add a way to abstract the concept of stack frame: this is the role of the `pushFrame` and `popFrame` methods: the first one takes the return address, and takes care of pushing it on the stack at the right moment, while the second one will return the previously stored return address.\\\\nAnother problem with encapsulating frame handling, is that of passing procedure arguments to the stack. In regular assembly, one first pushes the arguments, then pushes the return address and the base pointer, and the jumps to the procedure. From inside the procedure, to get the passed arguments it's then necessary to first temporarily pop the base pointer and the return address, then pop the arguments, and then push back return address and base pointer.\\\\nSince we don't want to write all this boilerplate every time we call a procedure, we'll provide an alternative `call` instruction, supporting argument passing: this instruction will take the size of the pushed arguments (in bytes), and will automatically move the pushed arguments over the return address and the base pointer, after having pushed these, so that the procedure code will immediately find the arguments at the top of the stack, without the need to do any boilerplate.\\\\n\"}]"}
{"File Name":"datalab\/0041-user-driven-dask-and-spark.md","Context":"## Context\\nPreviously we have provisioned centralised Dask & Spark clusters which users can consume\\nfrom notebook environments. However since this decision a number of other options\\nhave emerged, specifically being able to use the native Kubernetes scheduler as Dask & Spark\\nschedulers.\\nWe are now moving to a pattern of users being able to spin up their own clusters\\nwhen required.\\n","Decision":"We have decided to collapse the centralised Dask & Spark clusters in favour of writing\\ndocumentation\/working with users to provision their own clusters using projects such\\nas [Dask Labextension](https:\/\/github.com\/dask\/dask-labextension), which is now supported\\nwithin the Labs environment.\\n","tokens":78,"id":732,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nDuring the build of the Data Submission Service, we need to build and deploy\\nthe applications and their dependencies in a repeatable manner. We also need\\nthe ability to develop and run the applications locally.\\nThere are a number of approaches to managing this including configuring a\\ncombination of configuration management and virtualisation tools (like puppet\\nand chef combined with vagrant).\\nIdeally, we'd like to separate running the application from the server operating\\nsystem, so we can make use of Platform as a Service type hosting options. This\\nsuggests that containerisation using a tool like Docker would be useful.\\nContainerisation is a well understood approach for doing this, and is well\\nsupported in all the major cloud providers - with many providing dedicated\\ncontainer services.\\n\\n## Decision\\nWe will use Docker to package applications so that they can be deployed.\\nWe will look to use a Platform as a Service offering for deploying the Docker\\ncontainers.\\n\\n\\n## Context\\nThe JASMIN cloud portal and vCloud director portals give manual options for provisioning\\nservers into the JASMIN tennancy. This brings significant effort to rebuild a cluster\\nas all servers would need to be manually deleted and recreated. The new OpenStack\\ninfrastructure is supposed to be available in a matter of months and we need to decide\\nwhether we want to invest effort in automating server creation.\\nIf we do decide to automate we need to decide which technology to use between Ansible\\nand Terraform. Terraform is known to work with vCloud director but it would be preferable\\nto use the same tool for server creation as software provisioning.\\n\\n## Decision\\nWe have decided to use Ansible as our server provisioning tool as there are significant\\nbenefits to keeping a single tool for all provisioning steps.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nPreviously we have provisioned centralised Dask & Spark clusters which users can consume\\\\nfrom notebook environments. However since this decision a number of other options\\\\nhave emerged, specifically being able to use the native Kubernetes scheduler as Dask & Spark\\\\nschedulers.\\\\nWe are now moving to a pattern of users being able to spin up their own clusters\\\\nwhen required.\\\\n\"}, {'role': 'model', 'content': 'We have decided to collapse the centralised Dask & Spark clusters in favour of writing\\\\ndocumentation\/working with users to provision their own clusters using projects such\\\\nas [Dask Labextension](https:\/\/github.com\/dask\/dask-labextension), which is now supported\\\\nwithin the Labs environment.\\\\n'}]"}
{"File Name":"Wikibase\/0016-use-github-actions-as-secondary-ci.md","Context":"## Context\\nThe Wikibase CI which runs on WMF Jenkins is currently \"augmented\" by running php unit tests in a variety of additional configurations (e.g. non-English wiki, repo\/client-only environment, etc) on legacy Travis CI infrastructure, via the GitHub mirror of our Gerrit code repository, see [https:\/\/travis-ci.org\/github\/wikimedia\/Wikibase](https:\/\/travis-ci.org\/github\/wikimedia\/Wikibase).\\nThe Travis CI features we currently use include:\\n* PHP installed (multiple versions),\\n* MySQL running,\\n* notifications via\\n* email,\\n* IRC,\\n* composer cache.\\nDue to a change in their business model, the travis-ci.org service is being phased out and replaced by (paid) travis-ci.com. Since we have no intention of dropping the extended CI testing, there are several options how to handle the situation:\\n* migrate additional CI for Wikibase to some other CI infrastructure, e.g. _GitHub Actions_,\\n* negotiate with the WMF changing the Travis CI plan to a paid one which would have unlimited\/less limited resources available,\\n* migrate additional CI for Wikibase to additional jobs on WMF Jenkins CI.\\n","Decision":"We will migrate the additional CI for Wikibase to the GitHub CI infrastructure, using _GitHub Actions_.\\nReasons: The GitHub mirror of the Wikibase repository exists already and all of the Travis CI features we have been using so far, are available on _GitHub Actions_:\\n* [setup-php](https:\/\/github.com\/shivammathur\/setup-php),\\n* MySQL pre-installed on the runner or use [MySQL service container](https:\/\/firefart.at\/post\/using-mysql-service-with-github-actions\/) (to be investigated),\\n* notifications via\\n* [email](https:\/\/docs.github.com\/en\/free-pro-team@latest\/github\/managing-subscriptions-and-notifications-on-github\/configuring-notifications) (built-in),\\n* [notify-irc](https:\/\/github.com\/rectalogic\/notify-irc) + [failure() condition](https:\/\/docs.github.com\/en\/free-pro-team@latest\/actions\/reference\/context-and-expression-syntax-for-github-actions#failure),\\n* [cache](https:\/\/github.com\/actions\/cache).\\n","tokens":252,"id":1342,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe cloud platforms team currently use Docker Registry for storage of docker images. This solution is self-hosted, needs regular patching, and occasionally has downtime.\\nExample of an issue [CPT-274](https:\/\/dsdmoj.atlassian.net\/browse\/CPT-274).\\nWe want to update the container registry to avoid some of the problems we have been seeing with it. The container registry will also be a key part of our new [Kubernetes based infrastructure](https:\/\/github.com\/ministryofjustice\/cloud-platform\/blob\/master\/architecture-decision-record\/004-use-kubernetes-for-container-management.md).\\nThe criteria for selecting a new solution included:\\n* Finding a solution that would work with well GitHub based identity which is one of our [major architectural choices](https:\/\/github.com\/ministryofjustice\/cloud-platform\/blob\/master\/architecture-decision-record\/006-Use-github-as-user-directory.md)\\n* Decreasing the amount of day to day ops work, perhaps by using a managed service\\n* Good value for money\\n* Preference for open source tooling\\n* Can or is able to run addons i.e. security scanners\\n* We can make images that we host public by default\\nFrom this process we tentatively chose ECR. Unfortunately none of the SaaS registries in common use support Github authentication, but as ECR uses IAM for authentication, and IAM supports federated identity with Github via Auth0, it will in theory be possible to define ECR access policies that reference Github teams and roles.\\n\\n## Decision\\nWe will replace our self hosted Docker Registry to managed Elastic Container Registry (ECR).\\n\\n\\n## Context\\nThe static content on the Zooniverse (notably, the About, Publications, and Team pages on [PFE](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/)) has traditionally been [baked](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/blob\/master\/app\/locales\/en.js) [into](https:\/\/github.com\/zooniverse\/Panoptes-Front-End\/blob\/master\/app\/lib\/publications.js) the front end code.\\nWhile this works, it creates maintenance issues: to update the list of publications, for example, requires someone to dig through the code to find and modify the correct JSON file, and then create a pull request on GitHub. To do so for multiple publications becomes a long and tedious process.\\nAdditionally, JSON is fragile - a missing (or extra) comma can break the build.\\nAs a result, we\\'re considering using a content management system (CMS) that would allow us to more easily maintain our static content.\\nThese are the options we have:\\n1. Continue to hard code content into the front end code\\n2. Create our own CMS to manage our content\\n3. Install a third party CMS into one of our servers\\n4. Use an online third party CMS\\n\\n## Decision\\nWe\\'ve decided to use [Contentful](https:\/\/www.contentful.com\/), an online third party CMS, to host our content. (Or at least, part of our content, such as the often-changing Publications page.) There will be two users registered: one admin account with full access to the Content Model editor _and_ content, and an editor account for day-to-day use which only has content editing access.\\n**Reasons for choosing Contentful**\\n- [@rogerhutchings](https:\/\/github.com\/rogerhutchings) has used it for projects in the past\\n- It has a free tier\\n- It has a well-documented API\\n- It has an excellent admin interface for editing content and models\\n- It\\'s one of the larger hosted CMS offerings, meaning it\\'s less likely to disappear overnight\\n**Pros**\\n- CMS makes content management easier and updates faster, especially for non-developers.\\n- External platform reduces the need for any sort of code maintenance.\\n- We can run our Zooniverse content on the Free Tier (see [Note of Caution 3: Database Limits](#note-of-caution-3-database-limits))\\n**Cons**\\n- Reliance on external platfrom for storing data introduces the usual third-party risks. (See [Note of Caution 1: Exit Strategy](#note-of-caution-1-exit-strategy)) Possible dangers include:\\n- external CMS has downtime, shutting down our content by extension.\\n- external CMS changes pricing structure, forcing us out of the free tier.\\n- external CMS shuts down, forcing us to find an alternative data store.\\n- We need to maintain an admin account. (Shouldn\\'t be a worry as long as we have Passbolt.)\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThe Wikibase CI which runs on WMF Jenkins is currently \"augmented\" by running php unit tests in a variety of additional configurations (e.g. non-English wiki, repo\/client-only environment, etc) on legacy Travis CI infrastructure, via the GitHub mirror of our Gerrit code repository, see [https:\/\/travis-ci.org\/github\/wikimedia\/Wikibase](https:\/\/travis-ci.org\/github\/wikimedia\/Wikibase).\\\\nThe Travis CI features we currently use include:\\\\n* PHP installed (multiple versions),\\\\n* MySQL running,\\\\n* notifications via\\\\n* email,\\\\n* IRC,\\\\n* composer cache.\\\\nDue to a change in their business model, the travis-ci.org service is being phased out and replaced by (paid) travis-ci.com. Since we have no intention of dropping the extended CI testing, there are several options how to handle the situation:\\\\n* migrate additional CI for Wikibase to some other CI infrastructure, e.g. _GitHub Actions_,\\\\n* negotiate with the WMF changing the Travis CI plan to a paid one which would have unlimited\/less limited resources available,\\\\n* migrate additional CI for Wikibase to additional jobs on WMF Jenkins CI.\\\\n'}, {'role': 'model', 'content': 'We will migrate the additional CI for Wikibase to the GitHub CI infrastructure, using _GitHub Actions_.\\\\nReasons: The GitHub mirror of the Wikibase repository exists already and all of the Travis CI features we have been using so far, are available on _GitHub Actions_:\\\\n* [setup-php](https:\/\/github.com\/shivammathur\/setup-php),\\\\n* MySQL pre-installed on the runner or use [MySQL service container](https:\/\/firefart.at\/post\/using-mysql-service-with-github-actions\/) (to be investigated),\\\\n* notifications via\\\\n* [email](https:\/\/docs.github.com\/en\/free-pro-team@latest\/github\/managing-subscriptions-and-notifications-on-github\/configuring-notifications) (built-in),\\\\n* [notify-irc](https:\/\/github.com\/rectalogic\/notify-irc) + [failure() condition](https:\/\/docs.github.com\/en\/free-pro-team@latest\/actions\/reference\/context-and-expression-syntax-for-github-actions#failure),\\\\n* [cache](https:\/\/github.com\/actions\/cache).\\\\n'}]"}
{"File Name":"gatemint-sdk\/adr-016-validator-consensus-key-rotation.md","Context":"## Context\\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos-SDK.\\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos-SDK.\\n","Decision":"### Pseudo procedure for consensus key rotation\\n- create new random consensus key.\\n- create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\\n- old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\\n- start validating with new consensus key.\\n- validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\\n### Considerations\\n- consensus key mapping information management strategy\\n- store history of each key mapping changes in the kvstore.\\n- the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\\n- the state machine does not need any historical mapping information which is past more than unbonding period.\\n- key rotation costs related to LCD and IBC\\n- LCD and IBC will have traffic\/computation burden when there exists frequent power changes\\n- In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\\n- Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\\n- limits\\n- a validator cannot rotate its consensus key more than `MaxConsPubKeyRotations` time for any unbonding period, to prevent spam.\\n- parameters can be decided by governance and stored in genesis file.\\n- key rotation fee\\n- a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\\n- `KeyRotationFee` = (max(`VotingPowerPercentage` * 100, 1) * `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\\n- evidence module\\n- evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\\n- abci.ValidatorUpdate\\n- tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\\n- validator consensus key update can be done via creating new + delete old by change the power to zero.\\n- therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\\n- new genesis parameters in `staking` module\\n- `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\\n- `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\\n### Workflow\\n1. The validator generates a new consensus keypair.\\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\\n```go\\ntype MsgRotateConsPubKey struct {\\nValidatorAddress  sdk.ValAddress\\nNewPubKey         crypto.PubKey\\n}\\n```\\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\\n4. `RotateConsPubKey`\\n- checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\\n- checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\\n- checks if the signing account has enough balance to pay `KeyRotationFee`\\n- pays `KeyRotationFee` to community fund\\n- overwrites `NewPubKey` in `validator.ConsPubKey`\\n- deletes old `ValidatorByConsAddr`\\n- `SetValidatorByConsAddr` for `NewPubKey`\\n- Add `ConsPubKeyRotationHistory` for tracking rotation\\n```go\\ntype ConsPubKeyRotationHistory struct {\\nOperatorAddress         sdk.ValAddress\\nOldConsPubKey           crypto.PubKey\\nNewConsPubKey           crypto.PubKey\\nRotatedHeight           int64\\n}\\n```\\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\\n```go\\nabci.ValidatorUpdate{\\nPubKey: tmtypes.TM2PB.PubKey(OldConsPubKey),\\nPower:  0,\\n}\\nabci.ValidatorUpdate{\\nPubKey: tmtypes.TM2PB.PubKey(NewConsPubKey),\\nPower:  v.ConsensusPower(),\\n}\\n```\\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\\n- Note : All above features shall be implemented in `staking` module.\\n","tokens":173,"id":35,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n### Context\\nThere has been interest from other teams using LRUD that the ability to _cancel_ a movement as it's happening would be useful.\\nFor example, focussing on a node and pressing down would ordinarily take you to the specified node, but perhaps the developer wants to run some business logic at that point that would mean that _actually we don't want the move to happen._\\n### Decision\\nWe have decided to implement this feature in LRUD. It will be useful for the specific team that requested the feature, and as further discussions have happened, all interested parties agree that there are valid use cases in many different scenarios for such a feature.\\n### Status\\nApproved\\n### Consequences\\nIt makes LRUD internally more complex (and alongside that makes the final bundle larger too). However, it is only marginally increasing the bundle size, and we feel the complexity is managable and well understood.\\n### Further Reading\\n- [Github issue discussing topic of cancellable movement](https:\/\/github.com\/bbc\/lrud\/issues\/25)\\n\\n## Decision\\nWe have decided to implement this feature in LRUD. It will be useful for the specific team that requested the feature, and as further discussions have happened, all interested parties agree that there are valid use cases in many different scenarios for such a feature.\\n### Status\\nApproved\\n### Consequences\\nIt makes LRUD internally more complex (and alongside that makes the final bundle larger too). However, it is only marginally increasing the bundle size, and we feel the complexity is managable and well understood.\\n### Further Reading\\n- [Github issue discussing topic of cancellable movement](https:\/\/github.com\/bbc\/lrud\/issues\/25)\\n\\n\\n## Context\\nThe `p2p` package responsible for peer-to-peer networking is rather old and has a number of weaknesses, including tight coupling, leaky abstractions, lack of tests, DoS vulnerabilites, poor performance, custom protocols, and incorrect behavior. A refactor has been discussed for several years ([#2067](https:\/\/github.com\/tendermint\/tendermint\/issues\/2067)).\\nInformal Systems are also building a Rust implementation of Tendermint, [Tendermint-rs](https:\/\/github.com\/informalsystems\/tendermint-rs), and plan to implement P2P networking support over the next year. As part of this work, they have requested adopting e.g. [QUIC](https:\/\/datatracker.ietf.org\/doc\/draft-ietf-quic-transport\/) as a transport protocol instead of implementing the custom application-level `MConnection` stream multiplexing protocol that Tendermint currently uses.\\nThis ADR summarizes recent discussion with stakeholders on the scope of a P2P refactor. Specific designs and implementations will be submitted as separate ADRs.\\n\\n## Decision\\nThe P2P stack will be refactored and improved iteratively, in several phases:\\n* **Phase 1:** code and API refactoring, maintaining protocol compatibility as far as possible.\\n* **Phase 2:** additional transports and incremental protocol improvements.\\n* **Phase 3:** disruptive protocol changes.\\nThe scope of phases 2 and 3 is still uncertain, and will be revisited once the preceding phases have been completed as we'll have a better sense of requirements and challenges.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nValidator consensus key rotation feature has been discussed and requested for a long time, for the sake of safer validator key management policy (e.g. https:\/\/github.com\/tendermint\/tendermint\/issues\/1136). So, we suggest one of the simplest form of validator consensus key rotation implementation mostly onto Cosmos-SDK.\\\\nWe don't need to make any update on consensus logic in Tendermint because Tendermint does not have any mapping information of consensus key and validator operator key, meaning that from Tendermint point of view, a consensus key rotation of a validator is simply a replacement of a consensus key to another.\\\\nAlso, it should be noted that this ADR includes only the simplest form of consensus key rotation without considering multiple consensus keys concept. Such multiple consensus keys concept shall remain a long term goal of Tendermint and Cosmos-SDK.\\\\n\"}, {'role': 'model', 'content': \"### Pseudo procedure for consensus key rotation\\\\n- create new random consensus key.\\\\n- create and broadcast a transaction with a `MsgRotateConsPubKey` that states the new consensus key is now coupled with the validator operator with signature from the validator's operator key.\\\\n- old consensus key becomes unable to participate on consensus immediately after the update of key mapping state on-chain.\\\\n- start validating with new consensus key.\\\\n- validators using HSM and KMS should update the consensus key in HSM to use the new rotated key after the height `h` when `MsgRotateConsPubKey` committed to the blockchain.\\\\n### Considerations\\\\n- consensus key mapping information management strategy\\\\n- store history of each key mapping changes in the kvstore.\\\\n- the state machine can search corresponding consensus key paired with given validator operator for any arbitrary height in a recent unbonding period.\\\\n- the state machine does not need any historical mapping information which is past more than unbonding period.\\\\n- key rotation costs related to LCD and IBC\\\\n- LCD and IBC will have traffic\/computation burden when there exists frequent power changes\\\\n- In current Tendermint design, consensus key rotations are seen as power changes from LCD or IBC perspective\\\\n- Therefore, to minimize unnecessary frequent key rotation behavior, we limited maximum number of rotation in recent unbonding period and also applied exponentially increasing rotation fee\\\\n- limits\\\\n- a validator cannot rotate its consensus key more than `MaxConsPubKeyRotations` time for any unbonding period, to prevent spam.\\\\n- parameters can be decided by governance and stored in genesis file.\\\\n- key rotation fee\\\\n- a validator should pay `KeyRotationFee` to rotate the consensus key which is calculated as below\\\\n- `KeyRotationFee` = (max(`VotingPowerPercentage` * 100, 1) * `InitialKeyRotationFee`) * 2^(number of rotations in `ConsPubKeyRotationHistory` in recent unbonding period)\\\\n- evidence module\\\\n- evidence module can search corresponding consensus key for any height from slashing keeper so that it can decide which consensus key is supposed to be used for given height.\\\\n- abci.ValidatorUpdate\\\\n- tendermint already has ability to change a consensus key by ABCI communication(`ValidatorUpdate`).\\\\n- validator consensus key update can be done via creating new + delete old by change the power to zero.\\\\n- therefore, we expect we even do not need to change tendermint codebase at all to implement this feature.\\\\n- new genesis parameters in `staking` module\\\\n- `MaxConsPubKeyRotations` : maximum number of rotation can be executed by a validator in recent unbonding period. default value 10 is suggested(11th key rotation will be rejected)\\\\n- `InitialKeyRotationFee` : the initial key rotation fee when no key rotation has happened in recent unbonding period. default value 1atom is suggested(1atom fee for the first key rotation in recent unbonding period)\\\\n### Workflow\\\\n1. The validator generates a new consensus keypair.\\\\n2. The validator generates and signs a `MsgRotateConsPubKey` tx with their operator key and new ConsPubKey\\\\n```go\\\\ntype MsgRotateConsPubKey struct {\\\\nValidatorAddress  sdk.ValAddress\\\\nNewPubKey         crypto.PubKey\\\\n}\\\\n```\\\\n3. `handleMsgRotateConsPubKey` gets `MsgRotateConsPubKey`, calls `RotateConsPubKey` with emits event\\\\n4. `RotateConsPubKey`\\\\n- checks if `NewPubKey` is not duplicated on `ValidatorsByConsAddr`\\\\n- checks if the validator is does not exceed parameter `MaxConsPubKeyRotations` by iterating `ConsPubKeyRotationHistory`\\\\n- checks if the signing account has enough balance to pay `KeyRotationFee`\\\\n- pays `KeyRotationFee` to community fund\\\\n- overwrites `NewPubKey` in `validator.ConsPubKey`\\\\n- deletes old `ValidatorByConsAddr`\\\\n- `SetValidatorByConsAddr` for `NewPubKey`\\\\n- Add `ConsPubKeyRotationHistory` for tracking rotation\\\\n```go\\\\ntype ConsPubKeyRotationHistory struct {\\\\nOperatorAddress         sdk.ValAddress\\\\nOldConsPubKey           crypto.PubKey\\\\nNewConsPubKey           crypto.PubKey\\\\nRotatedHeight           int64\\\\n}\\\\n```\\\\n5. `ApplyAndReturnValidatorSetUpdates` checks if there is `ConsPubKeyRotationHistory` with `ConsPubKeyRotationHistory.RotatedHeight == ctx.BlockHeight()` and if so, generates 2 `ValidatorUpdate` , one for a remove validator and one for create new validator\\\\n```go\\\\nabci.ValidatorUpdate{\\\\nPubKey: tmtypes.TM2PB.PubKey(OldConsPubKey),\\\\nPower:  0,\\\\n}\\\\nabci.ValidatorUpdate{\\\\nPubKey: tmtypes.TM2PB.PubKey(NewConsPubKey),\\\\nPower:  v.ConsensusPower(),\\\\n}\\\\n```\\\\n6. at `previousVotes` Iteration logic of `AllocateTokens`,  `previousVote` using `OldConsPubKey` match up with `ConsPubKeyRotationHistory`, and replace validator for token allocation\\\\n7. Migrate `ValidatorSigningInfo` and `ValidatorMissedBlockBitArray` from `OldConsPubKey` to `NewConsPubKey`\\\\n- Note : All above features shall be implemented in `staking` module.\\\\n\"}]"}
{"File Name":"nr-arch\/2020-03-14-ARCH-record-architecture-decisions-detailed-template.md","Context":"## Context\\nThe issue motivatina# [short title of solved problem and solution]\\nWhat is the issue that we're seeing that is motivating this decision or change?\\n* Status: [proposed | rejected | accepted | deprecated | ... | superseded by [ADR-0005](0005-example.md)] <!-- optional -->\\n* Deciders: [list everyone involved in the decision] <!-- optional -->\\n* Date: [YYYY-MM-DD when the decision was last updated] <!-- optional -->\\nTechnical Story: [description | ticket\/issue URL] <!-- optional -->\\n## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\n","Decision":"* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | ... | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, ...]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, ...]\\n* ...\\nWhat is the change that we're proposing and\/or doing?\\nThe change that we're proposing or have agreed to implement.\\n","tokens":218,"id":4800,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe issue motivatina# [short title of solved problem and solution]\\nWhat is the issue that we\\'re seeing that is motivating this decision or change?\\n* Status: [proposed | rejected | accepted | deprecated | ... | superseded by [ADR-0005](0005-example.md)] <!-- optional -->\\n* Deciders: [list everyone involved in the decision] <!-- optional -->\\n* Date: [YYYY-MM-DD when the decision was last updated] <!-- optional -->\\nTechnical Story: [description | ticket\/issue URL] <!-- optional -->\\n## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\n\\n## Decision\\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | ... | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, ...]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, ...]\\n* ...\\nWhat is the change that we\\'re proposing and\/or doing?\\nThe change that we\\'re proposing or have agreed to implement.\\n\\n\\n## Context and Problem Statement <!-- required -->\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\n\\n## Decision\\n* [driver 1, e.g., a force, facing concern, \u2026]\\n* [driver 2, e.g., a force, facing concern, \u2026]\\n* ... <!-- numbers of drivers can vary -->\\nTBD\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* ...\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThe issue motivatina# [short title of solved problem and solution]\\\\nWhat is the issue that we\\'re seeing that is motivating this decision or change?\\\\n* Status: [proposed | rejected | accepted | deprecated | ... | superseded by [ADR-0005](0005-example.md)] <!-- optional -->\\\\n* Deciders: [list everyone involved in the decision] <!-- optional -->\\\\n* Date: [YYYY-MM-DD when the decision was last updated] <!-- optional -->\\\\nTechnical Story: [description | ticket\/issue URL] <!-- optional -->\\\\n## Context and Problem Statement\\\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\\\n## Decision Drivers <!-- optional -->\\\\n* [driver 1, e.g., a force, facing concern, ...]\\\\n* [driver 2, e.g., a force, facing concern, ...]\\\\n* ... <!-- numbers of drivers can vary -->\\\\n'}, {'role': 'model', 'content': '* [driver 1, e.g., a force, facing concern, ...]\\\\n* [driver 2, e.g., a force, facing concern, ...]\\\\n* ... <!-- numbers of drivers can vary -->\\\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | ... | comes out best (see below)].\\\\n### Positive Consequences <!-- optional -->\\\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, ...]\\\\n* ...\\\\n### Negative Consequences <!-- optional -->\\\\n* [e.g., compromising quality attribute, follow-up decisions required, ...]\\\\n* ...\\\\nWhat is the change that we\\'re proposing and\/or doing?\\\\nThe change that we\\'re proposing or have agreed to implement.\\\\n'}]"}
{"File Name":"james\/0009-disable-elasticsearch-dynamic-mapping.md","Context":"## Context\\nWe rely on dynamic mappings to expose our mail headers as a JSON map. Dynamic mapping is enabled for adding not yet encountered headers in the mapping.\\nThis causes a serie of functional issues:\\n- Maximum field count can easily be exceeded\\n- Field type 'guess' can be wrong, leading to subsequent headers omissions [1]\\n- Document indexation needs to be paused at the index level during mapping changes to avoid concurrent changes, impacting negatively performance.\\n","Decision":"Rely on nested objects to represent mail headers within a mapping\\n","tokens":96,"id":2135,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nThe markdown body of ADRs cannot be used as is, because:\\n- Links between ADRs have to be replaced with correct URLs\\n- Header (status, date, deciders etc...) has to be rendered with specific components\\n## Decision Drivers <!-- optional -->\\n- Potential future development of a VSCode extension\\n\\n## Decision\\n- Potential future development of a VSCode extension\\nChosen option: \"Option 2: the core API is responsible (with MDX)\".\\nBecause if we develop the VSCode extension, it is better to add more business logic into the core package, and it is better tested.\\n### Positive Consequences <!-- optional -->\\n- The metadata in the header is simply removed\\n### Negative Consequences <!-- optional -->\\n- Each UI package will have to implement its own Header component\\n\\n\\n## Context and Problem Statement\\nGiven a service is using the twirl template imports recommended by the play-frontend libraries. When a developer tries\\nto use an import with a name which is present in both libraries without explicitly importing the one they want. Then\\nthey will get a compilation error:\\n<img alt=\"Example of ambiguous import compilation exception\" src=\"0010-ambiguous-import-exception.png\" width=\"450\">\\nCurrently, we recommend the use of the wildcard twirl imports, but this possible consequence is undocumented behaviour.\\nThe compilation error can be resolved by explicitly importing the class they want.\\n## Decision Drivers\\n* Be unsurprising for developers (examples we give should be runnable without triggering an exception)\\n* As much as possible avoid breaking changes for existing services\\n\\n## Decision\\n* Be unsurprising for developers (examples we give should be runnable without triggering an exception)\\n* As much as possible avoid breaking changes for existing services\\nWe\\'ve decided to immediately implement Option 1.2 and defer further changes to be discussed as part of PLATUI-1294.\\n### Positive Consequences\\n* Less confusing for new developers where imports are coming from when we avoid recommending using TwirlKeys config.\\n* Developers can see as part of our recommended usage instructions the possibility for an ambiguous import exception and\\nwhat they can do to resolve them without.\\n### Negative Consequences\\n* Without an API change the naming of components and view models is not as intuitive as we think it could be which may\\nconfuse new developers.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe rely on dynamic mappings to expose our mail headers as a JSON map. Dynamic mapping is enabled for adding not yet encountered headers in the mapping.\\\\nThis causes a serie of functional issues:\\\\n- Maximum field count can easily be exceeded\\\\n- Field type \\'guess\\' can be wrong, leading to subsequent headers omissions [1]\\\\n- Document indexation needs to be paused at the index level during mapping changes to avoid concurrent changes, impacting negatively performance.\\\\n'}, {'role': 'model', 'content': 'Rely on nested objects to represent mail headers within a mapping\\\\n'}]"}
{"File Name":"cloud-sdk-js\/0018-openapi-generator.md","Context":"## Context\\nWe are currently using the [Java-based OpenAPI generator CLI](https:\/\/www.npmjs.com\/package\/@openapitools\/openapi-generator-cli).\\nAs this generator is targeting many different programming languages in some points it does not fit our needs, e. g. handling of unique names or API files, which also differ between components.\\nIn our current approach we wrap the generated API with our own to fit those needs, but we realized, that we ended up writing many workarounds.\\nMany workarounds can currently only be covered by preprocessing\/manipulating the original service definition.\\nAt the moment, some API designs cannot be realized or are not worth investing into.\\nWe have observed that due to the size of the project, there are quite a lot of known bugs, that take a long time to solve or are not solved at all.\\nThe probably biggest pain point is that the used CLI needs a Java runtime.\\nTo run the CLI we have to download the given .jar file (~24MB) and make it part of our npm package, making it much bigger in size than usual.\\nThis is necessary as the file is not downloaded on installation but on command execution, causing other errors down the line (hurts SLC-29, race conditions in async batch execution).\\n","Decision":"Write our own generator, that is purely based on TypeScript and supports the API we need out of the box.\\n","tokens":269,"id":3626,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\\nserver.\\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\\nObject Storage technologies.\\nObject Storage is known to be cheap and yet offers guarantee about durability.\\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\\nIt also contribute negatively to the project build time.\\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\\nour Swift code without dropping Swift support at the same time.\\n\\n## Decision\\n* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\\n\\n\\n## Context and Problem Statement\\nTegh is moving to Rust and some problems have developed in the prototype that need to be addressed to fully embrace Rust:\\n1. It's becoming a mess of files.\\n2. Sled's unstable binary & not-widely supported file format does not lend itself to easy user debugging like sqlite.\\n3. Sled's pub\/sub model has led to complicated debugging and using it as the primary means of synchronizing data means that even things that shouldn't be persisted are being saved to disk just to get pub\/sub functionality.\\n4. Without Sled's pub\/sub data is not easily synchronized. Actors may replace this more robustly.\\nWe need to make some bigg-ish changes and that makes this an opportune moment to write up an updated architecture to organize this.\\n## Decision Drivers\\nInitial experiments in rewriting Tegh modules in rust have shown the language to generally reduce the amount of bugs created and their debugging difficulty. Bug hunts were sometimes quite difficult in Javascript due to it's lack of type safety and null.\\nBut let's be real, in the beginning I just wanted to try out Rust.\\n\\n## Decision\\nInitial experiments in rewriting Tegh modules in rust have shown the language to generally reduce the amount of bugs created and their debugging difficulty. Bug hunts were sometimes quite difficult in Javascript due to it's lack of type safety and null.\\nBut let's be real, in the beginning I just wanted to try out Rust.\\n### Machine\\n`packages\/machine\/`\\nFor `packages\/machine\/src\/[machine|config]`\\n- `mutations`\\n- `resolvers`\\n- `messages`\\n- `actors` - persistent state machines or ephermal context able to receive messages one at a time.\\n- `data` or `structs`(?) - SQL or config-backed non-actors with helper functions.\\n```mermaid\\nclassDiagram\\nclass Machine {\\n<<Ephemeral Data>>\\nID\\nSQLX db\\n+MachineConfig config\\n}\\nclass MachineConfig {\\n<<Config File Backed>>\\nID\\n}\\nMachine --> MachineConfig\\n```\\nCurrently machine information is spread across 3 unintegrated modules:\\n- machine_config\\n- ephemeral_machine_data\\n- machine\\n- receive_loop\\n- send_loop\\nThis is cumbursome so instead:\\n- `ephemeral_machine_data`, `receive_loop` and `machine` should be merged into an actor and not persisted.\\n- `send_loop` should be replaced by handlers called from GraphQL mutations with the following messages:\\n- stop_machine\\n- reset_machine\\n- pause_task\\n- delete_task_history\\n- send_task_to_machine\\n- `machine_config` should be included in tegh-machine package but config file updates managed in a seperate actor.\\n- tegh-marlin should replace it's duplicated config implementation with a dependency on the machine package.\\n### Core\\n`packages\/core`\\n`packages\/core\/src\/[backup|bin|invites|materials|users|video]`\\n- rename tegh-auth to `core`\\n- includes `users`, `invites`, and `video` moved up to the top level\\n### Print Queue\\n`packages\/print-queue`\\n`packages\/print-queue\/src\/[print-queue|print|task|package|part|macros]`\\n```mermaid\\nclassDiagram\\nclass PrintQueue {\\n<<SQL Data>>\\nID\\n}\\nclass Print {\\n<<SQL Data>>\\nID\\n}\\nclass Task {\\n<<SQL Data>>\\nID\\nmachine_id\\n+Print print\\n}\\nclass Package {\\n<<SQL Data>>\\nID\\n}\\nclass Part {\\n<<SQL Data>>\\nID\\n}\\nPrintQueue *-- Print\\nPrint *-- Task\\nPackage *-- Part\\nPart *-- Print\\n```\\n- print should be refactored into it's own file.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe are currently using the [Java-based OpenAPI generator CLI](https:\/\/www.npmjs.com\/package\/@openapitools\/openapi-generator-cli).\\\\nAs this generator is targeting many different programming languages in some points it does not fit our needs, e. g. handling of unique names or API files, which also differ between components.\\\\nIn our current approach we wrap the generated API with our own to fit those needs, but we realized, that we ended up writing many workarounds.\\\\nMany workarounds can currently only be covered by preprocessing\/manipulating the original service definition.\\\\nAt the moment, some API designs cannot be realized or are not worth investing into.\\\\nWe have observed that due to the size of the project, there are quite a lot of known bugs, that take a long time to solve or are not solved at all.\\\\nThe probably biggest pain point is that the used CLI needs a Java runtime.\\\\nTo run the CLI we have to download the given .jar file (~24MB) and make it part of our npm package, making it much bigger in size than usual.\\\\nThis is necessary as the file is not downloaded on installation but on command execution, causing other errors down the line (hurts SLC-29, race conditions in async batch execution).\\\\n\"}, {'role': 'model', 'content': 'Write our own generator, that is purely based on TypeScript and supports the API we need out of the box.\\\\n'}]"}
{"File Name":"CrossyToad\/adr-0009-organise-by-feature.md","Context":"## Context\\nThere are two main strategies for organising source code that I'm aware of:\\n- Organise by kind\\n- Organise by feature\\nWith \"Organise by kind\" we group modules that are related architecturally together. I.e. group all\\n\"Effects\", group all \"Data\", group all \"Scenes\".\\nThis is a fairly common approach, particularly given that website architectures often typify this\\nstyle by grouping all \"modules\", \"views\" and \"controllers\".\\nAlternatively we can \"Organise by feature\", ideally in this style the source code required to\\nimplement a particular \"Feature\" is grouped together instead. Ideally this would mean that\\nremoving a feature would simply mean deleting it's grouping any removing any dependent links.\\nFor this project I want to experiement to see what \"Organise by feature\" would look like for a game.\\n","Decision":"We're going to organise by *feature*!\\n","tokens":182,"id":2485,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\n[Harbor][] has proved itself to be rather unstable (causing several incidents from\\nvarious root causes). We have also been gradually rolling back our usage within\\nGSP of the various tools that Harbor provides:\\n* we are not using notary for signing (image provenance); this is now being\\nhandled using image digests instead of tags, enforced by gatekeeper\\n* we haven\\'t yet implemented anything using Clair\\n* we no longer have a requirement for anonymous public access of docker repositories\\nIt is also worth noting that some of these (e.g. \"Clair\") have been integrated\\ninto AWS ECR since we last looked.\\nIn order to implement the decisions in [ADR045 dev namespaces][ADR045] we must\\nensure that each namespace has its own place to store docker images, and that\\none namespace cannot overwrite another namespace\\'s docker images. This would be\\nvery fiddly to achieve with Harbor because it would require interactions with\\nthe Harbor API which doesn\\'t fit very well with \"desired state configuration\".\\nHowever with ECR we can more easily leverage IAM and the existing Service\\nOperator patterns to allow tenants to provision their own ECR repositories with\\ntheir own credentials.\\nSee also [ADR012][].\\n\\n## Decision\\nWe will replace the Harbor registry (the last Harbor component in use in GSP at\\ntime of writing) with AWS ECR with \"push\" credentials managed by the [Service\\nOperator][].\\n\\n\\n## Context\\nThere are significant differences between AMP and regular article rendering.\\nThese include:\\n* The markup is different\\n* The HTML structure\/limitations are different\\n* AMP does not require (or support) client-side custom Javascript, which means\\nwebpack is likely a poor fit\\n* From a project perspective, it seems that the two project change at different\\nrates (historically at least)\\n* The caching rules are different\\n* Ads are very different\\n..and more.\\nCoupling things that are different (in terms of code, building, deployment, or\\notherwise) can lead to brittle and complex solutions; it is generally better to\\nkeep things separate and share only what is absolutely required.\\nFrom this context, the question arises: should we attempt to build AMP in the\\nsame (node) package as articles, or separately?\\n\\n## Decision\\n*For our initial milestone* we will build AMP as simply a new page in the\\nexisting frontend package. The advantage of this approach is to bootstrap\\nquickly while leaving room to separate things later if necessary.\\nWithin this, we will adhere to a couple of rules:\\n* no `if (amp)` - branching is not permitted other than the absolute top-level\\nThere are many disadvantages to this kind of branching and it also would make it\\ndifficult to separate the projects later on.\\n* no divergence of data model\\nInstead, we will document pressures here and use any to bring forward\\ndiscussions around moving things into a separate project. So note, the\\nmotivation here is not that branching of data model is bad at all, but that it\\nrequires thought and possibly more significant changes.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThere are two main strategies for organising source code that I\\'m aware of:\\\\n- Organise by kind\\\\n- Organise by feature\\\\nWith \"Organise by kind\" we group modules that are related architecturally together. I.e. group all\\\\n\"Effects\", group all \"Data\", group all \"Scenes\".\\\\nThis is a fairly common approach, particularly given that website architectures often typify this\\\\nstyle by grouping all \"modules\", \"views\" and \"controllers\".\\\\nAlternatively we can \"Organise by feature\", ideally in this style the source code required to\\\\nimplement a particular \"Feature\" is grouped together instead. Ideally this would mean that\\\\nremoving a feature would simply mean deleting it\\'s grouping any removing any dependent links.\\\\nFor this project I want to experiement to see what \"Organise by feature\" would look like for a game.\\\\n'}, {'role': 'model', 'content': \"We're going to organise by *feature*!\\\\n\"}]"}
{"File Name":"tdr-dev-documentation\/0012-keycloak-email-provider.md","Context":"## Context\\nSet up Keycloak email provider to be able to send transferring bodies emails for updating passwords, and other Keycloak based user actions.\\n","Decision":"Decided to go with GovUK Notify as the email provider because:\\n* Re-using existing UK Government service.\\n* Ability to leverage GovUK branding to create specific TDR branding for emails.\\n* Free service.\\n* 24 hour support.\\n* NCSC compliant service.\\n* Relatively easy to integrate into Keycloak.\\n* Supports SMS. Might make things easier if there is a need to implement MFA via text messages.\\n### Disadvantages to GovUK Notify\\n* Logging not integrated with TDR application logging on AWS.\\n* Another account to manage for TDR.\\n### Documentation\\nSee here for further information on GovUK Notify: https:\/\/www.notifications.service.gov.uk\\n","tokens":30,"id":1774,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nMember account users will need to share encrypted things such as backups and snapshots between AWS accounts.\\n\\n## Decision\\nWe've decided to use [AWS Key Management Service (KMS)](https:\/\/aws.amazon.com\/kms\/) for cross account encryption.\\nKeys will be created per business unit as standard, if users require application level keys we will create these as and when needed.\\n\\n\\n# Context\\nsshfs is a really handy tool, but I felt that if I could mount AND unmount with it while still using the local user's ssh config file, it would make it super easy and fast to manage.\\n# Decision\\nI created a global npm command called `sshfsa` that will mount an SFTP session to an existing file path.\\n# Consequences\\nI will be able to mount and unmount without needing to type out the local mount path.\\n# Status\\nAccepted\\n\\n## Decision\\nI created a global npm command called `sshfsa` that will mount an SFTP session to an existing file path.\\n# Consequences\\nI will be able to mount and unmount without needing to type out the local mount path.\\n# Status\\nAccepted\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nSet up Keycloak email provider to be able to send transferring bodies emails for updating passwords, and other Keycloak based user actions.\\\\n\"}, {'role': 'model', 'content': 'Decided to go with GovUK Notify as the email provider because:\\\\n* Re-using existing UK Government service.\\\\n* Ability to leverage GovUK branding to create specific TDR branding for emails.\\\\n* Free service.\\\\n* 24 hour support.\\\\n* NCSC compliant service.\\\\n* Relatively easy to integrate into Keycloak.\\\\n* Supports SMS. Might make things easier if there is a need to implement MFA via text messages.\\\\n### Disadvantages to GovUK Notify\\\\n* Logging not integrated with TDR application logging on AWS.\\\\n* Another account to manage for TDR.\\\\n### Documentation\\\\nSee here for further information on GovUK Notify: https:\/\/www.notifications.service.gov.uk\\\\n'}]"}
{"File Name":"GDD-app\/0004-decouple-domain-models-from-representation.md","Context":"## Context\\nDomain models were used as-is for persistence and over-the-wire communication, making it difficult\\nto define a proper and strong domain model.\\n","Decision":"Separate DTOs should be defined for persistence and over-the-wire communication, which can have a\\nmore adequate representation for their use-case, without affecting our domain model.\\n","tokens":33,"id":4869,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\n`service-stac` will be accepting machine-to-machine communication and will have an admin interface for operations\/debugging. Authentication methods for this two use cases need to be defined.\\n\\n## Decision\\nMachine-to-machine communication will be using token authentication, access to the admin interface will be granted with usernames\/passwords managed in the Django admin interface. At a later stage, this might be changed to a more advanced authentication scheme.\\n\\n\\n## Problem\\nSome plugins need to communicate more data than is possible to do with metadata.\\nThis can limit the functionality of plugins, which need to exchange binary or very\\ncomplex data.\\n\\n## Decision\\nTo make the communication between plugins easier, plugins will additionally\\nget a handle to a global keyset via `elektraPluginGetGlobalKeySet()`.\\nThe global keyset is tied to a KDB handle, initialized on `kdbOpen()`\\nand deleted on `kdbClose()`.\\nThe global keyset handle is initialized and accessible for all plugins except\\nmanually created plugins (by calling e.g. `elektraPluginOpen()`).\\nThis decision removes the need to exchange information between plugins\\nvia the parentKey.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nDomain models were used as-is for persistence and over-the-wire communication, making it difficult\\\\nto define a proper and strong domain model.\\\\n'}, {'role': 'model', 'content': 'Separate DTOs should be defined for persistence and over-the-wire communication, which can have a\\\\nmore adequate representation for their use-case, without affecting our domain model.\\\\n'}]"}
{"File Name":"ea-talk\/0001-use-markdown-for-adrs.md","Context":"## Context\\nThe decision records must be stored in a plain text format:\\n* This works well with version control systems.\\n* It allows the tool to modify the status of records and insert\\nhyperlinks when one decision supercedes another.\\n* Decisions can be read in the terminal, IDE, version control\\nbrowser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\n","Decision":"browser, etc.\\nPeople will want to use some formatting: lists, code examples,\\nand so on.\\nPeople will want to view the decision records in a more readable\\nformat than plain text, and maybe print them out.\\nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\\nDecisions will be formatted nicely and hyperlinked by the\\nbrowsers of project hosting sites like GitHub and Bitbucket.\\nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convert\\nthe decision records into HTML or PDF.\\n","tokens":114,"id":1107,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe decision records must be stored in a plain text format:\\nThis works well with version control systems.\\nIt allows the tool to modify the status of records and insert hyperlinks when one decision supersedes another.\\nDecisions can be read in the terminal, IDE, version control browser, etc.\\nPeople will want to use some formatting: lists, code examples, and so on.\\nPeople will want to view the decision records in a more readable format than plain text, and maybe print them out.\\n\\n## Decision\\nPeople will want to use some formatting: lists, code examples, and so on.\\nPeople will want to view the decision records in a more readable format than plain text, and maybe print them out.\\nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\\nDecisions will be formatted nicely and hyperlinked by the\\nbrowsers of project hosting sites like GitHub and Bitbucket.\\nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convert\\nthe decision records into HTML or PDF.\\n\\n\\n## Context\\nThe existing site (Frontend) includes Javascript in a variety of ways:\\n-   inline\\n-   script tags\\n-   dynamically added script tags\\nThere is a single (universal) boot process which conditionally loads additional\\nJS as required based on page metadata.\\nFor Dotcom Rendering we want to simplify things so that it is easy for a\\ndeveloper to know where to add a script\/functionality. We also want an approach\\nthat helps ensure good performance and, related, which is less monolithic.\\n\\n## Decision\\n-   Load scripts in the head as deferred, or as async at the bottom of the HTML\\nbody section\\n-   All other script loading (other than for commercial third-party code) is\\nprohibited\\n-   We will not use a universal 'boot' JS file, but load scripts which have a\\nunique purpose and which are self-contained aside from a few shared helpers\\nFuller discussion of this approach can be found here:\\nhttps:\/\/docs.google.com\/document\/d\/1EnkowJ7BFumrqkpkRacRl-gSkiJs9fUgRCixQTLxBvo\/edit#.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThe decision records must be stored in a plain text format:\\\\n* This works well with version control systems.\\\\n* It allows the tool to modify the status of records and insert\\\\nhyperlinks when one decision supercedes another.\\\\n* Decisions can be read in the terminal, IDE, version control\\\\nbrowser, etc.\\\\nPeople will want to use some formatting: lists, code examples,\\\\nand so on.\\\\nPeople will want to view the decision records in a more readable\\\\nformat than plain text, and maybe print them out.\\\\n\"}, {'role': 'model', 'content': 'browser, etc.\\\\nPeople will want to use some formatting: lists, code examples,\\\\nand so on.\\\\nPeople will want to view the decision records in a more readable\\\\nformat than plain text, and maybe print them out.\\\\nRecord architecture decisions in [Markdown format](https:\/\/daringfireball.net\/projects\/markdown\/).\\\\nDecisions will be formatted nicely and hyperlinked by the\\\\nbrowsers of project hosting sites like GitHub and Bitbucket.\\\\nTools like [Pandoc](http:\/\/pandoc.org\/) can be used to convert\\\\nthe decision records into HTML or PDF.\\\\n'}]"}
{"File Name":"cljdoc\/0017-use-nomad-for-deployment.md","Context":"## Context\\ncljdoc's deployment story has been simplistic but effective. To recap:\\n- During CI a zip file is pushed to S3 that contains all files to run the application\\n- On the live server there is systemd service that will download an archive and run it. The\\nversion of the downloaded archive is specified via a file on the server.\\nUpdating simply required updating a file on the server and restarting the service.\\nThe issue with this approach however is that every time a new release was pushed to the server\\nthe restart of the systemd service would incur up to a minute of downtime. While this generally\\nisn't a huge deal it discourages certain development practices that may be desirable such as\\nContinuous Deployment.\\nOur existing deployment setup (and tools used by it) are poorly equipped to handle this kind of\\ndeployment scenario. A large amount of bash scripts would be required to start a new cljdoc server,\\nwait for it to become available, update nginx's upstream port, and kill old cljdoc server instances\\nin a repeatable, automated manner. Likely these bash scripts would be error-prone and turn into\\nsomething nobody likes to touch.\\n","Decision":"Implement a *canary deploy* mechanism for the cljdoc server application using\\n[Nomad](https:\/\/nomadproject.io) and [Traefik](https:\/\/traefik.io).\\nWhile both of these tools are probably aimed at much more complex workloads they provide the\\nfollowing benefits over the existing systemd\/Nginx setup:\\n- Automatic SSL certificates via Lets Encrypt\\n- Declarative specification of jobs and their desired update semantics\\n- APIs to schedule new jobs and cycle old\/new deployments\\n- Health checks to verify new deployments work as expected\\n- Machine images become much simpler since they only need Nomad, Consul, Docker\\nThis simplifies a lot of cljdoc's operational tasks while also enabling Continuous Deployment.\\n","tokens":243,"id":2610,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWhen rapidly iterating on API for prerelease features, we will commonly\\nwant to make breaking API changes without revving the API endpoints and\\nmaintaining the previous versions of the endpoints indefinitely for\\nversions of the app that will never be seen by customers.\\nIn that context, remote kills stand to be an incredibly useful tool.\\nWhen you decide to make a breaking API change, you simply create\/update\\na remote kill for the app from version `0` through the first client\\nversion conforming to the new contract. Then, instead of getting crash\\nbugs on dev builds, the feature simply turns itself off, and testers\\nwill have to upgrade in order to see the feature again.\\nThis stands to eliminate false bug reports of crash bugs from\\ninternal\/alpha testers who simply need to upgrade.\\nBut if we want to rely on this as a safety mechanism, we need to make\\nsure that chrome extension assignment overrides don't have the\\nopportunity to win-out over a remote kill, because they are too blunt of\\na tool. As an alpha tester, you might see that the feature went away and\\nperform another override in the Chrome extension. If that happens,\\nyou'd be back in crash bug land.\\n\\n## Decision\\nOn TestTrack server, remote kills will be absolute and won't be\\noverridden by force assignments regardless of the relative recency of\\nthe override and the remote kill.\\n\\n\\n## Context\\nDalmatian configuration is sourced directly from AWS so that the latest\\ninformation can be worked against when running this app locally.\\nEach time we start the local server a new YML file is created. When the server\\nis restarted the contents are always thrown away and treated as ephemeral.\\nWe need a structured way to query and mutate data within the app that is not\\npassing around the same YML file all the way through the application. Having\\nresources allows us to build a more conventional Rails app with restful routing.\\nThe structure of dalmatian.yml is fragile. There is no shared schema, versioning\\nor domain model to allow us to set robust expectations upon its interface.\\nIt is subject to change from the upstream of Dalmatian Core, for which this app\\nseeks to provide helper support for. The Dalmatian config file (dalmatian.yml)\\nis likely to change to meet the needs of Dalmatian Core. If this app depends on\\nthe structure of this YML file rigidly the tool risks being overly brittle and\\nbreaking whenever the structure changes.\\nRails applications are provisioned by default with Postgres as a relational\\ndatabase. There would be overhead in creating and maintaining a schema to\\nsupport a structure that we have no direct control over from this application.\\nIn the past when dxw have used a document store, Mongodb and the Mongoid gem\\nhave served well. Given we do not plan to host the application the choice of\\ntool has less impact, though we could externally host or spin up a mongo\\ncontainer should we choose to.\\n\\n## Decision\\nUse MongoDB for Dalmatian data instead of Postgres and ActiveRecord.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\ncljdoc's deployment story has been simplistic but effective. To recap:\\\\n- During CI a zip file is pushed to S3 that contains all files to run the application\\\\n- On the live server there is systemd service that will download an archive and run it. The\\\\nversion of the downloaded archive is specified via a file on the server.\\\\nUpdating simply required updating a file on the server and restarting the service.\\\\nThe issue with this approach however is that every time a new release was pushed to the server\\\\nthe restart of the systemd service would incur up to a minute of downtime. While this generally\\\\nisn't a huge deal it discourages certain development practices that may be desirable such as\\\\nContinuous Deployment.\\\\nOur existing deployment setup (and tools used by it) are poorly equipped to handle this kind of\\\\ndeployment scenario. A large amount of bash scripts would be required to start a new cljdoc server,\\\\nwait for it to become available, update nginx's upstream port, and kill old cljdoc server instances\\\\nin a repeatable, automated manner. Likely these bash scripts would be error-prone and turn into\\\\nsomething nobody likes to touch.\\\\n\"}, {'role': 'model', 'content': \"Implement a *canary deploy* mechanism for the cljdoc server application using\\\\n[Nomad](https:\/\/nomadproject.io) and [Traefik](https:\/\/traefik.io).\\\\nWhile both of these tools are probably aimed at much more complex workloads they provide the\\\\nfollowing benefits over the existing systemd\/Nginx setup:\\\\n- Automatic SSL certificates via Lets Encrypt\\\\n- Declarative specification of jobs and their desired update semantics\\\\n- APIs to schedule new jobs and cycle old\/new deployments\\\\n- Health checks to verify new deployments work as expected\\\\n- Machine images become much simpler since they only need Nomad, Consul, Docker\\\\nThis simplifies a lot of cljdoc's operational tasks while also enabling Continuous Deployment.\\\\n\"}]"}
{"File Name":"uniprot-website\/0003-design-system.md","Context":"## Context\\nIn order to provide a consistent UI, we need to use a design system. This design system also helps reduce the amount of code required, and ensures designers and developers speak the \"same language\".\\n","Decision":"We have created our own design system\/pattern librarly, [Franklin](https:\/\/ebi-uniprot.github.io\/franklin-sites). It is built on top [Foundation](https:\/\/foundation.zurb.com\/) (Atomic level components) and uses React.js. The library is published to `npm` as [`franklin-sites`](https:\/\/www.npmjs.com\/package\/franklin-sites) and can be used by any React.js website.\\n","tokens":44,"id":2012,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nMany systems, teams, and designs require that the system will not utilize dynamic memory allocation. We should maintain a flexible design by allowing users to use dynamic memory allocation if they desire. However, we should be able to support the strictest operating model for maximum flexibility and potential use of the framework.\\n\\n## Decision\\nNo dynamic memory allocation will be utilized by framework core components\\n\\n\\n## Context\\nThe choice of which programming language to use is governed by a host of different factors. Most languages can be made to work for whatever task is required, though some are better at certain tasks than others.\\nWe expect the nature of the work in this project to benefit from concurrency, so choosing a language with good support for this is important. Since we will be deploying to AWS Lambda (See [5. Use AWS Lambda](0005-use-aws-lambda.md)), we are further limited to using one of the supported languages. Other considerations include ease of packaging and distribution, excellent data streaming abilities, and a healthy ecosystem of 3rd party libraries.\\n\\n## Decision\\nUse Go for the core application language.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nIn order to provide a consistent UI, we need to use a design system. This design system also helps reduce the amount of code required, and ensures designers and developers speak the \"same language\".\\\\n'}, {'role': 'model', 'content': 'We have created our own design system\/pattern librarly, [Franklin](https:\/\/ebi-uniprot.github.io\/franklin-sites). It is built on top [Foundation](https:\/\/foundation.zurb.com\/) (Atomic level components) and uses React.js. The library is published to `npm` as [`franklin-sites`](https:\/\/www.npmjs.com\/package\/franklin-sites) and can be used by any React.js website.\\\\n'}]"}
{"File Name":"html-integrations\/001-Generate--RFC-compliant--UUIDs--Telemetry.md","Context":"## Context (Discussion)\\nIn the context of the Telemetry project, we want to generate\\nvalid and consisten UUIDs since they are needed to ensure the uniqueness, randomeness and its validity.\\nUUID identifiers have an specification defined as the RFC-4122 standard  [A Universally Unique IDentifier (UUID) URN Namespace](https:\/\/tools.ietf.org\/html\/rfc4122) on ietf.org.\\nWe don't want to reinvent the wheel and it seems unwise to write our own library to generate this UUIDs since there are third party solutions with good support, small and secure.\\n","Decision":"We'll use [uuid package] to generate RFC4122 version 4 UUIDs to use on the Telemetry implementation. The code of the [uuid project] is available at github.\\nTherefore, **uuid** becomes the first functional dependency of the 'MathType Web Integration JavaScript SDK', known as npm package as '@wiris\/mathtype-html-integration-devkit'.\\n* [uuid project](https:\/\/github.com\/uuidjs\/uuid)\\n* [uuid package](https:\/\/www.npmjs.com\/package\/uuid)\\n### Pros and Cons of the Options\\n#### Implement our own Javascript library for that\\n- Bad, because Javascript Math.random function is not very good.\\n- Bad, because we'll need to maintain it.\\n- Bad, because more work to the backlog.\\n- Bad, because we're reinventing the wheel.\\n#### Using a third party library like github.com\/uuidjs\/uuid\\n- Good, because supports RFC4122 version 1, 3, 4, and 5 UUIDs.\\n- Good, because its well maintained, no issues and widely used; (26.085.977 downloads\/week).\\n- Good, because solves our problem immediately.\\n- Good, because it's secure, small and cross-platform.\\n- Bad, because we're adding a dependency to our core library, and therefore, to all our Javascript plugins.\\n","tokens":127,"id":337,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n# Context\\nWe need to store the imported data from the _Connect_ process in an\\nhomogeneous way for easy retrieval.\\n# Decision\\nAfter checking how other tools work (like Kibana) it seems that\\nElasticSearch is a good choice for storing the imported data.\\nElasticSearch has its own DSL for building complex queries and we can\\nreuse that part.\\n# Status\\nSuperseded. See [ADR-004](ADR-004.md)\\n# Consequences\\n* We need to know how to run and maintain a ElasticSearch cluster\\n* We need to address the security of the cluster ourselves. Elastic\\nhas a security plugin but is not open source.\\nExamples:\\n- http:\/\/floragunn.com\/searchguard\\n- http:\/\/keycloak.github.io\/docs\/userguide\/keycloak-server\/html\/proxy.html\\n\\n## Decision\\nAfter checking how other tools work (like Kibana) it seems that\\nElasticSearch is a good choice for storing the imported data.\\nElasticSearch has its own DSL for building complex queries and we can\\nreuse that part.\\n# Status\\nSuperseded. See [ADR-004](ADR-004.md)\\n# Consequences\\n* We need to know how to run and maintain a ElasticSearch cluster\\n* We need to address the security of the cluster ourselves. Elastic\\nhas a security plugin but is not open source.\\nExamples:\\n- http:\/\/floragunn.com\/searchguard\\n- http:\/\/keycloak.github.io\/docs\/userguide\/keycloak-server\/html\/proxy.html\\n\\n\\n## Context\\nHistorically, James has been an early adopter for the JMAP specification, and a first partial implementation was conducted when JMAP was just a draft.\\nBut with time, the IETF draft went with radical changes and the community could not keep this implementation up to date with the spec changes.\\nAs of summer 2019, JMAP core ([RFC 8620](https:\/\/tools.ietf.org\/html\/rfc8620)) and JMAP mail ([RFC 8621](https:\/\/tools.ietf.org\/html\/rfc8621)) have been officially published.\\nThus we should implement these new specifications to claim JMAP support.\\nWe need to keep in mind though that part of the community actively relies on the actual 'draft' implementation of JMAP existing in James.\\n\\n## Decision\\nWe decided to do as follow:\\n* Rename packages `server\/protocols\/jmap*` and guice packages `server\/container\/guice\/protocols\/jmap*` to `jmap-draft`. `JMAPServer` should also be renamed to `JMAPDraftServer` (this has already been contributed [here](https:\/\/github.com\/apache\/james-project\/pull\/164), thanks to @cketti).\\n* Port `jmap-draft` to be served with a reactive technology\\n* Implement a JMAP meta project to select the JMAP version specified in the accept header and map it to the correct implementation\\n* Create a new `jmap` package\\n* Implement the new JMAP request structure with the [echo](https:\/\/jmap.io\/spec-core.html#the-coreecho-method) method\\n* Implement authentication and session of the new JMAP protocol\\n* Implement protocol-level error handling\\n* Duplicate and adapt existing mailbox methods of `jmap-draft` to `jmap`\\n* Duplicate and adapt existing email methods of `jmap-draft` to `jmap`\\n* Duplicate and adapt existing vacation methods of `jmap-draft` to `jmap`\\n* Support uploads\/downloads\\nThen when we finish to port our existing methods to the new JMAP specifications, we can implement these new features:\\n* Accounts\\n* Identities\\n* EmailSubmission\\n* Push and queryChanges\\n* Threads\\nWe decided to support `jmap` on top of memory-guice and distributed-james products for now.\\nWe should ensure no changes is done to `jmap-draft` while implementing the new `jmap` one.\\nRegarding the versioning in the accept headers:\\n* `Accept: application\/json;jmapVersion=draft` would redirect to `jmap-draft`\\n* `Accept: application\/json;jmapVersion=rfc-8621` would redirect to `jmap`\\n* When the `jmapVersion` is omitted, we will redirect first towards `jmap-draft`, then to `jmap`\\nwhen `jmap-draft` becomes deprecated\\nIt's worth mentioning as well that we took the decision of writing this new implementation using `Scala`.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context (Discussion)\\\\nIn the context of the Telemetry project, we want to generate\\\\nvalid and consisten UUIDs since they are needed to ensure the uniqueness, randomeness and its validity.\\\\nUUID identifiers have an specification defined as the RFC-4122 standard  [A Universally Unique IDentifier (UUID) URN Namespace](https:\/\/tools.ietf.org\/html\/rfc4122) on ietf.org.\\\\nWe don't want to reinvent the wheel and it seems unwise to write our own library to generate this UUIDs since there are third party solutions with good support, small and secure.\\\\n\"}, {'role': 'model', 'content': \"We'll use [uuid package] to generate RFC4122 version 4 UUIDs to use on the Telemetry implementation. The code of the [uuid project] is available at github.\\\\nTherefore, **uuid** becomes the first functional dependency of the 'MathType Web Integration JavaScript SDK', known as npm package as '@wiris\/mathtype-html-integration-devkit'.\\\\n* [uuid project](https:\/\/github.com\/uuidjs\/uuid)\\\\n* [uuid package](https:\/\/www.npmjs.com\/package\/uuid)\\\\n### Pros and Cons of the Options\\\\n#### Implement our own Javascript library for that\\\\n- Bad, because Javascript Math.random function is not very good.\\\\n- Bad, because we'll need to maintain it.\\\\n- Bad, because more work to the backlog.\\\\n- Bad, because we're reinventing the wheel.\\\\n#### Using a third party library like github.com\/uuidjs\/uuid\\\\n- Good, because supports RFC4122 version 1, 3, 4, and 5 UUIDs.\\\\n- Good, because its well maintained, no issues and widely used; (26.085.977 downloads\/week).\\\\n- Good, because solves our problem immediately.\\\\n- Good, because it's secure, small and cross-platform.\\\\n- Bad, because we're adding a dependency to our core library, and therefore, to all our Javascript plugins.\\\\n\"}]"}
{"File Name":"ReportMI-service-manual\/0027-use-docker-in-development.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n- onboarding a fresh team has consumed a lot of time and we want to improve this for the future as this service moves from a build phase into the support phase.\\n- 2 ways to start and run the application were documented, depending on your preference you would use the ones you were most comfortable with. This led to docker documentation that fell behind and didn't work out of the box\\n- when supported by the dxw support team, a different developer will be on hand each week to fix issues. In order to effectively and confidently apply fixes we want to standardise and test the set up process, removing as much manual process as possible and allowing the developer to address problems quickly.\\n- in ADR 26 we discussed moving from AWS ECS (containers) to GOV.UK PaaS (no containers). Whilst we no longer use containers in production and there aren't the same advantages to parity we would normally fine, we believe there is still good value to using it to make consistent development environments.\\n","Decision":"We are going to use Docker in development exclusively for the frontend and the API.\\n","tokens":226,"id":2057,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nSince the initial decision to rebuild NOMIS and the Fix & Go infrastructure in Azure we have learned a few things and come to reevaluate our choice. Some context:\\n- NOMIS, OASys, CAFM, and the rest of Fix & Go resides in Azure under the control and maintenance of the DSO team.\\n- Database, WebLogic and application installation and management is provided by the Sheffield Studio DBA team.\\n- Fix & Go has network connectivity to a few key environments, particularly the Quantum network and PSN.\\n- Uncertainty around IR35 status and contractual renegotiations, as well as a shift in team focus and priorities means we have lost several members of the team. Unfortunately there\\'s a reasonable chance we\\'ll lose more people in the next few months.\\n- The Delius migration is now live, so the infrastructure code is tested and used in production.\\n- Robin helped deliver the Delius DevOps tooling, and has deep knowledge of how it works and where it could benefit us.\\n- Recent spikes demonstrated that:\\n+ We can spin up an environment in AWS very quickly \u2013 within 3 days we had automated builds and deployment for a simple Oracle\/WebLogic stack. While NOMIS wasn\\'t functional (Oracle Forms still needs to be built regardless of AWS\/Azure), the supporting infrastructure was \"production quality\".\\n+ The Packer images and Ansible is largely reusable in Azure \u2013 again with a few days we had a simple Oracle\/WebLogic stack, albeit without the production-quality supporting infrastructure and networking. The Terraform configurations would need to be rewritten for Azure, as they are provider-specific.\\n+ While NOMIS closely resembles a portion of Delius, the latter is significantly more complex.\\n\\n## Decision\\nWe will continue to build in Azure, and lean more heavily on Azure\\'s native offerings to offset some of the cost of _not_ using the Delius team\\'s AWS code.\\nThis decision is based on a couple of things (as well as the million more nuanced things):\\n1. Reducing the risk of knowledge silos, knowledge loss (bus\/lottery factor), and lowering the barrier to entry for new team members is really important given the current challenges around IR35 and the makeup of our team. Adding a new cloud provider increases the complexity of our systems, even with the potential support we could get from the Delius migration team.\\n2. Reducing our time to deliver DR in production. We already have network connectivity to Quantum, remote desktop infrastructure, etc. in Azure, and replicating sufficient services in AWS to allow a migration of production is likely to be a significant undertaking involving a number of external stateholders.\\n\\n\\n## Context\\nWe are starting to develop a number of user-facing applications with web\\ninterfaces that need to be GOV.UK branded. In order to keep things consistent\\nwe want to pick a single programming language to write these in.\\nWe\\'ve previously chosen Ruby on Rails as our desired framework in the previous\\n[ADR022]. And whether or not it was a right choice, we decided to revisit and\\nreconsider that choice.\\nThe reason for that, is simply the direction GDS is heading at. It would appear\\nthat old components will be soon deprecated and the Frontend Community has no\\ndesire to support gems in the future. Saying that, we could have remain with\\nRails and delegate the component management to NPM. This would however increase\\nthe amount of possible maintenance work we would need to undertake, due to the\\nuse of [nunjucks] by the Design System team.\\nWe questioned the need of running Rails application for something that\\nessentially is a templating system for existing data and API.\\nAfter some more research, discussion with the head of the Frontend Community,\\nmembers of the team and some others in GDS, the better choice would be Node for\\nthe following reasons:\\n* It\\'s the way Frontend Community is heading at\\n* It will be easier to rotate\/onboard Frontend Developers\\n* The initial applications are to be simple (thin layer between API calls and\\nHTML parsing)\\n* It\\'s light and essentially is JavaScript\\n* It supports [nunjucks] which will help us in maintenance\\n\\n## Decision\\nWe will use Node to create new user-facing applications that render a web\\ninterface for our service but will not be used to implement any significant\\n\u2018application logic\u2019.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\\\n- onboarding a fresh team has consumed a lot of time and we want to improve this for the future as this service moves from a build phase into the support phase.\\\\n- 2 ways to start and run the application were documented, depending on your preference you would use the ones you were most comfortable with. This led to docker documentation that fell behind and didn\\'t work out of the box\\\\n- when supported by the dxw support team, a different developer will be on hand each week to fix issues. In order to effectively and confidently apply fixes we want to standardise and test the set up process, removing as much manual process as possible and allowing the developer to address problems quickly.\\\\n- in ADR 26 we discussed moving from AWS ECS (containers) to GOV.UK PaaS (no containers). Whilst we no longer use containers in production and there aren\\'t the same advantages to parity we would normally fine, we believe there is still good value to using it to make consistent development environments.\\\\n'}, {'role': 'model', 'content': 'We are going to use Docker in development exclusively for the frontend and the API.\\\\n'}]"}
{"File Name":"SearchServices\/0004-community-mirror.md","Context":"## Context\\nIn [ADR 3: \"Combined Codebase\"](0003-combined-codebase.md) we decided to merge the Search Services and Insight Engine\\nrepositories.  Since we want to enable the community to submit pull requests to the Search Services project we need a\\nway to keep this code up to date on GitHub.\\n","Decision":"We will mirror `master` and all branches starting with `release\/` to a branch with the same name on GitHub.  We will\\nexclude the alfresco-insight-engine-parent directory. We will include these commands as part of our build to do this:\\n```\\n# This avoids making changes to the original branch.\\nget checkout -b tempBranch\\n# This strips all enterprise changes (in a reproducible way) and pushes any updates to the mirror.\\ngit filter-branch -f --prune-empty --index-filter 'git rm -r --cached --ignore-unmatch alfresco-insight-engine-parent'\\ngit push out HEAD:$branch\\n# This resets us back to where we were before the filtering.\\ngit checkout $branch\\n```\\n","tokens":69,"id":5130,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nIn [ADR 3: \"Combined Codebase\"](0003-combined-codebase.md) we decided to merge the Search Services and Insight Engine\\nrepositories.  In [ADR 4: \"Community Mirror\"](0004-community-mirror.md) we discussed how we would set up a build job to\\nensure community code is available to the community. In particular we decided:\\n> We will mirror `master` and all branches starting with `release\/` to a branch with the same name on GitHub.\\n\\n## Decision\\nDuring the combining of the Search Services and Insight Engine codebases we will create a branch `master` by merging\\nthe existing `master` branches of those two projects. We will update the version of Insight Engine to match Search Services - that\\nis 1.4.0-SNAPSHOT.\\n\\n\\n## Context\\nIn [ADR 3: \"Combined Codebase\"](0003-combined-codebase.md) we decided to merge the production and end-to-end test\\nrepositories. In [ADR 4: \"Community Mirror\"](0004-community-mirror.md) we discuss setting up a mirror for the community\\ncode.\\n\\n## Decision\\nWe will separate the end-to-end test code in half so that any code solely related to Insight Engine won\\'t be mirrored.\\nWe will remove the existing test groups for the different versions of Search Services and Insight Engine, and instead\\ndelete any tests from branches where they should not be run.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nIn [ADR 3: \"Combined Codebase\"](0003-combined-codebase.md) we decided to merge the Search Services and Insight Engine\\\\nrepositories.  Since we want to enable the community to submit pull requests to the Search Services project we need a\\\\nway to keep this code up to date on GitHub.\\\\n'}, {'role': 'model', 'content': \"We will mirror `master` and all branches starting with `release\/` to a branch with the same name on GitHub.  We will\\\\nexclude the alfresco-insight-engine-parent directory. We will include these commands as part of our build to do this:\\\\n```\\\\n# This avoids making changes to the original branch.\\\\nget checkout -b tempBranch\\\\n# This strips all enterprise changes (in a reproducible way) and pushes any updates to the mirror.\\\\ngit filter-branch -f --prune-empty --index-filter 'git rm -r --cached --ignore-unmatch alfresco-insight-engine-parent'\\\\ngit push out HEAD:$branch\\\\n# This resets us back to where we were before the filtering.\\\\ngit checkout $branch\\\\n```\\\\n\"}]"}
{"File Name":"deeplearning4j\/0001-kotlin_dsl_as_source_of_truth.md","Context":"## Context\\nThis code generation experiment is meant to be our starting point for both the API unification for ND4J and SameDiff,\\nand the multi-language support. For this reason we have to define ops, or their interface, in a language neutral way.\\nThe initial idea was to use a Language Workbench like MPS. This had to be discarded because of bugs and limitations\\nencountered while trying to define a language that would work for a few simple examples.\\nThe next idea was to use Ops defined in JSON files. This would have allowed us to define Ops as human readable data and\\nread and write those files from any programming language. However, the drawback with this approach is that writing json\\nmanually invites many problems if written manually (e.g. typos, bad structuring, having to look up the proper keys,...).\\nIn order to rectify that drawback, we would have to create custom tooling, that we would have to maintain and that\\ncontributors would have to use.\\nUsing a Java builder pattern based approach is very verbose.\\n","Decision":"We use a Kotlin-based DSL to define Ops.\\n","tokens":218,"id":2939,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThis code generation experiment is meant to be our starting point for both the API unification for ND4J and SameDiff,\\nand the multi-language support. For this reason we have to define ops, or their interface, in a language neutral way.\\nThe initial idea was to use a Language Workbench like MPS. This had to be discarded because of bugs and limitations\\nencountered while trying to define a language that would work for a few simple examples.\\nThe next idea was to use Ops defined in JSON files. This would have allowed us to define Ops as human readable data and\\nread and write those files from any programming language. However, the drawback with this approach is that writing json\\nmanually invites many problems if written manually (e.g. typos, bad structuring, having to look up the proper keys,...).\\nIn order to rectify that drawback, we would have to create custom tooling, that we would have to maintain and that\\ncontributors would have to use.\\nUsing a Java builder pattern based approach is very verbose.\\n\\n## Decision\\nWe use a Kotlin-based DSL to define Ops.\\n\\n\\n## Context and Problem Statement\\nTegh is moving to Rust and some problems have developed in the prototype that need to be addressed to fully embrace Rust:\\n1. It's becoming a mess of files.\\n2. Sled's unstable binary & not-widely supported file format does not lend itself to easy user debugging like sqlite.\\n3. Sled's pub\/sub model has led to complicated debugging and using it as the primary means of synchronizing data means that even things that shouldn't be persisted are being saved to disk just to get pub\/sub functionality.\\n4. Without Sled's pub\/sub data is not easily synchronized. Actors may replace this more robustly.\\nWe need to make some bigg-ish changes and that makes this an opportune moment to write up an updated architecture to organize this.\\n## Decision Drivers\\nInitial experiments in rewriting Tegh modules in rust have shown the language to generally reduce the amount of bugs created and their debugging difficulty. Bug hunts were sometimes quite difficult in Javascript due to it's lack of type safety and null.\\nBut let's be real, in the beginning I just wanted to try out Rust.\\n\\n## Decision\\nInitial experiments in rewriting Tegh modules in rust have shown the language to generally reduce the amount of bugs created and their debugging difficulty. Bug hunts were sometimes quite difficult in Javascript due to it's lack of type safety and null.\\nBut let's be real, in the beginning I just wanted to try out Rust.\\n### Machine\\n`packages\/machine\/`\\nFor `packages\/machine\/src\/[machine|config]`\\n- `mutations`\\n- `resolvers`\\n- `messages`\\n- `actors` - persistent state machines or ephermal context able to receive messages one at a time.\\n- `data` or `structs`(?) - SQL or config-backed non-actors with helper functions.\\n```mermaid\\nclassDiagram\\nclass Machine {\\n<<Ephemeral Data>>\\nID\\nSQLX db\\n+MachineConfig config\\n}\\nclass MachineConfig {\\n<<Config File Backed>>\\nID\\n}\\nMachine --> MachineConfig\\n```\\nCurrently machine information is spread across 3 unintegrated modules:\\n- machine_config\\n- ephemeral_machine_data\\n- machine\\n- receive_loop\\n- send_loop\\nThis is cumbursome so instead:\\n- `ephemeral_machine_data`, `receive_loop` and `machine` should be merged into an actor and not persisted.\\n- `send_loop` should be replaced by handlers called from GraphQL mutations with the following messages:\\n- stop_machine\\n- reset_machine\\n- pause_task\\n- delete_task_history\\n- send_task_to_machine\\n- `machine_config` should be included in tegh-machine package but config file updates managed in a seperate actor.\\n- tegh-marlin should replace it's duplicated config implementation with a dependency on the machine package.\\n### Core\\n`packages\/core`\\n`packages\/core\/src\/[backup|bin|invites|materials|users|video]`\\n- rename tegh-auth to `core`\\n- includes `users`, `invites`, and `video` moved up to the top level\\n### Print Queue\\n`packages\/print-queue`\\n`packages\/print-queue\/src\/[print-queue|print|task|package|part|macros]`\\n```mermaid\\nclassDiagram\\nclass PrintQueue {\\n<<SQL Data>>\\nID\\n}\\nclass Print {\\n<<SQL Data>>\\nID\\n}\\nclass Task {\\n<<SQL Data>>\\nID\\nmachine_id\\n+Print print\\n}\\nclass Package {\\n<<SQL Data>>\\nID\\n}\\nclass Part {\\n<<SQL Data>>\\nID\\n}\\nPrintQueue *-- Print\\nPrint *-- Task\\nPackage *-- Part\\nPart *-- Print\\n```\\n- print should be refactored into it's own file.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThis code generation experiment is meant to be our starting point for both the API unification for ND4J and SameDiff,\\\\nand the multi-language support. For this reason we have to define ops, or their interface, in a language neutral way.\\\\nThe initial idea was to use a Language Workbench like MPS. This had to be discarded because of bugs and limitations\\\\nencountered while trying to define a language that would work for a few simple examples.\\\\nThe next idea was to use Ops defined in JSON files. This would have allowed us to define Ops as human readable data and\\\\nread and write those files from any programming language. However, the drawback with this approach is that writing json\\\\nmanually invites many problems if written manually (e.g. typos, bad structuring, having to look up the proper keys,...).\\\\nIn order to rectify that drawback, we would have to create custom tooling, that we would have to maintain and that\\\\ncontributors would have to use.\\\\nUsing a Java builder pattern based approach is very verbose.\\\\n\"}, {'role': 'model', 'content': 'We use a Kotlin-based DSL to define Ops.\\\\n'}]"}
{"File Name":"external-service-operator\/0003-the-name-of-the-endpoints-service-and-ingress-gets-inherited-by-the-controlling-externalservice.md","Context":"## Context\\nTo easify finding the according Endpoints, Services and Ingress Ressources, they are named exactly the same as the Externalservice Ressource.\\nNethertheless, of course the Owner will be set correctly as well as every ressource gets the label:\\n```\\napp= <external-servicename>\\n```\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":76,"id":1733,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe Adyen notification needs to be matched by its commercetools payment equivalent.\\nWe are using the custom field for the merchantReference and fetching the commercetools payment object with query `custom(fields(merchantReference=\"${merchantReference}\"))`.\\nThe alternative for that is the native payment `key` field.\\n\\n## Decision\\n- We will use the native payment key for matching payment for notification.\\n- The extension module will validate the reference field makePaymentRequest#reference before handling the payment to avoid unnecessary calls to Adyen.\\n- The payment key will be set by the make payment handler, also makePaymentRequest#reference should be validated to avoid mismatches.\\n- The notification will use the native payment key to fetch payment. It first finds the payment by `key` where `key=${merchantReference}` and then it finds in this payment the corresponding transaction\\nby `interactionId` where `interactionId=${pspReference}`.\\n\\n\\n## Context\\nWe need to provide the [CSLS Processor][processor] with information suitable\\nfor it to map log events coming from the Adapter to manually configured Splunk\\nindexes for the tenant.\\nThe Processor [maps log events to indexes][mapping] based on the combination of the `Owner`\\nand `LogGroupName` fields in the CSLS\/CloudWatch log format.\\nThe Adapter currently sets the `Owner` field to the static string \"GOV.UK_PaaS\"\\nand the `LogGroupName` to the static string \"rfc5424_syslog\".\\nAll logs shipped through the Adapter currently end up in a single index in splunk.\\nA single index in splunk is NOT suitable for teams with security and compliance\\nrequirements, who need fine grained control over access control.\\n\\n## Decision\\nWe will set the `LogGroupName` on logs emitted from the Adapter to the\\nCloudfoundry service instance GUID of the splunk service provisioned by the\\ntenant.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nTo easify finding the according Endpoints, Services and Ingress Ressources, they are named exactly the same as the Externalservice Ressource.\\\\nNethertheless, of course the Owner will be set correctly as well as every ressource gets the label:\\\\n```\\\\napp= <external-servicename>\\\\n```\\\\n'}, {'role': 'model', 'content': \"The change that we're proposing or have agreed to implement.\\\\n\"}]"}
{"File Name":"polaris\/developmentLanguage.md","Context":"## Context\\nReactNative supports both JavaScript and TypeScript.\\n","Decision":"Javascript was chosen at this time as it is a more widely used development language and, whilst Babel enables you to use TypeScript, it only enables transpilation of TypeScript and not type checking.\\n","tokens":12,"id":4720,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n### Context\\nTypescript offers better IDE integration and type checking.\\n### Decision\\nStart using Typescript.\\n\\n## Decision\\nStart using Typescript.\\n\\n\\n## Context\\nVisual Studio 2017 added support for packaging applications using Docker and running them using Docker Compose.\\n\\n## Decision\\nWe will use Docker for packaging and running the application in a Linux container.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nReactNative supports both JavaScript and TypeScript.\\\\n'}, {'role': 'model', 'content': 'Javascript was chosen at this time as it is a more widely used development language and, whilst Babel enables you to use TypeScript, it only enables transpilation of TypeScript and not type checking.\\\\n'}]"}
