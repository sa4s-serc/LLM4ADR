{"File Name":"mediawiki-extensions-WikibaseManifest\/0002_use_open_api_spec.md","Context":"## Context\\nWikibase Manifest needs to provide essential metadata and configuration options about a Wikibase instance.\\nWe need to decide on the format of the information the manifest will provide.\\nWe took into consideration the following projects:\\n- the recent [REST API Prototype](https:\/\/github.com\/wmde\/wikibase-rest-fantasies) (by WMDE) and its [OpenAPI spec](https:\/\/raw.githubusercontent.com\/wmde\/wikibase-rest-fantasies\/gh-pages\/openapi.json)\\n- [OpenRefine's initiative](https:\/\/github.com\/OpenRefine\/wikibase-manifests) to collect manifests from different wikibases and their [json-schema spec](https:\/\/github.com\/OpenRefine\/wikibase-manifests\/blob\/master\/wikibase-manifest-schema-v1.json)\\nThe MediaWiki REST API which we are using (please refer to ADR 1 for more info) implements neither json schema nor openAPI.\\n### OpenAPI (swagger)\\nThe OpenAPI Specification (formerly Swagger Specification) is an API description format for REST APIs. An OpenAPI file allows you to describe your entire API. [Swagger](swagger.io) is a set of open-source tools built around the OpenAPI Specification, like the [api editor](https:\/\/editor.swagger.io\/).\\n- It\u2019s popularly used for mocking services and generating SDKs. It's not commonly used for run-time functionality.\\n- Useful when you want to describe your entire API.\\n- **OpenAPI is both a subset of JSON Schema Draft 5 and a superset**\\n### Json Schema\\nJSON Schema is a vocabulary that allows you to validate, annotate, and manipulate JSON documents.\\nThe specification is split into three parts, [Core](https:\/\/json-schema.org\/draft\/2019-09\/json-schema-core.html), [Validation](https:\/\/json-schema.org\/draft\/2019-09\/json-schema-validation.html), and [Hyper-Schema](https:\/\/json-schema.org\/draft\/2019-09\/json-schema-hypermedia.html).\\n- JSON Schema is a good option when there are data models whose schema needs to be defined\\n","Decision":"Use OpenAPI spec.\\nWe acknowledge that both are good options. We chose OpenAPI because the Wikidata team has created several products (e.g. [termbox](https:\/\/gerrit.wikimedia.org\/r\/plugins\/gitiles\/wikibase\/termbox\/+\/refs\/heads\/master\/openapi.json)) using the OpenAPI spec and plan on continue to do so when we have the opportunity.\\nThere're tools (e.g. [OpenAPI Schema to JSON Schema](https:\/\/github.com\/openapi-contrib\/openapi-schema-to-json-schema)) for converting from OpenAPI Schema Object or Parameter Object to JSON Schema in case the need arises to use json schema.\\n","tokens":439,"id":4998,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\n[WikibaseRepo](https:\/\/gerrit.wikimedia.org\/r\/plugins\/gitiles\/mediawiki\/extensions\/Wikibase\/+\/62ae43e\/repo\/includes\/WikibaseRepo.php) and [WikibaseClient](https:\/\/gerrit.wikimedia.org\/r\/plugins\/gitiles\/mediawiki\/extensions\/Wikibase\/+\/62ae43e\/client\/includes\/WikibaseClient.php) classes act as top level factories for services required inside of the respective extension.\\nThe methods to create\/retrieve them do not follow a standardized interface, and as many of them are required to build other services, the classes also hold a substantial amount of code to keep and pass references to the created instances once created. This code is low in conceptual value but highly repetitive and a burden on developers.\\nServices which are required in WikibaseRepo and WikibaseClient alike, e.g. [FormatterCacheFactory](https:\/\/gerrit.wikimedia.org\/r\/plugins\/gitiles\/mediawiki\/extensions\/Wikibase\/+\/62ae43e\/client\/includes\/WikibaseClient.php#1270), may also exhibit redundant implementations of their instantiator function as there is no ability to reuse them.\\nMediaWiki supports a concept called [ServiceWiring](https:\/\/www.mediawiki.org\/w\/index.php?title=Dependency_Injection&oldid=3977354#Quick_Start) which allows for the registration of services, is an important building block of recent MediaWiki and implements the well-known [PSR 11 container interface](https:\/\/www.php-fig.org\/psr\/psr-11\/).\\n\\n## Decision\\nWe will use MediaWiki ServiceWiring to describe and connect the services on which the wikibase repo and client extensions rely.\\n\\n\\n## Context\\nWe require something more sophisticated than handcrafted YAML when defining an application's Kubernetes resources, to ensure that applications can be defined once, and deployed to multiple targets with differing configurations. At a high level, we require a means to define an application's Kubernetes resources as an installable package, with configuration options passed in at deploy time.\\nThere are many options for packaging Kubernetes applictions, including but not limited to:\\n- [Helm](https:\/\/helm.sh)\\n- [Terraform](https:\/\/registry.terraform.io\/providers\/hashicorp\/kubernetes\/latest\/docs)\\n- [Kustomize](https:\/\/kubernetes.io\/docs\/tasks\/manage-kubernetes-objects\/kustomization\/)\\n- [Pulumi](https:\/\/www.pulumi.com\/docs\/intro\/cloud-providers\/kubernetes\/#pulumi-kubernetes-provider)\\n- [Skaffold](https:\/\/skaffold.dev)\\nWhile we are currently using Terraform for AWS infrastructure management, we should maintain a strict separation of concerns between managing cloud infrastructure and the software applications that run on top; using a single tool for both will inevitably lead to blurred lines between the two. In addition, [Terraform's kubernetes support is still quite new](https:\/\/www.hashicorp.com\/blog\/beta-support-for-crds-in-the-terraform-provider-for-kubernetes), and can lag behind Kubernetes itself.\\nWhile Kustomize has been adopted into the main Kubernetes project, that decision has [not been without controversy](https:\/\/goteleport.com\/blog\/kubernetes-kustomize-kep-kerfuffle\/). Kustomize has also not yet seen wide adoption within the industry, and offers little beyond the basic functions of interpolating values into YAML files.\\nHelm by comparison has widespread industry adoption, a [large number of existing packages](https:\/\/artifacthub.io\/packages\/search), and widespread support in the wider devops and development ecosystem (e.g. [Github Actions](https:\/\/github.com\/helm\/chart-releaser-action), [CircleCI Orb](https:\/\/circleci.com\/developer\/orbs\/orb\/circleci\/helm), [VS Code extension](https:\/\/marketplace.visualstudio.com\/items?itemName=ms-kubernetes-tools.vscode-kubernetes-tools), [Terraform provider](https:\/\/registry.terraform.io\/providers\/hashicorp\/helm\/latest\/docs)).\\n\\n## Decision\\nUse Helm v3+.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWikibase Manifest needs to provide essential metadata and configuration options about a Wikibase instance.\\\\nWe need to decide on the format of the information the manifest will provide.\\\\nWe took into consideration the following projects:\\\\n- the recent [REST API Prototype](https:\/\/github.com\/wmde\/wikibase-rest-fantasies) (by WMDE) and its [OpenAPI spec](https:\/\/raw.githubusercontent.com\/wmde\/wikibase-rest-fantasies\/gh-pages\/openapi.json)\\\\n- [OpenRefine's initiative](https:\/\/github.com\/OpenRefine\/wikibase-manifests) to collect manifests from different wikibases and their [json-schema spec](https:\/\/github.com\/OpenRefine\/wikibase-manifests\/blob\/master\/wikibase-manifest-schema-v1.json)\\\\nThe MediaWiki REST API which we are using (please refer to ADR 1 for more info) implements neither json schema nor openAPI.\\\\n### OpenAPI (swagger)\\\\nThe OpenAPI Specification (formerly Swagger Specification) is an API description format for REST APIs. An OpenAPI file allows you to describe your entire API. [Swagger](swagger.io) is a set of open-source tools built around the OpenAPI Specification, like the [api editor](https:\/\/editor.swagger.io\/).\\\\n- It\u2019s popularly used for mocking services and generating SDKs. It's not commonly used for run-time functionality.\\\\n- Useful when you want to describe your entire API.\\\\n- **OpenAPI is both a subset of JSON Schema Draft 5 and a superset**\\\\n### Json Schema\\\\nJSON Schema is a vocabulary that allows you to validate, annotate, and manipulate JSON documents.\\\\nThe specification is split into three parts, [Core](https:\/\/json-schema.org\/draft\/2019-09\/json-schema-core.html), [Validation](https:\/\/json-schema.org\/draft\/2019-09\/json-schema-validation.html), and [Hyper-Schema](https:\/\/json-schema.org\/draft\/2019-09\/json-schema-hypermedia.html).\\\\n- JSON Schema is a good option when there are data models whose schema needs to be defined\\\\n\"}, {'role': 'model', 'content': \"Use OpenAPI spec.\\\\nWe acknowledge that both are good options. We chose OpenAPI because the Wikidata team has created several products (e.g. [termbox](https:\/\/gerrit.wikimedia.org\/r\/plugins\/gitiles\/wikibase\/termbox\/+\/refs\/heads\/master\/openapi.json)) using the OpenAPI spec and plan on continue to do so when we have the opportunity.\\\\nThere're tools (e.g. [OpenAPI Schema to JSON Schema](https:\/\/github.com\/openapi-contrib\/openapi-schema-to-json-schema)) for converting from OpenAPI Schema Object or Parameter Object to JSON Schema in case the need arises to use json schema.\\\\n\"}]"}
{"File Name":"docnow-infra\/0003-use-terraform-enterprise-for-backend.md","Context":"## Context\\nTerraform writes plaintext of the state of our backend. The ability to collaborate in the workspaces is severely handicapped by this. Many groups use AWS and\/or GC storage with dynamodb locking on the state of the file to avoid clobbering on each other. Using Terraform Cloud for small teams will allow us a little more leeway and one less thing to manage.\\n","Decision":"Use Terraform Cloud for teams\\n","tokens":80,"id":329,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nIn [this issue](https:\/\/github.com\/raft-tech\/TANF-app\/issues\/771) we are investigating the use of pre-signed URLs to determine whether there are security issues with the approach.\\nWe had originally implemented pre-signed URLs for downloading files because the system would need to download the files from S3 and then download them again to the client, using resources on the backend for every download. This would not cause a problem in this stage of development, but when the backend will be charged with parsing data from potentially large files, those system resources would become more precious. Using pre-signed URLs takes the added pressure off of the backend entirely.\\n\\n## Decision\\nWe believe the use of time\/IP address limited signed URLs is a reasonably secure approach to downloading files from S3. However, we also believe that it may cause issues with our ATO approval as the data is highly sensitive. Furthermore, 18F published a recommendation today, [recommending to not use pre-signed URLs](https:\/\/engineering.18f.gov\/security\/cloud-services\/) for FISMA High projects.\\nIn our investigation we discovered a way that we can [securely download the files from the backend while [streaming the files](https:\/\/github.com\/jschneier\/django-storages\/blob\/master\/storages\/backends\/s3boto3.py#L83) directly from S3 to the client, taking any pressure off of resources needed for parsing files on the backend.\\nIn light of these facts we have decided to shift our efforts to download files from the backend.\\n\\n\\n## Context and Problem Statement\\nFollowing a GitOps operating model requires us to describe our running system in Git. Since we operate on OpenShift\/K8s, we can do this easily by specifying our entire system via `yaml` manifests and storing them in GitHub. Things get a bit more complicated when trying to reconcile where to store confidential information like `secrets`. We use the word `secrets` here a bit loosely to capture all confidential manifests (e.g. k8s secrets, routes with certs, etc.).\\nObviously we cannot store our `secrets` in Git in plaintext, we would like to store them in some encrypted fashion, ideally alongside the other system manifests. Other considerations are as follows:\\n- There should be freedom to store\/organize the `secrets` in any repo, path, and location.\\n- Encrypted manifests should be compatible with Kustomize.\\n- Ideally only the specific portions within manifests that are confidential will be encrypted (e.g. specific fields).\\n- ArgoCD should be able to decrypt these secrets and thus deploy them.\\n- Other teams can easily encrypt their manifests whilst maintaining ease of consumption by ArgoCD.\\n- Decryption capabilities are limited to code-owners, operate-first admins, and ArgoCD.\\n\\n## Decision\\nChosen Option `(3)`. Reasons are summarized as follows:\\n- `Sealed Secrets` only allow you to encrypt k8s `secrets`\\n- `Hashicorp Vault` a bit too heavy duty for our use case, steep learning curve. Secrets are also stored in a backend storage and not in Git\\n- We would like to avoid storing manifests in private\/internal repos. This is to stay in line with keeping our system's desired state as public as possible.\\nBy using KSOPS we can encrypt specific portions of a manifest that are confidential whilst keeping the rest of the non-confidential information visible. Encrypted manifests can be stored in whichever location as needed. By using an ArgoCD image that has KSOPS installed, we can have ArgoCD also decrypt these manifests.\\nSOPS allows you to use various methods to encrypt your manifests, we opt to use GPG as it's free to generate and use (other methods include using AWS KMS, GCP KMS, and Azure Key Vault).\\nWe will create a GPG key that we will use to encrypt all our manifests in Git. The secret key will be made available to ArgoCD and other operate-first admins. We offer the public key to other teams should they wish to encrypt their contents and store them in Git. Doing so will allow ArgoCD to decrypt and deploy their manifests. This means that other teams will be able to encrypt, but not decrypt, their manifests in Git. To get around this, SOPS allows you to encrypt the same files with multiple keys, allowing teams to encrypt a file using our Operate-First public key (where they do not have access to the private key) and their own key (for which they also have the private key). This will allow ArgoCD to decrypt their manifests, whilst also allowing teams to retain the ability to decrypt their manifests.\\n### Positive Consequences\\n- Can store secrets in Git alongside other manifests.\\n- Compatible with Kustomize.\\n- SOPS allows us to configure specific confidential fields we want encrypted inside manifests, while keeping other information public.\\n- ArgoCD can decrypt manifests.\\n- GPG key access, and SOPS configurations allow us to control who has encryption\/decryption capabilities.\\n### Negative Consequences\\n- By using GPG the cost of changing a key becomes expensive. For example, if a key is compromised, we need to update all `.sops.yaml` wherever that key is used and re-encrypt.\\n- KSOPS is a Kustomize plugin, and thus requires the `--enable_alpha_plugins`, this gives the impression that Kustomize plugins may be deprecated. However based on [this issue](https:\/\/github.com\/kubernetes-sigs\/kustomize\/issues\/1504), it seems the real purpose is to just `warn the user against accidentally running 3rd party plugins.`\\n- ArgoCD requires a custom image that includes the KSOPS tooling.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nTerraform writes plaintext of the state of our backend. The ability to collaborate in the workspaces is severely handicapped by this. Many groups use AWS and\/or GC storage with dynamodb locking on the state of the file to avoid clobbering on each other. Using Terraform Cloud for small teams will allow us a little more leeway and one less thing to manage.\\\\n\"}, {'role': 'model', 'content': 'Use Terraform Cloud for teams\\\\n'}]"}
{"File Name":"libelektra\/commit_function.md","Context":"## Problem\\nWhen `kdbSet()` is called, plugins implementing the commit role need to\\ntrack their state to distinguish between carrying out that role and\\ncarrying out potential other roles (commit and setresolver for the\\nresolver plugin, for example). This limits the possibilities of plugin\\nreuse and the ways plugins can be combined.\\n","Decision":"Committing will no longer be done by `kdbSet()`. Instead, the functionality\\nwill be implemented by its own function, `kdbCommit()`.\\n","tokens":70,"id":1300,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nMultiple projects need to publish or subscribe to messages from the `bus`.\\nProjects are written in a fixed set of supported programming languages.\\nThere is a set of non-functional requirements to cater for in integrations, especially when listening to queues (retries, graceful shutdown, timeouts, etc).\\n\\n## Decision\\nAccess the bus exclusively through the eLife SDKs, either for publishing or subscribing to messages:\\n- [bus-sdk-php](https:\/\/github.com\/elifesciences\/bus-sdk-php)\\n- [bus-sdk-python](https:\/\/github.com\/elifesciences\/bus-sdk-python)\\n\\n\\n## Context and Problem Statement\\nThis addresses the fact that we could not configure different environments to point to different API URLs as necessary (across Int, Test, and Live envs in Cosmos).\\nAt the point of deployment, `npm run build` is called in Jenkins to build the static  bundle and minify etc. Environment config (`REACT_APP_SERVER_URL` etc. variables declared in `.env`) get baked into the bundle and obfuscated at this point.\\nWe would need different Jenkins jobs or parameters to bake in the environment config (eg. the correct server URL) multiple times, which goes against the idea of having an _unconfigured \/ environment agnostic_ RPM in Cosmos.\\n## Decision Drivers <!-- optional -->\\n* Avoiding too much extra code overhead\\n* Trying to avoid security converns with proxy solutions (injection \/ DB manipulations)\\n* Avoiding having to re-build in Cosmos or on the box\\n\\n## Decision\\n* Avoiding too much extra code overhead\\n* Trying to avoid security converns with proxy solutions (injection \/ DB manipulations)\\n* Avoiding having to re-build in Cosmos or on the box\\nThis is a common problem with create-react-app and the need to configure it. We came across [this solution](https:\/\/github.com\/facebook\/create-react-app\/issues\/578#issuecomment-277843310) which detailed a simple way of consuming config with the `window` global.\\nIn source code this remains localhost:8080 as a local development fallback and it is passed in via Cosmos environment config.\\n\\n\\n\\nProvide a decision given the context below:\\n## Problem\\\\nWhen `kdbSet()` is called, plugins implementing the commit role need to\\\\ntrack their state to distinguish between carrying out that role and\\\\ncarrying out potential other roles (commit and setresolver for the\\\\nresolver plugin, for example). This limits the possibilities of plugin\\\\nreuse and the ways plugins can be combined.\\\\n'}, {'role': 'model', 'content': 'Committing will no longer be done by `kdbSet()`. Instead, the functionality\\\\nwill be implemented by its own function, `kdbCommit()`.\\\\n'}]"}
{"File Name":"monocle\/0004-high-level-components.md","Context":"## Context and Problem Statement\\nWe want to start with a set of components each dedicated to a task to ensure Monocle being able to scale and also to fit well in a cloud like deployment. No monolithic app, but microservices. As we start we also don't want to overengineer, so just keep the basic components and architecture.\\n","Decision":"Choosen Option: \"A database, a crawler, a, CLI, a WEB API, a WEB UI\".\\nBecause, it fits the described context. The database (Elasticsearch) is scalable, and the other components are stateless, each of them use the database or the API as backend so this will ease in term of scalability. This architecture makes it easy to deploy using docker\/podman or even k8s.\\n","tokens":72,"id":4641,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe were using [Material Design Lite](https:\/\/getmdl.io\/) as our front-end CSS framework.\\nThe selection of offered components is quite limited and most of them just include\\nthe basic logic. We have to dedicate a big amount of time writing front-end code and it's\\nhard to compose the provided components as they are mainly meant to be used in a single\\nway because of the strong Material design guidelines.\\nWe aim to focus on logic, not on developing front-end components. We need a solution\\nthat can offer a high number of already built components. The components should be\\neasy to configure and to compose together as our UI sometimes needs components that\\nare not contemplated on the common guidelines.\\n\\n## Decision\\nWe will use [Semantic UI](https:\/\/semantic-ui.com\/), as it's an open source and robust framework.\\nIt has a wide range of components, it's easily customizable and it uses modern\\napproaches. It's really well documented, although usually documentation is not even\\nneeded as it's really intuitive to work with it.\\n\\n\\n## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n\\n## Decision\\nUse Rollbar to collect and manage our application errors.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\nWe want to start with a set of components each dedicated to a task to ensure Monocle being able to scale and also to fit well in a cloud like deployment. No monolithic app, but microservices. As we start we also don't want to overengineer, so just keep the basic components and architecture.\\\\n\"}, {'role': 'model', 'content': 'Choosen Option: \"A database, a crawler, a, CLI, a WEB API, a WEB UI\".\\\\nBecause, it fits the described context. The database (Elasticsearch) is scalable, and the other components are stateless, each of them use the database or the API as backend so this will ease in term of scalability. This architecture makes it easy to deploy using docker\/podman or even k8s.\\\\n'}]"}
{"File Name":"profiles\/0002-store-oauth2-clients-in-json.md","Context":"## Context\\nProfiles acts as a proxy for ORCID's OAuth2 service, and has its own list of (eLife) clients, eg Journal. This list is small and reasonably static.\\n","Decision":"The list of clients will be maintained in a JSON file.\\n","tokens":40,"id":2805,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nOur users need an easier way to write alerts.  They currently have no\\neasy way to test queries before writing their alerts.\\nIn principle, they could use Prometheus's expression browser for this,\\nbut our Prometheus service requires basic auth, which locks our users\\nout of it.\\n\\n## Decision\\nWe will give our users access to Prometheus so they can use its\\nexpression browser to test queries when writing alerts. We will\\ndo this by using IP whitelisting instead of basic auth and\\nonly allowing our office IP addresses.\\nWe identified a number of different routes that we could have taken to\\nallow our users to access Prometheus.  One possible route that we\\nconsidered was using Oauth2 authentication. This would enable users to\\nauthenticate themselves to the platform with their Google account.\\nWe did not choose to go with this option this time for expediency.\\nThe idea behind this was to try to deliver the fastest value as\\npossible to the user. This method enables us to learn more about the user's\\nusage pattern. We do intend to add authentication but this will be done at\\na later date.\\n\\n\\n## Context\\nThe ETL process takes a long time to run, approximately 6 hours.\\nRunning the ETL process makes close to 30,000 requests to the Syndication API.\\nSeveral applications need access to the JSON file created by the ETL.\\n\\n## Decision\\nRather than each application running its own copy of the ETL to obtain the Syndication data in JSON format,\\na single instance of the ETL will run and provide access to the resultant file via an nginx web server\\nrunning in the ETL container.\\nThe output JSON is hosted in the container to remove reliance on external solutions, such as Azure.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nProfiles acts as a proxy for ORCID's OAuth2 service, and has its own list of (eLife) clients, eg Journal. This list is small and reasonably static.\\\\n\"}, {'role': 'model', 'content': 'The list of clients will be maintained in a JSON file.\\\\n'}]"}
{"File Name":"tdr-dev-documentation\/0021-service-unavailable-page.md","Context":"## Context\\nWe need a mechanism to manually deploy a service unavailable page. There are two main reasons we might want to do this.\\n* There is a serious issue with the site that we can't fix immediately.\\n* We want to carry out maintenance which will affect the service.\\nThis will be a rare occurrence as almost all deployments will be done with zero downtime.\\nThe page will be a single static html page which will be displayed when a user goes to any of the https:\/\/tdr.nationalarchives.gov.uk\/* urls.\\n![Service Unavailable Design](images\/service-unavailable-design.png)\\n","Decision":"We are going to use the target group switching method. This allows us to deploy the page manually and the changes are visible to the user almost instantly.\\nBecause of this decision, we were constrained in how we implement the page. Target groups can forward to IP addresses or to lambdas but not to static sites hosted in S3 or ECS containers in a private subnet without using other resources like Cloudfront which we decided is too large an architecture change for this use case.\\nWe will create this page as a lambda, using Python to create a small web server which will render the html and also load the css and images that we need.\\n","tokens":124,"id":1780,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nIn [ADR032](\/architecture_decision_records\/ADR032-ssl-only-for-applications-and-cf-endpoints) we decided that\\nwe would only support https for applications on the PaaS, and that we would\\ndrop plain http connections (port 80).\\nSince then, we've observed this causing confusion for users on numerous\\noccasions where they think their app isn't working after being pushed.\\nThe situation is improved with the inclusion of the `cloudapps.digital` domain\\nin the [HSTS preload](https:\/\/hstspreload.org\/?domain=cloudapps.digital) list,\\nbut this only helps users with recent versions of modern browsers.\\nAs a result of the continued confusion for users we should revisit the decision\\nfrom ADR443.\\nThere are a number of things that could be done to address this:\\n### Update the CF CLI to include the scheme\\nCurrently, the CF CLI outputs the fully-qualified hostname of the app after\\npushing, but doesn't include the scheme. This has caused confusion for users\\nwhen this is copy\/pasted into browsers, and then times out.\\nGetting the CLI to include the scheme here will help with the specific case of\\nusers getting confused immediately after pushing an app.\\nIt's unclear how much work this involves, as currently information about\\nwhether a route is http or https doesn't appear to be modeled in CloudFoundry\\nanywhere.\\nIf this involves changes to the CLI, there's no guarantee that users will\\nupgrade their CLI.\\nAdditionally, there is some debate about how effective this change would be. It\\nwill probably fix some cases, but won't cover everything.\\n### Redirect http to https\\nAdd an endpoint that listens to all http requests on cloudapps.digital and\\nredirects them to the corresponding https URL.\\nThere's a risk with this that a service could link to the http version of a\\npage by mistake and not notice due to the redirect. We can mitigate this be\\nhaving the redirect strip the path and query when redirecting so that it always\\nredirects to the base URL.\\nThere's another risk that a misconfiguration could allow non-encrypted traffic\\nthrough to applications. This would need to be mitigated by having acceptance\\ntests to cover this.\\n\\n## Decision\\nWe will redirect http traffic to the corresponding root https endpoint.\\nWe will continue to maintain HSTS preload lists for our production domains.\\n\\n\\n## Context\\nContent Publisher regularly has to present users with long lists of data that\\nis loaded from the Publishing API, examples of these are organisations,\\ncontacts and the topic taxonomy. Content Publisher typically needs to\\ndownload all the records of a particular datatype, which can involve\\nmultiple HTTP requests. When this is done during a web request the application\\nis at an increased risk of returning a slow response or even timing out.\\nThe risks of this have been partially mitigated by caching.\\nThese situations where Content Publisher needs all the data records for a\\nparticular dataset are colloquially referred to as bulk data. This is\\ndistinguished from other API look ups where we may need to access a\\nsingle resource (such as looking up the topic tags) that have less risks of\\nperformance penalties.\\nThis particular problem has been faced and solved for GOV.UK before. Whitehall\\ntakes an [approach](https:\/\/github.com\/alphagov\/whitehall\/pull\/3298) for the\\ntopic taxonomy where the data is loaded at runtime from\\n[Redis](https:\/\/redis.io\/). A periodic background job then refreshes the\\ndata in Redis. This avoids the need to look up the data during a web request.\\n\\n## Decision\\nIn Content Publisher we have chosen to implement a similar pattern to Whitehall.\\nData is read from Redis at runtime and a background process runs a periodic\\njob to re-populate the Redis data.\\nThis approach makes use of the [Rails Redis cache store][redis-cache-store]\\nwhich is wrapped within an object, [BulkData::Cache][bulk-data-cache], for\\nwriting and accessing data. We have introduced a concept of\\n[repository][government-repository] classes that can be used to work with the\\nloading and modelling of data that is stored in the cache.\\nOn application boot and on a recurring 15 minute interval a\\n[background job][populate-bulk-data-job] will run to try to re-populate the data.\\nIn the event of an error occurring this job will log this issue and retry. If\\nretries are exhausted a decision will be made whether to\\n[send the error to Sentry or not][error-handling] based on whether the error\\nis likely to be a problem we will investigate.\\nThe data within the bulk data cache is stored with an expiry of 24 hours, which\\nis a lot longer than the interval we use to re-populate the cache. The reason\\nfor such a long time is to provide as a safety net for problems to occur and\\nnot to present any sign of issues to users until this period has expired. If a\\nfrontend request is made for a resource that requires bulk data, and the\\ncache entry for that is empty, a [503 response][unavailable-response] is\\nreturned to the user and the job to re-populate the cache is enqueued.\\nIf we hit a scenario where there are errors with the jobs there is a chance\\nthat we will slowly build up a large queue of this same job multiple times. To\\nprevent this situation causing any significant problems (such as flooding the\\nPublishing API with requests) the job will only populate data where [the\\ndata is older than 5 minutes][older-than-check].\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need a mechanism to manually deploy a service unavailable page. There are two main reasons we might want to do this.\\\\n* There is a serious issue with the site that we can't fix immediately.\\\\n* We want to carry out maintenance which will affect the service.\\\\nThis will be a rare occurrence as almost all deployments will be done with zero downtime.\\\\nThe page will be a single static html page which will be displayed when a user goes to any of the https:\/\/tdr.nationalarchives.gov.uk\/* urls.\\\\n![Service Unavailable Design](images\/service-unavailable-design.png)\\\\n\"}, {'role': 'model', 'content': 'We are going to use the target group switching method. This allows us to deploy the page manually and the changes are visible to the user almost instantly.\\\\nBecause of this decision, we were constrained in how we implement the page. Target groups can forward to IP addresses or to lambdas but not to static sites hosted in S3 or ECS containers in a private subnet without using other resources like Cloudfront which we decided is too large an architecture change for this use case.\\\\nWe will create this page as a lambda, using Python to create a small web server which will render the html and also load the css and images that we need.\\\\n'}]"}
{"File Name":"corona-hackathon\/0009-frontend-framework.md","Context":"## Context\\nWe have to decide for a frontend application and a styling\/component framework.\\n","Decision":"We stick to [Angular](https:\/\/angular.io\/) together with [NG ZORRO Ant Design](https:\/\/ng.ant.design\/) and [NG Alain](https:\/\/ng-alain.com\/).\\n","tokens":18,"id":3656,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe have to decide for a programming language \/ application framework.\\n\\n## Decision\\nWe stick to [Spring Boot](https:\/\/spring.io\/projects\/spring-boot), with [Java 11](https:\/\/docs.aws.amazon.com\/corretto\/latest\/corretto-11-ug\/downloads-list.html) and [Gradle](https:\/\/gradle.org\/).\\n\\n\\n## Context\\nWe have to decide for a framework for web controller \/ adapter.\\n\\n## Decision\\nUtilize [Spring Reactive Web](https:\/\/docs.spring.io\/spring-framework\/docs\/5.0.0.M1\/spring-framework-reference\/html\/web-reactive.html).\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe have to decide for a frontend application and a styling\/component framework.\\\\n'}, {'role': 'model', 'content': 'We stick to [Angular](https:\/\/angular.io\/) together with [NG ZORRO Ant Design](https:\/\/ng.ant.design\/) and [NG Alain](https:\/\/ng-alain.com\/).\\\\n'}]"}
{"File Name":"ADR\/0014-drop-the-no-use-of-legacy-in-new-code-restriction.md","Context":"## Context\\nOriginally, PrestaShop was made out mainly of static classes, with no dependency injection. To address that problem, it [was decided](https:\/\/build.prestashop.com\/news\/new-architecture-1-6-1-0\/) that non namespaced code would be progressively refactored into a `Core` namespace, which would only contain code using with dependency injection. Furthermore, Core code wouldn't be allowed to depend directly on non namespaced classes, but it could to it indirectly by the means of `Adapter` classes that would act as a bridge between new and old code.\\nThe \"no direct dependency between Core and Legacy\" rule led to an ever-growing collection of adapters, which resulted in greatly increased code complexity and duplication. In some cases, the same service can have a legacy, adapter and core implementations, with subtle differences between each one. Furthermore, the constraints of backward compatibility further increase the difficulties to refactor code into Core, because the surface of the \"public API\" is larger.\\n","Decision":"The following decision applies to both `Core` and `PrestaShopBundle` classes (referred as to \"Core\" for shortness):\\n1. **All new Core classes SHOULD be placed either in the `Core` or the `PrestaShopBundle` namespace**, following on the rules established previously.\\n- New classes MUST NOT be added to the `Adapter` namespace, and SHOULD NOT be added to the legacy (root) namespace.\\n2. **Core classes MAY depend on instances of legacy classes**, provided the following rules are respected:\\n- Legacy classes MAY be used either as injected parameters or constructed within, but caution must be exerted when using legacy classes that produce side effects, have global state or don't guarantee internal consistency. In those cases, these classes SHOULD be accessed through dedicated services which enforce consistency.\\n- Core classes MUST NOT call static methods on other classes, except for factory methods, stateless tool methods, or within services dedicated to encapsulate a static class.\\n- Core classes MAY access to data provided by static classes or methods static classes by relying on dedicated services (Application services, Repositories, Data Providers...).\\n3. **Core classes MUST NOT reimplement code found in legacy classes**, without deprecating the original method\/class (and optionally, making it rely on the new implementation).\\n4. **The Adapter namespace MUST be phased out** eventually:\\n- Classes in the Adapter namespace MUST be copied to the Core namespace.\\n- The original Adapter classes MUST be emptied out, made to extend the Core classes, and deprecated so that they can be fully removed in a following major.\\n- Adapter services MUST be deprecated and copied into the core namespace as well.\\n- Code MUST NOT depend on Adapter classes or services.\\n","tokens":207,"id":439,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nOur initial design for tenancy in Corvus (which necessarily affected Marain.Tenancy) comingled read and write behaviour. The model was similar to the .NET Entity Framework: if you wanted to modify a tenant, you would first fetch an object representing that tenant, then make changes to that object, and then invoke an operation indicating that you wanted those changes to be written back.\\nWe made various changes to the Property Bag system that tenancy uses to store tenant properties to disassociate the API from any particular JSON serialization framework. We had previously forced a dependency on Json.NET, but we wanted to be able to move onto `System.Text.Json`, so we wanted to introduce a Property Bag abstraction that was independent of serialization mechanism (although still with a presumption that it must be possible for the properties to be serialized as JSON).\\nOne of the basic principles of efficient JSON parsing in the new world is that you don\\'t build an object model representing the JSON unless you really need to. Ideally, you leave the JSON in its raw UTF-8 state, referred to via one or more `IMemory<byte>` values, and extract what data you need only as you need it. This can dramatically reduce GC pressure, particularly in cases where most of the data in question is not used most of the time. However, this model does not fit well with the \"modifiable entities\" approach to updates. If anything is free to modify the properties at any time, this implies an ability to edit or regenerate the JSON.\\nIn practice, modification of tenant properties is the exception, not the rule. Most Marain services will only ever fetch tenant properties. Only the Marain.Tenancy service should normally directly edit these properties. So the \"modifiable entities\" approach is not really necessary, and causes problems for migration to allocation-efficient strategies.\\n\\n## Decision\\nSince `Corvus.Json.Abstractions` separates out read and update operations for `IPropertyBag`, and `Corvus.Tenancy` therefore does the same (since it uses property bags), Marain.Tenancy will follow suit.\\nThe web API presented by Marain.Tenancy for modifying tenants uses JSON Patch. So instead of this procedure:\\n* fetch a serialized representation of an ITenant from the web API\\n* modify that representation to reflect the changes you wish to make\\n* PUT that serialized representation of an ITenant back to the web API\\nwe now use this procedure instead:\\n* send a PATCH request in describing the changes required in JSON Patch format\\nFor example, to rename a tenant, you would send this PATCH to the Marain.Tenancy service, using the URL representing the tenant (the same URL from which you would fetch the tenant if reading) with an `application\/json-patch+json` content type:\\n```json\\n[{\\n\"path\": \"\/name\",\\n\"op\": \"replace\",\\n\"value\": \"NewTenantName\"\\n}]\\n```\\nJSON Patch supports multiple changes in a single request, e.g.:\\n```json\\n[\\n{\\n\"op\": \"add\",\\n\"path\": \"\/properties\/StorageConfiguration__corvustenancy\",\\n\"value\": {\\n\"AccountName\": \"mardevtenancy\",\\n\"Container\": null,\\n\"KeyVaultName\": \"mardevkv\",\\n\"AccountKeySecretName\": \"mardevtenancystore\",\\n\"DisableTenantIdPrefix\": false\\n}\\n},\\n{\\n\"op\": \"add\",\\n\"path\": \"\/properties\/Foo__bar\",\\n\"value\": \"Some string\"\\n},\\n{\\n\"op\": \"add\",\\n\"path\": \"\/properties\/Foo__spong\",\\n\"value\": 42\\n}\\n]\\n```\\nThe `op` can be set to `remove` to delete properties.\\nClients will not typically build these PATCH requests themselves, because the `ClientTenantStore` type contains the relevant code. `ClientTenantStore` provides an implementation of `ITenantStore` that works by using the web API provided by Marain.Tenancy. So in practice, updating the name of a tenant is as simple as:\\n```csharp\\nawait tenantStore.UpdateTenantAsync(tenantId, name: \"NewTenantName\");\\n```\\nAdding or changing a property looks like this:\\nawait tenantStore.UpdateTenantAsync(\\ntenantId,\\npropertiesToSetOrAdd: new Dictionary<string, object>()\\n{\\n{ \"StorageConfiguration__corvustenancy\", myStorageConfig },\\n{ \"SomeOtherSetting\": 42 },\\n});\\n\\n\\n## Context\\nBefore using ADR we decided that we would use vue. Vue needs a form of state management to work with. State in vue can be persisted on a vue component level as raw data objects but with growing scale and complexity of the application the need to share this state between components, without additional safety measures, can lead to code that is hard to understand and\/or debug.\\nThere is an officially recommended state management implementation for vue [[0]] - Vuex; which is community-supported, free, and follows the renowned Flux pattern [[1]].\\nVuex offers using a \"single source of truth\" for the application [[2]] with added guards. Only mutation through events are possible. This helps to protect from the down-sides of scattered code holding reference to the same objects.\\nVuex ships with an existing best practice for structure [[3]] and out-of-the-box integration with debug tools. The nature of event-based state mutation provides benefits like ease of debugging (\"time-travel\") and promises maintainability in a growing application.\\nWe already use Vuex within the WikibaseLexeme codebase. See: `resources\/widgets\/LexemeHeader.newLexemeHeaderStore.js`\\nThere is also an existing approach within WikibaseLexeme to persist, pass and partially validate state using rich models (`wikibase.lexeme.datamodel.Lexeme` et al.). These were written to mimic the backend data model but did so incompletely. To accomodate the need for components in a (temporarily while editing) \"dirty\" state, another `wikibase.lexeme.LemmaList` kind of state was added pointing out the need for flexibility beyond the model\\'s capability in this regard.\\nLosing object-oriented accessors and existing \"validation\" rules, for an apparent anemic model [[4]], could feel cumbersome at first [[5]] and consequences of actions are not always immediately obvious. But Vuex\u2019s event-based store system provides us with an extensible interface for us and 3rd parties that offers flexibility, robustness, existing & well-maintained documentation.\\n\\n## Decision\\nWe will use Vuex as state management.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nOriginally, PrestaShop was made out mainly of static classes, with no dependency injection. To address that problem, it [was decided](https:\/\/build.prestashop.com\/news\/new-architecture-1-6-1-0\/) that non namespaced code would be progressively refactored into a `Core` namespace, which would only contain code using with dependency injection. Furthermore, Core code wouldn\\'t be allowed to depend directly on non namespaced classes, but it could to it indirectly by the means of `Adapter` classes that would act as a bridge between new and old code.\\\\nThe \"no direct dependency between Core and Legacy\" rule led to an ever-growing collection of adapters, which resulted in greatly increased code complexity and duplication. In some cases, the same service can have a legacy, adapter and core implementations, with subtle differences between each one. Furthermore, the constraints of backward compatibility further increase the difficulties to refactor code into Core, because the surface of the \"public API\" is larger.\\\\n'}, {'role': 'model', 'content': 'The following decision applies to both `Core` and `PrestaShopBundle` classes (referred as to \"Core\" for shortness):\\\\n1. **All new Core classes SHOULD be placed either in the `Core` or the `PrestaShopBundle` namespace**, following on the rules established previously.\\\\n- New classes MUST NOT be added to the `Adapter` namespace, and SHOULD NOT be added to the legacy (root) namespace.\\\\n2. **Core classes MAY depend on instances of legacy classes**, provided the following rules are respected:\\\\n- Legacy classes MAY be used either as injected parameters or constructed within, but caution must be exerted when using legacy classes that produce side effects, have global state or don\\'t guarantee internal consistency. In those cases, these classes SHOULD be accessed through dedicated services which enforce consistency.\\\\n- Core classes MUST NOT call static methods on other classes, except for factory methods, stateless tool methods, or within services dedicated to encapsulate a static class.\\\\n- Core classes MAY access to data provided by static classes or methods static classes by relying on dedicated services (Application services, Repositories, Data Providers...).\\\\n3. **Core classes MUST NOT reimplement code found in legacy classes**, without deprecating the original method\/class (and optionally, making it rely on the new implementation).\\\\n4. **The Adapter namespace MUST be phased out** eventually:\\\\n- Classes in the Adapter namespace MUST be copied to the Core namespace.\\\\n- The original Adapter classes MUST be emptied out, made to extend the Core classes, and deprecated so that they can be fully removed in a following major.\\\\n- Adapter services MUST be deprecated and copied into the core namespace as well.\\\\n- Code MUST NOT depend on Adapter classes or services.\\\\n'}]"}
{"File Name":"smjs\/2019020401-handling-generic-input-output.md","Context":"## Context\\nCurrently we use a single View from the Presenter, to render the normal output of the use case, that is bound to the View interface in the general provider. However, we're not handling different views that might be needed, nor error messages, which are directly written to the STDERR of the application.\\nRegarding the former, we're not allowing the user to choose the output representation she prefers to get. Regarding the latter, if we're decoupling the presenter from the controller to support segregation between input and output devices, we cannot then just assume that the input device will also be able to support output, like we're doing now.\\n","Decision":"### Segregation of input and output devices\\nWe assume that a generic application can be connected to several different input and output devices: this means that input may come from different input devices (for example keyboard and mouse), and the output may go to different output devices (for example `STDOUT`, a GUI, and the filesystem). Of course this requires us to drop the assumption that all usual Web applications make, that the input device is the same as the output device (a Web browser, or maybe a Web\/application server to be more precise).\\nOne of the objectives of modern architectures is exactly allowing multiple different output devices to be used by the same application, and also from the same input device. To allow this, it's necessary that the controller is oblivious of the specific output device that will be used: for example, the controller shouldn't know if the output is going to a GUI, a CLI, a log, an audio speaker or a combination of these.\\nIt's also important to mention that the strict separation between input and output is an application-level concern, which expects input to come from an interface (input boundary), and to send output to a different interface (output boundary). This is important because the application doesn't need to care if the device providing the input is the same as the one receiving the output or not. However, in most cases the input device will also support some kind of output, at least to display errors of input validation, and it would be very weird if the input came from a device, and the output were sent to a different one, because the user using the first device would expect to see the output there as well. However, this kind of architecture easily supports also cases where the input device has no output capability, and the output is to be sent somewhere else (like a game controller and a monitor).\\n### Controllers and presenters\\nWhile a use case interactor performs its application logic, it may not need to perform a single presentation, for two reasons:\\n- there are many different kinds of presentations, that will likely need to be handled differently, like main successful output, and error output\\n- there are some presentations that will be produced immediately, like input validation results, and some that may be produced asynchronously, like command execution results\\nFor this reason, the interactor will in general have to define several different presenter interfaces, instead of just one, and juggle the various presenters according to the application logic that is unfolding.\\nNow, we're describing this on the grounds that the presenters will be injected into the interactor. Now, in theory it's possible to achieve the same by returning a single response from the interactor, instead of letting the interactor call the presenters. This would actually work, and keep the dependency inversion principle respected, since the interactor still knows nothing of the details of presentation, however:\\n- we need to stuff all possible response cases into a single response object, instead of cleanly separate the various cases\\n- the controller will need to parse the response, understand what case it's about, and call different presenters according to it, adding much more responsibility to the controller than just having to translate the input data from the adapter format to the application format; additionally, the controller would really be just unpacking the same information that was packaged by the application, i.e. that a certain presenter needs to be called in a certain output situation, and the controller cannot deviate from this logic either, otherwise it would be taking presentation responsibility\\n- the single response object could be returning asynchronous output, in addition to synchronous one, so the controller will also need to check which part of the response should be treated as asynchronous (like attaching a callback to a promise), and which as synchronous\\n- even if the interactor was a coroutine (avoiding the problem of creating a single response for all cases), the controller would still need to check the kind of output at each return, and do the job of the interactor of associating a presenter to that kind of output, while the interactor already knows this information in the first place, in addition to still having to handle asynchronous output\\nFor these reason, although returning the response from the interactor is technically feasible (and in certain scenarios it's certainly the best solution), it cannot be chosen as the general approach to use: rather it should be regarded as a special case, that works best only in specific situations.\\n### Presenters and views\\nEach specific presenter that can be used to implement the output port represents a specific output model, which is characterized by the set of view fields and data type conversions it supports, or in other words, the kind of view model it will produce. For example, a \"screen\" view model might convert the application response to a certain set of fields of certain type, with the intent of having them being displayed to a screen, while a \"print\" view model might produce from the same application response a different set of fields, of different types, specialized for being sent to a printer.\\nStill, the same view model, representing a certain output model, can be rendered on the selected output device in different ways: a \"screen\" output model, containing fields belonging to a screen representation, might still be rendered as an HTML page, or as TXT file, or again as a graph. All these alternatives are represented by different views. This means that a specific presenter component, for example the \"screen\" presenter component, will define a \"screen\" view interface, that will then be implemented by multiple actual \"screen\" view instances, like HTML screen view, graph screen view, etc., one of which will then be selected and injected in the presenter, to be used for rendering.\\n### Input definition\\nWe borrow a definition of \"input\" from the REST standard. According to this definition, an input will be made of:\\n- a resource identifier (for example `\/path\/to\/my\/resource`, URI in the Web context)\\n- some input parameters (for example `?some[]=value&some[]=other`)\\n- a resource representation (for example `.html`, `.pdf`, etc.)\\nthis means that:\\n- the identifier is used to pick a couple of controller and action, that needs to be called to perform the requested use case\\n- the parameters are just passed to the action when it's called\\n- the representation is related to the specific view that will be used to render the output generated by the use case's presenter; additionally, once a view is selected, it's presenter is also chosen, because a presenter might have multiple views, but a view only belongs to one presenter\\nFor example, let's assume we have an `entitiesList` control widget, which displays a specific representation of a list of entities. When we trigger the `update` event on the `entitiesList` control widget, a very specific input can be created: the identifier would reference the controller and the action, like `entitiesController.update`; the input parameters could perhaps be the name of a subset of entities that this specific widget is configured to represent; finally, the representation is related to the specific widget that is sending this input, meaning that the same resource needs different widgets to be represented with different views.\\n### Using a router\\nThe interesting problem that arises is that the view to be chosen is known at input time, because it's related to the representation: this may lead to think that it should be known by the controller. However, the controller should be independent from any choice regarding the output, including which representation is used.\\nA solution can be to introduce a *router* component, that takes an input, and chooses a combination of *controller*, *action* and *view* according to the given input. This is much like what happens with usual MVC Web framework, but generalized for any kind of adapter. This choice made by the router can be statically configured, since it does not depend on the user input.\\nThe input part is quite easy, in that the router needs just to create the right controller object, and call the right action method on it, passing the right parameters. The output part is trickier though: since the controller is decoupled from the presenter, it doesn't return any output object that can be then handled by the router, rather it forwards the execution to the presenter object, in a way that isn't in control of the router. What the router can do, though, is to select the correct view to be used, and use the container to bind it to the generic view interface. This way, when the presenter will be built, the right view object will automatically be injected into it.\\n### Main output and side output\\nA typical application can have many different kinds of output. One of these is used to talk back to the user that provided the input: we can call it *main output*, and it would be produced by the selected use case. The *side output* would be any other kind of output produced by the application, like logging, monitoring, etc., which is not directly related to the input, and usually is not expected to be received by the user who produced the input.\\nWhile the main output is handled with a presenter, side output can be handled by using a message bus to send it to a non-specified recipient from the application layer: inside the adapter, then, one or more configured handlers will receive the messages, and output them to the configured devices.\\nAs a case study, we can consider the verbose output of a console application. At first, this may seem a case where some side output (info logs) need to be redirected to the main output (console STDOUT). What's really happening, though, is that the side output is being produced in a way that is completely unrelated to the presenter and views used by the main output: messages are sent containing the side output, and then in the adapter a new handler is configured to listen to these messages, and print them to the same output device used by the main output. This way we can print to the same output device, but keep the two kinds of output completely independent.\\n### Handling errors\\nWe can identify three broad categories of errors: application errors, input errors and system errors. Errors generated by domain or application services can be caught inside the use case, and passed back to the presenter in the response. Input errors are related to wrong input, or input that doesn't pass validation, and they are again checked inside the application interactors: they shouldn't be handled by controllers, because controllers have no way to select a specific output device to forward errors to, and additionally it's better if controllers stick to their single responsibility of translating data using adapter-specific format, into request data using application-specific format; on the other hand, we can say that the use case interactor is taking the role that so-called \"controllers\" have in traditional Web applications, meaning validating input, using proper services to produce an output, and send the output to the proper presenter\/view. Finally, system errors are programming or configuration errors that may happen in the adapter-specific code, and that are generally caught during development, but which may still happen in production, and make the application crash: since these errors might happen outside of the application logic, they cannot be handled by the application service, like application and input errors, but still they should be caught and displayed on an output device to notify the user.\\nSystem errors, not being generated inside the use case, don't concern any use case, and as such they don't need to be handled by any presenter. The way to handle them is to let any piece of code that is catching them sending a message to a widget that is responsible for displaying error messages, without needing to go through any application logic. Errors generated by domain or application services can be caught inside the use case, and passed back to a specific error presenter, which in turn will be configured with a specific view to be used in that situation.\\n### User interface widgets\\nAny user interface, whether graphical or not, is composed of widgets, representing specific contexts for user interaction and presentation of data.\\nUser interfaces are meant to be frequently changed, so they need to be as flexible as possible. To support this, widgets have to be designed so that they are as independent, reusable and replaceable as possible. A key tool to achieve this is using an events mechanism: instead of knowing of the existence of a specific widget, and calling a specific method on it, we send an event, and widgets that are interested in that event will respond to it with some action. This way we can easily replace widgets, or add new ones, without disrupting existing functionality.\\nEvents can be generated by the domain, the application, or the widgets themselves. Thus, widgets must be allowed to know about all existing events. Of course who generates an event depends on where the event belongs to: if an event represents the completion of a domain task, then the domain will need to send it, whereas if an event represents the change of state of a graphical widget (not related to any application or domain concern), then the widget should send it.\\nSometimes, however, it's not possible, or not convenient, to interact with widgets using events. For example it can happen that we need to be sure that a specific widget is actually called, but the event system doesn't ensure that any widget will respond to the event: for example if we want to display an error message, it doesn't make sense to send an event, hoping that a message box widget will catch it, rather we want to make sure that the message is actually displayed the way we need. To support these cases, widgets' actions must also be callable directly.\\nWidgets can then be just regular objects, exposing a set of methods: these can both be registered as event handlers, to be automatically called when certain events are caught, and be called directly on the object itself. This way we can take into account any possible combination of input and output methods, without creating a taxonomy of widgets that can have input and output, or only input and not output, etc., which is not really relevant on the user interface layer.\\n### Widgets and use cases\\nThe model of the communication with the application layer (i.e. with a port) distinguishes controllers from presenters, where the first are responsible for handling input, and the second are responsible for handling output. The reason why the two must be separated is that the presenters need to be injected into the use case interactor, since it may need to call different kinds of presenters in different situations, and also in an asynchronous fashion, and these are concerns of the application layer, not of the adapter.\\nOf course the use case interactor knows nothing of which widgets are used in the adapter, nor of the kind of user interface there is, so it cannot decide to update widgets: rather, the interactor defines some presenter interfaces representing the kinds of output it needs to perform, and then it is injected with the presenters of the actual widgets that are selected to be used.\\nHowever, from the point of view of the adapter layer (user interface), we don't necessarily need to separate controller objects from presenter objects. For example, an \"entity list\" widget might have a \"load\" controller action, that can be called directly to load entity data from the application, or as response to an event: this action will delegate to the \"list entities\" use case interactor, which will go through the query model to fetch the entities, and then call the given presenter to display them. In this case, the presenter will be the \"entity list\" widget itself, having a \"fill\" method for example, to fill the list with data. From the point of view of the interactor, it's enough that the entity list (or an adapter of it) implements the interface required by the interactor.\\nIn the previous example the use case interactor will likely need to be injected also with an error presenter to display possible errors: this presenter can perhaps be implemented by a message box widget. Now, since the list widget will be injected with the interactor, it won't be required to know of the existence of the message box widget, nor of the fact that the list widget is used as a presenter itself, because these things are decided in the application configuration (the provider).\\nAdditionally, this is one of those situations where a method should only be called while going through the application layer: we cannot call the \"fill\" method with random data, because the list widget need to contain domain entities that have gone through the application logic. However, since this method is public (whether it is meant to be just an event handler or not), nothing prevents some other object, or widget, from directly calling this method passing in somer random data, so we are left to the discipline of the implementer to remember to never call this method directly, but only as a handler of the proper application event. This risk is however mitigated by the fact that we're anyway following the discipline of letting the controller be known only by those few objects that really need to call it directly, by virtue of dependency injection: for example, the main application widget will likely need to use the entity list widget to call \"load\" or \"show\" on it at startup, but this is likely the only object that will have a reference to it.\\nRegarding widgets that display data, each widget is a specific view, or representation, of the data: this means that we can have different widgets (even at the same time) representing the same data, and thus using the same presenter. This means that presenters and controllers are not really part of the widgets themselves, but are actually distinct objects, that are used by one or more widgets. For example, we may have multiple different widgets dealing with weather data: one would display a graph of temperatures over time, another will display a geographical map with different colors for different temperatures, etc. All these widgets will need to use the same presenter, which converts the entities data coming from the \"show temperatures\" use case, to simple values: then, each different view will use the same values differently. So, when the graph widget receives the signal that temperatures have been updated, it uses the shared \"show temperatures\" controller to trigger the use case, which will have the presenter injected: now this presenter must be able to communicate with multiple different views at the same time, and the actual views may change during the execution, because we may hide some widget for example. So, the view interface used by the presenter won't be an actual object interface, but just a messaging protocol, and the interactor will be injected with a message bus, and calling the interface would mean sending a \"present\" message, to which any number of actual views can respond.\\nA similar situation happens when the same controller is called by multiple event handlers, or methods, of the same widget, like \"load\" and \"update\" on the entities list: both will have to go through the \"list entities\" controller action, delegating to the same use case interactor.\\nThe application has two ways to communicate with the widgets. The most direct one is just calling the presenters that have been injected. However, the application can also send messages, to which widgets may respond. However, since calling a presenter may also be done by sending messages (in case we need to support concurrent presenters), there's not much difference between the two from a technical perspective. The real difference lays in the fact that presentation methods are meant to communicate to the adapter that the main output has been produced, while any other message is meant to communicate side events that happened, and to which someone may want to respond. If we're injecting a presenter object, it's important that the main output is communicated through that object, and not through sending messages, to keep the intent of the communication clear. If we're using a message bus to communicate with presenters too, it's important that messages are properly named, to make it clear which are sending back the main output.\\n### Examples of presentation situations\\nA common situation is the one where we create a new entity, from a view that doesn't need to be updated to display the updated entity data:\\nUse case, \"create order\":\\n- given I'm in the checkout view\\n- and the cart has valid information\\n- when I request to create a new order\\n- then a new order is created with the cart information\\n- and I am notified of the successful order creation\\nUse case \"error creating order\":\\n- given I'm in the checkout view\\n- and the cart has invalid information\\n- when I request to create a new order\\n- then no new order is created\\n- and I am notified of the error with the cart information\\nLet's say we have a GUI adapter, with a checkout window (view) and an orders window (view). The checkout window will have a \"create order\" control with a method that takes the data from the \"cart\" widget, crafts a use case request from it, and uses it to call the use case interactor, which will have been already injected with a \"message box\" widget as both error presenter and result presenter. When the interactor is called, it first does validation of the request data, immediately calling the error presenter if the validation failed, or using the command model to request the creation of a new order otherwise. The command model will be executed asynchronously, to avoid blocking the user interface: this will create the order, update the storage, and send a message when the operation is completed. This message is caught by the interactor again, which uses the result presenter (asynchronously here) to notify that the order has been succcessfully created. At this point, the user can use the \"list orders\" use case to open the orders window, whose interactor will just read the data from the denormalized storage and present it.\\nLet's see now a case where we create a new entity in the same view where all entities are listed:\\nUse case, \"create todo item\":\\n- given I'm in the todo list view\\n- and the item form has valid data\\n- when I request to create a new todo item\\n- then a new todo item is created with the given information\\n- and the todo list is updated to show the new todo item as well\\nUse case, \"error creating todo item\":\\n- given I'm in the todo list view\\n- and the item form has invalid data\\n- when I request to create a new todo item\\n- then no view item is created\\n- and I am notified of the error with the item information\\nHere we have a single window, which is the todo list view, with the following widgets: an \"item\" form, a \"create new item\" button, a todo list, an error box, and a loader widget. When the user loads the window, the \"list items\" use case is performed, meaning that an input is sent to the \"load\" action of the todo list, which will create a request for the \"list items\" interactor, which will be injected with the list presenter, and will use the query model to get all the currently stored items from the denormalized database, which will be used to create a response, which will be sent to the todo list presenter to be displayed in the widget. Then, when the user clicks the \"create new item\" button, its controller will read the data from the form, craft a use case request with it, and send it to the \"create item\" interactor, which will be pre-injected with the error presenter, and the result presenter: if the request data is invalid, the error presenter will be sent a response containing the validation errors, and the interactor will return, otherwise the interactor will asynchronously call the command model to create a new todo item, then call the result presenter to signal that the request has been accepted (this could perhaps trigger a loading animation to start), and finally call the result presenter again to signal that the request has been completed, from inside the event handler of the command model completion event. In the meanwhile, the todo list controller was already register to respond with the \"update\" action to the message that is sent by the command model after the creation of an item is completed, so as soon as the command model signals the termination of the job, the todo list controller is called, triggering the \"list items\" use case again to update the todo items in the list with the current values, containing also the new one. The same form widget can have a controller registered to respond to the creation started message: this way if the request fails because of validation issues, the information present in the form fields is maintained to allow the user to fix it without re-typing everything, but if instead the validation passes, the form can clean its fields to allow a new item to be added next. All of these are purely GUI concerns, and that's why they aren't handled by the interactor, which must be concerned only with what is explicitly mentioned in the use case scenarios.\\nA less common case is the one where input data comes from a source that doesn't support any output, or is not interested in getting any. In actuality, this should be described by the application layer, because we can't use an adapter that doesn't support an output, with a port that provides one, because the role of the adapter is to support the use cases defined by the port. Thus, we can think of a use case that doesn't produce any outout.\\nUse case, \"signal activates actuator\":\\n- when the sensor sends a new signal\\n- then the actuator is activated\\nHere the interactor receives an input, in the form of a signal, but then produces no output, because activating the actuator is a secondary concern, adn the sensor doesn't support any output, since it can't even receive data back from its adapter.\\n### Sloth machine views\\nBeing a command-line application, the Sloth machine uses the process as the input device, and the process itself, in addition perhaps to the filesystem, as output devices. In particular, we want to use the exit status and the STDOUT and STDERR as main output devices. It's important, though, to highlight that these same devices can be used also to render output coming from the program that is being executed by the virtual machine, in addition to the virtual machine itself, and they can also be used to display side output, for example information messages in a verbose configuration. For this reason, we should constantly keep in mind where some output is coming from, and not only on which device it's displayed.\\nThe first thing to do is clearly understand what's the output that the use case is supposed to provide: in our case the output comprises two elements:\\n- the exit status of the program execution\\n- the error message that might possibly have happened\\nIt's important to underline that we want errors to be part of the output, because they're still significant for the application. Including errors in the response means that they can be handled by a presenter, and the views attached to it. Alternatively, we could have chosen to send the errors to some kind of side output, for example events, but in that case the decision of whether to display these errors or not would have been taken by the adapter, and not by the application. By sending errors with the response, we are clearly stating that we want errors to be displayed, as an application rule.\\nThus, our application will need to handle two output boundaries: `ExitStatusPresenter`, receiving an `ExitStatusResponse`, and `ErrorPresenter`, receiving an `ErrorResponse`. For both of them, concrete instances will need to be injected into the interactor. From the adapter point of view, we should decide which output models we want to support. For instance, we can think of a \"console output\", which is meant to be used when the application should be used as a standard console application, properly using the exit status, STDOUT and STDERR; additionally, we can think of a \"textual output\", which is meant to be used when all data produced by the application should be available as human-friendly textual information. These output models define two distinct sets of presenters: for the console output, we'll have a `ConsoleExitStatusPresenter` and a `ConsoleErrorPresenter`, while for the textual output we'll have a `TextualExitStatusPresenter` and a `TextualConsoleErrorPresenter`:\\n- the `ConsoleExitStatusPresenter` converts the `ExitStatus` object of the response into an integer included between `0` and `255` (using a default value if needed), because it needs to be sent to a console application exit status, abiding by POSIX standard, and produces a `ConsoleExitStatusViewModel`;\\n- the `TextualExitStatusPresenter` converts the `ExitStatus` object of the response into a string, avoiding changing its semantic at all, and produces a `TextualExitStatusViewModel`;\\n- both the `ConsoleErrorPresenter` and the `TextualErrorPresenter` will just convert the `Error` object of the response into a string, producing a `ConsoleErrorViewModel` or a `TextualErrorViewModel`;\\nEach output model can then be rendered into different widgets (views): these will be organized into output configurations, that can be selected by the user, for example through command-line arguments.\\nThe *integrated configuration* is used to run programs on the virtual machine, as if they were normal executables; this means that the program's exit status is returned as the application's own exit status, and the error message is printed to STDERR:\\n- the `ExitStatusWidget` is used to return a number as the exit status of the console application;\\n- the `StderrWidget` is used to print a message to the STDERR of the console;\\n- the `IntegratedExitStatusView` is used to render the `ConsoleExitStatusViewModel` to the `ExitStatusWidget`;\\n- the `IntegratedErrorView` is used to render the `ConsoleErrorViewModel` to the `StderrWidget`;\\nThe *clean configuration* is used to run programs on the virtual machine, but hiding the error message, so to keep the console clean; the exit status is still returned as the application's own exit status, but now the error message is written to a file; a possible use case of this is when the actual program is writing stuff to STDERR, and we don't want it to mix with errors written by the application (instead of the program):\\n- the `ExitStatusWidget` is used to return a number as the exit status of the console application;\\n- the `OutputFileWidget` is used to write messages to an output file, instead of to STDOUT or STDERR;\\n- the `CleanExitStatusView` is used to render the `ConsoleExitStatusViewModel` to the `ExitStatusWidget` (here we could share a single `ConsoleExitStatusView` with the previous case instead perhaps);\\n- the `CleanErrorView` is used to render the `ConsoleErrorViewModel` to the `OutputFileWidget`;\\nThe *verbose configuration* is used to gather all information on a centralized place, immediately visible; both the exit status and the error message are printed to STDOUT:\\n- the `StdoutWidget` is used to print messages to the STDOUT of the console;\\n- the `VerboseExitStatusView` is used to render the `TextualExitStatusViewModel` to the `StdoutWidget`;\\n- the `VerboseErrorView` is used to render the `TextualErrorViewModel` to the `StdoutWidget`;\\nThe *archived configuration* is used to gather all information on a centralized place, but without clogging the console (notice that STDOUT and STDERR may still contain what the actual program is writing to them); both exit status and error message are printed to a file:\\n- the `OutputFileWidget` is used to write messages to an output file, instead of to STDOUT or STDERR;\\n- the `ArchivedExitStatusView` is used to render the `TextualExitStatusViewModel` to the `OutputFileWidget`;\\n- the `ArchivedErrorView` is used to render the `TextualErrorViewModel` to the `OutputFileWidget`;\\nHere's what some pseudo-code would look like:\\n```\\nConsoleExitStatusPresenter\\nConsoleExitStatusPresenter(ConsoleExitStatusView view)\\npresent(ExitStatusResponse response)\\nexitStatus = normalizeExitStatus(response.getExitStatus())\\nviewModel = new ConsoleExitStatusViewModel(exitStatus)\\nview.render(viewModel)\\nConsoleErrorPresenter\\nConsoleErrorPresenter(ConsoleErrorView view)\\npresent(ErrorResponse response)\\nerror = response.getError().getMessage()\\nviewModel = new ConsoleErrorViewModel(error)\\nview.render(viewModel)\\nIntegratedExitStatusView: ConsoleExitStatusView\\nIntegratedExitStatusView(ExitStatusWidget widget)\\nrender(ConsoleExitStatusViewModel viewModel)\\nwidget.setExitStatus(viewModel.getExitStatus())\\nIntegratedErrorView: ConsoleErrorView\\nIntegratedErrorView(StderrWidget widget)\\nrender(ConsoleErrorViewModel viewModel)\\nwidget.setError(viewModel.getError())\\nConsoleUi\\nConsoleUi(Container container)\\nthis.console = container.make(Console)\\nthis.exitStatus = new ExitStatusWidget()\\nthis.stderr = new StderrWidget()\\ncontainer.bind(ExitStatusWidget, exitStatus)\\ncontainer.bind(StderrWidget, stderr)\\ngetExitStatus(): ExitStatusWidget\\nreturn exitStatus\\ngetStderr(): StderrWidget\\nreturn stderr\\nrender()\\nconsole.writeError(stderr.getError())\\nconsole.exit(exitStatus.getExitStatus())\\nConsoleUi ui = new ConsoleUi(container);\\n\/\/ send input, render views...\\nui.render()\\n```\\nHere we encapsulate all UI related code into a single class, which binds specific instances of widgets to their classes, so when views are built by the container, they get the right widgets. This specific case is particularly interesting because we have to deal with the problem that when we call `console.exit()`, the application is terminated, so any other UI code that might need to run after that won't be able to. To avoid this, instead of directly call console methods inside views, we let the views populate the widgets, and then render the UI as the last thing. Had we allowed views to directly call the console, the order with which the various presenters were called inside the interactor would have been important for user interface concerns, because if the interactor called the exit status presenter before the error one (or a generic message one, since if there are errors, no exit status is produced in this specific case), then the application would have been terminated before the interactor even finished its execution, and this is of course unacceptable. Having had a graphical interactive UI, instead, we could've displayed the exit status in a widget as soon as it was produced, because it wouldn't have caused the application to terminate.\\n","tokens":132,"id":3978,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWhen preparing data for the rendering components we currently have up to three possible locations to do so: (1) the frontend Scala backend, (2) the dotcom rendering backend and (3) the end user's client side.\\nIn the interest of the user, we should avoid postponing computation to the client side and precompute data and state on either of the two backends whenever possible.\\n\\n## Decision\\n- Favour computation in frontend over computation in dotcom-rendering\\n- Favour computation on dotcom-rendering server than computation on the client\\n\\n\\n## Context\\n[pwa-starter-kit](https:\/\/github.com\/Polymer\/pwa-starter-kit) is an example of a Redux application working with Polymer 3. While it serves its purpose, the separation of concerns is not entirely clear between the state and the views, which means that in the example views have too much knowledge of what provides and modfies the state.\\nThis is potentially dangerous for scalability, and bloats the view code with decisions that shouldn't be of its concerns.\\nReact provides an architectural pattern to handle this abstraction, in the shape of `Containers`. Containers are wrapper around the view component, which connect (via the `react-redux` helper function of the same name) the view and the state (to read it, and to action it).\\nThis pattern is interesting because from an architectural standpoint every responsibility is properly distributed. The view receives props, may they be data or callback functions, and utilizes them, the container selects where to retrieve the data or callback functions, and the state solely stores the data and provide mechanisms to modify it.\\n\\n## Decision\\nWe decided to keep this approach for a Polymer project too. Because the out-of-the-box tools do not provide that abstraction, we implemented our own architectural approach to fit this need.\\nA visual representation of the intent is as follows:\\n![Chain of connection: State > Connector > Container > Component](https:\/\/user-images.githubusercontent.com\/12797962\/41294972-a254d432-6e59-11e8-8e08-214c43772173.png)\\n- The state handles the live data of the application.\\n- The connector has an instantiation of the store, but is also a metaview, which enables communication with the methods of the store, without exposing it elsewhere.\\n- The container is an instance of the connector, which allows defining which part of the state and which actions we would like to bind to its wrapped component.\\n- And the wrapped component is just a regular component which expects some props, but does not know nor care where they come from.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nCurrently we use a single View from the Presenter, to render the normal output of the use case, that is bound to the View interface in the general provider. However, we're not handling different views that might be needed, nor error messages, which are directly written to the STDERR of the application.\\\\nRegarding the former, we're not allowing the user to choose the output representation she prefers to get. Regarding the latter, if we're decoupling the presenter from the controller to support segregation between input and output devices, we cannot then just assume that the input device will also be able to support output, like we're doing now.\\\\n\"}, {'role': 'model', 'content': '### Segregation of input and output devices\\\\nWe assume that a generic application can be connected to several different input and output devices: this means that input may come from different input devices (for example keyboard and mouse), and the output may go to different output devices (for example `STDOUT`, a GUI, and the filesystem). Of course this requires us to drop the assumption that all usual Web applications make, that the input device is the same as the output device (a Web browser, or maybe a Web\/application server to be more precise).\\\\nOne of the objectives of modern architectures is exactly allowing multiple different output devices to be used by the same application, and also from the same input device. To allow this, it\\'s necessary that the controller is oblivious of the specific output device that will be used: for example, the controller shouldn\\'t know if the output is going to a GUI, a CLI, a log, an audio speaker or a combination of these.\\\\nIt\\'s also important to mention that the strict separation between input and output is an application-level concern, which expects input to come from an interface (input boundary), and to send output to a different interface (output boundary). This is important because the application doesn\\'t need to care if the device providing the input is the same as the one receiving the output or not. However, in most cases the input device will also support some kind of output, at least to display errors of input validation, and it would be very weird if the input came from a device, and the output were sent to a different one, because the user using the first device would expect to see the output there as well. However, this kind of architecture easily supports also cases where the input device has no output capability, and the output is to be sent somewhere else (like a game controller and a monitor).\\\\n### Controllers and presenters\\\\nWhile a use case interactor performs its application logic, it may not need to perform a single presentation, for two reasons:\\\\n- there are many different kinds of presentations, that will likely need to be handled differently, like main successful output, and error output\\\\n- there are some presentations that will be produced immediately, like input validation results, and some that may be produced asynchronously, like command execution results\\\\nFor this reason, the interactor will in general have to define several different presenter interfaces, instead of just one, and juggle the various presenters according to the application logic that is unfolding.\\\\nNow, we\\'re describing this on the grounds that the presenters will be injected into the interactor. Now, in theory it\\'s possible to achieve the same by returning a single response from the interactor, instead of letting the interactor call the presenters. This would actually work, and keep the dependency inversion principle respected, since the interactor still knows nothing of the details of presentation, however:\\\\n- we need to stuff all possible response cases into a single response object, instead of cleanly separate the various cases\\\\n- the controller will need to parse the response, understand what case it\\'s about, and call different presenters according to it, adding much more responsibility to the controller than just having to translate the input data from the adapter format to the application format; additionally, the controller would really be just unpacking the same information that was packaged by the application, i.e. that a certain presenter needs to be called in a certain output situation, and the controller cannot deviate from this logic either, otherwise it would be taking presentation responsibility\\\\n- the single response object could be returning asynchronous output, in addition to synchronous one, so the controller will also need to check which part of the response should be treated as asynchronous (like attaching a callback to a promise), and which as synchronous\\\\n- even if the interactor was a coroutine (avoiding the problem of creating a single response for all cases), the controller would still need to check the kind of output at each return, and do the job of the interactor of associating a presenter to that kind of output, while the interactor already knows this information in the first place, in addition to still having to handle asynchronous output\\\\nFor these reason, although returning the response from the interactor is technically feasible (and in certain scenarios it\\'s certainly the best solution), it cannot be chosen as the general approach to use: rather it should be regarded as a special case, that works best only in specific situations.\\\\n### Presenters and views\\\\nEach specific presenter that can be used to implement the output port represents a specific output model, which is characterized by the set of view fields and data type conversions it supports, or in other words, the kind of view model it will produce. For example, a \"screen\" view model might convert the application response to a certain set of fields of certain type, with the intent of having them being displayed to a screen, while a \"print\" view model might produce from the same application response a different set of fields, of different types, specialized for being sent to a printer.\\\\nStill, the same view model, representing a certain output model, can be rendered on the selected output device in different ways: a \"screen\" output model, containing fields belonging to a screen representation, might still be rendered as an HTML page, or as TXT file, or again as a graph. All these alternatives are represented by different views. This means that a specific presenter component, for example the \"screen\" presenter component, will define a \"screen\" view interface, that will then be implemented by multiple actual \"screen\" view instances, like HTML screen view, graph screen view, etc., one of which will then be selected and injected in the presenter, to be used for rendering.\\\\n### Input definition\\\\nWe borrow a definition of \"input\" from the REST standard. According to this definition, an input will be made of:\\\\n- a resource identifier (for example `\/path\/to\/my\/resource`, URI in the Web context)\\\\n- some input parameters (for example `?some[]=value&some[]=other`)\\\\n- a resource representation (for example `.html`, `.pdf`, etc.)\\\\nthis means that:\\\\n- the identifier is used to pick a couple of controller and action, that needs to be called to perform the requested use case\\\\n- the parameters are just passed to the action when it\\'s called\\\\n- the representation is related to the specific view that will be used to render the output generated by the use case\\'s presenter; additionally, once a view is selected, it\\'s presenter is also chosen, because a presenter might have multiple views, but a view only belongs to one presenter\\\\nFor example, let\\'s assume we have an `entitiesList` control widget, which displays a specific representation of a list of entities. When we trigger the `update` event on the `entitiesList` control widget, a very specific input can be created: the identifier would reference the controller and the action, like `entitiesController.update`; the input parameters could perhaps be the name of a subset of entities that this specific widget is configured to represent; finally, the representation is related to the specific widget that is sending this input, meaning that the same resource needs different widgets to be represented with different views.\\\\n### Using a router\\\\nThe interesting problem that arises is that the view to be chosen is known at input time, because it\\'s related to the representation: this may lead to think that it should be known by the controller. However, the controller should be independent from any choice regarding the output, including which representation is used.\\\\nA solution can be to introduce a *router* component, that takes an input, and chooses a combination of *controller*, *action* and *view* according to the given input. This is much like what happens with usual MVC Web framework, but generalized for any kind of adapter. This choice made by the router can be statically configured, since it does not depend on the user input.\\\\nThe input part is quite easy, in that the router needs just to create the right controller object, and call the right action method on it, passing the right parameters. The output part is trickier though: since the controller is decoupled from the presenter, it doesn\\'t return any output object that can be then handled by the router, rather it forwards the execution to the presenter object, in a way that isn\\'t in control of the router. What the router can do, though, is to select the correct view to be used, and use the container to bind it to the generic view interface. This way, when the presenter will be built, the right view object will automatically be injected into it.\\\\n### Main output and side output\\\\nA typical application can have many different kinds of output. One of these is used to talk back to the user that provided the input: we can call it *main output*, and it would be produced by the selected use case. The *side output* would be any other kind of output produced by the application, like logging, monitoring, etc., which is not directly related to the input, and usually is not expected to be received by the user who produced the input.\\\\nWhile the main output is handled with a presenter, side output can be handled by using a message bus to send it to a non-specified recipient from the application layer: inside the adapter, then, one or more configured handlers will receive the messages, and output them to the configured devices.\\\\nAs a case study, we can consider the verbose output of a console application. At first, this may seem a case where some side output (info logs) need to be redirected to the main output (console STDOUT). What\\'s really happening, though, is that the side output is being produced in a way that is completely unrelated to the presenter and views used by the main output: messages are sent containing the side output, and then in the adapter a new handler is configured to listen to these messages, and print them to the same output device used by the main output. This way we can print to the same output device, but keep the two kinds of output completely independent.\\\\n### Handling errors\\\\nWe can identify three broad categories of errors: application errors, input errors and system errors. Errors generated by domain or application services can be caught inside the use case, and passed back to the presenter in the response. Input errors are related to wrong input, or input that doesn\\'t pass validation, and they are again checked inside the application interactors: they shouldn\\'t be handled by controllers, because controllers have no way to select a specific output device to forward errors to, and additionally it\\'s better if controllers stick to their single responsibility of translating data using adapter-specific format, into request data using application-specific format; on the other hand, we can say that the use case interactor is taking the role that so-called \"controllers\" have in traditional Web applications, meaning validating input, using proper services to produce an output, and send the output to the proper presenter\/view. Finally, system errors are programming or configuration errors that may happen in the adapter-specific code, and that are generally caught during development, but which may still happen in production, and make the application crash: since these errors might happen outside of the application logic, they cannot be handled by the application service, like application and input errors, but still they should be caught and displayed on an output device to notify the user.\\\\nSystem errors, not being generated inside the use case, don\\'t concern any use case, and as such they don\\'t need to be handled by any presenter. The way to handle them is to let any piece of code that is catching them sending a message to a widget that is responsible for displaying error messages, without needing to go through any application logic. Errors generated by domain or application services can be caught inside the use case, and passed back to a specific error presenter, which in turn will be configured with a specific view to be used in that situation.\\\\n### User interface widgets\\\\nAny user interface, whether graphical or not, is composed of widgets, representing specific contexts for user interaction and presentation of data.\\\\nUser interfaces are meant to be frequently changed, so they need to be as flexible as possible. To support this, widgets have to be designed so that they are as independent, reusable and replaceable as possible. A key tool to achieve this is using an events mechanism: instead of knowing of the existence of a specific widget, and calling a specific method on it, we send an event, and widgets that are interested in that event will respond to it with some action. This way we can easily replace widgets, or add new ones, without disrupting existing functionality.\\\\nEvents can be generated by the domain, the application, or the widgets themselves. Thus, widgets must be allowed to know about all existing events. Of course who generates an event depends on where the event belongs to: if an event represents the completion of a domain task, then the domain will need to send it, whereas if an event represents the change of state of a graphical widget (not related to any application or domain concern), then the widget should send it.\\\\nSometimes, however, it\\'s not possible, or not convenient, to interact with widgets using events. For example it can happen that we need to be sure that a specific widget is actually called, but the event system doesn\\'t ensure that any widget will respond to the event: for example if we want to display an error message, it doesn\\'t make sense to send an event, hoping that a message box widget will catch it, rather we want to make sure that the message is actually displayed the way we need. To support these cases, widgets\\' actions must also be callable directly.\\\\nWidgets can then be just regular objects, exposing a set of methods: these can both be registered as event handlers, to be automatically called when certain events are caught, and be called directly on the object itself. This way we can take into account any possible combination of input and output methods, without creating a taxonomy of widgets that can have input and output, or only input and not output, etc., which is not really relevant on the user interface layer.\\\\n### Widgets and use cases\\\\nThe model of the communication with the application layer (i.e. with a port) distinguishes controllers from presenters, where the first are responsible for handling input, and the second are responsible for handling output. The reason why the two must be separated is that the presenters need to be injected into the use case interactor, since it may need to call different kinds of presenters in different situations, and also in an asynchronous fashion, and these are concerns of the application layer, not of the adapter.\\\\nOf course the use case interactor knows nothing of which widgets are used in the adapter, nor of the kind of user interface there is, so it cannot decide to update widgets: rather, the interactor defines some presenter interfaces representing the kinds of output it needs to perform, and then it is injected with the presenters of the actual widgets that are selected to be used.\\\\nHowever, from the point of view of the adapter layer (user interface), we don\\'t necessarily need to separate controller objects from presenter objects. For example, an \"entity list\" widget might have a \"load\" controller action, that can be called directly to load entity data from the application, or as response to an event: this action will delegate to the \"list entities\" use case interactor, which will go through the query model to fetch the entities, and then call the given presenter to display them. In this case, the presenter will be the \"entity list\" widget itself, having a \"fill\" method for example, to fill the list with data. From the point of view of the interactor, it\\'s enough that the entity list (or an adapter of it) implements the interface required by the interactor.\\\\nIn the previous example the use case interactor will likely need to be injected also with an error presenter to display possible errors: this presenter can perhaps be implemented by a message box widget. Now, since the list widget will be injected with the interactor, it won\\'t be required to know of the existence of the message box widget, nor of the fact that the list widget is used as a presenter itself, because these things are decided in the application configuration (the provider).\\\\nAdditionally, this is one of those situations where a method should only be called while going through the application layer: we cannot call the \"fill\" method with random data, because the list widget need to contain domain entities that have gone through the application logic. However, since this method is public (whether it is meant to be just an event handler or not), nothing prevents some other object, or widget, from directly calling this method passing in somer random data, so we are left to the discipline of the implementer to remember to never call this method directly, but only as a handler of the proper application event. This risk is however mitigated by the fact that we\\'re anyway following the discipline of letting the controller be known only by those few objects that really need to call it directly, by virtue of dependency injection: for example, the main application widget will likely need to use the entity list widget to call \"load\" or \"show\" on it at startup, but this is likely the only object that will have a reference to it.\\\\nRegarding widgets that display data, each widget is a specific view, or representation, of the data: this means that we can have different widgets (even at the same time) representing the same data, and thus using the same presenter. This means that presenters and controllers are not really part of the widgets themselves, but are actually distinct objects, that are used by one or more widgets. For example, we may have multiple different widgets dealing with weather data: one would display a graph of temperatures over time, another will display a geographical map with different colors for different temperatures, etc. All these widgets will need to use the same presenter, which converts the entities data coming from the \"show temperatures\" use case, to simple values: then, each different view will use the same values differently. So, when the graph widget receives the signal that temperatures have been updated, it uses the shared \"show temperatures\" controller to trigger the use case, which will have the presenter injected: now this presenter must be able to communicate with multiple different views at the same time, and the actual views may change during the execution, because we may hide some widget for example. So, the view interface used by the presenter won\\'t be an actual object interface, but just a messaging protocol, and the interactor will be injected with a message bus, and calling the interface would mean sending a \"present\" message, to which any number of actual views can respond.\\\\nA similar situation happens when the same controller is called by multiple event handlers, or methods, of the same widget, like \"load\" and \"update\" on the entities list: both will have to go through the \"list entities\" controller action, delegating to the same use case interactor.\\\\nThe application has two ways to communicate with the widgets. The most direct one is just calling the presenters that have been injected. However, the application can also send messages, to which widgets may respond. However, since calling a presenter may also be done by sending messages (in case we need to support concurrent presenters), there\\'s not much difference between the two from a technical perspective. The real difference lays in the fact that presentation methods are meant to communicate to the adapter that the main output has been produced, while any other message is meant to communicate side events that happened, and to which someone may want to respond. If we\\'re injecting a presenter object, it\\'s important that the main output is communicated through that object, and not through sending messages, to keep the intent of the communication clear. If we\\'re using a message bus to communicate with presenters too, it\\'s important that messages are properly named, to make it clear which are sending back the main output.\\\\n### Examples of presentation situations\\\\nA common situation is the one where we create a new entity, from a view that doesn\\'t need to be updated to display the updated entity data:\\\\nUse case, \"create order\":\\\\n- given I\\'m in the checkout view\\\\n- and the cart has valid information\\\\n- when I request to create a new order\\\\n- then a new order is created with the cart information\\\\n- and I am notified of the successful order creation\\\\nUse case \"error creating order\":\\\\n- given I\\'m in the checkout view\\\\n- and the cart has invalid information\\\\n- when I request to create a new order\\\\n- then no new order is created\\\\n- and I am notified of the error with the cart information\\\\nLet\\'s say we have a GUI adapter, with a checkout window (view) and an orders window (view). The checkout window will have a \"create order\" control with a method that takes the data from the \"cart\" widget, crafts a use case request from it, and uses it to call the use case interactor, which will have been already injected with a \"message box\" widget as both error presenter and result presenter. When the interactor is called, it first does validation of the request data, immediately calling the error presenter if the validation failed, or using the command model to request the creation of a new order otherwise. The command model will be executed asynchronously, to avoid blocking the user interface: this will create the order, update the storage, and send a message when the operation is completed. This message is caught by the interactor again, which uses the result presenter (asynchronously here) to notify that the order has been succcessfully created. At this point, the user can use the \"list orders\" use case to open the orders window, whose interactor will just read the data from the denormalized storage and present it.\\\\nLet\\'s see now a case where we create a new entity in the same view where all entities are listed:\\\\nUse case, \"create todo item\":\\\\n- given I\\'m in the todo list view\\\\n- and the item form has valid data\\\\n- when I request to create a new todo item\\\\n- then a new todo item is created with the given information\\\\n- and the todo list is updated to show the new todo item as well\\\\nUse case, \"error creating todo item\":\\\\n- given I\\'m in the todo list view\\\\n- and the item form has invalid data\\\\n- when I request to create a new todo item\\\\n- then no view item is created\\\\n- and I am notified of the error with the item information\\\\nHere we have a single window, which is the todo list view, with the following widgets: an \"item\" form, a \"create new item\" button, a todo list, an error box, and a loader widget. When the user loads the window, the \"list items\" use case is performed, meaning that an input is sent to the \"load\" action of the todo list, which will create a request for the \"list items\" interactor, which will be injected with the list presenter, and will use the query model to get all the currently stored items from the denormalized database, which will be used to create a response, which will be sent to the todo list presenter to be displayed in the widget. Then, when the user clicks the \"create new item\" button, its controller will read the data from the form, craft a use case request with it, and send it to the \"create item\" interactor, which will be pre-injected with the error presenter, and the result presenter: if the request data is invalid, the error presenter will be sent a response containing the validation errors, and the interactor will return, otherwise the interactor will asynchronously call the command model to create a new todo item, then call the result presenter to signal that the request has been accepted (this could perhaps trigger a loading animation to start), and finally call the result presenter again to signal that the request has been completed, from inside the event handler of the command model completion event. In the meanwhile, the todo list controller was already register to respond with the \"update\" action to the message that is sent by the command model after the creation of an item is completed, so as soon as the command model signals the termination of the job, the todo list controller is called, triggering the \"list items\" use case again to update the todo items in the list with the current values, containing also the new one. The same form widget can have a controller registered to respond to the creation started message: this way if the request fails because of validation issues, the information present in the form fields is maintained to allow the user to fix it without re-typing everything, but if instead the validation passes, the form can clean its fields to allow a new item to be added next. All of these are purely GUI concerns, and that\\'s why they aren\\'t handled by the interactor, which must be concerned only with what is explicitly mentioned in the use case scenarios.\\\\nA less common case is the one where input data comes from a source that doesn\\'t support any output, or is not interested in getting any. In actuality, this should be described by the application layer, because we can\\'t use an adapter that doesn\\'t support an output, with a port that provides one, because the role of the adapter is to support the use cases defined by the port. Thus, we can think of a use case that doesn\\'t produce any outout.\\\\nUse case, \"signal activates actuator\":\\\\n- when the sensor sends a new signal\\\\n- then the actuator is activated\\\\nHere the interactor receives an input, in the form of a signal, but then produces no output, because activating the actuator is a secondary concern, adn the sensor doesn\\'t support any output, since it can\\'t even receive data back from its adapter.\\\\n### Sloth machine views\\\\nBeing a command-line application, the Sloth machine uses the process as the input device, and the process itself, in addition perhaps to the filesystem, as output devices. In particular, we want to use the exit status and the STDOUT and STDERR as main output devices. It\\'s important, though, to highlight that these same devices can be used also to render output coming from the program that is being executed by the virtual machine, in addition to the virtual machine itself, and they can also be used to display side output, for example information messages in a verbose configuration. For this reason, we should constantly keep in mind where some output is coming from, and not only on which device it\\'s displayed.\\\\nThe first thing to do is clearly understand what\\'s the output that the use case is supposed to provide: in our case the output comprises two elements:\\\\n- the exit status of the program execution\\\\n- the error message that might possibly have happened\\\\nIt\\'s important to underline that we want errors to be part of the output, because they\\'re still significant for the application. Including errors in the response means that they can be handled by a presenter, and the views attached to it. Alternatively, we could have chosen to send the errors to some kind of side output, for example events, but in that case the decision of whether to display these errors or not would have been taken by the adapter, and not by the application. By sending errors with the response, we are clearly stating that we want errors to be displayed, as an application rule.\\\\nThus, our application will need to handle two output boundaries: `ExitStatusPresenter`, receiving an `ExitStatusResponse`, and `ErrorPresenter`, receiving an `ErrorResponse`. For both of them, concrete instances will need to be injected into the interactor. From the adapter point of view, we should decide which output models we want to support. For instance, we can think of a \"console output\", which is meant to be used when the application should be used as a standard console application, properly using the exit status, STDOUT and STDERR; additionally, we can think of a \"textual output\", which is meant to be used when all data produced by the application should be available as human-friendly textual information. These output models define two distinct sets of presenters: for the console output, we\\'ll have a `ConsoleExitStatusPresenter` and a `ConsoleErrorPresenter`, while for the textual output we\\'ll have a `TextualExitStatusPresenter` and a `TextualConsoleErrorPresenter`:\\\\n- the `ConsoleExitStatusPresenter` converts the `ExitStatus` object of the response into an integer included between `0` and `255` (using a default value if needed), because it needs to be sent to a console application exit status, abiding by POSIX standard, and produces a `ConsoleExitStatusViewModel`;\\\\n- the `TextualExitStatusPresenter` converts the `ExitStatus` object of the response into a string, avoiding changing its semantic at all, and produces a `TextualExitStatusViewModel`;\\\\n- both the `ConsoleErrorPresenter` and the `TextualErrorPresenter` will just convert the `Error` object of the response into a string, producing a `ConsoleErrorViewModel` or a `TextualErrorViewModel`;\\\\nEach output model can then be rendered into different widgets (views): these will be organized into output configurations, that can be selected by the user, for example through command-line arguments.\\\\nThe *integrated configuration* is used to run programs on the virtual machine, as if they were normal executables; this means that the program\\'s exit status is returned as the application\\'s own exit status, and the error message is printed to STDERR:\\\\n- the `ExitStatusWidget` is used to return a number as the exit status of the console application;\\\\n- the `StderrWidget` is used to print a message to the STDERR of the console;\\\\n- the `IntegratedExitStatusView` is used to render the `ConsoleExitStatusViewModel` to the `ExitStatusWidget`;\\\\n- the `IntegratedErrorView` is used to render the `ConsoleErrorViewModel` to the `StderrWidget`;\\\\nThe *clean configuration* is used to run programs on the virtual machine, but hiding the error message, so to keep the console clean; the exit status is still returned as the application\\'s own exit status, but now the error message is written to a file; a possible use case of this is when the actual program is writing stuff to STDERR, and we don\\'t want it to mix with errors written by the application (instead of the program):\\\\n- the `ExitStatusWidget` is used to return a number as the exit status of the console application;\\\\n- the `OutputFileWidget` is used to write messages to an output file, instead of to STDOUT or STDERR;\\\\n- the `CleanExitStatusView` is used to render the `ConsoleExitStatusViewModel` to the `ExitStatusWidget` (here we could share a single `ConsoleExitStatusView` with the previous case instead perhaps);\\\\n- the `CleanErrorView` is used to render the `ConsoleErrorViewModel` to the `OutputFileWidget`;\\\\nThe *verbose configuration* is used to gather all information on a centralized place, immediately visible; both the exit status and the error message are printed to STDOUT:\\\\n- the `StdoutWidget` is used to print messages to the STDOUT of the console;\\\\n- the `VerboseExitStatusView` is used to render the `TextualExitStatusViewModel` to the `StdoutWidget`;\\\\n- the `VerboseErrorView` is used to render the `TextualErrorViewModel` to the `StdoutWidget`;\\\\nThe *archived configuration* is used to gather all information on a centralized place, but without clogging the console (notice that STDOUT and STDERR may still contain what the actual program is writing to them); both exit status and error message are printed to a file:\\\\n- the `OutputFileWidget` is used to write messages to an output file, instead of to STDOUT or STDERR;\\\\n- the `ArchivedExitStatusView` is used to render the `TextualExitStatusViewModel` to the `OutputFileWidget`;\\\\n- the `ArchivedErrorView` is used to render the `TextualErrorViewModel` to the `OutputFileWidget`;\\\\nHere\\'s what some pseudo-code would look like:\\\\n```\\\\nConsoleExitStatusPresenter\\\\nConsoleExitStatusPresenter(ConsoleExitStatusView view)\\\\npresent(ExitStatusResponse response)\\\\nexitStatus = normalizeExitStatus(response.getExitStatus())\\\\nviewModel = new ConsoleExitStatusViewModel(exitStatus)\\\\nview.render(viewModel)\\\\nConsoleErrorPresenter\\\\nConsoleErrorPresenter(ConsoleErrorView view)\\\\npresent(ErrorResponse response)\\\\nerror = response.getError().getMessage()\\\\nviewModel = new ConsoleErrorViewModel(error)\\\\nview.render(viewModel)\\\\nIntegratedExitStatusView: ConsoleExitStatusView\\\\nIntegratedExitStatusView(ExitStatusWidget widget)\\\\nrender(ConsoleExitStatusViewModel viewModel)\\\\nwidget.setExitStatus(viewModel.getExitStatus())\\\\nIntegratedErrorView: ConsoleErrorView\\\\nIntegratedErrorView(StderrWidget widget)\\\\nrender(ConsoleErrorViewModel viewModel)\\\\nwidget.setError(viewModel.getError())\\\\nConsoleUi\\\\nConsoleUi(Container container)\\\\nthis.console = container.make(Console)\\\\nthis.exitStatus = new ExitStatusWidget()\\\\nthis.stderr = new StderrWidget()\\\\ncontainer.bind(ExitStatusWidget, exitStatus)\\\\ncontainer.bind(StderrWidget, stderr)\\\\ngetExitStatus(): ExitStatusWidget\\\\nreturn exitStatus\\\\ngetStderr(): StderrWidget\\\\nreturn stderr\\\\nrender()\\\\nconsole.writeError(stderr.getError())\\\\nconsole.exit(exitStatus.getExitStatus())\\\\nConsoleUi ui = new ConsoleUi(container);\\\\n\/\/ send input, render views...\\\\nui.render()\\\\n```\\\\nHere we encapsulate all UI related code into a single class, which binds specific instances of widgets to their classes, so when views are built by the container, they get the right widgets. This specific case is particularly interesting because we have to deal with the problem that when we call `console.exit()`, the application is terminated, so any other UI code that might need to run after that won\\'t be able to. To avoid this, instead of directly call console methods inside views, we let the views populate the widgets, and then render the UI as the last thing. Had we allowed views to directly call the console, the order with which the various presenters were called inside the interactor would have been important for user interface concerns, because if the interactor called the exit status presenter before the error one (or a generic message one, since if there are errors, no exit status is produced in this specific case), then the application would have been terminated before the interactor even finished its execution, and this is of course unacceptable. Having had a graphical interactive UI, instead, we could\\'ve displayed the exit status in a widget as soon as it was produced, because it wouldn\\'t have caused the application to terminate.\\\\n'}]"}
{"File Name":"monitoring-doc\/0008-use-of-verify-egress-proxies.md","Context":"## Context\\nVerify employ egress proxies to control access to external resources.\\nThese are a security measure to help prevent data from being exfiltrated from within Verify.\\nThe Prometheus server will need access to external resources, notibly an Ubuntu APT mirror during the bootstrap process.\\nThe Prometheus server should not setup it's own routes to bypass the egress proxy i.e. use a NAT gateway or Elastic IP, as this will potentially open up a route for data exfiltration.\\n","Decision":"The Prometheus server should use Verify's egress proxies and choice of APT mirror.\\n","tokens":98,"id":1370,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nIf running serverless we could choose to operate outside\\na VPC. This has advantages in terms of the latency of\\nrequests. We would not need a keep-warm function to\\nensure network interfaces were provisioned.\\nLambdas running outside the VPC could not easily access\\nRDS running inside a VPC.\\nGiven the sensitive nature of the data involved it seems\\nsensible to protect it further by running RDS inside a VPC\\nwhere we have much easier control over ingress and egress.\\n\\n## Decision\\nImplement a VPC.\\n\\n\\n## Context\\nThis system needs storage in the data store for every VPN it deals with.\\nSince it shall be used to analyse IPsec tunnels it could create the needed storage on the fly when interesting traffic reaches the system.\\nThis would make it easier to get started with a new VPN but it would make the system vulnerable to resource exhaustion.\\nAdditionally it could be triggered with spoofed source addresses to send traffic to arbitrary systems.\\n\\n## Decision\\nFor every VPN gateway that this system shall interact with, at least the storage in the data store has to be initialized.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nVerify employ egress proxies to control access to external resources.\\\\nThese are a security measure to help prevent data from being exfiltrated from within Verify.\\\\nThe Prometheus server will need access to external resources, notibly an Ubuntu APT mirror during the bootstrap process.\\\\nThe Prometheus server should not setup it's own routes to bypass the egress proxy i.e. use a NAT gateway or Elastic IP, as this will potentially open up a route for data exfiltration.\\\\n\"}, {'role': 'model', 'content': \"The Prometheus server should use Verify's egress proxies and choice of APT mirror.\\\\n\"}]"}
{"File Name":"python-tuf\/0002-pre-1-0-deprecation-strategy.md","Context":"## Context and Problem Statement\\nWe plan to refactor the reference implementation significantly and, as part of\\nthat effort, drop support for no-longer maintained versions of Python\\n(see ADR 0001).\\nHowever, a major user of (and contributor to) the project has users of the\\nclient stuck on older Python versions.\\nWe would like to define a reasonable support policy for the current, Python 2.7\\nsupporting, codebase.\\n## Decision Drivers\\n* We have finite resources.\\n* A major adopter\/user of the project has a need to maintain support for\\nPython 2.7 clients.\\n","Decision":"* We have finite resources.\\n* A major adopter\/user of the project has a need to maintain support for\\nPython 2.7 clients.\\nChosen option: \"Support the old code on a best-effort basis once the refactored\\ncode is released\", because we only have finite resources and want to focus them\\non moving the project forward, including supporting PyPI\/pip integration and\\nproviding a solid implementation for developing specification enhancements in.\\nWe should document this outcome clearly in a governance document describing\\nthe release process with words along the lines of:\\n\"Support for older releases:\\nBugs reported with tuf versions prior to 1.0.0 will likely not be addressed\\ndirectly by tuf\u2019s maintainers. Pull Requests to fix bugs in the last release\\nprior to 1.0.0 will be considered, and merged (subject to normal review\\nprocesses). Note that there may be delays due to the lack of developer resources\\nfor reviewing such pull requests.\"\\n","tokens":131,"id":4257,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nThe plan is to implement a refactored TUF (1.0.0) alongside the current\\ncode base, in order to not disrupt existing usage and keep providing\\na Python 2.7 client.\\nWe need to decide on the best place to do this development.\\n## Decision Drivers\\n* Developing the new code piecemeal\\n* Continuing to make releases in the interim\\n* Avoiding maintenance overhead\\n\\n## Decision\\n* Developing the new code piecemeal\\n* Continuing to make releases in the interim\\n* Avoiding maintenance overhead\\nChosen option: \"Develop TUF 1.0.0 in a subdirectory of the current TUF\\nimplementation\", because we want to add the new TUF code gradually\\nwhile keep maintaining the current implementation given limited\\nmaintenance resources.\\nOnce development of the new version is complete, we will transition\\nfrom TUF 1.0.0 in a subdirectory to stand-alone TUF 1.0.0 by the following\\nprocedure:\\n* flesh out tuf\/api\/*\\n* implement tuf\/client\/new-updater.py\\n* implement tuf\/repository\/*\\n* \\\\<iterate\\\\>\\n* git mv tuf\/client\/new-updater.py tuf\/client\/updater.py\\n* git rm tuf\/\\\\*.py\\n* tag 1.0.0\\n\\n\\n## Context\\nIn [ADR-0005](0005-implement-minimal-api-abstracting-grimoire.md) a decision was made to\\nimplement some bundling mechanisms to make it easier for tooling authors to retrieve\\ndocumentation data.\\nIn the pursuit to better show off cljdoc\\'s multi-version capabilities I was going to\\nimplement a similar caching\/bundling mechanism for the list of versions available.\\nFollowing problems have been identified:\\n- Creating and updating an index of versions requires knowledge of prior versions which in\\nturn requires us to talk to the Grimoire store first.\\n- The Grimoire store may not be fully available on the machine building documentation.\\n- This results in additional roundtrips and, should there ever be multiple machines bundling\\ndocumentation, distributed writes.\\n\\n## Decision\\nDue to the above issues I will not implement any caches to make lists of groups, artifacts\\nor versions easily available for now. Already made changes in this direction will be\\nreverted.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\nWe plan to refactor the reference implementation significantly and, as part of\\\\nthat effort, drop support for no-longer maintained versions of Python\\\\n(see ADR 0001).\\\\nHowever, a major user of (and contributor to) the project has users of the\\\\nclient stuck on older Python versions.\\\\nWe would like to define a reasonable support policy for the current, Python 2.7\\\\nsupporting, codebase.\\\\n## Decision Drivers\\\\n* We have finite resources.\\\\n* A major adopter\/user of the project has a need to maintain support for\\\\nPython 2.7 clients.\\\\n'}, {'role': 'model', 'content': '* We have finite resources.\\\\n* A major adopter\/user of the project has a need to maintain support for\\\\nPython 2.7 clients.\\\\nChosen option: \"Support the old code on a best-effort basis once the refactored\\\\ncode is released\", because we only have finite resources and want to focus them\\\\non moving the project forward, including supporting PyPI\/pip integration and\\\\nproviding a solid implementation for developing specification enhancements in.\\\\nWe should document this outcome clearly in a governance document describing\\\\nthe release process with words along the lines of:\\\\n\"Support for older releases:\\\\nBugs reported with tuf versions prior to 1.0.0 will likely not be addressed\\\\ndirectly by tuf\u2019s maintainers. Pull Requests to fix bugs in the last release\\\\nprior to 1.0.0 will be considered, and merged (subject to normal review\\\\nprocesses). Note that there may be delays due to the lack of developer resources\\\\nfor reviewing such pull requests.\"\\\\n'}]"}
{"File Name":"contact-frontend\/0002-remove-external-deskpro-from-load-test-dependencies.md","Context":"## Context and Problem Statement\\nThe performance tests for `contact-frontend` and `hmrc-deskpro` live in the repository\\n[contact-hmrcdeskpro-performance-tests](https:\/\/github.com\/hmrc\/contact-hmrcdeskpro-performance-tests). As part of the performance test journey, they were previously making calls that didn't reflect a real user journey:\\n1. Making a call to the endpoint `\/ticket\/$id` in `hmrc-deskpro` to get the IDs for the ticket in MongoDB and external Deskpro\\n2. Failing the test if there was no ID yet available from external Deskpro for the ticket\\nThe problems with this are:\\n1. The calls to POST ticket data to external Deskpro happens asynchronously as a scheduled job, with queueing in place to handle high load. In the real world, it is not coupled the an end user's journey, so this test is not representative of a real user journey.\\n2. The Staging instance of external DeskPro ends up tightly coupled to our performance tests, and our performance tests fail if external DeskPro is not.\\nas performant as `contact-frontend` and `hrmc-deskpro`. We should not be performance testing external services, especially as we have queueing in place to manage load.\\nIn short, by coupling in this way to external DeskPro:\\n* we test a service that we don't own\\n* we don't reflect the user journey\\n* we put unnecessary load on the system by calling `\/ticket\/$id`.\\n## Decision Drivers <!-- optional -->\\n* Upcoming SA peak 2021 requires load testing to 700,000 tickets created from1 to 31 Jnuary inclusive\\n","Decision":"* Upcoming SA peak 2021 requires load testing to 700,000 tickets created from1 to 31 Jnuary inclusive\\nChosen option: \"Remove calls to external DeskPro from the tests\", because our tests should be testing the\\nservices that we own, not external DeskPro which is owned by DDCOps. Additionally, we have queueing backed by MongoDB in place within `hmrc-deskpro` precisely so that if there is high load on `contact-frontend` and `hmrc-deskpro`, this can handle the throttling of tickets POSTed to external DeskPro.\\n### Positive Consequences <!-- optional -->\\n* The performance tests are an accurate representation of the user journey\\n* We as PlatUI are only testing the services that we own\\n### Negative Consequences <!-- optional -->\\n* Team DDC Ops no longer get performance testing of their DeskPro instances as part of the\\nPlatUI performance tests. However, a separate repo can be created to performance test\\nexternal DeskPro.\\n","tokens":355,"id":1752,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nWe want to automatically deploy the fundraising content repository\\nwhenever there is a change (i.e. a Git commit on the `test` or `production`\\nbranch) to the destination servers associated with the branch.\\n## Decision Drivers\\n* For security reasons, we want to host the system that does the\\ndeployment on our own infrastructure.\\n* The repository consists of several files. We want the deployment to be\\n*atomic*, i.e our application either uses the new set of files or the\\nold one, but never a mix.\\n* We have three production servers that need to receive the new version\\nsimultaneously (although not atomically).\\n* We would like the system to notify us (via email) when a deployment\\nfailed.  We would like to have a record (e.g. log file) of successful\\ndeployments.  We have an Ansible playbook that does the atomic\\ndeployment to the test and production servers.\\n* We have a `Dockerfile` definition that packages Ansible, the deployment\\nplaybooks and our server configuration into a standalone Docker image.\\nThe deployment software can run this Docker image to do the deployment.\\n* We prefer \"configuration as code\" over setting up workflows in a GUI.\\n* The maintenance and onboarding effort for the system should be as low\\nas possible.\\n* (Optional) It would be nice if we had a \"button\" to trigger the\\ndeployment\\n* (Optional) It would be nice if feature branches of the application\\nrepository could be associated with feature branches in the content\\nrepository (instead of using `test`).\\n\\n## Decision\\n* For security reasons, we want to host the system that does the\\ndeployment on our own infrastructure.\\n* The repository consists of several files. We want the deployment to be\\n*atomic*, i.e our application either uses the new set of files or the\\nold one, but never a mix.\\n* We have three production servers that need to receive the new version\\nsimultaneously (although not atomically).\\n* We would like the system to notify us (via email) when a deployment\\nfailed.  We would like to have a record (e.g. log file) of successful\\ndeployments.  We have an Ansible playbook that does the atomic\\ndeployment to the test and production servers.\\n* We have a `Dockerfile` definition that packages Ansible, the deployment\\nplaybooks and our server configuration into a standalone Docker image.\\nThe deployment software can run this Docker image to do the deployment.\\n* We prefer \"configuration as code\" over setting up workflows in a GUI.\\n* The maintenance and onboarding effort for the system should be as low\\nas possible.\\n* (Optional) It would be nice if we had a \"button\" to trigger the\\ndeployment\\n* (Optional) It would be nice if feature branches of the application\\nrepository could be associated with feature branches in the content\\nrepository (instead of using `test`).\\nChosen option: \"Drone CI\", because the benefits outweigh the drawbacks (see below).\\n\\n\\n## Context\\nABCI errors should provide an abstraction between application details\\nand the client interface responsible for formatting & displaying errors to the user.\\nCurrently, this abstraction consists of a single integer (the `code`), where any\\n`code > 0` is considered an error (ie. invalid transaction) and all type\\ninformation about the error is contained in the code. This integer is\\nexpected to be decoded by the client into a known error string, where any\\nmore specific data is contained in the `data`.\\nIn a [previous conversation](https:\/\/github.com\/tendermint\/abci\/issues\/165#issuecomment-353704015),\\nit was suggested that not all non-zero codes need to be errors, hence why it\\'s called `code` and not `error code`.\\nIt is unclear exactly how the semantics of the `code` field will evolve, though\\nbetter lite-client proofs (like discussed for tags\\n[here](https:\/\/github.com\/tendermint\/tendermint\/issues\/1007#issuecomment-413917763))\\nmay play a role.\\nNote that having all type information in a single integer\\nprecludes an easy coordination method between \"module implementers\" and \"client\\nimplementers\", especially for apps with many \"modules\". With an unbounded error domain (such as a string), module\\nimplementers can pick a globally unique prefix & error code set, so client\\nimplementers could easily implement support for \"module A\" regardless of which\\nparticular blockchain network it was running in and which other modules were running with it. With\\nonly error codes, globally unique codes are difficult\/impossible, as the space\\nis finite and collisions are likely without an easy way to coordinate.\\nFor instance, while trying to build an ecosystem of modules that can be composed into a single\\nABCI application, the Cosmos-SDK had to hack a higher level \"codespace\" into the\\nsingle integer so that each module could have its own space to express its\\nerrors.\\n\\n## Decision\\nInclude a `string code_space` in all ABCI messages that have a `code`.\\nThis allows applications to namespace the codes so they can experiment with\\ntheir own code schemes.\\nIt is the responsibility of applications to limit the size of the `code_space`\\nstring.\\nHow the codespace is hashed into block headers (ie. so it can be queried\\nefficiently by lite clients) is left for a separate ADR.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\nThe performance tests for `contact-frontend` and `hmrc-deskpro` live in the repository\\\\n[contact-hmrcdeskpro-performance-tests](https:\/\/github.com\/hmrc\/contact-hmrcdeskpro-performance-tests). As part of the performance test journey, they were previously making calls that didn\\'t reflect a real user journey:\\\\n1. Making a call to the endpoint `\/ticket\/$id` in `hmrc-deskpro` to get the IDs for the ticket in MongoDB and external Deskpro\\\\n2. Failing the test if there was no ID yet available from external Deskpro for the ticket\\\\nThe problems with this are:\\\\n1. The calls to POST ticket data to external Deskpro happens asynchronously as a scheduled job, with queueing in place to handle high load. In the real world, it is not coupled the an end user\\'s journey, so this test is not representative of a real user journey.\\\\n2. The Staging instance of external DeskPro ends up tightly coupled to our performance tests, and our performance tests fail if external DeskPro is not.\\\\nas performant as `contact-frontend` and `hrmc-deskpro`. We should not be performance testing external services, especially as we have queueing in place to manage load.\\\\nIn short, by coupling in this way to external DeskPro:\\\\n* we test a service that we don\\'t own\\\\n* we don\\'t reflect the user journey\\\\n* we put unnecessary load on the system by calling `\/ticket\/$id`.\\\\n## Decision Drivers <!-- optional -->\\\\n* Upcoming SA peak 2021 requires load testing to 700,000 tickets created from1 to 31 Jnuary inclusive\\\\n'}, {'role': 'model', 'content': '* Upcoming SA peak 2021 requires load testing to 700,000 tickets created from1 to 31 Jnuary inclusive\\\\nChosen option: \"Remove calls to external DeskPro from the tests\", because our tests should be testing the\\\\nservices that we own, not external DeskPro which is owned by DDCOps. Additionally, we have queueing backed by MongoDB in place within `hmrc-deskpro` precisely so that if there is high load on `contact-frontend` and `hmrc-deskpro`, this can handle the throttling of tickets POSTed to external DeskPro.\\\\n### Positive Consequences <!-- optional -->\\\\n* The performance tests are an accurate representation of the user journey\\\\n* We as PlatUI are only testing the services that we own\\\\n### Negative Consequences <!-- optional -->\\\\n* Team DDC Ops no longer get performance testing of their DeskPro instances as part of the\\\\nPlatUI performance tests. However, a separate repo can be created to performance test\\\\nexternal DeskPro.\\\\n'}]"}
{"File Name":"otm-docs\/api_description.md","Context":"## Context\\nApi will be responsible for administering vehicle timetables. It will use map service for real time vehicle tracking and will recive informations about it localizations from any source.\\n","Decision":"Team decide to make API wich allows to organize timetables, receive vehicles localizations and send informations to front-end api.\\n","tokens":38,"id":3057,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe want to be able to store a projection of the tides and especially their tasks statuses. The aim is to display\\na per-task detailed pipeline-like UI, as much real time as possible.\\n\\n## Decision\\nFirebase helped us a lot with the logs and appear to be successful, it makes sense to extend its usage\\nto this projection.\\nIn order to ensure a permission control, the tides will be stored with the following hierarchy:\\n- `\/flows\/{flowUuid}\/tides\/{tideUuid}`\\n\\n\\n## Context\\nWe currently have a number of MySQL primaries, secondaries and backup machines\\nmanaged via Puppet agents that run on the database nodes themselves. While we are currently moving the\\ndata stores to the managed RDS service there are some supporting functions,\\nsuch as adding databases and users, that still need to be conducted by a config management\\nrunning host.\\nAn additional piece of context is that once the \\'lift and shift\\'\\nis done we will convert the signon application and database to PostgreSQL.\\nThis ADR is to document where they will run in the future.\\n\\n## Decision\\nWe discussed three options:\\n1. have one admin instance per MySQL database and keep the databases separate\\n2. manage the users and databases in terraform itself\\n3. centralise the MySQL databases and have one admin machine manage them all\\nThe first option is the simplest, has the most moving parts and the highest expense while also\\nproviding the greatest level of isolation. Due to the low amount of traffic most of the\\nMySQL databases receive we decided against this due to cost and the number of\\ninstances it would require.\\nOption 2 was discounted as we do not want to manage individual aspects of databases and their users\\nusing Terraform providers at this date as we have a large, existing, investment in using puppet\\nfor these tasks. We would also need a way to pass secrets back and forth between the two code\\nbases; a complexity we\\'d like to defer.\\nThe final option, and the one we\\'ve chosen for now, is to have a single instance with a dedicated \"db admin\"\\npuppet role that will manage these tasks for all of our databases while also centralising the\\ndatabases themselves in to a single RDS instance in order to simplify our connectivity code,\\nconnection strings and secret management. This is also a much cheaper approach.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nApi will be responsible for administering vehicle timetables. It will use map service for real time vehicle tracking and will recive informations about it localizations from any source.\\\\n'}, {'role': 'model', 'content': 'Team decide to make API wich allows to organize timetables, receive vehicles localizations and send informations to front-end api.\\\\n'}]"}
{"File Name":"jabref\/0023-localized-preferences.md","Context":"## Context and Problem Statement\\nCurrently, JabRef uses some localized preferences. We want to remove the Localization-dependency from the JabRefPreferences and move the Localization to where the String is used.\\nThe problem is how to store the default values.\\n","Decision":"Chosen option: \"_Store the unlocalized String._ Consumers then check the String they got as a preference against the defaults. If it matches, localize it. Otherwise, use it.\", because Achieves goals without requiring too much refactoring and without (known) downsides.\\n","tokens":50,"id":4744,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe want to build a reusable React library, that generates multipage forms\\nquickly. We need logic that extends beyond simple \"show this element on this\\npage\", such as conditional logic for displaying sections dependant on values in\\nother sections, and to skip pages based on values elsewhere in the journey.\\n\\n## Decision\\nWe will build remultiform to export a series of small orchestration components.\\nWe will pass the entire multipage form configuration in via props to those\\norchestrators. The configuration should be agnostic of the set of components it\\nmight receive.\\n\\n\\n## Context\\nThis ADR serves to document how Custom Domain Properties are rendered in JSON-LD.\\nCustom Domain Properties represent dynamic parts of the model. The original domain is extended with annotations which\\nmay or may not include their own semantic information (when used with semantic extensions).\\nGiven that there is a problem, the AMF Web API model is mostly static. We have static `*Model.scala` objects with each\\nmodel.\\n\\n## Decision\\nWe will introduce a hack when rendering Custom Domain properties in JSON-LD that will make the graph look like it was\\ngenerated by a dynamic model while still representing the model statically.\\nConcretely, the `DomainElementModel` has a `CustomDomainProperties` field which is an `Array(DomainExtensionModel)`.\\nEach `DomainExtensionModel` has a `DefinedBy: CustomDomainPropertyModel` and `Extension: DataNode` fields that\\nrepresent the definition and value of the domain extension.\\nThis in the \"traditional & static\" parts of the model would be rendered like this (summary):\\n```json\\n{\\n\"@id\": \"myDomainElement\",\\n\"customDomainProperties\": [\\n{\\n\"@id\": \"myDomainExtension\",\\n\"definedBy\": {\\n\"@id\": \"myCustomDomainProperty\"\\n},\\n\"extension\": {\\n\"@id\": \"myDataNode\"\\n}\\n}\\n]\\n}\\n```\\nInstead a \"dynamic\" model is simulated\\n```json\\n{\\n\"@id\": \"myDomainElement\",\\n\"myCustomDomainProperty\": {\\n\"@id\": \"myDataNode\"\\n},\\n\"customDomainProperties\": [\\n{\\n\"@id\": \"myCustomDomainProperty\"\\n}\\n]\\n}\\n```\\nAs you can see a new property was generated using the `@id` of the `CustomDomainProperty` from the original `definedBy`\\nfield which does not exist in the static model. The `customDomainProperties` property now points to an array of custom\\ndomain property nodes rather than an array of domain extension nodes. The new `customDomainProperties` value indicates\\nwhich properties of the serialized node \"are dynamic\".\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\nCurrently, JabRef uses some localized preferences. We want to remove the Localization-dependency from the JabRefPreferences and move the Localization to where the String is used.\\\\nThe problem is how to store the default values.\\\\n'}, {'role': 'model', 'content': 'Chosen option: \"_Store the unlocalized String._ Consumers then check the String they got as a preference against the defaults. If it matches, localize it. Otherwise, use it.\", because Achieves goals without requiring too much refactoring and without (known) downsides.\\\\n'}]"}
{"File Name":"james\/0007-distributed-task-cancellation.md","Context":"## Context\\nA `Task` could be run on any node of the cluster. To interrupt it we need to notify all nodes of the cancel request.\\n","Decision":"* We will add an EventHandler to broadcast the `CancelRequested` event to all the workers listening on a RabbitMQ broadcasting exchange.\\n* The `TaskManager` should register to the exchange and will apply `cancel` on the `TaskManagerWorker` if the `Task` is waiting or in progress on it.\\n","tokens":32,"id":2158,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nA `Task` could be run on any node of the cluster. To interrupt it we need to notify all nodes of the cancel request.\\n\\n## Decision\\n* We will add an EventHandler to broadcast the `CancelRequested` event to all the workers listening on a RabbitMQ broadcasting exchange.\\n* The `TaskManager` should register to the exchange and will apply `cancel` on the `TaskManagerWorker` if the `Task` is waiting or in progress on it.\\n\\n\\n## Context\\nBy switching the task manager to a distributed implementation, we need to be able to `await` a `Task` running on any node of the cluster.\\n\\n## Decision\\n* Broadcast `Event`s in `RabbitMQ`\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nA `Task` could be run on any node of the cluster. To interrupt it we need to notify all nodes of the cancel request.\\\\n'}, {'role': 'model', 'content': '* We will add an EventHandler to broadcast the `CancelRequested` event to all the workers listening on a RabbitMQ broadcasting exchange.\\\\n* The `TaskManager` should register to the exchange and will apply `cancel` on the `TaskManagerWorker` if the `Task` is waiting or in progress on it.\\\\n'}]"}
{"File Name":"opg-digideps\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":4412,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need to record the architectural decisions made on this project.\\\\n'}, {'role': 'model', 'content': 'We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\\\n'}]"}
{"File Name":"sepa-customer-platform\/0002-use-azure-blob-storage-store-large-supporting-data-files.md","Context":"## Context\\nA cloud storage solution is required to allow operators and SEPA staff to upload the large supporting data files required to process an application.\\nThe storage solution would need to integrate with the case management tool (Microsoft Dynamics 365)\\n","Decision":"MS Dynamics offers close integration with Azure Cloud services, therefore Azure Blob Storage has been selected.\\n","tokens":48,"id":5166,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nWe want a way to define and manage the deployment of resources which are shared across multiple solutions.\\nA number of services offered by cloud providers, namely AWS have services which can only be deployed once per AWS Account or enable a service across an entire account\\nHow should we manage these resources to scale with our plugin support and to support the features we have introduced for our other deployments\\n## Decision Drivers\\n* Support required for multi region deployments to allow global image registry\\n* Deprecation of the fragment and case style template processing which is currently used for account level deployments\\n\\n## Decision\\n* Support required for multi region deployments to allow global image registry\\n* Deprecation of the fragment and case style template processing which is currently used for account level deployments\\nChosen option: \"Extend occurrence support to work at any layer\", because this allows for greater flexibility over the resources we can deploy and aligns with our other deployment processes\\n### Positive Consequences\\n* Occurrences will be the approach we use for all deployments\\n* Opens support for any occurrence features that we have in place already or develop in the future\\n* Makes our existing deployment model easier to follow ( our current tiers deployments will belong to the Segment and show that relationship )\\n### Negative Consequences\\n* Complexity in extending the occurrence building process\\n* May introduce some complexity in managing default and required singleton instances of deployments. We can manage this and could potentially be a useful feature\\n\\n\\n## Context\\nThe data warehouse was intended to house data for all CommCare HQ reports.\\nThe warehouse would replace Elasticsearch in almost all contexts that it is currently used.\\nThe migration began in 2017 with the Application Status report and the effort\\nto move the report to the warehouse and ensure it is stable, performs well and\\nprovides the same features as the ES-backed reports was much higher than\\nanticipated.\\n\\n## Decision\\nTo reduce our infrastructure dependencies and focus our efforts on existing databases,\\nwe have decided to remove the warehouse and stop any efforts to iterate on it.\\nThis decision is not because we believe that the warehouse is a worse implementation than Elasticsearch.\\nThis decision is because we believe that with our current priorities, we will\\nnot be able to spend the appropriate amount of time to make the warehouse a\\nrobust solution for generic reports in the near future.\\nBecause no current reports are backed by the warehouse, it is an important time\\nto reconsider our approach and decide on what will be appropriate long term.\\nWhen there are more dedicated resources for generic reports, we believe that\\na warehouse-style approach should be considered when implementing.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nA cloud storage solution is required to allow operators and SEPA staff to upload the large supporting data files required to process an application.\\\\nThe storage solution would need to integrate with the case management tool (Microsoft Dynamics 365)\\\\n'}, {'role': 'model', 'content': 'MS Dynamics offers close integration with Azure Cloud services, therefore Azure Blob Storage has been selected.\\\\n'}]"}
{"File Name":"up-fiscal-data\/004-receipts.md","Context":"## Context and Problem Statement\\nDecision was required on what sub-section to be analysed under Receipts. The requirements for Receipts section in the current scope was discussed.\\n","Decision":"- Not in the project scope for either of the engagements\\n- Good to have and can be incorporated at a later date\\nReceipts section parked for analysis scope, can be picked up later depending to requirements.\\n","tokens":34,"id":441,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nDefine a default data structure for the schedule information within interal structures of delayer-aws. The outside schedule model was already defined.\\n## Decision Drivers\\n*   Structure is an internal representation, and format should be concerned with it\\'s interal functionalityes\\n*   Could be used in future event dispatchers\\n\\n## Decision\\n*   Structure is an internal representation, and format should be concerned with it\\'s interal functionalityes\\n*   Could be used in future event dispatchers\\nThe json structure of the schedule will be:\\n```javascript\\n{\\nscheduleId: \"484a127d-6ce3-11e8-8fdd-fb8b5aab61ea\",\\npointInTime: 1528662600,\\napiKey: \"nVMIr6J5Do1qutXsZt1dhaaJfjyq4YgzThPQWkQ1\",\\ntopicArn: \"arn:aws:sns:us-east-1:472249637553:http-request_action-topic\",\\nactionConfig: \\'{ \"method\" : \"GET\", \"url\" : \"https:\/\/google.com\/\" }\\',\\ncontext: \\'{ \"headers\" : [], \"payload\": \"\" }\\'\\n}\\n```\\nWhere:\\n*   `scheduleId`: unique identifier of the schedule - generated\\n*   `pointInTime`: unix timestamp created in UTC basis to the exact time of the execution of the schedule - informed by the client in request body\\n*   `apiKey`: if informed, the client\\'s api key - informed by the client in headers\\n*   `topicArn`: the Amazon Resource identifier of the action topic on where the schedule will be published. If only the topic name was provided, the current account and the region of the *api-schedule-post* currently in execution will be used to build the arn.\\n*   `actionConfig`: config object provided by the user based on the type of the action\\n*   `context`: the data that will be used as context in the actions\\n\\n\\n## Context and Problem Statement\\nWe want to record architectural decisions made in this project.\\nWhich format and structure should these records follow?\\n\\n## Decision\\n* Google docs - hard to find and keep structured\\n* Confluence - hard to edit and search\\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions), because\\n* Implicit assumptions should be made explicit.\\nDesign documentation is important to enable people understanding the decisions later on.\\nSee also [A rational design process: How and why to fake it](https:\/\/doi.org\/10.1109\/TSE.1986.6312940).\\n* The MADR format is lean and fits our development style.\\n* The MADR structure is comprehensible and facilitates usage & maintenance.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\nDecision was required on what sub-section to be analysed under Receipts. The requirements for Receipts section in the current scope was discussed.\\\\n'}, {'role': 'model', 'content': '- Not in the project scope for either of the engagements\\\\n- Good to have and can be incorporated at a later date\\\\nReceipts section parked for analysis scope, can be picked up later depending to requirements.\\\\n'}]"}
{"File Name":"agentframework\/0007-cancel-adr-0002-and-use-codegen-to-generate-interceptor.md","Context":"## Context\\nUsing Proxy will not as fast as native code. so we use CodeGen to generate a transparent layer on top of user's class to provide interceptor function.\\n","Decision":"Use CodeGen\\n","tokens":35,"id":2364,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe want to unit test the ViewModel classes but the ViewModel class itself creates a specific UseCase and is using hardwired Rx Schedulers.\\nThis makes it impossible (or unnecessary hard) to mock the dependencies used by the ViewModel.\\n\\n## Decision\\nThe creation of a UseCase instance is moved to a factory class. The factory itself is passed to every ViewModel class (using an interface for inversion of control).\\nFor unit testing we simply can mock the factory.\\nAs the factory only holds the creation of a specific UseCase, it should be enough to only create a single factory for all UseCases.\\nIn the future it could be necessary to create additional factories if a single one is polluted to much.\\nThe Schedulers are also passed as parameter to be able to mock them too.\\n\\n\\n## Context\\nWe don't store any user data on back-end yet, so we can do it in AsyncStorage.\\n\\n## Decision\\nWe will use [redux-persist](https:\/\/github.com\/rt2zz\/redux-persist)\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nUsing Proxy will not as fast as native code. so we use CodeGen to generate a transparent layer on top of user's class to provide interceptor function.\\\\n\"}, {'role': 'model', 'content': 'Use CodeGen\\\\n'}]"}
{"File Name":"winery\/0020-TOSCA-definitions-contain-extactly-one-element.md","Context":"## Context and Problem Statement\\nHow should TOSCA data be stored?\\n","Decision":"Chosen option: \"Allow exactly one TOSCA Definitions child in a definition\", because\\n- Definitions are not modeled as explicit element. Only the nested elements are handled by Winery.\\n- That means, it is not possible to specify custom definitions bundling a customized subset of available elements.\\n","tokens":16,"id":4310,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nWhich build tool should be used?\\n\\n## Decision\\nChosen option: \"Gradle\", because it is lean and fits our development style.\\n\\n\\n## Context and Problem Statement\\nShould we consider and address performance requirements?\\n\\n## Decision\\nFor the initial start of the project we do not consider performance as a must have.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\nHow should TOSCA data be stored?\\\\n'}, {'role': 'model', 'content': 'Chosen option: \"Allow exactly one TOSCA Definitions child in a definition\", because\\\\n- Definitions are not modeled as explicit element. Only the nested elements are handled by Winery.\\\\n- That means, it is not possible to specify custom definitions bundling a customized subset of available elements.\\\\n'}]"}
{"File Name":"wordpress-template\/0002-use-acf-pro.md","Context":"## Context\\nThe themes and plugins we make often need to add fields to the admin interface.\\nWe've been using ACF PRO on nearly every new site we've made in the past few years, and we've used the free version of ACF for perhaps 5-7 years. We're comfortable with it and it's proven its stability and usefulness.\\n","Decision":"Install ACF PRO on every site we create. If we don't need it, we can remove it before deployment.\\nTo add a field to a theme or plugin we should copy the PHP code and paste it into the codebase.\\n","tokens":74,"id":2263,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe have seen a number of OutOfMemoryErrors with Solr, mostly related to garbage collection.  Our memory\\nsettings include many overrides of the defaults, and it's not clear what they do individually or as a group.\\nThese settings have also been in use for several years, and we're not sure if they are still a good match for\\nour hardware and usage.\\n\\n## Decision\\nRemove the following memory tuning options from the Java options:\\n* `-Xss=256k`\\n* `-XX:NewRatio=3`\\n* `-XX:SurvivorRatio=4`\\n* `-XX:TargetSurvivorRatio=90`\\n* `-XX:MaxTenuringThreshold=8`\\n* `-XX:ConcGCThreads=4`\\n* `-XX:ParallelGCThreads=4`\\n* `-XX:PretenureSizeThreshold=64m`\\n* `-XX:CMSInitiatingOccupancyFraction=50`\\n* `-XX:CMSMaxAbortablePrecleanTime=6000`\\nAdd the following option to disable throwing an OutOfMemoryError if garbage collection takes too long:\\n* `-XX:-UseGCOverheadLimit`\\nDecrease total memory allocation from 72 GB to 40 GB.\\n\\n\\n## Context\\nIn general, we have decided to rewrite the admin UI as we port link-sf into the new Link Platform project. This is because we intend to make a number of improvements on its reliability and UX consistency.\\nWe considered using the [Admin on rest](https:\/\/github.com\/marmelab\/admin-on-rest) library which would give us a lot of data handling for free, including a REST client and validation. However, when we use [Zendesk Garden](https:\/\/garden.zendesk.com\/) we will also get validation styles, and then will be able to a) control how we handle data ourselves, b) support another one of Zendesk's open source projects, and c) get the nicer styles provided by Garden which are also reflective of the Zendesk brand.\\n\\n## Decision\\nAdmin pages will be rewritten using React, Zendesk Garden, and a REST client to be determined. We will use Create React App to scaffold the directory. Its contents will be separated into folders for each stateful component.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThe themes and plugins we make often need to add fields to the admin interface.\\\\nWe've been using ACF PRO on nearly every new site we've made in the past few years, and we've used the free version of ACF for perhaps 5-7 years. We're comfortable with it and it's proven its stability and usefulness.\\\\n\"}, {'role': 'model', 'content': \"Install ACF PRO on every site we create. If we don't need it, we can remove it before deployment.\\\\nTo add a field to a theme or plugin we should copy the PHP code and paste it into the codebase.\\\\n\"}]"}
{"File Name":"bibdata\/0001-document-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will need to record the architectural decisions made on this project to orient new developers.\\n","tokens":16,"id":1806,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need to record the architectural decisions made on this project.\\\\n'}, {'role': 'model', 'content': 'We will need to record the architectural decisions made on this project to orient new developers.\\\\n'}]"}
{"File Name":"GoStudy\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":430,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need to record the architectural decisions made on this project.\\\\n'}, {'role': 'model', 'content': 'We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\\\n'}]"}
{"File Name":"Marain.Instance\/0003-shared-application-insights.md","Context":"## Context\\nMarain services need to be able to deliver diagnostic information somewhere.\\n","Decision":"`Marain.Instance` creates a single Application Insights instance and makes its key available to all services. All services use it.\\n","tokens":17,"id":355,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nFor our current monitoring to work we need Icinga to be able to communicate with hosts and services.\\n\\n## Decision\\nEach stack will have an internal, private, zone for internal services such as the puppetmaster. These\\nwill be in the following format:\\n$servicename.$stackname.internal\\npuppet.perftesting.internal\\nmonitoring.mystack.internal\\n\\n\\n## Context\\nWe need a reliable infrastructure definition that will allow developers to replicate the environment on their local\\nmachine.\\n\\n## Decision\\nWe use Docker (1.13.0+) to encapsulate all architecture components. We use Docker Compose (1.13.0+) to orchestrate\\nthe containers in a local environment.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nMarain services need to be able to deliver diagnostic information somewhere.\\\\n'}, {'role': 'model', 'content': '`Marain.Instance` creates a single Application Insights instance and makes its key available to all services. All services use it.\\\\n'}]"}
{"File Name":"cosmos-sdk\/adr-044-protobuf-updates-guidelines.md","Context":"## Context\\nThe Cosmos SDK maintains a set of [Protobuf definitions](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/proto\/cosmos). It is important to correctly design Protobuf definitions to avoid any breaking changes within the same version. The reasons are to not break tooling (including indexers and explorers), wallets and other third-party integrations.\\nWhen making changes to these Protobuf definitions, the Cosmos SDK currently only follows [Buf's](https:\/\/docs.buf.build\/) recommendations. We noticed however that Buf's recommendations might still result in breaking changes in the SDK in some cases. For example:\\n* Adding fields to `Msg`s. Adding fields is a not a Protobuf spec-breaking operation. However, when adding new fields to `Msg`s, the unknown field rejection will throw an error when sending the new `Msg` to an older node.\\n* Marking fields as `reserved`. Protobuf proposes the `reserved` keyword for removing fields without the need to bump the package version. However, by doing so, client backwards compatibility is broken as Protobuf doesn't generate anything for `reserved` fields. See [#9446](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/9446) for more details on this issue.\\nMoreover, module developers often face other questions around Protobuf definitions such as \"Can I rename a field?\" or \"Can I deprecate a field?\" This ADR aims to answer all these questions by providing clear guidelines about allowed updates for Protobuf definitions.\\n","Decision":"We decide to keep [Buf's](https:\/\/docs.buf.build\/) recommendations with the following exceptions:\\n* `UNARY_RPC`: the Cosmos SDK currently does not support streaming RPCs.\\n* `COMMENT_FIELD`: the Cosmos SDK allows fields with no comments.\\n* `SERVICE_SUFFIX`: we use the `Query` and `Msg` service naming convention, which doesn't use the `-Service` suffix.\\n* `PACKAGE_VERSION_SUFFIX`: some packages, such as `cosmos.crypto.ed25519`, don't use a version suffix.\\n* `RPC_REQUEST_STANDARD_NAME`: Requests for the `Msg` service don't have the `-Request` suffix to keep backwards compatibility.\\nOn top of Buf's recommendations we add the following guidelines that are specific to the Cosmos SDK.\\n### Updating Protobuf Definition Without Bumping Version\\n#### 1. Module developers MAY add new Protobuf definitions\\nModule developers MAY add new `message`s, new `Service`s, new `rpc` endpoints, and new fields to existing messages. This recommendation follows the Protobuf specification, but is added in this document for clarity, as the SDK requires one additional change.\\nThe SDK requires the Protobuf comment of the new addition to contain one line with the following format:\\n```protobuf\\n\/\/ Since: cosmos-sdk <version>{, <version>...}\\n```\\nWhere each `version` denotes a minor (\"0.45\") or patch (\"0.44.5\") version from which the field is available. This will greatly help client libraries, who can optionally use reflection or custom code generation to show\/hide these fields depending on the targeted node version.\\nAs examples, the following comments are valid:\\n```protobuf\\n\/\/ Since: cosmos-sdk 0.44\\n\/\/ Since: cosmos-sdk 0.42.11, 0.44.5\\n```\\nand the following ones are NOT valid:\\n```protobuf\\n\/\/ Since cosmos-sdk v0.44\\n\/\/ since: cosmos-sdk 0.44\\n\/\/ Since: cosmos-sdk 0.42.11 0.44.5\\n\/\/ Since: Cosmos SDK 0.42.11, 0.44.5\\n```\\n#### 2. Fields MAY be marked as `deprecated`, and nodes MAY implement a protocol-breaking change for handling these fields\\nProtobuf supports the [`deprecated` field option](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto#options), and this option MAY be used on any field, including `Msg` fields. If a node handles a Protobuf message with a non-empty deprecated field, the node MAY change its behavior upon processing it, even in a protocol-breaking way. When possible, the node MUST handle backwards compatibility without breaking the consensus (unless we increment the proto version).\\nAs an example, the Cosmos SDK v0.42 to v0.43 update contained two Protobuf-breaking changes, listed below. Instead of bumping the package versions from `v1beta1` to `v1`, the SDK team decided to follow this guideline, by reverting the breaking changes, marking those changes as deprecated, and modifying the node implementation when processing messages with deprecated fields. More specifically:\\n* The Cosmos SDK recently removed support for [time-based software upgrades](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/8849). As such, the `time` field has been marked as deprecated in `cosmos.upgrade.v1beta1.Plan`. Moreover, the node will reject any proposal containing an upgrade Plan whose `time` field is non-empty.\\n* The Cosmos SDK now supports [governance split votes](.\/adr-037-gov-split-vote.md). When querying for votes, the returned `cosmos.gov.v1beta1.Vote` message has its `option` field (used for 1 vote option) deprecated in favor of its `options` field (allowing multiple vote options). Whenever possible, the SDK still populates the deprecated `option` field, that is, if and only if the `len(options) == 1` and `options[0].Weight == 1.0`.\\n#### 3. Fields MUST NOT be renamed\\nWhereas the official Protobuf recommendations do not prohibit renaming fields, as it does not break the Protobuf binary representation, the SDK explicitly forbids renaming fields in Protobuf structs. The main reason for this choice is to avoid introducing breaking changes for clients, which often rely on hard-coded fields from generated types. Moreover, renaming fields will lead to client-breaking JSON representations of Protobuf definitions, used in REST endpoints and in the CLI.\\n### Incrementing Protobuf Package Version\\nTODO, needs architecture review. Some topics:\\n* Bumping versions frequency\\n* When bumping versions, should the Cosmos SDK support both versions?\\n* i.e. v1beta1 -> v1, should we have two folders in the Cosmos SDK, and handlers for both versions?\\n* mention ADR-023 Protobuf naming\\n","tokens":314,"id":825,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nOpenShift likes to shift (pun intended) the UID -- i.e., assign arbitrary UIDs -- to containers. They do this as an additional security feature, given that OpenShift is a multi-tentant Kubernetes solution. Each OpenShift project received a non-overlapping UID range. Hence, in case an attacker escapes a container, it will be more difficult to interfere with other processes.\\nHowever, this shifting of UIDs introduces an additional complexity: What if a process wants to write to the filesystem? What uid, gid and permissions should the files and folders have? To solve this problem, the OpenShift documentation (see [\"Support arbitrary user ids\"][openshift-docs]) recommends setting gid=0 on those files and folders. Specifically, the Dockerfiles of the container images should contain:\\n```Dockerfile\\nRUN chgrp -R 0 \/some\/directory && chmod -R g=u \/some\/directory\\n```\\nDuring execution, OpenShift assigns `gid=0` as a supplementary group to containers, so as to give them access to the required files.\\nIn contrast to OpenShift, Compliant Kubernetes is not a multi-tenant solution. Given previous vulnerabilities in Kubernetes that affected tenant isolation (e.g., [CVE-2020-8554][cve]\\n), we believe that non-trusting users should not share a workload cluster. Hence, we do not assign arbitrary UIDs to containers and do not need to assign `gid=0` as a supplementary group.\\nThe `gid=0` practice above seems to have made its way in [quite a few Dockerfiles][github-search], however, it is far from being the default outside OpenShift.\\nWhat should Compliant Kubernetes do with the `gid=0` practice?\\n## Decision Drivers\\n* For user expectations, we want to make it easy to start with Compliant Kubernetes.\\n* For better security and easier audits, we do not want to add unnecessary permissions.\\n* [ID mapping in mounts][idmapping] has landed in Linux 5.12. Once this feature is used in container runtimes and Kubernetes, the `gid=0` problem will go away.\\n\\n## Decision\\n* For user expectations, we want to make it easy to start with Compliant Kubernetes.\\n* For better security and easier audits, we do not want to add unnecessary permissions.\\n* [ID mapping in mounts][idmapping] has landed in Linux 5.12. Once this feature is used in container runtimes and Kubernetes, the `gid=0` problem will go away.\\nChosen option: \"disallow `gid=0` by default\". Enabling it on a case-by-case basis is okay.\\n### Positive Consequences\\n* We do not unnecessarily add a permission to containers.\\n### Negative Consequences\\n* Some users will complain about their container images not starting, and we will need to add a less restricted PodSecurityPolicy in their cluster.\\n\\n\\n## Context\\nStrongly-typed code can provide a number of advantages in a medium-large codebase:\\n- Catch errors much earlier (at build-time). Runtime errors require manual testing or tools such as Selenium to track down\u2014this is more expensive and prone to failure.\\n- Codebase is easier to understand as the code is much more explicit\u2014potentially easier on-boarding of new developers.\\n- Available tooling such as [IntellisenseTM](https:\/\/code.visualstudio.com\/docs\/editor\/intellisense) enhances developer experience.\\nHowever, there are a number of risks:\\n- Increased complexity of code\u2014many developers are unfamiliar with stongly-typed languages.\\n- Increased complexity of tooling needed to compile the code. This is somewhat negated by the fact the existing build setup already transpiles code via Babel.\\n### Flow vs. TypeScript\\nThere are two significant options in the strongly-typed Javascript market\u2014[Flow](https:\/\/flow.org\/) (Facebook) and [TypeScript](https:\/\/www.typescriptlang.org\/) (Microsoft). Whilst Flow is a type-checker, TypeScript is a superset of the JavaScript language itself and both require additional tooling (in our case, via Webpack) to work.\\nFor this exercise, we evaluated both options by setting up the relevant tools and refactoring some existing code to include types.\\n#### Flow\\nAlthough fairly easy to setup initially, integration with third-party libraries was problematic and poorly documented in some cases. [https:\/\/github.com\/flowtype\/flow-typed](flow-typed) was used (as recommended) and things worked briefly, errors due to incompatibilities between versions of software quickly became a problem.\\n#### TypeScript\\nTypescript was again, fairly easy to setup but, again, integrations with third-party libraries was the main cause of problems. The solution to this, installing type definitions via NPM seemed a better option than flow-typed. We were able to refactor some existing code successfully.\\n\\n## Decision\\nTypeScript was slightly easier to set up and we were successful in refactoring existing code and so was the obvious choice. Going forward, it is recommended that we apply first to the [https:\/\/github.com\/ONSdigital\/eq-author-api](eQ Author API) codebase as it is smaller and simpler with less dependencies. This will be a valuable exercise, allowing us to understand the system better and make a decision regarding implementing TypeScript in the Author Application itself.\\nThe [https:\/\/github.com\/ONSdigital\/eq-author\/tree\/spike-typescript](branch used for this spike) is still available on GitHub for reference.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThe Cosmos SDK maintains a set of [Protobuf definitions](https:\/\/github.com\/cosmos\/cosmos-sdk\/tree\/main\/proto\/cosmos). It is important to correctly design Protobuf definitions to avoid any breaking changes within the same version. The reasons are to not break tooling (including indexers and explorers), wallets and other third-party integrations.\\\\nWhen making changes to these Protobuf definitions, the Cosmos SDK currently only follows [Buf\\'s](https:\/\/docs.buf.build\/) recommendations. We noticed however that Buf\\'s recommendations might still result in breaking changes in the SDK in some cases. For example:\\\\n* Adding fields to `Msg`s. Adding fields is a not a Protobuf spec-breaking operation. However, when adding new fields to `Msg`s, the unknown field rejection will throw an error when sending the new `Msg` to an older node.\\\\n* Marking fields as `reserved`. Protobuf proposes the `reserved` keyword for removing fields without the need to bump the package version. However, by doing so, client backwards compatibility is broken as Protobuf doesn\\'t generate anything for `reserved` fields. See [#9446](https:\/\/github.com\/cosmos\/cosmos-sdk\/issues\/9446) for more details on this issue.\\\\nMoreover, module developers often face other questions around Protobuf definitions such as \"Can I rename a field?\" or \"Can I deprecate a field?\" This ADR aims to answer all these questions by providing clear guidelines about allowed updates for Protobuf definitions.\\\\n'}, {'role': 'model', 'content': 'We decide to keep [Buf\\'s](https:\/\/docs.buf.build\/) recommendations with the following exceptions:\\\\n* `UNARY_RPC`: the Cosmos SDK currently does not support streaming RPCs.\\\\n* `COMMENT_FIELD`: the Cosmos SDK allows fields with no comments.\\\\n* `SERVICE_SUFFIX`: we use the `Query` and `Msg` service naming convention, which doesn\\'t use the `-Service` suffix.\\\\n* `PACKAGE_VERSION_SUFFIX`: some packages, such as `cosmos.crypto.ed25519`, don\\'t use a version suffix.\\\\n* `RPC_REQUEST_STANDARD_NAME`: Requests for the `Msg` service don\\'t have the `-Request` suffix to keep backwards compatibility.\\\\nOn top of Buf\\'s recommendations we add the following guidelines that are specific to the Cosmos SDK.\\\\n### Updating Protobuf Definition Without Bumping Version\\\\n#### 1. Module developers MAY add new Protobuf definitions\\\\nModule developers MAY add new `message`s, new `Service`s, new `rpc` endpoints, and new fields to existing messages. This recommendation follows the Protobuf specification, but is added in this document for clarity, as the SDK requires one additional change.\\\\nThe SDK requires the Protobuf comment of the new addition to contain one line with the following format:\\\\n```protobuf\\\\n\/\/ Since: cosmos-sdk <version>{, <version>...}\\\\n```\\\\nWhere each `version` denotes a minor (\"0.45\") or patch (\"0.44.5\") version from which the field is available. This will greatly help client libraries, who can optionally use reflection or custom code generation to show\/hide these fields depending on the targeted node version.\\\\nAs examples, the following comments are valid:\\\\n```protobuf\\\\n\/\/ Since: cosmos-sdk 0.44\\\\n\/\/ Since: cosmos-sdk 0.42.11, 0.44.5\\\\n```\\\\nand the following ones are NOT valid:\\\\n```protobuf\\\\n\/\/ Since cosmos-sdk v0.44\\\\n\/\/ since: cosmos-sdk 0.44\\\\n\/\/ Since: cosmos-sdk 0.42.11 0.44.5\\\\n\/\/ Since: Cosmos SDK 0.42.11, 0.44.5\\\\n```\\\\n#### 2. Fields MAY be marked as `deprecated`, and nodes MAY implement a protocol-breaking change for handling these fields\\\\nProtobuf supports the [`deprecated` field option](https:\/\/developers.google.com\/protocol-buffers\/docs\/proto#options), and this option MAY be used on any field, including `Msg` fields. If a node handles a Protobuf message with a non-empty deprecated field, the node MAY change its behavior upon processing it, even in a protocol-breaking way. When possible, the node MUST handle backwards compatibility without breaking the consensus (unless we increment the proto version).\\\\nAs an example, the Cosmos SDK v0.42 to v0.43 update contained two Protobuf-breaking changes, listed below. Instead of bumping the package versions from `v1beta1` to `v1`, the SDK team decided to follow this guideline, by reverting the breaking changes, marking those changes as deprecated, and modifying the node implementation when processing messages with deprecated fields. More specifically:\\\\n* The Cosmos SDK recently removed support for [time-based software upgrades](https:\/\/github.com\/cosmos\/cosmos-sdk\/pull\/8849). As such, the `time` field has been marked as deprecated in `cosmos.upgrade.v1beta1.Plan`. Moreover, the node will reject any proposal containing an upgrade Plan whose `time` field is non-empty.\\\\n* The Cosmos SDK now supports [governance split votes](.\/adr-037-gov-split-vote.md). When querying for votes, the returned `cosmos.gov.v1beta1.Vote` message has its `option` field (used for 1 vote option) deprecated in favor of its `options` field (allowing multiple vote options). Whenever possible, the SDK still populates the deprecated `option` field, that is, if and only if the `len(options) == 1` and `options[0].Weight == 1.0`.\\\\n#### 3. Fields MUST NOT be renamed\\\\nWhereas the official Protobuf recommendations do not prohibit renaming fields, as it does not break the Protobuf binary representation, the SDK explicitly forbids renaming fields in Protobuf structs. The main reason for this choice is to avoid introducing breaking changes for clients, which often rely on hard-coded fields from generated types. Moreover, renaming fields will lead to client-breaking JSON representations of Protobuf definitions, used in REST endpoints and in the CLI.\\\\n### Incrementing Protobuf Package Version\\\\nTODO, needs architecture review. Some topics:\\\\n* Bumping versions frequency\\\\n* When bumping versions, should the Cosmos SDK support both versions?\\\\n* i.e. v1beta1 -> v1, should we have two folders in the Cosmos SDK, and handlers for both versions?\\\\n* mention ADR-023 Protobuf naming\\\\n'}]"}
{"File Name":"community\/dr-011-Jaeger_as_a_tracing_back-end.md","Context":"## Context\\nBy default, Istio uses Zipkin for distributed tracing. Zipkin is not the best solution for Kyma as it is a heavy-weight Spring-based Java application.\\nThe Kyma developers evaluated Jaeger which is a lightweight alternative to Zipkin.\\n","Decision":"The decision is to use Jaeger as a back-end tool for tracing. Istio, Envoy, and some Kyma components, such as the Event Bus or the Connector Gateway, will continue to use the Zipkin client. This approach is necessary as Istio supports only the Zipkin client. Jaeger as a back-end tool is compatible with both the Zipkin client and the wire protocol. For the customer developer this is transparent since Zipkin and the wire protocol propagate headers only when making HTTP calls, and they do not need any instrumentation.\\nThe decision is to use Jaeger with the in-memory storage as, similarly to Zipkin, Jaeger can lose traces. However, there are efforts underway in the Jaeger project to make the in-memory solution production-ready. See this [Jaeger GitHub issue](https:\/\/github.com\/jaegertracing\/jaeger\/issues\/551) for more information.\\n","tokens":55,"id":3454,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\\nserver.\\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\\nObject Storage technologies.\\nObject Storage is known to be cheap and yet offers guarantee about durability.\\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\\n[Apache jcloud library](https:\/\/jclouds.apache.org\/) because we wanted to target both S3 and Swift API.\\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\\nIt also contribute negatively to the project build time.\\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\\nour Swift code without dropping Swift support at the same time.\\n\\n## Decision\\n* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\\n\\n\\n## Context\\nTo integrate with the HSES oauth2 a package providing that functionality can be used to help the development. Two packages were considered for this purpose:\\n1. passport - one of the more popular and extensive Javascript packages covering various authorization flows with multiple strategies. Passport provides a lot of functionality of out the box and was initially explored.\\n2. client-oauth2 - a smaller package allowing a straight-forward implementation.\\n\\n## Decision\\nEven though passport was originally considered, client-oauth2 will be used.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nBy default, Istio uses Zipkin for distributed tracing. Zipkin is not the best solution for Kyma as it is a heavy-weight Spring-based Java application.\\\\nThe Kyma developers evaluated Jaeger which is a lightweight alternative to Zipkin.\\\\n\"}, {'role': 'model', 'content': 'The decision is to use Jaeger as a back-end tool for tracing. Istio, Envoy, and some Kyma components, such as the Event Bus or the Connector Gateway, will continue to use the Zipkin client. This approach is necessary as Istio supports only the Zipkin client. Jaeger as a back-end tool is compatible with both the Zipkin client and the wire protocol. For the customer developer this is transparent since Zipkin and the wire protocol propagate headers only when making HTTP calls, and they do not need any instrumentation.\\\\nThe decision is to use Jaeger with the in-memory storage as, similarly to Zipkin, Jaeger can lose traces. However, there are efforts underway in the Jaeger project to make the in-memory solution production-ready. See this [Jaeger GitHub issue](https:\/\/github.com\/jaegertracing\/jaeger\/issues\/551) for more information.\\\\n'}]"}
{"File Name":"platform\/2020-12-03-The-best-practice-to-always-re-fetch-the-data-after-saving.md","Context":"## Context\\nWe should always re-fetch the entity data after saving within admin pages.\\n","Decision":"Reload the data after each saving progress to ensure the user will work only the latest data.\\nWhen you save data without reloading the entity, then you need to re-assign the values. But you can't be sure, that these values are the latest ones, because of possible data inconsistency during the saving process. That's why re-fetching data is always important for further CRUD operations.\\nFor example:\\n```html\\n<!-- we change the status by click to switch for example -->\\n<sw-switch-field\\nv-model=\"data.status\"\\n:label=\"$tc('sw-review.detail.labelStatus')\">\\n<\/sw-switch-field>\\n<!-- we will save data with onSave method -->\\n<sw-button-process @click=\"onSave\">\\n{{ $tc('global.default.save') }}\\n<\/sw-button-process>\\n```\\n```javascript\\n\/\/ This method for button save\\nonSave() {\\nthis.repository.save(this.data, Shopware.Context.api).then(() => {\\n\/\/ We should add the method to re-fetch the entity data after save success here\\nthis.loadEntityData();\\n});\\n},\\n\/\/ This method to re-fetch the data\\nloadEntityData() {\\nconst criteria = new Criteria();\\nconst context = { ...Shopware.Context.api, inheritance: true };\\nthis.repository.get(this.data.id, context, criteria).then((data) => {\\nthis.data = data;\\n});\\n},\\n```\\n","tokens":18,"id":4530,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe need a consistent way to add menus within fragments.\\n\\n## Decision\\nEvery fragment handles its menu independently. This is achieved through calling setHasOptionsMenu(true)\\nand overriding the specific methods needed for creating the menu and handling selection events.\\n\\n\\n## Context\\nWe need to decide the format and semantics of a command type within the actor system.\\n\\n## Decision\\nWe will use a regular class for a command. The data of a command is added as properties with public getters and setters. A command should be handled as an immutable type.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe should always re-fetch the entity data after saving within admin pages.\\\\n'}, {'role': 'model', 'content': 'Reload the data after each saving progress to ensure the user will work only the latest data.\\\\nWhen you save data without reloading the entity, then you need to re-assign the values. But you can\\'t be sure, that these values are the latest ones, because of possible data inconsistency during the saving process. That\\'s why re-fetching data is always important for further CRUD operations.\\\\nFor example:\\\\n```html\\\\n<!-- we change the status by click to switch for example -->\\\\n<sw-switch-field\\\\nv-model=\"data.status\"\\\\n:label=\"$tc(\\'sw-review.detail.labelStatus\\')\">\\\\n<\/sw-switch-field>\\\\n<!-- we will save data with onSave method -->\\\\n<sw-button-process @click=\"onSave\">\\\\n{{ $tc(\\'global.default.save\\') }}\\\\n<\/sw-button-process>\\\\n```\\\\n```javascript\\\\n\/\/ This method for button save\\\\nonSave() {\\\\nthis.repository.save(this.data, Shopware.Context.api).then(() => {\\\\n\/\/ We should add the method to re-fetch the entity data after save success here\\\\nthis.loadEntityData();\\\\n});\\\\n},\\\\n\/\/ This method to re-fetch the data\\\\nloadEntityData() {\\\\nconst criteria = new Criteria();\\\\nconst context = { ...Shopware.Context.api, inheritance: true };\\\\nthis.repository.get(this.data.id, context, criteria).then((data) => {\\\\nthis.data = data;\\\\n});\\\\n},\\\\n```\\\\n'}]"}
{"File Name":"nebula\/0005-stack-k8s-controllers-with-etcd.md","Context":"Context\\n-------\\nThere are three basic ways to lay out controller nodes in a kubernetes\\ncluster:\\n1.  A single master is the easiest way to do it, but if that node is\\never unavailable, then the entire cluster is unavailable.\\n2.  A highly-available group of at least three controller nodes that\\nalso run etcd.\\n3.  Two highly-available groups: one of at least three controller nodes,\\nand the other of at least three etcd nodes.\\nThe kubernetes website has [a detailed rundown][1], but it boils down to\\nthis: high availability complicates the set-up significantly, and\\nseparating etcd from the control plane complicates it yet more.\\nHowever, if we want to allocate a lot of resources to high availability\\nin our kubernetes clusters, then etcd is a nice seam to further divide\\nresponsibility as we scale up the count of control nodes.\\n[1]: https:\/\/kubernetes.io\/docs\/setup\/independent\/ha-topology\/\\nDecision\\n--------\\nWe will start with the middle path of minimal (but extant) high\\navailability. Low availability would fail to grant us a lot of what\\nmakes kubernetes worthwhile, but we're just not in a place where we're\\nready to commit the extra resources to get the highest availability\\npossible.\\nConsequences\\n------------\\nIf we start growing our pool of stacked controller nodes beyond 5, then\\nwe should consider revisiting this decision.\\n","Decision":"--------\\nWe will start with the middle path of minimal (but extant) high\\navailability. Low availability would fail to grant us a lot of what\\nmakes kubernetes worthwhile, but we're just not in a place where we're\\nready to commit the extra resources to get the highest availability\\npossible.\\nConsequences\\n------------\\nIf we start growing our pool of stacked controller nodes beyond 5, then\\nwe should consider revisiting this decision.\\n","tokens":311,"id":4947,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\nContext\\n-------\\nKubernetes requires an internal private network, just for its pods and\\nservices (i.e. not for physical\/virtual nodes). It has specific\\nexpectations, but it doesn\\'t actually provide networking itself. There\\nare a few options for internal networking: flannel, calico, canal, and\\nweave.\\nOf these, Flannel is the most popular. It\\'s known for being easy to set\\nup and then never having problems ever. The network it creates is a\\nlayer 3 IPv4 overlay network that spans across every node in the\\ncluster. Each node gets its own subnet for allocating internal IP\\naddresses for docker containers.\\nProject Calico is also popular, but is less simple and (allegedly) more\\nperformant. The network it creates isn\\'t an overlay but rather a layer 3\\nnetwork using the BGP protocol to route packets between hosts.\\nCanal is a combination of flannel and calico. It\\'s not a project anymore\\nbecause it collaborated itself out of existence by making pull requests\\nto flannel and calico until they worked well enough together that there\\nwas nothing extra for canal to do. People still use the word \"canal\" to\\nrefer to using a combination of flannel and calico.\\nWeave Net creates a mesh overlay network between each of the nodes in\\nthe cluster. Each host becomes a router, and they\\'re always exchanging\\ntopology information with each other. One of weave\\'s unique features is\\nthat it can (with some network overhead) encrypt all routed traffic.\\nThe rancher project wrote [a pretty informative comparison][1] if you\\'re\\ninterested in more details.\\n[1]: https:\/\/rancher.com\/blog\/2019\/2019-03-21-comparing-kubernetes-cni-providers-flannel-calico-canal-and-weave\/\\nDecision\\n--------\\nWe will use calico because it\\'s rancher\\'s default. It\\'s easy enough to\\nset up.\\nConsequences\\n------------\\nAs of yet unknown.\\n\\n## Decision\\n--------\\nWe will use calico because it\\'s rancher\\'s default. It\\'s easy enough to\\nset up.\\nConsequences\\n------------\\nAs of yet unknown.\\n\\n\\nContext\\n-------\\nThere are many cloud kubernetes offerings (Amazon, Microsoft, and Google\\nall offer it), and part of the appeal in general is to not have to think\\nabout where the servers are and what kind of maintenance they need.\\nHowever, we already operate servers in three datacenters, and we fully\\nmanage one of those. We may prefer this not to be the case, but as of\\nwriting this, we deal with bare-metal servers regardless of what we\\'d\\nprefer. Adding ec2 instances costs extra money, where running additional\\nsoftware on servers (with an electric bill we\\'re already paying) costs\\nthe labor of setting it up.\\nThe attitude of the field so far appears to be \"definitely just run it\\nin AKS,\" but the field is largely made up of people whose primary\\ninterests are reliability and profits. As a digital academic library, we\\nalso have those interests, but we have an additional responsibility to\\nalways have our own copy of everything.\\nDecision\\n--------\\nWe will provide kubernetes clusters at our datacenters with the\\npossibility of expanding outward into cloud providers as needed.\\nConsequences\\n------------\\nBy providing a service that mirrors a cloud service, we can use the same\\ncode and configuration to deploy locally and remotely. We\\'ll need to\\nfigure out how we want to manage applications and how we\\'ll want them to\\nscale depending on where they live. Since developers will essentially be\\ndeploying to three different clusters, we\\'ll need to decide how they\\nshould do that.\\n\\n## Decision\\n--------\\nWe will provide kubernetes clusters at our datacenters with the\\npossibility of expanding outward into cloud providers as needed.\\nConsequences\\n------------\\nBy providing a service that mirrors a cloud service, we can use the same\\ncode and configuration to deploy locally and remotely. We\\'ll need to\\nfigure out how we want to manage applications and how we\\'ll want them to\\nscale depending on where they live. Since developers will essentially be\\ndeploying to three different clusters, we\\'ll need to decide how they\\nshould do that.\\n\\n\\n\\nProvide a decision given the context below:\\nContext\\\\n-------\\\\nThere are three basic ways to lay out controller nodes in a kubernetes\\\\ncluster:\\\\n1.  A single master is the easiest way to do it, but if that node is\\\\never unavailable, then the entire cluster is unavailable.\\\\n2.  A highly-available group of at least three controller nodes that\\\\nalso run etcd.\\\\n3.  Two highly-available groups: one of at least three controller nodes,\\\\nand the other of at least three etcd nodes.\\\\nThe kubernetes website has [a detailed rundown][1], but it boils down to\\\\nthis: high availability complicates the set-up significantly, and\\\\nseparating etcd from the control plane complicates it yet more.\\\\nHowever, if we want to allocate a lot of resources to high availability\\\\nin our kubernetes clusters, then etcd is a nice seam to further divide\\\\nresponsibility as we scale up the count of control nodes.\\\\n[1]: https:\/\/kubernetes.io\/docs\/setup\/independent\/ha-topology\/\\\\nDecision\\\\n--------\\\\nWe will start with the middle path of minimal (but extant) high\\\\navailability. Low availability would fail to grant us a lot of what\\\\nmakes kubernetes worthwhile, but we\\'re just not in a place where we\\'re\\\\nready to commit the extra resources to get the highest availability\\\\npossible.\\\\nConsequences\\\\n------------\\\\nIf we start growing our pool of stacked controller nodes beyond 5, then\\\\nwe should consider revisiting this decision.\\\\n'}, {'role': 'model', 'content': \"--------\\\\nWe will start with the middle path of minimal (but extant) high\\\\navailability. Low availability would fail to grant us a lot of what\\\\nmakes kubernetes worthwhile, but we're just not in a place where we're\\\\nready to commit the extra resources to get the highest availability\\\\npossible.\\\\nConsequences\\\\n------------\\\\nIf we start growing our pool of stacked controller nodes beyond 5, then\\\\nwe should consider revisiting this decision.\\\\n\"}]"}
{"File Name":"lbh-adrs\/Record-Versioning.md","Context":"## **Context**\\nIn order to\\n","Decision":"What is the change that we're proposing and\/or doing?\\n","tokens":10,"id":2312,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\n\\n## Decision\\nTBD\\n\\n\\n## Context\\n\\n## Decision\\nTBD\\n\\n\\n\\nProvide a decision given the context below:\\n## **Context**\\\\nIn order to\\\\n'}, {'role': 'model', 'content': \"What is the change that we're proposing and\/or doing?\\\\n\"}]"}
{"File Name":"manage-frontend\/03-typescript.md","Context":"## Context\\nWe've found the use of strongly typed programming languages to be advantageous in reducing software defects. We have previously used Flow, however its error messages are regularly impenetrable. Without the burden of a legacy codebase to support, the benefits of Flow over Typescript are significantly less.\\n","Decision":"We will use Typescript as the programming language for this project.\\n","tokens":61,"id":3709,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe choice of which programming language to use is governed by a host of different factors. Most languages can be made to work for whatever task is required, though some are better at certain tasks than others.\\nWe expect the nature of the work in this project to benefit from concurrency, so choosing a language with good support for this is important. Since we will be deploying to AWS Lambda (See [5. Use AWS Lambda](0005-use-aws-lambda.md)), we are further limited to using one of the supported languages. Other considerations include ease of packaging and distribution, excellent data streaming abilities, and a healthy ecosystem of 3rd party libraries.\\n\\n## Decision\\nUse Go for the core application language.\\n\\n\\n## Context\\nWe've avoided using React hooks for some time in order to ensure un-desired complexity is avoided in the code base. But as hooks are now standard fare in React applications, it makes sense to review our usage of them in DCR.\\n\\n## Decision\\n- Prefer non-stateful components if possible\\n- Prefer React's official hooks to custom hooks\\n- Avoid abstractions that could lead to hooks within hooks within hooks.\\n- Prefer hooks to classes with component lifecycle methods\\n- Try to build hooks that are generic and reusable\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe've found the use of strongly typed programming languages to be advantageous in reducing software defects. We have previously used Flow, however its error messages are regularly impenetrable. Without the burden of a legacy codebase to support, the benefits of Flow over Typescript are significantly less.\\\\n\"}, {'role': 'model', 'content': 'We will use Typescript as the programming language for this project.\\\\n'}]"}
{"File Name":"uniprot-website\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":2015,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need to record the architectural decisions made on this project.\\\\n'}, {'role': 'model', 'content': 'We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\\\n'}]"}
{"File Name":"eq-questionnaire-runner\/0007-relationships.md","Context":"## Context\\nWe need to be able to collect the (one way) relationships between list items. For example, given a list of items A, B and C, we need to be able to capture the relationships between A and B, A and C, and B and C. The previous implementation of relationships relied on group instance, functionality that has now been removed. Now that we have a new [url structure](0005-simplify-urls-and-extend-to-support-repeating-effectively.md) and [lists](0006-make-named-lists-a-first-class-construct.md), we can design how relationships will work.\\n","Decision":"### URLs\\nThe url for an individual relation for a 'relationships' block would be:\\n```\\n\/<block_id>\/<list_item_id>\/to\/<to_list_item_id>\\n\/relationships\/a9hd8j\/to\/skjwo9\\n```\\nWhere the `list_item_id` and `to_list_item_id` identify which list items the relation is being formed between.\\n### Storing relationships\\nFor each relationship we need to store both the relationship type and the items that are related. This is represented by storing all relationships as a list within a single answer:\\n```\\n{\\n\"answer_id\": \"relationship\"\\n\"value\": [\\n{\\n\"list_item_id\": \"a9hd8j\",\\n\"relationship\": \"Husband or Wife\",\\n\"to_list_item_id\": \"kl9s3y\"\\n},\\n{\\n\"list_item_id\": \"a9hd8j\",\\n\"relationship\": \"Father or Mother\",\\n\"to_list_item_id\": \"skjwo9\"\\n}\\n]\\n}\\n```\\n### Schema\\nThe relationship block will be defined as one block in the schema.\\nThe question text for relationships is dynamic and changes dependent the two people the relationship is being asked of:\\n```\\nThinking of {first_person_name}, how are they related to {second_person_name}\\n```\\nThe first person name can be resolved by the current placeholder resolution for repeating questions - assume any reference to answer ids when in a repeating section should resolve to the answers for the list item being repeated over:\\n```\\n{\\n\"source\": \"answers\",\\n\"identifiers\": [\"first-name\", \"last-name\"],\\n}\\n```\\nThe second person name is more difficult, as we will need to resolve the answers for a list item outside of the current repeat. Adding a `list_item_selector` property to the placeholder source definition provides a way to select a list item. This would be a fixed value used to resolve a list item id:\\n```\\n{\\n\"source\": \"answers\",\\n\"list_item_selector\": \"related_list_item_id\"\\n\"identifiers\": [\"first-name\", \"last-name\"],\\n}\\n```\\nIn this example runner would need to resolve `related_list_item_id` to the url parameter.\\n","tokens":128,"id":3675,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nCQRS stands for Command Query Responsibility Segregation. It's a pattern that I first heard described by Greg Young. At its heart is the notion that you can use a different model to update information than the model you use to read information.\\nThe core of the ODH are the [pubsub](0016-pub-sub-implements-event-sourcing.md) topics implementing [event sourcing](0010-event-sourcing-captures-every-change-to-business-state.md). This allows consumers of these events to build a data representation (model) from these events that exactly fits their use case. The result is that different models exist, representing the same real-life entities.\\n\\n## Decision\\nWe will implement Command Query Responsibility Segregation (CQRS) on the ODH between  [projects](0028-a-solution-is-implemented-by-one-or-more-gcp-projects.md).\\nCQRS is not required within a project and should only be applied at project level if the additional complexity is justified by CQRS-specific advantages.\\n\\n\\n## Context\\nDatasets are made up of observations and dimensions. Each of the options for a\\ndimension have wider definitions and metadata which apply to that option in all\\nscenarios, across multiple datasets. These code definitions live in code-lists.\\nThe list of codes in the code-list may not exactly match the list of codes in\\nthe dimension options, due to sparsity or how datasets are broken down.\\nIn order to build a list of datasets which use each code, a relationship needs\\nto be built between the code-list\/code graph and the instance\/dimension-option\\ngraph. There was a question as to whether this should link:\\n- **A) From: code-list, To: instance** -  As instances may contain sparse\\nsubsets of a code-list, this option would still require extra work to know\\nwhich codes specifically existed per instance.\\n- **B) From: code, To: instance** - This effectively duplicates the existing relationships to an instance, as it is the same as the dimension options.\\n- **C) From: code, To: dimension option** - The extra step to traverse back up\\nto the instance decreases the performance.\\n- **D) Don't build a relationship at all, just query for it on request** -\\nThis is very un-performant, and something that can easily be optimized by\\nbuilding the relationships.\\n\\n## Decision\\nA relationship should be created between a code node and each instance node which\\nfeatures the code as a dimension option. (Option B)\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need to be able to collect the (one way) relationships between list items. For example, given a list of items A, B and C, we need to be able to capture the relationships between A and B, A and C, and B and C. The previous implementation of relationships relied on group instance, functionality that has now been removed. Now that we have a new [url structure](0005-simplify-urls-and-extend-to-support-repeating-effectively.md) and [lists](0006-make-named-lists-a-first-class-construct.md), we can design how relationships will work.\\\\n\"}, {'role': 'model', 'content': '### URLs\\\\nThe url for an individual relation for a \\'relationships\\' block would be:\\\\n```\\\\n\/<block_id>\/<list_item_id>\/to\/<to_list_item_id>\\\\n\/relationships\/a9hd8j\/to\/skjwo9\\\\n```\\\\nWhere the `list_item_id` and `to_list_item_id` identify which list items the relation is being formed between.\\\\n### Storing relationships\\\\nFor each relationship we need to store both the relationship type and the items that are related. This is represented by storing all relationships as a list within a single answer:\\\\n```\\\\n{\\\\n\"answer_id\": \"relationship\"\\\\n\"value\": [\\\\n{\\\\n\"list_item_id\": \"a9hd8j\",\\\\n\"relationship\": \"Husband or Wife\",\\\\n\"to_list_item_id\": \"kl9s3y\"\\\\n},\\\\n{\\\\n\"list_item_id\": \"a9hd8j\",\\\\n\"relationship\": \"Father or Mother\",\\\\n\"to_list_item_id\": \"skjwo9\"\\\\n}\\\\n]\\\\n}\\\\n```\\\\n### Schema\\\\nThe relationship block will be defined as one block in the schema.\\\\nThe question text for relationships is dynamic and changes dependent the two people the relationship is being asked of:\\\\n```\\\\nThinking of {first_person_name}, how are they related to {second_person_name}\\\\n```\\\\nThe first person name can be resolved by the current placeholder resolution for repeating questions - assume any reference to answer ids when in a repeating section should resolve to the answers for the list item being repeated over:\\\\n```\\\\n{\\\\n\"source\": \"answers\",\\\\n\"identifiers\": [\"first-name\", \"last-name\"],\\\\n}\\\\n```\\\\nThe second person name is more difficult, as we will need to resolve the answers for a list item outside of the current repeat. Adding a `list_item_selector` property to the placeholder source definition provides a way to select a list item. This would be a fixed value used to resolve a list item id:\\\\n```\\\\n{\\\\n\"source\": \"answers\",\\\\n\"list_item_selector\": \"related_list_item_id\"\\\\n\"identifiers\": [\"first-name\", \"last-name\"],\\\\n}\\\\n```\\\\nIn this example runner would need to resolve `related_list_item_id` to the url parameter.\\\\n'}]"}
{"File Name":"elife-xpub\/0005-hindawi-pattern.md","Context":"## Context\\nThe decision to go with a Domain Driven Design approach to architecture is well documented in this [shared document](https:\/\/docs.google.com\/document\/d\/1fN4_0tAMCbPptn1cjauJdDwLMonOeaM7Ha4FK-Vao-k\/) and summarised in this [presentation](https:\/\/docs.google.com\/presentation\/d\/13YlQLMZPCsvCVdKVh6hsOUkS3PCAyuMATwikXVRMjQA) given during the March 2019 PubSweet meet. To summarise the objectives of the re-architecture, we want to make the application:\\n- Easy to extend without making existing code harder to maintain\\n- Easy to re-use with differing workflows without the need to edit large amounts of code\\nDDD allows us to separate our application into bound contexts (ie: submission\/review steps) to make them easier to maintain and more modular. This separation should mean that adding new contexts should not make maintaining existing contexts more challenging and that contexts that do not have overlapping boundaries (ie: **some** submission\/review steps) could be altered or removed entirely without requiring changes to other, non-related contexts.\\nWhile at the March 2019 PubSweet meet, Hindawi demonstrated their open-source project [xpub-reviewer](https:\/\/gitlab.com\/hindawi\/xpub\/xpub-review) which showed their approach to DDD put into practice. As we at eLife were still early on in the planning stage of how to implement a DDD approach to our own application, we decided it would be beneficial to take a similar approach to Hindawi so as to be able to share experience between organisations and possibly feed back into our shared framework [PubSweet](https:\/\/gitlab.coko.foundation\/pubsweet\/pubsweet) making it easier for others to follow a similar design pattern.\\n","Decision":"We initially plan to convert our current code base structure to match that of Hindawi's repository. Once this is complete we will then attempt to fix some of the concerns we have identified with this implementation, making sure to consult with Hindawi to ensure we maintain as similar an approach as possible. Once these details have been addressed we can then try to identify sections of the two applications which are the same\/similar to then recommend inclusion in future releases of PubSweet where they can then be easily adopted and maintained by the wider community.\\n### Overview of suggested architecture pattern\\nThe suggested pattern of architecture would be to place each bound context, both client and server side code it its own self-contained `component` directory, with the naming convention `component-xxxx` for bound contexts and `component-model-xxxx` for any shared data-model components. These would then leverage PubSweet's ability to pull in resolvers to form a full server side system while the client would make use of an `app` directory which contains a React router which pulls together the different client-side components. An example file structure can be found below\\n```\\n\/packages\\n\/app\\n\/component-dashboard\\n\/component-login\\n\/component-submission\\n\/component-model-manuscript\\n\/component-model-file\\n\/component-elife-ui\\n...\\n```\\nEach component behaves like a stand alone module with its own `package.json`, `\/config`, `\/src` and `\/test` directory (some don't have `\/config`).\\nContext components are then structured:\\n```\\n\/component-xxxx\\n\/client\\n\/components\\n\/graphql\\n\/pages\\n\/tests\\n\/config\\n\/server\\n\/src\\n\/use-cases\\nresolvers.js\\ntypeDefs.graphqls\\n\/tests\\nindex.js\\npackage.json\\n```\\nModel components are structured:\\n```\\n\/component-model-xxxx\\n\/src\\n\/entities\\n\/use-cases\\nresolvers.js\\ntypeDefs.graphqls\\n\/congig\\n\/test\\nindex.js\\npackage.json\\n```\\n### Pros\\n- Easy to extend without making existing code harder to maintain\\n- Easy to re-use with differing workflows without the need to edit large amounts of code\\n- Flexible directory structure within components means we can pick and choose whether we include tests, client-side code, etc.\\n- Components should have a single responsibility\\n- Models are injected making testing easier\\n- Better seperation of code makes testing easier\\n### Cons\\n- Some components don't need the same resources across the stack, so some components won't have UI or resolvers\\n- Components can have the need to rely on shared code, which is a sign of domains not having been decoupled from each other. This may lead to code duplication or components depending on each other\\n","tokens":388,"id":3779,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nAs part of our [planning principles](https:\/\/docs.google.com\/document\/d\/1kHaghp-68ooK-NwxozYkScGZThYJVrdOGWf4_K8Wo6s\/edit) we highlighted \"Building in access control\" as a key principle for planning our building our new cloud platform.\\nMaking this work for the new cloud platform means implementing ways that our users &mdash; mainly developers &mdash; can access the various bits of the new infrastructure. This is likely to include access to Kubernetes (CLI and API), AWS (things like S3, RDS), GitHub, and any tooling we put on top of Kubernetes that users will access as part of running their apps (e.g. ELK, [Prometheus](https:\/\/github.com\/ministryofjustice\/cloud-platform\/blob\/main\/architecture-decision-record\/026-Managed-Prometheus.md#choice-of-prometheus), [Concourse](https:\/\/github.com\/ministryofjustice\/cloud-platform\/blob\/main\/architecture-decision-record\/003-Use-Concourse-CI.md)).\\nAt the current time there is no consistent access policy for tooling. We use a mixture of the Google domain, GitHub and AWS accounts to access and manage the various parts of our infrastructure. This makes it hard for users to make sure that they have the correct permissions to do what they need to do, resulting in lots of requests for permissions. It also makes it harder to manage the user lifecycle (adding, removing, updating user permissions) and to track exactly who has access to what.\\nWe are proposing that we aim for a \"single sign on\" approach where users can use a single logon to access different resources. For this we will need a directory where we can store users and their permissions, including what teams they belong to and what roles they have.\\nThe current most complete source of this information for people who will be the first users of the cloud platform is GitHub. So our proposal is to use GitHub as our initial user directory - authentication for the new services that we are building will be through GitHub.\\n\\n## Decision\\nWe will use GitHub as the identify provider for the cloud platform.\\nWe will design and build the new cloud platform with the assumption that users will login to all components using a single GitHub id.\\n\\n\\n## Context and Problem Statement\\nFirefox Accounts is introducing a new visual design for Settings (the interface that exists at the address `https:\/\/accounts.firefox.com\/settings`). The changes involved are significant enough such that it [has been decided](https:\/\/github.com\/mozilla\/fxa\/blob\/main\/docs\/adr\/0011-create-new-react-app-for-settings-redesign.md) that an entirely new React App will be built more or less independent of the existing Settings web app. With this we can beta test and roll out the new design with minimal disruption.\\nAs there are a multitude of ways a React App can be set up and configured, this decision has introduced the question, \"What [toolchain](https:\/\/en.wikipedia.org\/wiki\/Toolchain) should we set up a new React App with?\". This ADR serves to answer that question by going over the various approaches we can take to set up a new React App and how it might integrate with our existing FxA ecosystem.\\nIt\\'s important to note that we are not deciding the languages, libraries, or other tools that we will use within the React development environment, but rather, the system that will be used to develop the React App with.\\nFinally, we are not making decisions about the App\\'s production server here. We\\'ll be using [fxa-content-server](https:\/\/github.com\/mozilla\/fxa\/tree\/main\/packages\/fxa-content-server\/server)\\'s Express server for the production build.\\n## Decision Drivers\\n- **Familiarity** - unless there is an overwhelming reason, we don\\'t really want to introduce a completely foreign build process for everyone to learn.\\n- **Configurability** - if it is less configurable does it meet our needs; if it is more configurable is the time spent worth it?\\n- **Maintainability** - how much work do we set up for ourselves if we\\'re maintaining all upgrades and configuration?\\nAdditionally, our team has a desire to share React components and other files across projects. Both the Payments and Admin Panel web apps use React, and we would benefit from being able to develop components that could be imported into any of these React Apps.\\n\\n## Decision\\n- **Familiarity** - unless there is an overwhelming reason, we don\\'t really want to introduce a completely foreign build process for everyone to learn.\\n- **Configurability** - if it is less configurable does it meet our needs; if it is more configurable is the time spent worth it?\\n- **Maintainability** - how much work do we set up for ourselves if we\\'re maintaining all upgrades and configuration?\\nAdditionally, our team has a desire to share React components and other files across projects. Both the Payments and Admin Panel web apps use React, and we would benefit from being able to develop components that could be imported into any of these React Apps.\\nChosen option: \"B3 - Create React App and use Rescripts\", because:\\n- Our team has sufficient knowledge in working with CRA over the other options.\\n- We don\\'t need to re-invent the wheel. Our needs for the Settings project are not so great that it would be worth spending time creating a ground-up toolchain. To some extent this option allows us to do what we do best without needing to worry about the specifics of the development workflow.\\n- Rescripts allows us to layer additional configuration overtop of the existing configuration without jeopardizing the integrity or trading away any of the powers of CRA. Specifically, we can use Rescripts to modify CRA to allow external React components to be imported, as demonstrated in [this repo](https:\/\/github.com\/jodyheavener\/rescripts-react-app) (technical notes [here](https:\/\/github.com\/jodyheavener\/rescripts-react-app\/blob\/master\/.rescriptsrc.js)). This capability could be implemented in our other React Apps.\\n- If, down the road, we end up in a place where we absolutely need access to the internals we can always eject.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThe decision to go with a Domain Driven Design approach to architecture is well documented in this [shared document](https:\/\/docs.google.com\/document\/d\/1fN4_0tAMCbPptn1cjauJdDwLMonOeaM7Ha4FK-Vao-k\/) and summarised in this [presentation](https:\/\/docs.google.com\/presentation\/d\/13YlQLMZPCsvCVdKVh6hsOUkS3PCAyuMATwikXVRMjQA) given during the March 2019 PubSweet meet. To summarise the objectives of the re-architecture, we want to make the application:\\\\n- Easy to extend without making existing code harder to maintain\\\\n- Easy to re-use with differing workflows without the need to edit large amounts of code\\\\nDDD allows us to separate our application into bound contexts (ie: submission\/review steps) to make them easier to maintain and more modular. This separation should mean that adding new contexts should not make maintaining existing contexts more challenging and that contexts that do not have overlapping boundaries (ie: **some** submission\/review steps) could be altered or removed entirely without requiring changes to other, non-related contexts.\\\\nWhile at the March 2019 PubSweet meet, Hindawi demonstrated their open-source project [xpub-reviewer](https:\/\/gitlab.com\/hindawi\/xpub\/xpub-review) which showed their approach to DDD put into practice. As we at eLife were still early on in the planning stage of how to implement a DDD approach to our own application, we decided it would be beneficial to take a similar approach to Hindawi so as to be able to share experience between organisations and possibly feed back into our shared framework [PubSweet](https:\/\/gitlab.coko.foundation\/pubsweet\/pubsweet) making it easier for others to follow a similar design pattern.\\\\n'}, {'role': 'model', 'content': \"We initially plan to convert our current code base structure to match that of Hindawi's repository. Once this is complete we will then attempt to fix some of the concerns we have identified with this implementation, making sure to consult with Hindawi to ensure we maintain as similar an approach as possible. Once these details have been addressed we can then try to identify sections of the two applications which are the same\/similar to then recommend inclusion in future releases of PubSweet where they can then be easily adopted and maintained by the wider community.\\\\n### Overview of suggested architecture pattern\\\\nThe suggested pattern of architecture would be to place each bound context, both client and server side code it its own self-contained `component` directory, with the naming convention `component-xxxx` for bound contexts and `component-model-xxxx` for any shared data-model components. These would then leverage PubSweet's ability to pull in resolvers to form a full server side system while the client would make use of an `app` directory which contains a React router which pulls together the different client-side components. An example file structure can be found below\\\\n```\\\\n\/packages\\\\n\/app\\\\n\/component-dashboard\\\\n\/component-login\\\\n\/component-submission\\\\n\/component-model-manuscript\\\\n\/component-model-file\\\\n\/component-elife-ui\\\\n...\\\\n```\\\\nEach component behaves like a stand alone module with its own `package.json`, `\/config`, `\/src` and `\/test` directory (some don't have `\/config`).\\\\nContext components are then structured:\\\\n```\\\\n\/component-xxxx\\\\n\/client\\\\n\/components\\\\n\/graphql\\\\n\/pages\\\\n\/tests\\\\n\/config\\\\n\/server\\\\n\/src\\\\n\/use-cases\\\\nresolvers.js\\\\ntypeDefs.graphqls\\\\n\/tests\\\\nindex.js\\\\npackage.json\\\\n```\\\\nModel components are structured:\\\\n```\\\\n\/component-model-xxxx\\\\n\/src\\\\n\/entities\\\\n\/use-cases\\\\nresolvers.js\\\\ntypeDefs.graphqls\\\\n\/congig\\\\n\/test\\\\nindex.js\\\\npackage.json\\\\n```\\\\n### Pros\\\\n- Easy to extend without making existing code harder to maintain\\\\n- Easy to re-use with differing workflows without the need to edit large amounts of code\\\\n- Flexible directory structure within components means we can pick and choose whether we include tests, client-side code, etc.\\\\n- Components should have a single responsibility\\\\n- Models are injected making testing easier\\\\n- Better seperation of code makes testing easier\\\\n### Cons\\\\n- Some components don't need the same resources across the stack, so some components won't have UI or resolvers\\\\n- Components can have the need to rely on shared code, which is a sign of domains not having been decoupled from each other. This may lead to code duplication or components depending on each other\\\\n\"}]"}
{"File Name":"architecture-decision-log\/0010-licensing.md","Context":"* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n","Decision":"* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\nAfter further research, we\u2019ve decided to use [Apache License 2.0](https:\/\/www.apache.org\/licenses\/LICENSE-2.0) based on the following assumptions:\\n* The source code doesn\u2019t need to be public when a distribution of the software is made.\\n* Modifications to the software can be released under any license.\\n* Changes made to the source code must be documented.\\n* It offers the same patent usage protection as GPLv3.\\n* It explicitly prohibits the use of trademarked names found in the project.\\n","tokens":45,"id":4907,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n\\n## Decision\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\nWe've decided to use [Hashicorp's Waypoint](https:\/\/www.waypointproject.io\/) to handle the build\/deployment task for us. It is a pretty new tool (released for public in 2020), but Hashicorp made some of the most used and stable tools in the market (like Terraform and Vault).\\nWaypoint can handle the container build process, deployment, and release. Since we're using a Gitops process, it still lacks some plugins for our deployment\/release steps, but using it only to build even helps us make the deployment process more accessible.\\nWe've tried this tool in our [Design System application](https:\/\/github.com\/budproj\/design-system). It was straightforward to create containers for our storybooks (the front-end of this application).\\nThe next step is to create a Gitops plugin that can deploy the updated image manifest in our Argo Git repository or find a way to make Argo receive information regarding Waypoint deployment and release.\\n\\n\\n* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n\\n## Decision\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\nWe are going to use three main patterns for our development process:\\n* [Forking](https:\/\/docs.github.com\/en\/free-pro-team@latest\/github\/getting-started-with-github\/fork-a-repo)\\n* [Gitflow](https:\/\/www.atlassian.com\/git\/tutorials\/comparing-workflows\/gitflow-workflow)\\n* [Commit](https:\/\/www.conventionalcommits.org\/en\/v1.0.0\/)\\nThe first one relates to **how we host our development code**. Based on this ADR, you should not create branches in the `origin` remote. Instead, create your fork and make your branches there. This decision avoids many unwanted side effects and accidents.\\nYou can create branches at `origin` if those branches are long-lived and relate to a newer version or a massive work from multiple developers. There are some default branches all our repositories might have:\\n* `main` (default branch)\\n* `develop` (next stable release branch)\\n* `test` (working environment for testing purposes)\\nThe second one relates to naming and Git workflow. Our branches naming should follow Gitflow standards, like: `feature\/<sub-path>\/<name>`, and others.\\nAlso, the `develop` branch must remain stable, receiving only new finished features in it.\\nThe third one relates to linting our commits. We should learn how to write useful commit messages, using that convention, to improve our developer experience.\\n\\n\\n\\nProvide a decision given the context below:\\n* [Context](#context)\\\\n* [Decision](#decision)\\\\n* [Status](#status)\\\\n* [Consequences](#consequences)\\\\n* [More reading](#more-reading)\\\\n\"}, {'role': 'model', 'content': '* [Status](#status)\\\\n* [Consequences](#consequences)\\\\n* [More reading](#more-reading)\\\\nAfter further research, we\u2019ve decided to use [Apache License 2.0](https:\/\/www.apache.org\/licenses\/LICENSE-2.0) based on the following assumptions:\\\\n* The source code doesn\u2019t need to be public when a distribution of the software is made.\\\\n* Modifications to the software can be released under any license.\\\\n* Changes made to the source code must be documented.\\\\n* It offers the same patent usage protection as GPLv3.\\\\n* It explicitly prohibits the use of trademarked names found in the project.\\\\n'}]"}
{"File Name":"cli\/0007-integration-vs-command-test.md","Context":"## Context\\nThe integration tests' coverage (e.g.\\n`integration\/v7\/isolated\/create_org_command_test.go`) overlaps with the command\\ntests' (`command\/v7\/create_org_command_test.go`), and this causes confusion\\nfor developers\u2014does a test belong in integration, command, or both? This ADR\\ngives guidelines in order to assist developers making that decision.\\n","Decision":"We want to separate [black\\nbox](https:\/\/en.wikipedia.org\/wiki\/Black-box_testing) and [white\\nbox](https:\/\/en.wikipedia.org\/wiki\/White-box_testing) testing, where\\nblack box testing (functionality of an application) is done at the\\nintegration level, and white box testing (internal structures or\\nworkings of an application) is done at the unit (command) level.\\n#### Integration\\nIntegration tests should continue to test things from a user's perspective:\\n- Build contexts around use cases\\n- Test output based on story parameters and the style guides.\\nFor example, the following behavior (from the `unbind-security-group`\\nstory) should be tested at the integration level because it describes\\noutput\/functionality:\\n> When `cf unbind-security-group` attempts to unbind a running security group\\n> that is not bound to a space, it should return the following:\\n>\\n> ```\\n> Security group my-group not bound to space my-space for lifecycle phase 'running'.\\n> OK\\n>\\n> TIP: If Dynamic ASG's are enabled, changes will automatically apply for running and staging applications. Otherwise, changes will require an app restart (for running) or restage (for staging) to apply to existing applications.\\n> ```\\nTo do so, we would have to make a `BeforeEach()` that creates the scenario\\nlaid out in the story and write expectations on each line of output in\\nthe order they are listed. For example, from\\n`integration\/v7\/global\/unbind_security_group_command_test.go` (edited\\nfor readability):\\n```golang\\nBeforeEach(func() {\\nport := \"8443\"\\ndescription := \"some-description\"\\nsomeSecurityGroup := helpers.NewSecurityGroup(securityGroupName, \"tcp\", \"127.0.0.1\", &port, &description)\\nhelpers.CreateSecurityGroup(someSecurityGroup)\\nhelpers.CreateSpace(spaceName)\\n})\\nWhen(\"the space isn't bound to the security group in any lifecycle\", func() {\\nIt(\"successfully runs the command\", func() {\\nsession := helpers.CF(\"unbind-security-group\", securityGroupName, orgName, spaceName)\\nEventually(session).Should(Say(`Unbinding security group %s from org %s \/ space %s as %s\\.\\.\\.`, securityGroupName, orgName, spaceName, username))\\nEventually(session.Err).Should(Say(`Security group %s not bound to space %s for lifecycle phase 'running'\\.`, securityGroupName, spaceName))\\nEventually(session).Should(Say(\"OK\"))\\nEventually(session).Should(Say(`TIP: If Dynamic ASG's are enabled, changes will automatically apply for running and staging applications. Otherwise, changes will require an app restart \\(for running\\) or restage \\(for staging\\) to apply to existing applications\\.`))\\nEventually(session).Should(Exit(0))\\n})\\n})\\n```\\n#### Command (Unit)\\nIn unit tests we want to break away from this perspective. We\\nwant to organize our tests by the [line of\\nsight](https:\/\/engineering.pivotal.io\/post\/go-flow-tests-like-code\/)\\nmethod, where we cover code paths as we see them.\\n<!-- Brian is really uncomfortable linking to something he wrote -->\\nIn general, every `if err != nil {` should have a corresponding `When(\"actor returns an\\nerror\", func() {`, but in most cases we do not need to test different\\npossible errors that could be returned.\\nHere's an example from `bind-security-group` code and unit test:\\n```golang\\nsecurityGroup, warnings, err := cmd.Actor.GetSecurityGroup(cmd.RequiredArgs.SecurityGroupName)\\ncmd.UI.DisplayWarnings(warnings)\\nif err != nil {\\nreturn err\\n}\\n```\\n```golang\\nIt(\"Retrieves the security group information\", func() {\\nExpect(fakeActor.GetSecurityGroupCallCount).To(Equal(1))\\nsecurityGroupName := fakeActor.GetSecurityGroupArgsForCall(0)\\nExpect(securityGroupName).To(Equal(cmd.RequiredArgs.SecurityGroupName))\\n})\\nIt(\"prints the warnings\", func() {\\nExpect(testUI.Err).To(Say(getSecurityGroupWarning[0]))\\n})\\nWhen(\"an error is encountered getting the provided security group\", func() {\\nvar expectedErr error\\nBeforeEach(func() {\\nexpectedErr = errors.New(\"get security group error\")\\nfakeActor.GetSecurityGroupReturns(\\nresources.SecurityGroup{},\\nv7action.Warnings{\"get security group warning\"},\\nexpectedErr)\\n})\\nIt(\"returns the error and displays all warnings\", func() {\\nExpect(executeErr).To(MatchError(expectedErr))\\n})\\n})\\n```\\n","tokens":81,"id":3643,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context (Discussion)\\nWhen writing custom scripts in the package.json files, one must take into account that those commands\\nshould be compatible with both sh and CMD.exe. So, it is common to use third-party npm packages that\\nact as an API to mimic these tasks in both environments.\\nOne of the most common tasks we do in these scripts is file manipulation (deleting, copying, moving).\\n### Pros and Cons of the Options:\\n#### Difference Between ShellJS and shx\\n**ShellJS:**\\nGood for writing long scripts, all in JS, running via NodeJS (e.g. node myScript.js).\\n**shx:**\\nGood for writing one-off commands in npm package scripts (e.g. \"clean\": \"shx rm -rf out\/\").\\n\\n## Decision\\nIt is proposed to use [shx][shx-url], a wrapper for sh commands that works on Windows too, and behaves as the corresponding sh commands.\\nIt has more than 150K downloads per week, is well-maintaned by the community and it occupies less space than our current past alternatives: rimfaf and copyfiles.\\n\\n\\n## Context\\n[govuk-frontend](https:\/\/github.com\/alphagov\/govuk-frontend) doesn\\'t do a good\\njob of detecting if it\\'s in a browser environment or not, and therefore, whether\\n`document` and friends are available or not. It also performs a number of\\npolyfills that rely on the presence of those browser globals. This means that it\\nthrows exceptions when attempting to import some JavaScript modules from that\\nlibrary.\\nThe way `govuk-frontend` JavaScript is generally written is under the assumption\\nthat you\\'ll want, and be able, to pass the raw DOM nodes to the module\\nconstructors. React doesn\\'t work that way, and we could dig into the prototypes\\nof the objects created by those constructors, but given that these methods are\\nnot intended to be used directly, it would be compliant with\\n[Semantic Versioning](https:\/\/semver.org\/spec\/v2.0.0.html) to completely change\\ntheir implementation without releasing a new major version. This makes relying\\non their behaviour being fixed unreliable.\\n\\n## Decision\\nWe will re-implement JavaScript features from `govuk-frontend` in more idiomatic\\nReact in this library.\\nWe will link to the version and the source of the original implementation\\nalongside our implementation for reference, linking to the code in the specific\\ncommit we were referring to, not `master`.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThe integration tests\\' coverage (e.g.\\\\n`integration\/v7\/isolated\/create_org_command_test.go`) overlaps with the command\\\\ntests\\' (`command\/v7\/create_org_command_test.go`), and this causes confusion\\\\nfor developers\u2014does a test belong in integration, command, or both? This ADR\\\\ngives guidelines in order to assist developers making that decision.\\\\n'}, {'role': 'model', 'content': 'We want to separate [black\\\\nbox](https:\/\/en.wikipedia.org\/wiki\/Black-box_testing) and [white\\\\nbox](https:\/\/en.wikipedia.org\/wiki\/White-box_testing) testing, where\\\\nblack box testing (functionality of an application) is done at the\\\\nintegration level, and white box testing (internal structures or\\\\nworkings of an application) is done at the unit (command) level.\\\\n#### Integration\\\\nIntegration tests should continue to test things from a user\\'s perspective:\\\\n- Build contexts around use cases\\\\n- Test output based on story parameters and the style guides.\\\\nFor example, the following behavior (from the `unbind-security-group`\\\\nstory) should be tested at the integration level because it describes\\\\noutput\/functionality:\\\\n> When `cf unbind-security-group` attempts to unbind a running security group\\\\n> that is not bound to a space, it should return the following:\\\\n>\\\\n> ```\\\\n> Security group my-group not bound to space my-space for lifecycle phase \\'running\\'.\\\\n> OK\\\\n>\\\\n> TIP: If Dynamic ASG\\'s are enabled, changes will automatically apply for running and staging applications. Otherwise, changes will require an app restart (for running) or restage (for staging) to apply to existing applications.\\\\n> ```\\\\nTo do so, we would have to make a `BeforeEach()` that creates the scenario\\\\nlaid out in the story and write expectations on each line of output in\\\\nthe order they are listed. For example, from\\\\n`integration\/v7\/global\/unbind_security_group_command_test.go` (edited\\\\nfor readability):\\\\n```golang\\\\nBeforeEach(func() {\\\\nport := \"8443\"\\\\ndescription := \"some-description\"\\\\nsomeSecurityGroup := helpers.NewSecurityGroup(securityGroupName, \"tcp\", \"127.0.0.1\", &port, &description)\\\\nhelpers.CreateSecurityGroup(someSecurityGroup)\\\\nhelpers.CreateSpace(spaceName)\\\\n})\\\\nWhen(\"the space isn\\'t bound to the security group in any lifecycle\", func() {\\\\nIt(\"successfully runs the command\", func() {\\\\nsession := helpers.CF(\"unbind-security-group\", securityGroupName, orgName, spaceName)\\\\nEventually(session).Should(Say(`Unbinding security group %s from org %s \/ space %s as %s\\\\.\\\\.\\\\.`, securityGroupName, orgName, spaceName, username))\\\\nEventually(session.Err).Should(Say(`Security group %s not bound to space %s for lifecycle phase \\'running\\'\\\\.`, securityGroupName, spaceName))\\\\nEventually(session).Should(Say(\"OK\"))\\\\nEventually(session).Should(Say(`TIP: If Dynamic ASG\\'s are enabled, changes will automatically apply for running and staging applications. Otherwise, changes will require an app restart \\\\(for running\\\\) or restage \\\\(for staging\\\\) to apply to existing applications\\\\.`))\\\\nEventually(session).Should(Exit(0))\\\\n})\\\\n})\\\\n```\\\\n#### Command (Unit)\\\\nIn unit tests we want to break away from this perspective. We\\\\nwant to organize our tests by the [line of\\\\nsight](https:\/\/engineering.pivotal.io\/post\/go-flow-tests-like-code\/)\\\\nmethod, where we cover code paths as we see them.\\\\n<!-- Brian is really uncomfortable linking to something he wrote -->\\\\nIn general, every `if err != nil {` should have a corresponding `When(\"actor returns an\\\\nerror\", func() {`, but in most cases we do not need to test different\\\\npossible errors that could be returned.\\\\nHere\\'s an example from `bind-security-group` code and unit test:\\\\n```golang\\\\nsecurityGroup, warnings, err := cmd.Actor.GetSecurityGroup(cmd.RequiredArgs.SecurityGroupName)\\\\ncmd.UI.DisplayWarnings(warnings)\\\\nif err != nil {\\\\nreturn err\\\\n}\\\\n```\\\\n```golang\\\\nIt(\"Retrieves the security group information\", func() {\\\\nExpect(fakeActor.GetSecurityGroupCallCount).To(Equal(1))\\\\nsecurityGroupName := fakeActor.GetSecurityGroupArgsForCall(0)\\\\nExpect(securityGroupName).To(Equal(cmd.RequiredArgs.SecurityGroupName))\\\\n})\\\\nIt(\"prints the warnings\", func() {\\\\nExpect(testUI.Err).To(Say(getSecurityGroupWarning[0]))\\\\n})\\\\nWhen(\"an error is encountered getting the provided security group\", func() {\\\\nvar expectedErr error\\\\nBeforeEach(func() {\\\\nexpectedErr = errors.New(\"get security group error\")\\\\nfakeActor.GetSecurityGroupReturns(\\\\nresources.SecurityGroup{},\\\\nv7action.Warnings{\"get security group warning\"},\\\\nexpectedErr)\\\\n})\\\\nIt(\"returns the error and displays all warnings\", func() {\\\\nExpect(executeErr).To(MatchError(expectedErr))\\\\n})\\\\n})\\\\n```\\\\n'}]"}
{"File Name":"deeplearning4j\/0007-configuration_objects.md","Context":"## Context\\nSome Ops (esp. convolution) have many parameters. Many of them can have reasonable defaults, but even then creating\\nsignatures for evey reasonable configuration may be impossible, as those signatures would require different naming in\\norder to be actually distinguishable from each other.\\nIn other cases, an op may have a lot of same typed parameters that are required (e.g. GRU, LSTM, SRU) but it is very\\neasy to mix them up.\\nFor both of those cases (many optional parameters, easily mixed up required parameters) it is reasonable to use a\\nconfig holder with builder pattern in languages that do not support named or default parameters.\\nIn our current codebase those configurations are often used across several related ops.\\n","Decision":"We add a `Config(\"name\"){ ... }` section to the namespace context. It supports `Input` and `Arg` definitions in the same\\nway that `Op` does.\\nOps that want to use that config can use `useConfig(conf)`. As configs are often reused across related objects, this\\nwill have the effect of a mixin: All inputs and args defined in that config will also be automatically defined on that\\nOp. If there is a naming conflict, an exception will be thrown at construction time.\\nFor default signatures, configs will be passed at the end, in the order that they were added to the Op.\\nIf other signatures are desired, configs, like regular inputs and args, can be passed to `Signature`.\\nIn languages that do not support default or named parameters, a config holder will be created, that will take the\\nparameters of the config using a builder pattern. For languages with default and named parameters, no additional config\\nholder will be created, and the parameters of the config will be treated as if they were directly configured on the Op.\\n### Example\\nThis example shows a very simple case in order to highlight how this feature would be used.\\n```kotlin\\nfun RNN() = Namespace(\"RNN\"){\\nval sruWeights = Config(\"SRUWeights\"){\\nInput(FLOATING_POINT, \"weights\"){ description = \"Weights, with shape [inSize, 3*inSize]\" }\\nInput(FLOATING_POINT, \"bias\"){ description = \"Biases, with shape [2*inSize]\" }\\n}\\nOp(\"SRU\"){\\nInput(FLOATING_POINT, \"x\"){ description = \"...\" }\\nInput(FLOATING_POINT, \"initialC\"){ description = \"...\" }\\nInput(FLOATING_POINT, \"mask\"){ description = \"...\" }\\nuseConfig(sruWeights)\\nOutput(FLOATING_POINT, \"out\"){ description = \"...\" }\\n}\\nOp(\"SRUCell\"){\\nval x = Input(FLOATING_POINT, \"x\"){ description = \"...\" }\\nval cLast = Input(FLOATING_POINT, \"cLast\"){ description = \"...\" }\\nval conf = useConfig(sruWeights)\\nOutput(FLOATING_POINT, \"out\"){ description = \"...\" }\\n\/\/ Just for demonstration purposes\\nSignature(x, cLast, conf)\\n}\\n}\\n```\\n","tokens":156,"id":2936,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nSome Ops (esp. convolution) have many parameters. Many of them can have reasonable defaults, but even then creating\\nsignatures for evey reasonable configuration may be impossible, as those signatures would require different naming in\\norder to be actually distinguishable from each other.\\nIn other cases, an op may have a lot of same typed parameters that are required (e.g. GRU, LSTM, SRU) but it is very\\neasy to mix them up.\\nFor both of those cases (many optional parameters, easily mixed up required parameters) it is reasonable to use a\\nconfig holder with builder pattern in languages that do not support named or default parameters.\\nIn our current codebase those configurations are often used across several related ops.\\n\\n## Decision\\nWe add a `Config(\"name\"){ ... }` section to the namespace context. It supports `Input` and `Arg` definitions in the same\\nway that `Op` does.\\nOps that want to use that config can use `useConfig(conf)`. As configs are often reused across related objects, this\\nwill have the effect of a mixin: All inputs and args defined in that config will also be automatically defined on that\\nOp. If there is a naming conflict, an exception will be thrown at construction time.\\nFor default signatures, configs will be passed at the end, in the order that they were added to the Op.\\nIf other signatures are desired, configs, like regular inputs and args, can be passed to `Signature`.\\nIn languages that do not support default or named parameters, a config holder will be created, that will take the\\nparameters of the config using a builder pattern. For languages with default and named parameters, no additional config\\nholder will be created, and the parameters of the config will be treated as if they were directly configured on the Op.\\n### Example\\nThis example shows a very simple case in order to highlight how this feature would be used.\\n```kotlin\\nfun RNN() = Namespace(\"RNN\"){\\nval sruWeights = Config(\"SRUWeights\"){\\nInput(FLOATING_POINT, \"weights\"){ description = \"Weights, with shape [inSize, 3*inSize]\" }\\nInput(FLOATING_POINT, \"bias\"){ description = \"Biases, with shape [2*inSize]\" }\\n}\\nOp(\"SRU\"){\\nInput(FLOATING_POINT, \"x\"){ description = \"...\" }\\nInput(FLOATING_POINT, \"initialC\"){ description = \"...\" }\\nInput(FLOATING_POINT, \"mask\"){ description = \"...\" }\\nuseConfig(sruWeights)\\nOutput(FLOATING_POINT, \"out\"){ description = \"...\" }\\n}\\nOp(\"SRUCell\"){\\nval x = Input(FLOATING_POINT, \"x\"){ description = \"...\" }\\nval cLast = Input(FLOATING_POINT, \"cLast\"){ description = \"...\" }\\nval conf = useConfig(sruWeights)\\nOutput(FLOATING_POINT, \"out\"){ description = \"...\" }\\n\/\/ Just for demonstration purposes\\nSignature(x, cLast, conf)\\n}\\n}\\n```\\n\\n\\n## Context\\nThe OpenAPI generator has to generate names for APIs, their operations and schemas.\\nWe initially used the file name to generate an API name, but found that for bigger APIs this is would result in very complex APIs, therefore we need a more sensible default.\\nIn addition API owners whish to be able to customize names of APIs and operations.\\nTo follow naming conventions in JavaScript\/TypeScript we planned on formatting API names as pascal case, operation names as camel case and property and parameter names as camel case.\\nFor properties and parameters we would have to resolve the original names to use them in the requests.\\nWe found that for most cases this is straight forward, but there are corner cases that require a different\/more complex architecture.\\n### Issues with properties and parameters\\nGiven the following two types and their original properties, if those were used in combination, e. g. `allOf`, there is no possibility to set both values.\\n```ts\\ntype A {\\nsomeProperty: string; \/\/ original name: \\'some_property`\\n}\\ntype A {\\nsomeProperty: string; \/\/ original name: \\'some-property`\\n}\\n```\\nThis could be solved by making names unique throughout the whole service.\\nHowever this again would lead to less convenient names, e.g. `name`, `name1`, `name2`.\\n\\n## Decision\\n### API naming\\nAPIs are named based on the first tag, if there are any.\\nIf there is no tag, a \"default\" tag is used.\\nTo allow customizing of APIs, we introduced the `x-sap-cloud-sdk-api-name` extension, which takes precedence over the default name generation.\\nThis extension can be set on on multiple levels:\\n- operation, causing this operation to be treated as part of the given API.\\n- path, causing all operations under this path to be treated as part of the given API.\\n- document root, causing all operations in the document to be treated as part of the given API.\\nAll names are transformed to pascal case (`SomeApi`).\\nAPI names are formed by appending an \"Api\" suffix.\\nIf the original name ends with an \"api\" suffix (case independent), the original suffix is removed prior to appending \"Api\".\\nExamples:\\n- `my-api` => `MyApi`\\n- `my` => `MyApi`\\n- `my-api-api` => `MyApiApi`\\n### Operation naming\\nOperations are named based on their given `operationId`.\\nIf no `operationId` is given the name is parsed from the method and the path under which the operation occurred.\\nExample:\\n- path: `entity\/{id}`\\n- method: `get`\\n- resulting name: `getEntityById`\\nUsers can set specific operation names using the `x-sap-cloud-sdk-operation-name` extension.\\nThis extension can be set on the operation only.\\nNames will be transformed to camel case, duplicate names will cause generation to fail.\\n### Property and query parameter naming\\nKeep the names as provided per the specification.\\nNo camel case renaming is done to avoid the original name corner cases discussed above.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nSome Ops (esp. convolution) have many parameters. Many of them can have reasonable defaults, but even then creating\\\\nsignatures for evey reasonable configuration may be impossible, as those signatures would require different naming in\\\\norder to be actually distinguishable from each other.\\\\nIn other cases, an op may have a lot of same typed parameters that are required (e.g. GRU, LSTM, SRU) but it is very\\\\neasy to mix them up.\\\\nFor both of those cases (many optional parameters, easily mixed up required parameters) it is reasonable to use a\\\\nconfig holder with builder pattern in languages that do not support named or default parameters.\\\\nIn our current codebase those configurations are often used across several related ops.\\\\n'}, {'role': 'model', 'content': 'We add a `Config(\"name\"){ ... }` section to the namespace context. It supports `Input` and `Arg` definitions in the same\\\\nway that `Op` does.\\\\nOps that want to use that config can use `useConfig(conf)`. As configs are often reused across related objects, this\\\\nwill have the effect of a mixin: All inputs and args defined in that config will also be automatically defined on that\\\\nOp. If there is a naming conflict, an exception will be thrown at construction time.\\\\nFor default signatures, configs will be passed at the end, in the order that they were added to the Op.\\\\nIf other signatures are desired, configs, like regular inputs and args, can be passed to `Signature`.\\\\nIn languages that do not support default or named parameters, a config holder will be created, that will take the\\\\nparameters of the config using a builder pattern. For languages with default and named parameters, no additional config\\\\nholder will be created, and the parameters of the config will be treated as if they were directly configured on the Op.\\\\n### Example\\\\nThis example shows a very simple case in order to highlight how this feature would be used.\\\\n```kotlin\\\\nfun RNN() = Namespace(\"RNN\"){\\\\nval sruWeights = Config(\"SRUWeights\"){\\\\nInput(FLOATING_POINT, \"weights\"){ description = \"Weights, with shape [inSize, 3*inSize]\" }\\\\nInput(FLOATING_POINT, \"bias\"){ description = \"Biases, with shape [2*inSize]\" }\\\\n}\\\\nOp(\"SRU\"){\\\\nInput(FLOATING_POINT, \"x\"){ description = \"...\" }\\\\nInput(FLOATING_POINT, \"initialC\"){ description = \"...\" }\\\\nInput(FLOATING_POINT, \"mask\"){ description = \"...\" }\\\\nuseConfig(sruWeights)\\\\nOutput(FLOATING_POINT, \"out\"){ description = \"...\" }\\\\n}\\\\nOp(\"SRUCell\"){\\\\nval x = Input(FLOATING_POINT, \"x\"){ description = \"...\" }\\\\nval cLast = Input(FLOATING_POINT, \"cLast\"){ description = \"...\" }\\\\nval conf = useConfig(sruWeights)\\\\nOutput(FLOATING_POINT, \"out\"){ description = \"...\" }\\\\n\/\/ Just for demonstration purposes\\\\nSignature(x, cLast, conf)\\\\n}\\\\n}\\\\n```\\\\n'}]"}
{"File Name":"react-template\/0006-use-snapshot-testing.md","Context":"## Context\\nWe want to be confident that any changes to how our React components display are\\nintentional. Jest has built in support for snapshot testing.\\n","Decision":"We will use snapshot testing as part of testing components.\\n","tokens":33,"id":3326,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe need tooling to ensure that our Javascript is in consistent code style and for us to be notified when\\nit is not.\\n\\n## Decision\\nWe decided as a team that option 2 would be the easiest and quickest option to implement. It also follows the ways that GDS work.\\nIf we found it too restrictive at a later date we could then switch to option 3 and customise the rules if need be.\\n\\n\\n## Context\\nWe want to ensure we're creating a secure application from the start, by adding\\nsome vulnerability checking to our CI we can have confidence that we aren't\\nintroducing known insecure code.\\n\\n## Decision\\nWe will run the Brakeman vulnerability check as part of our CI pipeline.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe want to be confident that any changes to how our React components display are\\\\nintentional. Jest has built in support for snapshot testing.\\\\n\"}, {'role': 'model', 'content': 'We will use snapshot testing as part of testing components.\\\\n'}]"}
{"File Name":"portfolio\/0002-use-nextjs.md","Context":"## Context\\nThere's a few large architectural decisions that were made before I started using ADRs, so I'm backfilling them a little.\\n[NextJS](https:\/\/nextjs.org\/) is a fantastic framework for building scalable, enterprise-grade applications with React.\\nNextJS has a few notable contrasts to [Gatsby](https:\/\/www.gatsbyjs.com\/) (which this blog was previously built on) which factored into the decision to use it here. Some of these are captured below, some of my thoughts can be found in [this article by Jared Palmer](https:\/\/jaredpalmer.com\/gatsby-vs-nextjs).\\nHere's some loose bullet points behind my reasoning behind using NextJS for this site.\\n- NextJS comes baked in with routing, webpack configurations, TypeScript support, and a whole host of other things. Less fiddling with tools, more writing meaningful code.\\n- In general, Gatsby is more \"plugin-oriented\", NextJS is more \"recipe-oriented\". While plugins are nice to get a ton of functionality, each plugin introduces another dependency that you have to maintain, upgrade. Each new dependency introduces another point of failure.\\n- NextJS doesn't have a GraphQL layer baked into it. For a website of this size I felt like being required to use the GraphQL layer to access markdown files was overkill.\\n- NextJS allows you to build \"hybrid\" applications\u2014ones where there's a combination of server-rendered (SSR) pages, static-rendered pages, and prerendered pages. There's not likely to be any SSR pages in this website, but NextJS sets you up for flexibility as time goes on in regards to rendering strategies.\\n- NextJS is a super in-demand, \"hot\" skill in the React community, so having my website built on top of it both keeps me fresh on the latest patterns and showcases my abilities.\\n- I like working with NextJS! I like the framework and how they've chosen the APIs!\\n","Decision":"I will use [NextJS](https:\/\/nextjs.org\/) as the framework on top of React to build this website\\n","tokens":413,"id":2086,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n### Context\\nThe app needs tests. This is a must. Although test driven development is best, the app added tests after development began to test logic and encourage future changes do not break functionality. It would be best to test all aspects of the MVVM architecture, but view models are where the majority of the app logic lives.\\n### Decision\\nSeveral testing frameworks exist for .NET apps, with the main three being xUnit, NUnit and MSTest. The app uses xUnit to test view models, but any of the other two frameworks would\\'ve also been suitable. I chose xUnit because the testing syntax looked cleaner, the framework was a bit newer, and I found some nice tutorials that recommended xUnit above the other two. Perhaps most importantly, I wanted to get started with testing instead of spending too much time debating the merits of which one to chose.\\nPerhaps just as important as which testing framework to use is which mocking package to use when faking API or other function calls in my tests. For that, Moq seems to be the clear winner. It\\'s easy to find documentation online about using Moq with each testing framework.\\n### Status\\nAccepted\\n### Consequences\\nGetting started with testing is an undertaking, and it is very easy to let tests fall by the wayside when the touch table has a deadline and new features need to be added quickly. However, many xUnit tutorials seem incomplete and only test the view models. I\\'m curious if this is a sufficient amount of test coverage.\\n_In Retrospect:_ Unfortunately, I added tests later in the development process, and that caused some unnecessary delay while I was restructuring the architecture. This could have been avoided if I was a bit more familiar with how [Moq](https:\/\/www.nuget.org\/packages\/moq\/) worked from the outset of building the app. I also had to explore using [Unity](https:\/\/www.nuget.org\/packages\/Unity\/) to make sure dependency injection was in place for my unit tests. For more information in using Unity, refer to the `ContainerHelper` class in the `Lib` folder of the app.\\n\\n## Decision\\nSeveral testing frameworks exist for .NET apps, with the main three being xUnit, NUnit and MSTest. The app uses xUnit to test view models, but any of the other two frameworks would\\'ve also been suitable. I chose xUnit because the testing syntax looked cleaner, the framework was a bit newer, and I found some nice tutorials that recommended xUnit above the other two. Perhaps most importantly, I wanted to get started with testing instead of spending too much time debating the merits of which one to chose.\\nPerhaps just as important as which testing framework to use is which mocking package to use when faking API or other function calls in my tests. For that, Moq seems to be the clear winner. It\\'s easy to find documentation online about using Moq with each testing framework.\\n### Status\\nAccepted\\n### Consequences\\nGetting started with testing is an undertaking, and it is very easy to let tests fall by the wayside when the touch table has a deadline and new features need to be added quickly. However, many xUnit tutorials seem incomplete and only test the view models. I\\'m curious if this is a sufficient amount of test coverage.\\n_In Retrospect:_ Unfortunately, I added tests later in the development process, and that caused some unnecessary delay while I was restructuring the architecture. This could have been avoided if I was a bit more familiar with how [Moq](https:\/\/www.nuget.org\/packages\/moq\/) worked from the outset of building the app. I also had to explore using [Unity](https:\/\/www.nuget.org\/packages\/Unity\/) to make sure dependency injection was in place for my unit tests. For more information in using Unity, refer to the `ContainerHelper` class in the `Lib` folder of the app.\\n\\n\\n## Context\\nAt the Zooniverse Team Meeting April 2018, it was decided that Panoptes Front End would be broken up into modular libraries and client-side apps. To aid in this rewrite, it was also decided that we would try out server-side rendering (SSR) since now there are several frameworks for React that make this easy. SSR has several benefits including improved load times and rendered HTML that search providers could crawl to index.\\nIt was initially chosen to use a SSR framework called [Next.js](https:\/\/github.com\/zeit\/next.js\/). Next.js is an opinionated framework that supports out of the box SSR, routing, production build compiling. In particular its API provides a method, `getInitialProps` to easily hydrate client-side app state. However, in the brief experience we have had so far:\\n- It\\'s been difficult to configure. We had to configure it to use [Grommet 2](https:\/\/v2.grommet.io\/) and [styled-components](https:\/\/www.styled-components.com\/) as these are not defaults.\\n- It\\'s been difficult with major version upgrades. Version 6 was released right when we started and the upgrade path took a lot of time.\\n- It\\'s had issues with [Mocha](https:\/\/mochajs.org\/). This is concerning since Mocha is the most popular test framework for javascript, and our current default.\\nThere is an [open issue](https:\/\/github.com\/zeit\/next.js\/issues\/1632) confirming that Next.js will be integrating React Router v4 as its routing solution. React Router v4 is not a router that we wish to use due to its significant API changes.\\nIn addition, Sarah had been recently advised by fellow developers in the Chicago Javascript community that Next.js is difficult to configure which can outweigh any benefits it might provide.\\nRoger and Sarah built several prototypes with other libraries, including [react-server](https:\/\/react-server.io\/), [razzle](https:\/\/github.com\/jaredpalmer\/razzle), and [react-universally](https:\/\/github.com\/ctrlplusb\/react-universally). However, none of these provided the same ease-of-use as Next.js. Additionally, react-server doesn\\'t appear to be actively maintained.\\n\\n## Decision\\nFor the time being, we will continue to use NextJS. We\\'re currently using v5; v6 is available, and likely to be the last version that is router-agnostic, so we can revisit this decision then.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThere\\'s a few large architectural decisions that were made before I started using ADRs, so I\\'m backfilling them a little.\\\\n[NextJS](https:\/\/nextjs.org\/) is a fantastic framework for building scalable, enterprise-grade applications with React.\\\\nNextJS has a few notable contrasts to [Gatsby](https:\/\/www.gatsbyjs.com\/) (which this blog was previously built on) which factored into the decision to use it here. Some of these are captured below, some of my thoughts can be found in [this article by Jared Palmer](https:\/\/jaredpalmer.com\/gatsby-vs-nextjs).\\\\nHere\\'s some loose bullet points behind my reasoning behind using NextJS for this site.\\\\n- NextJS comes baked in with routing, webpack configurations, TypeScript support, and a whole host of other things. Less fiddling with tools, more writing meaningful code.\\\\n- In general, Gatsby is more \"plugin-oriented\", NextJS is more \"recipe-oriented\". While plugins are nice to get a ton of functionality, each plugin introduces another dependency that you have to maintain, upgrade. Each new dependency introduces another point of failure.\\\\n- NextJS doesn\\'t have a GraphQL layer baked into it. For a website of this size I felt like being required to use the GraphQL layer to access markdown files was overkill.\\\\n- NextJS allows you to build \"hybrid\" applications\u2014ones where there\\'s a combination of server-rendered (SSR) pages, static-rendered pages, and prerendered pages. There\\'s not likely to be any SSR pages in this website, but NextJS sets you up for flexibility as time goes on in regards to rendering strategies.\\\\n- NextJS is a super in-demand, \"hot\" skill in the React community, so having my website built on top of it both keeps me fresh on the latest patterns and showcases my abilities.\\\\n- I like working with NextJS! I like the framework and how they\\'ve chosen the APIs!\\\\n'}, {'role': 'model', 'content': 'I will use [NextJS](https:\/\/nextjs.org\/) as the framework on top of React to build this website\\\\n'}]"}
{"File Name":"libelektra\/global_validation.md","Context":"## Problem\\nLinks and validation constraints might point to keys not loaded\\nwith the respective `kdbGet`.\\n","Decision":"Not supported, admins\/maintainers need to stay with their spec within what applications request by `kdbGet`.\\n- [Internal Cache](internal_cache.md)\\n","tokens":24,"id":1315,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Problem\\n- KeySet might get modified on access (hash rebuilds)\\n- Expectation that already all keys are there after `kdbGet()`\\n- No default value calculation\\n\\n## Decision\\n- spec-plugin does a lookup for values (Maybe also resolving missing fallback\/override links?)\\n\\n\\n## Context and Problem Statement\\nLack of a certain method in mapper use causes implementation confusion.\\n\\n## Decision\\nIn the context of implementation mapper classes facing concern of methods of application we decided to use template and neglected using interface, to achieve project integrity and consistency.\\n\\n\\n\\nProvide a decision given the context below:\\n## Problem\\\\nLinks and validation constraints might point to keys not loaded\\\\nwith the respective `kdbGet`.\\\\n'}, {'role': 'model', 'content': 'Not supported, admins\/maintainers need to stay with their spec within what applications request by `kdbGet`.\\\\n- [Internal Cache](internal_cache.md)\\\\n'}]"}
{"File Name":"govuk-aws\/0027-move-publicapi-away-from-frontend-lb.md","Context":"## Context\\nTraffic flows for frontend applications in the following way:\\n![Frontend traffic flow](.\/0027-move-publicapi-away-from-frontend-lb-img01.jpg?raw=true \"Frontend traffic flow\")\\nMost frontend applications can go straight from the cache instances to the application instances, bypassing frontend-lb. They do this by creating a DNS entry for each application service ELB, and setting the appropriate upstream DNS entry for the nginx configuration on the cache machines.\\nThe exception to this rule is the \"publicapi\" application ([reference](https:\/\/github.com\/alphagov\/govuk-puppet\/blob\/34c587b76abee63d0c84b9f67212ac2f1c65ba2e\/modules\/govuk\/manifests\/apps\/publicapi.pp)).\\nThis is a set of nginx configuration that sits on frontend-lb, and forwards specific paths to applications upstream.\\nRouter on the cache instances forwards all traffic bound for `\/api` to frontend-lb, and then publicapi handles the forwarding from there.\\nThe configuration for the paths is [relatively simple](https:\/\/github.com\/alphagov\/govuk-puppet\/blob\/34c587b76abee63d0c84b9f67212ac2f1c65ba2e\/modules\/govuk\/templates\/publicapi_nginx_extra_config.erb).\\n","Decision":"Move publicapi application to either the cache instances themselves, or replace the configuration with an Application Load Balancer.\\n","tokens":283,"id":4031,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThis is related to story [#158847588](https:\/\/www.pivotaltracker.com\/story\/show\/158847588)\\nPrior to the story above, when route-integrity was turned on (soon to be the\\ndefault) we did not prune routes that received most [retriable]() errors. The\\ncode ensures that there are only two types of errors that [result in a\\nprune](https:\/\/github.com\/cloudfoundry\/gorouter\/blob\/b63e6fb16c2a422ec5108a19debc9adb81f2d1dd\/route\/pool.go#L369-L372):\\n[Hostname Mismatch and Attempted TLS with Non TLS\\nBackend](https:\/\/github.com\/cloudfoundry\/gorouter\/blob\/b63e6fb16c2a422ec5108a19debc9adb81f2d1dd\/proxy\/fails\/classifier_group.go#L17-L20)\\nThe prune operation should have little user impact - the route will get added\\nagain the next time the route-registrar runs if the application is still\\nrunning.\\n\\n## Decision\\nWe will prune any TLS route that has had a failure immediately. Consequently,\\nwe are immediately pruning on more errors, such that the final list includes the\\nfollowing errors: AttemptedTLSWithNonTLSBackend, Dial, RemoteFailedCertCheck,\\nRemoteHandshakeFailure, HostnameMismatch, UntrustedCert\\nWe will also add logging to the cases when an endpoint is pruned.\\n\\n\\n## Context\\nCurrently, we have a main class called [KonduitServingMain](https:\/\/github.com\/KonduitAI\/konduit-serving\/blob\/45af79d15abe4912ccd81e78c9d215306472036e\/konduit-serving-core\/src\/main\/java\/ai\/konduit\/serving\/configprovider\/KonduitServingMain.java) that is the entrypoint for a konduit-serving application to run. The main command line arguments are defined inside [KonduitServingNodeConfigurer](https:\/\/github.com\/KonduitAI\/konduit-serving\/blob\/e791741b80721980f8b66a35ed42f20b30612d5c\/konduit-serving-core\/src\/main\/java\/ai\/konduit\/serving\/configprovider\/KonduitServingNodeConfigurer.java) class. We also have an additional [Python CLI](https:\/\/github.com\/KonduitAI\/konduit-serving\/blob\/7965965b58217f2b4d983fd41aaea013264491ee\/python\/cli.py) that can be just implemented in Java. Vert.x Launcher supports the ability to start, stop and list running verticles out of the box.\\n\\n## Decision\\n- Extend `KonduitServingNodeConfigurer` from [Vert.x Launcher](https:\/\/vertx.io\/docs\/vertx-core\/java\/#_the_vert_x_launcher) class.\\n- Write all the initialization\/tear-down logic inside the lifecycle methods of Vert.x Launcher which is present inside both of the above classes (KonduitServingMain and KonduitServingNodeConfigurer).\\n- Extend the CLI and write separate classes for every command we want to add to the CLI (see the documentation [here](https:\/\/vertx.io\/docs\/vertx-core\/java\/#_extending_the_vert_x_launcher)).\\n- Depending on the final decided API, register or unregister commands in Vert.x Launcher.\\n### Example CLI:\\nThe following CLI will be the start for how this will be look:\\n#### Starting Verticles\\nEvery verticle will be named some sensible value instead of the full classpath so that the verticles can be started with a simple name. For example:\\nkonduit serve inference-server --config \\'{}\\' # With a JSON string\\nkonduit serve inference-server --config config.json # With a JSON file\\nkonduit serve inference-server --config config.json --name=mnist-prediction-server # With a server name. If no names are given then a random name\\n# is generated.\\nThe CLI can also have options for setting up deployment options like:\\nkonduit serve inference-server --config config.json --instances 3 # For running 3 instances of a Verticle. Usually it runs on a single port with\\n# round robin fashion requests transfer for load balancing\\n#### Stopping Verticles\\nkonduit stop inference --name=mnist-prediction-server # stops the server by the name of \"mnist-prediction-server\"\\n#### Listing Verticles\\nkonduit list # Lists all of the running verticle services with their names, host, port, configuration\\n#### Inspecting Verticles\\nPossible details will include:\\n- configuration\\n- host and port\\n- current resource usage\\n```bash\\nkonduit inspect # Give details of all the running verticles\\nkonduit inspect --name=mnist-prediction-server # Details of a specific verticle\\n```\\n#### Running Predictions\\n#### Predict with JSON (application\/json)\\nkonduit predict --name=mnist-prediction-server --input \\'{\"key\":\"value\"}\\' # With JSON string\\nkonduit predict --name=mnist-prediction-server --input input.json # With JSON file\\n#### Predict with Files (multipart\/form-data)\\nkonduit predict --name=mnist-prediction-server --input file.npy # Numpy\\nkonduit predict --name=mnist-prediction-server --input file.zip # DL4J\\nkonduit predict --name=mnist-prediction-server --input image.jspg # Image\\nkonduit predict --name=mnist-prediction-server --input file1.zip,file2.zip # DL4J with multiple inputs\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nTraffic flows for frontend applications in the following way:\\\\n![Frontend traffic flow](.\/0027-move-publicapi-away-from-frontend-lb-img01.jpg?raw=true \"Frontend traffic flow\")\\\\nMost frontend applications can go straight from the cache instances to the application instances, bypassing frontend-lb. They do this by creating a DNS entry for each application service ELB, and setting the appropriate upstream DNS entry for the nginx configuration on the cache machines.\\\\nThe exception to this rule is the \"publicapi\" application ([reference](https:\/\/github.com\/alphagov\/govuk-puppet\/blob\/34c587b76abee63d0c84b9f67212ac2f1c65ba2e\/modules\/govuk\/manifests\/apps\/publicapi.pp)).\\\\nThis is a set of nginx configuration that sits on frontend-lb, and forwards specific paths to applications upstream.\\\\nRouter on the cache instances forwards all traffic bound for `\/api` to frontend-lb, and then publicapi handles the forwarding from there.\\\\nThe configuration for the paths is [relatively simple](https:\/\/github.com\/alphagov\/govuk-puppet\/blob\/34c587b76abee63d0c84b9f67212ac2f1c65ba2e\/modules\/govuk\/templates\/publicapi_nginx_extra_config.erb).\\\\n'}, {'role': 'model', 'content': 'Move publicapi application to either the cache instances themselves, or replace the configuration with an Application Load Balancer.\\\\n'}]"}
{"File Name":"buildit-all\/0003-use-aws-codepipeline-and-codebuild-instead-of-travis.md","Context":"## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted\/PaaS CI\/CD solution\\n","Decision":"* Use AWS CodePipeline and CodeBuild instead of Travis\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n","tokens":36,"id":4425,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted\/PaaS CI\/CD solution\\n\\n## Decision\\n* Use AWS CodePipeline and CodeBuild instead of Travis\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n\\n\\n## Context\\nThe NOC use case requires access to a large (~1TB) NetCDF dataset currently stored on the\\nARCHER system. The current usage requires data to be extracted using shell scripts and\\nthis process takes a long time. We need to identify a better way to access this\\ndataset to allow the Datalabs environment to make best use of it.\\n\\n## Decision\\nWe have decided to use a [Thredds](http:\/\/www.unidata.ucar.edu\/software\/thredds\/current\/tds\/)\\nserver to present a unified view of the dataset as it should provide significant\\nperformance improvements over manual scripting.\\nIn order to achieve this, we need the data to be moved to a JASMIN Group Workspace (GWS)\\nto allow us to provision a Thredds server in the JASMIN managed cloud.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted\/PaaS CI\/CD solution\\\\n'}, {'role': 'model', 'content': '* Use AWS CodePipeline and CodeBuild instead of Travis\\\\n* We will aim to create a new Pipeline\/Build and potentially execution environment per branch.\\\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\\\n'}]"}
{"File Name":"play-frontend-hmrc\/0002-maintain-parity-with-govuk-frontend.md","Context":"## Context and Problem Statement\\n`play-frontend-govuk` and `play-frontend-hmrc` provide Scala \/ Play \/ Twirl implementations of the components provided\\nas Nunjucks in `govuk-frontend` and `hmrc-frontend`, using the assets provided by those libraries. How much should the\\nplay-frontend implementations diverge from their \u201cbase\u201d repositories?\\n## Decision Drivers\\n* Need to create frontend components that are easy to use by Scala developers on the platform\\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\\n`hmrc-frontend` are released\\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\\n","Decision":"* Need to create frontend components that are easy to use by Scala developers on the platform\\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\\n`hmrc-frontend` are released\\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\\nChosen option: Option 3, because it allows for quick and continuous upgrading to follow GDS and HMRC design system\\nchanges, allows for a robust testing strategy of multiple implementations of the templates thanks to YAML provided by\\nGDS.\\n### Positive Consequences\\n* Design of case classes to follow GDS \/ HMRC design system means PlatUI as library maintainers do not have to create\\nviewmodel structure from scratch every time\\n* Adding new components can follow a clear and straightforward path\\n* Robust test strategy can be developed using Nunjucks components and Twirl templates using a parser\\n### Negative Consequences\\n* Feedback suggests that some developers do not find the API to be intuitive\\n* Separate decisions need to be made on handling multilingual support\\n* Enrichment of library needs to be done via separate Twirl helpers,\\nsee [related ADR](..\/adr\/0001-play-frontend-hmrc-mirrors-hmrc-frontend.md)\\n","tokens":156,"id":564,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\n`play-frontend-govuk` and `play-frontend-hmrc` provide Scala \/ Play \/ Twirl implementations of the components provided\\nas Nunjucks in `govuk-frontend` and `hmrc-frontend`, using the assets provided by those libraries. How much should the\\nplay-frontend implementations diverge from their \u201cbase\u201d repositories?\\n## Decision Drivers\\n* Need to create frontend components that are easy to use by Scala developers on the platform\\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\\n`hmrc-frontend` are released\\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\\n\\n## Decision\\n* Need to create frontend components that are easy to use by Scala developers on the platform\\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\\n`hmrc-frontend` are released\\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\\nChosen option: Option 3, because it allows for quick and continuous upgrading to follow GDS and HMRC design system\\nchanges, allows for a robust testing strategy of multiple implementations of the templates thanks to YAML provided by\\nGDS.\\n### Positive Consequences\\n* Design of case classes to follow GDS \/ HMRC design system means PlatUI as library maintainers do not have to create\\nviewmodel structure from scratch every time\\n* Adding new components can follow a clear and straightforward path\\n* Robust test strategy can be developed using Nunjucks components and Twirl templates using a parser\\n### Negative Consequences\\n* Feedback suggests that some developers do not find the API to be intuitive\\n* Separate decisions need to be made on handling multilingual support\\n* Enrichment of library needs to be done via separate Twirl helpers\\n(see https:\/\/github.com\/hmrc\/play-frontend-hmrc\/blob\/main\/docs\/maintainers\/adr\/0001-play-frontend-hmrc-mirrors-hmrc-frontend.md)\\n\\n\\n## Context and Problem Statement\\n- `play-fronted-govuk` library is not Play Framework aware\\n- Adding helper methods to the library to utilise Play Framework features will reduce repetition in service code\\n## Decision Drivers\\n- Reducing repetition and boilerplate is not only good practice generally, but will important in helping service teams\\nmigrate off older frontend libraries to `play-frontend-govuk` and `play-frontend-hmrc`\\n- `play-frontend-govuk` is intended to be a direct port of `govuk-frontend`, and so helpers should live in an\\nappropriate helper class alongside the view models\\n- We would prefer to implicit classes rather than directly modifying the viewmodel case classes, as the viewmodel classes are derived from the govuk-frontend API\\n- We want to avoid replicatign the `govuk-frontend` parameter lists in the helpers, to keep the overhead of upgrading the library low\\n\\n## Decision\\n- Reducing repetition and boilerplate is not only good practice generally, but will important in helping service teams\\nmigrate off older frontend libraries to `play-frontend-govuk` and `play-frontend-hmrc`\\n- `play-frontend-govuk` is intended to be a direct port of `govuk-frontend`, and so helpers should live in an\\nappropriate helper class alongside the view models\\n- We would prefer to implicit classes rather than directly modifying the viewmodel case classes, as the viewmodel classes are derived from the govuk-frontend API\\n- We want to avoid replicatign the `govuk-frontend` parameter lists in the helpers, to keep the overhead of upgrading the library low\\nChosen option: Option 5, because it adds useful functionality (class enrichment of `Radios` via implicit\\n`RichRadios`, without adding new Twirl templates to maintain, and without adding new methods directly\\nto `Radios`, which would cause a divergence between `play-frontend-govuk` and `govuk-template` (undesirable).\\n### Positive Consequences\\n* Adds useful optional class enrichment to `Radios`\\n* Reduces code repetition between `Field` and `Radios` (DRY)\\n* Pattern is extensible, i.e. similar implicit helpers can be added to other form inputs\\n### Negative Consequences\\n* Doesn't add as much value as we would like\\n* Need to document carefully what behaviour occurs when information provided via both `Radios` and\\n`Field`, i.e. which takes precedence (currently intended to be `Radios` values take precendence)\\n* Need to document to teams that this helper is available (service developers won't use it if they don't know it's there)\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\n`play-frontend-govuk` and `play-frontend-hmrc` provide Scala \/ Play \/ Twirl implementations of the components provided\\\\nas Nunjucks in `govuk-frontend` and `hmrc-frontend`, using the assets provided by those libraries. How much should the\\\\nplay-frontend implementations diverge from their \u201cbase\u201d repositories?\\\\n## Decision Drivers\\\\n* Need to create frontend components that are easy to use by Scala developers on the platform\\\\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\\\\n`hmrc-frontend` are released\\\\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\\\\n\"}, {'role': 'model', 'content': '* Need to create frontend components that are easy to use by Scala developers on the platform\\\\n* Need to create a library that has a clear and quick upgrade path when new versions of `govuk-frontend` and\\\\n`hmrc-frontend` are released\\\\n* Need to have a robust testing strategy for library developers to have faith in when upgrading\\\\nChosen option: Option 3, because it allows for quick and continuous upgrading to follow GDS and HMRC design system\\\\nchanges, allows for a robust testing strategy of multiple implementations of the templates thanks to YAML provided by\\\\nGDS.\\\\n### Positive Consequences\\\\n* Design of case classes to follow GDS \/ HMRC design system means PlatUI as library maintainers do not have to create\\\\nviewmodel structure from scratch every time\\\\n* Adding new components can follow a clear and straightforward path\\\\n* Robust test strategy can be developed using Nunjucks components and Twirl templates using a parser\\\\n### Negative Consequences\\\\n* Feedback suggests that some developers do not find the API to be intuitive\\\\n* Separate decisions need to be made on handling multilingual support\\\\n* Enrichment of library needs to be done via separate Twirl helpers,\\\\nsee [related ADR](..\/adr\/0001-play-frontend-hmrc-mirrors-hmrc-frontend.md)\\\\n'}]"}
{"File Name":"simple-server\/006-no-op-updates-on-discarded-records.md","Context":"## Context\\nThe addition of soft-deletes means that we need a way to deal with updates\\nthat can happen on top of the soft-deleted records.\\nAn example could be follows,\\n* Nurse A from Facility X soft-deletes BP#1 and syncs to the server\\n* Nurse B from the same Facility X updates BP#1 without syncing changes from Nurse A\\n* When Nurse B syncs, her update to BP#1 will take place, although on a technically soft-deleted record\\nAll of this is an artifact of how the client decides to deal with this situation.\\nThe client currently does not send `deleted_at: nil` for an update to a non-soft-deleted record.\\nSince the app currently excludes all `null` values for serialization.\\nIf this behaviour changes in the future, wherein, the app sends `deleted_at: nil` for all requests, then we can get into\\nreally hairy situations where one could _un-discard_ a record because of an un-synced update from a different client.\\n","Decision":"Instead of relying on client-side behaviour, we will explicitly no-op the changes to soft-deleted records.\\nWe can do this simply by introducing another `merge_status` in the `Mergeable` concern as `:discarded`.\\n","tokens":215,"id":1696,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe new print holdings system has a persistent backend to which we build\\nincrementally. Data updates from any source may come at any time. Since data is\\nloaded incrementally, effects based on the order in which data is loaded could\\nhave substantial long-term effects. It would also be very difficult to\\nunderstand and debug concrete state that depended on a past state.\\nIn the current system, for holdings records with multiple OCNs the OCN is set\\nonce at scrub time and retained until the member re-sends holdings data.  The\\nchosen OCN is dependent on what items are in the repository at the time the\\ndata is scrubbed. This causes a potential issue where:\\n- a member submits a holding record with multiple OCNs\\n- if no OCN matches a HathiTrust item, the first OCN is used\\n- a HathiTrust item matching one of the OCNs is later ingested\\n- if it matches an OCN other than the first one, it will still not match the member holding\\nAn option to avoid this would be to require that members submit only one OCN\\nper holding, but there are sometimes valid reasons to submit multiple OCNs\\n(e.g. bound-withs, deprecated OCNs, etc).\\nThe current holdings validation\/normalization process also checks the current maximum\\nOCN using an OCLC API:\\n```bash\\ncurl -s \"https:\/\/www.oclc.org\/apps\/oclc\/wwg\" | egrep -o \\'\"oclcNumber\":\"[0-9]+\"\\' | egrep -o \\'[0-9]+\\'\\n```\\n\\n## Decision\\nThe concrete state of the database should depend only on the data that was\\nloaded, not on the order in which it was loaded.\\nThis particularly includes (but is not limited to) the clusters that data items\\n(OCNs, holdings, HathiTrust items, shared print commitments) are attached to.\\nThis decision may be revisited if a situation arises that makes it infeasibly\\nexpensive or impossible to maintain strict order independence, but the\\nconsequences of breaking that must be documented and well-understood.\\n\\n\\n## Context\\nJMAP allows users to upload binary content called blobs to be later referenced via method calls. This includes but is not\\nlimited to `Email\/set` for specifying the blobId of attachments and `Email\/import`.\\nThe [specification](https:\/\/jmap.io\/spec-core.html#binary-data) strongly encourages enforcing the cleanup of these uploads:\\n```\\nA blob that is not referenced by a JMAP object (e.g., as a message attachment) MAY be deleted by the server to free up\\nresources. Uploads (see below) are initially unreferenced blobs.\\n[...] An unreferenced blob MUST NOT be deleted for at least 1 hour from the time of upload; if reuploaded, the same\\nblobId MAY be returned, but this SHOULD reset the expiry time.\\n```\\nDeleting such uploads in a timely manner is important as:\\n- It enables freeing server resources.\\n- failing to do so may compromise privacy: content the user have uploaded and long forgotten might still be accessible\\nin the underlying data-store. Failing to delete uploads in a timely fashion may jeopardize for instance GDPR compliance.\\nToday, uploads are stored along side email attachments. This means:\\n- We can hardly apply a specific lifecycle that cleans up uploads, as distinguishing attachment from uploads is not\\ntrivial.\\n- We currently have a complex right resolution system on attachment, handling both the upload case (were the attachment\\nis linked to a user) and the \\'true\\' attachment case (linked to a message, those who can access the message can access\\nthe attachment). This leads to sub-optimal code (slow).\\n\\n## Decision\\nWe need to create a separate interface `UploadRepository` in `data-jmap` to store uploads for each user. We would provide a memory\\nimplementation as well as a distributed implementation of it.\\nThe distributed implementation would host metadata of the upload in Cassandra, and the content using the BlobStore API,\\nso object storage.\\nThis `UploadRepository` would be used by JMAP RFC-8620 to back uploads (instead of the attachment manager), we will\\nprovide a `BlobResolver` to enable interactions with the uploaded blob. Similarly, we will use the `UploadRepository` to\\nback uploads of JMAP draft.\\nWe will implement cleanup of the distributed `UploadRepository`. This will be done via:\\n- TTLs on the Cassandra metadata.\\n- Organisation of the blobs in time ranged buckets, only the two most recent buckets are kept.\\n- A WebAdmin endpoint would allow to plan a CRON triggering the cleanup.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThe addition of soft-deletes means that we need a way to deal with updates\\\\nthat can happen on top of the soft-deleted records.\\\\nAn example could be follows,\\\\n* Nurse A from Facility X soft-deletes BP#1 and syncs to the server\\\\n* Nurse B from the same Facility X updates BP#1 without syncing changes from Nurse A\\\\n* When Nurse B syncs, her update to BP#1 will take place, although on a technically soft-deleted record\\\\nAll of this is an artifact of how the client decides to deal with this situation.\\\\nThe client currently does not send `deleted_at: nil` for an update to a non-soft-deleted record.\\\\nSince the app currently excludes all `null` values for serialization.\\\\nIf this behaviour changes in the future, wherein, the app sends `deleted_at: nil` for all requests, then we can get into\\\\nreally hairy situations where one could _un-discard_ a record because of an un-synced update from a different client.\\\\n'}, {'role': 'model', 'content': 'Instead of relying on client-side behaviour, we will explicitly no-op the changes to soft-deleted records.\\\\nWe can do this simply by introducing another `merge_status` in the `Mergeable` concern as `:discarded`.\\\\n'}]"}
{"File Name":"once-ui\/0011-semantic-beta-versioning.md","Context":"## Context\\nWe have some issues with the versioning because sometimes the code updated regardless of the fix version in the once-ui library. So for this we used the beta versioning mechanism.\\n","Decision":"- In beta versioning mechanism we publish the prerelease version only for the team environment for testing purpose that would be from the once-ui QA branch and all the release version will be updated in the Master branch of once-ui library.\\n","tokens":39,"id":1155,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nUpdating the release process and branches to give us flexibility in term of fixing issues when a release\\nis not working fine. What was happening with the current release process is that we had to skip one or more versions\\nwith issues inside, creating confusion and putting in Maven central invalid versions\\n\\n## Decision\\nAfter long discussion we have decided to update our branches and how we do the release with a new process.\\nFirst of all, in order to standardize with other projects (USI in primis) we have decided to use a *dev* branch for\\ncontinuous development and the *master* branch to release new version of the software. Here the process described\\nstep-by-step\\n*From `dev` to `master`*\\n- From the `dev` branch we create feature branch - usually associated to a ticket - to work on a specific task\\n- When the developer has finished the task, he creates a pull request (PR) from the feature branch back into the `dev` branch on github\\n- If everybody is ok with the PR, it is merged into the `dev` branch\\n- When the team wants to do a new release of the software, any user can create the release notes and prepare a PR from the `dev` branch to the `master` branch\\n- When everybody is ok with the release notes, the developer can merge the PR\\n- Update the version on dev to the next SNAPSHOT (e.g. 4.1.6-SNAPSHOT --> 4.1.7-SNAPSHOT)\\n*From `master` to release of the software*\\n- In the `master` branch, the developer needs to update the version of the software to a release candidate (e.g. 4.1.6-SNAPSHOT --> 4.1.6-RC1)\\n- The software with the corresponding new version is then pushed to the remote master branch, which will trigger a build and release plan on snowy\\n- If everything is going to be fine, the developer can promote the artifact to scooby and then wwwdev\\n- After the release on wwwdev, the developer should send an email to the biosamples-users mailing list to announce the new software release and ask people to test it\\n- If any error is found, the developer need to make an hot-fix on master, update the version of the software (e.g. 4.1.6-RC1 --> 4.1.6-RC2) and release on the various environments till wwwdev\\n- After the software has been on wwwdev for a week and everybody is happy with it, is time to make the final release\\n- Update the version to the final version (e.g. 4.1.6-RC2 --> 4.1.6)\\n- Tag the current commit on github with `git tag v4.1.6` and push both the code and the tag to the remote repository\\n- Release on all the environments one after the other: snowy --> scooby --> wwwdev --> www\\n- After the release on production is done, send an email to the biosamples-announcements mailin list announcing the new release\\n- Merge the `master` branch back into `dev` to make sure `dev` also contains all the hot-fixes done on master\\n\\n\\n## Context and Problem Statement\\nWe need to be able to regenerate versions from snapshots. As documents is aim to change over time (location or filters) we can't rely on the last version of the declaration to regenerate the version from an old snapshot. So we need a system to keep track of declaration changes, that's what we called declarations and filters versioning.\\n\\n## Decision\\n[After consulting the community](https:\/\/github.com\/OpenTermsArchive\/engine\/issues\/156), the options 2A is retained as it hide complexity (compared to Option 1) of the history while increasing its discoverability (compared to Option 3) for contributors who might become more \u201cadventurous\u201d.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe have some issues with the versioning because sometimes the code updated regardless of the fix version in the once-ui library. So for this we used the beta versioning mechanism.\\\\n\"}, {'role': 'model', 'content': '- In beta versioning mechanism we publish the prerelease version only for the team environment for testing purpose that would be from the once-ui QA branch and all the release version will be updated in the Master branch of once-ui library.\\\\n'}]"}
{"File Name":"educational-platform\/0005-identifier-between-modules.md","Context":"## Context\\nDuring communications between bounded contexts, global identifiers for entities are needed. Also, these identifiers are needed for possible future integrations with external systems.\\n","Decision":"Natural keys or uuids should be used. Primary keys are forbidden for communications between modules or with external systems. If entity has good natural key - it's the most preferable choice for identifier between modules.\\nUseful links:\\n- https:\/\/tomharrisonjr.com\/uuid-or-guid-as-primary-keys-be-careful-7b2aa3dcb439\\n","tokens":32,"id":2501,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\n`service-stac` will be accepting machine-to-machine communication and will have an admin interface for operations\/debugging. Authentication methods for this two use cases need to be defined.\\n\\n## Decision\\nMachine-to-machine communication will be using token authentication, access to the admin interface will be granted with usernames\/passwords managed in the Django admin interface. At a later stage, this might be changed to a more advanced authentication scheme.\\n\\n\\n## Context\\nNot all inputs or args (= parameters) are always required.\\nOften there are sensible defaults available. We want to be able to make those defaults explicit where possible.\\nEven though some parameters may be optional, they might become required in the presence of other optional parameters.\\nWe need a way to explicitly define what combinations are possible.\\n\\n## Decision\\nWe drop the `optional` property on parameters. Instead, parameters get an additional property `defaultValue`. It can be\\nset to either a fixed literal value (e.g. `7`, `\"something\"`, `null`), an Arg, or it may reference the specific methods\\n`shape()` and `dataType()` on inputs and outputs. Parameters with `defaultValue` specified are treated as optional.\\nTo be able to deal with languages that do not support default values for arguments, Signatures will be specified.\\nSignatures are specified using a `Signature(a,b,c){ \"signature specific documentation\" }` section for each signature.\\nWith the signature specific documentation being optional.\\nSignatures making use of outputs will only be generated for NDArray programming mode, not in SameDiff mode. This also\\nmeans that parameters with a `defaultValue` based on an output will be treated as required in SameDiff mode.\\nIf signatures are specified, only the specified signatures will be generated.\\nIf no signatures are explicitly specified, only the \"all-arg\" and \"no-optional-arg\" signatures will be generated. In\\nNDArray programming mode, the default signatures also include a variant that includes the output.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nDuring communications between bounded contexts, global identifiers for entities are needed. Also, these identifiers are needed for possible future integrations with external systems.\\\\n'}, {'role': 'model', 'content': \"Natural keys or uuids should be used. Primary keys are forbidden for communications between modules or with external systems. If entity has good natural key - it's the most preferable choice for identifier between modules.\\\\nUseful links:\\\\n- https:\/\/tomharrisonjr.com\/uuid-or-guid-as-primary-keys-be-careful-7b2aa3dcb439\\\\n\"}]"}
{"File Name":"paas-team-manual\/ADR013-building-bosh-releases.md","Context":"Context\\n=======\\nWe use [Bosh](https:\/\/bosh.io\/) to create and manage our cloudfoundry deployment on AWS.\\nTo deploy software, Bosh needs certain binary dependencies available.\\nThese are known as bosh [releases](https:\/\/bosh.io\/docs\/release.html).\\nBefore this decision, we usually built and uploaded releases to Bosh as part of our [concourse](https:\/\/concourse.ci\/) pipeline.\\nOccasionally, we would manually build a release, store it on GitHub, and point Bosh to it there.\\n### Building Bosh Releases\\nWe investigated different approaches to creating bosh releases, in particular\\n* Multiple pipelines created dynamically using [branch manager](https:\/\/github.com\/alphagov\/paas-concourse-branch-manager)\\n* A single pipeline using [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource)\\nThe work on these spikes was recorded in\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/115142265\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/128937731\\nDecision\\n========\\nWe will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe must gradually migrate all our Bosh release builds to their own build pipelines.\\nWe will need separate jobs to build from master - this already has a proof of concept in the spike.\\nWe may have to add additional config in projects we fork to allow us to create final builds.\\n","Decision":"========\\nWe will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe must gradually migrate all our Bosh release builds to their own build pipelines.\\nWe will need separate jobs to build from master - this already has a proof of concept in the spike.\\nWe may have to add additional config in projects we fork to allow us to create final builds.\\n","tokens":353,"id":219,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe use [Bosh](https:\/\/bosh.io\/) to create and manage our cloudfoundry deployment on AWS.\\nTo deploy software, Bosh needs certain binary dependencies available.\\nThese are known as bosh [releases](https:\/\/bosh.io\/docs\/release.html).\\nBefore this decision, we usually built and uploaded releases to Bosh as part of our [concourse](https:\/\/concourse-ci.org\/) pipeline.\\nOccasionally, we would manually build a release, store it on GitHub, and point Bosh to it there.\\n### Building Bosh Releases\\nWe investigated different approaches to creating bosh releases, in particular\\n* Multiple pipelines created dynamically using [branch manager](https:\/\/github.com\/alphagov\/paas-concourse-branch-manager)\\n* A single pipeline using [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource)\\nThe work on these spikes was recorded in\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/115142265\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/128937731\\n\\n## Decision\\nWe will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\\n\\n\\nContext\\n=======\\nWe need to provide tenants with the ability to provision databases for use in\\ntheir applications. Our first iteration of this will be using RDS.\\nWe investigated some implementations of a service broker which supported RDS\\n- [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\n- [18F](https:\/\/github.com\/18F\/rds-service-broker)\\nDecision\\n========\\nWe will use the [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\nrds broker. As this is not a supported product, we will fork this and maintain\\nthis and implement new features ourselves.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe will be maintaining a new service broker, but have a head start on creating\\nit by basing it on an existing service broker.\\n\\n## Decision\\n========\\nWe will use the [cf platform eng](https:\/\/github.com\/cf-platform-eng\/rds-broker)\\nrds broker. As this is not a supported product, we will fork this and maintain\\nthis and implement new features ourselves.\\nStatus\\n======\\nAccepted\\nConsequences\\n============\\nWe will be maintaining a new service broker, but have a head start on creating\\nit by basing it on an existing service broker.\\n\\n\\n\\nProvide a decision given the context below:\\nContext\\\\n=======\\\\nWe use [Bosh](https:\/\/bosh.io\/) to create and manage our cloudfoundry deployment on AWS.\\\\nTo deploy software, Bosh needs certain binary dependencies available.\\\\nThese are known as bosh [releases](https:\/\/bosh.io\/docs\/release.html).\\\\nBefore this decision, we usually built and uploaded releases to Bosh as part of our [concourse](https:\/\/concourse.ci\/) pipeline.\\\\nOccasionally, we would manually build a release, store it on GitHub, and point Bosh to it there.\\\\n### Building Bosh Releases\\\\nWe investigated different approaches to creating bosh releases, in particular\\\\n* Multiple pipelines created dynamically using [branch manager](https:\/\/github.com\/alphagov\/paas-concourse-branch-manager)\\\\n* A single pipeline using [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource)\\\\nThe work on these spikes was recorded in\\\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/115142265\\\\nhttps:\/\/www.pivotaltracker.com\/n\/projects\/1275640\/stories\/128937731\\\\nDecision\\\\n========\\\\nWe will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\\\\nStatus\\\\n======\\\\nAccepted\\\\nConsequences\\\\n============\\\\nWe must gradually migrate all our Bosh release builds to their own build pipelines.\\\\nWe will need separate jobs to build from master - this already has a proof of concept in the spike.\\\\nWe may have to add additional config in projects we fork to allow us to create final builds.\\\\n'}, {'role': 'model', 'content': '========\\\\nWe will use the [pullrequest-resource](https:\/\/github.com\/jtarchie\/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\\\\nStatus\\\\n======\\\\nAccepted\\\\nConsequences\\\\n============\\\\nWe must gradually migrate all our Bosh release builds to their own build pipelines.\\\\nWe will need separate jobs to build from master - this already has a proof of concept in the spike.\\\\nWe may have to add additional config in projects we fork to allow us to create final builds.\\\\n'}]"}
{"File Name":"document-evidence-store-frontend\/0003-use-superjson-for-serialising-complex-objects.md","Context":"## Context\\nThe [Next.js docs on `getServerSideProps`](https:\/\/nextjs.org\/docs\/basic-features\/data-fetching#getserversideprops-server-side-rendering) specify that objects returned from this function must be JSON serializable\u2014this means only primitives, no complex objects or class instances.\\nOur gateways return domain models (instances of classes)\u2014these cannot be directly serialized to JSON to be passed to the client side components.\\nWe have a number of options:\\n1. provide methds on the domain classes to serialize and deserialize instances, then manually use these in our `getServerSideProps` functions and component definitions\\n2. return JSON response objects from our gateways and map them to domain objects in the components\\n3. use one of a number of third party libraries to do the serialisation for us\\n","Decision":"Option 1 is reasonable, but would mean writing and maintaining more code that necessarily required.\\nOption 2 goes against our [principles](https:\/\/github.com\/madetech\/clean-architecture) and would litter our presentation components with boundary logic.\\nTherefore we have decided to use option 3: [`superjson`](https:\/\/github.com\/blitz-js\/superjson) was built for this explicit purpose and has a [Babel plugin](https:\/\/github.com\/blitz-js\/babel-plugin-superjson-next) which handles the serialisation and deserialisation for us implicitly, meaning our functions are clean from this logic.\\n","tokens":170,"id":2596,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nSerializing and deserializing JSON in Java works best when performing it to\/from plain old java objects (POJOs).\\nWhile it is possible to serialize into generic `JsonNode` or `Map` objects this results in untyped values that\\nmust be referenced via String keys, meaning the compiler is limited in what it can check for us.\\nThe classes required for JSON serialization and deserialization are reptitive and error prone to write by hand.\\n[Immutables](https:\/\/immutables.github.io\/) helps to some extent, but there are still many classes and fields to be written.\\nThe [Discord API documentation](https:\/\/discordapp.com\/developers\/docs\/intro) is mostly consistent with how it documents\\nthe JSON structures it returns and accepts.\\nCode generation is often difficult to use with IDEs.\\nCode is often referring to classes that will not be committed, so we depend upon the IDE to generate these classes\\nso that the IDE can provide useful feedback regarding compilation errors.\\n\\n## Decision\\nWe will use code generation to generate the POJOs to be used for JSON serialization and deserialization.\\nWe will generate classes that make use of Immutables.\\n\\n\\n## Context\\nSymfony provides a very useful tool called [Autowiring](https:\/\/symfony.com\/doc\/3.4\/service_container\/autowiring.html). It allows to magically bind classes and their dependencies as long as both are declared as services, and the dependencies are declared using their FQCN as service identifier.\\nAdvantages:\\n- Less boilerplate configuration code for every service as you don\\'t have to manually bind dependencies manually.\\nDisadvantages:\\n- Dependencies must be declared using the FQCN instead of a service identifier like \"prestashop.core.foo.bar\".\\n- Currently existing services would have to be aliased in order to have service names follow the required naming convention for autowiring. This would lead to confusion as to which service name use in code, and in case a module wanted to replace them, they would have to replace both.\\n- Dependencies type-hinted as interfaces can have one and **only one** implementation for autowiring to work.\\n\\n## Decision\\nActiviting autoriwing is rejected for the 1.7 version.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThe [Next.js docs on `getServerSideProps`](https:\/\/nextjs.org\/docs\/basic-features\/data-fetching#getserversideprops-server-side-rendering) specify that objects returned from this function must be JSON serializable\u2014this means only primitives, no complex objects or class instances.\\\\nOur gateways return domain models (instances of classes)\u2014these cannot be directly serialized to JSON to be passed to the client side components.\\\\nWe have a number of options:\\\\n1. provide methds on the domain classes to serialize and deserialize instances, then manually use these in our `getServerSideProps` functions and component definitions\\\\n2. return JSON response objects from our gateways and map them to domain objects in the components\\\\n3. use one of a number of third party libraries to do the serialisation for us\\\\n'}, {'role': 'model', 'content': 'Option 1 is reasonable, but would mean writing and maintaining more code that necessarily required.\\\\nOption 2 goes against our [principles](https:\/\/github.com\/madetech\/clean-architecture) and would litter our presentation components with boundary logic.\\\\nTherefore we have decided to use option 3: [`superjson`](https:\/\/github.com\/blitz-js\/superjson) was built for this explicit purpose and has a [Babel plugin](https:\/\/github.com\/blitz-js\/babel-plugin-superjson-next) which handles the serialisation and deserialisation for us implicitly, meaning our functions are clean from this logic.\\\\n'}]"}
{"File Name":"fixcity\/0001-sys-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1798,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need to record the architectural decisions made on this project.\\\\n'}, {'role': 'model', 'content': 'We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\\\n'}]"}
{"File Name":"architecture-decision-log\/0015-model-overview.md","Context":"- [Context](#context)\\n- [Decision](#decision)\\n- [Status](#status)\\n- [More reading](#more-reading)\\n","Decision":"- [Status](#status)\\n- [More reading](#more-reading)\\nTo reduce that gap, we've decided to use the Domain-Driven Design technique (as you can see in [ADR#0003](0003-domain-driven-design.md)).\\nWe've divided our domain architecture into two scopes:\\n1. **Model Overview:** As you can see here, an overview of our architecture, considering only the most relevant entities.\\n2. **Local Domain Overview:** Every other file in this `domain` folder. They're only considering their domain, with a more detailed view and exploring their inner objects and considering only external entities that affect them.\\n![Snapshot of last know state of our domain model 2021-05-04](..\/assets\/0015-model-overview\/2021-05-04-diagram.jpg)\\n","tokens":35,"id":4916,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n\\n## Decision\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\nAfter further research, we\u2019ve decided to use [Apache License 2.0](https:\/\/www.apache.org\/licenses\/LICENSE-2.0) based on the following assumptions:\\n* The source code doesn\u2019t need to be public when a distribution of the software is made.\\n* Modifications to the software can be released under any license.\\n* Changes made to the source code must be documented.\\n* It offers the same patent usage protection as GPLv3.\\n* It explicitly prohibits the use of trademarked names found in the project.\\n\\n\\n* [Context](#context)\\n* [Decision](#decision)\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\n\\n## Decision\\n* [Status](#status)\\n* [Consequences](#consequences)\\n* [More reading](#more-reading)\\nWe've decided to use [Hashicorp's Waypoint](https:\/\/www.waypointproject.io\/) to handle the build\/deployment task for us. It is a pretty new tool (released for public in 2020), but Hashicorp made some of the most used and stable tools in the market (like Terraform and Vault).\\nWaypoint can handle the container build process, deployment, and release. Since we're using a Gitops process, it still lacks some plugins for our deployment\/release steps, but using it only to build even helps us make the deployment process more accessible.\\nWe've tried this tool in our [Design System application](https:\/\/github.com\/budproj\/design-system). It was straightforward to create containers for our storybooks (the front-end of this application).\\nThe next step is to create a Gitops plugin that can deploy the updated image manifest in our Argo Git repository or find a way to make Argo receive information regarding Waypoint deployment and release.\\n\\n\\n\\nProvide a decision given the context below:\\n- [Context](#context)\\\\n- [Decision](#decision)\\\\n- [Status](#status)\\\\n- [More reading](#more-reading)\\\\n\"}, {'role': 'model', 'content': \"- [Status](#status)\\\\n- [More reading](#more-reading)\\\\nTo reduce that gap, we've decided to use the Domain-Driven Design technique (as you can see in [ADR#0003](0003-domain-driven-design.md)).\\\\nWe've divided our domain architecture into two scopes:\\\\n1. **Model Overview:** As you can see here, an overview of our architecture, considering only the most relevant entities.\\\\n2. **Local Domain Overview:** Every other file in this `domain` folder. They're only considering their domain, with a more detailed view and exploring their inner objects and considering only external entities that affect them.\\\\n![Snapshot of last know state of our domain model 2021-05-04](..\/assets\/0015-model-overview\/2021-05-04-diagram.jpg)\\\\n\"}]"}
{"File Name":"exercise3-group-6\/0005-create-observation-repository-sort.md","Context":"## Context\\nResearchers may require methods for sorting the lists of Whale Observations to extract key information regarding\\nwhale migration, population, etc. Through Observation's available comparison methods the proposed sorting is possible.\\n","Decision":"Create `sortByDate` and `sortById` methods in ObservationRepository.\\n`sortByDate` uses the comparator `compareByDate` from the `Comparator<Observation>` interface and `Collections.sort()`\\nto sort the list of observations by field `sightingTime` (Date).\\n`sortById` uses the default `compareTo` method from the `Comparable<Observation>` interface and `Collections.sort()`\\nto sort the list of observations by field `ObservationId` (long).\\n`Collection.sort()` implements the abstract strategy of the `Comparator` interface using the concrete strategy\\n`compareByDate` and returns an integer referring to the order of the objects.\\n","tokens":42,"id":2776,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWhile developing APIs which store metadata about datasets, versions, filter jobs\\nand import jobs using Postgres, it has been found that some updates required\\ncomplex transactional updates across multiple tables.\\nWe need to assess our data model and decide if a relational database\\nbest fits this use case, or what alternative would be best.\\n\\n## Decision\\nMongoDB will be used to store metadata for datasets, versions, filter and import\\njobs. This allows us to prioritise the read heavy interactions, storing the\\ndata in the same JSON format it will be presented to users.\\n\\n\\n## Context\\nThe quality of information included in our pull requests varies greatly which\\ncan lead to code reviews which take longer and are harder for the person to\\nunderstand the considerations, outcomes and consquences of a series of changes.\\nA couple of recent projects have found a GitHub pull request template to have\\nbeen a positive change. Prompting what pull request descriptions should include\\nhas lead to better documented changes that have been easier to review on the\\nwhole.\\n\\n## Decision\\nInclude a basic pull request template for GitHub so that every pull request\\nprompts every author to fill it out.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nResearchers may require methods for sorting the lists of Whale Observations to extract key information regarding\\\\nwhale migration, population, etc. Through Observation's available comparison methods the proposed sorting is possible.\\\\n\"}, {'role': 'model', 'content': 'Create `sortByDate` and `sortById` methods in ObservationRepository.\\\\n`sortByDate` uses the comparator `compareByDate` from the `Comparator<Observation>` interface and `Collections.sort()`\\\\nto sort the list of observations by field `sightingTime` (Date).\\\\n`sortById` uses the default `compareTo` method from the `Comparable<Observation>` interface and `Collections.sort()`\\\\nto sort the list of observations by field `ObservationId` (long).\\\\n`Collection.sort()` implements the abstract strategy of the `Comparator` interface using the concrete strategy\\\\n`compareByDate` and returns an integer referring to the order of the objects.\\\\n'}]"}
{"File Name":"macondo\/0006-allow-printing-the-contents-of-the-repositories.md","Context":"## Context\\nNow that the repositories are behind a cache (http ones), it might be handy to\\nsee their contents as YAML wihtout having to fetch its content manually\\n","Decision":"Not worth it. It's easy enough to inspect a repository.\\n","tokens":36,"id":2195,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nOur users need an easier way to write alerts.  They currently have no\\neasy way to test queries before writing their alerts.\\nIn principle, they could use Prometheus's expression browser for this,\\nbut our Prometheus service requires basic auth, which locks our users\\nout of it.\\n\\n## Decision\\nWe will give our users access to Prometheus so they can use its\\nexpression browser to test queries when writing alerts. We will\\ndo this by using IP whitelisting instead of basic auth and\\nonly allowing our office IP addresses.\\nWe identified a number of different routes that we could have taken to\\nallow our users to access Prometheus.  One possible route that we\\nconsidered was using Oauth2 authentication. This would enable users to\\nauthenticate themselves to the platform with their Google account.\\nWe did not choose to go with this option this time for expediency.\\nThe idea behind this was to try to deliver the fastest value as\\npossible to the user. This method enables us to learn more about the user's\\nusage pattern. We do intend to add authentication but this will be done at\\na later date.\\n\\n\\n## Context\\nCache control headers can be used to prevent a client's browser from\\nre-requesting a page that has not changed, and may be leveraged by proxies to\\nreturn the same cached pages for multiple clients.\\n\\n## Decision\\nCache-control headers will be added to all valid requests. 500 and 404 errors\\nwill not be cached.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nNow that the repositories are behind a cache (http ones), it might be handy to\\\\nsee their contents as YAML wihtout having to fetch its content manually\\\\n\"}, {'role': 'model', 'content': \"Not worth it. It's easy enough to inspect a repository.\\\\n\"}]"}
{"File Name":"gatemint-sdk\/adr-018-extendable-voting-period.md","Context":"## Context\\nCurrently the voting period for all governance proposals is the same.  However, this is suboptimal as all governance proposals do not require the same time period.  For more non-contentious proposals, they can be dealt with more efficently with a faster period, while more contentious or complex proposals may need a longer period for extended discussion\/consideration.\\n","Decision":"We would like to design a mechanism for making the voting period of a governance proposal variable based on the demand of voters.  We would like it to be based on the view of the governance participants, rather than just the proposer of a governance proposal (thus, allowing the proposer to select the voting period length is not sufficient).\\nHowever, we would like to avoid the creation of an entire second voting process to determine the length of the voting period, as it just pushed the problem to determining the length of that first voting period.\\nThus, we propose the following mechanism:\\n### Params:\\n- The current gov param `VotingPeriod` is to be replaced by a `MinVotingPeriod` param.  This is the the default voting period that all governance proposal voting periods start with.\\n- There is a new gov param called `MaxVotingPeriodExtension`.\\n### Mechanism\\nThere is a new `Msg` type called `MsgExtendVotingPeriod`, which can be sent by any staked account during a proposal's voting period.  It allows the sender to unilaterally extend the length of the voting period by `MaxVotingPeriodExtension * sender's share of voting power`.  Every address can only call `MsgExtendVotingPeriod` once per proposal.\\nSo for example, if the `MaxVotingPeriodExtension` is set to 100 Days, then anyone with 1% of voting power can extend the voting power by 1 day.  If 33% of voting power has sent the message, the voting period will be extended by 33 days.  Thus, if absolutely everyone chooses to extend the voting period, the absolute maximum voting period will be `MinVotingPeriod + MaxVotingPeriodExtension`.\\nThis system acts as a sort of distributed coordination, where individual stakers choosing to extend or not, allows the system the guage the conentiousness\/complexity of the proposal.  It is extremely unlikely that many stakers will choose to extend at the exact same time, it allows stakers to view how long others have already extended thus far, to decide whether or not to extend further.\\n### Dealing with Unbonding\/Redelegation\\nThere is one thing that needs to be addressed.  How to deal with redelegation\/unbonding during the voting period.  If a staker of 5% calls `MsgExtendVotingPeriod` and then unbonds, does the voting period then decrease by 5 days again?  This is not good as it can give people a false sense of how long they have to make their decision.  For this reason, we want to design it such that the voting period length can only be extended, not shortened.  To do this, the current extension amount is based on the highest percent that voted extension at any time.  This is best explained by example:\\n1. Let's say 2 stakers of voting power 4% and 3% respectively vote to extend.  The voting period will be extended by 7 days.\\n2. Now the staker of 3% decides to unbond before the end of the voting period.  The voting period extension remains 7 days.\\n3. Now, let's say another staker of 2% voting power decides to extend voting period.  There is now 6% of active voting power choosing the extend.  The voting power remains 7 days.\\n4. If a fourth staker of 10% chooses to extend now, there is a total of 16% of active voting power wishing to extend.  The voting period will be extended to 16 days.\\n### Delegators\\nJust like votes in the actual voting period, delegators automatically inherit the extension of their validators.  If their validator chooses to extend, their voting power will be used in the validator's extension.  However, the delegator is unable to override their validator and \"unextend\" as that would contradict the \"voting power length can only be ratcheted up\" principle described in the previous section.  However, a delegator may choose the extend using their personal voting power, if their validator has not done so.\\n","tokens":75,"id":22,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nCurrently the voting period for all governance proposals is the same.  However, this is suboptimal as all governance proposals do not require the same time period.  For more non-contentious proposals, they can be dealt with more efficiently with a faster period, while more contentious or complex proposals may need a longer period for extended discussion\/consideration.\\n\\n## Decision\\nWe would like to design a mechanism for making the voting period of a governance proposal variable based on the demand of voters.  We would like it to be based on the view of the governance participants, rather than just the proposer of a governance proposal (thus, allowing the proposer to select the voting period length is not sufficient).\\nHowever, we would like to avoid the creation of an entire second voting process to determine the length of the voting period, as it just pushed the problem to determining the length of that first voting period.\\nThus, we propose the following mechanism:\\n### Params\\n* The current gov param `VotingPeriod` is to be replaced by a `MinVotingPeriod` param.  This is the default voting period that all governance proposal voting periods start with.\\n* There is a new gov param called `MaxVotingPeriodExtension`.\\n### Mechanism\\nThere is a new `Msg` type called `MsgExtendVotingPeriod`, which can be sent by any staked account during a proposal\\'s voting period.  It allows the sender to unilaterally extend the length of the voting period by `MaxVotingPeriodExtension * sender\\'s share of voting power`.  Every address can only call `MsgExtendVotingPeriod` once per proposal.\\nSo for example, if the `MaxVotingPeriodExtension` is set to 100 Days, then anyone with 1% of voting power can extend the voting power by 1 day.  If 33% of voting power has sent the message, the voting period will be extended by 33 days.  Thus, if absolutely everyone chooses to extend the voting period, the absolute maximum voting period will be `MinVotingPeriod + MaxVotingPeriodExtension`.\\nThis system acts as a sort of distributed coordination, where individual stakers choosing to extend or not, allows the system the gauge the conentiousness\/complexity of the proposal.  It is extremely unlikely that many stakers will choose to extend at the exact same time, it allows stakers to view how long others have already extended thus far, to decide whether or not to extend further.\\n### Dealing with Unbonding\/Redelegation\\nThere is one thing that needs to be addressed.  How to deal with redelegation\/unbonding during the voting period.  If a staker of 5% calls `MsgExtendVotingPeriod` and then unbonds, does the voting period then decrease by 5 days again?  This is not good as it can give people a false sense of how long they have to make their decision.  For this reason, we want to design it such that the voting period length can only be extended, not shortened.  To do this, the current extension amount is based on the highest percent that voted extension at any time.  This is best explained by example:\\n1. Let\\'s say 2 stakers of voting power 4% and 3% respectively vote to extend.  The voting period will be extended by 7 days.\\n2. Now the staker of 3% decides to unbond before the end of the voting period.  The voting period extension remains 7 days.\\n3. Now, let\\'s say another staker of 2% voting power decides to extend voting period.  There is now 6% of active voting power choosing the extend.  The voting power remains 7 days.\\n4. If a fourth staker of 10% chooses to extend now, there is a total of 16% of active voting power wishing to extend.  The voting period will be extended to 16 days.\\n### Delegators\\nJust like votes in the actual voting period, delegators automatically inherit the extension of their validators.  If their validator chooses to extend, their voting power will be used in the validator\\'s extension.  However, the delegator is unable to override their validator and \"unextend\" as that would contradict the \"voting power length can only be ratcheted up\" principle described in the previous section.  However, a delegator may choose the extend using their personal voting power, if their validator has not done so.\\n\\n\\n## Context\\nAs described in ADR0001, we are intending to build a new Marain API providing a standardized schema for common entities used in line-of-business applications.\\nThis API is logically separated into three, as described by [the service definition files here.](https:\/\/github.com\/marain-dotnet\/Marain.LineOfBusiness\/tree\/master\/Solutions\/ServiceDefinitions\/Prototyping):\\n- Organizational units and their data and relationships\\n- People and their data and relationships\\n- The relationships between organizational units and people\\nFor the initial implementation we have a number of decisions to make:\\n### Level of separation of APIs\\nHere we have three choices.\\n1. Implement the three APIs as completely separated services, each with their own underlying storage. This would give the most flexibility in implementation, but would also be the most complex. It would likely require an element of data synchronization between the services, as some level of denormalization would be necessary to make the APIs useful in their own right.\\n2. Implement the three APIs as a single service, sharing a common store. This would be the simplest solution, but would be a difficult decision to reverse at a later date should it become necessary.\\n3. Implement the three APIs as separate services but use common storage. This is a balance between the first two options, providing a simple initial implementation but making it relatively straightforward to separate out the three services at a later date if required.\\n### Underlying storage selection\\nAs always, we need to determine the most appropriate storage mechanism for the APIs. To some degree, this will depend on the level of separation decided upon.\\nHowever, since a large part of these APIs is about describing relationships between entities, a graph model is the obvious choice. If we do completely separate the three APIs, then it will make sense to use a graph database for the organizational unit and person APIs; the API that holds the relationship between organizational units and people could be either a standard document database or a graph database.\\n\\n## Decision\\n### We will implement separate services over a single data store\\nThis option gives us a simple path to v1 of the service but leaves us the option of further separating the APIs in the future.\\n### We will use a Graph database to implement storage\\nAs described above, this is the obvious choice for an API of this nature.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nCurrently the voting period for all governance proposals is the same.  However, this is suboptimal as all governance proposals do not require the same time period.  For more non-contentious proposals, they can be dealt with more efficently with a faster period, while more contentious or complex proposals may need a longer period for extended discussion\/consideration.\\\\n'}, {'role': 'model', 'content': 'We would like to design a mechanism for making the voting period of a governance proposal variable based on the demand of voters.  We would like it to be based on the view of the governance participants, rather than just the proposer of a governance proposal (thus, allowing the proposer to select the voting period length is not sufficient).\\\\nHowever, we would like to avoid the creation of an entire second voting process to determine the length of the voting period, as it just pushed the problem to determining the length of that first voting period.\\\\nThus, we propose the following mechanism:\\\\n### Params:\\\\n- The current gov param `VotingPeriod` is to be replaced by a `MinVotingPeriod` param.  This is the the default voting period that all governance proposal voting periods start with.\\\\n- There is a new gov param called `MaxVotingPeriodExtension`.\\\\n### Mechanism\\\\nThere is a new `Msg` type called `MsgExtendVotingPeriod`, which can be sent by any staked account during a proposal\\'s voting period.  It allows the sender to unilaterally extend the length of the voting period by `MaxVotingPeriodExtension * sender\\'s share of voting power`.  Every address can only call `MsgExtendVotingPeriod` once per proposal.\\\\nSo for example, if the `MaxVotingPeriodExtension` is set to 100 Days, then anyone with 1% of voting power can extend the voting power by 1 day.  If 33% of voting power has sent the message, the voting period will be extended by 33 days.  Thus, if absolutely everyone chooses to extend the voting period, the absolute maximum voting period will be `MinVotingPeriod + MaxVotingPeriodExtension`.\\\\nThis system acts as a sort of distributed coordination, where individual stakers choosing to extend or not, allows the system the guage the conentiousness\/complexity of the proposal.  It is extremely unlikely that many stakers will choose to extend at the exact same time, it allows stakers to view how long others have already extended thus far, to decide whether or not to extend further.\\\\n### Dealing with Unbonding\/Redelegation\\\\nThere is one thing that needs to be addressed.  How to deal with redelegation\/unbonding during the voting period.  If a staker of 5% calls `MsgExtendVotingPeriod` and then unbonds, does the voting period then decrease by 5 days again?  This is not good as it can give people a false sense of how long they have to make their decision.  For this reason, we want to design it such that the voting period length can only be extended, not shortened.  To do this, the current extension amount is based on the highest percent that voted extension at any time.  This is best explained by example:\\\\n1. Let\\'s say 2 stakers of voting power 4% and 3% respectively vote to extend.  The voting period will be extended by 7 days.\\\\n2. Now the staker of 3% decides to unbond before the end of the voting period.  The voting period extension remains 7 days.\\\\n3. Now, let\\'s say another staker of 2% voting power decides to extend voting period.  There is now 6% of active voting power choosing the extend.  The voting power remains 7 days.\\\\n4. If a fourth staker of 10% chooses to extend now, there is a total of 16% of active voting power wishing to extend.  The voting period will be extended to 16 days.\\\\n### Delegators\\\\nJust like votes in the actual voting period, delegators automatically inherit the extension of their validators.  If their validator chooses to extend, their voting power will be used in the validator\\'s extension.  However, the delegator is unable to override their validator and \"unextend\" as that would contradict the \"voting power length can only be ratcheted up\" principle described in the previous section.  However, a delegator may choose the extend using their personal voting power, if their validator has not done so.\\\\n'}]"}
{"File Name":"otrv4\/006-no-postquantum.md","Context":"### Context\\nWith the coming of quantum computers (QC), the community have started discussing\\naround the use of quantum resistant algorithms in privacy enhancing\\ntechnologies.\\n### Decision\\nOTRv4 does not take advantage of quantum resistant algorithms for the following\\nreasons:\\nFirstly, OTRv4 aims to be possible and easy to implement in today's environments\\nin a reasonable time frame. OTRv4 only aims to lay the foundation for future\\nchanges by adding version rollback protection, DAKEs, upgrade of primitives, and\\nnon-interactive conversations.\\nSecondly, current quantum resistant algorithms and their respective libraries\\nare not ready for incorporation. Production level libraries may take up to 6-18\\nmonths to be ready. Future versions of the protocol may incorporate these\\nlibraries and algorithms when they come into place.\\n### Consequences\\nOTRv4 does not use any algorithms which aim to provide effective resistance to\\nattacks done in quantum computers. If elliptic curve cryptography and 3072-bit\\nDiffie-Hellman can be attacked by quantum computers in the next upcoming years,\\nOTRv4's primitives will become unsafe and unusable. However, the use of a\\n3072-bit Diffie-Hellman \"brace key\" is used partly due to the potential of\\nquantum computers arriving earlier than predicted. When fault-tolerant quantum\\ncomputers break Ed448-Goldilocks keys, it will take some years beyond that point\\nto break 3072-bit Diffie-Hellman keys. Notice, though, that the 3072-bit Diffie\\nHellman keys does not provide any kind of post-quantum confidentiality. We use\\nthem because, in theory, the Elliptic Curve Discrete Logarithm Problem (ECDLP)\\nwill be broken faster than the Discrete Logarithm Problem (DLP). According to\\n[\\[1\\]](#references)), it will be needed 4060 qubits for breaking Ed448 and 6146\\nfor breaking 3072-bit Diffie Hellman.\\n### References\\n1. Roetteler, M., Naehri, M., Krysta M., and Lauter K. (2017).\\n*Quantum Resource Estimates for Computing Elliptic Curve Discrete Logarithms*.\\nAvailable at: https:\/\/eprint.iacr.org\/2017\/598.pdf\\n","Decision":"OTRv4 does not take advantage of quantum resistant algorithms for the following\\nreasons:\\nFirstly, OTRv4 aims to be possible and easy to implement in today's environments\\nin a reasonable time frame. OTRv4 only aims to lay the foundation for future\\nchanges by adding version rollback protection, DAKEs, upgrade of primitives, and\\nnon-interactive conversations.\\nSecondly, current quantum resistant algorithms and their respective libraries\\nare not ready for incorporation. Production level libraries may take up to 6-18\\nmonths to be ready. Future versions of the protocol may incorporate these\\nlibraries and algorithms when they come into place.\\n### Consequences\\nOTRv4 does not use any algorithms which aim to provide effective resistance to\\nattacks done in quantum computers. If elliptic curve cryptography and 3072-bit\\nDiffie-Hellman can be attacked by quantum computers in the next upcoming years,\\nOTRv4's primitives will become unsafe and unusable. However, the use of a\\n3072-bit Diffie-Hellman \"brace key\" is used partly due to the potential of\\nquantum computers arriving earlier than predicted. When fault-tolerant quantum\\ncomputers break Ed448-Goldilocks keys, it will take some years beyond that point\\nto break 3072-bit Diffie-Hellman keys. Notice, though, that the 3072-bit Diffie\\nHellman keys does not provide any kind of post-quantum confidentiality. We use\\nthem because, in theory, the Elliptic Curve Discrete Logarithm Problem (ECDLP)\\nwill be broken faster than the Discrete Logarithm Problem (DLP). According to\\n[\\[1\\]](#references)), it will be needed 4060 qubits for breaking Ed448 and 6146\\nfor breaking 3072-bit Diffie Hellman.\\n### References\\n1. Roetteler, M., Naehri, M., Krysta M., and Lauter K. (2017).\\n*Quantum Resource Estimates for Computing Elliptic Curve Discrete Logarithms*.\\nAvailable at: https:\/\/eprint.iacr.org\/2017\/598.pdf\\n","tokens":499,"id":3701,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe require symmetric ciphers to handle how we encrypt keys in the sdk,\\nand to potentially encrypt `priv_validator.json` in tendermint.\\nCurrently we use AEAD\\'s to support symmetric encryption,\\nwhich is great since we want data integrity in addition to privacy and authenticity.\\nWe don\\'t currently have a scenario where we want to encrypt without data integrity,\\nso it is fine to optimize our code to just use AEAD\\'s.\\nCurrently there is not a way to switch out AEAD\\'s easily, this ADR outlines a way\\nto easily swap these out.\\n### How do we encrypt with AEAD\\'s\\nAEAD\\'s typically require a nonce in addition to the key.\\nFor the purposes we require symmetric cryptography for,\\nwe need encryption to be stateless.\\nBecause of this we use random nonces.\\n(Thus the AEAD must support random nonces)\\nWe currently construct a random nonce, and encrypt the data with it.\\nThe returned value is `nonce || encrypted data`.\\nThe limitation of this is that does not provide a way to identify\\nwhich algorithm was used in encryption.\\nConsequently decryption with multiple algoritms is sub-optimal.\\n(You have to try them all)\\n\\n## Decision\\nWe should create the following two methods in a new `crypto\/encoding\/symmetric` package:\\n```golang\\nfunc Encrypt(aead cipher.AEAD, plaintext []byte) (ciphertext []byte, err error)\\nfunc Decrypt(key []byte, ciphertext []byte) (plaintext []byte, err error)\\nfunc Register(aead cipher.AEAD, algo_name string, NewAead func(key []byte) (cipher.Aead, error)) error\\n```\\nThis allows you to specify the algorithm in encryption, but not have to specify\\nit in decryption.\\nThis is intended for ease of use in downstream applications, in addition to people\\nlooking at the file directly.\\nOne downside is that for the encrypt function you must have already initialized an AEAD,\\nbut I don\\'t really see this as an issue.\\nIf there is no error in encryption, Encrypt will return `algo_name || nonce || aead_ciphertext`.\\n`algo_name` should be length prefixed, using standard varuint encoding.\\nThis will be binary data, but thats not a problem considering the nonce and ciphertext are also binary.\\nThis solution requires a mapping from aead type to name.\\nWe can achieve this via reflection.\\n```golang\\nfunc getType(myvar interface{}) string {\\nif t := reflect.TypeOf(myvar); t.Kind() == reflect.Ptr {\\nreturn \"*\" + t.Elem().Name()\\n} else {\\nreturn t.Name()\\n}\\n}\\n```\\nThen we maintain a map from the name returned from `getType(aead)` to `algo_name`.\\nIn decryption, we read the `algo_name`, and then instantiate a new AEAD with the key.\\nThen we call the AEAD\\'s decrypt method on the provided nonce\/ciphertext.\\n`Register` allows a downstream user to add their own desired AEAD to the symmetric package.\\nIt will error if the AEAD name is already registered.\\nThis prevents a malicious import from modifying \/ nullifying an AEAD at runtime.\\n\\n\\n## Context\\nWe require symmetric ciphers to handle how we encrypt keys in the sdk,\\nand to potentially encrypt `priv_validator.json` in tendermint.\\nCurrently we use AEAD\\'s to support symmetric encryption,\\nwhich is great since we want data integrity in addition to privacy and authenticity.\\nWe don\\'t currently have a scenario where we want to encrypt without data integrity,\\nso it is fine to optimize our code to just use AEAD\\'s.\\nCurrently there is not a way to switch out AEAD\\'s easily, this ADR outlines a way\\nto easily swap these out.\\n### How do we encrypt with AEAD\\'s\\nAEAD\\'s typically require a nonce in addition to the key.\\nFor the purposes we require symmetric cryptography for,\\nwe need encryption to be stateless.\\nBecause of this we use random nonces.\\n(Thus the AEAD must support random nonces)\\nWe currently construct a random nonce, and encrypt the data with it.\\nThe returned value is `nonce || encrypted data`.\\nThe limitation of this is that does not provide a way to identify\\nwhich algorithm was used in encryption.\\nConsequently decryption with multiple algoritms is sub-optimal.\\n(You have to try them all)\\n\\n## Decision\\nWe should create the following two methods in a new `crypto\/encoding\/symmetric` package:\\n```golang\\nfunc Encrypt(aead cipher.AEAD, plaintext []byte) (ciphertext []byte, err error)\\nfunc Decrypt(key []byte, ciphertext []byte) (plaintext []byte, err error)\\nfunc Register(aead cipher.AEAD, algo_name string, NewAead func(key []byte) (cipher.Aead, error)) error\\n```\\nThis allows you to specify the algorithm in encryption, but not have to specify\\nit in decryption.\\nThis is intended for ease of use in downstream applications, in addition to people\\nlooking at the file directly.\\nOne downside is that for the encrypt function you must have already initialized an AEAD,\\nbut I don\\'t really see this as an issue.\\nIf there is no error in encryption, Encrypt will return `algo_name || nonce || aead_ciphertext`.\\n`algo_name` should be length prefixed, using standard varuint encoding.\\nThis will be binary data, but thats not a problem considering the nonce and ciphertext are also binary.\\nThis solution requires a mapping from aead type to name.\\nWe can achieve this via reflection.\\n```golang\\nfunc getType(myvar interface{}) string {\\nif t := reflect.TypeOf(myvar); t.Kind() == reflect.Ptr {\\nreturn \"*\" + t.Elem().Name()\\n} else {\\nreturn t.Name()\\n}\\n}\\n```\\nThen we maintain a map from the name returned from `getType(aead)` to `algo_name`.\\nIn decryption, we read the `algo_name`, and then instantiate a new AEAD with the key.\\nThen we call the AEAD\\'s decrypt method on the provided nonce\/ciphertext.\\n`Register` allows a downstream user to add their own desired AEAD to the symmetric package.\\nIt will error if the AEAD name is already registered.\\nThis prevents a malicious import from modifying \/ nullifying an AEAD at runtime.\\n\\n\\n\\nProvide a decision given the context below:\\n### Context\\\\nWith the coming of quantum computers (QC), the community have started discussing\\\\naround the use of quantum resistant algorithms in privacy enhancing\\\\ntechnologies.\\\\n### Decision\\\\nOTRv4 does not take advantage of quantum resistant algorithms for the following\\\\nreasons:\\\\nFirstly, OTRv4 aims to be possible and easy to implement in today\\'s environments\\\\nin a reasonable time frame. OTRv4 only aims to lay the foundation for future\\\\nchanges by adding version rollback protection, DAKEs, upgrade of primitives, and\\\\nnon-interactive conversations.\\\\nSecondly, current quantum resistant algorithms and their respective libraries\\\\nare not ready for incorporation. Production level libraries may take up to 6-18\\\\nmonths to be ready. Future versions of the protocol may incorporate these\\\\nlibraries and algorithms when they come into place.\\\\n### Consequences\\\\nOTRv4 does not use any algorithms which aim to provide effective resistance to\\\\nattacks done in quantum computers. If elliptic curve cryptography and 3072-bit\\\\nDiffie-Hellman can be attacked by quantum computers in the next upcoming years,\\\\nOTRv4\\'s primitives will become unsafe and unusable. However, the use of a\\\\n3072-bit Diffie-Hellman \"brace key\" is used partly due to the potential of\\\\nquantum computers arriving earlier than predicted. When fault-tolerant quantum\\\\ncomputers break Ed448-Goldilocks keys, it will take some years beyond that point\\\\nto break 3072-bit Diffie-Hellman keys. Notice, though, that the 3072-bit Diffie\\\\nHellman keys does not provide any kind of post-quantum confidentiality. We use\\\\nthem because, in theory, the Elliptic Curve Discrete Logarithm Problem (ECDLP)\\\\nwill be broken faster than the Discrete Logarithm Problem (DLP). According to\\\\n[\\\\[1\\\\]](#references)), it will be needed 4060 qubits for breaking Ed448 and 6146\\\\nfor breaking 3072-bit Diffie Hellman.\\\\n### References\\\\n1. Roetteler, M., Naehri, M., Krysta M., and Lauter K. (2017).\\\\n*Quantum Resource Estimates for Computing Elliptic Curve Discrete Logarithms*.\\\\nAvailable at: https:\/\/eprint.iacr.org\/2017\/598.pdf\\\\n'}, {'role': 'model', 'content': 'OTRv4 does not take advantage of quantum resistant algorithms for the following\\\\nreasons:\\\\nFirstly, OTRv4 aims to be possible and easy to implement in today\\'s environments\\\\nin a reasonable time frame. OTRv4 only aims to lay the foundation for future\\\\nchanges by adding version rollback protection, DAKEs, upgrade of primitives, and\\\\nnon-interactive conversations.\\\\nSecondly, current quantum resistant algorithms and their respective libraries\\\\nare not ready for incorporation. Production level libraries may take up to 6-18\\\\nmonths to be ready. Future versions of the protocol may incorporate these\\\\nlibraries and algorithms when they come into place.\\\\n### Consequences\\\\nOTRv4 does not use any algorithms which aim to provide effective resistance to\\\\nattacks done in quantum computers. If elliptic curve cryptography and 3072-bit\\\\nDiffie-Hellman can be attacked by quantum computers in the next upcoming years,\\\\nOTRv4\\'s primitives will become unsafe and unusable. However, the use of a\\\\n3072-bit Diffie-Hellman \"brace key\" is used partly due to the potential of\\\\nquantum computers arriving earlier than predicted. When fault-tolerant quantum\\\\ncomputers break Ed448-Goldilocks keys, it will take some years beyond that point\\\\nto break 3072-bit Diffie-Hellman keys. Notice, though, that the 3072-bit Diffie\\\\nHellman keys does not provide any kind of post-quantum confidentiality. We use\\\\nthem because, in theory, the Elliptic Curve Discrete Logarithm Problem (ECDLP)\\\\nwill be broken faster than the Discrete Logarithm Problem (DLP). According to\\\\n[\\\\[1\\\\]](#references)), it will be needed 4060 qubits for breaking Ed448 and 6146\\\\nfor breaking 3072-bit Diffie Hellman.\\\\n### References\\\\n1. Roetteler, M., Naehri, M., Krysta M., and Lauter K. (2017).\\\\n*Quantum Resource Estimates for Computing Elliptic Curve Discrete Logarithms*.\\\\nAvailable at: https:\/\/eprint.iacr.org\/2017\/598.pdf\\\\n'}]"}
{"File Name":"docnow-infra\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":330,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need to record the architectural decisions made on this project.\\\\n'}, {'role': 'model', 'content': 'We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\\\n'}]"}
{"File Name":"tamr-client\/0004-documentation-and-docstrings.md","Context":"## Context\\nDocumentation can take four forms:\\n1. Explanation\\n2. Tutorial\\n3. How-to\\n4. Reference\\nWe need a way to author and host prosey documentation and generate reference docs based on source code.\\n","Decision":"Doc compilation will be done via [sphinx](https:\/\/www.sphinx-doc.org\/en\/master\/).\\nProsey documentation (1-3) via [recommonmark](https:\/\/github.com\/readthedocs\/recommonmark).\\nReference documentation (4) will be generated based on type annotations and docstrings via:\\n- Automatic docs based on docstrings via [sphinx-autodoc](https:\/\/www.sphinx-doc.org\/en\/master\/usage\/extensions\/autodoc.html), [sphinx-autodoc-typehints](https:\/\/github.com\/agronholm\/sphinx-autodoc-typehints)\\n- Google-style docstrings via [napoleon](https:\/\/www.sphinx-doc.org\/en\/master\/usage\/extensions\/napoleon.html)\\n- Hosting on [ReadTheDocs](https:\/\/readthedocs.org\/) (RTD)\\n- Build docs in CI and fail on errors or warnings.\\n","tokens":48,"id":540,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nWe want to code a backend that will handle inbound informations through the\\nmessage broker, and expose them to the dashboard front-end.\\nWhat programming language should we use?\\n## Decision Drivers\\n- Should be interesting (fun, new, ...) for people\\n- Either already adopted by Xebians or not too hard to adopt based on existing\\nknowledge\\n- Library support for our usecases\\n\\n## Decision\\n- Should be interesting (fun, new, ...) for people\\n- Either already adopted by Xebians or not too hard to adopt based on existing\\nknowledge\\n- Library support for our usecases\\nChosen option: **\"Go\"**, because:\\n- It has good library support for things such as **HTTP\/2** and **Protobuf**\\n(separately, we\\'re not talking about gRPC  here). This will help establish\\nstrong bidirectionnal and persistant streaming with the dashboard (HTTP\/2).\\nIt will also open the way to Protobuf serialized messages over the message\\nbus that will maybe comme in addition to RabbitMQ.\\n- People on the project want to play more with it!\\n- It seems appropriate for small (micro)services like the one we\\'re going to\\nbuild\\nHowever, we are **not excluding other languages** depending on usecases and\\nwishes of people working on it. We want people to contribute so we won\\'t force\\nanyone to use a given language. **People are free to use the language they\\nwant**, but we\\'ll try to have **Go as a default main language as much as\\npossible**.\\n\\n\\n## Context\\nThe existing site (Frontend) includes Javascript in a variety of ways:\\n-   inline\\n-   script tags\\n-   dynamically added script tags\\nThere is a single (universal) boot process which conditionally loads additional\\nJS as required based on page metadata.\\nFor Dotcom Rendering we want to simplify things so that it is easy for a\\ndeveloper to know where to add a script\/functionality. We also want an approach\\nthat helps ensure good performance and, related, which is less monolithic.\\n\\n## Decision\\n-   Load scripts in the head as deferred, or as async at the bottom of the HTML\\nbody section\\n-   All other script loading (other than for commercial third-party code) is\\nprohibited\\n-   We will not use a universal \\'boot\\' JS file, but load scripts which have a\\nunique purpose and which are self-contained aside from a few shared helpers\\nFuller discussion of this approach can be found here:\\nhttps:\/\/docs.google.com\/document\/d\/1EnkowJ7BFumrqkpkRacRl-gSkiJs9fUgRCixQTLxBvo\/edit#.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nDocumentation can take four forms:\\\\n1. Explanation\\\\n2. Tutorial\\\\n3. How-to\\\\n4. Reference\\\\nWe need a way to author and host prosey documentation and generate reference docs based on source code.\\\\n'}, {'role': 'model', 'content': 'Doc compilation will be done via [sphinx](https:\/\/www.sphinx-doc.org\/en\/master\/).\\\\nProsey documentation (1-3) via [recommonmark](https:\/\/github.com\/readthedocs\/recommonmark).\\\\nReference documentation (4) will be generated based on type annotations and docstrings via:\\\\n- Automatic docs based on docstrings via [sphinx-autodoc](https:\/\/www.sphinx-doc.org\/en\/master\/usage\/extensions\/autodoc.html), [sphinx-autodoc-typehints](https:\/\/github.com\/agronholm\/sphinx-autodoc-typehints)\\\\n- Google-style docstrings via [napoleon](https:\/\/www.sphinx-doc.org\/en\/master\/usage\/extensions\/napoleon.html)\\\\n- Hosting on [ReadTheDocs](https:\/\/readthedocs.org\/) (RTD)\\\\n- Build docs in CI and fail on errors or warnings.\\\\n'}]"}
{"File Name":"tamr-client\/0002-linting-and-formatting.md","Context":"## Context\\nInconsistent code formatting slows down development and the review process.\\nCode should be linted for things like:\\n- unused imports and variables\\n- consistent import order\\nCode formatting should be done automatically or programmatically, taking the burden off of reviewers.\\n","Decision":"For linting, use [flake8](https:\/\/flake8.pycqa.org\/en\/latest\/) and [flake8-import-order](https:\/\/github.com\/PyCQA\/flake8-import-order).\\nFor formatting, use [black](https:\/\/github.com\/psf\/black).\\n","tokens":55,"id":541,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nWe would like to generally flatten the directory structure in FDS as well as improving the\\nimport statements for client applications. This will be a breaking change for a major\\nversion, as clients will need to update imports once we make the change.\\n## Decision Drivers\\n- Remove unnecessary hierarchy in components dir\\n- Simplify documentation\\n- Adopt the unofficial CBI standard of organizing by component directories (each component\\nhas its own directory).\\n- Improve ergonomics and performance of import statements for client applications\\n\\n## Decision\\n- Remove unnecessary hierarchy in components dir\\n- Simplify documentation\\n- Adopt the unofficial CBI standard of organizing by component directories (each component\\nhas its own directory).\\n- Improve ergonomics and performance of import statements for client applications\\n### Import statements\\nBecause individual imports for each component has a negative performance impact, we decided\\nto use destructured imports and rely on a tree shaking strategy:\\n```diff\\n-import Flex from \\'@cbinsights\/fds\/lib\/components\/layout\/Flex\\';\\n-import FlexItem from \\'@cbinsights\/fds\/lib\/components\/layout\/FlexItem\\';\\n-import TextInput from \\'@cbinsights\/fds\/lib\/components\/forms\/TextInput\\';\\n+import { Flex, FlexItem, TextInput } from \\'@cbinsights\/fds\/lib\/components\\';\\n```\\nThe \"root\" file for each import corresponds to an FDS code module.\\n| code module   | Import example                                                                        |\\n| ------------- | ------------------------------------------------------------------------------------- |\\n| `dictionary`  | `import { COLOR_TIMBERWOLF, COLOR_PURPLE } from \\'@cbinsights\/fds\/lib\/dictionary\\';`    |\\n| `components`  | `import { Avatar, TextInput } from \\'@cbinsights\/fds\/lib\/components\/\\';`                |\\n| `icons`       | `import { CloudIcon, BellIcon } from \\'@cbinsights\/fds\/lib\/icons\\';`                    |\\n| `assets`      | Use path `@cbinsights\/fds\/lib\/assets\/<file>` where applicable                         |\\n| `base-styles` | not applicable; we only publish this CSS as part of the main styles file in `assets\/` |\\n### `components` file structure\\nCategorization directories will be removed.\\nEach component will have its own directory under `components\/`.\\nTests, stories, private components, and helpers will be colocated in component\\ndirectories.\\n```\\ncomponents\/\\n\u251c\u2500\u2500 Foo\/\\n\u2502   \u251c\u2500\u2500 FooHelper.jsx\\n\u2502   \u251c\u2500\u2500 foo.test.jsx\\n\u2502   \u251c\u2500\u2500 foo.stories.jsx\\n\u2502   \u251c\u2500\u2500 utils.js\\n\u2502   \u2514\u2500\u2500 index.js  \/\/ contains Foo component code\\n...\\n```\\n### Documentation\\nThe \"Components\" section in Storybook will no longer have categorization. All components\\nwill be documented flat under the section heading.\\n\\n\\n## Context\\nThe existing site (Frontend) includes Javascript in a variety of ways:\\n-   inline\\n-   script tags\\n-   dynamically added script tags\\nThere is a single (universal) boot process which conditionally loads additional\\nJS as required based on page metadata.\\nFor Dotcom Rendering we want to simplify things so that it is easy for a\\ndeveloper to know where to add a script\/functionality. We also want an approach\\nthat helps ensure good performance and, related, which is less monolithic.\\n\\n## Decision\\n-   Load scripts in the head as deferred, or as async at the bottom of the HTML\\nbody section\\n-   All other script loading (other than for commercial third-party code) is\\nprohibited\\n-   We will not use a universal \\'boot\\' JS file, but load scripts which have a\\nunique purpose and which are self-contained aside from a few shared helpers\\nFuller discussion of this approach can be found here:\\nhttps:\/\/docs.google.com\/document\/d\/1EnkowJ7BFumrqkpkRacRl-gSkiJs9fUgRCixQTLxBvo\/edit#.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nInconsistent code formatting slows down development and the review process.\\\\nCode should be linted for things like:\\\\n- unused imports and variables\\\\n- consistent import order\\\\nCode formatting should be done automatically or programmatically, taking the burden off of reviewers.\\\\n'}, {'role': 'model', 'content': 'For linting, use [flake8](https:\/\/flake8.pycqa.org\/en\/latest\/) and [flake8-import-order](https:\/\/github.com\/PyCQA\/flake8-import-order).\\\\nFor formatting, use [black](https:\/\/github.com\/psf\/black).\\\\n'}]"}
{"File Name":"heptaconnect-docs\/2020-12-10-portal-service-container.md","Context":"## Context\\nPortal extensions shall be able to interfere with any operation the supported portal is doing to make any business logic within the supported portal adjustable.\\nThis is already possible having the explorer, emitter and receiver stacks when it is about changing the flow and the incoming data from HEPTAconnect and the outgoing data towards HEPTAconnect.\\nThere is no way yet to change the exact behaviour how an API is used within the supported portal.\\nIt has been suggested to expose the operational APIs as public methods in the supported portal class.\\nPortal extension are able to interact with the same APIs like their supported portal but not yet able to change the implementation of these public methods.\\n","Decision":"* Use PSR-11 containers\\n* Do not use inheritance \/ decoration between portal extensions and the portal itself\\n* Do not use a container builder as there is no common interface yet and is not needed yet\\n* Do not use a hook pattern\\n","tokens":137,"id":3207,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nWhile developing the new, unified API, there weren\\'t clear guidelines for structuring new API endpoints. The first approach\\nwas introducing two different endpoint prefixes, similar to what is currently done in a regular shop. On the 30th of April,\\nwe have merged a unification of our endpoints in https:\/\/github.com\/Sylius\/Sylius\/pull\/11397. This move was dictated by\\nthe fact that we wanted to push unification even further. Depending on the currently logged in user, we had to expose different\\ndata for available entities in both contexts. Besides, it ought to simplify API usage (as it would reduce the number of endpoints)\\nand simplify the relations between resources. However, this change rose a lot of new issues to solve:\\n* Different serialization for different users is not trivial. Some relations between objects are also contextual, so\\nadmin can see many more links than the regular visitor or shop user.\\n* Endpoints like \"Product Show\" are resolved differently depending on the context as well. The shop products are determined\\nbased on the slug (taking into account the current channel), while the administrator is browsing products by code. This\\nseparation blurs the product identifier, and it is not clear which route should be considered as IRI.\\n* the split was not complete. Two separate log in endpoints have to be preserved, due to our user architecture and the\\ndecision that we want to be explicit, which user is trying to log in.\\nUsing one unified identifier in both admin and shop panel is a no go as well. In the shop we should use a `slug`, to be able\\nto expose product data based on URL (or provide redirect to proper resource based on the channel and locale), while in admin\\nresources are channel and locale agnostic, and we should use `codes` instead.\\n## Decision Drivers\\n* Driver 1: There is not strict requirement to have a direct 1 to 1 correlation between entities in database and exposed API Resources.\\nOne resource in the database may aggregate several API Resources\\n* Driver 2: There should be a clear way to determine an IRI for each of exposed API Resources\\n* Driver 3: A clear separation of available resources and paths for visitor and admin may be expected\\n\\n## Decision\\n* Driver 1: There is not strict requirement to have a direct 1 to 1 correlation between entities in database and exposed API Resources.\\nOne resource in the database may aggregate several API Resources\\n* Driver 2: There should be a clear way to determine an IRI for each of exposed API Resources\\n* Driver 3: A clear separation of available resources and paths for visitor and admin may be expected\\nChosen option: \"Moving back to prefixed paths for back-office and shop\". This option was selected because it allows us to\\neasily leverage the Symfony Security component\\'s benefits like firewalls and ACL. The only second and third option provides\\npredictable behavior and clear separation of concerns for admin and shop. Still, suffixes on most of the routes seem like\\na worse solution compared to the common prefix. The common prefix will indicate that these endpoints are grouped in the\\nsame context.\\n\\n\\n## Context\\nDalmatian configuration is sourced directly from AWS so that the latest\\ninformation can be worked against when running this app locally.\\nEach time we start the local server a new YML file is created. When the server\\nis restarted the contents are always thrown away and treated as ephemeral.\\nWe need a structured way to query and mutate data within the app that is not\\npassing around the same YML file all the way through the application. Having\\nresources allows us to build a more conventional Rails app with restful routing.\\nThe structure of dalmatian.yml is fragile. There is no shared schema, versioning\\nor domain model to allow us to set robust expectations upon its interface.\\nIt is subject to change from the upstream of Dalmatian Core, for which this app\\nseeks to provide helper support for. The Dalmatian config file (dalmatian.yml)\\nis likely to change to meet the needs of Dalmatian Core. If this app depends on\\nthe structure of this YML file rigidly the tool risks being overly brittle and\\nbreaking whenever the structure changes.\\nRails applications are provisioned by default with Postgres as a relational\\ndatabase. There would be overhead in creating and maintaining a schema to\\nsupport a structure that we have no direct control over from this application.\\nIn the past when dxw have used a document store, Mongodb and the Mongoid gem\\nhave served well. Given we do not plan to host the application the choice of\\ntool has less impact, though we could externally host or spin up a mongo\\ncontainer should we choose to.\\n\\n## Decision\\nUse MongoDB for Dalmatian data instead of Postgres and ActiveRecord.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nPortal extensions shall be able to interfere with any operation the supported portal is doing to make any business logic within the supported portal adjustable.\\\\nThis is already possible having the explorer, emitter and receiver stacks when it is about changing the flow and the incoming data from HEPTAconnect and the outgoing data towards HEPTAconnect.\\\\nThere is no way yet to change the exact behaviour how an API is used within the supported portal.\\\\nIt has been suggested to expose the operational APIs as public methods in the supported portal class.\\\\nPortal extension are able to interact with the same APIs like their supported portal but not yet able to change the implementation of these public methods.\\\\n'}, {'role': 'model', 'content': '* Use PSR-11 containers\\\\n* Do not use inheritance \/ decoration between portal extensions and the portal itself\\\\n* Do not use a container builder as there is no common interface yet and is not needed yet\\\\n* Do not use a hook pattern\\\\n'}]"}
{"File Name":"adr-poc\/0007-use-unity-for-dependency-injection.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":4969,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\\n## Decision\\nThe change that we're proposing or have agreed to implement.\\n\\n\\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\\n## Decision\\nThe change that we're proposing or have agreed to implement.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\\\n\"}, {'role': 'model', 'content': \"The change that we're proposing or have agreed to implement.\\\\n\"}]"}
{"File Name":"fundraising-application\/011_CSS_Class_Schema.md","Context":"## Context\\nWe are rewriting our client-side from scratch. That gives us the opportunity to restructure and rethink our css classes.\\nWe discussed two CSS schemas `naming classes as components` vs `naming classes according to visual properties`.\\nNaming classes as components - This will make it more clear where the css classes are used, but will duplicate what we already reflect in our markup. A variation of this would be to use the [Block-Element-Modifier (BEM)](http:\/\/getbem.com\/introduction\/) pattern.\\nNaming classes as styling - This will make the CSS classes reusable and fits well with the naming schema of our CSS framework, [Bulma](https:\/\/bulma.io\/). Examples of this schema are the [Atomic CSS](https:\/\/github.com\/nemophrost\/atomic-css) schema and [Tailwind CSS](https:\/\/tailwindcss.com\/docs\/what-is-tailwind\/). There were several small concerns with this style:\\n* The style makes our markup less performant: because each element has several classes, the resulting size will be bigger. Since the markup will be compressed anyway and the compression algorithm is efficient for reducing repeating patterns, this is not an issue.\\n* We might end up with \"lying\" class names, e.g. `.is-red { color: blue}`. This will be mitigated by the style names having a slightly higher and more purpose-oriented schema, e.g. `.primary-color`. We strive to achieve a sweet spot between the extremes of BEM and Atomic CSS.\\n* The added classes make the HTML source less readable. \"Readability\", especially when compared to BEM, is kind of subjective.\\n","Decision":"We will name CSS classes according to the styling they produce instead of the component they are used in. Adding component class names to Vue components feels like a violation of the [DRY principle](https:\/\/en.wikipedia.org\/wiki\/Don%27t_repeat_yourself).\\nWe will share as much of those styling classes between components as possible, instead of each component defining their own styles.\\n","tokens":347,"id":1525,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nAs part of the process of rewriting the admin components for link platform, we need to decide on what strategy we want\\nto use for including styling in our application.\\nThere were two main choices:\\n* CSS stylesheets\\n* CSS-in-JS\\n### Why CSS-in-JS?\\nThere are many benefits to CSS-in-JS which are nicely outlined in\\n[this article](\\'https:\/\/medium.com\/seek-blog\/a-unified-styling-language-d0c208de2660\\'):\\n* Enforces fundamental scoping without relying on pure convention (automatically generated classes).\\n* Renders critical CSS baked in, as components bring their styles with them automatically.\\n* Removes the necessity to hard code class names.\\n* Increases reusability of styles.\\n* Improves maintainability and dead code detection\\n### Why Fela?\\n[Fela](https:\/\/github.com\/rofrischmann\/fela) is a high performance, framework agnostic\\ntool to handle state-driven styling in Javascript. It also uses Atomic Class design to\\ngenerate class names, and creates stylesheets with those class names, which allows for\\nmodular style reuse. Fela performs about\\n[four times faster](https:\/\/github.com\/A-gambit\/CSS-IN-JS-Benchmarks\/blob\/master\/RESULT.md)\\nthan Glamorous. Fela has also been used by Zendesk engineers in the past and so comes\\nwith the added bonus of having some internal knowledge already in place to hit the\\nground running.\\n\\n## Decision\\nWe will use CSS-in-JS, in the form of Fela and React-Fela.\\n\\n\\n## Context and Problem Statement\\nTo prepare for the transition from the Silex to the Symfony framework we need to convert the anonymous route functions to controller classes. This document is about how to name those classes and their methods.\\nSome anonymous route functions already had classes, called \"RouteHandlers\", with a single method called `handle`. Some controllers have adopted this scheme as their action method name, while others have more informative method names.\\nThe state of the controller naming(October 2020): Most of the controllers have an `ActionSentenceController::action` schema with only one action, two controllers, `ApplyForMembershipController` and `ContactController` have a ``NounController::action`` schema.\\nSome of the code in the controllers (e.g. validation) might become obsolete if we use more Symfony features, making the controllers shorter.\\n## Decision Drivers\\n* **Consistency** - We want to have some general rule of thumb to apply to all controller classes.\\n* **Developer Experience**\\n* We want to jump quickly to a controller class via the code search features of the IDE.\\n* We want the controllers to be as short as possible, avoid scrolling.\\n* We want to easily navigate to the right controller in the file system, keeping the number of files in a directory\/namespace as small as possible.\\n* **Following established standards** - We want to do what other frameworks are doing\\n* **Gradual refactoring** - The naming changes must not force the transition form Silex to Symfony by choosing a pattern that\\'s incompatible with Silex.\\n\\n## Decision\\n* **Consistency** - We want to have some general rule of thumb to apply to all controller classes.\\n* **Developer Experience**\\n* We want to jump quickly to a controller class via the code search features of the IDE.\\n* We want the controllers to be as short as possible, avoid scrolling.\\n* We want to easily navigate to the right controller in the file system, keeping the number of files in a directory\/namespace as small as possible.\\n* **Following established standards** - We want to do what other frameworks are doing\\n* **Gradual refactoring** - The naming changes must not force the transition form Silex to Symfony by choosing a pattern that\\'s incompatible with Silex.\\nTBD: Chosen option: \"\", because (justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)).\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe are rewriting our client-side from scratch. That gives us the opportunity to restructure and rethink our css classes.\\\\nWe discussed two CSS schemas `naming classes as components` vs `naming classes according to visual properties`.\\\\nNaming classes as components - This will make it more clear where the css classes are used, but will duplicate what we already reflect in our markup. A variation of this would be to use the [Block-Element-Modifier (BEM)](http:\/\/getbem.com\/introduction\/) pattern.\\\\nNaming classes as styling - This will make the CSS classes reusable and fits well with the naming schema of our CSS framework, [Bulma](https:\/\/bulma.io\/). Examples of this schema are the [Atomic CSS](https:\/\/github.com\/nemophrost\/atomic-css) schema and [Tailwind CSS](https:\/\/tailwindcss.com\/docs\/what-is-tailwind\/). There were several small concerns with this style:\\\\n* The style makes our markup less performant: because each element has several classes, the resulting size will be bigger. Since the markup will be compressed anyway and the compression algorithm is efficient for reducing repeating patterns, this is not an issue.\\\\n* We might end up with \"lying\" class names, e.g. `.is-red { color: blue}`. This will be mitigated by the style names having a slightly higher and more purpose-oriented schema, e.g. `.primary-color`. We strive to achieve a sweet spot between the extremes of BEM and Atomic CSS.\\\\n* The added classes make the HTML source less readable. \"Readability\", especially when compared to BEM, is kind of subjective.\\\\n'}, {'role': 'model', 'content': 'We will name CSS classes according to the styling they produce instead of the component they are used in. Adding component class names to Vue components feels like a violation of the [DRY principle](https:\/\/en.wikipedia.org\/wiki\/Don%27t_repeat_yourself).\\\\nWe will share as much of those styling classes between components as possible, instead of each component defining their own styles.\\\\n'}]"}
{"File Name":"automated-workstation-setup\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n","tokens":16,"id":1791,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need to record the architectural decisions made on this project.\\\\n'}, {'role': 'model', 'content': 'We will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\\\n'}]"}
{"File Name":"lobiani\/0009-develop-admin-tool-as-spa.md","Context":"## Context\\nAs part of the functional requirements, we need to have a means of managing the content through an intuitive web\\ninterface. There are a couple of options to achieve this\\n### 1. As a \"traditional\" web application in the MVC architectural style, embedded within the backend (monolith)\\n#### Pros\\n- Server-side rendering sometimes can provide better user experience since content is immediately visible once\\nthe page is loaded\\n- Compatibility issues between the frontend and backend can be detected at the earlier stage\\n#### Cons\\n- Tools and libraries for the backend and frontend are mixed in the same project, making it a bit messy and mentally harder to grasp\\n- Strong coupling between web layer and backend discourages us to design general-purpose API for other types of potential consumers\\n- Provides limited level of interactivity\\n### 2. As a Single Page Application in the MVVM architectural style, packaged and deployed separately from the backend\\n#### Pros\\n- Frontend is more decoupled from backend technologies since they interact with each other via API\\n- Backend API can be potentially used by other consumers too: CLI, Native & Mobile (as long as it is general-purpose)\\n- Frontend and backend can be delivered independently (and hence faster) from each other\\n- Provides greater level of interactivity\\n#### Cons\\n- More pipelines need to be maintained in CI\\n- Compatibility issues between the frontend and backend may be detected later, during the integration stage\\n- Complete client-side rendering may degrade the user experience a bit\\n","Decision":"We will go for the SPA approach\\n","tokens":309,"id":1039,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nThe Metadata API provides a modern Python API for accessing individual pieces\\nof metadata. It does not provide any wider context help to someone looking to\\nimplement a TUF repository.\\nThe legacy python-tuf implementation offers tools for this but suffers from\\nsome issues (as do many other implementations):\\n* There is a _very_ large amount of code to maintain: repo.py,\\nrepository_tool.py and repository_lib.py alone are almost 7000 lines of code.\\n* The \"library like\" parts of the implementation do not form a good coherent\\nAPI: methods routinely have a large number of arguments, code still depends\\non globals in a major way and application (repo.py) still implements a lot of\\n\"repository code\" itself\\n* The \"library like\" parts of the implementation make decisions that look like\\napplication decisions. As an example, repository_tool loads _every_ metadata\\nfile in the repository: this is fine for CLI that operates on a small\\nrepository but is unlikely to be a good choice for a large scale server.\\n## Decision Drivers\\n* There is a consensus on removing the legacy code from python-tuf due to\\nmaintainability issues\\n* Metadata API makes modifying metadata far easier than legacy code base: this\\nmakes significantly different designs possible\\n* Not providing a \"repository library\" (and leaving implementers on their own)\\nmay be a short term solution because of the previous point, but to make\\nadoption easier and to help adopters create safe implementations the project\\nwould benefit from some shared repository code and a shared repository design\\n* Maintainability of new library code must be a top concern\\n* Allowing a wide range of repository implementations (from CLI tools to\\nminimal in-memory implementations to large scale application servers)\\nwould be good: unfortunately these can have wildly differing requirements\\n\\n## Decision\\n* There is a consensus on removing the legacy code from python-tuf due to\\nmaintainability issues\\n* Metadata API makes modifying metadata far easier than legacy code base: this\\nmakes significantly different designs possible\\n* Not providing a \"repository library\" (and leaving implementers on their own)\\nmay be a short term solution because of the previous point, but to make\\nadoption easier and to help adopters create safe implementations the project\\nwould benefit from some shared repository code and a shared repository design\\n* Maintainability of new library code must be a top concern\\n* Allowing a wide range of repository implementations (from CLI tools to\\nminimal in-memory implementations to large scale application servers)\\nwould be good: unfortunately these can have wildly differing requirements\\nOption 3: Minimal repository abstraction\\nWhile option 1 might be used temporarily, the goal should be to implement a\\nminimal repository abstraction as soon as possible: this should give the\\nproject a path forward where the maintenance burden is reasonable and results\\nshould be usable very soon. The python-tuf repository functionality can be\\nlater extended as ideas are experimented with in upstream projects and in\\npython-tuf example code.\\nThe concept is still unproven but validating the design should be straight\\nforward: decision could be re-evaluated in a few months if not in weeks.\\n\\n\\n## Context\\nThis service will need to process applications automatically and periodically in order to enforce time-dependent business rules and modify application state. Therefore, we need a way to run domain logic code outside the context of web requests, reliably and in a proactive\/scheduled way.\\n### Findings\\nWe have considered:\\n- Find-style: run jobs via the infrastructure (they use sidekiq\/sidekiq\\\\_cron)\\n- clockwork gem on its own\\n- some sort of secure API endpoint we can hit at intervals to trigger the sweep\\nConsidering the ability to enforce time-dependent business rules is core functionality\\nfor the service, we have decided to follow best practices and implement scheduling\\nand background processing properly, using a similar approach to Find, but using\\nclockwork instead of sidekiq\\\\_cron.\\n#### Pros\\n- The sidekiq\/clockwork combination is a proven, stable and scalable combination which can take us all the way to public launch and beyond.\\n- Code scheduling and organisation will be transparent, as the schedule is defined within the source code of the app and calls standard Rails workers and services. This also gives us an update path for schedules and background tasks, re-using the standard deployment pipelines, as well as auditing information through git version control.\\n- Because background processing will be handled by Sidekiq, failed jobs can be retried automatically.\\n#### Cons\\n- Requires additional Azure work: support multiple containers\/services (web\/worker\/clock) and a Redis instance.\\n\\n## Decision\\nWe will add the following capabilities to the app:\\n#### Scheduling\\nThis will be achieved via the ```clockwork``` gem. It alllows periodic jobs to be defined in a ruby file within the main Rails app. For example:\\n```ruby\\n# config\/clock.rb\\nclass Clock\\ninclude Clockwork\\nevery(1.minute,  \\'SayHello\\') { Rails.logger.info \"hi!\" }\\nend\\n```\\nClockwork requires the scheduling daemon to be run as a separate process to the Rails server, but it uses the same codebase, so all models\/services\/workers defined in Rails are available to be triggered. This extra process can be started like this: ```bundle exec clockwork .\/config\/clock.rb```\\n**Note** While many Rails server instances may be run in a scaling out\/load-balancing scenario, there must only always be ONLY ONE clockwork process, otherwise tasks will be triggered multiple times. Furthermore, in production this process should be supervised and restarted, if needed.\\nWhile it is technically possible to perform any kind of processing within the \\'clock\\' process, this is to be avoided, as any application errors could terminate the scheduler and prevent other tasks from running. Good practice suggests the \\'clock\\' process should only be used to trigger\/enqueue tasks, which are then processed within the context of a background processing system. That\\'s why config\/clock.rb should be a list of times and Sidekiq worker perform_async statements.\\n#### Background processing\\nWe will use the ```sidekiq``` gem, which is the current standard for Rails background processing. This also needs to be run as a separate process (e.g. ```bundle exec sidekiq -c 5 -C sidekiq.yml```, where 5 is a concurrency setting), but it also introduces an infrastructure requirement for Redis.\\nWorkers are usually placed within ```app\/workers``` and can call any other classes within the Rails app, such as service objects to achieve their goals.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nAs part of the functional requirements, we need to have a means of managing the content through an intuitive web\\\\ninterface. There are a couple of options to achieve this\\\\n### 1. As a \"traditional\" web application in the MVC architectural style, embedded within the backend (monolith)\\\\n#### Pros\\\\n- Server-side rendering sometimes can provide better user experience since content is immediately visible once\\\\nthe page is loaded\\\\n- Compatibility issues between the frontend and backend can be detected at the earlier stage\\\\n#### Cons\\\\n- Tools and libraries for the backend and frontend are mixed in the same project, making it a bit messy and mentally harder to grasp\\\\n- Strong coupling between web layer and backend discourages us to design general-purpose API for other types of potential consumers\\\\n- Provides limited level of interactivity\\\\n### 2. As a Single Page Application in the MVVM architectural style, packaged and deployed separately from the backend\\\\n#### Pros\\\\n- Frontend is more decoupled from backend technologies since they interact with each other via API\\\\n- Backend API can be potentially used by other consumers too: CLI, Native & Mobile (as long as it is general-purpose)\\\\n- Frontend and backend can be delivered independently (and hence faster) from each other\\\\n- Provides greater level of interactivity\\\\n#### Cons\\\\n- More pipelines need to be maintained in CI\\\\n- Compatibility issues between the frontend and backend may be detected later, during the integration stage\\\\n- Complete client-side rendering may degrade the user experience a bit\\\\n'}, {'role': 'model', 'content': 'We will go for the SPA approach\\\\n'}]"}
{"File Name":"architectural-decision-log\/0004-provide-error-codes-on-handled-exception.md","Context":"## Context and Problem Statement\\nHamlet Deploy is not a compiled binary file nor is it written in a single programming or scripting language. It is a combination of all of the above;\\n* Java for the Freemarker wrapper,\\n* Freemarker has its own DSL,\\n* the original Hamlet Deploy Executor is presently primarily Bash scripts and\\n* the \"Click\" CLI Executor is written in Python.\\nWhen a handled exception occurs, there is no consistency in the error information provided. A handled error in Freemarker is likely to provide a large JSON object for review, whilst an error in the Bash Executor will provide a brief summary of the problem. Because these summaries are not always unique-per-error, many provide only a generic error message.\\n","Decision":"Chosen option: introduce logging classes, that group together messages of a simmilar kind. Later we will re-evaluate to determine if a more specific approach to error handling (such as individual error codes outlined above) would remain useful \/ necessary and if so can build upon this framework.\\n### Manually-maintained library of error codes\\n* Good - will provide a method for providing tailored error messages and possible fixes to the user\\n* Good - could be extended to provide an avenue for review of the most-encountered error messages\\n* Good - improved reporting \/ communication around errors experienced\\n* Good - consistency across the spectrum of potential error sources\\n* Bad - increase in maintenance overhead\\n* Bad - may be a challenge to enforce consistently\\n* Bad - difficult to make extensible by community content\\n### Logging classes\\n* Good - provide a method for tailoring messages, whilst allowing for re-usability\\n* Good - accessible to community content\\n* Good - consistency across the spectrum of potential error sources\\n* Good - framework could be extended \/ upgraded at a later time\\n* Good - minor maintenance overhead once established\\n* Bad - less specificity is possible when compared to unique error messages\\n","tokens":151,"id":2979,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nHamlet is currently built on two application stacks, a set of bash scripts which run deployments and invoke the generation of outputs, and a freemarker wrapper with supporting templates which handles the creation of outputs. There is a tight coupling between these components and it is difficult to determine the authoritative source of context which we share between the these to stacks.\\n## Decision Drivers\\n* To improve user experience we have introduced a cli tool which is based on the [click library](https:\/\/click.palletsprojects.com) which is built in python. Integrating this cli into the environment has highlighted the need for this cli tool but also exposed the tight coupling between these two application stacks\\n\\n## Decision\\n* To improve user experience we have introduced a cli tool which is based on the [click library](https:\/\/click.palletsprojects.com) which is built in python. Integrating this cli into the environment has highlighted the need for this cli tool but also exposed the tight coupling between these two application stacks\\nChosen option: \"Create a clearly defined abstraction layer between the existing bash scripts and the freemarker wrapper\"\\nMakes our architecture cleaner and provides a clear way for hamlet to scale in the future\\n### Positive Consequences\\n* We have more explicit definitions of what hamlet is made of and how it works\\n* We can make choices on appropriate tools for specific services within hamlet\\n### Negative Consequences\\n* Defining and building the layer separation will add complexity to hamlet\\n\\n\\n## Context\\nSerializing and deserializing JSON in Java works best when performing it to\/from plain old java objects (POJOs).\\nWhile it is possible to serialize into generic `JsonNode` or `Map` objects this results in untyped values that\\nmust be referenced via String keys, meaning the compiler is limited in what it can check for us.\\nThe classes required for JSON serialization and deserialization are reptitive and error prone to write by hand.\\n[Immutables](https:\/\/immutables.github.io\/) helps to some extent, but there are still many classes and fields to be written.\\nThe [Discord API documentation](https:\/\/discordapp.com\/developers\/docs\/intro) is mostly consistent with how it documents\\nthe JSON structures it returns and accepts.\\nCode generation is often difficult to use with IDEs.\\nCode is often referring to classes that will not be committed, so we depend upon the IDE to generate these classes\\nso that the IDE can provide useful feedback regarding compilation errors.\\n\\n## Decision\\nWe will use code generation to generate the POJOs to be used for JSON serialization and deserialization.\\nWe will generate classes that make use of Immutables.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\nHamlet Deploy is not a compiled binary file nor is it written in a single programming or scripting language. It is a combination of all of the above;\\\\n* Java for the Freemarker wrapper,\\\\n* Freemarker has its own DSL,\\\\n* the original Hamlet Deploy Executor is presently primarily Bash scripts and\\\\n* the \"Click\" CLI Executor is written in Python.\\\\nWhen a handled exception occurs, there is no consistency in the error information provided. A handled error in Freemarker is likely to provide a large JSON object for review, whilst an error in the Bash Executor will provide a brief summary of the problem. Because these summaries are not always unique-per-error, many provide only a generic error message.\\\\n'}, {'role': 'model', 'content': 'Chosen option: introduce logging classes, that group together messages of a simmilar kind. Later we will re-evaluate to determine if a more specific approach to error handling (such as individual error codes outlined above) would remain useful \/ necessary and if so can build upon this framework.\\\\n### Manually-maintained library of error codes\\\\n* Good - will provide a method for providing tailored error messages and possible fixes to the user\\\\n* Good - could be extended to provide an avenue for review of the most-encountered error messages\\\\n* Good - improved reporting \/ communication around errors experienced\\\\n* Good - consistency across the spectrum of potential error sources\\\\n* Bad - increase in maintenance overhead\\\\n* Bad - may be a challenge to enforce consistently\\\\n* Bad - difficult to make extensible by community content\\\\n### Logging classes\\\\n* Good - provide a method for tailoring messages, whilst allowing for re-usability\\\\n* Good - accessible to community content\\\\n* Good - consistency across the spectrum of potential error sources\\\\n* Good - framework could be extended \/ upgraded at a later time\\\\n* Good - minor maintenance overhead once established\\\\n* Bad - less specificity is possible when compared to unique error messages\\\\n'}]"}
{"File Name":"verify-event-store-schema\/0002-database-migrations-are-standalone-releases.md","Context":"## Context\\nDatabase schema migrations are often considered as part of an application change -\\n\"in order to do X we will need a new index Y\"\\nHowever, this does not work well with a zero-downtime approach, when we have\\na single replicated database - we can't deploy version 0.2 of the application\\nto some servers simultaneously with version 0.2 of the schema to their databases;\\nwe need to deploy changes to the database either before or after the\\ncorresponding application changes\\n","Decision":"Database schema changes should be made independently of application changes.\\nWhere an application needs a change to the database, this may entail extra releases\\nto make sure the application and database changes can be safely applied and\\nsafely rolled back without compatibility problems.\\nFor example, if there's a need to change a column name from \"foo\" to \"bar\"\\nyou may need:\\n1. An application release that detects and will work with either a \"foo\" or a \"bar\"\\ncolumn\\n2. A database schema release that renames the column from \"foo\" to \"bar\"\\n3. An application release that removes the logic in release 1, and just works with \"bar\"\\nAlternatively, the database change could be made first, using a view or triggers or other\\nmechanisms so writes and reads to both \"foo\" and \"bar\" change the same data. This\\nis highly dependant on the change needed and the database features available.\\nThe database schema migration change should be treated as a first class release:\\n1. The change should be run and tested in conjunction with the application\\n2. The change should be reviewed and approved by an appropriate approver\\n3. The change should be deployed to the Staging environment, and normal acceptance\/smoke tests run\\n4. The change should be deployed to Production\/DR\\n### A note on backups\\nDepending on the complexity of the change, you may wish to coordinate a backup snapshot\\nof the database before running a migration.  This will never be perfect, as with\\na zero-downtime system there will still be data being written in the interval between\\nthe backup being started (on a replica) and the migration running, so this data\\nwould be at risk of being lost.\\nMany database migrations however are totally safe and should not need a backup -\\nfor instance, adding an index or adding a column is a very low risk change.  Nightly\\nbackups should be enough to mitigate against any risk with this sort of change.\\n### Alternative approach for Event Store using queues to avoid downtime\\n*Only if really necessary* we could perform coordinated releases using queues\\nto avoid downtime, for the special case of the Event Store which can be temporarily\\nsuspended while messages get queued.\\nThe approach would be similar to:\\n1. Modify the event recorder Lambda timing so the event recorder does not run, or point it to a test queue instead\\n2. Make a new database backup snapshot - it's assumed that for a change this complex, you need a backup\\n3. Deploy the database change\\n4. Test the change\\n5. Re-enable the Lambda\\nNote that the SQS queue has a limit of 100,000 queued messages - at peak we have\\nhistorically received around 75,000 messages an hour.  So this technique is quite time constrained;\\nif anything goes wrong you only have a small amount of time to fix it.\\n","tokens":109,"id":316,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe needed to test parallel login of users to ensure that\\nsession data isn\\'t accidentally shared between users\\n(see [decision 0002](.\/0002-custom-save-handler-in-service-front.md)).\\nAlso, Make an LPA currently has no load tests. These are useful\\nfor capacity planning and finding bottlenecks which may be\\ncausing errors for users on the site. While we are auto-scaling,\\nwe don\\'t have a way to verify that it is sufficient to manage\\nexpected load, which load testing can provide.\\nLoad testing can also provide a benchmark we can use to ensure\\nthat any new work we do does not degrade performance of the stack\\nas a whole.\\n\\n## Decision\\nUse [locust.io](https:\/\/locust.io\/) to test parallel logins.\\nWhile there are alternatives to locust, it is implemented in Python\\n(our chosen language going forward), has an intuitive API, and some\\nmembers of the team have experience with it already (and can reuse\\npreviously-written code).\\nLoad tests will be added to the tests\/load directory in the project,\\nas they are not component-specific and apply to the whole stack.\\nDoing the above has the happy side effect of opening up the possibility of\\nload testing the whole application stack in future.\\nInitially, this will only run locally and is not integrated into\\nthe CI pipeline. We will consider extending this testing into CI\\nin future.\\n\\n\\n# Context\\nUsers _connect_ to a datasource and import their data into the system.\\nThe original idea (inspired by Kibana) was to use ElasticSearch and\\nits DSL to compute aggregations on-the-fly.\\nAfter some iterations we realized that the majority of the imported\\ndata will be _tabular_,that is, rows and columns. This shape of\\ndata fits well in the relational model (database tables).\\nWe also realized that the transformations will be something that\\nwill be computed _ahead of time_ and the resulting values will\\nbe persisted.\\nAnother argument for using the same datastore for metadata and\\ndataset data is to reduce operational complexity.\\n# Decision\\nWe\\'ll use PostgreSQL to store dataset metadata and imported data.\\n# Status\\nAccepted\\n# Consequences\\n* We need to have a highly available PostgreSQL installation in\\nproduction (we can rely on hosted solutions if needed)\\n\\n## Decision\\nWe\\'ll use PostgreSQL to store dataset metadata and imported data.\\n# Status\\nAccepted\\n# Consequences\\n* We need to have a highly available PostgreSQL installation in\\nproduction (we can rely on hosted solutions if needed)\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nDatabase schema migrations are often considered as part of an application change -\\\\n\"in order to do X we will need a new index Y\"\\\\nHowever, this does not work well with a zero-downtime approach, when we have\\\\na single replicated database - we can\\'t deploy version 0.2 of the application\\\\nto some servers simultaneously with version 0.2 of the schema to their databases;\\\\nwe need to deploy changes to the database either before or after the\\\\ncorresponding application changes\\\\n'}, {'role': 'model', 'content': 'Database schema changes should be made independently of application changes.\\\\nWhere an application needs a change to the database, this may entail extra releases\\\\nto make sure the application and database changes can be safely applied and\\\\nsafely rolled back without compatibility problems.\\\\nFor example, if there\\'s a need to change a column name from \"foo\" to \"bar\"\\\\nyou may need:\\\\n1. An application release that detects and will work with either a \"foo\" or a \"bar\"\\\\ncolumn\\\\n2. A database schema release that renames the column from \"foo\" to \"bar\"\\\\n3. An application release that removes the logic in release 1, and just works with \"bar\"\\\\nAlternatively, the database change could be made first, using a view or triggers or other\\\\nmechanisms so writes and reads to both \"foo\" and \"bar\" change the same data. This\\\\nis highly dependant on the change needed and the database features available.\\\\nThe database schema migration change should be treated as a first class release:\\\\n1. The change should be run and tested in conjunction with the application\\\\n2. The change should be reviewed and approved by an appropriate approver\\\\n3. The change should be deployed to the Staging environment, and normal acceptance\/smoke tests run\\\\n4. The change should be deployed to Production\/DR\\\\n### A note on backups\\\\nDepending on the complexity of the change, you may wish to coordinate a backup snapshot\\\\nof the database before running a migration.  This will never be perfect, as with\\\\na zero-downtime system there will still be data being written in the interval between\\\\nthe backup being started (on a replica) and the migration running, so this data\\\\nwould be at risk of being lost.\\\\nMany database migrations however are totally safe and should not need a backup -\\\\nfor instance, adding an index or adding a column is a very low risk change.  Nightly\\\\nbackups should be enough to mitigate against any risk with this sort of change.\\\\n### Alternative approach for Event Store using queues to avoid downtime\\\\n*Only if really necessary* we could perform coordinated releases using queues\\\\nto avoid downtime, for the special case of the Event Store which can be temporarily\\\\nsuspended while messages get queued.\\\\nThe approach would be similar to:\\\\n1. Modify the event recorder Lambda timing so the event recorder does not run, or point it to a test queue instead\\\\n2. Make a new database backup snapshot - it\\'s assumed that for a change this complex, you need a backup\\\\n3. Deploy the database change\\\\n4. Test the change\\\\n5. Re-enable the Lambda\\\\nNote that the SQS queue has a limit of 100,000 queued messages - at peak we have\\\\nhistorically received around 75,000 messages an hour.  So this technique is quite time constrained;\\\\nif anything goes wrong you only have a small amount of time to fix it.\\\\n'}]"}
{"File Name":"rotc\/0006-use-standalone-jaeger.md","Context":"## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n","Decision":"The change that we're proposing or have agreed to implement.\\n","tokens":21,"id":3541,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\\n## Decision\\nThe change that we're proposing or have agreed to implement.\\n\\n\\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\\n## Decision\\nThe change that we're proposing or have agreed to implement.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\\\n\"}, {'role': 'model', 'content': \"The change that we're proposing or have agreed to implement.\\\\n\"}]"}
{"File Name":"terraform-provider-harbor\/0001-use-swagger-for-generate-http-client.md","Context":"## Context and Problem Statement\\nFor a quick development from the Terraform Provider Functions, it makes sense to generate or use a HarborRestAPI Client.\\n","Decision":"Chosen option: \"Swagger Based\", because this solution supports the fastes development Start without writting any boilerplate code.\\n### Positive Consequences\\n* No Painfull HTTP Client Implementation\\n### Negative Consequences\\n* the API Client Implementation dependents to the Swagger Spec Quality...\\n","tokens":31,"id":595,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThis application will need to present a user interface based on the result of multiple API calls.\\nThe majority of client side developers at the Guardian have some React experience, and it is succesfully used in several projects.\\n\\n## Decision\\nWe will build the front end in React.\\n\\n\\n## Context and Problem Statement\\nRight now, JabRef does not have support for Biblatex-Software out of the box, users have to add custom entry types.\\nWith citing software becoming fairly common, native support is helpful.\\n## Decision Drivers\\n* None of the existing flows should be impacted\\n\\n## Decision\\n* None of the existing flows should be impacted\\nChosen option: \"Add a new divider\", because comes out best (see below).\\n### Positive Consequences\\n* Inbuilt coverage for a entry type that is getting more and more importance\\n### Negative Consequences\\n* Adds a little bit more clutter to the Add Entry pane\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\nFor a quick development from the Terraform Provider Functions, it makes sense to generate or use a HarborRestAPI Client.\\\\n'}, {'role': 'model', 'content': 'Chosen option: \"Swagger Based\", because this solution supports the fastes development Start without writting any boilerplate code.\\\\n### Positive Consequences\\\\n* No Painfull HTTP Client Implementation\\\\n### Negative Consequences\\\\n* the API Client Implementation dependents to the Swagger Spec Quality...\\\\n'}]"}
{"File Name":"corona-hackathon\/0002-hexagonal-architecture.md","Context":"## Context\\nWe have to structure our backend into a certain kind of architecture.\\n","Decision":"We stick to the [Hexagonal \/ Ports & Adapter](https:\/\/softwarecampament.wordpress.com\/portsadapters\/) architecture.\\n","tokens":17,"id":3660,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe have to decide how to structure our backend code.\\n\\n## Decision\\nFor each [bounded context](https:\/\/codeburst.io\/ddd-strategic-patterns-how-to-define-bounded-contexts-2dc70927976e) we stick to the structure as proposed by [T. Homberg outstanding book](https:\/\/reflectoring.io\/book\/):\\n```\\n.\\n|-- adapter\\n|   |-- in\\n|   `-- out\\n|-- application\\n|   |-- port\\n|   |   |-- in\\n|   |   `-- out\\n|   `-- service\\n`-- domain\\n```\\n\\n\\n## Context\\nWe need a way to store secrets that are used by our application.\\n\\n## Decision\\nWe are going to use AWS SSM. We will store our secrets as Secure Strings.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe have to structure our backend into a certain kind of architecture.\\\\n'}, {'role': 'model', 'content': 'We stick to the [Hexagonal \/ Ports & Adapter](https:\/\/softwarecampament.wordpress.com\/portsadapters\/) architecture.\\\\n'}]"}
{"File Name":"devops-challenge\/0006-feat-add-autoscaling-policy.md","Context":"## Context\\nGoal: The platform must be scalable according to the load\\nWe need to autoscale nodes in case we need to cope with a high load.\\nI have researching, and I have found that we have to scale the nodes\\nas well as the pods.\\nFor some loads, scaling pods can be enough, but for other workloads it\\ncan be better to scale the cluster nodes.\\nIssues then:\\n1. Autoscaling pods\\n2. Autoscaling nodes\\n","Decision":"For the first issue, I'm going to use:\\n* [Horizontal Pod Autoscaler](https:\/\/kubernetes.io\/docs\/tasks\/run-application\/horizontal-pod-autoscale\/)\\nThis object is a controller and an API resource that is included in kubectl.  It\\nallows to set a period of time to check the load of a pod (cpu or custom metrics)\\nand increase the number of pods running.\\nFor the second issue, I'm going to use:\\n* [Cluster Autoscaler Addon](https:\/\/github.com\/kubernetes\/kops\/tree\/master\/addons\/cluster-autoscaler)\\nWith this addon, you can set a policy in AWS IAM and attach it to the previously\\ndefined autoscaling group for nodes.\\nI haven't found another way to set the AutoScale of the nodes instanceGroup\\n","tokens":100,"id":3479,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe first architecture considered was using the PaaS.\\nThis tool collects data about misconfigurations in a\\ndatabase. Since the data held is quite sensitive we\\nwanted to take all reasonable measures to protect that\\ndata.\\nThere are a lot of advantages to using the PaaS in that\\nit limits the amount of work required for common\\noperational tasks like deployment and monitoring.\\n\\n## Decision\\nAt the present time we felt the Paas was not a viable\\noption for this tool because of the following:\\n* Shared tenancy RDS\\n* Shared tenancy VPC\\n* No ability to natively control ingress\\n* No ability to control egress\\n__Some of these issues are being addressed by the PaaS so\\nwe may revisit this decision in future.__\\n\\n\\n## Context and Problem Statement\\nThe project is growing with more features\/systems and some of them may be not compatible together\\nlike for example builder and avatars moving through comns. Also, some scenes are starting\\nto be perfomance-intensive. If the user want to give all the resources to a scene to speed it up,\\nit should be able to do it. In order to try to tackle this problem, we need a way to disconnect\\nfeatures\/systems and scenes\\n\\n## Decision\\nWe went for the option 1: Isolated modes because this way kernel and the renderer will be decoupled and we\\nstill can have control about the systems\/scenes that are enabled\/disabled\\n<!--\\n```sequence\\nparticipant Unity as R\\nparticipant Kernel as K\\nparticipant Scene worker as W\\nnote over R: Start isolated mode\\nR->K: enter isolated mode(mode)\\nK-->K: Enable\/disable required systems\\nK->R: Enable\/disable required systems\\nK-->W: Create new worker (only if necessary)\\nK->R: LoadParcelScenes(sceneId) (only if necessary)\\n-->\\n![resources\/fig-initial-load.svg](resources\/fig-initial-load.svg)\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nGoal: The platform must be scalable according to the load\\\\nWe need to autoscale nodes in case we need to cope with a high load.\\\\nI have researching, and I have found that we have to scale the nodes\\\\nas well as the pods.\\\\nFor some loads, scaling pods can be enough, but for other workloads it\\\\ncan be better to scale the cluster nodes.\\\\nIssues then:\\\\n1. Autoscaling pods\\\\n2. Autoscaling nodes\\\\n'}, {'role': 'model', 'content': \"For the first issue, I'm going to use:\\\\n* [Horizontal Pod Autoscaler](https:\/\/kubernetes.io\/docs\/tasks\/run-application\/horizontal-pod-autoscale\/)\\\\nThis object is a controller and an API resource that is included in kubectl.  It\\\\nallows to set a period of time to check the load of a pod (cpu or custom metrics)\\\\nand increase the number of pods running.\\\\nFor the second issue, I'm going to use:\\\\n* [Cluster Autoscaler Addon](https:\/\/github.com\/kubernetes\/kops\/tree\/master\/addons\/cluster-autoscaler)\\\\nWith this addon, you can set a policy in AWS IAM and attach it to the previously\\\\ndefined autoscaling group for nodes.\\\\nI haven't found another way to set the AutoScale of the nodes instanceGroup\\\\n\"}]"}
{"File Name":"tendermint\/adr-037-deliver-block.md","Context":"## Context\\nInitial conversation: https:\/\/github.com\/tendermint\/tendermint\/issues\/2901\\nSome applications can handle transactions in parallel, or at least some\\npart of tx processing can be parallelized. Now it is not possible for developer\\nto execute txs in parallel because Tendermint delivers them consequentially.\\n","Decision":"Now Tendermint have `BeginBlock`, `EndBlock`, `Commit`, `DeliverTx` steps\\nwhile executing block. This doc proposes merging this steps into one `DeliverBlock`\\nstep. It will allow developers of applications to decide how they want to\\nexecute transactions (in parallel or consequentially). Also it will simplify and\\nspeed up communications between application and Tendermint.\\nAs @jaekwon [mentioned](https:\/\/github.com\/tendermint\/tendermint\/issues\/2901#issuecomment-477746128)\\nin discussion not all application will benefit from this solution. In some cases,\\nwhen application handles transaction consequentially, it way slow down the blockchain,\\nbecause it need to wait until full block is transmitted to application to start\\nprocessing it. Also, in the case of complete change of ABCI, we need to force all the apps\\nto change their implementation completely. That's why I propose to introduce one more ABCI\\ntype.\\n# Implementation Changes\\nIn addition to default application interface which now have this structure\\n```go\\ntype Application interface {\\n\/\/ Info and Mempool methods...\\n\/\/ Consensus Connection\\nInitChain(RequestInitChain) ResponseInitChain    \/\/ Initialize blockchain with validators and other info from TendermintCore\\nBeginBlock(RequestBeginBlock) ResponseBeginBlock \/\/ Signals the beginning of a block\\nDeliverTx(tx []byte) ResponseDeliverTx           \/\/ Deliver a tx for full processing\\nEndBlock(RequestEndBlock) ResponseEndBlock       \/\/ Signals the end of a block, returns changes to the validator set\\nCommit() ResponseCommit                          \/\/ Commit the state and return the application Merkle root hash\\n}\\n```\\nthis doc proposes to add one more:\\n```go\\ntype Application interface {\\n\/\/ Info and Mempool methods...\\n\/\/ Consensus Connection\\nInitChain(RequestInitChain) ResponseInitChain           \/\/ Initialize blockchain with validators and other info from TendermintCore\\nDeliverBlock(RequestDeliverBlock) ResponseDeliverBlock  \/\/ Deliver full block\\nCommit() ResponseCommit                                 \/\/ Commit the state and return the application Merkle root hash\\n}\\ntype RequestDeliverBlock struct {\\nHash                 []byte\\nHeader               Header\\nTxs                  Txs\\nLastCommitInfo       LastCommitInfo\\nByzantineValidators  []Evidence\\n}\\ntype ResponseDeliverBlock struct {\\nValidatorUpdates      []ValidatorUpdate\\nConsensusParamUpdates *ConsensusParams\\nTags                  []kv.Pair\\nTxResults             []ResponseDeliverTx\\n}\\n```\\nAlso, we will need to add new config param, which will specify what kind of ABCI application uses.\\nFor example, it can be `abci_type`. Then we will have 2 types:\\n- `advanced` - current ABCI\\n- `simple` - proposed implementation\\n","tokens":67,"id":1976,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nInitial conversation: https:\/\/github.com\/tendermint\/tendermint\/issues\/2901\\nSome applications can handle transactions in parallel, or at least some\\npart of tx processing can be parallelized. Now it is not possible for developer\\nto execute txs in parallel because Tendermint delivers them consequentially.\\n\\n## Decision\\nNow Tendermint have `BeginBlock`, `EndBlock`, `Commit`, `DeliverTx` steps\\nwhile executing block. This doc proposes merging this steps into one `DeliverBlock`\\nstep. It will allow developers of applications to decide how they want to\\nexecute transactions (in parallel or consequentially). Also it will simplify and\\nspeed up communications between application and Tendermint.\\nAs @jaekwon [mentioned](https:\/\/github.com\/tendermint\/tendermint\/issues\/2901#issuecomment-477746128)\\nin discussion not all application will benefit from this solution. In some cases,\\nwhen application handles transaction consequentially, it way slow down the blockchain,\\nbecause it need to wait until full block is transmitted to application to start\\nprocessing it. Also, in the case of complete change of ABCI, we need to force all the apps\\nto change their implementation completely. That's why I propose to introduce one more ABCI\\ntype.\\n# Implementation Changes\\nIn addition to default application interface which now have this structure\\n```go\\ntype Application interface {\\n\/\/ Info and Mempool methods...\\n\/\/ Consensus Connection\\nInitChain(RequestInitChain) ResponseInitChain    \/\/ Initialize blockchain with validators and other info from TendermintCore\\nBeginBlock(RequestBeginBlock) ResponseBeginBlock \/\/ Signals the beginning of a block\\nDeliverTx(tx []byte) ResponseDeliverTx           \/\/ Deliver a tx for full processing\\nEndBlock(RequestEndBlock) ResponseEndBlock       \/\/ Signals the end of a block, returns changes to the validator set\\nCommit() ResponseCommit                          \/\/ Commit the state and return the application Merkle root hash\\n}\\n```\\nthis doc proposes to add one more:\\n```go\\ntype Application interface {\\n\/\/ Info and Mempool methods...\\n\/\/ Consensus Connection\\nInitChain(RequestInitChain) ResponseInitChain           \/\/ Initialize blockchain with validators and other info from TendermintCore\\nDeliverBlock(RequestDeliverBlock) ResponseDeliverBlock  \/\/ Deliver full block\\nCommit() ResponseCommit                                 \/\/ Commit the state and return the application Merkle root hash\\n}\\ntype RequestDeliverBlock struct {\\nHash                 []byte\\nHeader               Header\\nTxs                  Txs\\nLastCommitInfo       LastCommitInfo\\nByzantineValidators  []Evidence\\n}\\ntype ResponseDeliverBlock struct {\\nValidatorUpdates      []ValidatorUpdate\\nConsensusParamUpdates *ConsensusParams\\nTags                  []kv.Pair\\nTxResults             []ResponseDeliverTx\\n}\\n```\\nAlso, we will need to add new config param, which will specify what kind of ABCI application uses.\\nFor example, it can be `abci_type`. Then we will have 2 types:\\n- `advanced` - current ABCI\\n- `simple` - proposed implementation\\n\\n\\n## Context\\nWe need a mechanism to allow Kubernetes manifest files to be applied to different\\nenvironments as currently we would have to manually update them in order to apply them to\\ndifferent environments. The options available are to either use\\n[Helm](https:\/\/github.com\/kubernetes\/helm) or to build a custom tool.\\n\\n## Decision\\nWe have decided to build a custom tool called Bara to deploy our templates. This will use\\nthe simple mustache rendering engine to allow template YAML files to be rendered and then\\napplied using the command line `kubectl` tool.\\nThis approach seemed easier than learning and deploying Helm and building Helm charts for\\neach independent component given our current requirements are very simple and the tool\\nwill only take a few hours to write.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nInitial conversation: https:\/\/github.com\/tendermint\/tendermint\/issues\/2901\\\\nSome applications can handle transactions in parallel, or at least some\\\\npart of tx processing can be parallelized. Now it is not possible for developer\\\\nto execute txs in parallel because Tendermint delivers them consequentially.\\\\n\"}, {'role': 'model', 'content': \"Now Tendermint have `BeginBlock`, `EndBlock`, `Commit`, `DeliverTx` steps\\\\nwhile executing block. This doc proposes merging this steps into one `DeliverBlock`\\\\nstep. It will allow developers of applications to decide how they want to\\\\nexecute transactions (in parallel or consequentially). Also it will simplify and\\\\nspeed up communications between application and Tendermint.\\\\nAs @jaekwon [mentioned](https:\/\/github.com\/tendermint\/tendermint\/issues\/2901#issuecomment-477746128)\\\\nin discussion not all application will benefit from this solution. In some cases,\\\\nwhen application handles transaction consequentially, it way slow down the blockchain,\\\\nbecause it need to wait until full block is transmitted to application to start\\\\nprocessing it. Also, in the case of complete change of ABCI, we need to force all the apps\\\\nto change their implementation completely. That's why I propose to introduce one more ABCI\\\\ntype.\\\\n# Implementation Changes\\\\nIn addition to default application interface which now have this structure\\\\n```go\\\\ntype Application interface {\\\\n\/\/ Info and Mempool methods...\\\\n\/\/ Consensus Connection\\\\nInitChain(RequestInitChain) ResponseInitChain    \/\/ Initialize blockchain with validators and other info from TendermintCore\\\\nBeginBlock(RequestBeginBlock) ResponseBeginBlock \/\/ Signals the beginning of a block\\\\nDeliverTx(tx []byte) ResponseDeliverTx           \/\/ Deliver a tx for full processing\\\\nEndBlock(RequestEndBlock) ResponseEndBlock       \/\/ Signals the end of a block, returns changes to the validator set\\\\nCommit() ResponseCommit                          \/\/ Commit the state and return the application Merkle root hash\\\\n}\\\\n```\\\\nthis doc proposes to add one more:\\\\n```go\\\\ntype Application interface {\\\\n\/\/ Info and Mempool methods...\\\\n\/\/ Consensus Connection\\\\nInitChain(RequestInitChain) ResponseInitChain           \/\/ Initialize blockchain with validators and other info from TendermintCore\\\\nDeliverBlock(RequestDeliverBlock) ResponseDeliverBlock  \/\/ Deliver full block\\\\nCommit() ResponseCommit                                 \/\/ Commit the state and return the application Merkle root hash\\\\n}\\\\ntype RequestDeliverBlock struct {\\\\nHash                 []byte\\\\nHeader               Header\\\\nTxs                  Txs\\\\nLastCommitInfo       LastCommitInfo\\\\nByzantineValidators  []Evidence\\\\n}\\\\ntype ResponseDeliverBlock struct {\\\\nValidatorUpdates      []ValidatorUpdate\\\\nConsensusParamUpdates *ConsensusParams\\\\nTags                  []kv.Pair\\\\nTxResults             []ResponseDeliverTx\\\\n}\\\\n```\\\\nAlso, we will need to add new config param, which will specify what kind of ABCI application uses.\\\\nFor example, it can be `abci_type`. Then we will have 2 types:\\\\n- `advanced` - current ABCI\\\\n- `simple` - proposed implementation\\\\n\"}]"}
{"File Name":"tdr-dev-documentation\/0023-judgment-transfer-urls-scheme.md","Context":"## Context\\nTDR application will support the upload of court judgments.\\nThe court judgment transfers require a different UI and functionality compared to standard record transfers.\\n","Decision":"To help support the court judgment transfers, it was decided to use a different url scheme for such transfers.\\nThe advantages of doing this are:\\n* providing a more descriptive url for court judgment transfer users.\\n* improving accessibility for court judgment transfers.\\n* providing a more isolated domain for court judgment transfers to future proof against further requirements.\\n* providing better support for any web analytics on TDR that are required.\\nThe advantages of introducing a different url scheme for court judgment transfers was felt to outweigh using the same url scheme for both types of transfers.\\n","tokens":31,"id":1782,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nIt is possible to install any dependencies for child services in this base\\nimage. Doing so increases the image size, but speeds up child image builds.\\n\\n## Decision\\nWe will install all common base dependencies in the base image. This includes:\\n* WSGI libraries\\n* Flask and related plugins\\n* Development requirements\\n\\n\\n## Context\\nThe ETL process takes a long time to run, approximately 6 hours.\\nRunning the ETL process makes close to 30,000 requests to the Syndication API.\\nSeveral applications need access to the JSON file created by the ETL.\\n\\n## Decision\\nRather than each application running its own copy of the ETL to obtain the Syndication data in JSON format,\\na single instance of the ETL will run and provide access to the resultant file via an nginx web server\\nrunning in the ETL container.\\nThe output JSON is hosted in the container to remove reliance on external solutions, such as Azure.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nTDR application will support the upload of court judgments.\\\\nThe court judgment transfers require a different UI and functionality compared to standard record transfers.\\\\n'}, {'role': 'model', 'content': 'To help support the court judgment transfers, it was decided to use a different url scheme for such transfers.\\\\nThe advantages of doing this are:\\\\n* providing a more descriptive url for court judgment transfer users.\\\\n* improving accessibility for court judgment transfers.\\\\n* providing a more isolated domain for court judgment transfers to future proof against further requirements.\\\\n* providing better support for any web analytics on TDR that are required.\\\\nThe advantages of introducing a different url scheme for court judgment transfers was felt to outweigh using the same url scheme for both types of transfers.\\\\n'}]"}
{"File Name":"js-sdk\/0011-nginx-expose-option.md","Context":"## Context\\nMinio solution listens for HTTP only. To safely expose it and use it for backups with restic for example, it needs to use HTTPS and use a valid certifcate.\\n","Decision":"Add option to solution expose chatflow to expose solutions using nginx reverse proxy to terminate ssl connections.\\n","tokens":40,"id":5193,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe service needs a way to build resilience against the external Contentful API becoming unexpectedly unavailable. This service is a single point of failure.\\ndxw use often use Rails and Redis together and recommend it as a technical choice.\\n\\n## Decision\\nAdd and use Redis for a read cache.\\n\\n\\n## Context and Problem Statement\\nExternal services must authorize web clients to the WebSocket API.\\nThe WebSocket API is stateless and not maintain a user Session with Cookies.\\nOnly little data should be stored for an open WebSocket connection.\\n## Decision Drivers\\n* decoupling of the authorization service and the WebSocket API\\n* flexible and well supported on many platforms\\n\\n## Decision\\n* decoupling of the authorization service and the WebSocket API\\n* flexible and well supported on many platforms\\nChosen option: \"JWT from pre-shared keys\", because the WebSocket API is loosely coupled and it is well supported on many platforms.\\n### Positive Consequences\\n* Simple to implement\\n* Authorization data can be send in a portable and verifiable way\\n### Negative Consequences\\n* The shared keys must be handled\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nMinio solution listens for HTTP only. To safely expose it and use it for backups with restic for example, it needs to use HTTPS and use a valid certifcate.\\\\n'}, {'role': 'model', 'content': 'Add option to solution expose chatflow to expose solutions using nginx reverse proxy to terminate ssl connections.\\\\n'}]"}
{"File Name":"register-a-food-business-front-end\/0005-implement-yarn-as-a-cli-for-node-package-management.md","Context":"## Context\\nYarn and NPM can both manage the Node packages for a project. Recent updates to NPM mean that Yarn only has a negligible performance advantage over NPM.\\n","Decision":"We will use `yarn`, `yarn start`, `yarn add`, `yarn remove` etc. for the management of Node packages in our project.\\n","tokens":38,"id":3042,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nThe Java implementation of the protocol uses `final static` properties to define\\nthe schema for requests and responses. These are defined using objects, making\\nit easy to support multiple versions for each API call.\\nPHP doesn't have this feature so we need decide how to solve this.\\n\\n## Decision\\nWe're still going to use objects to manipulate data and write\/read content\\nto\/from Kafka, however it would be simpler to use arrays in constants of each\\nrequest\/response class.\\nEach field would be an array item, the key would be the field name and the value\\nwould be the field type (or another array for more complex configuration).\\nThe mapping would like this:\\n```php\\nuse Lcobucci\\\\Kafka\\\\Protocol\\\\Type;\\nfinal class DoSomethingRequest\\n{\\nprivate const SCHEMAS = [\\n[\\n'error_code'   => Type\\\\Int16::class,\\n'api_versions' => [\\n'type'     => Type\\\\ArrayOf::class,\\n'nullable' => false, \/\/ optional, default = false\\n'items'    => [ \/\/ just type name if items don't have complex structure\\n'api_key'     => Type\\\\Int16::class,\\n'min_version' => Type\\\\Int16::class,\\n'max_version' => Type\\\\Int16::class,\\n],\\n],\\n],\\n];\\n}\\n```\\n\\n\\n## Context\\nWe need a log rotation for the nginx webserver. As the nginx does not support log rotation out of the box, we need our own solution.\\n\\n## Decision\\nWe implement the log rotation using the programm logrotate inside the docker container for the nginx webserver. We do not trigger the log rotation from outside of the docker container, because we want a solution that is independed from the host system.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nYarn and NPM can both manage the Node packages for a project. Recent updates to NPM mean that Yarn only has a negligible performance advantage over NPM.\\\\n\"}, {'role': 'model', 'content': 'We will use `yarn`, `yarn start`, `yarn add`, `yarn remove` etc. for the management of Node packages in our project.\\\\n'}]"}
{"File Name":"arch\/0006-replace-svn-with-git.md","Context":"## Context\\n\u5f53\u524d\u6211\u4eec\u7528\u7684\u662f SVN \u505a\u4ee3\u7801\u3001\u4ea7\u54c1\u6587\u6863\u3001UI \u8bbe\u8ba1\u56fe\u7684\u7ba1\u7406\uff0c\u5728\u6b64\u8bf4\u8bf4\u5176\u4e2d\u7684\u4ee3\u7801\u7ba1\u7406\\n1. \u4ee3\u7801\u4ed3\u5e93\u8fc7\u5927\uff0c\u65b0\u5206\u652f\uff0c\u65b0Tag \u4ee3\u7801\u90fd\u662f\u4e00\u4efd\u5b8c\u6574\u7684\u62f7\u8d1d;\\n2. \u5fc5\u987b\u8054\u7f51\u63d0\u4ea4\uff1b\\n3. SVN \u4ee3\u7801\u5408\u5e76\u65b9\u5f0f\u4f4e\u6548\uff0c\u76ee\u524d\u4f7f\u7528 Beyond Compare \u505a\u4ee3\u7801\u5408\u5e76\uff0c\u5206\u652f\u4f7f\u7528\u65b9\u5f0f\u843d\u540e\uff1b\\n4. \u65e0\u6cd5\u5f88\u597d\u7684\u505a code review\uff08\u53ea\u80fd patch \u6216\u7b2c\u4e09\u65b9\u5de5\u5177\uff09\uff1b\\n5. \u9762\u8bd5\u8005\u770b\u5230\u662f\u8fd9\u4e48\u843d\u540e\uff0c\u4ee5\u6b64\u7c7b\u522b\u5176\u4ed6\u6280\u672f\u6808\uff0c\u7efc\u5408\u7406\u89e3\u5c31\u662f\uff0c\u6211\u80fd\u5b66\u5230\u5565\u3002\\n","Decision":"\u4f7f\u7528\u5168\u7403\u6700\u6d41\u884c\u7684\u5206\u5e03\u5f0f\u7ba1\u7406\u5de5\u5177 Git \u53ca\u5e73\u53f0 Github\uff0c\u5176\u7279\u70b9\u4e3a\u5206\u5e03\u5f0f\uff0c\u76ee\u5f55\u7ed3\u6784\u7b80\u5355\uff0c\u4ee3\u7801\u65e0\u5197\u4f59\uff0c\u53ef review with PR\u3002\\n### \u65b9\u5f0f\u4e00\uff1a\\ngit svn clone `svn project url`\u00a0-T trunk\\n\u5c06 SVN \u9879\u76ee\u7684 trunk \u8f6c\u4e3a git \u9879\u76ee\u7684 master\uff0c\u4ec5\u4fdd\u7559\u4e86 trunk \u5206\u652f\u7684\u63d0\u4ea4\u8bb0\u5f55\uff0c\u6b64\u65b9\u5f0f\u9002\u7528\u4e8e\u6240\u6709\u4ee3\u7801\u90fd\u89c4\u6574\u5230\u4e86\u4e00\u4e2a\u5206\u652f\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u5176\u4ed6\u5206\u652f\u7684\u63d0\u4ea4\u8bb0\u5f55\u3002\\n### \u65b9\u5f0f\u4e8c\uff1a\\n\u4f7f\u7528\u547d\u4ee4 `https:\/\/github.com\/nirvdrum\/svn2git`\uff0c\u5b83\u53ef\u4ee5\u4fdd\u7559\u6240\u6709branch, tags\uff0c\u4ee5\u53ca\u6240\u6709\u5206\u652f\u7684\u63d0\u4ea4\u5386\u53f2\u3002\\nsvn2git http:\/\/svn.example.com\/path\/to\/repo --trunk trunk --tags tag --branches branch\\ngit push --all origin\\ngit push --tags\\nuse `--revision number` to reduce the commit history.\\n\u76ee\u524d\u751f\u4ea7\u73af\u5883\u4f7f\u7528\u7684 centos \u7248\u672c\u8fc7\u4f4e\uff0c\u5bfc\u81f4 git \u4e5f\u65e0\u6cd5\u5347\u7ea7\u7684\u5904\u7406\u65b9\u6cd5\uff1a\\nyum install http:\/\/opensource.wandisco.com\/centos\/6\/git\/x86\\_64\/wandisco-git-release-6-1.noarch.rpm\\nyum update git\\n","tokens":181,"id":2439,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\n1. \u5f53\u524d\u5b58\u50a8\u65b9\u6848\u6709 3 \u79cd\uff08FastDFS, nginx static, OSS\uff09\uff0c\u5404\u81ea\u5206\u522b\u7ef4\u62a4\uff0c\u4eba\u529b\u6210\u672c\u9ad8\uff0c\u90fd\u662f\u5355\u70b9\uff0c\u8d44\u6e90\u6709\u6548\u6027\u65e0\u6cd5\u4fdd\u969c\uff1b\\n2. \u6587\u4ef6\u5b58\u50a8\u548c\u4e1a\u52a1\u670d\u52a1\u5171\u4eab\u670d\u52a1\u5668\u8d44\u6e90\uff08CPU, \u5185\u5b58\uff0c\u5e26\u5bbd\uff0c\u78c1\u76d8 IO\uff09\uff0c\u670d\u52a1\u5668\u8d44\u6e90\u4f7f\u7528\u4e0d\u5408\u7406\uff0c\u6587\u4ef6\u670d\u52a1\u4f1a\u5360\u7528\u5927\u91cf\u5185\u5b58\u548c\u78c1\u76d8 IO \u53ca\u7f51\u7edc IO\uff0c\u5f71\u54cd\u4e1a\u52a1\uff0c\u5e76\u4e14\u4e1a\u52a1\u670d\u52a1\u65e0\u6cd5\u505a\u9ad8\u53ef\u7528\uff0c\u9047\u5230 DDOS \u53ea\u80fd\u50bb\u773c\uff1b\\n3. \u9762\u5411\u7528\u6237\u7684\u8d44\u6e90\u65e0\u6cd5\u505a\u52a0\u901f\uff1b\\n4. \u6240\u6709\u6587\u4ef6\u90fd\u662f\u516c\u5f00\u53ef\u8bbf\u95ee\uff0c\u5b58\u5728\u4e25\u91cd\u7684\u5b89\u5168\u95ee\u9898\uff08\u7528\u6237\u8eab\u4efd\u8bc1\u7167\u7247\u3001\u5408\u540c\u53ca\u62a5\u8868\u6570\u636e\u7684\u6cc4\u9732\uff09\u3002\\n5. \u6240\u6709\u654f\u611f\u4fe1\u606f\u53ef\u88ab\u4efb\u4f55\u4eba\u5168\u7f51\u4e0b\u8f7d\uff1b\\n\\n## Decision\\n1. \u5c06\u6240\u6709\u6587\u4ef6\u5b58\u50a8\u548c\u4e1a\u52a1\u673a\u5668\u5206\u79bb\uff0c\u5c06 FastDFS \u914d\u7f6e\u5355\u72ec\u673a\u5668\u63d0\u4f9b\u670d\u52a1\uff1b\\n2. \u6240\u6709\u9759\u6001\u6587\u4ef6\u96c6\u4e2d\u4e0a\u4e91\uff0c\u5f55\u97f3\uff0c\u7535\u5b50\u5408\u540c\u7b49\u9759\u6001\u6587\u4ef6\u4e0a\u4e91\uff1b\\n3. \u6587\u4ef6\u5206\u6743\u9650\u8bbf\u95ee\uff0c\u5206\u914d\u4e34\u65f6 token \u53bb\u83b7\u53d6\u6587\u4ef6\uff1b\\n4. css\/js\/images \u7b49\u4e0a\u4e91\u5e76\u914d\u7f6e CDN\uff0c\u52a0\u901f\u7528\u6237\u7aef\u8bbf\u95ee\u3002\\n\\n\\n## Context\\n\u5f53\u524d\u6211\u4eec\u4f7f\u7528\u7684\u662f\u7528\u6237\u540d\u3001\u5bc6\u7801\u65b9\u5f0f\u8fdb\u884c\u670d\u52a1\u5668\u767b\u5f55\uff0c\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\\n1. \u5b89\u5168\u6027\u95ee\u9898\uff0c\u5bc6\u7801\u9762\u4e34\u88ab\u66b4\u529b\u7834\u89e3\u7684\u98ce\u9669\uff1b\\n2. \u6613\u7528\u6027\u95ee\u9898\uff0c\u65e0\u6cd5\u4f7f\u7528 ssh config \u8bb0\u5f55\u5bc6\u7801\uff0c\u867d\u7136\u53ef\u4ee5\u4f7f\u7528\u7b2c\u4e09\u65b9\u8f6f\u4ef6\u89e3\u51b3\uff0c\u5982\uff0cSecureCRT\uff0cZOC7\uff0c\u4f46\u56e0\u4e3a\u4e0b\u8f7d\u6e20\u9053\u95ee\u9898\uff0c\u4e5f\u4f1a\u51fa\u73b0\u5b89\u5168\u98ce\u9669\uff1b\\n3. \u4f7f\u7528\u4e86\u7b2c\u4e09\u65b9\u8f6f\u4ef6\u5c31\u65e0\u6cd5\u5145\u5206\u4f7f\u7528 local terminal\uff0c\u5982 iTerm2\uff1b\\n\\n## Decision\\n![][image-1]\\n\u7981\u7528\u7528\u6237\u540d\u3001\u5bc6\u7801\u767b\u5f55\uff0c\u4f7f\u7528 ssh key \u8fdb\u884c\u767b\u5f55\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\n\u5f53\u524d\u6211\u4eec\u7528\u7684\u662f SVN \u505a\u4ee3\u7801\u3001\u4ea7\u54c1\u6587\u6863\u3001UI \u8bbe\u8ba1\u56fe\u7684\u7ba1\u7406\uff0c\u5728\u6b64\u8bf4\u8bf4\u5176\u4e2d\u7684\u4ee3\u7801\u7ba1\u7406\\\\n1. \u4ee3\u7801\u4ed3\u5e93\u8fc7\u5927\uff0c\u65b0\u5206\u652f\uff0c\u65b0Tag \u4ee3\u7801\u90fd\u662f\u4e00\u4efd\u5b8c\u6574\u7684\u62f7\u8d1d;\\\\n2. \u5fc5\u987b\u8054\u7f51\u63d0\u4ea4\uff1b\\\\n3. SVN \u4ee3\u7801\u5408\u5e76\u65b9\u5f0f\u4f4e\u6548\uff0c\u76ee\u524d\u4f7f\u7528 Beyond Compare \u505a\u4ee3\u7801\u5408\u5e76\uff0c\u5206\u652f\u4f7f\u7528\u65b9\u5f0f\u843d\u540e\uff1b\\\\n4. \u65e0\u6cd5\u5f88\u597d\u7684\u505a code review\uff08\u53ea\u80fd patch \u6216\u7b2c\u4e09\u65b9\u5de5\u5177\uff09\uff1b\\\\n5. \u9762\u8bd5\u8005\u770b\u5230\u662f\u8fd9\u4e48\u843d\u540e\uff0c\u4ee5\u6b64\u7c7b\u522b\u5176\u4ed6\u6280\u672f\u6808\uff0c\u7efc\u5408\u7406\u89e3\u5c31\u662f\uff0c\u6211\u80fd\u5b66\u5230\u5565\u3002\\\\n'}, {'role': 'model', 'content': '\u4f7f\u7528\u5168\u7403\u6700\u6d41\u884c\u7684\u5206\u5e03\u5f0f\u7ba1\u7406\u5de5\u5177 Git \u53ca\u5e73\u53f0 Github\uff0c\u5176\u7279\u70b9\u4e3a\u5206\u5e03\u5f0f\uff0c\u76ee\u5f55\u7ed3\u6784\u7b80\u5355\uff0c\u4ee3\u7801\u65e0\u5197\u4f59\uff0c\u53ef review with PR\u3002\\\\n### \u65b9\u5f0f\u4e00\uff1a\\\\ngit svn clone `svn project url`\\xa0-T trunk\\\\n\u5c06 SVN \u9879\u76ee\u7684 trunk \u8f6c\u4e3a git \u9879\u76ee\u7684 master\uff0c\u4ec5\u4fdd\u7559\u4e86 trunk \u5206\u652f\u7684\u63d0\u4ea4\u8bb0\u5f55\uff0c\u6b64\u65b9\u5f0f\u9002\u7528\u4e8e\u6240\u6709\u4ee3\u7801\u90fd\u89c4\u6574\u5230\u4e86\u4e00\u4e2a\u5206\u652f\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u5176\u4ed6\u5206\u652f\u7684\u63d0\u4ea4\u8bb0\u5f55\u3002\\\\n### \u65b9\u5f0f\u4e8c\uff1a\\\\n\u4f7f\u7528\u547d\u4ee4 `https:\/\/github.com\/nirvdrum\/svn2git`\uff0c\u5b83\u53ef\u4ee5\u4fdd\u7559\u6240\u6709branch, tags\uff0c\u4ee5\u53ca\u6240\u6709\u5206\u652f\u7684\u63d0\u4ea4\u5386\u53f2\u3002\\\\nsvn2git http:\/\/svn.example.com\/path\/to\/repo --trunk trunk --tags tag --branches branch\\\\ngit push --all origin\\\\ngit push --tags\\\\nuse `--revision number` to reduce the commit history.\\\\n\u76ee\u524d\u751f\u4ea7\u73af\u5883\u4f7f\u7528\u7684 centos \u7248\u672c\u8fc7\u4f4e\uff0c\u5bfc\u81f4 git \u4e5f\u65e0\u6cd5\u5347\u7ea7\u7684\u5904\u7406\u65b9\u6cd5\uff1a\\\\nyum install http:\/\/opensource.wandisco.com\/centos\/6\/git\/x86\\\\_64\/wandisco-git-release-6-1.noarch.rpm\\\\nyum update git\\\\n'}]"}
{"File Name":"easyvista_toolkit\/0003_Easyvista_locations_path.md","Context":"## Context\\nWe need to be able to assign a location when creating an employee and to have autocompletion that list all availables location on cmdlet.\\nBut there are issues related to the location_path attributes on users in easyvista's API.\\nAccording to the documentation ([employees creation](https:\/\/wiki.easyvista.com\/xwiki\/bin\/view\/Documentation\/REST+API+-+Create+an+employee)) the field can't be\\nused in a request to create an employee.\\nAlso filtering on location_path when querying \/locations does not work : queries ignore the field an simply returns the 100 first results.\\n","Decision":"Instead of using the [search option](https:\/\/wiki.easyvista.com\/xwiki\/bin\/view\/Documentation\/REST+API+-+See+a+list+of+locations) from the API we will rely instead on powershell\\nWhere-Object cmdlet with the max_rows parameter from the API.\\n","tokens":129,"id":2328,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nTo add\/modify content on our patternpediacontent repo \\'on behalf of the user\\', we need to use OAuth Authentification in our client-side application.\\nBut requesting https:\/\/github.com\/login\/oauth\/access_token endpoint for creating an OAuth access token does not support CORS ([documented here](https:\/\/github.com\/isaacs\/github\/issues\/330)).\\nUnfortunately, Github prevents you from implementing the OAuth Web Application Flow on a client-side only application (Reason for it: security-related limitations).\\n\\n## Decision\\nChosen option: \"Use our own server-side component\", because\\n* we don\\'t want to provide our their client id and secret to an (possibly untrusted) open reverse proxy (comment from the stackoverflow-article: If the owner of the proxy wants to log credentials from the requests they can)\\n\\n\\n## Context\\nJMAP custom quota extension, as well as IMAP [RFC-2087](https:\/\/tools.ietf.org\/html\/rfc2087) enables a user to monitor\\nthe amount of space and message count he is allowed to use, and that he is effectively using.\\nTo track the quota values a user is effectively using, James relies on the\\n[eventBus](..\/site\/markdown\/server\/manage-guice-distributed-james.md#mailbox-event-bus) to increment a Cassandra counter\\ncorresponding to this user.\\nHowever, upon Cassandra failure, this value can be incorrect, hence the need of correcting it.\\n\\n## Decision\\nWe will implement a generic corrective task exposed via webadmin.\\nThis task can reuse the `CurrentQuotaCalculator` and call it for each and every quotaRoot of each user.\\nThis way, non-Cassandra implementation will also benefit from this task.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need to be able to assign a location when creating an employee and to have autocompletion that list all availables location on cmdlet.\\\\nBut there are issues related to the location_path attributes on users in easyvista\\'s API.\\\\nAccording to the documentation ([employees creation](https:\/\/wiki.easyvista.com\/xwiki\/bin\/view\/Documentation\/REST+API+-+Create+an+employee)) the field can\\'t be\\\\nused in a request to create an employee.\\\\nAlso filtering on location_path when querying \/locations does not work : queries ignore the field an simply returns the 100 first results.\\\\n'}, {'role': 'model', 'content': 'Instead of using the [search option](https:\/\/wiki.easyvista.com\/xwiki\/bin\/view\/Documentation\/REST+API+-+See+a+list+of+locations) from the API we will rely instead on powershell\\\\nWhere-Object cmdlet with the max_rows parameter from the API.\\\\n'}]"}
{"File Name":"google-cloud-cpp-common\/2019-01-04-error-reporting-with-statusor.md","Context":"**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must\\nwork for all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a [Google\\nCloud Storage][gcs-link] component that is at the Alpha quality level, and a\\n[Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/master\/google\/cloud\/storage\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/master\/google\/cloud\/bigtable\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","Decision":"Instead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI\/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a [Google\\nCloud Storage][gcs-link] component that is at the Alpha quality level, and a\\n[Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/master\/google\/cloud\/storage\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/master\/google\/cloud\/bigtable\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n","tokens":436,"id":5065,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nOpenShift likes to shift (pun intended) the UID -- i.e., assign arbitrary UIDs -- to containers. They do this as an additional security feature, given that OpenShift is a multi-tentant Kubernetes solution. Each OpenShift project received a non-overlapping UID range. Hence, in case an attacker escapes a container, it will be more difficult to interfere with other processes.\\nHowever, this shifting of UIDs introduces an additional complexity: What if a process wants to write to the filesystem? What uid, gid and permissions should the files and folders have? To solve this problem, the OpenShift documentation (see [\"Support arbitrary user ids\"][openshift-docs]) recommends setting gid=0 on those files and folders. Specifically, the Dockerfiles of the container images should contain:\\n```Dockerfile\\nRUN chgrp -R 0 \/some\/directory && chmod -R g=u \/some\/directory\\n```\\nDuring execution, OpenShift assigns `gid=0` as a supplementary group to containers, so as to give them access to the required files.\\nIn contrast to OpenShift, Compliant Kubernetes is not a multi-tenant solution. Given previous vulnerabilities in Kubernetes that affected tenant isolation (e.g., [CVE-2020-8554][cve]\\n), we believe that non-trusting users should not share a workload cluster. Hence, we do not assign arbitrary UIDs to containers and do not need to assign `gid=0` as a supplementary group.\\nThe `gid=0` practice above seems to have made its way in [quite a few Dockerfiles][github-search], however, it is far from being the default outside OpenShift.\\nWhat should Compliant Kubernetes do with the `gid=0` practice?\\n## Decision Drivers\\n* For user expectations, we want to make it easy to start with Compliant Kubernetes.\\n* For better security and easier audits, we do not want to add unnecessary permissions.\\n* [ID mapping in mounts][idmapping] has landed in Linux 5.12. Once this feature is used in container runtimes and Kubernetes, the `gid=0` problem will go away.\\n\\n## Decision\\n* For user expectations, we want to make it easy to start with Compliant Kubernetes.\\n* For better security and easier audits, we do not want to add unnecessary permissions.\\n* [ID mapping in mounts][idmapping] has landed in Linux 5.12. Once this feature is used in container runtimes and Kubernetes, the `gid=0` problem will go away.\\nChosen option: \"disallow `gid=0` by default\". Enabling it on a case-by-case basis is okay.\\n### Positive Consequences\\n* We do not unnecessarily add a permission to containers.\\n### Negative Consequences\\n* Some users will complain about their container images not starting, and we will need to add a less restricted PodSecurityPolicy in their cluster.\\n\\n\\n## Context\\nTDR will store information about consignments and files in a relational\\ndatabase. This needs to be updated when a user makes a change in the TDR\\nfrontend, or after backend processing (e.g. with the results of an antivirus\\nscan). Rather than connect each of these applications to the database, which\\nhas several disadvantages (e.g. requiring multiple applications to have a secure\\nDB connection and coupling them to the DB structure), we are building an API\\nwhich can be called by the other applications in TDR.\\nDuring the Alpha prototyping phase, we looked into the options of building the\\nAPI using a REST or GraphQL style. See our Alpha notes on [GraphQL vs\\nREST][graphql-vs-rest] and [GraphQL prototypes][graphql-prototypes].\\nThe full [API for the Alpha prototype][prototype-api] was built in GraphQL,\\nusing the [Scala Sangria library][sangria]. We did not prototype a REST API\\nbecause all the developers on the team were familiar with REST from previous\\nprojects.\\n[graphql-vs-rest]: ..\/technology-considerations\/API_management_AppSync_GraphQL_REST.md\\n[graphql-prototypes]: ..\/technology-considerations\/graphql-prototypes.md\\n[prototype-api]: https:\/\/github.com\/nationalarchives\/tdr-prototype-sangria\\n[sangria]: https:\/\/sangria-graphql.github.io\/\\n\\n## Decision\\nUse GraphQL in the TDR Beta phase.\\nWe found Sangria straightforward to work with on the Alpha prototype. It was\\nsimple to define fields, generate a GraphQL schema and add [deferred resolvers]\\nto avoid overfetching.\\nThere is a lack of mature GraphQL client libraries in Scala, but we came up with\\na working approach in the [prototype][prototype-frontend] of [defining individual\\nqueries][prototype-queries] and validating them against the schema using\\n[sbt-graphql].\\nThe TDR API is an internal API, and is very likely to stay that way because TDR\\nis just a temporary record store. Once they have been transferred to TNA, they\\nwill be moved to the preservation system and (eventually) an access system to\\nlet people see the data. That access system may well have a public API, but the\\ndata will not come directly from TDR, so we don\\'t expect the choice of TDR API\\nto constrain future APIs.\\nThis makes the TDR API a good candidate for testing a slightly more experimental\\napproach (GraphQL) rather than the well-established pattern (REST), without\\nimpacting other systems if it turns out to be harder to work with. We\\'re aware\\nof other government departments who are considering GraphQL, so this is a also a\\ngood opportunity to share what we learn with other developers across government.\\nSome of the team still have some concerns around GraphQL, particularly around\\nerror handling: REST has well-established conventions around error codes, which\\nare usually understood by other layers such as caching. GraphQL allows more\\nfine-grained error handling, but we will have to decide on custom errors and\\nhandle them in the client applications. We will keep an eye on this in Beta and\\nsee how it works in practice.\\n[deferred resolvers]: https:\/\/sangria-graphql.github.io\/learn\/#deferred-value-resolution\\n[prototype-frontend]: https:\/\/github.com\/nationalarchives\/tdr-prototype-mvc\\n[sbt-graphql]: https:\/\/github.com\/muuki88\/sbt-graphql\\n[prototype-queries]: https:\/\/github.com\/nationalarchives\/tdr-prototype-mvc\/tree\/master\/app\/graphql\\n\\n\\n\\nProvide a decision given the context below:\\n**Context**: We know there will be users of these C++ libraries who want to use\\\\nC++ exceptions as well as those who are not able to. Our C++ libraries must\\\\nwork for all of our users, regardless of their ability to use exceptions.\\\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\\\nInstead, our APIs will typically report errors to callers by returning a\\\\n`Status` or `StatusOr<T>` object, unless the library we\\'re using has another\\\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\\\nI\/O library).\\\\n**Consequences**: This decision will result in a single set of APIs and a\\\\nconsistent vocabulary for all users, whether or not they choose to compile with\\\\nexceptions. This decision does not prevent callers from using exceptions in\\\\ntheir own code.\\\\nA downside of this decision is that our APIs will not be natural or idiomatic\\\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\\\nreporting.\\\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\\\nbreaking change. As of this writing (Jan 2019), this project has a [Google\\\\nCloud Storage][gcs-link] component that is at the Alpha quality level, and a\\\\n[Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\\\ntimeline to change this API in a separate document.\\\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/master\/google\/cloud\/storage\\\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/master\/google\/cloud\/bigtable\\\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\\\n'}, {'role': 'model', 'content': \"Instead, our APIs will typically report errors to callers by returning a\\\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\\\nI\/O library).\\\\n**Consequences**: This decision will result in a single set of APIs and a\\\\nconsistent vocabulary for all users, whether or not they choose to compile with\\\\nexceptions. This decision does not prevent callers from using exceptions in\\\\ntheir own code.\\\\nA downside of this decision is that our APIs will not be natural or idiomatic\\\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\\\nreporting.\\\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\\\nbreaking change. As of this writing (Jan 2019), this project has a [Google\\\\nCloud Storage][gcs-link] component that is at the Alpha quality level, and a\\\\n[Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\\\ntimeline to change this API in a separate document.\\\\n[badbit-link]: https:\/\/en.cppreference.com\/w\/cpp\/io\/ios_base\/iostate\\\\n[gcs-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/master\/google\/cloud\/storage\\\\n[bigtable-link]: https:\/\/github.com\/googleapis\/google-cloud-cpp\/tree\/master\/google\/cloud\/bigtable\\\\n[survey-link]: https:\/\/isocpp.org\/blog\/2018\/03\/results-summary-cpp-foundation-developer-survey-lite-2018-02\\\\n\"}]"}
{"File Name":"wikibase-release-pipeline\/0004-wdqs-tarball-content.md","Context":"## Context\\nUnlike in the case of Mediawiki extensions, where the extension code is packaged as a tarball to be installed in end user's environment, there is no immediate idea of how to package the Query Service (in particular as it actually involves a number of software components). Components included [Query Service backend] (including the WMF Blazegraph fork) and [Query Service UI]. Should these be bundled as separate tarballs? Or as a single one?\\n[Query Service backend] releases is currently [built and published](https:\/\/gerrit.wikimedia.org\/r\/plugins\/gitiles\/wikidata\/query\/rdf\/+\/refs\/heads\/master\/dist\/) as a [service zip archive] which contains the necessary components including the built-in UI of blazegraph.\\nThe [Query Service UI] has at one point been part of this build process but the two components have recently been [separated](https:\/\/phabricator.wikimedia.org\/T241291).\\n","Decision":"As the [Query Service backend] does not depend on the [Query Service UI] and they are currently two separate components there is no need to alter the structure of the [service zip archive] as no obvious benefit of doing so can be identified.\\n","tokens":199,"id":4473,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nHistorically Alfresco has had a bad experience of having a monolithic codebase in SVN. The main issue with this was the\\ninability to easily work on feature branches, resulting in frequent conflicting changes. To make matters worse this was\\nseen as hard to move to git, since GitHub has\/had a maximum limit on repository size and there were several large\\nbinary artifacts stored in the Alfresco history [[1]].\\nMore recently the Alfresco codebase has been split into a large number of small git repositories [[2]].\\nThis proposal is to merge the various git repositories together. By doing this we aim to:\\n1. Remove the effort of creating releases of library projects.\\n1. Allow tests to be written on the same branch (in the same repository) as the production code (i.e. facilitate TDD).\\n1. Make it easier for everyone (especially people outside the team and new starters) to find the 'right bit' of code.\\n[1]: https:\/\/community.alfresco.com\/community\/ecm\/blog\/2015\/04\/01\/so-when-is-alfresco-moving-to-github\\n[2]: https:\/\/ts.alfresco.com\/share\/proxy\/alfresco\/api\/node\/content\/versionStore\/version2Store\/a0c2492f-6354-4b98-adfc-e63d5c2209f5\/SearchCodeBase.png\\n\\n## Decision\\nWe will merge the search-related repositories together and preserve their history.  We will not attempt to merge code\\nthat other teams also need (for example the TAS test utilities or the alfresco-data-model projects).\\n\\n\\n## Context\\nWikibase Manifest needs to provide essential metadata and configuration options about a Wikibase instance.\\nWe need to decide on the format of the information the manifest will provide.\\nWe took into consideration the following projects:\\n- the recent [REST API Prototype](https:\/\/github.com\/wmde\/wikibase-rest-fantasies) (by WMDE) and its [OpenAPI spec](https:\/\/raw.githubusercontent.com\/wmde\/wikibase-rest-fantasies\/gh-pages\/openapi.json)\\n- [OpenRefine's initiative](https:\/\/github.com\/OpenRefine\/wikibase-manifests) to collect manifests from different wikibases and their [json-schema spec](https:\/\/github.com\/OpenRefine\/wikibase-manifests\/blob\/master\/wikibase-manifest-schema-v1.json)\\nThe MediaWiki REST API which we are using (please refer to ADR 1 for more info) implements neither json schema nor openAPI.\\n### OpenAPI (swagger)\\nThe OpenAPI Specification (formerly Swagger Specification) is an API description format for REST APIs. An OpenAPI file allows you to describe your entire API. [Swagger](swagger.io) is a set of open-source tools built around the OpenAPI Specification, like the [api editor](https:\/\/editor.swagger.io\/).\\n- It\u2019s popularly used for mocking services and generating SDKs. It's not commonly used for run-time functionality.\\n- Useful when you want to describe your entire API.\\n- **OpenAPI is both a subset of JSON Schema Draft 5 and a superset**\\n### Json Schema\\nJSON Schema is a vocabulary that allows you to validate, annotate, and manipulate JSON documents.\\nThe specification is split into three parts, [Core](https:\/\/json-schema.org\/draft\/2019-09\/json-schema-core.html), [Validation](https:\/\/json-schema.org\/draft\/2019-09\/json-schema-validation.html), and [Hyper-Schema](https:\/\/json-schema.org\/draft\/2019-09\/json-schema-hypermedia.html).\\n- JSON Schema is a good option when there are data models whose schema needs to be defined\\n\\n## Decision\\nUse OpenAPI spec.\\nWe acknowledge that both are good options. We chose OpenAPI because the Wikidata team has created several products (e.g. [termbox](https:\/\/gerrit.wikimedia.org\/r\/plugins\/gitiles\/wikibase\/termbox\/+\/refs\/heads\/master\/openapi.json)) using the OpenAPI spec and plan on continue to do so when we have the opportunity.\\nThere're tools (e.g. [OpenAPI Schema to JSON Schema](https:\/\/github.com\/openapi-contrib\/openapi-schema-to-json-schema)) for converting from OpenAPI Schema Object or Parameter Object to JSON Schema in case the need arises to use json schema.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nUnlike in the case of Mediawiki extensions, where the extension code is packaged as a tarball to be installed in end user's environment, there is no immediate idea of how to package the Query Service (in particular as it actually involves a number of software components). Components included [Query Service backend] (including the WMF Blazegraph fork) and [Query Service UI]. Should these be bundled as separate tarballs? Or as a single one?\\\\n[Query Service backend] releases is currently [built and published](https:\/\/gerrit.wikimedia.org\/r\/plugins\/gitiles\/wikidata\/query\/rdf\/+\/refs\/heads\/master\/dist\/) as a [service zip archive] which contains the necessary components including the built-in UI of blazegraph.\\\\nThe [Query Service UI] has at one point been part of this build process but the two components have recently been [separated](https:\/\/phabricator.wikimedia.org\/T241291).\\\\n\"}, {'role': 'model', 'content': 'As the [Query Service backend] does not depend on the [Query Service UI] and they are currently two separate components there is no need to alter the structure of the [service zip archive] as no obvious benefit of doing so can be identified.\\\\n'}]"}
{"File Name":"docs\/0021-kafka-as-messaging-middleware.md","Context":"## Context and Problem Statement\\nTo compose service with the mico application we need a messaging middleware that is highly scalable and supports the [Enterprise Integration Patterns](https:\/\/www.enterpriseintegrationpatterns.com).\\n## Decision Drivers\\n* Should be a well known and proven solution\\n* Support a pipes and filters style architecture\\n","Decision":"* Should be a well known and proven solution\\n* Support a pipes and filters style architecture\\nWe want to use Kafka as our messaging middleware.\\n### Important things to know about Kafka\\nKafka topics have a different semantic than traditional message queues. Kafka also does NOT log wether a message was consumed. It only stores the last comitted offset for a consumer. Because of that and the different topic semantic some patterns need to be adapted to work with Kafka.\\nFor more detailed information about the inner workings of Kafka consult the following links:\\n*  [Client subscriptions](https:\/\/kafka.apache.org\/intro#intro_consumers)\\n*  [Client positions](https:\/\/kafka.apache.org\/documentation\/#design_consumerposition)\\n*  [Guarantees for message delivery](https:\/\/kafka.apache.org\/intro#intro_guarantees)\\n*  [Message delivery modes (how to achive exactly once)](https:\/\/kafka.apache.org\/documentation\/#semantics)\\n*  [Use topics storage for key based data](https:\/\/kafka.apache.org\/documentation\/#compaction)\\n","tokens":63,"id":4679,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nFor over the past 5 years (at least), we have been using [InternJS](https:\/\/github.com\/theintern\/intern) for our functional tests in the content-server. While this has worked well and caught several bugs, we should reevaluate it since we are migrating to react and have an opportunityto use newer\/faster\/better supported testing frameworks.\\n## Decision Drivers\\n- Ease of writing tests\\n- Good community support\\n- Learning curve of settings V2 development\\n\\n## Decision\\n- Ease of writing tests\\n- Good community support\\n- Learning curve of settings V2 development\\nUse InternJS for functional testing in the settings v2 app, but also update the tests to support async\/await. The biggest factor was not having to introduce another testing framework for developers to learn.\\n\\n\\n## Context and Problem Statement\\nFor Microservices architecture it is a good idea to have API Gateway service to manage interservices communication and handle cross-cutting concerns (logging, security, caching, etc.).\\n## Decision Drivers\\n* Easy to use\\n* Easy to learn\\n* Can be hosted on Azure\\n* Can be hosted in Docker container\\n\\n## Decision\\n* Easy to use\\n* Easy to learn\\n* Can be hosted on Azure\\n* Can be hosted in Docker container\\nNo outcome yet. Have to investigate both a bit and then will decide.\\n### Positive Consequences\\n* I will be able to manage communication between services more effectively and securely with API Gateway.\\n### Negative Consequences\\n* n\/a\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\nTo compose service with the mico application we need a messaging middleware that is highly scalable and supports the [Enterprise Integration Patterns](https:\/\/www.enterpriseintegrationpatterns.com).\\\\n## Decision Drivers\\\\n* Should be a well known and proven solution\\\\n* Support a pipes and filters style architecture\\\\n'}, {'role': 'model', 'content': '* Should be a well known and proven solution\\\\n* Support a pipes and filters style architecture\\\\nWe want to use Kafka as our messaging middleware.\\\\n### Important things to know about Kafka\\\\nKafka topics have a different semantic than traditional message queues. Kafka also does NOT log wether a message was consumed. It only stores the last comitted offset for a consumer. Because of that and the different topic semantic some patterns need to be adapted to work with Kafka.\\\\nFor more detailed information about the inner workings of Kafka consult the following links:\\\\n*  [Client subscriptions](https:\/\/kafka.apache.org\/intro#intro_consumers)\\\\n*  [Client positions](https:\/\/kafka.apache.org\/documentation\/#design_consumerposition)\\\\n*  [Guarantees for message delivery](https:\/\/kafka.apache.org\/intro#intro_guarantees)\\\\n*  [Message delivery modes (how to achive exactly once)](https:\/\/kafka.apache.org\/documentation\/#semantics)\\\\n*  [Use topics storage for key based data](https:\/\/kafka.apache.org\/documentation\/#compaction)\\\\n'}]"}
{"File Name":"tech-team\/0009-supported-python-versions-containers.md","Context":"## Context\\neLife has numerous projects written completely and partly with the [Python programming language](https:\/\/www.python.org\/).\\nIn order to provide language version consistency across projects we need to get a consensus on which versions we are going to support.\\nWe previously have only gone up to Python 3.5 due to the default Python versions pre installed on the Ubuntu distributions we use.\\nPython official images make it easy to support a new Python version without custom PPAs.\\n","Decision":"We will use Python >=2.7.14 as our default version for any project that solely uses or supports Python 2.\\nWe will use Python 3.6 as our default supported version for any containerized project that solely uses or supports Python 3.\\nWe will use Python 3.5 as our default supported version for any project that is not containerized at the moment.\\n","tokens":98,"id":932,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\neLife has several microservices (and larger projects) written in PHP.\\nIn order to provide language version consistency across projects we need to get a consensus on which versions we are going to support.\\nThere is an exception, `crm` not being upgraded on PHP 7 (using 5.6 instead) but supporting it.\\nStaying up to date with the major and minor versions of PHP is important:\\n- to be able to use new features and libraries (nice to have)\\n- to keep working on a version that receives bug fixes and security updates (must have)\\nAll infrastructure is currently based on:\\n- Ubuntu 14.04 (doesn\\'t have PHP 7.x by default)\\n- Ubuntu 16.04 (does have PHP 7.0 by default)\\n- a popular [PPA](https:\/\/launchpad.net\/~ondrej\/+archive\/ubuntu\/php) filling in the blanks, supporting 7.0, 7.1, and 7.2.\\n- official [PHP Docker images](https:\/\/hub.docker.com\/_\/php\/) supporting 5.6, 7.0, 7.1, and 7.2.\\nPHP 7.0 has ceased active support, but has [security support](http:\/\/php.net\/supported-versions.php) until 2018-12-03.\\n\\n## Decision\\nWe will use PHP 7.0 on all existing and new PHP projects bar exceptions that do not support it (`crm` if needed).\\nWe will upgrade to PHP 7.1, PHP 7.2 and similar minor versions as a concerted effort on all libraries and projects, before the end of the security support.\\n\\n\\n## Context and Problem Statement\\nWe plan to refactor the reference implementation significantly and, as part of\\nthat effort, drop support for no-longer maintained versions of Python\\n(see ADR 0001).\\nHowever, a major user of (and contributor to) the project has users of the\\nclient stuck on older Python versions.\\nWe would like to define a reasonable support policy for the current, Python 2.7\\nsupporting, codebase.\\n## Decision Drivers\\n* We have finite resources.\\n* A major adopter\/user of the project has a need to maintain support for\\nPython 2.7 clients.\\n\\n## Decision\\n* We have finite resources.\\n* A major adopter\/user of the project has a need to maintain support for\\nPython 2.7 clients.\\nChosen option: \"Support the old code on a best-effort basis once the refactored\\ncode is released\", because we only have finite resources and want to focus them\\non moving the project forward, including supporting PyPI\/pip integration and\\nproviding a solid implementation for developing specification enhancements in.\\nWe should document this outcome clearly in a governance document describing\\nthe release process with words along the lines of:\\n\"Support for older releases:\\nBugs reported with tuf versions prior to 1.0.0 will likely not be addressed\\ndirectly by tuf\u2019s maintainers. Pull Requests to fix bugs in the last release\\nprior to 1.0.0 will be considered, and merged (subject to normal review\\nprocesses). Note that there may be delays due to the lack of developer resources\\nfor reviewing such pull requests.\"\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\neLife has numerous projects written completely and partly with the [Python programming language](https:\/\/www.python.org\/).\\\\nIn order to provide language version consistency across projects we need to get a consensus on which versions we are going to support.\\\\nWe previously have only gone up to Python 3.5 due to the default Python versions pre installed on the Ubuntu distributions we use.\\\\nPython official images make it easy to support a new Python version without custom PPAs.\\\\n'}, {'role': 'model', 'content': 'We will use Python >=2.7.14 as our default version for any project that solely uses or supports Python 2.\\\\nWe will use Python 3.6 as our default supported version for any containerized project that solely uses or supports Python 3.\\\\nWe will use Python 3.5 as our default supported version for any project that is not containerized at the moment.\\\\n'}]"}
{"File Name":"bananatabs\/0004-using-react-context.md","Context":"# 4. Using React Context\\nDate: 2019-07-01\\n## Context\\nThere was a lot of prop-drilling in the component tree.\\n### Old Render Tree -- A lot of prop-drilling\\n```\\n<App\/>\\n<BananaTabs \/>\\n<MainView \/> (\u2705session, \u274csessionMutator, \u274cwindowMutator, \u274ctabMutator, \u2705browserController)\\n<Title \/>\\n<WindowListView \/>          (\u2705windows, \u2705sessionMutator, \u274cwindowMutator, \u274ctabMutator)\\n<WindowView \/>            (\u2705window, \u274cwindowMutator, \u274ctabMutator)\\n<WindowHeader \/>        (\ufe0f\ufe0f\u26a0\ufe0fwindow\ufe0f, \u2705windowMutator, \u274ctabMutator)\\n<DisclosureButton \/>  (\u2705window, \u2705windowMutator)\\n<VisibilityIcon \/>    (\u2705window, \u2705windowMutator, \u2705tabMutator)\\n<WindowTitle \/>       (\u2705window, \u2705windowMutator)\\n<TabList \/>             (\u2705window, \u274ctabMutator)\\n<TabView \/>           (\u2705window, \u2705tab, \u2705tabMutator)\\n<TabToolsView \/>\\n<MainViewCmdButtons \/> (none)\\nLegend:\\n\u2705prop: actually used by component\\n\u26a0\ufe0fprop: only reading id (e.g. window.id, tab.id)\\n\u274cprop: only passing it down to children\\n```\\n","Decision":"I'm going to try to use React Context with the `useContext` hook to see if I can reduce or eliminate prop-drilling.\\n","tokens":332,"id":2818,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n### Context\\nPHPUnit offers a range of `@covers` annotations with the possible to enforce a strict mode or to\\nenforce them. The question is when should those annotations be enforced and\/or if we need to enable\\nanother settings as well?\\n### Decision\\nSince we are using the [`symfony\/phpunit-bridge`][phpunit-bridge], we decide to leverage the\\n[`Symfony\\\\Bridge\\\\PhpUnit\\\\CoverageListener`][code-coverage-listener] in `phpunit.xml.dist` in order to avoid to require the\\n`@covers` annotations whilst still benefit from it.\\nThis however does not allow to completely forgo its usage due to the following cases:\\n- A test testing more than one class, requiring multiple `@covers` annotations\\n- A test case testing a \"test class\", i.e. code reserved for testing purposes\\nFor this reason, the proposal to remove the `@covers` annotations via the [PHP-CS-Fixer][php-cs-fixer]\\nsetting `general_phpdoc_annotation_remove` has been refused.\\nSince no one came up with an easy or acceptable proposal to automate the process of whether a\\n`@covers` annotation is necessary or not, no further action has been voted for automating this\\nprocess.\\n### Status\\nAccepted ([#1060][1060])\\n[code-coverage-listener]: https:\/\/symfony.com\/doc\/current\/components\/phpunit_bridge.html#code-coverage-listener\\n[phpunit-bridge]: https:\/\/packagist.org\/packages\/symfony\/phpunit-bridge\\n[php-cs-fixer]: https:\/\/github.com\/FriendsOfPHP\/PHP-CS-Fixer\\n[1060]: https:\/\/github.com\/infection\/infection\/pull\/1060\\n\\n## Decision\\nSince we are using the [`symfony\/phpunit-bridge`][phpunit-bridge], we decide to leverage the\\n[`Symfony\\\\Bridge\\\\PhpUnit\\\\CoverageListener`][code-coverage-listener] in `phpunit.xml.dist` in order to avoid to require the\\n`@covers` annotations whilst still benefit from it.\\nThis however does not allow to completely forgo its usage due to the following cases:\\n- A test testing more than one class, requiring multiple `@covers` annotations\\n- A test case testing a \"test class\", i.e. code reserved for testing purposes\\nFor this reason, the proposal to remove the `@covers` annotations via the [PHP-CS-Fixer][php-cs-fixer]\\nsetting `general_phpdoc_annotation_remove` has been refused.\\nSince no one came up with an easy or acceptable proposal to automate the process of whether a\\n`@covers` annotation is necessary or not, no further action has been voted for automating this\\nprocess.\\n### Status\\nAccepted ([#1060][1060])\\n[code-coverage-listener]: https:\/\/symfony.com\/doc\/current\/components\/phpunit_bridge.html#code-coverage-listener\\n[phpunit-bridge]: https:\/\/packagist.org\/packages\/symfony\/phpunit-bridge\\n[php-cs-fixer]: https:\/\/github.com\/FriendsOfPHP\/PHP-CS-Fixer\\n[1060]: https:\/\/github.com\/infection\/infection\/pull\/1060\\n\\n\\n## Context\\nIn flutter application, we can receive file that is shared from other app. At this moment, LinShare app support receiving single file shared.\\nThe following config is for receiving single file shared IOS (LinShareExtension\/info.plist):\\n```\\n<key>NSExtensionActivationSupportsImageWithMaxCount<\/key>\\n<integer>1<\/integer>\\n<key>NSExtensionActivationSupportsMovieWithMaxCount<\/key>\\n<integer>1<\/integer>\\n<key>NSExtensionActivationSupportsFileWithMaxCount<\/key>\\n<integer>1<\/integer>\\n```\\nThe following config is for receiving single file shared Android (AndroidManifest.xml):\\n```\\n<intent-filter>\\n<action android:name=\"android.intent.action.SEND\" \/>\\n<category android:name=\"android.intent.category.DEFAULT\" \/>\\n<data android:mimeType=\"*\/*\" \/>\\n<\/intent-filter>\\n```\\n- Update on 14\/09\/2021:\\n- By the issue from [here](https:\/\/github.com\/KasemJaffer\/receive_sharing_intent\/issues\/81), need to update setting on Android `AndroidManifest.xml`:\\nChange from:\\n```xml\\nandroid:launchMode=\"singleTop\"\\n```\\nto:\\n```xml\\nandroid:launchMode=\"singleTask\"\\n```\\n- Adapt to version change [`1.4.5`](https:\/\/pub.dev\/packages\/receive_sharing_intent\/changelog#145):\\n- Support null-safety\\n- New CFBundleURLSchemes\\n\\n## Decision\\nWe decided that LinShare app support receiving single file shared at the moment.\\n\\n\\n\\nProvide a decision given the context below:\\n# 4. Using React Context\\\\nDate: 2019-07-01\\\\n## Context\\\\nThere was a lot of prop-drilling in the component tree.\\\\n### Old Render Tree -- A lot of prop-drilling\\\\n```\\\\n<App\/>\\\\n<BananaTabs \/>\\\\n<MainView \/> (\u2705session, \u274csessionMutator, \u274cwindowMutator, \u274ctabMutator, \u2705browserController)\\\\n<Title \/>\\\\n<WindowListView \/>          (\u2705windows, \u2705sessionMutator, \u274cwindowMutator, \u274ctabMutator)\\\\n<WindowView \/>            (\u2705window, \u274cwindowMutator, \u274ctabMutator)\\\\n<WindowHeader \/>        (\ufe0f\ufe0f\u26a0\ufe0fwindow\ufe0f, \u2705windowMutator, \u274ctabMutator)\\\\n<DisclosureButton \/>  (\u2705window, \u2705windowMutator)\\\\n<VisibilityIcon \/>    (\u2705window, \u2705windowMutator, \u2705tabMutator)\\\\n<WindowTitle \/>       (\u2705window, \u2705windowMutator)\\\\n<TabList \/>             (\u2705window, \u274ctabMutator)\\\\n<TabView \/>           (\u2705window, \u2705tab, \u2705tabMutator)\\\\n<TabToolsView \/>\\\\n<MainViewCmdButtons \/> (none)\\\\nLegend:\\\\n\u2705prop: actually used by component\\\\n\u26a0\ufe0fprop: only reading id (e.g. window.id, tab.id)\\\\n\u274cprop: only passing it down to children\\\\n```\\\\n'}, {'role': 'model', 'content': \"I'm going to try to use React Context with the `useContext` hook to see if I can reduce or eliminate prop-drilling.\\\\n\"}]"}
{"File Name":"simple-android\/003-weak-references-between-sql-tables.md","Context":"## Context\\nStrong references between SQL tables using foreign keys are useful for maintaining data integrity. They introduce safeguards against orphaned records\\nby,\\n- making it impossible to create data records that do not fulfill relation.\\n- deleting referenced records in a cascading fashion when the root dependent record is deleted.\\nIn the context of Simple, a good example would be the `Patient` and `BloodPressureMeasurement` tables. In real world, it\u2019s impossible to record a\\nblood pressure for a non-existent patient. Likewise, maintaining a strong reference between these two tables in SQL would have ensured that blood\\npressures do not get created either unless the associated patient already exists in the database.\\nAdditionally, if the patient ever gets deleted, SQLite would have handled deletion of all their blood pressures automatically.\\nUnfortunately, strong references work only when the data storage is centralized. Because Simple\u2019s data is distributed across many devices, it\u2019s\\nimpossible to guarantee the existence of all data records at the same time due to its offline by default nature. For instance, it\u2019s easy to imagine a\\nscenario where the app receives some blood pressures from the server, but fails to receive their associated patients because of bad network\\nconnectivity. This is expected behavior and we do not want the app to fail because of a foreign key integrity failure.\\n","Decision":"SQL tables in Simple will not keep strong references using foreign keys unless they can be synced **together** in the same network call with the\\nserver.\\n","tokens":269,"id":1135,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nIdempotency guarantees for an action are normally provided for a **limited period of time**. That creates an opportunity for a service to periodically do a **cleanup** of stored actions and as a result avoid ever growing storage requirements and performance implications.\\nWe\u2019ve considered few approaches to this problem:\\n- partition\\n- periodical purge\\nPerformance wise introducing partitions in idempotent_action table seemed like the best idea. Performance benefits outweighed any complications of facilitating and dropping partitions (i.e. there is no way to automatically create partition in Postgres). The issue comes with an absence of global index across partitions in some databases (i.e. Postgres) that creates a limitation that any idempotent action key has to **uniquely map into a partition**. Since the most natural choice of a partition key is a date, it would mean each `action_id` would need to come along with a date or have a date bit embedded, i.e. **UUIDv2**. So far we\u2019ve tried to avoid imposing any limitations on a key format, hence we put the idea of partitioning on hold.\\nPeriodical purge seemed like the only other feasible approach for a database that is lacking time-to-live (TTL) indexes.\\nIt comes with its own challenges like multi-instance set up and DB performance.\\nClients have to be extremely careful with batch size of purging to avoid any negative effect on database performance.\\nTo avoid any need for co-ordination of purge job execution in a multi instance setup or a dependency on Zookeeper\/etcd for a leader election we opted for a lightweight database scheduler `db-scheduler` that guarantees single instance execution.\\n\\n## Decision\\nAllow clients to configure retention policy for idempotent actions.\\nClients can specify:\\n- retention period (ISO-8601 format)\\n- cron schedule for purge job (spring format)\\n- batch-size for removal operation\\n```yaml\\nidempotence4j:\\nretention:\\nenabled: true\\nperiod: P0Y0M20D\\npurge:\\nschedule: \"*\/15 * * * * ?\"\\nbatchSize: 150\\n```\\nPurge job will only be executed on a single node in a multi instance set up.\\n\\n\\n## Context\\nCassandra eventual consistency is all about \"replication\", but \"denormalization\" consistency needs\\nto be handled at the applicative layer (due to the lack of transactions in a NoSQL database).\\nIn the past we did set up \"Solve inconsistency\" tasks that can be assimilated to Cassandra repairs. Such\\ntasks, after being scheduled, ensure that the according entity denormalization is correctly denormalized.\\nHowever, the inconsistencies persist between runs. We experienced inconsistencies in some production platform\\nfor both the mailbox entity, and the mailbox counter entity (whose table structure is exposed in\\n[these](0020-cassandra-mailbox-object-consistency.md), [ADRs](0023-cassandra-mailbox-counters-inconsistencies.md)).\\nMonitoring is required to detect when to run them and is time consuming for the platform administrator.\\nGiven a large dataset, it could even be impossible to run such tasks in a timely fashion.\\nAnother classic eventual consistency mechanism, that enables auto-healing is read-repair. Randomly piggy back upon reads\\nsynchronous or asynchronous consistency checks. If missed a repair is performed.\\nIn order to achieve denormalization auto-healing, we thus need to implement \"applicative read repairs\".\\n\\n## Decision\\nProvide a Proof of concept for \"Applicative read repairs\" for the mailbox and mailbox-counters entities.\\nThis enables read path simplification (and performance enhancements) for the mailbox object.\\nIMAP LIST should not read mailbox counters. This information is uneeded and we should avoid paying the\\nprice of read repairs for this operation.\\nProvide a comprehensive documentation page regarding \"Distributed James consistency model\".\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nStrong references between SQL tables using foreign keys are useful for maintaining data integrity. They introduce safeguards against orphaned records\\\\nby,\\\\n- making it impossible to create data records that do not fulfill relation.\\\\n- deleting referenced records in a cascading fashion when the root dependent record is deleted.\\\\nIn the context of Simple, a good example would be the `Patient` and `BloodPressureMeasurement` tables. In real world, it\u2019s impossible to record a\\\\nblood pressure for a non-existent patient. Likewise, maintaining a strong reference between these two tables in SQL would have ensured that blood\\\\npressures do not get created either unless the associated patient already exists in the database.\\\\nAdditionally, if the patient ever gets deleted, SQLite would have handled deletion of all their blood pressures automatically.\\\\nUnfortunately, strong references work only when the data storage is centralized. Because Simple\u2019s data is distributed across many devices, it\u2019s\\\\nimpossible to guarantee the existence of all data records at the same time due to its offline by default nature. For instance, it\u2019s easy to imagine a\\\\nscenario where the app receives some blood pressures from the server, but fails to receive their associated patients because of bad network\\\\nconnectivity. This is expected behavior and we do not want the app to fail because of a foreign key integrity failure.\\\\n'}, {'role': 'model', 'content': 'SQL tables in Simple will not keep strong references using foreign keys unless they can be synced **together** in the same network call with the\\\\nserver.\\\\n'}]"}
{"File Name":"connecting-to-services\/0009-search-by-place.md","Context":"## Context\\nThere are a lot of automated checks performed against the application. These\\nchecks are being counted as part of the normal traffic and there is no way to\\ncurrently filter them out. There is a need to remove the bot traffic so real\\nuser behaviour can be identified.\\n","Decision":"The decision is to count any request that includes a query string parameter `check`.\\n","tokens":60,"id":2213,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWhenever a domain related events happens (checkin, checkout, project update etc.), a number of\\ninterested components are to be notified. Those components might show warnings, update the UI etc.\\nUnfortunately, not all components are being executed in the main process all the time, so Otto\\nevents will not reach them. There needs to be a mechanism that reaches background services.\\n\\n## Decision\\nJede an einem Domain Event interessierte Komponente stellt einen BroadcastReceiver bereit, der\\nmittels Intents informiert wird. Alle DomainEvents werden per Broadcast verteilt.\\nAlle Events sollen haben die Intent-Kategorie `com.tastybug.timetracker.LIFECYCLE_EVENT` sowie eine\\nAction, die das Domainobjekt beschreibt: `com.tastybug.timetracker.PROJECT_CHANGE` oder\\n`com.tastybug.timetracker.TRACKING_RECORD_CHANGE`.\\nIn den Extras des Intents sind die UUIDs der betroffenen Entities abgelegt.\\nEvery component interested in Domain Events provides a BroadcastReceiver that will be notified via\\nIntents. Every Domain Event will be propagated using Broadcasts.\\nAll events have category `com.tastybug.timetracker.LIFECYCLE_EVENT` an an action that describes the\\naffected object type. Currently this is `com.tastybug.timetracker.PROJECT_CHANGE` and\\n`com.tastybug.timetracker.TRACKING_RECORD_CHANGE`. UUIDs of affected objects can be taken from\\nintent extras.\\n\\n\\n## Context\\nWe needed to test parallel login of users to ensure that\\nsession data isn't accidentally shared between users\\n(see [decision 0002](.\/0002-custom-save-handler-in-service-front.md)).\\nAlso, Make an LPA currently has no load tests. These are useful\\nfor capacity planning and finding bottlenecks which may be\\ncausing errors for users on the site. While we are auto-scaling,\\nwe don't have a way to verify that it is sufficient to manage\\nexpected load, which load testing can provide.\\nLoad testing can also provide a benchmark we can use to ensure\\nthat any new work we do does not degrade performance of the stack\\nas a whole.\\n\\n## Decision\\nUse [locust.io](https:\/\/locust.io\/) to test parallel logins.\\nWhile there are alternatives to locust, it is implemented in Python\\n(our chosen language going forward), has an intuitive API, and some\\nmembers of the team have experience with it already (and can reuse\\npreviously-written code).\\nLoad tests will be added to the tests\/load directory in the project,\\nas they are not component-specific and apply to the whole stack.\\nDoing the above has the happy side effect of opening up the possibility of\\nload testing the whole application stack in future.\\nInitially, this will only run locally and is not integrated into\\nthe CI pipeline. We will consider extending this testing into CI\\nin future.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThere are a lot of automated checks performed against the application. These\\\\nchecks are being counted as part of the normal traffic and there is no way to\\\\ncurrently filter them out. There is a need to remove the bot traffic so real\\\\nuser behaviour can be identified.\\\\n\"}, {'role': 'model', 'content': 'The decision is to count any request that includes a query string parameter `check`.\\\\n'}]"}
{"File Name":"bookit-api\/0009-use-jpa.md","Context":"## Context\\nOriginally, we utilized a Spring's JdbcTemplate to quickly CRUD our entities against the data store.  While this was quick and easy, we did most of our filtering in the application as opposed to SQL WHERE clauses.  As we continued, each addition to our entities required a lot of boilerplate code.\\nSpring has great JPA support and Boot uses Hibernate out of the box.  Our entity models are still relatively simple, but using JPA reduces a lot of the boilerplate code, and opens up a lot of additional features \"for free.\"  Specifically, we can utilize JPA to manage our schema updates (naively, later if we need something more robust we can look to Liquibase or Flyway).  It also simplifies joins, where clauses, and gives us more database independence.\\n","Decision":"* Use JPA to map objects into database tables\\n* Use Hibernate as the JPA implementation - Spring Boot's default\\n* Leverage Spring Data's JPA support to implement queries via Repository interface patterns\\n","tokens":166,"id":3222,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nOriginally, we utilized a Spring\\'s JdbcTemplate to quickly CRUD our entities against the data store.  While this was quick and easy, we did most of our filtering in the application as opposed to SQL WHERE clauses.  As we continued, each addition to our entities required a lot of boilerplate code.\\nSpring has great JPA support and Boot uses Hibernate out of the box.  Our entity models are still relatively simple, but using JPA reduces a lot of the boilerplate code, and opens up a lot of additional features \"for free.\"  Specifically, we can utilize JPA to manage our schema updates (naively, later if we need something more robust we can look to Liquibase or Flyway).  It also simplifies joins, where clauses, and gives us more database independence.\\n\\n## Decision\\n* Use JPA to map objects into database tables\\n* Use Hibernate as the JPA implementation - Spring Boot\\'s default\\n* Leverage Spring Data\\'s JPA support to implement queries via Repository interface patterns\\n\\n\\n## Context\\nWhile we had great benefits from using an event sourced system in `Wegwijs`, the specific implementation had some drawbacks, specifically the transactional part.\\nTo get up and running quickly, we used a cqrs-es template application as the starting point of `Wegwijs`. To protect us from some of the harder parts of event sourcing, we choose to use an in-process, transactional approach to the way our events and projections were stored, ie: in one all-or-nothing transaction.\\nThis enabled us to get a quick start on `Wegwijs`, delivering features fast. Though we still stand by that decision, the template application also had some drawbacks in terms of testability and modularity. Since we also had in-memory state, this forced us to create rollback logic for this in-memory state in case the transaction failed.\\nIn short: we feel that this approach was great for fast start-up, but over time brings in more complexity than needed.\\nWe feel that, with the knowledge and experience we built on `Wegwijs`, we could now take the step a more modular approach using async event handlers.\\n\\n## Decision\\nWe will use async event handlers, and thus eventual consistency in our application.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nOriginally, we utilized a Spring\\'s JdbcTemplate to quickly CRUD our entities against the data store.  While this was quick and easy, we did most of our filtering in the application as opposed to SQL WHERE clauses.  As we continued, each addition to our entities required a lot of boilerplate code.\\\\nSpring has great JPA support and Boot uses Hibernate out of the box.  Our entity models are still relatively simple, but using JPA reduces a lot of the boilerplate code, and opens up a lot of additional features \"for free.\"  Specifically, we can utilize JPA to manage our schema updates (naively, later if we need something more robust we can look to Liquibase or Flyway).  It also simplifies joins, where clauses, and gives us more database independence.\\\\n'}, {'role': 'model', 'content': \"* Use JPA to map objects into database tables\\\\n* Use Hibernate as the JPA implementation - Spring Boot's default\\\\n* Leverage Spring Data's JPA support to implement queries via Repository interface patterns\\\\n\"}]"}
{"File Name":"datalab\/0029-dask-for-python-distributed-compute.md","Context":"## Context\\nFollowing a meeting with the Met Office it is clear that their Python users were seeing\\ngreat success using [Dask](https:\/\/dask.pydata.org\/en\/latest\/) as their distributed\\ncompute environment. Dask appears that it could be easier to use than Spark for users\\nwho already know Python and NumPy.\\n","Decision":"We have decided to offer Dask in addition to Spark within the Datalabs platform. This\\nenables us to appeal to more users at limited cost.\\n","tokens":69,"id":773,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nRunning a service like cljdoc requires infrastructure of various kinds. For\\nthe initial prototype I generated a static site stack using [a library I\\ncreated](https:\/\/github.com\/confetti-clj\/confetti) which uses AWS\\nCloudFormation under the hood. This has been useful while prototyping but\\nwith the addition of [a server component](\/doc\/adr\/0009-introduce-server-component.md)\\nand potentially other infrastructure components in the future it becomes\\nharder to stick to Cloudformation.\\n[Terraform](https:\/\/www.terraform.io\/) seems to be a popular tool in this space\\nand provides all the functionality of my library as well as support for using\\nmultiple cloud providers.\\n\\n## Decision\\nRebuild the static site stack using Terraform. Once that has been transitioned,\\nextend the Terraform configuration to include a server that we can use to host\\nthe server component of cljdoc.\\n\\n\\n## Context\\nhttp:\/\/www.pac4j.org\\nWhile exporing alternatives to our custom filter and token validator, we came across pac4j which can deal with Azure AD out of the box.\\nThe driver for this alternative is to have a library where we can point it to the discovery url (https:\/\/login.microsoftonline.com\/organizations\/v2.0\/.well-known\/openid-configuration) and have it \"just work.\"  Microsoft is a bit off standard where they add {tenantId} to the Issuer URL because this is a multi-tenant endpoint.  While it\\'s not hard, openid connect libraries need to be adjusted to handle this.  Pac4j does this for free.\\n\\n## Decision\\n* Use Pac4J to validate MS id_tokens instead of our current custom filter code\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nFollowing a meeting with the Met Office it is clear that their Python users were seeing\\\\ngreat success using [Dask](https:\/\/dask.pydata.org\/en\/latest\/) as their distributed\\\\ncompute environment. Dask appears that it could be easier to use than Spark for users\\\\nwho already know Python and NumPy.\\\\n'}, {'role': 'model', 'content': 'We have decided to offer Dask in addition to Spark within the Datalabs platform. This\\\\nenables us to appeal to more users at limited cost.\\\\n'}]"}
{"File Name":"qc-atlas\/0001-use-URI-for-entities.md","Context":"## Context and Problem Statement\\nIn the near future, QC Algorithms stored in the platform will reference QC patterns stored in the Pattern Atlas and vice versa.\\nWe need references for the links.\\n","Decision":"Chosen option: \"[URIs]\", because UUIDs are generated and thus depend on the underlying database system.\\nWe will use them as natural ids, so the database will check uniqueness of the uri identifiers.\\n### Positive Consequences <!-- optional -->\\n* We follow solid [W3C specification](https:\/\/www.w3.org\/Addressing\/URL\/uri-spec.html)\\n","tokens":39,"id":700,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nIn the near future, QC Algorithms stored in the platform will reference QC patterns stored in the Pattern Atlas and vice versa.\\nWe need references for the links.\\n\\n## Decision\\nChosen option: \"[URIs]\", because UUIDs are generated and thus depend on the underlying database system.\\nWe will use them as natural ids, so the database will check uniqueness of the uri identifiers.\\n### Positive Consequences <!-- optional -->\\n* We follow solid [W3C specification](https:\/\/www.w3.org\/Addressing\/URL\/uri-spec.html)\\n\\n\\n## Context\\nThis ADR serves to document how Custom Domain Properties are rendered in JSON-LD.\\nCustom Domain Properties represent dynamic parts of the model. The original domain is extended with annotations which\\nmay or may not include their own semantic information (when used with semantic extensions).\\nGiven that there is a problem, the AMF Web API model is mostly static. We have static `*Model.scala` objects with each\\nmodel.\\n\\n## Decision\\nWe will introduce a hack when rendering Custom Domain properties in JSON-LD that will make the graph look like it was\\ngenerated by a dynamic model while still representing the model statically.\\nConcretely, the `DomainElementModel` has a `CustomDomainProperties` field which is an `Array(DomainExtensionModel)`.\\nEach `DomainExtensionModel` has a `DefinedBy: CustomDomainPropertyModel` and `Extension: DataNode` fields that\\nrepresent the definition and value of the domain extension.\\nThis in the \"traditional & static\" parts of the model would be rendered like this (summary):\\n```json\\n{\\n\"@id\": \"myDomainElement\",\\n\"customDomainProperties\": [\\n{\\n\"@id\": \"myDomainExtension\",\\n\"definedBy\": {\\n\"@id\": \"myCustomDomainProperty\"\\n},\\n\"extension\": {\\n\"@id\": \"myDataNode\"\\n}\\n}\\n]\\n}\\n```\\nInstead a \"dynamic\" model is simulated\\n```json\\n{\\n\"@id\": \"myDomainElement\",\\n\"myCustomDomainProperty\": {\\n\"@id\": \"myDataNode\"\\n},\\n\"customDomainProperties\": [\\n{\\n\"@id\": \"myCustomDomainProperty\"\\n}\\n]\\n}\\n```\\nAs you can see a new property was generated using the `@id` of the `CustomDomainProperty` from the original `definedBy`\\nfield which does not exist in the static model. The `customDomainProperties` property now points to an array of custom\\ndomain property nodes rather than an array of domain extension nodes. The new `customDomainProperties` value indicates\\nwhich properties of the serialized node \"are dynamic\".\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\nIn the near future, QC Algorithms stored in the platform will reference QC patterns stored in the Pattern Atlas and vice versa.\\\\nWe need references for the links.\\\\n'}, {'role': 'model', 'content': 'Chosen option: \"[URIs]\", because UUIDs are generated and thus depend on the underlying database system.\\\\nWe will use them as natural ids, so the database will check uniqueness of the uri identifiers.\\\\n### Positive Consequences <!-- optional -->\\\\n* We follow solid [W3C specification](https:\/\/www.w3.org\/Addressing\/URL\/uri-spec.html)\\\\n'}]"}
{"File Name":"linshare-mobile-flutter-app\/0009-enhancement-retry-authen-interceptor.md","Context":"## Context\\n- The current `retry` logic is limited in the number of `retry` for failed requests. For example, at a screen where more than 3 different requests fail, the chance of each request being retried is very little, since they share a variable that counts the number of retry times.\\n- Using a (in memory) counter variable for `retry` is risky and difficult to control\\n","Decision":"1. Send the `retry` counter into request header for each request\\n2. Retrieve the `retry` count from request data\\n3. Validate with the maximum allowed. If `retry` in the limit, increment it then re-send a request network with updated `retry` counter\\n```\\n{\\n...\\nSET retriesCount from request extra map with key RETRY_KEY\\nIF _isAuthenticationError(dioError, retriesCount) THEN\\nINCREMENT retriesCount\\nADD retriesCount to header with key RETRY_KEY\\nCALL request again\\nENDIF\\n...\\n}\\n```\\n","tokens":84,"id":3303,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n**Context**: many APIs impose restrictions on their parameters. Names may have\\nlength limits, or strings may need to conform to specific formats, or ranges\\nmust be non-empty.  It is tempting to validate these arguments in the\\nclient-side, since we *know* the API will fail if it is allowed to continue\\nwith the invalid parameters. However, any such validation is redundant, since\\nthe server is going to validate its inputs as well. Furthermore, any such\\nvalidation is wasteful: most of the time the application will pass on valid\\narguments, checking them twice is just wasting client-side CPU cycles. Moreover,\\nthe server-side is the source of truth, so having the validations performed on\\nthe client side wil require extra work if the server restrictions are ever\\nmodified.\\n**Decision**: the client libraries do not validate any of the request\\nparameters beyond what is required to avoid local crashes.\\n**Consequences**: the main downsides are:\\n- The error messages could be better if generated locally.\\n- The server must validate all inputs correctly.\\n- Sometimes, we will need to validate inputs to avoid library crashes, and\\nthat will be confusing because it will seem like a violation of this ADR.\\n\\n## Decision\\nparameters beyond what is required to avoid local crashes.\\n**Consequences**: the main downsides are:\\n- The error messages could be better if generated locally.\\n- The server must validate all inputs correctly.\\n- Sometimes, we will need to validate inputs to avoid library crashes, and\\nthat will be confusing because it will seem like a violation of this ADR.\\n\\n\\n**Context**: many APIs impose restrictions on their parameters. Names may have\\nlength limits, or strings may need to conform to specific formats, or ranges\\nmust be non-empty. It is tempting to validate these arguments in the\\nclient-side, since we *know* the API will fail if it is allowed to continue with\\nthe invalid parameters. However, any such validation is redundant, since the\\nserver is going to validate its inputs as well. Furthermore, any such validation\\nis wasteful: most of the time the application will pass on valid arguments,\\nchecking them twice is just wasting client-side CPU cycles. Moreover, the\\nserver-side is the source of truth, so having the validations performed on the\\nclient side will require extra work if the server restrictions are ever\\nmodified.\\n**Decision**: the client libraries do not validate any of the request parameters\\nbeyond what is required to avoid local crashes.\\n**Consequences**: the main downsides are:\\n- The error messages could be better if generated locally.\\n- The server must validate all inputs correctly.\\n- Sometimes, we will need to validate inputs to avoid library crashes, and that\\nwill be confusing because it will seem like a violation of this ADR.\\n\\n## Decision\\nbeyond what is required to avoid local crashes.\\n**Consequences**: the main downsides are:\\n- The error messages could be better if generated locally.\\n- The server must validate all inputs correctly.\\n- Sometimes, we will need to validate inputs to avoid library crashes, and that\\nwill be confusing because it will seem like a violation of this ADR.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\n- The current `retry` logic is limited in the number of `retry` for failed requests. For example, at a screen where more than 3 different requests fail, the chance of each request being retried is very little, since they share a variable that counts the number of retry times.\\\\n- Using a (in memory) counter variable for `retry` is risky and difficult to control\\\\n'}, {'role': 'model', 'content': '1. Send the `retry` counter into request header for each request\\\\n2. Retrieve the `retry` count from request data\\\\n3. Validate with the maximum allowed. If `retry` in the limit, increment it then re-send a request network with updated `retry` counter\\\\n```\\\\n{\\\\n...\\\\nSET retriesCount from request extra map with key RETRY_KEY\\\\nIF _isAuthenticationError(dioError, retriesCount) THEN\\\\nINCREMENT retriesCount\\\\nADD retriesCount to header with key RETRY_KEY\\\\nCALL request again\\\\nENDIF\\\\n...\\\\n}\\\\n```\\\\n'}]"}
{"File Name":"operational-data-hub\/0006-openapi-guidelines.md","Context":"## Context\\nWe feel the need to use a naming convention for OAuth scopes.\\n","Decision":"When creating an OpenAPI based REST API, there are the guidelines to adhere to:\\nTo get started, create a valid OpenAPI specification yaml file inside an [app folder](https:\/\/github.com\/vwt-digital\/operational-data-hub\/blob\/develop\/coding_guidelines\/adr\/0003-repo-directory-structure.md).\\nNext, use the [OpenAPI generator](  https:\/\/github.com\/OpenAPITools\/openapi-generator) to generate you API server. This will create a Readme, requirements and an openapi_server folder with controllers, models, and a generated openapi\/openapi.yaml file.\\nNormally OpenAPIGenerator is a write-once tool (any changes will be overridden by generating a new project.) However, changing requirements will force you to regenerate a project (Especially when the structure of the data changes.) To ensure a correct usage of the openapigenerator tool, perform the following steps:\\n### Instruct the SAST scanner to ignore generated files.\\n-   For flake8, these are:\\n-   app\/setup.py\\n-   app\/openapi_server\/util.py\\n-   app\/openapi_server\/models\/*.py\\n-   For yamllint this is:\\n-   app\/openapi_server\/\\_\\_init__.py\\n-   app\/openapi_server\/openapi\/openapi.yaml\\n-   Any other files that need to be edited.\\n### Making changes to the OpenAPI spec.\\nWhenever you want to make changes to the specification, it is important not to edit the openapi_server\/openapi.yaml file, this will be overridden by the generate command. It is better to change the user created source yaml file and regenerate the project. When generating, for now we'll only regenerate the models. Any other changes will need to be done manually.\\n### Enable validate_responses.\\nTo ensure that the Spec is honored, enable validate responses in the openapiserver\/\\_\\_init__.py file (Example can be found [here](https:\/\/github.com\/vwt-digital-solutions\/snbplanningapi\/blob\/develop\/api_server\/openapi_server\/__init__.py#L20).)\\n### Use models to your advantage.\\nTo ensure correctness of the response returned by the API, the developer should always work with the models provided by the generator. A model\u2019s to_dict method can be used to construct the response.\\n### Naming conventions.\\nInside the controllers, operations should be named like so:\\n-   list_<object>s for the REST list operation (GET without id)\\n-   get_<object> for the REST get operation (GET with id)\\n-   create_<object> for the REST create operation. (POST without id)\\n-   update_<object> for the update operation (POST, PUT with id)\\n","tokens":17,"id":2693,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe need a consistent way to add menus within fragments.\\n\\n## Decision\\nEvery fragment handles its menu independently. This is achieved through calling setHasOptionsMenu(true)\\nand overriding the specific methods needed for creating the menu and handling selection events.\\n\\n\\n## Context\\nWe need a versioning scheme for our project.\\n\\n## Decision\\n- We will use [semver](https:\/\/semver.org\/) which is the most prevalent versioning scheme for node.js libraries and is easy to use.\\n- We will follow the rules of updating `MAJOR.MINOR.PATCH` version numbers and their semantic meaning.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe feel the need to use a naming convention for OAuth scopes.\\\\n'}, {'role': 'model', 'content': \"When creating an OpenAPI based REST API, there are the guidelines to adhere to:\\\\nTo get started, create a valid OpenAPI specification yaml file inside an [app folder](https:\/\/github.com\/vwt-digital\/operational-data-hub\/blob\/develop\/coding_guidelines\/adr\/0003-repo-directory-structure.md).\\\\nNext, use the [OpenAPI generator](  https:\/\/github.com\/OpenAPITools\/openapi-generator) to generate you API server. This will create a Readme, requirements and an openapi_server folder with controllers, models, and a generated openapi\/openapi.yaml file.\\\\nNormally OpenAPIGenerator is a write-once tool (any changes will be overridden by generating a new project.) However, changing requirements will force you to regenerate a project (Especially when the structure of the data changes.) To ensure a correct usage of the openapigenerator tool, perform the following steps:\\\\n### Instruct the SAST scanner to ignore generated files.\\\\n-   For flake8, these are:\\\\n-   app\/setup.py\\\\n-   app\/openapi_server\/util.py\\\\n-   app\/openapi_server\/models\/*.py\\\\n-   For yamllint this is:\\\\n-   app\/openapi_server\/\\\\_\\\\_init__.py\\\\n-   app\/openapi_server\/openapi\/openapi.yaml\\\\n-   Any other files that need to be edited.\\\\n### Making changes to the OpenAPI spec.\\\\nWhenever you want to make changes to the specification, it is important not to edit the openapi_server\/openapi.yaml file, this will be overridden by the generate command. It is better to change the user created source yaml file and regenerate the project. When generating, for now we'll only regenerate the models. Any other changes will need to be done manually.\\\\n### Enable validate_responses.\\\\nTo ensure that the Spec is honored, enable validate responses in the openapiserver\/\\\\_\\\\_init__.py file (Example can be found [here](https:\/\/github.com\/vwt-digital-solutions\/snbplanningapi\/blob\/develop\/api_server\/openapi_server\/__init__.py#L20).)\\\\n### Use models to your advantage.\\\\nTo ensure correctness of the response returned by the API, the developer should always work with the models provided by the generator. A model\u2019s to_dict method can be used to construct the response.\\\\n### Naming conventions.\\\\nInside the controllers, operations should be named like so:\\\\n-   list_<object>s for the REST list operation (GET without id)\\\\n-   get_<object> for the REST get operation (GET with id)\\\\n-   create_<object> for the REST create operation. (POST without id)\\\\n-   update_<object> for the update operation (POST, PUT with id)\\\\n\"}]"}
{"File Name":"opg-use-an-lpa\/0017-brute-force-protection-of-application-endpoints.md","Context":"## Context\\n> This ADR details the mitigation of brute force attacks on certain endpoints - other types of protection are out of\\n> scope.\\nWe have a need to mitigate brute force attacks on certain endpoints in the application. Currently these are:\\n* Actor Login\\n* One Time Passcode (adding an LPA)\\n* Viewer Code (viewing an LPA)\\nBrute force can be defined as multiple failed attempts to carry out these actions.\\n","Decision":"Protecting against these kinds of attacks is a problem in two parts; reliable identification of users and the recording\\nof attempts by that user against protected endpoints.\\n#### Identification\\nThe bare minimum of information that we can use to identify a user is their originating IP address. This is not without\\nissue as it can be spoofed, or more likely the user is behind a NAT or proxy layer (as will likely be the case with our\\ncorporate users). We can couple the browser sent headers that offer extra user identifying information (such as 'Accept'\\n'DNT', 'User-Agent' etc) in a hashing function to generate an ID that should more uniquely identify a user.\\nWe will identify users using a hash calculated using the incoming IP and associated headers.\\nWe will track this identity in the session to guard against changing headers within a session.\\n#### Attempt Tracking\\nWe will use a cache service (AWS Elasticache) to track failure attempts at each of the three endpoints defined above.\\nWe will use a per-endpoint moving window rate-limit to reduce brute force impact - these will be individually\\nconfigurable in terms of window size and request limit.\\n","tokens":95,"id":4846,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n**Context**: operations that change state in the server may complete with an\\nindeterminate status code. For example: an error in the 500 range may be\\nproduced by a middle-tier server after the operation was started by the backend.\\nRe-issuing the request may not work as expected for the application developer.\\n**Decision**: by default the library will retry all requests, including\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more\\nrare cases the operation may fail, for example, an operation to create an object\\nwith `IfGenerationMatch(0)` would fail on the second attempt.\\n\\n## Decision\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more\\nrare cases the operation may fail, for example, an operation to create an object\\nwith `IfGenerationMatch(0)` would fail on the second attempt.\\n\\n\\n## Context and Problem Statement\\nGiven a service is using the twirl template imports recommended by the play-frontend libraries. When a developer tries\\nto use an import with a name which is present in both libraries without explicitly importing the one they want. Then\\nthey will get a compilation error:\\n<img alt=\"Example of ambiguous import compilation exception\" src=\"0010-ambiguous-import-exception.png\" width=\"450\">\\nCurrently, we recommend the use of the wildcard twirl imports, but this possible consequence is undocumented behaviour.\\nThe compilation error can be resolved by explicitly importing the class they want.\\n## Decision Drivers\\n* Be unsurprising for developers (examples we give should be runnable without triggering an exception)\\n* As much as possible avoid breaking changes for existing services\\n\\n## Decision\\n* Be unsurprising for developers (examples we give should be runnable without triggering an exception)\\n* As much as possible avoid breaking changes for existing services\\nWe\\'ve decided to immediately implement Option 1.2 and defer further changes to be discussed as part of PLATUI-1294.\\n### Positive Consequences\\n* Less confusing for new developers where imports are coming from when we avoid recommending using TwirlKeys config.\\n* Developers can see as part of our recommended usage instructions the possibility for an ambiguous import exception and\\nwhat they can do to resolve them without.\\n### Negative Consequences\\n* Without an API change the naming of components and view models is not as intuitive as we think it could be which may\\nconfuse new developers.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\n> This ADR details the mitigation of brute force attacks on certain endpoints - other types of protection are out of\\\\n> scope.\\\\nWe have a need to mitigate brute force attacks on certain endpoints in the application. Currently these are:\\\\n* Actor Login\\\\n* One Time Passcode (adding an LPA)\\\\n* Viewer Code (viewing an LPA)\\\\nBrute force can be defined as multiple failed attempts to carry out these actions.\\\\n'}, {'role': 'model', 'content': \"Protecting against these kinds of attacks is a problem in two parts; reliable identification of users and the recording\\\\nof attempts by that user against protected endpoints.\\\\n#### Identification\\\\nThe bare minimum of information that we can use to identify a user is their originating IP address. This is not without\\\\nissue as it can be spoofed, or more likely the user is behind a NAT or proxy layer (as will likely be the case with our\\\\ncorporate users). We can couple the browser sent headers that offer extra user identifying information (such as 'Accept'\\\\n'DNT', 'User-Agent' etc) in a hashing function to generate an ID that should more uniquely identify a user.\\\\nWe will identify users using a hash calculated using the incoming IP and associated headers.\\\\nWe will track this identity in the session to guard against changing headers within a session.\\\\n#### Attempt Tracking\\\\nWe will use a cache service (AWS Elasticache) to track failure attempts at each of the three endpoints defined above.\\\\nWe will use a per-endpoint moving window rate-limit to reduce brute force impact - these will be individually\\\\nconfigurable in terms of window size and request limit.\\\\n\"}]"}
{"File Name":"GoStudy\/0002-use-c4-for-architecture-diagrams.md","Context":"## Context\\nWe need to document the architecture (precisely, the static model ie. diagrams) of the project.\\n","Decision":"We will use Simon Brown C4 model.\\n","tokens":26,"id":427,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nAPI should have the documentation, we should define the process of delivering API and documentation.\\n\\n## Decision\\nBy defining APIs outside the code, we want to facilitate early review feedback and also a development discipline that focus service interface design on:\\n- profound understanding of the domain and required functionality\\n- generalized business entities \/ resources, i.e. avoidance of use case specific APIs\\n- clear separation of WHAT vs. HOW concerns, i.e. abstraction from implementation aspects \u2014 APIs should be stable even if we replace complete service implementation including its underlying technology stack\\nUseful links:\\n- https:\/\/opensource.zalando.com\/restful-api-guidelines\/#api-first\\n\\n\\n## Context\\nWe have to decide for a frontend application and a styling\/component framework.\\n\\n## Decision\\nWe stick to [Angular](https:\/\/angular.io\/) together with [NG ZORRO Ant Design](https:\/\/ng.ant.design\/) and [NG Alain](https:\/\/ng-alain.com\/).\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need to document the architecture (precisely, the static model ie. diagrams) of the project.\\\\n'}, {'role': 'model', 'content': 'We will use Simon Brown C4 model.\\\\n'}]"}
{"File Name":"verify-proxy-node\/20190320-anti-affinity.md","Context":"## Context\\neIDAS project has been challenged with some security concerns provided by\\nNational Cyber Security Centre (NCSC). One of these concerns was a hypothesis\\nthat an attacker could compromise the public facing service (Gateway) by\\nproviding insecure SAML, could perform a Kernel Attack against the more fragile\\nand sensitive but not exposed service (Translator) which has capability to\\ninteracting with Hardware Security Module (HSM).\\nProvided we're running on GDS Supported Platform (GSP), Reliability Engineering\\n(RE) team has suggested separating any service capable of interacting with HSM\\ninto a different physical machine. This should prevent this sort of attack from\\nhappening.\\n","Decision":"We'll trust that the GSP is be capable of deploying sensitive applications away\\nfrom the ones that are interacting with HSM.\\n","tokens":142,"id":4072,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe are working on a new feature of Form Builder where a form creator (user) can\\nopt to have submissions (citizen answers) sent as JSON to an endpoint of their choosing.\\nThese endpoints can be any url, but will likely be CMS or case management systems.\\nThe first use case of this is the HMCTS complaints form. This form requires an\\nintegration to HMCTS' Optics system. We will be creating a custom adapter that receives\\nthe JSON output from Form Builder, and then generates the complaint in Optics. This\\nadapter will live outside the Form Builder infrastructure. In the future, the hope\\nis that teams will create their own adapters in order to integrate Form Builder\\nwith various external systems.\\nThis ADR is regarding how to secure this JSON output from Form Builder to the\\nendpoint set by the user.\\nThreats\\n- Payload may be intercepted, exposing sensitive user answers.\\n- Payload may be modified in transit, with potentially damaging consequences.\\n- POST requests could be made to the receiver (adapter) from sources other than Form Builder,\\ncreating records in their systems that don't reflect user submissions.\\nRisks\\n- Reputational damage due to data breaches\\n- Security threats from modified payloads\\nMistakes from the User\\n- Users could potentially enter an incorrect endpoint, which would deliver sensitive\\ndata to an incorrect endpoint.\\n- Users could share security information accidentally, allowing malicious actors\\nto receive payloads.\\n\\n## Decision\\n### TLS over HTTP\\nUse TLS over HTTP (HTTPS). This would ensure\\nencrypted data in transit, which [satisfies MOJ standards](https:\/\/ministryofjustice.github.io\/security-guidance\/standards\/cryptography\/#cryptography).\\nWe came to the conclusion that mutual TLS doesn\u2019t really bring any benefits and\\ninvolves way too much management overhead in this context, so agreed on normal\\nTLS + payload encryption with a pre-shared secret.\\n### Encrypted JSON Payload\\nEncryption of the payload is possible with JWE (JOSE toolkit). This would require\\ndecryption at the other end. Payload encryption is preferred over TLS alone as TLS\\nis often terminated at the edge of a large network with communications travelling in the clear\\ninside that network. Given the sensitivity of the submitted data for some forms\\nand that it may need to be relied on in court, we need more confidence that only\\nthe intended recipient can read the data and that other actors on that network\\ncan\u2019t impersonate FB by sending other requests to the endpoint.\\n### Shared Secret\\nThe shared secret will be set as an ENV var in the publisher to be consumed by\\nthe form's runner instance. The shared secret will be used to encrypt \/ decrypt\\nthe payload. This shared secret would ideally be system generated rather than user\\ngenerated to ensure that it is appropriate for the encryption method we choose.\\n### Certificates vs Shared Secret\\nBoth certificates and shared secrets can be used for signing and de-serialising the payload.\\nGiven users may not be technical, a shared secret would be preferred as requiring users\\nto generate and upload certificates may be too much to ask.\\n### Validation of remote endpoint\\nThe Form Builder system should validate the endpoint that is entered by the user.\\nAt minimum this should be an HTTPS endpoint and `*.gov.uk`.\\n### Overview of Solution\\nForm Builder:\\n- Connects with adapter via HTTPS using a ruby library such as [Net:HTTP](https:\/\/ruby-doc.org\/stdlib-2.6.3\/libdoc\/net\/http\/rdoc\/Net\/HTTP.html).\\n- Connection with TLS using `Net::HTTP` gem or similar.\\n- Encrypts JSON payload using a shared secret and JWE protocol.\\n- Sends as POST request.\\nAdapter:\\n- Receives HTTPS POST request from Form Builder.\\n- Decrypts JSON payload using shared secret.\\n\\n\\n## Context\\nOpenBounty is a system which has value flowing through it.\\nNaturally security is a concern that should be taken into consideration.\\nCurrently an attacker might get access to an account of a team member\\nand pose as that developer, merging PRs and pushing changes.\\nStatus.im as a company is also encouraging the use of GPG signing and\\nhas a Pull Request check in place on Github. This check will mark PRs\\nas failing if the commits come from an organization member and have not\\nbeen GPG-signed.\\n\\n## Decision\\nIn order to verify that commits in the repository are actually authored by the specified\\nauthor we adopt [GPG signing of Git commits](https:\/\/git-scm.com\/book\/id\/v2\/Git-Tools-Signing-Your-Work).\\nThis will allow us to verify authenticity of the author information saved in\\na Git commit and make workflows like deploying on push safer.\\nIt also introduces some complexity because contributors who want to sign\\ntheir commits need to set up the appropriate tooling. Due to that we will\\nnot require outside contributors to sign their commits for now.\\nAdopting GPG signing for contributors will also make our PR checks pass\\nallowing us to more easily discern actually broken and working PRs.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\neIDAS project has been challenged with some security concerns provided by\\\\nNational Cyber Security Centre (NCSC). One of these concerns was a hypothesis\\\\nthat an attacker could compromise the public facing service (Gateway) by\\\\nproviding insecure SAML, could perform a Kernel Attack against the more fragile\\\\nand sensitive but not exposed service (Translator) which has capability to\\\\ninteracting with Hardware Security Module (HSM).\\\\nProvided we're running on GDS Supported Platform (GSP), Reliability Engineering\\\\n(RE) team has suggested separating any service capable of interacting with HSM\\\\ninto a different physical machine. This should prevent this sort of attack from\\\\nhappening.\\\\n\"}, {'role': 'model', 'content': \"We'll trust that the GSP is be capable of deploying sensitive applications away\\\\nfrom the ones that are interacting with HSM.\\\\n\"}]"}
{"File Name":"uniprot-rest-api\/0004-lombok.md","Context":"## Context\\nJava projects often contain a large amount of boilerplate code, e.g., defining data\/value classes, builders, etc. All\\nsuch code follows a certain pattern and needs testing -- and writing both of these can be error prone. A library\\nthat enables cutting down boilerplate code, and which generates tested code would be beneficial to the project.\\n","Decision":"We will use the [Lombok](https:\/\/projectlombok.org\/) library to reduce the amount of boilerplate code we need to write.\\n","tokens":73,"id":1475,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nMultiple projects need to publish or subscribe to messages from the `bus`.\\nProjects are written in a fixed set of supported programming languages.\\nThere is a set of non-functional requirements to cater for in integrations, especially when listening to queues (retries, graceful shutdown, timeouts, etc).\\n\\n## Decision\\nAccess the bus exclusively through the eLife SDKs, either for publishing or subscribing to messages:\\n- [bus-sdk-php](https:\/\/github.com\/elifesciences\/bus-sdk-php)\\n- [bus-sdk-python](https:\/\/github.com\/elifesciences\/bus-sdk-python)\\n\\n\\n## Context and Problem Statement\\nOn the Application Services team, we have successfully built several re-useable components for sync- and storage-related browser\\nfunctionality by following what we\\'ve dubbed the \"rust-components\" approach: write the bulk of the code in rust\\nso we can cross-compile it for different target platforms, have it expose a C-compatible FFI layer, then write a small amount\\nof FFI bindings code to expose the functionality to each of several different target languages (e.g. Swift and Kotlin).\\nThe FFI layer and foreign-language bindings code is currently written by hand, a tedious and potentially error-prone\\nprocess.\\nGiven that we expect to build additional components in this style in the future, and expect more teams at Mozilla to\\ndo the same, can we increase the efficiency and reliability of this work by auto-generating some of this code?\\n## Decision Drivers\\n* Reduce time taken to launch a new rust component.\\n* Improve maintainability of existing rust components.\\n* Reduce possibility of errors in hand-written foreign language bindings code.\\n* Continue shipping components on a regular cadence.\\n\\n## Decision\\n* Reduce time taken to launch a new rust component.\\n* Improve maintainability of existing rust components.\\n* Reduce possibility of errors in hand-written foreign language bindings code.\\n* Continue shipping components on a regular cadence.\\nChosen option:\\n* **Option D: Write and maintain a custom tool that automates our current best practices**\\nOn balance, this option provides us with the best tradeoff of potential upside and the ability to limit downside.\\nIf the approach succeeds then we expect to realize significant improvement in maintenance costs of rust-components\\ncode by reducing boilerplate and human error. Building our own will involve the least up-front investment before\\nwe can start to show results, because we did not identify any existing tools that were a close-enough fit for our needs.\\nThe first versions of the tool don\\'t have to be perfect, or even particularly *good* - they just have to have a\\nbetter value-proposition than writing the generated code by hand.\\nWe accept the risk that writing our own tool for this may turn out to be much more complex than expected, and\\nwill mitigate it by aggressively time-boxing initial prototypes, by developing it in parallel with a real shipping\\nconsumer with real deadlines, and by regularly asking the hard questions about whether the approach is working out.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nJava projects often contain a large amount of boilerplate code, e.g., defining data\/value classes, builders, etc. All\\\\nsuch code follows a certain pattern and needs testing -- and writing both of these can be error prone. A library\\\\nthat enables cutting down boilerplate code, and which generates tested code would be beneficial to the project.\\\\n'}, {'role': 'model', 'content': 'We will use the [Lombok](https:\/\/projectlombok.org\/) library to reduce the amount of boilerplate code we need to write.\\\\n'}]"}
{"File Name":"cena\/0008-package-application-in-docker-image.md","Context":"## Context\\n`menu-generation` application is packaged as a [Spring Boot](0005-use-spring-framework.md) executable JAR file.\\nRunning acceptance tests on development machine or during Continuous Integration must be quick, easy and the least\\ndependent of the underlying system.\\n[Docker](https:\/\/www.docker.com\/) is a widespread container based solution that can be used during development lifecycle\\non most operating systems as well as in well established Cloud solutions such as [Kubernetes](https:\/\/kubernetes.io\/).\\n","Decision":"`menu-generation` application will be packaged as a docker image. A `docker-compose` definition will also be provided\\nto help running the application and its dependencies in a consistent and isolated environment.\\n","tokens":107,"id":653,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe want to be able to run feature and end to end tests in browsers and as part\\nof continuous integration. We only want to write tests once, and have them run\\nin all environments.\\n[Selenium WebDriver](https:\/\/docs.seleniumhq.org\/projects\/webdriver\/\/) is a\\nbrowser automation framework. It supports all major browsers and is supported by\\ncloud based browser testing services like\\n[BrowserStack](https:\/\/www.browserstack.com\/).\\n\\n## Decision\\nUse Selenium WebDriver to write feature and end to end tests via Jest.\\n\\n\\n## Context\\nRunning a service like cljdoc requires infrastructure of various kinds. For\\nthe initial prototype I generated a static site stack using [a library I\\ncreated](https:\/\/github.com\/confetti-clj\/confetti) which uses AWS\\nCloudFormation under the hood. This has been useful while prototyping but\\nwith the addition of [a server component](\/doc\/adr\/0009-introduce-server-component.md)\\nand potentially other infrastructure components in the future it becomes\\nharder to stick to Cloudformation.\\n[Terraform](https:\/\/www.terraform.io\/) seems to be a popular tool in this space\\nand provides all the functionality of my library as well as support for using\\nmultiple cloud providers.\\n\\n## Decision\\nRebuild the static site stack using Terraform. Once that has been transitioned,\\nextend the Terraform configuration to include a server that we can use to host\\nthe server component of cljdoc.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\n`menu-generation` application is packaged as a [Spring Boot](0005-use-spring-framework.md) executable JAR file.\\\\nRunning acceptance tests on development machine or during Continuous Integration must be quick, easy and the least\\\\ndependent of the underlying system.\\\\n[Docker](https:\/\/www.docker.com\/) is a widespread container based solution that can be used during development lifecycle\\\\non most operating systems as well as in well established Cloud solutions such as [Kubernetes](https:\/\/kubernetes.io\/).\\\\n'}, {'role': 'model', 'content': '`menu-generation` application will be packaged as a docker image. A `docker-compose` definition will also be provided\\\\nto help running the application and its dependencies in a consistent and isolated environment.\\\\n'}]"}
{"File Name":"cljdoc\/0020-support-links-from-docs-to-SCM-when-testing-locally.md","Context":"## Context\\n### Background\\nCljdoc imports documentation from clojars artifacts and SCM (Source Code Management) repositories.\\nThe SCM is a git repository and usually one hosted on GitHub.\\nDocumentation is imported to cljdoc from 2 distinct sources:\\n1. API docstrings from sources in a clojars artifact\\n2. Stand-alone articles such as `README.md` and documentation under `doc\/**` from an SCM repository.\\nIt is not uncommon for documentation to contain links to other files under in the SCM repository.\\nIn these cases, cljdoc automatically rewrites these links to target the correct resource.\\nIf the SCM document is an article that has been imported to cljdoc, the link will target the imported article hosted by cljdoc,\\notherwise the link will target the hosted SCM, which in most cases, is GitHub.\\n### Issues with Testing Locally\\nBefore a project is released to clojars and automatically imported to cljdoc, an author will often want to preview their\\ndocumentation by running cljdoc locally.\\nBecause cljdoc works from the published artifact, when testing locally, authors will first publish their project to their local maven repository.\\nThis will remain true for whatever solution we choose here.\\nA current limitation of running cljdoc locally, as directed by current guidance, is that direct links to SCM files cannot be previewed.\\nConcrete examples of non-functioning links are images, source code and articles.\\nThese broken links make it difficult to get a real sense of how docs will appear and behave on cljdoc.\\n","Decision":"After evaluating several alternatives (see below), I realize that our guidance on running cljdoc locally is more\\nthe issue than anything else.\\nI propose that we complement our guidance to include a workflow of importing from hosted SCM (i.e. GitHub) rather than just the local file system.\\nThe tester would commit their changes and push to a branch on their hosted SCM.\\nThey would then ingest docs from this branch.\\nAt doc browse time, because ingested docs are referencing a hosted SCM, the links to the SCM would automatically work.\\nThe advantages are no extra local setup and an SCM preview that truly reflects production.\\nA minor con is that the author has to push to a hosted SCM.\\n**#2 [rejected, not viable] rewrite SCM links to target the file system**\\nTargeting `file:\/\/` has the advantage of having nothing extra to run for the tester but is\\nsimply not technically feasible; the browser prohibits it for security reasons.\\n**#3 [rejected] rewrite SCM links to target a local static web server**\\nThe tester would launch a static web server from their project root.\\nA locally running cljdoc would render SCM links to the static web server.\\nWhile this would be easy to setup for the tester, the fetches would be to an unversioned\\nfile system rather than SCM. Also the fetches would always be raw, if the browser\/server\\ncombo cannot figure out the MIME type, it would prompt for save\/download. And links to source\\nwould not support navigating to a specific line number.\\nAlthough this was my original plan, because it does not offer a full preview,\\nI have since rejected it.\\n**#3 [rejected]  rewrite SCM links to target a local git instaweb server**\\nThe tester would run `git instaweb` from his\/her project root to startup a local SCM web server.\\nThe advantages to this approach include easy setup for tester, fetches from SCM are versioned,\\nsupport for raw fetches and support for formatted fetches with ability to navigate to line number.\\nThe cons are that git instaweb URLs and UI are not GitHub-like.\\nWe are also relying on the instaweb implementation remaining static, which seems to be a safe bet, but who knows?\\nNote that I have also experimented with [git instaweb via docker](https:\/\/hub.docker.com\/r\/leeread\/gitweb-quick).\\nAlthough this was my secondary choice, I have since decided to that option #1 is simpler.\\n**#4 [rejected] rewrite SCM links to target a local GitHub URL compatible server**\\nThere are GitHub clones like [gitea](https:\/\/gitea.io\/en-us\/) and [gogs](https:\/\/gogs.io\/).\\nPerhaps one of these could be setup to deliver a local project.\\nI had a brief look, but did not qucikly see something that would work, even via docker.\\nThis does not mean this couldn't work, just that I gave up in favor of something simpler.\\nThe pros are GitHub-like URLs and UI.\\nThe cons are a more difficult setup by the user, more moving parts and the maintenance burden\\nof tracking an external component.\\n**#5 [rejected] rewrite SCM links to target a local custom GitHub compatible server**\\nI suppose this would not be terribly hard but it is also not something I want us to have to maintain.\\n**#6 [rejected] add preview support to cljdoc**\\nI'm not sure how this would work, but we could add preview support to the cljdoc production server.\\nThis would be a larger project and have to address many more concerns, so I am rejecting for now.\\n","tokens":330,"id":2599,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nOur current update process has several flaws. These lead towards an\\nunreliable autoupdate process.\\nThe existing autoupdate functionality works as follows:\\nOn posix:\\n1. A new version is downloaded into the staging area\\n2. The new version is moved to replace the current binary\\n3. The new binary is exec\\'ed\\nOn Windows:\\n1. A new version is downloaded into the staging area\\n2. The current exe is moved out of the way\\n3. The new exe is moved into place\\n4. The service is restarted\\nThere are some inherent problems with this design:\\n* Running binaries no longer appear on disk\\n* Files on disk no longer match what was installed via the package manager\\n* On windows, there is a failure condition where the new binary isn\\'t\\nmoved into place, and the next service restart causes a service\\nfailure (https:\/\/github.com\/kolide\/launcher\/issues\/509)\\nThese lead us to needing a new update process.\\n### Assumptions in TUF\\nAdditionally, our implementation must work within the assumptions of\\n[TUF](https:\/\/godoc.org\/github.com\/kolide\/updater\/tuf).\\nTUF has a very simple model. It\\'s designed to notice remote metadata\\nchanges for a single file, download it into a given location, and\\ntrigger into a callback function.\\nBecause it has a single file model, we cannot easily store these by\\nversion.\\nFurthermore, updates happen when the tuf metadata changes. Not when on\\na binary mismatch. This means that if the local `launcher` executable\\nchanges, it will not be refreshed until the tuf metadata\\nchanges. This makes testing somewhat harder.\\n\\n## Decision\\nInstead of replacing the running binary, we will create a directory to\\nstore updates in. These updates will be launched from there. This new\\nflow will look like:\\n1. a new binary is downloaded into the staging area\\n2. It is moved into the updates directory, and stored by date\\n3. It is spawned from the updates directory\\nThe implementation is documented as part of [\/pkg\/autoupdate], or\\n[godoc](https:\/\/godoc.org\/github.com\/kolide\/launcher\/pkg\/autoupdate)\\n\\n\\n## Context\\nWe provide a CD Pipeline for SAP Cloud Platform applications, which adheres to the \"build once\" principle.\\nStill, the pipeline does each build step, like building the artifact, running tests or static code checks in separate stages.\\nWe use Maven for building the application, using Gradle or other build tools is not an option without much rework.\\nFor this document, the term \"release\" (noun) refers to a uniquely identifiable version of software.\\nThis includes the source code version (commit or tag) from which the artifacts are built, and the build artifacts themselves.\\nThe verb \"to release\" refers to the process of creating a new release.\\nPart of this process is to determine the version number of the release candidate.\\nThe release candidate becomes a release, when its build pipeline succeeded, and the build artifact is deployed to the Cloud Platform and the artifact repository.\\nWith Maven, this is usually facilitated with the [Maven Release Plugin](http:\/\/maven.apache.org\/maven-release\/maven-release-plugin\/).\\nUsing this plugin does not satisfy our requirements as described below.\\nThe pipeline automatically uploads build artifacts to an artifact repository and deploys the app to the Cloud Platform.\\nArtifact uploads and deployments happen only for commits on the so co called \"productive branch\" (`master` by default).\\nMaven\\'s versioning schema appends `SNAPSHOT` suffix to versions which are not released.\\nA version like `1.0.2-SNAPSHOT` does not say from which commit this was built.\\nArtifact repositories might delete `SNAPSHOT` versions after some time, because those are not releases.\\n\\n## Decision\\nWe implement an automated versioning schema, in which each commit to the productive branch is equivalent to a new release.\\nThis feature is enabled by default, but can be disabled.\\nThe version number shall contain a human readable _build_ time stamp (ISO 8601, without colons for file-name compatibility on Windows, always `UTC`) and the git commit id of the most recent commit to `master`, for example `2.7.3-2018-03-02T114757UTC_ff46bb0f00a663018f3efea697b2fb5e86fe6d41`.\\nAn auto-created release does not imply creating a tag in the repository.\\nCreating tags may be done manually to mark noteworthy versions by the developer.\\n### Reasoning\\n* Each commit on `master` is a new release: We assume the work happens in feature branches, which are merged once they implement a feature and meet the team\\'s definition of done.\\nMerging to `master` is implicitly approval for release.\\n* Feature can be disabled: You might still have builds which don\\'t follow this release approach.\\nFor those, it must be possible to disable automatic versioning.\\n* _Build_ instead of _commit_ time stamp: This implies that multiple builds of the same commit have a different version number.\\nThis avoids conflicts, when uploading a second build of a commit to a artifact repository.\\n* Always ISO 8601 date-time format: Can be sorted in lexical order which results in a chronological list.\\n* Always `UTC`: Most simple solution, avoids daylight saving time issues and is unambiguous for teams working distributed in multiple time zones.\\n* Don\\'t create git tags: The version number contains the commit id, which is sufficient to check out this particular version.\\nIf we created tags automatically for each version, tags would be cluttered very quickly.\\nTags still can be used to mark a version on purpose, with semantic versioning if desired.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\n### Background\\\\nCljdoc imports documentation from clojars artifacts and SCM (Source Code Management) repositories.\\\\nThe SCM is a git repository and usually one hosted on GitHub.\\\\nDocumentation is imported to cljdoc from 2 distinct sources:\\\\n1. API docstrings from sources in a clojars artifact\\\\n2. Stand-alone articles such as `README.md` and documentation under `doc\/**` from an SCM repository.\\\\nIt is not uncommon for documentation to contain links to other files under in the SCM repository.\\\\nIn these cases, cljdoc automatically rewrites these links to target the correct resource.\\\\nIf the SCM document is an article that has been imported to cljdoc, the link will target the imported article hosted by cljdoc,\\\\notherwise the link will target the hosted SCM, which in most cases, is GitHub.\\\\n### Issues with Testing Locally\\\\nBefore a project is released to clojars and automatically imported to cljdoc, an author will often want to preview their\\\\ndocumentation by running cljdoc locally.\\\\nBecause cljdoc works from the published artifact, when testing locally, authors will first publish their project to their local maven repository.\\\\nThis will remain true for whatever solution we choose here.\\\\nA current limitation of running cljdoc locally, as directed by current guidance, is that direct links to SCM files cannot be previewed.\\\\nConcrete examples of non-functioning links are images, source code and articles.\\\\nThese broken links make it difficult to get a real sense of how docs will appear and behave on cljdoc.\\\\n'}, {'role': 'model', 'content': \"After evaluating several alternatives (see below), I realize that our guidance on running cljdoc locally is more\\\\nthe issue than anything else.\\\\nI propose that we complement our guidance to include a workflow of importing from hosted SCM (i.e. GitHub) rather than just the local file system.\\\\nThe tester would commit their changes and push to a branch on their hosted SCM.\\\\nThey would then ingest docs from this branch.\\\\nAt doc browse time, because ingested docs are referencing a hosted SCM, the links to the SCM would automatically work.\\\\nThe advantages are no extra local setup and an SCM preview that truly reflects production.\\\\nA minor con is that the author has to push to a hosted SCM.\\\\n**#2 [rejected, not viable] rewrite SCM links to target the file system**\\\\nTargeting `file:\/\/` has the advantage of having nothing extra to run for the tester but is\\\\nsimply not technically feasible; the browser prohibits it for security reasons.\\\\n**#3 [rejected] rewrite SCM links to target a local static web server**\\\\nThe tester would launch a static web server from their project root.\\\\nA locally running cljdoc would render SCM links to the static web server.\\\\nWhile this would be easy to setup for the tester, the fetches would be to an unversioned\\\\nfile system rather than SCM. Also the fetches would always be raw, if the browser\/server\\\\ncombo cannot figure out the MIME type, it would prompt for save\/download. And links to source\\\\nwould not support navigating to a specific line number.\\\\nAlthough this was my original plan, because it does not offer a full preview,\\\\nI have since rejected it.\\\\n**#3 [rejected]  rewrite SCM links to target a local git instaweb server**\\\\nThe tester would run `git instaweb` from his\/her project root to startup a local SCM web server.\\\\nThe advantages to this approach include easy setup for tester, fetches from SCM are versioned,\\\\nsupport for raw fetches and support for formatted fetches with ability to navigate to line number.\\\\nThe cons are that git instaweb URLs and UI are not GitHub-like.\\\\nWe are also relying on the instaweb implementation remaining static, which seems to be a safe bet, but who knows?\\\\nNote that I have also experimented with [git instaweb via docker](https:\/\/hub.docker.com\/r\/leeread\/gitweb-quick).\\\\nAlthough this was my secondary choice, I have since decided to that option #1 is simpler.\\\\n**#4 [rejected] rewrite SCM links to target a local GitHub URL compatible server**\\\\nThere are GitHub clones like [gitea](https:\/\/gitea.io\/en-us\/) and [gogs](https:\/\/gogs.io\/).\\\\nPerhaps one of these could be setup to deliver a local project.\\\\nI had a brief look, but did not qucikly see something that would work, even via docker.\\\\nThis does not mean this couldn't work, just that I gave up in favor of something simpler.\\\\nThe pros are GitHub-like URLs and UI.\\\\nThe cons are a more difficult setup by the user, more moving parts and the maintenance burden\\\\nof tracking an external component.\\\\n**#5 [rejected] rewrite SCM links to target a local custom GitHub compatible server**\\\\nI suppose this would not be terribly hard but it is also not something I want us to have to maintain.\\\\n**#6 [rejected] add preview support to cljdoc**\\\\nI'm not sure how this would work, but we could add preview support to the cljdoc production server.\\\\nThis would be a larger project and have to address many more concerns, so I am rejecting for now.\\\\n\"}]"}
{"File Name":"twig-infrastructure\/0004-use-cloudwatch-logs-for-log-aggregation.md","Context":"## Context\\nLogs are generated by each instance of a container.  We need the ability to see and search across all instances.\\n","Decision":"We will use Cloudwatch Logs to aggregate our logs.  We will utliize Cloudwatch Alarms to notify when ERROR logs are generated\\n","tokens":27,"id":352,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nLogs are generated by each instance of a container.  We need the ability to see and search across all instances.\\n\\n## Decision\\nWe will use Cloudwatch Logs to aggregate our logs.  We will utliize Cloudwatch Alarms to notify when ERROR logs are generated\\n\\n\\n## Context\\nLogs are generated by each instance of a container.  We need the ability to see and search across all instances.\\n\\n## Decision\\nWe will use Cloudwatch Logs to aggregate our logs.  We will utliize Cloudwatch Alarms to notify when ERROR logs are generated\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nLogs are generated by each instance of a container.  We need the ability to see and search across all instances.\\\\n'}, {'role': 'model', 'content': 'We will use Cloudwatch Logs to aggregate our logs.  We will utliize Cloudwatch Alarms to notify when ERROR logs are generated\\\\n'}]"}
{"File Name":"publicservice-registry\/0001-record-architecture-decisions.md","Context":"## Context\\nWe need to record the architectural decisions made on this project.\\n","Decision":"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\n### Changes to Nygard's approach\\nWe will keep ADRs in the project repository under `docs\/adr\/NNN-explanation-of-adr.md`.\\n","tokens":16,"id":3130,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as [described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n## Context\\nWe need to record the architectural decisions made on this project.\\n\\n## Decision\\nWe will use Architecture Decision Records, as\\n[described by Michael Nygard](http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions).\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe need to record the architectural decisions made on this project.\\\\n'}, {'role': 'model', 'content': \"We will use Architecture Decision Records, as described by Michael Nygard in this article: http:\/\/thinkrelevance.com\/blog\/2011\/11\/15\/documenting-architecture-decisions\\\\n### Changes to Nygard's approach\\\\nWe will keep ADRs in the project repository under `docs\/adr\/NNN-explanation-of-adr.md`.\\\\n\"}]"}
{"File Name":"react-library-template\/0003-use-rollup-to-build-distributables.md","Context":"## Context\\nWe want to be able to distribute this library to me ingested by TypeScript or\\nplain JavaScript (both commonJS and module) applications.\\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\\nsupport for TypeScript and simple configuration.\\n","Decision":"We will build distributables using Rollup.js.\\n","tokens":62,"id":4227,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nWe want to be able to distribute this library to me ingested by TypeScript or\\nplain JavaScript (both commonJS and module) applications.\\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\\nsupport for TypeScript and simple configuration.\\n\\n## Decision\\nWe will build distributables using Rollup.js.\\n\\n\\n## Context\\nWe want to be able to distribute this library to me ingested by TypeScript or\\nplain JavaScript (both commonJS and module) applications.\\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\\nsupport for TypeScript and simple configuration.\\n\\n## Decision\\nWe will build distributables using Rollup.js.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nWe want to be able to distribute this library to me ingested by TypeScript or\\\\nplain JavaScript (both commonJS and module) applications.\\\\n[Rollup](https:\/\/rollupjs.org\/guide\/en\/) is a popular JavaScript bundler with\\\\nsupport for TypeScript and simple configuration.\\\\n'}, {'role': 'model', 'content': 'We will build distributables using Rollup.js.\\\\n'}]"}
{"File Name":"heroes-board-game\/0003-broadcast-messages.md","Context":"## Context and Problem Statement\\nHeroes can kill nearby enemies on a certain range, but there is no central place in the game that knows every hero position on the board at any time. Each one of them is the source of truth.\\nIn order to do that a hero must broadcast the intent to the rest of players.\\nHow to send messages to all heroes at once?\\n## Decision Drivers\\n* Less possible changes\\n* Performance is not a concern\\n","Decision":"* Less possible changes\\n* Performance is not a concern\\nChosen option: \"`Supervisor.which_children\/1`\", because it comes out best.\\n","tokens":93,"id":3165,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n### Context\\nThese hunts are designed to be 1-3 hour long or longer. Some players may not\\nhave that time to block out. There should be a way to save the progress of a\\nhunt inside the game.\\n### Decision\\nWe do not want to administer a database and everything that comes along with\\nthat (users, access, groups, etc). Store progress on a hunt in a local file\\nor \"database\" thing. Allow for resets of the hunt when loading.\\n### Consequences\\n- Removes need to admin a database\\n- Removes network complexity\\n- Keeps costs low\\n\\n## Decision\\nWe do not want to administer a database and everything that comes along with\\nthat (users, access, groups, etc). Store progress on a hunt in a local file\\nor \"database\" thing. Allow for resets of the hunt when loading.\\n### Consequences\\n- Removes need to admin a database\\n- Removes network complexity\\n- Keeps costs low\\n\\n\\n## Context and Problem Statement\\nHeroes are now permanent, which means they are always restarted.\\nShould they be?\\n## Decision Drivers\\n* A Hero must not exist on the system once the player leaves the game (normal exit)\\n* Restoring them from failures is not a requirement but nice to have\\n* Project is not finished, lacks most important features\\n\\n## Decision\\n* A Hero must not exist on the system once the player leaves the game (normal exit)\\n* Restoring them from failures is not a requirement but nice to have\\n* Project is not finished, lacks most important features\\nChosen option: \"Restart `:temporary`\", because it has the minimum impact.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\nHeroes can kill nearby enemies on a certain range, but there is no central place in the game that knows every hero position on the board at any time. Each one of them is the source of truth.\\\\nIn order to do that a hero must broadcast the intent to the rest of players.\\\\nHow to send messages to all heroes at once?\\\\n## Decision Drivers\\\\n* Less possible changes\\\\n* Performance is not a concern\\\\n'}, {'role': 'model', 'content': '* Less possible changes\\\\n* Performance is not a concern\\\\nChosen option: \"`Supervisor.which_children\/1`\", because it comes out best.\\\\n'}]"}
{"File Name":"island.is\/0011-open-source-license.md","Context":"## Context and Problem Statement\\nIt is the offical policy of the Digital Iceland and stated in the Techical Direction that it is to be implemented as free and open source. Open source software by definition is open to anyone to use, modify, distribute and study. These permissions are enforced an open source license. There are a number of well-known and widely used open source licenses available and we need to choose a license that best fits the goals of digital iceland.\\nThere are two main types of open source licences: more permissive licences that confer broad freedoms and minimal obligations (e.g., the MIT, BSD and the Apache 2.0 licences); and sharealike licences that require licensing adaptations with the same licence if they distribute them (e.g., the GNU GPL).\\nDevelopment for Digital Iceland will be open and free with minimum complications for development for all involved. Reuse and transparency will be promoted.\\n## Decision Drivers\\n- The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.\\n- It is important to build on the experience of similar government led inititives in other countries.\\n- Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.\\n- It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.\\n","Decision":"- The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.\\n- It is important to build on the experience of similar government led inititives in other countries.\\n- Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.\\n- It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.\\nThe MIT license was chosen, for the following reasons:\\n- It is the least restrictive of the licenses.\\n- It is very consise, simple and easy to understand and therefore should be clear to users and developers.\\n- Digital Iceland does not require protection of patents or existing intelletual property.\\n- Well known government lead initiatives like uk.gov and X-Road use the MIT license.\\n- The MIT license is the best known and most widely used free and open-source license in the world.\\n","tokens":284,"id":1110,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nIt is the offical policy of the Digital Iceland and stated in the Techical Direction that it is to be implemented as free and open source. Open source software by definition is open to anyone to use, modify, distribute and study. These permissions are enforced an open source license. There are a number of well-known and widely used open source licenses available and we need to choose a license that best fits the goals of digital iceland.\\nThere are two main types of open source licences:  more permissive licences that confer broad freedoms and minimal obligations (e.g., the MIT, BSD and the Apache 2.0 licences); and sharealike licences that require licensing adaptations with the same licence if they distribute them (e.g., the GNU GPL).\\nDevelopment for Digital Iceland will be open and free with minimum complications for development for all involved. Reuse and transparency will be promoted.\\n## Decision Drivers\\n* The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.\\n* It is important to build on the experience of similar government led inititives in other countries.\\n* Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.\\n* It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.\\n\\n## Decision\\n* The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.\\n* It is important to build on the experience of similar government led inititives in other countries.\\n* Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.\\n* It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.\\nThe MIT license was chosen, for the following reasons:\\n* It is the least restrictive of the licenses.\\n* It is very consise, simple and easy to understand and therefore should be clear to users and developers.\\n* Digital Iceland does not require protection of patents or existing intelletual property.\\n* Well known government lead initiatives like uk.gov and X-Road use the MIT license.\\n* The MIT license is the best known and most widely used free and open-source license in the world.\\n\\n\\n## Context\\nIn order for a project to be useful, it has to be shared with others. In order for it to be shared effectively, there needs to be a shared expectation of what can be done with it, how to make changes, whether or not changes can be made, and what if any warranty or suitablity for use claims there may be.\\nThe license dictates how other people can use the software, and will set the tone for the community - making it easier or harder for individuals and organziations to use the software and contribute back to it.\\nIn order to foster an active community, it is important to pick the correct license.\\n\\n## Decision\\nThe decision was made to use the Apache License version 2.0\\n\\n\\n\\nProvide a decision given the context below:\\n## Context and Problem Statement\\\\nIt is the offical policy of the Digital Iceland and stated in the Techical Direction that it is to be implemented as free and open source. Open source software by definition is open to anyone to use, modify, distribute and study. These permissions are enforced an open source license. There are a number of well-known and widely used open source licenses available and we need to choose a license that best fits the goals of digital iceland.\\\\nThere are two main types of open source licences: more permissive licences that confer broad freedoms and minimal obligations (e.g., the MIT, BSD and the Apache 2.0 licences); and sharealike licences that require licensing adaptations with the same licence if they distribute them (e.g., the GNU GPL).\\\\nDevelopment for Digital Iceland will be open and free with minimum complications for development for all involved. Reuse and transparency will be promoted.\\\\n## Decision Drivers\\\\n- The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.\\\\n- It is important to build on the experience of similar government led inititives in other countries.\\\\n- Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.\\\\n- It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.\\\\n'}, {'role': 'model', 'content': '- The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.\\\\n- It is important to build on the experience of similar government led inititives in other countries.\\\\n- Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.\\\\n- It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.\\\\nThe MIT license was chosen, for the following reasons:\\\\n- It is the least restrictive of the licenses.\\\\n- It is very consise, simple and easy to understand and therefore should be clear to users and developers.\\\\n- Digital Iceland does not require protection of patents or existing intelletual property.\\\\n- Well known government lead initiatives like uk.gov and X-Road use the MIT license.\\\\n- The MIT license is the best known and most widely used free and open-source license in the world.\\\\n'}]"}
{"File Name":"dependency-track-maven-plugin\/0002-use-maven-to-build-plugin.md","Context":"## Context\\nThis Maven Plugin needed a build tool to compile, test and package it.  Maven and Gradle were the only viable options.\\n","Decision":"I decided to use Maven as it felt a better fit to use Maven to build a Maven plugin, rather than using Gradle.\\n","tokens":30,"id":3364,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context and Problem Statement\\nFor a quick development from the Terraform Provider Functions, it makes sense to generate or use a HarborRestAPI Client.\\n\\n## Decision\\nChosen option: \"Swagger Based\", because this solution supports the fastes development Start without writting any boilerplate code.\\n### Positive Consequences\\n* No Painfull HTTP Client Implementation\\n### Negative Consequences\\n* the API Client Implementation dependents to the Swagger Spec Quality...\\n\\n\\n## Context\\nIoP had a Mercury\/Connect SDK before this, but it was implemented in Java\/C# and it was not maintainable anymore. We had to decide if we want to maintain somehow or\\nrewrite it from scratch.\\n\\n## Decision\\nThe Rust language was chosen to be used based on it\u2019s almost C level speed and rusts memory safety.\\nThe language also possesses really good bindings. Basically you can bind any code written in C into Rust.\\nWhile Rust is still in its early years, it\u2019s growing steadily, and it also has a good, stable, and growing community.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThis Maven Plugin needed a build tool to compile, test and package it.  Maven and Gradle were the only viable options.\\\\n'}, {'role': 'model', 'content': 'I decided to use Maven as it felt a better fit to use Maven to build a Maven plugin, rather than using Gradle.\\\\n'}]"}
{"File Name":"wiki-treasure-hunt\/002-saved-data.md","Context":"### Context\\nThese hunts are designed to be 1-3 hour long or longer. Some players may not\\nhave that time to block out. There should be a way to save the progress of a\\nhunt inside the game.\\n### Decision\\nWe do not want to administer a database and everything that comes along with\\nthat (users, access, groups, etc). Store progress on a hunt in a local file\\nor \"database\" thing. Allow for resets of the hunt when loading.\\n### Consequences\\n- Removes need to admin a database\\n- Removes network complexity\\n- Keeps costs low\\n","Decision":"We do not want to administer a database and everything that comes along with\\nthat (users, access, groups, etc). Store progress on a hunt in a local file\\nor \"database\" thing. Allow for resets of the hunt when loading.\\n### Consequences\\n- Removes need to admin a database\\n- Removes network complexity\\n- Keeps costs low\\n","tokens":124,"id":1804,"text":"[{'role': 'user', 'content': 'You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nSoftware is built incrementally as an accumulation of changes.\\nWe want to continuously deliver changes on the mainline, but that has to be protected from breakages.\\nShort-lived pull requests allow:\\n- visibility of who is changing what.\\n- discussion and review from other people in the team.\\n- automated testing and other kind of checks to run, offloading work from humans to machines.\\n\\n## Decision\\nWe will open short-lived pull requests as the primary means for deploying a change.\\n\\n\\n## Context\\nSoftware is built incrementally as an accumulation of changes.\\nWe want to continuously deliver changes on the mainline, but that has to be protected from breakages.\\nShort-lived pull requests allow:\\n- visibility of who is changing what.\\n- discussion and review from other people in the team.\\n- automated testing and other kind of checks to run, offloading work from humans to machines.\\n\\n## Decision\\nWe will open short-lived pull requests as the primary means for deploying a change.\\n\\n\\n\\nProvide a decision given the context below:\\n### Context\\\\nThese hunts are designed to be 1-3 hour long or longer. Some players may not\\\\nhave that time to block out. There should be a way to save the progress of a\\\\nhunt inside the game.\\\\n### Decision\\\\nWe do not want to administer a database and everything that comes along with\\\\nthat (users, access, groups, etc). Store progress on a hunt in a local file\\\\nor \"database\" thing. Allow for resets of the hunt when loading.\\\\n### Consequences\\\\n- Removes need to admin a database\\\\n- Removes network complexity\\\\n- Keeps costs low\\\\n'}, {'role': 'model', 'content': 'We do not want to administer a database and everything that comes along with\\\\nthat (users, access, groups, etc). Store progress on a hunt in a local file\\\\nor \"database\" thing. Allow for resets of the hunt when loading.\\\\n### Consequences\\\\n- Removes need to admin a database\\\\n- Removes network complexity\\\\n- Keeps costs low\\\\n'}]"}
{"File Name":"portfolio\/0011-use-preact.md","Context":"## Context\\nSince this app doesn't rely heavily on React libraries or things like suspense, switching to [`preact`](https:\/\/preactjs.com\/) offers a significantly smaller bundle size (~70kb => ~40kb).\\nDue to preact's compatability package very little should change in day-to-day development.\\n","Decision":"Use `preact` instead of `react`.\\n","tokens":66,"id":2090,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\nRunning a service like cljdoc requires infrastructure of various kinds. For\\nthe initial prototype I generated a static site stack using [a library I\\ncreated](https:\/\/github.com\/confetti-clj\/confetti) which uses AWS\\nCloudFormation under the hood. This has been useful while prototyping but\\nwith the addition of [a server component](\/doc\/adr\/0009-introduce-server-component.md)\\nand potentially other infrastructure components in the future it becomes\\nharder to stick to Cloudformation.\\n[Terraform](https:\/\/www.terraform.io\/) seems to be a popular tool in this space\\nand provides all the functionality of my library as well as support for using\\nmultiple cloud providers.\\n\\n## Decision\\nRebuild the static site stack using Terraform. Once that has been transitioned,\\nextend the Terraform configuration to include a server that we can use to host\\nthe server component of cljdoc.\\n\\n\\n## Context\\nWe want to be able to support full-featured React apps, without reinventing the\\nwheel when it comes to routing and server-side rendering.\\n[Next.js](https:\/\/nextjs.org\/) is a framework that supports these features out\\nof the box.\\n\\n## Decision\\nWe will use Next.js as the basis for the project.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nSince this app doesn't rely heavily on React libraries or things like suspense, switching to [`preact`](https:\/\/preactjs.com\/) offers a significantly smaller bundle size (~70kb => ~40kb).\\\\nDue to preact's compatability package very little should change in day-to-day development.\\\\n\"}, {'role': 'model', 'content': 'Use `preact` instead of `react`.\\\\n'}]"}
{"File Name":"FlowKit\/0001-pipenv-for-package-and-dependency-management.md","Context":"## Context\\nThe discussion about which package & dependency manager to use has come up\\nrepeatedly over time. The main contenders are `pip`, `pipenv` and `conda`.\\nAll of them have been used (and even been mixed) in different Flowminder\\nprojects and deployments.\\nEach of them has different pros\/cons depending on the context. Here is a\\nbrief summary given our context.\\n(a) pip:\\n- Pros: pure Python, no extra dependencies, [PyPA recommended](https:\/\/packaging.python.org\/guides\/tool-recommendations\/)\\n- Cons: installing packages can be a pain if they involve C extensions or other compiled code (and no wheels are provided); even more so for packages that depend on external libraries\\n(b) [pipenv](https:\/\/docs.pipenv.org\/):\\n- Pros: same as `pip`; comes with support for managing virtual environments; improved dependency management over pure `pip`\\n- Cons: same as `pip`\\n&#40;c) [conda](https:\/\/conda.io\/miniconda.html)\\n- Pros: supports packaging of non-Python dependencies (e.g. third-party libraries), which is advantageous if the user doesn't have full control over the system (e.g. installation in a non-admin environment)\\n- Cons: introduces a third-party dependency; not always clear which conda channels provide which packages\\n","Decision":"We will use pipenv for dependency management.\\n","tokens":290,"id":5049,"text":"[{'role': 'user', 'content': \"You are an expert architect and are tasked with taking decisions given a particular context. Here are some examples:\\n\\n## Context\\n[govuk-frontend](https:\/\/github.com\/alphagov\/govuk-frontend) doesn't do a good\\njob of detecting if it's in a browser environment or not, and therefore, whether\\n`document` and friends are available or not. It also performs a number of\\npolyfills that rely on the presence of those browser globals. This means that it\\nthrows exceptions when attempting to import some JavaScript modules from that\\nlibrary.\\nThe way `govuk-frontend` JavaScript is generally written is under the assumption\\nthat you'll want, and be able, to pass the raw DOM nodes to the module\\nconstructors. React doesn't work that way, and we could dig into the prototypes\\nof the objects created by those constructors, but given that these methods are\\nnot intended to be used directly, it would be compliant with\\n[Semantic Versioning](https:\/\/semver.org\/spec\/v2.0.0.html) to completely change\\ntheir implementation without releasing a new major version. This makes relying\\non their behaviour being fixed unreliable.\\n\\n## Decision\\nWe will re-implement JavaScript features from `govuk-frontend` in more idiomatic\\nReact in this library.\\nWe will link to the version and the source of the original implementation\\nalongside our implementation for reference, linking to the code in the specific\\ncommit we were referring to, not `master`.\\n\\n\\n## Context\\nAs part of the process of rewriting the admin components for link platform, we need to decide on what strategy we want\\nto use for including styling in our application.\\nThere were two main choices:\\n* CSS stylesheets\\n* CSS-in-JS\\n### Why CSS-in-JS?\\nThere are many benefits to CSS-in-JS which are nicely outlined in\\n[this article]('https:\/\/medium.com\/seek-blog\/a-unified-styling-language-d0c208de2660'):\\n* Enforces fundamental scoping without relying on pure convention (automatically generated classes).\\n* Renders critical CSS baked in, as components bring their styles with them automatically.\\n* Removes the necessity to hard code class names.\\n* Increases reusability of styles.\\n* Improves maintainability and dead code detection\\n### Why Fela?\\n[Fela](https:\/\/github.com\/rofrischmann\/fela) is a high performance, framework agnostic\\ntool to handle state-driven styling in Javascript. It also uses Atomic Class design to\\ngenerate class names, and creates stylesheets with those class names, which allows for\\nmodular style reuse. Fela performs about\\n[four times faster](https:\/\/github.com\/A-gambit\/CSS-IN-JS-Benchmarks\/blob\/master\/RESULT.md)\\nthan Glamorous. Fela has also been used by Zendesk engineers in the past and so comes\\nwith the added bonus of having some internal knowledge already in place to hit the\\nground running.\\n\\n## Decision\\nWe will use CSS-in-JS, in the form of Fela and React-Fela.\\n\\n\\n\\nProvide a decision given the context below:\\n## Context\\\\nThe discussion about which package & dependency manager to use has come up\\\\nrepeatedly over time. The main contenders are `pip`, `pipenv` and `conda`.\\\\nAll of them have been used (and even been mixed) in different Flowminder\\\\nprojects and deployments.\\\\nEach of them has different pros\/cons depending on the context. Here is a\\\\nbrief summary given our context.\\\\n(a) pip:\\\\n- Pros: pure Python, no extra dependencies, [PyPA recommended](https:\/\/packaging.python.org\/guides\/tool-recommendations\/)\\\\n- Cons: installing packages can be a pain if they involve C extensions or other compiled code (and no wheels are provided); even more so for packages that depend on external libraries\\\\n(b) [pipenv](https:\/\/docs.pipenv.org\/):\\\\n- Pros: same as `pip`; comes with support for managing virtual environments; improved dependency management over pure `pip`\\\\n- Cons: same as `pip`\\\\n&#40;c) [conda](https:\/\/conda.io\/miniconda.html)\\\\n- Pros: supports packaging of non-Python dependencies (e.g. third-party libraries), which is advantageous if the user doesn't have full control over the system (e.g. installation in a non-admin environment)\\\\n- Cons: introduces a third-party dependency; not always clear which conda channels provide which packages\\\\n\"}, {'role': 'model', 'content': 'We will use pipenv for dependency management.\\\\n'}]"}
