  0%|          | 0/14740 [00:00<?, ?it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)
  batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)
/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 3/14740 [00:09<9:24:07,  2.30s/it]
{'loss': 3.3566, 'grad_norm': 38.07674789428711, 'learning_rate': 4.99966078697422e-05, 'epoch': 0.0}
{'loss': 1.8147, 'grad_norm': 29.59360694885254, 'learning_rate': 4.9993215739484404e-05, 'epoch': 0.0}
{'loss': 2.1151, 'grad_norm': 6.756989479064941, 'learning_rate': 4.99898236092266e-05, 'epoch': 0.0}

  0%|          | 7/14740 [00:11<3:15:56,  1.25it/s]
{'loss': 0.2555, 'grad_norm': 2.4208567142486572, 'learning_rate': 4.998303934871099e-05, 'epoch': 0.01}
{'loss': 2.0088, 'grad_norm': 3.3620615005493164, 'learning_rate': 4.997964721845319e-05, 'epoch': 0.01}
{'loss': 1.6332, 'grad_norm': 1.4673447608947754, 'learning_rate': 4.997625508819539e-05, 'epoch': 0.01}

  0%|          | 11/14740 [00:13<2:16:57,  1.79it/s]
{'loss': 0.7127, 'grad_norm': 0.7765845060348511, 'learning_rate': 4.996947082767979e-05, 'epoch': 0.01}
{'loss': 0.6674, 'grad_norm': 1.0383098125457764, 'learning_rate': 4.996607869742198e-05, 'epoch': 0.01}

  0%|          | 14/14740 [00:15<2:27:57,  1.66it/s]
{'loss': 0.2755, 'grad_norm': 0.42117074131965637, 'learning_rate': 4.9959294436906375e-05, 'epoch': 0.02}
{'loss': 1.0396, 'grad_norm': 1.125282645225525, 'learning_rate': 4.995590230664858e-05, 'epoch': 0.02}
{'loss': 0.5972, 'grad_norm': 0.905970573425293, 'learning_rate': 4.9952510176390776e-05, 'epoch': 0.02}

  0%|          | 18/14740 [00:17<2:08:33,  1.91it/s]
{'loss': 1.1226, 'grad_norm': 0.9534592628479004, 'learning_rate': 4.994572591587518e-05, 'epoch': 0.02}
{'loss': 1.2353, 'grad_norm': 0.9204946756362915, 'learning_rate': 4.994233378561737e-05, 'epoch': 0.02}
{'loss': 0.5584, 'grad_norm': 0.5621119141578674, 'learning_rate': 4.9938941655359565e-05, 'epoch': 0.02}

  0%|          | 22/14740 [00:19<2:04:10,  1.98it/s]
{'loss': 1.5395, 'grad_norm': 1.3556026220321655, 'learning_rate': 4.9932157394843966e-05, 'epoch': 0.03}
{'loss': 0.7972, 'grad_norm': 0.8333075046539307, 'learning_rate': 4.9928765264586164e-05, 'epoch': 0.03}

  0%|          | 26/14740 [00:21<2:20:49,  1.74it/s]
{'loss': 0.2565, 'grad_norm': 0.36740896105766296, 'learning_rate': 4.992198100407056e-05, 'epoch': 0.03}
{'loss': 0.7492, 'grad_norm': 0.8606246709823608, 'learning_rate': 4.9918588873812755e-05, 'epoch': 0.03}
{'loss': 0.528, 'grad_norm': 0.5812143087387085, 'learning_rate': 4.991519674355495e-05, 'epoch': 0.03}
{'loss': 0.5209, 'grad_norm': 0.524712324142456, 'learning_rate': 4.9911804613297156e-05, 'epoch': 0.04}

  0%|          | 30/14740 [00:23<2:06:00,  1.95it/s]
{'loss': 0.5902, 'grad_norm': 0.5829033851623535, 'learning_rate': 4.990502035278155e-05, 'epoch': 0.04}
{'loss': 1.0695, 'grad_norm': 0.783544659614563, 'learning_rate': 4.990162822252375e-05, 'epoch': 0.04}
{'loss': 0.6744, 'grad_norm': 0.8594589233398438, 'learning_rate': 4.9898236092265945e-05, 'epoch': 0.04}
