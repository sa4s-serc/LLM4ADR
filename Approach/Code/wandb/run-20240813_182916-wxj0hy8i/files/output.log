
Max Decision Length:  15536
/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
You are adding a <class 'transformers.integrations.integration_utils.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
WandbCallback
[34m[1mwandb[39m[22m: [33mWARNING[39m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|                                                                                                       | 0/200 [00:00<?, ?it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)
  batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)
/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  2%|█▍                                                                                             | 3/200 [00:06<05:01,  1.53s/it]
{'loss': 2.9723, 'grad_norm': 26.61275291442871, 'learning_rate': 4.975e-05, 'epoch': 0.1}
{'loss': 2.6163, 'grad_norm': 12.278548240661621, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.2}
{'loss': 0.9382, 'grad_norm': 8.400705337524414, 'learning_rate': 4.9250000000000004e-05, 'epoch': 0.3}
{'loss': 0.9244, 'grad_norm': 5.464892864227295, 'learning_rate': 4.9e-05, 'epoch': 0.4}

  4%|███▎                                                                                           | 7/200 [00:08<02:05,  1.53it/s]
{'loss': 1.3264, 'grad_norm': 1.5234456062316895, 'learning_rate': 4.85e-05, 'epoch': 0.6}
{'loss': 2.0563, 'grad_norm': 1.9107943773269653, 'learning_rate': 4.825e-05, 'epoch': 0.7}
{'loss': 0.489, 'grad_norm': 7.260460376739502, 'learning_rate': 4.8e-05, 'epoch': 0.8}

  5%|████▋                                                                                         | 10/200 [00:10<01:43,  1.83it/s]
{'loss': 0.3417, 'grad_norm': 0.8465836048126221, 'learning_rate': 4.75e-05, 'epoch': 1.0}
  5%|████▋                                                                                         | 10/200 [00:10<01:43,  1.83it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  6%|█████▏                                                                                        | 11/200 [00:14<05:49,  1.85s/it]
{'loss': 1.4422, 'grad_norm': 1.9534355401992798, 'learning_rate': 4.7249999999999997e-05, 'epoch': 1.1}

  7%|██████▌                                                                                       | 14/200 [00:15<02:54,  1.07it/s]
{'loss': 1.4356, 'grad_norm': 1.108046293258667, 'learning_rate': 4.6750000000000005e-05, 'epoch': 1.3}
{'loss': 1.3994, 'grad_norm': 1.041646957397461, 'learning_rate': 4.6500000000000005e-05, 'epoch': 1.4}
{'loss': 0.1673, 'grad_norm': 0.5328992009162903, 'learning_rate': 4.6250000000000006e-05, 'epoch': 1.5}

  9%|████████▍                                                                                     | 18/200 [00:18<01:54,  1.59it/s]
{'loss': 1.6999, 'grad_norm': 1.2682796716690063, 'learning_rate': 4.575e-05, 'epoch': 1.7}
{'loss': 0.758, 'grad_norm': 0.7292169332504272, 'learning_rate': 4.55e-05, 'epoch': 1.8}
{'loss': 0.6732, 'grad_norm': 0.6882845163345337, 'learning_rate': 4.525e-05, 'epoch': 1.9}

 10%|█████████▍                                                                                    | 20/200 [00:19<01:36,  1.87it/s]
 10%|█████████▍                                                                                    | 20/200 [00:19<01:36,  1.87it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 11%|██████████▎                                                                                   | 22/200 [00:24<04:07,  1.39s/it]
{'loss': 0.8499, 'grad_norm': 0.6225957870483398, 'learning_rate': 4.4750000000000004e-05, 'epoch': 2.1}
{'loss': 0.811, 'grad_norm': 0.791418194770813, 'learning_rate': 4.4500000000000004e-05, 'epoch': 2.2}
{'loss': 1.2592, 'grad_norm': 0.8180542588233948, 'learning_rate': 4.4250000000000005e-05, 'epoch': 2.3}

 13%|████████████▏                                                                                 | 26/200 [00:26<02:00,  1.45it/s]
{'loss': 1.2512, 'grad_norm': 0.784422755241394, 'learning_rate': 4.375e-05, 'epoch': 2.5}
{'loss': 0.7417, 'grad_norm': 0.8004555702209473, 'learning_rate': 4.35e-05, 'epoch': 2.6}
{'loss': 1.0702, 'grad_norm': 0.7213929891586304, 'learning_rate': 4.325e-05, 'epoch': 2.7}
 15%|██████████████                                                                                | 30/200 [00:27<01:27,  1.95it/s]
 50%|████████████████████████████████████████████████▌                                                | 2/4 [00:00<00:00, 10.05it/s]
{'loss': 1.8724, 'grad_norm': 0.9989622235298157, 'learning_rate': 4.275e-05, 'epoch': 2.9}
{'loss': 1.5831, 'grad_norm': 1.3259207010269165, 'learning_rate': 4.25e-05, 'epoch': 3.0}

{'eval_loss': 0.45666971802711487, 'eval_runtime': 0.6795, 'eval_samples_per_second': 14.717, 'eval_steps_per_second': 5.887, 'epoch': 3.0}
 15%|██████████████                                                                                | 30/200 [00:28<01:27,  1.95it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 17%|███████████████▉                                                                              | 34/200 [00:34<02:37,  1.06it/s]
{'loss': 0.5834, 'grad_norm': 0.5835831165313721, 'learning_rate': 4.2e-05, 'epoch': 3.2}
{'loss': 0.8437, 'grad_norm': 0.7017230987548828, 'learning_rate': 4.175e-05, 'epoch': 3.3}
{'loss': 0.6336, 'grad_norm': 0.6637865304946899, 'learning_rate': 4.15e-05, 'epoch': 3.4}

 19%|█████████████████▊                                                                            | 38/200 [00:36<01:34,  1.71it/s]
{'loss': 0.1536, 'grad_norm': 0.2586842179298401, 'learning_rate': 4.1e-05, 'epoch': 3.6}
{'loss': 0.8056, 'grad_norm': 0.7208858132362366, 'learning_rate': 4.075e-05, 'epoch': 3.7}
{'loss': 0.1776, 'grad_norm': 0.35887661576271057, 'learning_rate': 4.05e-05, 'epoch': 3.8}
{'loss': 1.0821, 'grad_norm': 0.8864157199859619, 'learning_rate': 4.025e-05, 'epoch': 3.9}

 20%|██████████████████▊                                                                           | 40/200 [00:37<01:22,  1.94it/s]
 20%|██████████████████▊                                                                           | 40/200 [00:37<01:22,  1.94it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 21%|███████████████████▋                                                                          | 42/200 [00:42<03:38,  1.38s/it]
{'loss': 0.1776, 'grad_norm': 0.2845774292945862, 'learning_rate': 3.9750000000000004e-05, 'epoch': 4.1}
{'loss': 0.7707, 'grad_norm': 0.5649351477622986, 'learning_rate': 3.9500000000000005e-05, 'epoch': 4.2}
{'loss': 0.6368, 'grad_norm': 0.6842445135116577, 'learning_rate': 3.9250000000000005e-05, 'epoch': 4.3}

 23%|█████████████████████▌                                                                        | 46/200 [00:44<01:45,  1.45it/s]
{'loss': 0.606, 'grad_norm': 0.6645668148994446, 'learning_rate': 3.875e-05, 'epoch': 4.5}
{'loss': 1.9987, 'grad_norm': 1.1427406072616577, 'learning_rate': 3.85e-05, 'epoch': 4.6}
 25%|███████████████████████▌                                                                      | 50/200 [00:46<01:21,  1.85it/s]
  0%|                                                                                                         | 0/4 [00:00<?, ?it/s]
{'loss': 1.1157, 'grad_norm': 1.0041357278823853, 'learning_rate': 3.8e-05, 'epoch': 4.8}
{'loss': 0.8238, 'grad_norm': 0.8241899013519287, 'learning_rate': 3.775e-05, 'epoch': 4.9}
{'loss': 0.127, 'grad_norm': 0.3851756453514099, 'learning_rate': 3.7500000000000003e-05, 'epoch': 5.0}

 25%|███████████████████████▌                                                                      | 50/200 [00:46<01:21,  1.85it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 26%|████████████████████████▉                                                                     | 53/200 [00:52<02:54,  1.19s/it]
{'loss': 0.7206, 'grad_norm': 0.6135075688362122, 'learning_rate': 3.7250000000000004e-05, 'epoch': 5.1}
{'loss': 0.2957, 'grad_norm': 0.4612252116203308, 'learning_rate': 3.7e-05, 'epoch': 5.2}
{'loss': 1.0953, 'grad_norm': 0.7451620101928711, 'learning_rate': 3.675e-05, 'epoch': 5.3}

 28%|██████████████████████████▊                                                                   | 57/200 [00:54<01:31,  1.56it/s]
{'loss': 1.6432, 'grad_norm': 0.8635806441307068, 'learning_rate': 3.625e-05, 'epoch': 5.5}
{'loss': 0.9615, 'grad_norm': 0.7080898880958557, 'learning_rate': 3.6e-05, 'epoch': 5.6}
{'loss': 1.2799, 'grad_norm': 0.8335105180740356, 'learning_rate': 3.575e-05, 'epoch': 5.7}
{'loss': 1.5284, 'grad_norm': 0.9844273924827576, 'learning_rate': 3.55e-05, 'epoch': 5.8}

 30%|████████████████████████████▏                                                                 | 60/200 [00:56<01:13,  1.92it/s]
{'loss': 0.015, 'grad_norm': 0.24279414117336273, 'learning_rate': 3.5e-05, 'epoch': 6.0}
 30%|████████████████████████████▏                                                                 | 60/200 [00:56<01:13,  1.92it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 31%|█████████████████████████████▏                                                                | 62/200 [01:02<03:54,  1.70s/it]
{'loss': 0.2002, 'grad_norm': 0.3044767379760742, 'learning_rate': 3.475e-05, 'epoch': 6.1}
{'loss': 0.8868, 'grad_norm': 0.61576247215271, 'learning_rate': 3.45e-05, 'epoch': 6.2}
{'loss': 1.1162, 'grad_norm': 0.8373762965202332, 'learning_rate': 3.4250000000000006e-05, 'epoch': 6.3}

 34%|███████████████████████████████▍                                                              | 67/200 [01:04<01:30,  1.48it/s]
{'loss': 1.2957, 'grad_norm': 0.7985504865646362, 'learning_rate': 3.375000000000001e-05, 'epoch': 6.5}
{'loss': 1.1532, 'grad_norm': 0.7469347715377808, 'learning_rate': 3.35e-05, 'epoch': 6.6}
{'loss': 1.2135, 'grad_norm': 0.9989757537841797, 'learning_rate': 3.325e-05, 'epoch': 6.7}
 35%|████████████████████████████████▉                                                             | 70/200 [01:05<01:08,  1.88it/s]
 50%|████████████████████████████████████████████████▌                                                | 2/4 [00:00<00:00, 10.49it/s]
{'loss': 1.6464, 'grad_norm': 1.123687505722046, 'learning_rate': 3.275e-05, 'epoch': 6.9}
{'loss': 0.1485, 'grad_norm': 0.4866187572479248, 'learning_rate': 3.2500000000000004e-05, 'epoch': 7.0}

 35%|████████████████████████████████▉                                                             | 70/200 [01:06<01:08,  1.88it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 36%|█████████████████████████████████▎                                                            | 71/200 [01:12<04:54,  2.28s/it]
{'loss': 0.6645, 'grad_norm': 0.6297659277915955, 'learning_rate': 3.2250000000000005e-05, 'epoch': 7.1}
{'loss': 2.6006, 'grad_norm': 1.2699161767959595, 'learning_rate': 3.2000000000000005e-05, 'epoch': 7.2}

 38%|███████████████████████████████████▎                                                          | 75/200 [01:14<01:53,  1.10it/s]
{'loss': 1.0388, 'grad_norm': 0.70443195104599, 'learning_rate': 3.15e-05, 'epoch': 7.4}
{'loss': 0.452, 'grad_norm': 0.419030100107193, 'learning_rate': 3.125e-05, 'epoch': 7.5}
{'loss': 0.7297, 'grad_norm': 0.5993978977203369, 'learning_rate': 3.1e-05, 'epoch': 7.6}

 40%|█████████████████████████████████████▏                                                        | 79/200 [01:16<01:19,  1.52it/s]
{'loss': 0.5776, 'grad_norm': 0.5138329863548279, 'learning_rate': 3.05e-05, 'epoch': 7.8}
{'loss': 0.642, 'grad_norm': 0.5850180983543396, 'learning_rate': 3.025e-05, 'epoch': 7.9}

 40%|█████████████████████████████████████▌                                                        | 80/200 [01:17<01:11,  1.68it/s]
 40%|█████████████████████████████████████▌                                                        | 80/200 [01:17<01:11,  1.68it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 41%|██████████████████████████████████████▌                                                       | 82/200 [01:24<03:50,  1.95s/it]
{'loss': 0.2186, 'grad_norm': 0.37627455592155457, 'learning_rate': 2.975e-05, 'epoch': 8.1}
{'loss': 0.0874, 'grad_norm': 0.3546534478664398, 'learning_rate': 2.95e-05, 'epoch': 8.2}

 43%|████████████████████████████████████████▍                                                     | 86/200 [01:26<01:34,  1.21it/s]
{'loss': 1.0981, 'grad_norm': 0.8620954751968384, 'learning_rate': 2.9e-05, 'epoch': 8.4}
{'loss': 0.1416, 'grad_norm': 0.23397956788539886, 'learning_rate': 2.8749999999999997e-05, 'epoch': 8.5}
{'loss': 1.4938, 'grad_norm': 0.9822704792022705, 'learning_rate': 2.8499999999999998e-05, 'epoch': 8.6}
 45%|██████████████████████████████████████████▎                                                   | 90/200 [01:28<00:59,  1.83it/s]
  0%|                                                                                                         | 0/4 [00:00<?, ?it/s]
{'loss': 1.6286, 'grad_norm': 1.0331752300262451, 'learning_rate': 2.8000000000000003e-05, 'epoch': 8.8}
{'loss': 0.5589, 'grad_norm': 0.7295504212379456, 'learning_rate': 2.7750000000000004e-05, 'epoch': 8.9}
{'loss': 2.4953, 'grad_norm': 1.491382122039795, 'learning_rate': 2.7500000000000004e-05, 'epoch': 9.0}

 45%|██████████████████████████████████████████▎                                                   | 90/200 [01:29<00:59,  1.83it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 46%|███████████████████████████████████████████▏                                                  | 92/200 [01:34<02:54,  1.61s/it]
{'loss': 0.0686, 'grad_norm': 0.285450279712677, 'learning_rate': 2.725e-05, 'epoch': 9.1}
{'loss': 0.1536, 'grad_norm': 0.38377127051353455, 'learning_rate': 2.7000000000000002e-05, 'epoch': 9.2}

 48%|█████████████████████████████████████████████                                                 | 96/200 [01:36<01:17,  1.34it/s]
{'loss': 2.1138, 'grad_norm': 1.0550435781478882, 'learning_rate': 2.6500000000000004e-05, 'epoch': 9.4}
{'loss': 1.0622, 'grad_norm': 1.14047110080719, 'learning_rate': 2.625e-05, 'epoch': 9.5}
{'loss': 1.5392, 'grad_norm': 0.9503298997879028, 'learning_rate': 2.6000000000000002e-05, 'epoch': 9.6}
 50%|██████████████████████████████████████████████▌                                              | 100/200 [01:38<00:52,  1.90it/s]
  0%|                                                                                                         | 0/4 [00:00<?, ?it/s]
{'loss': 0.2016, 'grad_norm': 0.3131377100944519, 'learning_rate': 2.5500000000000003e-05, 'epoch': 9.8}
{'loss': 0.9884, 'grad_norm': 0.8935886025428772, 'learning_rate': 2.525e-05, 'epoch': 9.9}
{'loss': 0.3021, 'grad_norm': 0.4152575433254242, 'learning_rate': 2.5e-05, 'epoch': 10.0}

 50%|██████████████████████████████████████████████▌                                              | 100/200 [01:38<00:52,  1.90it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 51%|███████████████████████████████████████████████▍                                             | 102/200 [01:44<02:41,  1.64s/it]
{'loss': 1.4648, 'grad_norm': 0.8783196210861206, 'learning_rate': 2.4750000000000002e-05, 'epoch': 10.1}
{'loss': 0.5996, 'grad_norm': 0.6045038104057312, 'learning_rate': 2.45e-05, 'epoch': 10.2}

 53%|█████████████████████████████████████████████████▎                                           | 106/200 [01:46<01:10,  1.33it/s]
{'loss': 0.5324, 'grad_norm': 0.5931342840194702, 'learning_rate': 2.4e-05, 'epoch': 10.4}
{'loss': 0.0827, 'grad_norm': 0.2627500295639038, 'learning_rate': 2.375e-05, 'epoch': 10.5}

 55%|██████████████████████████████████████████████████▋                                          | 109/200 [01:48<00:58,  1.56it/s]
{'loss': 0.6804, 'grad_norm': 0.7937456369400024, 'learning_rate': 2.3250000000000003e-05, 'epoch': 10.7}
{'loss': 0.95, 'grad_norm': 0.738027036190033, 'learning_rate': 2.3000000000000003e-05, 'epoch': 10.8}
{'loss': 0.6348, 'grad_norm': 0.5778951048851013, 'learning_rate': 2.275e-05, 'epoch': 10.9}

 55%|███████████████████████████████████████████████████▏                                         | 110/200 [01:49<00:52,  1.72it/s]
 55%|███████████████████████████████████████████████████▏                                         | 110/200 [01:49<00:52,  1.72it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 56%|███████████████████████████████████████████████████▌                                         | 111/200 [01:56<03:57,  2.66s/it]
{'loss': 0.7122, 'grad_norm': 0.5958314538002014, 'learning_rate': 2.2250000000000002e-05, 'epoch': 11.1}

 57%|█████████████████████████████████████████████████████▍                                       | 115/200 [01:58<01:24,  1.00it/s]
{'loss': 1.0822, 'grad_norm': 0.7589821815490723, 'learning_rate': 2.175e-05, 'epoch': 11.3}
{'loss': 0.0771, 'grad_norm': 0.2362462878227234, 'learning_rate': 2.15e-05, 'epoch': 11.4}
{'loss': 1.1334, 'grad_norm': 0.7416064739227295, 'learning_rate': 2.125e-05, 'epoch': 11.5}
{'loss': 0.6749, 'grad_norm': 0.6147127747535706, 'learning_rate': 2.1e-05, 'epoch': 11.6}

 60%|███████████████████████████████████████████████████████▊                                     | 120/200 [02:00<00:43,  1.82it/s]
{'loss': 1.8116, 'grad_norm': 1.084702730178833, 'learning_rate': 2.05e-05, 'epoch': 11.8}
{'loss': 0.473, 'grad_norm': 0.477209210395813, 'learning_rate': 2.025e-05, 'epoch': 11.9}

 60%|███████████████████████████████████████████████████████▊                                     | 120/200 [02:01<00:43,  1.82it/s]
{'eval_loss': 0.44888603687286377, 'eval_runtime': 0.686, 'eval_samples_per_second': 14.577, 'eval_steps_per_second': 5.831, 'epoch': 12.0}
 60%|███████████████████████████████████████████████████████▊                                     | 120/200 [02:01<00:43,  1.82it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 62%|█████████████████████████████████████████████████████████▏                                   | 123/200 [02:08<01:53,  1.47s/it]
{'loss': 0.7973, 'grad_norm': 0.7268287539482117, 'learning_rate': 1.9500000000000003e-05, 'epoch': 12.2}
{'loss': 1.3033, 'grad_norm': 0.9369505643844604, 'learning_rate': 1.925e-05, 'epoch': 12.3}

 64%|███████████████████████████████████████████████████████████▌                                 | 128/200 [02:10<00:46,  1.56it/s]
{'loss': 1.1066, 'grad_norm': 0.799540102481842, 'learning_rate': 1.8750000000000002e-05, 'epoch': 12.5}
{'loss': 1.0623, 'grad_norm': 0.8613993525505066, 'learning_rate': 1.85e-05, 'epoch': 12.6}
{'loss': 0.0791, 'grad_norm': 0.31200504302978516, 'learning_rate': 1.825e-05, 'epoch': 12.7}
{'loss': 0.5281, 'grad_norm': 0.5450392961502075, 'learning_rate': 1.8e-05, 'epoch': 12.8}

 65%|████████████████████████████████████████████████████████████▍                                | 130/200 [02:12<00:38,  1.84it/s]
{'loss': 0.873, 'grad_norm': 0.9074739217758179, 'learning_rate': 1.75e-05, 'epoch': 13.0}
{'eval_loss': 0.4495619237422943, 'eval_runtime': 0.6835, 'eval_samples_per_second': 14.63, 'eval_steps_per_second': 5.852, 'epoch': 13.0}
 65%|████████████████████████████████████████████████████████████▍                                | 130/200 [02:12<00:38,  1.84it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 67%|██████████████████████████████████████████████████████████████▎                              | 134/200 [02:20<01:19,  1.21s/it]
{'loss': 2.3396, 'grad_norm': 1.244720458984375, 'learning_rate': 1.7000000000000003e-05, 'epoch': 13.2}
{'loss': 0.1904, 'grad_norm': 0.2910374104976654, 'learning_rate': 1.675e-05, 'epoch': 13.3}
{'loss': 0.1326, 'grad_norm': 0.2722261846065521, 'learning_rate': 1.65e-05, 'epoch': 13.4}

 69%|████████████████████████████████████████████████████████████████▏                            | 138/200 [02:22<00:40,  1.54it/s]
{'loss': 0.194, 'grad_norm': 0.34020981192588806, 'learning_rate': 1.6000000000000003e-05, 'epoch': 13.6}
{'loss': 1.5278, 'grad_norm': 0.9827840924263, 'learning_rate': 1.575e-05, 'epoch': 13.7}

 70%|█████████████████████████████████████████████████████████████████                            | 140/200 [02:24<00:38,  1.54it/s]
{'loss': 0.7253, 'grad_norm': 0.8065499067306519, 'learning_rate': 1.525e-05, 'epoch': 13.9}
{'loss': 0.8276, 'grad_norm': 0.9843521118164062, 'learning_rate': 1.5e-05, 'epoch': 14.0}
 70%|█████████████████████████████████████████████████████████████████                            | 140/200 [02:24<00:38,  1.54it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 71%|██████████████████████████████████████████████████████████████████                           | 142/200 [02:30<01:41,  1.75s/it]
{'loss': 0.6994, 'grad_norm': 0.5729923844337463, 'learning_rate': 1.475e-05, 'epoch': 14.1}
{'loss': 0.3305, 'grad_norm': 0.39797794818878174, 'learning_rate': 1.45e-05, 'epoch': 14.2}

 74%|████████████████████████████████████████████████████████████████████▎                        | 147/200 [02:32<00:36,  1.45it/s]
{'loss': 0.4971, 'grad_norm': 0.5663689374923706, 'learning_rate': 1.4000000000000001e-05, 'epoch': 14.4}
{'loss': 0.9981, 'grad_norm': 0.8358920216560364, 'learning_rate': 1.3750000000000002e-05, 'epoch': 14.5}
{'loss': 0.0652, 'grad_norm': 0.2990747392177582, 'learning_rate': 1.3500000000000001e-05, 'epoch': 14.6}
 75%|█████████████████████████████████████████████████████████████████████▊                       | 150/200 [02:34<00:26,  1.87it/s]
 75%|████████████████████████████████████████████████████████████████████████▊                        | 3/4 [00:00<00:00,  6.82it/s]
{'loss': 0.8899, 'grad_norm': 0.8712998032569885, 'learning_rate': 1.3000000000000001e-05, 'epoch': 14.8}
{'loss': 0.4656, 'grad_norm': 0.46628788113594055, 'learning_rate': 1.2750000000000002e-05, 'epoch': 14.9}
{'loss': 0.8769, 'grad_norm': 1.1474196910858154, 'learning_rate': 1.25e-05, 'epoch': 15.0}

 75%|█████████████████████████████████████████████████████████████████████▊                       | 150/200 [02:34<00:26,  1.87it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 76%|██████████████████████████████████████████████████████████████████████▏                      | 151/200 [02:40<01:52,  2.30s/it]
{'loss': 0.5076, 'grad_norm': 0.676231861114502, 'learning_rate': 1.225e-05, 'epoch': 15.1}

 78%|████████████████████████████████████████████████████████████████████████                     | 155/200 [02:42<00:44,  1.02it/s]
{'loss': 1.1983, 'grad_norm': 0.9358130693435669, 'learning_rate': 1.175e-05, 'epoch': 15.3}
{'loss': 0.0736, 'grad_norm': 0.18729917705059052, 'learning_rate': 1.1500000000000002e-05, 'epoch': 15.4}
{'loss': 1.0692, 'grad_norm': 0.8997834324836731, 'learning_rate': 1.125e-05, 'epoch': 15.5}

 80%|█████████████████████████████████████████████████████████████████████████▉                   | 159/200 [02:44<00:24,  1.68it/s]
{'loss': 0.624, 'grad_norm': 0.5883557796478271, 'learning_rate': 1.075e-05, 'epoch': 15.7}
{'loss': 0.0093, 'grad_norm': 0.35082191228866577, 'learning_rate': 1.05e-05, 'epoch': 15.8}
{'loss': 1.0023, 'grad_norm': 0.7294955849647522, 'learning_rate': 1.025e-05, 'epoch': 15.9}

 80%|██████████████████████████████████████████████████████████████████████████▍                  | 160/200 [02:45<00:21,  1.83it/s]
 80%|██████████████████████████████████████████████████████████████████████████▍                  | 160/200 [02:45<00:21,  1.83it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 81%|███████████████████████████████████████████████████████████████████████████▎                 | 162/200 [02:50<00:56,  1.49s/it]
{'loss': 1.014, 'grad_norm': 1.062874674797058, 'learning_rate': 9.750000000000002e-06, 'epoch': 16.1}
{'loss': 1.3621, 'grad_norm': 1.020889163017273, 'learning_rate': 9.5e-06, 'epoch': 16.2}

 83%|█████████████████████████████████████████████████████████████████████████████▏               | 166/200 [02:52<00:24,  1.40it/s]
{'loss': 0.9408, 'grad_norm': 0.7751940488815308, 'learning_rate': 9e-06, 'epoch': 16.4}
{'loss': 0.6035, 'grad_norm': 0.7424492835998535, 'learning_rate': 8.75e-06, 'epoch': 16.5}
{'loss': 1.0314, 'grad_norm': 0.7670726180076599, 'learning_rate': 8.500000000000002e-06, 'epoch': 16.6}
 85%|███████████████████████████████████████████████████████████████████████████████              | 170/200 [02:54<00:15,  1.92it/s]
  0%|                                                                                                         | 0/4 [00:00<?, ?it/s]
{'loss': 0.4095, 'grad_norm': 0.4306725263595581, 'learning_rate': 8.000000000000001e-06, 'epoch': 16.8}
{'loss': 1.0967, 'grad_norm': 0.8093069195747375, 'learning_rate': 7.75e-06, 'epoch': 16.9}

{'loss': 0.2054, 'grad_norm': 0.4753628671169281, 'learning_rate': 7.5e-06, 'epoch': 17.0}
{'eval_loss': 0.45055246353149414, 'eval_runtime': 0.9664, 'eval_samples_per_second': 10.348, 'eval_steps_per_second': 4.139, 'epoch': 17.0}
 85%|███████████████████████████████████████████████████████████████████████████████              | 170/200 [02:55<00:15,  1.92it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 87%|████████████████████████████████████████████████████████████████████████████████▉            | 174/200 [03:02<00:29,  1.14s/it]
{'loss': 1.367, 'grad_norm': 1.0055100917816162, 'learning_rate': 7.000000000000001e-06, 'epoch': 17.2}
{'loss': 0.2984, 'grad_norm': 0.36727845668792725, 'learning_rate': 6.750000000000001e-06, 'epoch': 17.3}
{'loss': 0.1946, 'grad_norm': 0.31433364748954773, 'learning_rate': 6.5000000000000004e-06, 'epoch': 17.4}

 89%|██████████████████████████████████████████████████████████████████████████████████▊          | 178/200 [03:04<00:13,  1.59it/s]
{'loss': 0.0025, 'grad_norm': 0.11450014263391495, 'learning_rate': 6e-06, 'epoch': 17.6}
{'loss': 0.6972, 'grad_norm': 0.6680743098258972, 'learning_rate': 5.750000000000001e-06, 'epoch': 17.7}
{'loss': 1.0433, 'grad_norm': 0.6182865500450134, 'learning_rate': 5.500000000000001e-06, 'epoch': 17.8}

 90%|███████████████████████████████████████████████████████████████████████████████████▋         | 180/200 [03:06<00:10,  1.86it/s]
{'loss': 0.1144, 'grad_norm': 0.3174634277820587, 'learning_rate': 5e-06, 'epoch': 18.0}
 90%|███████████████████████████████████████████████████████████████████████████████████▋         | 180/200 [03:06<00:10,  1.86it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 91%|████████████████████████████████████████████████████████████████████████████████████▋        | 182/200 [03:12<00:33,  1.85s/it]
{'loss': 1.7284, 'grad_norm': 1.2698338031768799, 'learning_rate': 4.75e-06, 'epoch': 18.1}
{'loss': 0.1151, 'grad_norm': 0.3330490291118622, 'learning_rate': 4.5e-06, 'epoch': 18.2}

 93%|██████████████████████████████████████████████████████████████████████████████████████▍      | 186/200 [03:14<00:11,  1.24it/s]
{'loss': 0.1991, 'grad_norm': 0.3007233738899231, 'learning_rate': 4.000000000000001e-06, 'epoch': 18.4}
{'loss': 0.5696, 'grad_norm': 0.5222905278205872, 'learning_rate': 3.75e-06, 'epoch': 18.5}

 95%|████████████████████████████████████████████████████████████████████████████████████████▎    | 190/200 [03:16<00:05,  1.74it/s]
{'loss': 0.1168, 'grad_norm': 0.415718674659729, 'learning_rate': 3.2500000000000002e-06, 'epoch': 18.7}
{'loss': 1.7764, 'grad_norm': 1.0790077447891235, 'learning_rate': 3e-06, 'epoch': 18.8}
{'loss': 0.5052, 'grad_norm': 0.6072474718093872, 'learning_rate': 2.7500000000000004e-06, 'epoch': 18.9}

 95%|████████████████████████████████████████████████████████████████████████████████████████▎    | 190/200 [03:17<00:05,  1.74it/s]
 95%|████████████████████████████████████████████████████████████████████████████████████████▎    | 190/200 [03:17<00:05,  1.74it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 96%|████████████████████████████████████████████████████████████████████████████████████████▊    | 191/200 [03:22<00:19,  2.14s/it]
{'loss': 1.1854, 'grad_norm': 0.8860952258110046, 'learning_rate': 2.25e-06, 'epoch': 19.1}

 98%|██████████████████████████████████████████████████████████████████████████████████████████▋  | 195/200 [03:24<00:04,  1.15it/s]
{'loss': 0.8818, 'grad_norm': 0.7267269492149353, 'learning_rate': 1.7500000000000002e-06, 'epoch': 19.3}
{'loss': 0.1515, 'grad_norm': 0.27353358268737793, 'learning_rate': 1.5e-06, 'epoch': 19.4}
{'loss': 1.428, 'grad_norm': 1.1034830808639526, 'learning_rate': 1.25e-06, 'epoch': 19.5}

100%|█████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [03:26<00:00,  1.85it/s]
{'loss': 0.143, 'grad_norm': 0.3601832389831543, 'learning_rate': 7.5e-07, 'epoch': 19.7}
{'loss': 1.4446, 'grad_norm': 0.9887689352035522, 'learning_rate': 5.000000000000001e-07, 'epoch': 19.8}
{'loss': 0.9475, 'grad_norm': 0.9287693500518799, 'learning_rate': 2.5000000000000004e-07, 'epoch': 19.9}
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [03:26<00:00,  1.85it/s]/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [03:31<00:00,  1.85it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [03:31<00:00,  1.85it/s]There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [04:15<00:00,  1.28s/it]
/home2/adyansh/LLM4ADR/research/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.19it/s]
{'train_runtime': 255.9099, 'train_samples_per_second': 2.266, 'train_steps_per_second': 0.782, 'train_loss': 0.8433267057384364, 'epoch': 20.0}