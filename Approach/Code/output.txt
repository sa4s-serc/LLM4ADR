2024-10-02 15:21:53.687824: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-10-02 15:21:53.717678: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-10-02 15:21:53.717715: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-10-02 15:21:53.736914: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-10-02 15:21:56.253073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.29s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.39s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]
Map:   0%|          | 0/982 [00:00<?, ? examples/s]Map:   2%|▏         | 16/982 [00:00<00:06, 144.00 examples/s]Map:   4%|▍         | 37/982 [00:00<00:05, 179.76 examples/s]Map:   6%|▌         | 59/982 [00:00<00:04, 195.55 examples/s]Map:   9%|▊         | 84/982 [00:00<00:04, 212.11 examples/s]Map:  11%|█         | 110/982 [00:00<00:03, 226.88 examples/s]Map:  14%|█▍        | 137/982 [00:00<00:03, 235.45 examples/s]Map:  16%|█▋        | 161/982 [00:00<00:03, 234.26 examples/s]Map:  19%|█▉        | 188/982 [00:00<00:03, 242.39 examples/s]Map:  23%|██▎       | 224/982 [00:00<00:03, 236.49 examples/s]Map:  25%|██▌       | 248/982 [00:01<00:03, 234.93 examples/s]Map:  28%|██▊       | 274/982 [00:01<00:03, 235.22 examples/s]Map:  31%|███       | 301/982 [00:01<00:02, 238.34 examples/s]Map:  34%|███▎      | 329/982 [00:01<00:02, 246.55 examples/s]Map:  37%|███▋      | 363/982 [00:01<00:02, 231.85 examples/s]Map:  40%|███▉      | 392/982 [00:01<00:02, 244.67 examples/s]Map:  44%|████▍     | 431/982 [00:01<00:02, 244.39 examples/s]Map:  47%|████▋     | 458/982 [00:01<00:02, 246.85 examples/s]Map:  49%|████▉     | 483/982 [00:02<00:02, 245.97 examples/s]Map:  53%|█████▎    | 522/982 [00:02<00:01, 244.08 examples/s]Map:  56%|█████▌    | 550/982 [00:02<00:01, 248.84 examples/s]Map:  59%|█████▉    | 578/982 [00:02<00:01, 254.73 examples/s]Map:  62%|██████▏   | 608/982 [00:02<00:01, 265.65 examples/s]Map:  65%|██████▍   | 636/982 [00:02<00:01, 263.54 examples/s]Map:  69%|██████▊   | 673/982 [00:02<00:01, 251.77 examples/s]Map:  72%|███████▏  | 703/982 [00:02<00:01, 231.68 examples/s]Map:  75%|███████▍  | 733/982 [00:03<00:01, 243.10 examples/s]Map:  77%|███████▋  | 760/982 [00:03<00:00, 247.66 examples/s]Map:  81%|████████  | 795/982 [00:03<00:00, 241.10 examples/s]Map:  84%|████████▎ | 822/982 [00:03<00:00, 246.26 examples/s]Map:  87%|████████▋ | 854/982 [00:03<00:00, 263.16 examples/s]Map:  90%|█████████ | 885/982 [00:03<00:00, 272.79 examples/s]Map:  94%|█████████▍| 922/982 [00:03<00:00, 254.36 examples/s]Map:  98%|█████████▊| 963/982 [00:03<00:00, 256.01 examples/s]Map: 100%|██████████| 982/982 [00:05<00:00, 188.69 examples/s]
Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
You are adding a <class 'transformers.integrations.integration_utils.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
TensorBoardCallback
WandbCallback
CodeCarbonCallback
[codecarbon INFO @ 15:22:25] [setup] RAM Tracking...
[codecarbon WARNING @ 15:22:25] Could not find mem= after running `scontrol show job $SLURM_JOB_ID` to count SLURM-available RAM. Using the machine's total RAM.
[codecarbon INFO @ 15:22:25] [setup] GPU Tracking...
[codecarbon INFO @ 15:22:25] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 15:22:25] [setup] CPU Tracking...
[codecarbon WARNING @ 15:22:25] No CPU tracking mode found. Falling back on CPU constant mode.
[codecarbon WARNING @ 15:22:26] We saw that you have a Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz but we don't know it. Please contact us.
[codecarbon INFO @ 15:22:26] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz
[codecarbon INFO @ 15:22:26] >>> Tracker's metadata:
[codecarbon INFO @ 15:22:26]   Platform system: Linux-4.15.0-213-generic-x86_64-with-glibc2.27
[codecarbon INFO @ 15:22:26]   Python version: 3.11.3
[codecarbon INFO @ 15:22:26]   CodeCarbon version: 2.3.5
[codecarbon INFO @ 15:22:26]   Available RAM : 125.783 GB
[codecarbon INFO @ 15:22:26]   CPU count: 36
[codecarbon INFO @ 15:22:26]   CPU model: Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz
[codecarbon INFO @ 15:22:26]   GPU count: 4
[codecarbon INFO @ 15:22:26]   GPU model: 4 x NVIDIA GeForce RTX 2080 Ti
[2024-10-02 15:22:31,679] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
Warning: The default cache directory for DeepSpeed Triton autotune, /home2/adyansh/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
  0%|          | 0/982 [00:00<?, ?it/s]  0%|          | 2/982 [00:07<1:00:05,  3.68s/it]  0%|          | 3/982 [00:14<1:25:12,  5.22s/it]  0%|          | 4/982 [00:22<1:38:22,  6.04s/it]  1%|          | 5/982 [00:29<1:46:02,  6.51s/it]  1%|          | 6/982 [00:36<1:50:35,  6.80s/it]  1%|          | 7/982 [00:44<1:53:43,  7.00s/it]  1%|          | 8/982 [00:51<1:55:51,  7.14s/it]  1%|          | 9/982 [00:59<1:57:20,  7.24s/it]  1%|          | 10/982 [01:06<1:58:10,  7.29s/it]slurmstepd: error: *** JOB 1219822 ON gnode092 CANCELLED AT 2024-10-02T15:23:58 ***
