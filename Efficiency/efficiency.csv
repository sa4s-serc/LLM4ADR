context,id
"## Context and Problem Statement\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\n## Decision Drivers <!-- optional -->\n* Performance - decision should support high throughput scenarios\n* Maintainability - amount of code needed to write should be minimized\n",2526
"## Context\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\nOnly the owner of a table can alter/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\nWe attempted to work around the PostgreSQL permissions system in the following ways:\n* Using [`ALTER DEFAULT PRIVILEGES`](https://www.postgresql.org/docs/9.5/static/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\n* Creating a ""parent"" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https://www.postgresql.org/docs/9.5/static/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\n",224
## Context and Problem Statement\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\n,699
## Context\nWe need a system to manage the lifecycle of a grant application.\n,5009
"## Context\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\nGoals:\n- Layer Merkle proofs without decoding/reencoding\n- Provide general way to chain proofs\n- Make the proof format extensible, allowing thirdparty proof types\n",4130
"## Context\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage/SEPA users).\n## Decision Drivers\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\n",5157
"## Context\nAs it's hard to understand code, it is crucial that anybody can easily\nunderstand the code you're working on. This applies to all levels of code, not\nonly the code itself, but groups of code, complete applications and groups of\napplications.\n",1416
## Context\nWe need to record the architectural decisions made on this project.\n,4002
"## Context\nThe rig defined at [Bookit Infrastructure](https://github.com/buildit/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\nThe only way to capture that is via branches which can be hard to discover.\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\n",1742
"## Context\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\nPHP [appears to be on a downwards trend as a language](https://pypl.github.io/PYPL.html?country=GB),\nespecially in contrast with Python. It's likely it will become increasingly difficult\nto find good PHP developers in future.\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\nlanguages of similar pedigree, such as Python.\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\nA mix of contractors and developers working on the code base over several years has\nresulted in a mix of styles and approaches. While we have already cleared out a lot\nof unused and/or broken code, there is likely to be more we haven't found yet.\nWe are also lagging behind the latest Design System guidelines, as our application was one\nof the first to go live, before the current iteration of the Design System existed.\nThis means that any changes to design have to be done piecemeal and manually: we can't\nsimply import the newest version of the design system and have everything magically update.\nThis combination of factors means that the code base can be difficult to work with:\nresistant to change and easy to break.\n",1652
"## Context\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\n- MovementX/Y is a clean browser only method for determining distance without having to track previous coordinates.\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\n",873
"## Context\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\n",78
## Problem\nMost of the credentials/information of the environment (like a service address) are only available as soon the application is deployed.\n,1745
"## Context and Problem Statement\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\nidentity and access management solution must be provisioned which\nsupports user creation and registration, user management, and\nauthentication and authorisation.\n## Decision Drivers\n* Ease of initial deployment and configuration of the solution.\n* Ongoing hosting and maintenance costs of the solution.\n* Availability of core features to satisfy the project requirements with\nno / minimal costs.\n",4857
"## Context\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted/PaaS.\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\n",1740
## Context\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\n,1558
"## Problem\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\n(as they are designed to do so). Limitations of other storage plugins are\ne.g., that not every structure of configuration is allowed.\nSome of these limitations were documented `infos/status`, others were not.\n",1318
"## Context\n[Command-query separation](https://martinfowler.com/bliki/CommandQuerySeparation.html) states that every method should\neither be a command that performs an action, or a query that returns data to the caller, but not both.\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\nuse queries with much more confidence, and only be careful with commands orchestration.\nCommands and queries terminology is already used in the `menu-generation` application.\n",647
"## Context\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\n",2198
## Context\nThe software could be developed in one big (Gradle) project.\nThis would make integration easier.\nAt the same time this would make re-use of the code outside of this project harder.\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\n,2626
"## Context\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\n",2829
## Context\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\n,5024
"## Context\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\n",3291
"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",4804
"## Context\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\n",2837
## Context and Problem Statement\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\n,5243
"## Context\nPIMS requires a database to store all property information.\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\nAdditionally it must be supported by Entity Framework Core 3.1.\n",3966
"## Context and Problem Statement\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for ""thread-safe\ninterfaces"" - possibly leading to the impression that there is such a thing as\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\nthe only question is who manages this thread-safety. Interfaces which are not\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\nhidden in the generated code and therefore not obvious to the casual reader.\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\nmanaging their own locking internally. This ADR proposes that uniffi forces\ncomponent authors to explicitly manage that locking in all cases - or to put\nthis in Rust terms, that all structs supported by uniffi must already be\n`Send+Sync`\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\n""Threadsafe"" because it more accurately describes the actual intent and avoids\nany misunderstandings that might be caused by using the somewhat broad and\ngeneric ""Threadsafe"".\n## Decision Drivers\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\nthem `Send+Sync`. We consider this a ""foot-gun"" as it may lead to accidentally\nhaving method calls unexpectedly block for long periods, such as\n[this Fenix bug](https://github.com/mozilla-mobile/fenix/issues/17086)\n(with more details available in [this JIRA ticket](https://jira.mozilla.com/browse/SDK-157)).\n* Supporting such structs will hinder uniffi growing in directions that we've\nfound are desired in practice, such as allowing structs to use [alternative\nmethod receivers](https://github.com/mozilla/uniffi-rs/issues/417) or to\n[pass interface references over the FFI](https://github.com/mozilla/uniffi-rs/issues/419).\n",4952
## Context\nWe need to record the architectural decisions made on this project.\n,165
## Context and Problem Statement\nWe found that without a standardised format our javascript files ended up with different\nformats in different files or even multiple formats in the same file.  We also found that\nour IDEs had different configurations which meant that using an autoformat tool would give\ndifferent results when each of us do it.\n## Decision Drivers\n* We wanted to spend less time doing manual formatting\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\n* We wanted to see easily which lines had actually changed when reviewing PRs\n* We wanted to avoid discussions about individual's preferences for particular\n,3521
## Context\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\nmakes usage and documentation easier.\n,994
"## Context\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\n",3153
"## Context and Problem Statement\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\nWhich query syntax should be used for the abstract queries?\nWhich features should the syntax support?\n",4738
"## Context\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\nGovUK services have a ""trial mode"", and a ""live mode"".\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\n",1784
## Context\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\n,1171
"## Context\nWe want to enforce consistency in our code, and catch as many errors\nautomatically as we are able to. Linting the code is good practice to achieve\nthese aims. [Stylelint](https://stylelint.io/) is one of the more popular CSS\nlinters with support for SASS, and is easily configurable for our purposes.\n",458
"## Context\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\nWhen determining who has access to a given resource, both these sources may need to be consulted.\n",4554
"## Context\nThe Data Migrate gem has caused us a number of issues in the past, it runs\nsilently as part of a deploy, and this can result in surprising errors\nduring a deploy. We've also had issues with the gem itself - most recently\na bug in a new version causing strange errors in deployment.\n",2396
"## Context\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\n",1647
"## Context\nWe are doing a rewrite.\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\n",1495
## Problem\nCurrently multiple warnings are saved in an elektra non-conforming array\nnotation which is limited to 100 entries. The notation of `#00` is against\nthe design [decision made](array.md).\n,1303
## Context\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\nThere is currently no formal recommendation for a tool of choice in the technical guidance https://github.com/DFE-Digital/technical-guidance.\nWe want a tool that provides an open and modern security standard.\n,1252
"## Context and Problem Statement\nHow to guarantee reproducibility of Jupyter Notebooks?\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\nNotebooks should be treated as component/service that use their own dependencies, therefore when storing notebooks,\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\n## Decision Drivers <!-- optional -->\n* user prospective\n* reproducibility\n* traecability\n",1228
## Context\nWe need to record the architectural decisions that we make as we develop Ockam.\n,4539
"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\n",3546
"## Context\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\n* Status: proposed\n* Deciders: Licence Change\n* Date: 2020-08-27\nTechnical Story: [description | <https://apps.nrs.gov.bc.ca/int/jira/browse/ARCH-62]>\n## Context and Problem Statement\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\n## Decision Drivers\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\n* Oracle JDK should be, for the most part, interchangeable with Oracle’s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\n* Java 10 is the suggested release by Oracle\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\n* For security reasons, IITD Architecture encourages upgrade/migration of all java applications to at least JDK 11.\n* Scope - IITD all server and client side applications owned by IITD that run Java.\n",4802
"## **Context**\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\nWe will look at two options for validation:\n- **Data Annotations**\nThis involves ""annotating"" each class model with specific validation, such as\n- `[Required(ErrorMessage = ""This field is required"")]`\n- `[MaxLength(20)]`\nThere are a number of issues with this approach:\n- Validation is scattered throughout the codebase as attributes on data model classes\n- Testing is not straightforward\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\n- Does not allow for conditional validation\n- **Fluent Validation**\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\n- Lives outside of the data model classes\n- Very easy to test validation in isolation\n- Error messaging can be externalised using dependency injection\n- Allows for chaining of validators and conditional validation\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\n",2307
"## Context\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\nTasks are scheduled and ran on the same node they are scheduled.\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\n",2127
## Context\nWe need to record the architectural decisions made on this project.\n,4401
"## Context\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\nsandboxed environment. A Docker image has been created to help with this but this still\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\nto run Bitcoin miners and other compute-stealing stuff.\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\nissues) and ""hacking"" a continous integration service to do the job for us. More detail can\nbe found in the [notes on Isolation](https://github.com/martinklepsch/cljdoc/blob/72da65055ab94942f33fb63b29b732e81b559508/doc/isolation.md)\n",2601
"## Context\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\nAs part of this, considerations were made around:\n* Ease of use for creating new sites\n* Ability to share components\n* Ability to segregate content for specific channels (sites)\n* Ability to share content up and down the stack where needed and appropriate\n* Permissions model required to support this model\n",1205
"## Context\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\n* Use an established library, like [Symfony Validation](https://symfony.com/doc/current/validation.html).\n* Write our own validation logic.\nThe arguments in favor of writing our own logic were:\n* We don't want to bind our domain layer to a concrete validation library implementation.\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\n* We don't know the ""maintenance cycles"" of the library, either we need to constantly update or the library is not maintained properly.\n* Every developer would have to learn the API of the external library.\nAt the start of the project we did not know where we should put the validation logic:\n* At the framework/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\n* At the use case layer, making validation part of the use case.\n",1532
"## Context and Problem Statement\nAs a user, I want to get Changes metrics related to tasks defined\nin a task tracker. A simple usecase example is to get insight of the\nratio of changes related to Feature Requests vs Bug fixing.\n## Decision Drivers\n* Simple implementation\n* No assumption about the tasks tracker\n* Support of a set of generic fields related to a task\n",4636
"## Context\nOur service-operator allows service teams to provision various AWS services by\ndeclaratively defining resources and submitting them via the kubernetes api.\nSome of these resources require IAM to authorise how the provisioned service\ncan be used. The types of actions that can be performed on.\n#### Example\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\n```\n---\napiVersion: storage.govsvc.uk/v1beta1\nkind: S3Bucket\nmetadata:\nname: s3-bucket-sample\nspec:\naws:\nLifecycleRules:\n- Expiration: 90days\nVersioning:\nEnabled: true\n```\nIn order to access a provisioned bucket via the the AWS SDK users will require\nan IAM role/policy that allows access.\nWe want things like bucket ACL, versioning configuration and lifecycle policy\nto be defined declaratively via the resource manifest (see example above), and continuously managed\nby the service operator.\nWe want users of the provisioned bucket to be able to read back all\nconfiguration, and be able to fully utilise the specific bucket for reading,\nwriting and managing their objects within the provisioned bucket, but we want\nto avoid giving permissions to users that could cause conflicts with the\nproperties that are managed by the service operator's reconcile loop.\nFor example, given the example manifest above, we would like to avoid giving\npermissions that would allow a user to alter the Expiration LifeCycleRules,\nsince any changes the user made would be periodically overwritten by the\nservice operator's reconciliation.\n",3907
## Context\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\n,1255
"## Context and Problem Statement\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\n## Decision Drivers <!-- optional -->\n* Simple and straightforward way to reason around passing data between components\n* Extensible anticipating use cases when using the component ""in the wild"" and having internal info accessible/when if needed.\n",3187
"## Context\nWe want to avoid testing implementation details in our integration tests.\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\ne.g. toggling the visibility of different tabs and window groups.\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\nIt could be more sensible to only verify the state/model.\n",2816
## Context\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\nAdditionally Geocoder can be used to verify addresses that are manually entered.\n- [Geocoder](https://www2.gov.bc.ca/gov/content/data/geographic-data-services/location-services/geocoder)\n- [Data BC](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service)\n- [API Swagger](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service/resource/40d6411e-ab98-4df9-a24e-67f81c45f6fa/view/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\n- [Developer API Keys](https://github.com/bcgov/gwa/wiki/Developer-Guide#developer-api-keys)\n- API Host = `https://geocoder.api.gov.bc.ca`\n,3968
## Context and Problem Statement\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\n## Decision Drivers\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\n,447
"## Context and Problem Statement\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\n```\n<xs:element name=""take"" type=""xs:integer"">\n```\nor with attributes as:\n```\n<xs:element name=""take"">\n<xs:complexType>\n<xs:attribute name=""value""/>\n</xs:complexType>\n</xs:element>\n```\n## Decision Drivers\n* consistency\n* expressiveness\n",4713
"Context\n-------\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\nDecision\n--------\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\nBenefits\n---------\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\n.. _method: https://github.com/openedx/event-routing-backends/blob/f430d4cf58bdab01e42fcc944241898606873d82/event_routing_backends/processors/mixins/base_transformer.py#L139\n",4477
"# Context\nThe way the Government Digital Service (GDS) [makes technology choices is\ndescribed in the service manual](https://www.gov.uk/service-manual/making-software/choosing-technology). We are selecting which technology will to use to provide\npersistence for the Performance Platform.\nGDS has experience in running MongoDB and MySQL in production.\nWe envisage the Performance Platform as taking in unstructured data from a\nvariety of data sources (spreadsheets, analytics, logs, other databases and\napplications) and allowing people to collect this data in a single place. This\nshould enable service managers to:\n- make comparisons\n- see how well their service is performing\n- see how the performance changes over time, as they iterate the service\nSo we want a persistent data store that will store unstructured data, and\nallow us to apply a structure either by post-processing the data, or at query\ntime.\nThe volume of the data that we are envisaging at this stage is pretty small.\nWe will be building a small thing to start; as we learn more about the\nuser needs and problem space, then we will revisit this decision. Since the\nvolume is small, it does not seem likely that we need Hadoop / HDFS or\nCassandra.\nWe are not the canonical source of this data. We are an aggregator; the\ncanonical source remains the data sources which will be providing feeds or\npushing the data into the Performance Platform.\nBecause of this position, we do not need ACID properties for this data, nor\nneed worry about the CAP theorem in any detail.\n# Decision\nWe will use MongoDB. We are comfortable operating it in production,\nit will allow unstructured data (in the form of JSON documents) and we can\napply structure at query time.\n# Status\nAccepted.\n# Consequences\nUse MongoDB with an appropriate replica-set configuration.\n",4266
"## Context and Problem Statement\nTax users are sometimes, without warning, unnecessarily signed out when accessing\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\nsuccess criterion 2.1.1 (Timing adjustable)](https://www.w3.org/WAI/WCAG21/Understanding/timing-adjustable.html).\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https://github.com/hmrc/play-frontend-hmrc#warning-users-before-timing-them-out)\ncurrently has no way of knowing about this activity following initial page load.\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\nremaining on the user's active session via an endpoint that is itself excluded from\nsession management.\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\nknowledge and avoiding introducing additional coupling between frontend microservices?\n## Decision Drivers\n* The need to minimise code changes for service teams other than a library upgrade.\n* The avoidance of requiring service teams to add any additional routing rules.\n* The avoidance of requiring service teams to add any additional configuration.\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\nany other library or service.\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\nof failure between frontend microservices.\n* The need for the endpoint used to interrogate the session to not itself affect the session.\n",560
## Context\nYou never think of everything. Sticking to standards is a very good\nthing to prevent you from doing things that can go bad. Those also\nhelps making the code be more readable and structured.\n,1418
"## Context\nThis ADR has been superseded by `ADR-0019`.\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\ntools it seemed prudent to explore additional options as well.\n| Workflow Tool   | Pros | Cons |\n|-----------------|------|------|\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\n",1688
"## Context and Problem Statement\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\n## Decision Drivers <!-- optional -->\n* High availability/Fault tolerance\n* Learning curve\n* Maintenance overhead\n* Vendor lock-in\n",1510
## Context\nWe need to record the architectural decisions made on this project.\n,3475
"## Context\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\n",3682
"## Context\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\nto not overload a server.\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\nallows, among others:\n- Delaying retries upon MX delivery failure to a remote site.\n- Throttling, which could be helpful for not being considered a spammer.\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\ndelays, purging the queue, etc.\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\nto interact with all its James servers, which is not friendly in a distributed setup.\nDistributed James relies on the following third party softwares (among other):\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\n- **ObjectStorage** (Swift or S3) holds byte content.\n",2892
"## Context\nThe main data source of the API catalogue website was originally a collection of\nmarkdown files. The build process uses the Middleman static site generator\nconfigured by the [Tech Docs Gem](https://github.com/alphagov/tech-docs-gem)\n('TDG').\nThe TDG provides additional functionality including search, sidebar\nnavigation ('Table of Contents'), the layout, and styling.\nThe TDG is not necessarily a good fit for the API catalogue because the project\nisn't purely documentation, and our data source is now a CSV.\nIn particular it is difficult to override templates inherited from the gem, to\nadjust the layout on a particular page or add page-specific JavaScript for\nexample.\nUsing TDG to render the Table of Contents is slow for our site because\nby design every page is re-rendered multiple times to pull out the headings\n(adding over a minute to build times).\nThe TDG also requires specific dependency versions. These version\nrestrictions prevent us being in control of version upgrades which are necessary\nto remain on support versions and receive security patches.\n",3586
"## Context and Problem Statement\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https://github.com/Semantic-Org/Semantic-UI-React/pull/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https://github.com/decentraland/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\n- The `@artsy/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\n- Doesn't have hooks support.\n#### Second alternative (react-semantic-ui)\n##### Advantages\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\n##### Disadvantages\n- Bad SSR support.\n",4613
"## Context\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\nIn the future we will want to more automatically match buyers’ articulations of need so as to work out which products CCS can offer to meet their need.\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\n",1001
## Context\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted/PaaS CI/CD solution\n,1741
"## Context\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\n",3987
## Context\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\n,3691
"## Context\nDrop FreeTFT, TFTA tokens from SDK\n",5198
"## Context\nThe issue motivating this decision, and any context that influences or constrains the decision.\nThe original author of go-bindata delete their account and then the repo was\nrecreated under a different owner. The dependency has shifted around enough that\nwe have lost faith in the intention of the maintainers\n[more details here](https://twitter.com/francesc/status/961249107020001280?lang=en)\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\nduring a test run or final build) made it hard to reason about.\n",3873
"## Context\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\n[Prettier](https://prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\n",3451
## Context\nWe have to make a decision on the branching strategy for development.\n,3653
## Context\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\n,3572
"## Context\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\n```\nstruct tof\n{\nusing distance_t = uint16_t;\nusing cb_t = stdext::inplace_function<void(distance_t)>;\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\nenum class mode\n{\ndefaultRange = 0,\nshortRange,\nmedRange,\nlongRange,\n};\n}''\n```\nInterface classes would inherit from these structs:\n```\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\n```\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\n```\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\n```\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\n",3027
"### Context\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\n```php\nfunction test_something(): void {\n// ...\ntry {\n// the statement that fail\n$this->fail();\n} catch (Exception $e) {\n// ...\n}\n}\n```\nOr:\n```php\nfunction test_something(): void {\n// ...\n$this->expectException($exception)\n// the statement that fail\n}\n```\n### Decision\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\nA pull request to fix this practice in the whole codebase may be done but has not been made\nmandatory. New pull requests though should stick to this practice.\n### Status\nAccepted ([#1090][1090])\n[sebastian-bergmann]: https://thephp.cc/company/consultants/sebastian-bergmann\n[phpunit-exception-best-practices]: https://thephp.cc/news/2016/02/questioning-phpunit-best-practices\n[1090]: https://github.com/infection/infection/pull/1061\n",68
"## Context\nWhen developing new algorithms and features for PACE-related software,\ndevelopers often create useful demo/visualisation scripts for their own use.\nThese scripts could be useful or interesting for other developers, and are\nimportant for reproducibility or justifying design decisions. They should be\nstored somewhere in version control so that they can be easily accessed by any\ndevelopers and referred to later. However, they are not intended for general\nuse and will not be actively maintained or tested. There are 2 main options:\n* Store them in a `dev_scripts` directory in each separate project repository\n* Store them in a `scripts` directory in `pace-developers`\nIf they're in the `dev_scripts` directory for each project repository:\n+ All in one place\n+ Scripts will be close to the code they are used for\n- Scripts may not work with the version of the code they are distributed with\n- It's unclear where scripts that use more than one project would go\n- Despite the folder being called `dev_scripts` people might expect the scripts\nto actually work as they're in the main project repository\nIf they're in a `scripts` directory in `pace-developers`:\n+ They can be kept close to the decision-making developer documentation that\nthey support\n+ A version can be specified for any project dependencies\n",5176
"**Context**: We know there will be users of these C++ libraries who want to use\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\nfor all of our users, regardless of their ability to use exceptions.\n**Decision**: None of our APIs will throw exceptions to indicate errors.\nInstead, our APIs will typically report errors to callers by returning a\n`Status` or `StatusOr<T>` object, unless the library we're using has another\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\nI/O library).\n**Consequences**: This decision will result in a single set of APIs and a\nconsistent vocabulary for all users, whether or not they choose to compile with\nexceptions. This decision does not prevent callers from using exceptions in\ntheir own code.\nA downside of this decision is that our APIs will not be natural or idiomatic\nfor the [50+%][survey-link] of users who might prefer exceptions for error\nreporting.\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\nbreaking change. As of this writing (Jan 2019), this project has a\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\ntimeline to change this API in a separate document.\n[badbit-link]: https://en.cppreference.com/w/cpp/io/ios_base/iostate\n[bigtable-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/bigtable\n[gcs-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/storage\n[survey-link]: https://isocpp.org/blog/2018/03/results-summary-cpp-foundation-developer-survey-lite-2018-02\n",2467
## Context and Problem Statement\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\n,3860
"## Context and Problem Statement\nIn order to simplify the tracking consent build and deploy process and\nmake integrating with tracking consent less surprising, should\nthe configuration of the GTM container used by tracking consent be via\ndata attributes rather than separate bundles?\n## Decision Drivers\n* The need to keep things simple for service developers\n* The need to improve the operability of tracking consent\n",3529
## Context\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\n,1890
## Context\nWe need to record the architectural decisions made on this project.\n,5220
## Context\nWe need to record the architectural decisions made on this project.\n,4722
"## Context\nI need to implement two special pages, the privacy policy page and the about me page.\nThey can't be in the map, I don't want them there because the user would\nbe forced to read them when scanning the spiral.\n",5001
"## Context\nThe current request builder APIs are not able to handle some odata requests like:\n- query navigation properties `GET /People('scottketchum')/Friends`\n- getting ""raw value"" of a property `/People('scottketchum')/$value`\n",3630
"## Context\nFrequency Capping allows Content Managers to limit the number of\nimpressions or interactions users have with content. Is a widely\navailable tool in Publishing Platforms.\nIt's usually developed on the server side where the system can decide\nhow many times to serve the content to the requesting users which we\ncall ""Global Frequency Capping"". Additionally the system may be able\nto limit the number of impressions per user which we call ""Local"" or\n""User Frequency Capping"".\nFor example a Content Piece can be set to 1,000,000 Global Impressions\nand 1 Impression per User, thus indirectly driving 1,000,000 different\nusers to this Content.\nThis functionality has been lacking from the Snippet Service due to\ntechnical limitations imposed by the way metrics were collected and\ncontent selection was handled on the client side. The latest\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\nPipeline unblock this capability. [0]\n",3229
## Context\nThe application is about finding services closest to the search point. All\nservices have a co-ordinate in lat/lon format. Currently the application\nrequests either an out-code or a postcode as the search point. The submitted\npostcode needs to be resolved to a lat/lon that can be used to query against.\n,2204
## Context or problem to solve\n* How should git histories look like?\n* What format should we use to write commit messages?\n,4600
## Context\nWe need to record the architectural decisions made on this project.\n,4597
## Context\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\n,4761
"## Context\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\n",3598
"## Context and Problem Statement\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\n## Decision Drivers\n* readable API documentation\n* effort of manually creating client services\n",693
"## Context and Problem Statement\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\nand was only noticed some days after when the Project Manager needed to\nexport the data. Upon investigation, it was discovered that:\n* Error logging was inactive on the application. (Now fixed)\n* We can’t debug using the responses our system returned to PayPal as\nwe don’t have access to the IPN log.\nThis led to a situation where we couldn't get the information required\nto debug the error. It was suggested we queue all incoming requests from\nPayPal on our own system for processing by our system.\n## Decision Drivers\n* **Transparency**: If our system fails we would have a stored queue to\nuse for debugging.\n* **Automation**: The IPNs wouldn't need to be fired again once an error\nbecomes fixed as our system would resume processing the queue.\n",1523
"## Context\nWe need to deploy the Broker somewhere.\nThe Broker implements the service broker API to generate per-application syslog\ndrain URLs (Adapter URLs).\nThe Adapter is written in Go.\nThe Broker is written in Go.\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\n",3295
