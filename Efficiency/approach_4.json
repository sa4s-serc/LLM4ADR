{"context": "## Context and Problem Statement\\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\\n## Decision Drivers <!-- optional -->\\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\n", "input": "## Context and Problem Statement\\n> Dex requires persisting state to perform various tasks such as track refresh tokens, preventing replays, and rotating keys.\\nWhat persistence option should we use?\\n## Decision Drivers\\n* CRDs add complexity\\n* Storage adds complexity\\n* We want to frequently reboot Nodes for security patching\\n* We want to deliver excellent user experience\\n\n## Decision\n* CRDs add complexity\\n* Storage adds complexity\\n* We want to frequently reboot Nodes for security patching\\n* We want to deliver excellent user experience\\nChosen option: \"use CRD-based storage\", because it improves user experience when Nodes are rebooted.\\nWith \"memory\" storage, Dex loses the OpenID keys when restarted, which leads to the user being forced to eventually re-login. Worst off, this forced re-login happens unexpectedly from the user's perspective, when the Kubernetes apiserver chooses to refresh the OpenID keys.\\nHere is the experiment to illustrate the issue:\\n```console\\n$ curl https://dex.$DOMAIN/.well-known/openid-configuration > before-openid-configuration.json\\n$ curl https://dex.$DOMAIN/keys > before-keys.json\\n$ kubectl delete pods -n dex -l app.kubernetes.io/instance=dex\\n$ curl https://dex.$DOMAIN/.well-known/openid-configuration > after-openid-configuration.json\\n$ curl https://dex.$DOMAIN/keys > after-keys.json\\n$ diff -y before-openid-configuration.json after-openid-configuration.json\\n[empty output, no differences]\\n$ diff -y before-keys.json after-keys.json\\n[all keys are replaced]\\n```\\n### Positive Consequences\\n* Nodes which host Dex can be rebooted for security patching\\n* User experience is optimized\\n### Negative Consequences\\n* Dex will have a more permissions in the Service Cluster (see [`rbac.yaml`](https://github.com/dexidp/helm-charts/blob/dex-0.6.3/charts/dex/templates/rbac.yaml))\\n* We will need to closely monitor migration steps for Dex\\n\n\n## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nThe issue is how often to save to local storage when a user types, previous implementation saved every 5 characters. But that caused issues [#86](https://github.com/bbc/react-transcript-editor/issues/86)\\n## Decision Drivers <!-- optional -->\\n<!-- * [driver 1, e.g., a force, facing concern, \u2026] -->\\n* A simple and straight forward way to save to local storage\\n* saving on a good frequency\\n* without introducing performance issues especially on less performant devices\\n* if possible without introducing third party dependencies\\n\n## Decision\n<!-- * [driver 1, e.g., a force, facing concern, \u2026] -->\\n* A simple and straight forward way to save to local storage\\n* saving on a good frequency\\n* without introducing performance issues especially on less performant devices\\n* if possible without introducing third party dependencies\\nChosen option: **using a js timer**.\\nIt uses a timer that can be consolidated into one final one rather then having a lot of saves being delayed, we just have one final save once after user has stopped typing for more then 5 seconds.\\nThe timer is cleared before being called so that there is only the final one left. Leaving only one final save at the end. As a performance optimization.\\n```js\\nif (this.saveTimer!== undefined) {\\nclearTimeout(this.saveTimer);\\n}\\nthis.saveTimer = setTimeout(() => {\\nthis.localSave(this.props.mediaUrl);\\n}, 5000);\\n```\\n<!-- because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n<!--\\n### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n\n\n## Context and Problem Statement\\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\\n## Decision Drivers <!-- optional -->\\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\n\n## Decision\n", "response": "<pad> * Performance - decision should support high throughput scenariosn* Maintainability - amount of code needed to write should be minimizednChosen option: **Status based on concurrency checks**, because it provides a clear separation between performance and maintenance.n### Positive Consequencesn* [e.g., improvement of quality attribute satisfaction, follow-up decisions required,...]n*...n### Negative Consequencesn* Dex will have a more permissions in the Service Cluster (see [#86](https://github.com/bbc/react-transcript-editor/issues/86)).n", "matched_ids": [3112, 3181], "time": "0:00:53.487295", "post": "## Decision\n* Performance - decision should support high throughput scenariosn* Maintainability - amount of code needed to write should be minimizednChosen option: **Status based on concurrency checks**, because it provides a clear separation between performance and maintenance \n ### Positive Consequencesn* [e.g., improvement of quality attribute satisfaction, follow-up decisions required,...]n*.. \n ### Negative Consequencesn* Dex will have a more permissions in the Service Cluster (see [#86](https://github.com/bbc/react-transcript-editor/issues/86)) \n "}
{"context": "## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https://www.postgresql.org/docs/9.5/static/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https://www.postgresql.org/docs/9.5/static/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n", "input": "## Context\\nMany ASP.NET applications use [Entity Framework (EF)](https://docs.microsoft.com/en-us/ef/), an Object Relational Mapper (ORM) that helps access data stored in database.\\nEF allows data in a database to be accessed by extending a DbContext class and adding properties to this extending class of type DbSet. DbContext and DbSet provide methods for performing basic CRUD operations against entities in a database that are defined in model classes. These model classes contain annotations that define the table name, columns and relationships with other entities. When a query is performed, EF handles creating instances of model classes and filling them with the received data.\\nSome properties are lazily loaded, with the queries related to fetching the required data only being run when thoses properties are accessed. This approach is commonly used when accessing a property representing a relationship with another entity.\\nA DbContext by default tracks changes to entities returned as the result of queries, with changes being saved when a call is made to a DbContext's SaveChanges or SaveChangesAsync methods.\\nThe DbContext and DbSet classes provide methods that can be used to fetch data, with the ability to apply limitations on what data is returned. EF will generate the required query, execute it, parse the response data and return the appropriate entity model instances.\\nEF supports migrations written as classes with Up and Down methods, to support upgrading and rolling back, respectively. These methods are implemented by adding calls to a provided MigrationBuilder instance.\\nDapper is a library that is commonly referred to as a \"micro-ORM\". It provides methods to support executing SQL queries and parsing the results to create instances of particular model classes. Unlike EF, Dapper does not support the tracking of changes and queries must be written using SQL.\\nDapper was developed for the StackOverflow website to address performance issues, as outlined in [this blog post](https://samsaffron.com/archive/2011/03/30/How+I+learned+to+stop+worrying+and+write+my+own+ORM).\\n\n## Decision\nWe will use Dapper with the [repository pattern](http://blog.mantziaris.eu/blog/2016/10/24/the-repository-and-unit-of-work-pattern/) to access data stored in the database.\\n\n\n## Context\\nThe configuration of validators are mounted into Connaisseur as a configmap, as it is common practice in the Kubernetes ecosystem. When this configmap is upgraded, say with a `helm upgrade`, the resource itself in Kubernetes is updated accordingly, but that doesn't mean it's automatically updated inside the pods which mounted it. That only occurs once the pods are restarted and until they are the pods still have an old version of the configuration lingering around. This is a fairly unintuitive behavior and the reason why Connaisseur doesn't mount the image policy into its pods. Instead, the pods have access to the kube API and get the image policy dynamically from there. The same could be done for the validator configuration, but there is also another solution.\\n## Problem 1 - Access to configuration\\nHow should Connaisseur get access to its configuration files?\\n### Solution 1.1 - Dynamic access\\nThis is the same solution as currently employed for the image policy configuration. The validators will get their own CustomResourceDefinition and Connaisseur gets access to this resource via RBAC so it can use the kube API to read the configuration.\\n**Pros:** Pods don't need to be restarted and the configuration can be changed \"on the fly\", without using Helm.\\n**Cons:** Not a very Kubernetes native approach and Connaisseur must always do some network requests to access its config.\\n### Solution 1.2 - Restart pods\\nThe other solution would be to use ConfigMaps for validators and image policy and then restart the pods, once there were changes in the configurations. This can be achieved by setting the hash of the config files as annotations into the deployment. If there are changes in the configuration, the hash will change and thus a new deployment will be rolled out as it has a new annotation. This corresponds to the [suggestion](https://helm.sh/docs/howto/charts_tips_and_tricks/#automatically-roll-deployments) made by Helm.\\n**Pros:** Kubernetes native and no more CustomResourceDefinitions!\\n**Cons:** No more \"on the fly\" changes.\\n### Decision Outcome (1)\\nSolution 1.2 was chosen, going with the more Kubernetes native way.\\n\n## Decision\nSolution 1.2 was chosen, going with the more Kubernetes native way.\\nSolution 2.2 was chosen as it is the more simpler of the two.\\n\n\n## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https://www.postgresql.org/docs/9.5/static/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https://www.postgresql.org/docs/9.5/static/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n\n## Decision\n", "response": "<pad> We will use the [repository pattern](http://blog.mantziaris.eu/blog/2016/10/24/the-repository-and-unit-of-work-pattern/) to access data stored in the database.n", "matched_ids": [727, 4398], "time": "0:00:30.582429", "post": "## Decision\nWe will use the [repository pattern](http://blog.mantziaris.eu/blog/2016/10/24/the-repository-and-unit-of-work-pattern/) to access data stored in the database \n "}
{"context": "## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n", "input": "## Context and Problem Statement\\nSeveral operations (crawl repo, deploy services, ...) are required to run in the background. Therefore, a background job architecture is required being able to launch arbitrary jobs in the background.\\n## Decision Drivers\\n* MUST be implemented in Java\\n* MUST offer a callback functionality\\n\n## Decision\n* MUST be implemented in Java\\n* MUST offer a callback functionality\\nChosen option: Java CompletableFuture, because of the possibility to chain multiple actions and the option to attach callbacks on completion of a background task.\\n### Positive Consequences\\n* Possibility to chain together multiple actions after a succesful execution of a background task.\\n* Use of FixedThreadPool, i.e., a pool of threads with a predefined number of threads which are reused. If all threads are active at a time and additional tasks are submitted, they will wait in a queue.\\n### Negative Consequences\\n* With the use of a FixedThreadPool, a background task may be delayed if all threads are active at the time of submission.This may produces time of waiting in the frontend of MICO.\\n\n\n## Context\\nEncryption keys for frontend user sessions are provided in environment variables and cycled during every release.\\nThis has resulted in one incident of losing syncronisation, causing errors for users when services scale up and then scale down.\\n\n## Decision\nWe will use a AWS owned Customer Master Key for the Sessions Dynamo DB tables to encrypt session tokens, and not push encryption keys into containers.\\nTable names                                       |\\n--------------------------------------------------|\\nrefunds-sessions-front-<opg_stackname>            |\\nrefunds-sessions-caseworker-front-<opg_stackname> |\\n\n\n## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n\n## Decision\n", "response": "<pad> * PostgreSQL is a database that is used by PostgreSQLn* PostgreSQL is a database that is used by PostgreSQLnChosen option: PostgreSQL, because of the possibility to use multiple databases at once.n### Positive Consequencesn* PostgreSQL is a database that is used by PostgreSQLn* PostgreSQL is a database that is used by PostgreSQLn### Negative Consequencesn* PostgreSQL is not a database that is used by PostgreSQLn", "matched_ids": [4674, 295], "time": "0:00:51.010973", "post": "## Decision\n* PostgreSQL is a database that is used by PostgreSQLn* PostgreSQL is a database that is used by PostgreSQLnChosen option: PostgreSQL, because of the possibility to use multiple databases at once \n ### Positive Consequencesn* PostgreSQL is a database that is used by PostgreSQLn* PostgreSQL is a database that is used by PostgreSQLn### Negative Consequencesn* PostgreSQL is not a database that is used by PostgreSQLn"}
{"context": "## Context\\nWe need a system to manage the lifecycle of a grant application.\\n", "input": "## Context\\nWe need a workflow to build our appliation and components.\\n\n## Decision\nWe use Storybook only for building new self contained components.\\n\n\n## Context and Problem Statement\\nWe need a way to send emails to users for several workflows.\\n\n## Decision\nChosen option: SendGrid, because it is SaaS and has generous free tier. Integration is easy.\\n### Positive Consequences <!-- optional -->\\n* Fast time to develop\\n### Negative Consequences\\n* We need to think about setting up development environment\\n\n\n## Context\\nWe need a system to manage the lifecycle of a grant application.\\n\n## Decision\n", "response": "<pad> Chosen option: Security, because it is a free and open source project. It has a lot of features and is easy to use.n### Positive Consequences!-- optional -->n* Fast time to developn### Negative Consequencesn* We need to think about setting up development environmentn", "matched_ids": [3722, 2523], "time": "0:00:37.105756", "post": "## Decision\nChosen option: Security, because it is a free and open source project. It has a lot of features and is easy to use \n ### Positive Consequences!-- optional -->n* Fast time to developn### Negative Consequencesn* We need to think about setting up development environmentn"}
{"context": "## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n", "input": "## Context and Problem Statement\\nJabRef has translation files `JabRef_it.properties`, ... There are translated and untranslated strings. Which ones should be in the translation file?\\n## Decision Drivers\\n* Translators should find new strings to translate easily\\n* New strings to translate should be written into `JabRef_en.properties` to enable translation by the translators\\n* Crowdin should be kept as translation platform, because 1\\) it is much easier for the translators than the GitHub workflow and 2\\) it is free for OSS projects.\\n\n## Decision\n* Translators should find new strings to translate easily\\n* New strings to translate should be written into `JabRef_en.properties` to enable translation by the translators\\n* Crowdin should be kept as translation platform, because 1\\) it is much easier for the translators than the GitHub workflow and 2\\) it is free for OSS projects.\\nChosen option: \"Only translated strings in language file\", because comes out best \\(see below.\\n\n\n## Context\\nABCI-server could expose its own RPC-server and act as a proxy to Tendermint.\\nThe idea was for the Tendermint RPC to just be a transparent proxy to the app.\\nClients need to talk to Tendermint for proofs, unless we burden all app devs\\nwith exposing Tendermint proof stuff. Also seems less complex to lock down one\\nserver than two, but granted it makes querying a bit more kludgy since it needs\\nto be passed as a `Query`. Also, **having a very standard rpc interface means\\nthe light-client can work with all apps and handle proofs**. The only\\napp-specific logic is decoding the binary data to a more readable form (eg.\\njson). This is a huge advantage for code-reuse and standardization.\\n\n## Decision\nWe dont expose an RPC server on any of our ABCI-apps.\\n\n\n## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n\n## Decision\n", "response": "<pad> * Translators should find new strings to translate easilyn* New strings to translate should be written into JabRef_en.properties to enable translation by the translatorsn* Crowdin should be kept as translation platform, because 1) it is much easier for the translators than the GitHub workflow and 2) it is free for OSS projects.nChosen option: \"Only translated strings in language file\", because comes out best (see below.n", "matched_ids": [4752, 1985], "time": "0:00:42.782604", "post": "## Decision\n* Translators should find new strings to translate easilyn* New strings to translate should be written into JabRef_en.properties to enable translation by the translatorsn* Crowdin should be kept as translation platform, because 1) it is much easier for the translators than the GitHub workflow and 2) it is free for OSS projects \n Chosen option: \"Only translated strings in language file\", because comes out best (see below \n "}
{"context": "## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n", "input": "## Context and Problem Statement\\nDocument uploads will be stored in Azure Blob storage because of the requirement to store very large files & the fact that Azure Blob storage offers high availability and low costs.\\nThe Azure Blob storage system is a separate cloud service which is not directly linked to Dynamics CRM, therefore we need to consider how the permits, stored using CRM case/incident entities, will be linked to the the files within Azure.\\n## Decision Drivers\\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n\n## Decision\n* Files will be displayed to users via a web frontend, therefore the entity used to store the file reference attribute should be accessible and filterable via the WebAPI.\\n* The existing Dynamics UI should be able to display links to the Azure files for administrators.\\n[Option 1] Use the existing 'annotation' entity in the CRM & set the entity attributes to match Azure blob data.\\n### Positive Consequences\\n* Minimises customisation of the CRM data, reuse of existing\\n### Negative Consequences\\n* Labelling within the CRM does not match the context of the upload.\\n\n\n## Context\\nGOV.UK uses MongoDB as the backing database for a number of apps. In a generic\\nGOV.UK environment, there are 5 MongoDB clusters: one for Licensify (a legacy\\nservice), one for performance monitoring, one for router and another per cloud\\nprovider (i.e. AWS and Carrenza) for all other apps (e.g. assets, imminence,\\nmaslow, content, publisher and short url manager).\\nOne architectural strategy principle of TechOps is to make use as much as\\npossible AWS managed services in order to reduce the burden on GOV.UK teams to\\nmanage Commercial off-the-shelf (COTS) functionality/services.\\n\n## Decision\nThe approaches taken for migrating from MongoDB to AWS DocumentDB are:\\n1. new apps that are being migrated from legacy cloud providers (e.g UKCloud and\\nCarrenza) to AWS and use MongoDB should be evaluated for compatibility with\\nAWS DocumentDB during the migration phase. Based on this evaluation, a\\ndecision should be made based on cost benefit whether a particular migrating\\napp should be using DocumentDB or MongoDB.\\n2. there are some apps (e.g. content store and imminence) that have already been\\nmigrated to AWS and uses the MongoDB cluster there. These apps should be\\nevaluated for compatibility with AWS DocumentDB and prioritised accordingly\\nfor migration to AWS DocumentDB.\\n\n\n## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n\n## Decision\n", "response": "<pad> * The Azure Blob storage system is a separate service which is not directly linked to Dynamics CRM, therefore we need to consider how the permits, stored using CRM case/incident entities, will be linked to the files within Azure Blob storage.n[Option 1] Use the existing 'annotation' entity in the CRM & set the entity attributes to match Azure blob data.n# Positive Consequencesn* Minimises customisation of the CRM data, reuse of existingn# Negative Consequencesn* Labelling within the CRM does not match the context of the upload.n", "matched_ids": [5163, 4047], "time": "0:00:44.711530", "post": "## Decision\n* The Azure Blob storage system is a separate service which is not directly linked to Dynamics CRM, therefore we need to consider how the permits, stored using CRM case/incident entities, will be linked to the files within Azure Blob storage \n [Option 1] Use the existing 'annotation' entity in the CRM & set the entity attributes to match Azure blob data \n # Positive Consequencesn* Minimises customisation of the CRM data, reuse of existingn# Negative Consequencesn* Labelling within the CRM does not match the context of the upload \n "}
{"context": "## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n", "input": "## Context\\nThe tool will have a `help` subcommand to provide documentation\\nfor users.\\nIt's nice to have usage documentation in the script files\\nthemselves, in comments.  When reading the code, that's the first\\nplace to look for information about how to run a script.\\n\n## Decision\nWrite usage documentation in comments in the source file.\\nDistinguish between documentation comments and normal comments.\\nDocumentation comments have two hash characters at the start of\\nthe line.\\nThe `adr help` command can parse comments out from the script\\nusing the standard Unix tools `grep` and `cut`.\\n\n\n## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n\n## Decision\nUse Static Code Analysis to find violations of standards.\\n\n\n## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n\n## Decision\n", "response": "<pad> The tool will have a help subcommand to provide documentation.n", "matched_ids": [3566, 1418], "time": "0:00:22.694399", "post": "## Decision\nThe tool will have a help subcommand to provide documentation \n "}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "input": "## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard\\nin this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n", "response": "<pad> We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).n", "matched_ids": [494, 1269], "time": "0:00:23.989702", "post": "## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) \n "}
{"context": "## Context\\nThe rig defined at [Bookit Infrastructure](https://github.com/buildit/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\\nThe only way to capture that is via branches which can be hard to discover.\\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\\n", "input": "## Context\\nWe've got a Single Page app, it needs to talk to the server. Our use of scala and scalajs means we can use [lihaoyi's autowire\\nmacros](https://github.com/lihaoyi/autowire)\\nAlthough this is essentially a 0 on the [Richardson maturity model](https://martinfowler.com/articles/richardsonMaturityModel.html)\\nit has huge benefits in terms of speed of change. We also (at the moment) only have the one client of the SPA so we can afford the tight coupling.\\nIt doesn't preclude moving toward something more restful, as we can just add routes when we recognise a need.\\n\n## Decision\nUse autowire for now.\\n\n\n## Context\\nCurrently we are using [Docker](https://www.docker.com/) as the runtime and orchestration platform for implementing the pipeline steps.\\nGiven its maturity, ubiquitous deployment and tooling around it, it gave us great ease to use it specially via its REST API.\\nBob is what it is mainly due to the enablement Docker had.\\nHowever, it raises the following issues:\\n- Docker mainly runs as a daemon and moreover **needs root access** for the daemon function\\n- Bob is generally intended as a [Cloud Native](https://en.wikipedia.org/wiki/Cloud_native_computing) tool which means all components should be containerized.\\n- Given that docker needs root permissions to run, the runner needs to be privileged to function, causing a security risk to the cluster\\n- The alternative being to mount the host's docker socket into the runner container which is an even bigger security risk\\n- Docker running as a daemon in a container is not a very reliable setup and its own monitoring is still a concern\\n\n## Decision\nBased on the above facts the following is decided:\\n- Use [Podman](https://podman.io/) as the container runtime and orchestration engine for the following reasons:\\n- It is rootless and daemonless\\n- Developed by [RedHat](https://www.redhat.com/) and the [OCI](https://opencontainers.org/) community\\n- Fully FOSS\\n- Exposes a REST API which is docker complaint too\\n- Brings in the possibilities of more things like pods and more\\n- Swap out [clj-docker-client](https://github.com/into-docker/clj-docker-client) in favor of [contajners](https://github.com/lispyclouds/contajners) as the Clojure interface to the engine\\n- Have a self contained image having the runner, the JVM and Podman and run it **unprivileged**.\\n\n\n## Context\\nThe rig defined at [Bookit Infrastructure](https://github.com/buildit/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\\nThe only way to capture that is via branches which can be hard to discover.\\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\\n\n## Decision\n", "response": "<pad> Based on the above facts the following is decided:n- Use [Podman](https://podman.io/) as the container runtime and orchestration engine for the following reasons:n- It is rootless and daemonlessn- Developed by [RedHat](https://www.redhat.com/) and the [OCI](https://opencontainers.org/) communityn- Fully FOSSn- Exposes a REST API which is docker complain toon- Brings in the possibilities of more things like pods and moren- Swap out [clj-docker-client](https://github.com/into-docker/clj-docker-client) in favor of [contajners](https://github.com/lispyclouds/contajners) as the Clojure interface to the enginen- Have a self contained image having the runner, the JVM and Podman and run it **unprivileged**.n", "matched_ids": [1907, 4289], "time": "0:01:04.811492", "post": "## Decision\nBased on the above facts the following is decided:n- Use [Podman](https://podman.io/) as the container runtime and orchestration engine for the following reasons:n- It is rootless and daemonlessn- Developed by [RedHat](https://www.redhat.com/) and the [OCI](https://opencontainers.org/) communityn- Fully FOSSn- Exposes a REST API which is docker complain toon- Brings in the possibilities of more things like pods and moren- Swap out [clj-docker-client](https://github.com/into-docker/clj-docker-client) in favor of [contajners](https://github.com/lispyclouds/contajners) as the Clojure interface to the enginen- Have a self contained image having the runner, the JVM and Podman and run it **unprivileged** \n "}
{"context": "## Context\\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\\nPHP [appears to be on a downwards trend as a language](https://pypl.github.io/PYPL.html?country=GB),\\nespecially in contrast with Python. It's likely it will become increasingly difficult\\nto find good PHP developers in future.\\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\\nlanguages of similar pedigree, such as Python.\\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\\nA mix of contractors and developers working on the code base over several years has\\nresulted in a mix of styles and approaches. While we have already cleared out a lot\\nof unused and/or broken code, there is likely to be more we haven't found yet.\\nWe are also lagging behind the latest Design System guidelines, as our application was one\\nof the first to go live, before the current iteration of the Design System existed.\\nThis means that any changes to design have to be done piecemeal and manually: we can't\\nsimply import the newest version of the design system and have everything magically update.\\nThis combination of factors means that the code base can be difficult to work with:\\nresistant to change and easy to break.\\n", "input": "## Context\\nWe are currently using the [Java-based OpenAPI generator CLI](https://www.npmjs.com/package/@openapitools/openapi-generator-cli).\\nAs this generator is targeting many different programming languages in some points it does not fit our needs, e. g. handling of unique names or API files, which also differ between components.\\nIn our current approach we wrap the generated API with our own to fit those needs, but we realized, that we ended up writing many workarounds.\\nMany workarounds can currently only be covered by preprocessing/manipulating the original service definition.\\nAt the moment, some API designs cannot be realized or are not worth investing into.\\nWe have observed that due to the size of the project, there are quite a lot of known bugs, that take a long time to solve or are not solved at all.\\nThe probably biggest pain point is that the used CLI needs a Java runtime.\\nTo run the CLI we have to download the given .jar file (~24MB) and make it part of our npm package, making it much bigger in size than usual.\\nThis is necessary as the file is not downloaded on installation but on command execution, causing other errors down the line (hurts SLC-29, race conditions in async batch execution).\\n\n## Decision\nWrite our own generator, that is purely based on TypeScript and supports the API we need out of the box.\\n\n\n### Context\\nThe touch table app should support multi touch while also being robust enough to handle various UI demands. Although we initially considered a 3D model of the universe in the center of the table, that approach was dropped as it didn't help accomplish our learning goals. Given that we use a Windows device for the touch table, we must choose a technology that can run on such a machine.\\n### Decision\\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https://lifeonearth.seas.harvard.edu/learning-activities/deeptree/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https://github.com/zooniverse/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n\n## Decision\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https://lifeonearth.seas.harvard.edu/learning-activities/deeptree/), another app built in WPF.\\n### Status\\nAccepted\\n### Consequences\\nFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The .NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.\\n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes .NET client](https://github.com/zooniverse/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).\\nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.\\n\n\n## Context\\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\\nPHP [appears to be on a downwards trend as a language](https://pypl.github.io/PYPL.html?country=GB),\\nespecially in contrast with Python. It's likely it will become increasingly difficult\\nto find good PHP developers in future.\\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\\nlanguages of similar pedigree, such as Python.\\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\\nA mix of contractors and developers working on the code base over several years has\\nresulted in a mix of styles and approaches. While we have already cleared out a lot\\nof unused and/or broken code, there is likely to be more we haven't found yet.\\nWe are also lagging behind the latest Design System guidelines, as our application was one\\nof the first to go live, before the current iteration of the Design System existed.\\nThis means that any changes to design have to be done piecemeal and manually: we can't\\nsimply import the newest version of the design system and have everything magically update.\\nThis combination of factors means that the code base can be difficult to work with:\\nresistant to change and easy to break.\\n\n## Decision\n", "response": "<pad> Although considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https://lifeonearth.seas.harvard.edu/learning-activities/deeptree/), another app built in WPF.n### StatusnAcceptedn### ConsequencesnFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The.NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old.n_In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes.NET client](https://github.com/zooniverse/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK).nIn the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs.n", "matched_ids": [3626, 5076], "time": "0:01:29.700532", "post": "## Decision\nAlthough considering Unity early in the planning phases, we dropped that consideration due to the learning curve of C++. Unity has also dropped support of Javascript, a language most of our team is familiar with. The transition from Javascript to C# seemed less jarring, and we have more resources and shared knowledge in C#. Initial work on the table was also done with the help of Florian Block, who designed the [DeepTree](https://lifeonearth.seas.harvard.edu/learning-activities/deeptree/), another app built in WPF \n ### StatusnAcceptedn### ConsequencesnFortunately, documentation for C# is quite strong, and the language allows enough flexibility to fit the needs of the table. The.NET community is also very active, and it is easy to find answers to obstacles we would encounter during development. However, WPF seems to be losing popularity, and many of the articles concerning WPF were written a decade ago, although some Stack Overflow posts are only a year or two old \n _In Retrospect_: Although I'm concerned about the longevity of WPF, the subsystem fits our needs for this project. It's also helpful to have the [Panoptes.NET client](https://github.com/zooniverse/panoptes-net-client) available for future C# projects. However, it is discerning to see many frameworks and packages used by WPF becoming deprecated (Silverlight, Microsoft Surface SDK) \n In the future, I think it would be worthwhile to explore new technologies; however, WPF is a suitable choice for most of our needs \n "}
{"context": "## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n", "input": "## Context\\nKorin would look much better with artist images. Unfortunately LastFm [removed artist images][lastfm-images-gone] from their API and have replaced it with a placeholder. There are some options to get artist images which are:\\n- [Music story][music-story-api]: The documentation is not easy to understand and the SDK is designed for web browsers. It also uses a custom ID to associate artists, not IMDB. One unique feature is that it has lyrics too.\\n- [Music brainz][music-brainz-api]: Creative commons but has everything except artist images.\\n- [Spotify][spotify-api]: Has artist information including images but uses a custom ID, not IMDB.\\n\n## Decision\nSpotify is the only viable alternative.\\n\n\n## Context\\nHistorically we have had \"two ways\" of configuring Verify Service Provider:\\n- Using environment variables\\n- Using a YAML file\\nWhen using environment variables the application used the verify-service-provider-env.yml\\nfile from the resources directory (so inside the jar). When using the YAML file you would\\npass the path to a different file as a command line parameter - usually people\\nwould use the example one that's contained in the repo.\\nThere were a couple of reasons for the extra complexity of managing two files, both due to\\nrestrictions with the java buildpack used by cloudfoundry:\\n- It's not possible to specify command line arguments through the java buildpack,\\nso you can't specify a path to your config file\\n- We weren't confident in the way cloudfoundry manages static files, so we didn't want\\nto rely on one.\\nThere was also a philosophical point that 12 factor applications should be configured through\\ntheir environment. This made the \"hide the configuration in the .jar and do everything through\\nenv vars\" way appealing.\\n\n## Decision\nWe will remove the verify-service-provider-env.yml file from src/main/resources\\nThe application will default to the verify-service-provider.yml\\nfile that's included in the .zip if no command line arguments are provided.\\nIf the application is started without command line arguments specifying a yml file\\nAND no environment variables have been set, startup should error gracefully and tell\\nthe user that the configuration fields have not been specified for example:\\n\"ERROR - no configuration fields found, either set environment variables or specify\\na configuration file using command line arguments ```server <path/to/verify-service-provider.yml>```\"\\nWe will establish the path to verify-service-provider.yml by asking java for the\\npath to the .jar file containing the Application class and looking in the parent\\nfolder.\\n\n\n## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n\n## Decision\n", "response": "<pad> We will remove the verify-service-provider-env.yml file from src/main/resourcesnThe application will default to the verify-service-provider.ymlnfile that's included in the.zip if no command line arguments are provided.nIf the application is started without command line arguments specifying a yml filenAND no environment variables have been set, startup should error gracefully and tellnthe user that the configuration fields have not been specified for example:n\"ERROR - no configuration fields found, either set environment variables or specifyna configuration file using command line arguments server path/to/verify-service-provider.yml>\"nWe will establish the path to verify-service-provider.yml by asking java for thenpath to the.jar file containing the Application class and looking in the parentnfolder.n", "matched_ids": [1598, 4448], "time": "0:00:55.894420", "post": "## Decision\nWe will remove the verify-service-provider-env.yml file from src/main/resourcesnThe application will default to the verify-service-provider.ymlnfile that's included in the.zip if no command line arguments are provided \n If the application is started without command line arguments specifying a yml filenAND no environment variables have been set, startup should error gracefully and tellnthe user that the configuration fields have not been specified for example:n\"ERROR - no configuration fields found, either set environment variables or specifyna configuration file using command line arguments server path/to/verify-service-provider.yml>\"nWe will establish the path to verify-service-provider.yml by asking java for thenpath to the.jar file containing the Application class and looking in the parentnfolder \n "}
{"context": "## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n", "input": "## Context\\nWe need to organize our code. We also want to run integration tests, and code examples that use the code as a 3rd party library.\\n\n## Decision\nWe want a monorepo style setup and settle for yarn workspaces. Lerna seems to complicated at the moment.\\n\n\n## Context\\nAs we rename Event Sources to Bindings, and formally separate State Stores, Message Buses, and Bindings, we need to decide if we need to introduce different manifest types.\\n\n## Decision\n### Dapr\\n* All components use the same **Component** manifests, identified by a component **type**.\\n* We'll come up with a mechanism to support pluggable secret stores. We'll support Kubernetes native secret store and Azure Key Vault in the initial release.\\n\n\n## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n\n## Decision\n", "response": "<pad> We will use [Apache](https://github.com/Apache/Apache/Apache) for the runtime version.n", "matched_ids": [2030, 84], "time": "0:00:24.804860", "post": "## Decision\nWe will use [Apache](https://github.com/Apache/Apache/Apache) for the runtime version \n "}
{"context": "## Problem\\nMost of the credentials/information of the environment (like a service address) are only available as soon the application is deployed.\\n", "input": "## Context\\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\\nand the server side code (which will be a dropwizard app).\\nDepending on how the users want to run the service provider we may need\\ndifferent security solutions.\\n\n## Decision\nIf possible users can talk to the service provider on the loopback (127.0.0.1)\\nIf that doesn't work for some reason then they can use the dropwizard config\\nto set up basic auth or tls or something.\\nSee http://www.dropwizard.io/1.1.0/docs/manual/configuration.html#connectors\\n\n\n## Context\\nNeed of using Infrastructure as code to, track what resources are running in the AWS account\\n\n## Decision\nUse Terraform\\n\n\n## Problem\\nMost of the credentials/information of the environment (like a service address) are only available as soon the application is deployed.\\n\n## Decision\n", "response": "<pad> We will use a service_name field to identify the environment.n", "matched_ids": [578, 1464], "time": "0:00:21.990565", "post": "## Decision\nWe will use a service_name field to identify the environment \n "}
{"context": "## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno / minimal costs.\\n", "input": "## Context and Problem Statement\\nBFI's IIIF Univeral Viewer auditing platform requires that each event\\nwhich is subject to long term persistence is captured in a manner that\\nis secure, robust and performant.\\n## Decision Drivers\\n* Ability for internal BFI staff to leverage their existing experience\\nwith managing and maintaining the RDMS.\\n* Ease of deployment and configuration.\\n* Ease of ongoing maintenance, patching and support.\\n* Open source, mature and in active support / maintenance.\\n\n## Decision\n* Ability for internal BFI staff to leverage their existing experience\\nwith managing and maintaining the RDMS.\\n* Ease of deployment and configuration.\\n* Ease of ongoing maintenance, patching and support.\\n* Open source, mature and in active support / maintenance.\\nMySQL is selected as the solution as it aligns provides BFI with an RDMS\\nwith which they already have experience and are able to support and\\nmaintaining in the long term.\\n### Positive Consequences\\n* No initial or ongoing licensing costs associated with the use of\\nMySQL.\\n* Ease of ongoing maintenance for BFI owning to their existing\\nfamiliarity with MySQL.\\n* Availability of multiple storage engines (InnoDB, MyISAM, etc) and\\ndistributions (MySQL Community Edition, MariaDB, PerconaDB, etc).\\n* Simple deployment and configuration (e.g., MySQL publish official\\nDocker images).\\n### Negative Consequences\\n* Certain SQL features are not available (although it is not expected\\nthat these will be required).\\n\n\n## Context and Problem Statement\\nIn order to simplify the tracking consent build and deploy process and\\nmake integrating with tracking consent less surprising, should\\nthe configuration of the GTM container used by tracking consent be via\\ndata attributes rather than separate bundles?\\n## Decision Drivers\\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\n\n## Decision\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\nChosen option: \"Use a data attribute\", because based on the benefits listed below the team\\nbelieves this is the best way forward.\\n### Positive Consequences\\n* The Javascript bundle creation process is simplified.\\n* The Scala Play routing is simplified\\n* Only one endpoint needs to be managed in production\\n* Future additional containers can be supported more easily\\n* Service developers only have to configure a single URL to tracking consent and use\\na data attribute to configure the container.\\n* The central common configuration repo (owned by a separate team) only requires a single URL to\\ntracking consent defining for each environment, rather than one for each GTM container.\\n### Negative Consequences\\n* Service developers need to add id=\"tracking-consent-script-tag\" to the SCRIPT\\ntag when integrating.\\n\n\n## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno / minimal costs.\\n\n## Decision\n", "response": "<pad> * Ease of initial deployment and configuration of the solution.n* Ongoing hosting and maintenance costs of the solution.n* Availability of core features to satisfy the project requirements withnno / minimal costs.nChosen option: \"Use a data attribute\", because based on the benefits listed below the teamnbelieves this is the best way forward.n### Positive Consequencesn* The Javascript bundle creation process is simplified.n* The Scala Play routing is simplifiedn* Only one endpoint needs to be managed in productionn* Future additional containers can be supported more easilyn* Service developers only have to configure a single URL to tracking consent and usena data attribute to configure the container.n* The central common configuration repo (owned by a separate team) only requires a single URL to tracking consent defining for each environment, rather than one for each GTM container.n### Negative Consequencesn* Service developers need to add id=\"tracking-consent-script-tag\" to the SCRIPTntag when integrating.n", "matched_ids": [4854, 3529], "time": "0:01:06.706708", "post": "## Decision\n* Ease of initial deployment and configuration of the solution \n * Ongoing hosting and maintenance costs of the solution \n * Availability of core features to satisfy the project requirements withnno / minimal costs \n Chosen option: \"Use a data attribute\", because based on the benefits listed below the teamnbelieves this is the best way forward \n ### Positive Consequencesn* The Javascript bundle creation process is simplified \n * The Scala Play routing is simplifiedn* Only one endpoint needs to be managed in productionn* Future additional containers can be supported more easilyn* Service developers only have to configure a single URL to tracking consent and usena data attribute to configure the container \n * The central common configuration repo (owned by a separate team) only requires a single URL to tracking consent defining for each environment, rather than one for each GTM container \n ### Negative Consequencesn* Service developers need to add id=\"tracking-consent-script-tag\" to the SCRIPTntag when integrating \n "}
{"context": "## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n", "input": "## Context\\nJames has an interface called blob-store that allows to choose how Blobs are stored between several implementations.\\nFor Distributed product, the default backend is Cassandra because it's fast and it doesn't require yet another\\nserver.\\nWhen storage cost concerns are more important than raw performances, James can store Blobs in S3 or Swift\\nObject Storage technologies.\\nObject Storage is known to be cheap and yet offers guarantee about durability.\\nWhen we implemented Object Storage blob-store in the past, we decided to implement it with\\n[Apache jcloud library](https://jclouds.apache.org/) because we wanted to target both S3 and Swift API.\\nHowever, we found that our implementation was complex and it didn't fit the Reactive style we now have in the codebase.\\nIt also contribute negatively to the project build time.\\nAt the same time, we figured out Swift was providing a good compatibility layer for S3 and that we may drop\\nour Swift code without dropping Swift support at the same time.\\n\n## Decision\n* Use AWS S3 v2 SDK to implement a Reactive S3 blob-store (`blobstore-s3`).\\n* Replace current `blob-store-objectstorage` with `blobstore-s3`\\n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swift\\n\n\n## Context\\nSince we decided to use event sourcing, we need a way to store events in our database.\\nIn `Wegwijs` we stored events in `MSSQL`, which allows easy debugging of events. All sql statements to save/read events were hand-written.\\n**However**, since we decided on async event handlers in a previous ADR, we would benefit a lot from having catch-up subscriptions for our event handlers. Catch-up subscriptions allow event handlers to be in charge of what events they are interested in, and give event handlers more autonomy over their own rebuilds.\\nWhile `GetEventStore` supports this, and is most likely a top-notch choice for storing events, this would require us to take care of hosting this. We also have doubts about the support for storing business-critical data outside of `MSSQL` in `AIV`.\\nWe currently host no VMs for business-critical concerns, and we feel that hosting `GetEventStore` ourselves, would add a significant burden.\\nAs an alternative, `SqlStreamStore` is an OSS library on GitHub which supports storing events into `MSSQL`, and has support for catch-up subscriptions. It has an active community, and has been used in several production systems successfully according to that community.\\n\n## Decision\nWe will use the `SqlStreamStore` library as our event store. We will keep an eye on ongoing developments from `SqlStreamStore`.\\n\n\n## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n\n## Decision\n", "response": "<pad> * Use AWS RDS SDK to implement a Reactive S3 blob-store (blobstore-s3).n* Replace current blob-store-objectstorage with blobstore-s3n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swiftn", "matched_ids": [2897, 3123], "time": "0:00:33.187526", "post": "## Decision\n* Use AWS RDS SDK to implement a Reactive S3 blob-store (blobstore-s3) \n * Replace current blob-store-objectstorage with blobstore-s3n* Run load tests to ensure there's no penalty when switching from Swift to S3 over Swiftn"}
{"context": "## Context\\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\\n", "input": "**Context**: operations that change state in the server may complete with an\\nindeterminate status code. For example: an error in the 500 range may be\\nproduced by a middle-tier server after the operation was started by the backend.\\nRe-issuing the request may not work as expected for the application developer.\\n**Decision**: by default the library will retry all requests, including\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n\n## Decision\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n\n\n## Context\\nWhen an ingest fails in the middle of a transaction which is adding files, the\\nFileSets will not get persisted. However, the files will have already been\\ncopied to the repository via `FileAppender`. This results in files in the\\nrepository which have no corresponding database record.\\nFixing this will require development of a transactional disk StorageAdapter\\nwhich moves files at the end of a metadata transaction.\\n\n## Decision\n1. We don't have time to implement a transactional disk StorageAdapter at this\\ntime.\\n2. Accept this situation, document it here, and know we can free up space in the\\nfuture by looking for unlinked files and deleting them.\\n\n\n## Context\\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\\n\n## Decision\n", "response": "<pad> 1. We don't have time to implement a transactional disk StorageAdapter at thisntime.n2. Accept this situation, document it here, and know we can free up space in thenfuture by looking for unlinked files and deleting them.n", "matched_ids": [2466, 4824], "time": "0:00:30.496381", "post": "## Decision\n1. We don't have time to implement a transactional disk StorageAdapter at thisntime \n 2. Accept this situation, document it here, and know we can free up space in thenfuture by looking for unlinked files and deleting them \n "}
{"context": "## Problem\\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\\n(as they are designed to do so). Limitations of other storage plugins are\\ne.g., that not every structure of configuration is allowed.\\nSome of these limitations were documented `infos/status`, others were not.\\n", "input": "## Context and Problem Statement\\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`/`PUT` HTTP methods),\\nRead(`GET` HTTP method), Update(`PUT`/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\\nworkflow in a state machine) or command execution.\\n## Decision Drivers\\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\n\n## Decision\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\nThe \"Taking advantage of the `Controller` REST archetype\" should be considered as a recommended solution. All Sylius\\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\\nor new schema should be introduced for the JSON-LD structure.\\nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing is\\ndelivered with vanilla Sylius installation.\\n\n\n## Context and Problem Statement\\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\\narbitrary style overrides on the component.\\nWe want all components in FDS to follow a similar prop interface, so this decision should\\napply to _all_ components.\\n\n## Decision\nChosen option: \"No components accept a `className`\", because...\\n- It's too risky to introduce style overrides to low-level layout components.\\n- Style customization should happen via props, where the component can be responsible for\\nstyle changes and variations.\\n### Positive Consequences <!-- optional -->\\n- Prop API consistency across all components.\\n### Negative Consequences <!-- optional -->\\n- Consumers of FDS components must handle the breaking change. Any custom `className`\\nprops on FDS components must be moved to a direct parent or direct child of the\\ncomponent.\\n\n\n## Problem\\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\\n(as they are designed to do so). Limitations of other storage plugins are\\ne.g., that not every structure of configuration is allowed.\\nSome of these limitations were documented `infos/status`, others were not.\\n\n## Decision\n", "response": "<pad> * It should not be limited by its convention. We may need to support two different actions of the same name,nbut with different business logic. E.g., one object may have two transitions with the same name but described byntwo different state machines.n* Solution should allow for an easy understanding of what is expected behavior of its execution.n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.nThe \"Taking advantage of the Controller REST arcetype\" should be considered as a recommended solution. All Syliusnusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should providenthe discoverability of the new endpoints. The possible operation may be sent in the Link headernor new schema should be introduced for the JSON-LD structure.nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing isndelivered with vanilla Sylius installation.n", "matched_ids": [674, 802], "time": "0:01:08.103902", "post": "## Decision\n* It should not be limited by its convention. We may need to support two different actions of the same name,nbut with different business logic. E.g., one object may have two transitions with the same name but described byntwo different state machines \n * Solution should allow for an easy understanding of what is expected behavior of its execution \n * Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm \n * Solution should provide a clean way of adding additional fields during the execution of requested operations \n The \"Taking advantage of the Controller REST arcetype\" should be considered as a recommended solution. All Syliusnusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should providenthe discoverability of the new endpoints. The possible operation may be sent in the Link headernor new schema should be introduced for the JSON-LD structure \n Option 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing isndelivered with vanilla Sylius installation \n "}
{"context": "## Context\\n[Command-query separation](https://martinfowler.com/bliki/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n", "input": "## Context and Problem Statement\\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`/`PUT` HTTP methods),\\nRead(`GET` HTTP method), Update(`PUT`/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\\nworkflow in a state machine) or command execution.\\n## Decision Drivers\\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\n\n## Decision\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\nThe \"Taking advantage of the `Controller` REST archetype\" should be considered as a recommended solution. All Sylius\\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\\nor new schema should be introduced for the JSON-LD structure.\\nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing is\\ndelivered with vanilla Sylius installation.\\n\n\n# ADR 0276: Problem Matchers\\n**Date** 2019-06-05\\n**Status** Accepted\\n## Context\\nCompilation failures during a CI build should surface good error messages.\\nFor example, the actual compile errors from the typescript compiler should bubble as issues in the UI. And not simply \"tsc exited with exit code 1\".\\nVSCode has an extensible model for solving this type of problem. VSCode allows users to configure which [problems matchers](https://code.visualstudio.com/docs/editor/tasks#_defining-a-problem-matcher) to use, when scanning output. For example, a user can apply the `tsc` problem matcher to receive a rich error output experience in VSCode, when compiling their typescript project.\\nThe problem-matcher concept fits well with \"setup\" actions. For example, the `setup-nodejs` action will download node.js, add it to the PATH, and register the `tsc` problem matcher. For the duration of the job, the `tsc` problem matcher will be applied against the output.\\n\n## Decision\n### Registration\\n#### Using `::` command\\n`::add-matcher::path-to-problem-matcher-config.json`\\nUsing a `::` command allows for flexibility:\\n- Ad hoc scripts can register problem matchers\\n- Allows problem matchers to be conditionally registered\\nNote, if a matcher with the same name is registered a second time, it will clobber the first instance.\\n\n\n## Context\\n[Command-query separation](https://martinfowler.com/bliki/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n\n## Decision\n", "response": "<pad> * Solution should not be limited by its convention. We may need to support two different actions of the same name,nbut with different business logic. E.g., one object may have two transitions with the same name but described byntwo different state machines.n* Solution should allow for an easy understanding of what is expected behavior of its execution.n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.n* Solution should provide a clean way of adding additional fields during the execution of requested operations.nThe \"Taking advantage of the Controller REST archetype\" should be considered as a recommended solution. All Syliusnusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should providenthe discoverability of the new endpoints. The possible operation may be sent in the Link headernor new schema should be introduced for the JSON-LD structure.nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing isndelivered with vanilla Sylius installation.n# ADR 0276: Problem Matchersn**Date** 2019-06-05n**Status** Acceptedn", "matched_ids": [674, 3734], "time": "0:01:15.400074", "post": "## Decision\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,nbut with different business logic. E.g., one object may have two transitions with the same name but described byntwo different state machines \n * Solution should allow for an easy understanding of what is expected behavior of its execution \n * Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm \n * Solution should provide a clean way of adding additional fields during the execution of requested operations \n The \"Taking advantage of the Controller REST archetype\" should be considered as a recommended solution. All Syliusnusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should providenthe discoverability of the new endpoints. The possible operation may be sent in the Link headernor new schema should be introduced for the JSON-LD structure \n Option 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing isndelivered with vanilla Sylius installation \n # ADR 0276: Problem Matchersn**Date** 2019-06-05n**Status** Acceptedn"}
{"context": "## Context\\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\\n", "input": "## Context\\nWhile most of cljdoc's publishing pipeline could be ran with\\n\"job-runners\" (e.g. CircleCI, AWS Lambda) the isolation provided by\\nthose isn't required for many parts of the process.\\nFurthermore some API to trigger documentation builds will be required\\nat some point and this API will need to hold some credentials, e.g.\\nAWS keys for S3 and CircleCI API tokens. Exposing access to job-runners\\ndirectly would make it harder to implement things like rate-limiting etc.\\nAn API may also be useful to expose further utility endpoints, like\\nways to list available versions or download the metadata for a given\\nnamespace or artifact.\\n\n## Decision\nWe will implement a server component which has the following responsibilties:\\n- be a central API to end users to request documentation builds\\n- hold credentials for various services which should not be exposed in analysis sandbox\\n- manage flow of documentation builds\\n- trigger analysis of jars\\n- receive webhooks about completed analysis\\n- populate Grimoire store using analysis results and a project's Git repository\\n- build HTML documentation based on data in Grimoire store\\n- deploy HTML documentation to S3/Cloudfront\\n\n\n## Context\\nOffer an API without a good documentation is not a choice. Document this API with text documents is not efficient, and as the code changes more and more effort are necessary to keep it up-to-date. It's necessary to document APIs in a simply and efficient way, preferably in a way that developers can test it.\\n\n## Decision\nI decided to use Swagger as a documentation tool for the APIs, as it is a de facto standard. I choosed springfox-swagger2 and springfox-swagger-ui because of its smooth integration with Spring Boot.\\n\n\n## Context\\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\\n\n## Decision\n", "response": "<pad> We will implement a server component which has the following responsibilties:n- be a central API to end users to request documentation buildsn- hold credentials for various services which should not be exposed in analysis sandboxn- manage flow of documentation buildsn- trigger analysis of jarsn- receive webhooks about completed analysisn- populate Grimoire store using analysis results and a project's Git repositoryn- build HTML documentation based on data in Grimoire storen- deploy HTML documentation to S3/Cloudfrontn", "matched_ids": [2605, 3069], "time": "0:00:42.782366", "post": "## Decision\nWe will implement a server component which has the following responsibilties:n- be a central API to end users to request documentation buildsn- hold credentials for various services which should not be exposed in analysis sandboxn- manage flow of documentation buildsn- trigger analysis of jarsn- receive webhooks about completed analysisn- populate Grimoire store using analysis results and a project's Git repositoryn- build HTML documentation based on data in Grimoire storen- deploy HTML documentation to S3/Cloudfrontn"}
{"context": "## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n", "input": "## Context\\nIt is required to create a demo app to show the component parts for DECODE\\nworking together in order to be able to prove that the system works. An\\nearlier version of this was implemented in Python using Flask and\\nFlask-socketio. This largely worked but was prone to weird socket failures,\\nand I wasn't very happy with maintaining or extending it (which may be\\nrequired).\\nThe finished service should be easy to deploy somewhere public (e.g. Heroku).\\nThe system should be amenable to being extended should we need to add any\\nmore functionality (e.g. to support logging into the dashboard).\\n\n## Decision\nWe will reimplement the demo service using Elixir/Phoenix rather than the\\nPython/Flask/Socketio version previously worked on.\\n\n\n## Context\\nI want to be able to mix and match different implementations\\nof the ddd sample application, without needing to pull in\\na large collection of unnecessary dependencies.\\nI want all of the code to be together in one place; which\\nis to say, I want to treat the entire project as a mono-repo.\\nI can't be bothered to maven install/maven deploy each\\nlittle piece to propagate the necessary changes between\\nisolated libraries.\\n\n## Decision\nUse a maven reactor project to track the dependencies between\\ndifferent libraries within the project\\n\n\n## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n\n## Decision\n", "response": "<pad> We will use a Gradle library to create the application.n", "matched_ids": [4000, 127], "time": "0:00:48.708988", "post": "## Decision\nWe will use a Gradle library to create the application \n "}
{"context": "## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n", "input": "## Context\\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\\nWe need to make a decision, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\\nFrom various discussions, we have a few findings that guide our decision:\\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\\n- We have two separate concerns that needs two separate configurations\\n\n## Decision\n- We will define one new CRD object to configure topics and access to this\\n- App developers will create this in the cluster when deploying their application\\n- Kafkarator will watch this CRD and take needed actions\\n- App developers will add configuration to their Application resource listing kafka pools they need access to\\n\n\n## Context\\n*This section describes the forces at play, including technological, political, social, and project local.\\nThese forces are probably in tension, and should be called out as such. The language in this section is value-neutral.\\nIt is simply describing facts.*\\nThe Office of Development Transformation (ODT) is focused on helping CAD learn and adopt lean, agile, and devOps techniques and\\nprocesses. ODT will be initialy focus on Green Field Developent. The Online Bank is a sample app used in the ODT University classes.  The Purpose of theapplication is to provide a\\nsample app as a context for the training courses.  We believe most people are familiar with online\\nbanking, and won't need to spend a lot of time learning the problem domain.\\nThe purpose of this document is to determine the technology stack for the sample application.\\n**NOTE:** While we are initially focused on a Java Web stack, we may elect to develop out an example in the Microsoft Stack.\\nIf you would like to help, please contact us.\\n\n## Decision\n*This section describes our response to these forces. It is stated in full sentences, with active voice.\\n\"We will ...\"*\\nWe will be focusing on the modern web java stack.\\n### UI Technology Stack:\\n- Angular 2\\n- Bootstrap\\n- protractor\\n- jasmine\\n### Server Side Technology Stack:\\n- Java 1.8+\\n- SpringBoot 1.4+\\n- Restful\\n- MySQL\\n- Spring Data JPA (Hibernate)\\n- JUnit 5 / Spock\\n### Development Technology Stack:\\n- Local Development\\n- SCM: GIT\\n- Intellij\\n- Favorite Editor\\n- Gradle\\n- Angular Quickstart\\n- Concourse CI (CI/CD)\\n- Jira\\n- Sonar\\n#### Deployment:\\n- Pivotal Cloud Foundry (Java and Static Build Packs)\\n\n\n## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n\n## Decision\n", "response": "<pad> * We will define one new CRD object to configure topics and access to thisn* App developers will create this in the cluster when deploying their applicationn* Kafkarator will watch this CRD and take needed actionsn* App developers will add configuration to their Application resource listing kafka pools they need access ton", "matched_ids": [2834, 1665], "time": "0:00:27.001141", "post": "## Decision\n* We will define one new CRD object to configure topics and access to thisn* App developers will create this in the cluster when deploying their applicationn* Kafkarator will watch this CRD and take needed actionsn* App developers will add configuration to their Application resource listing kafka pools they need access ton"}
{"context": "## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n", "input": "## Context\\nGOV.UK uses MongoDB as the backing database for a number of apps. In a generic\\nGOV.UK environment, there are 5 MongoDB clusters: one for Licensify (a legacy\\nservice), one for performance monitoring, one for router and another per cloud\\nprovider (i.e. AWS and Carrenza) for all other apps (e.g. assets, imminence,\\nmaslow, content, publisher and short url manager).\\nOne architectural strategy principle of TechOps is to make use as much as\\npossible AWS managed services in order to reduce the burden on GOV.UK teams to\\nmanage Commercial off-the-shelf (COTS) functionality/services.\\n\n## Decision\nThe approaches taken for migrating from MongoDB to AWS DocumentDB are:\\n1. new apps that are being migrated from legacy cloud providers (e.g UKCloud and\\nCarrenza) to AWS and use MongoDB should be evaluated for compatibility with\\nAWS DocumentDB during the migration phase. Based on this evaluation, a\\ndecision should be made based on cost benefit whether a particular migrating\\napp should be using DocumentDB or MongoDB.\\n2. there are some apps (e.g. content store and imminence) that have already been\\nmigrated to AWS and uses the MongoDB cluster there. These apps should be\\nevaluated for compatibility with AWS DocumentDB and prioritised accordingly\\nfor migration to AWS DocumentDB.\\n\n\n## Context\\nWe like to control our infrastructure with code. On a previous project we used shell scripts to set up the spaces and services within a PaaS organisation and to also do deploys from Travis.\\nWhile shell scripts are fine there is now a useful [terraform provider](https://github.com/cloudfoundry-community/terraform-provider-cf) for cloudfoundry.\\nThe cloudfoundry provider also deploys the app.\\nTerraform is the default choice for provisioning infrastructure these days.\\n\n## Decision\n- spaces and services should be created with Terraform\\n- the application should be deployed with Terraform\\n- Terraform should be run from Travis to deploy the application\\n\n\n## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n\n## Decision\n", "response": "<pad> The approach taken for migrating from MongoDB to AWS DocumentDB is:n1. new apps that are being migrated from legacy cloud providers (e.g. UKCloud andnCarrenza) to AWS and use MongoDB during the migration phase. Based this evaluation, andecision should be made based on cost benefit whether a particular migratingnapp should be using DocumentDB or MongoDB.n2. there are some apps (e.g. content store and imminence) that have already beennmigrated to AWS and uses the MongoDB cluster there. These apps should benevaluated for compatibility with AWS DocumentDB and prioritised accordinglynfor migration to AWS DocumentDB.n", "matched_ids": [4047, 2409], "time": "0:00:45.140917", "post": "## Decision\nThe approach taken for migrating from MongoDB to AWS DocumentDB is:n1. new apps that are being migrated from legacy cloud providers (e.g. UKCloud andnCarrenza) to AWS and use MongoDB during the migration phase. Based this evaluation, andecision should be made based on cost benefit whether a particular migratingnapp should be using DocumentDB or MongoDB \n 2. there are some apps (e.g. content store and imminence) that have already beennmigrated to AWS and uses the MongoDB cluster there. These apps should benevaluated for compatibility with AWS DocumentDB and prioritised accordinglynfor migration to AWS DocumentDB \n "}
{"context": "## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n", "input": "## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\n", "response": "<pad> The change that we're proposing or have agreed to implement.n", "matched_ids": [3544, 2025], "time": "0:00:20.291555", "post": "## Decision\nThe change that we're proposing or have agreed to implement \n "}
{"context": "## Context\\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\\n", "input": "## Context\\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\\nand the server side code (which will be a dropwizard app).\\nDepending on how the users want to run the service provider we may need\\ndifferent security solutions.\\n\n## Decision\nIf possible users can talk to the service provider on the loopback (127.0.0.1)\\nIf that doesn't work for some reason then they can use the dropwizard config\\nto set up basic auth or tls or something.\\nSee http://www.dropwizard.io/1.1.0/docs/manual/configuration.html#connectors\\n\n\n## Context\\nThe OpenFIDO project will mediate access to a workflow service that runs\\nGridLabD jobs. The frontend React application will need access to these\\nresources, but only if they have permission to access via an organization.\\n\n## Decision\nCreate a Flask Rest service - since the other microservices in this suite of\\nservices are also Flask based, keep this using the same kind of infrastructure\\nso that common utilities can be shared (openfido-utils) and the maintenance will\\nbe simplified.\\nOrganize the database logic into a simplified CQRS-inspired style code\\nstructure. Since we anticipate many conceptual resources, each resource will\\nhave its own module:\\n* app/RESOURCE/models.py - contains all models for RESOURCE.\\n* app/RESOURCE/routes.py - contains all Rest routes specific to RESOURCE.\\n* app/RESOURCE/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\\n* app/RESOURCE/services.py - all db commands that modify database state.\\n* app/RESOURCE/queries.py - all db queries to the database.\\nAdditional libraries we anticipate using:\\n* [marshmallow](https://marshmallow.readthedocs.io/en/stable/) will be used since there are many rest api endpoints that will take nested bodies.\\n* [alembic](https://alembic.sqlalchemy.org/en/latest/) to manage database schema migrations.\\nOther thoughts on design:\\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\\nUse the structure of the [openfido-workflow-service](https://github.com/slacgismo/openfido-workflow-service) project as a reference.\\n\n\n## Context\\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\\n\n## Decision\n", "response": "[ERROR]: An error occurred while generating decision. Please rate anything for this decision.HTTPSConnectionPool(host='b2w8w5mmlr342ulg.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=300)", "matched_ids": [578, 2470], "time": "0:05:00.793219", "post": "## Decision\n[ERROR]: An error occurred while generating decision. Please rate anything for this decision.HTTPSConnectionPool(host='b2w8w5mmlr342ulg.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=300)"}
{"context": "## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n", "input": "## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is to buy hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n\n## Decision\nKafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n\n\n## Context\\nThe Middleware team has strived to practice Infrastructure-As-Code, meaning DLP's AWS account should be reflected entirely within GitHub.\\nThis has mostly been accomplished but Ansible playbooks are currently run in Ansible Core on individual laptops of the Middleware team.\\nThe eventual goal should be GitOps, automated infrastructure changes triggered by a commit.\\nFor that purpose we will need a platform capable of running playbooks automatically.\\nThis platform would need to be able to run the playbooks on demand or scheduled.\\nRedHat provides two solutions: AWX and Ansible Tower.\\nAWX is the upline of Ansible Tower, meaning it is a more advanced version that lacks official support.\\n\n## Decision\nThe Middleware team will use AWX to manage DLP's infrastructure automation needs.\\nA different tool may be used for application automation.\\n\n\n## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n\n## Decision\n", "response": "<pad> Kafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.n", "matched_ids": [863, 1729], "time": "0:00:22.190903", "post": "## Decision\nKafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka \n "}
{"context": "## Context and Problem Statement\\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\\n", "input": "## Context\\nWe need a tool to provision servers and software for the datalabs project.\\n\n## Decision\nWe will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessella\\nteam have experience using it.\\n\n\n## Context\\nWe need to use a common library for managing HTML views within the service.\\nTwo libraries were investigated; Plates and Twig.\\n\n## Decision\nTo use Twig.\\n\n\n## Context and Problem Statement\\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\\n\n## Decision\n", "response": "<pad> We will use [Azure](https://azure.io/) as our STT service.n", "matched_ids": [771, 4838], "time": "0:00:22.506882", "post": "## Decision\nWe will use [Azure](https://azure.io/) as our STT service \n "}
{"context": "## Context\\nPIMS requires a database to store all property information.\\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\\nAdditionally it must be supported by Entity Framework Core 3.1.\\n", "input": "## Context\\nThe build processes for all of the PACE projects require credentials to access GitHub and the SAN area.\\nThese need to be stored securely and be accessible to steps in the Jenkinsfile and any scripts launched from that.\\nFor security reasons this data cannot be stored in the build or pipeline scripts as these are stored in GitHub.\\n\n## Decision\nThe data will be stored in ANVIL through the available Jenkins [Credentials](https://plugins.jenkins.io/credentials/) plugin.\\nA `file` object will be used for the SAN credentials as this maps directly onto the format required by be `gio mount` syntax on Linux build nodes.\\nA `string` object will be used for other credentials types, e.g. GitHub access tokens.\\nCredentials will be stored in the `PACE-neutrons`  store and use `snake_case` IDs that clearly identify their use. The first word will identify the associated system, e.g. `SAN_credentials_file`.\\n\n\n## Context\\n`stentor` needs to pick a config file format.\\nThe options under consideration are yaml and toml.\\nBoth options are human readable and writable,\\nallow for easy parsing of structured data,\\nand are supported by well maintained libraries.\\nyaml has the benefit of being more straightforward to write,\\nespecially for nested structures.\\nHowever, toml is intended for config files,\\nand provides stricter parsing out of the box.\\n\n## Decision\n`stentor` will use toml for its config file.\\n\n\n## Context\\nPIMS requires a database to store all property information.\\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\\nAdditionally it must be supported by Entity Framework Core 3.1.\\n\n## Decision\n", "response": "<pad> The PIMS database will be built using the [ADR](https://github.com/ADR-008/adr-008-api-database)nplatform.n", "matched_ids": [4239, 2164], "time": "0:00:25.892776", "post": "## Decision\nThe PIMS database will be built using the [ADR](https://github.com/ADR-008/adr-008-api-database)nplatform \n "}
{"context": "## Context and Problem Statement\\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\\ninterfaces\" - possibly leading to the impression that there is such a thing as\\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\\nthe only question is who manages this thread-safety. Interfaces which are not\\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\\nhidden in the generated code and therefore not obvious to the casual reader.\\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\\nmanaging their own locking internally. This ADR proposes that uniffi forces\\ncomponent authors to explicitly manage that locking in all cases - or to put\\nthis in Rust terms, that all structs supported by uniffi must already be\\n`Send+Sync`\\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\\nany misunderstandings that might be caused by using the somewhat broad and\\ngeneric \"Threadsafe\".\\n## Decision Drivers\\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https://github.com/mozilla-mobile/fenix/issues/17086)\\n(with more details available in [this JIRA ticket](https://jira.mozilla.com/browse/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https://github.com/mozilla/uniffi-rs/issues/417) or to\\n[pass interface references over the FFI](https://github.com/mozilla/uniffi-rs/issues/419).\\n", "input": "## Context\\nThe Wikidata Bridge app uses VueJs version 2 and Vuex version 3 for state management.\\nBoth within the store and in vue components the calls to `dispatch()` and `commit()` are not type safe.\\nThat means that they accept any arguments and TypeScript will still compile without error.\\nSince `dispatch()` and `commit()` are typical seams that are usually mocked during unit testing,\\nit is up to integration, end-to-end and browser tests to detect these errors.\\nThis is particularly unfortunate as the store is one of the central locations where business logic happens.\\nWe considered two options:\\n1. writing our own set of wrappers for `dispatch()` and `commit()` to get type safety\\n1. using vuex-smart-modules\\nThe advantages of doing it ourselves included us not having another dependency.\\nThe risks include that this would be yet another homebrew layer of abstraction.\\nThe advantages of using vuex-smart-modules include:\\n- it gives us proper native type safety both in the store and in components\\n- we can call actions/getters/mutations with [method style access](https://github.com/ktsn/vuex-smart-module#method-style-access-for-actions-and-mutations)\\n- we can get rid of all the `BRIDGE_SET_TARGET_VALUE` constants without having to fall back to string literals\\n- we can actually use the IDE's `go to method definition` functionality\\n- we can rely on the action's return type instead of dispatch's `Promise<any>` being used everywhere\\n- we can drop the `vuex-class` dependency as we can use vuex-smart-modules for all store access in components\\n- it is developed by a VueJs core contributor\\nThe risks include:\\n- it is still a very new project with a 0.x.y version number\\n- it is another layer on top of vuex, which means the documentation may not be as good as it could be\\n- we are the first big project to use it\\n- mocking of dependencies and nested modules in testing still seems to be not handled as diligently as one would wish\\n\n## Decision\nWe decided to rewrite our Vuex store using vuex-smart-modules version 0.3.4\\n\n\n## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https://www.w3.org/WAI/WCAG21/Understanding/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https://github.com/hmrc/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n\n## Decision\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,\\nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would be\\nto revisit option 7 (play-session) at a later date in order to address any outstanding concerns\\naround knowledge duplication.\\n\n\n## Context and Problem Statement\\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\\ninterfaces\" - possibly leading to the impression that there is such a thing as\\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\\nthe only question is who manages this thread-safety. Interfaces which are not\\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\\nhidden in the generated code and therefore not obvious to the casual reader.\\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\\nmanaging their own locking internally. This ADR proposes that uniffi forces\\ncomponent authors to explicitly manage that locking in all cases - or to put\\nthis in Rust terms, that all structs supported by uniffi must already be\\n`Send+Sync`\\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\\nany misunderstandings that might be caused by using the somewhat broad and\\ngeneric \"Threadsafe\".\\n## Decision Drivers\\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https://github.com/mozilla-mobile/fenix/issues/17086)\\n(with more details available in [this JIRA ticket](https://jira.mozilla.com/browse/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https://github.com/mozilla/uniffi-rs/issues/417) or to\\n[pass interface references over the FFI](https://github.com/mozilla/uniffi-rs/issues/419).\\n\n## Decision\n", "response": "<pad> * The need to minimise code changes for service teams other than a library upgrade.n* The avoidance of requiring service teams to add any additional routing rules.n* The avoidance of requiring service teams to add any additional configuration.n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play andnany other library or service.n* The preference for maintaining loose coupling of frontend microservices and avoiding adding single pointsnof failure between frontend microservices.n* The need for the endpoint used to interrogate the session to not itself affect the session.nChosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would bento revisit option 7 (play-session) at a later date in order to address any outstanding concernsnaround knowledge duplication.n", "matched_ids": [1345, 560], "time": "0:00:55.906789", "post": "## Decision\n* The need to minimise code changes for service teams other than a library upgrade \n * The avoidance of requiring service teams to add any additional routing rules \n * The avoidance of requiring service teams to add any additional configuration \n * The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play andnany other library or service \n * The preference for maintaining loose coupling of frontend microservices and avoiding adding single pointsnof failure between frontend microservices \n * The need for the endpoint used to interrogate the session to not itself affect the session \n Chosen option: option 9, because it is an option PlatUI and PlatOps agree on, is technically feasible,nsatisfies most of the decision drivers and is the smallest possible change with the least impact to service teams. The intention would bento revisit option 7 (play-session) at a later date in order to address any outstanding concernsnaround knowledge duplication \n "}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "input": "## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard\\nin this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n", "response": "<pad> We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).n", "matched_ids": [494, 1269], "time": "0:00:23.599212", "post": "## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) \n "}
{"context": "## Context and Problem Statement\\nWe found that without a standardised format our javascript files ended up with different\\nformats in different files or even multiple formats in the same file.  We also found that\\nour IDEs had different configurations which meant that using an autoformat tool would give\\ndifferent results when each of us do it.\\n## Decision Drivers\\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\n", "input": "## Context\\nHistorically we have had \"two ways\" of configuring Verify Service Provider:\\n- Using environment variables\\n- Using a YAML file\\nWhen using environment variables the application used the verify-service-provider-env.yml\\nfile from the resources directory (so inside the jar). When using the YAML file you would\\npass the path to a different file as a command line parameter - usually people\\nwould use the example one that's contained in the repo.\\nThere were a couple of reasons for the extra complexity of managing two files, both due to\\nrestrictions with the java buildpack used by cloudfoundry:\\n- It's not possible to specify command line arguments through the java buildpack,\\nso you can't specify a path to your config file\\n- We weren't confident in the way cloudfoundry manages static files, so we didn't want\\nto rely on one.\\nThere was also a philosophical point that 12 factor applications should be configured through\\ntheir environment. This made the \"hide the configuration in the .jar and do everything through\\nenv vars\" way appealing.\\n\n## Decision\nWe will remove the verify-service-provider-env.yml file from src/main/resources\\nThe application will default to the verify-service-provider.yml\\nfile that's included in the .zip if no command line arguments are provided.\\nIf the application is started without command line arguments specifying a yml file\\nAND no environment variables have been set, startup should error gracefully and tell\\nthe user that the configuration fields have not been specified for example:\\n\"ERROR - no configuration fields found, either set environment variables or specify\\na configuration file using command line arguments ```server <path/to/verify-service-provider.yml>```\"\\nWe will establish the path to verify-service-provider.yml by asking java for the\\npath to the .jar file containing the Application class and looking in the parent\\nfolder.\\n\n\n## Context\\nIn [adr-008][1] we agreed on focusing only on English pages.\\nWe have noticed during the last few months that this restriction is causing more\\nissues than benefits, because:\\n- We have Content Items with the same `content_id` and different `locale`s.\\n- We needed to handle edge cases when retrieving the information from Publishing API in order to work around only retrieving English language content.\\n\n## Decision\nTrack Content metrics of all Content Item regardless of their locale.\\n\n\n## Context and Problem Statement\\nWe found that without a standardised format our javascript files ended up with different\\nformats in different files or even multiple formats in the same file.  We also found that\\nour IDEs had different configurations which meant that using an autoformat tool would give\\ndifferent results when each of us do it.\\n## Decision Drivers\\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\n\n## Decision\n", "response": "<pad> We will remove the verify-service-provider-env.yml file from src/main/resourcesnThe application will default to the verify-service-provider.yml file that's included in the.zip if no command line arguments are provided.nIf the application is started without command line arguments specifying a yml file, startup should error gracefully and tellnthe user that the configuration fields have not been specified for example:n\"ERROR - no configuration fields found, either set environment variables or specifyna configuration file using command line arguments server path/to/verify-service-provider.yml>\"\"nWe will establish the path to verify-service-provider.yml by asking java for thenpath to the.jar file containing the Application class and looking in the parentnfolder.n", "matched_ids": [4448, 1862], "time": "0:01:01.906915", "post": "## Decision\nWe will remove the verify-service-provider-env.yml file from src/main/resourcesnThe application will default to the verify-service-provider.yml file that's included in the.zip if no command line arguments are provided \n If the application is started without command line arguments specifying a yml file, startup should error gracefully and tellnthe user that the configuration fields have not been specified for example:n\"ERROR - no configuration fields found, either set environment variables or specifyna configuration file using command line arguments server path/to/verify-service-provider.yml>\"\"nWe will establish the path to verify-service-provider.yml by asking java for thenpath to the.jar file containing the Application class and looking in the parentnfolder \n "}
{"context": "## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n", "input": "# Context and Problem Statement\\nUsers want to add custom metadata to items. For example, for invoices\\nfields like `invoice-number` or `total`/`amount` make sense. When\\nusing a pagination stamp, every item gets a pagination number.\\nThis is currently not possible to realize in docspell. But it is an\\nessential part when organizing and archiving documents. It should be\\nsupported.\\n# Considered Options\\n\n## Decision\n- values are strings at the database\\n- values are strings when transported from/to server\\n- client must provide the correct formatted strings per type\\n- numeric: some decimal number\\n- money: decimal number\\n- text: no restrictions\\n- date: a local date as iso string, e.g. `2011-10-09`\\n- bool: either `\"true\"` or `\"false\"`, case insensitive\\n\n\n## Context and Problem Statement\\nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\\ndefaults that differ between schematic types.\\nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files\\nand directories.\\n## Decision Drivers\\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\n\n## Decision\n* Provide consistency when navigating the codebase\\n* The earlier we decide on this, the better\\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\\nNaming directories should follow these guidelines:\\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\\n`import { Box } from '@island.is/island-ui/core'`\\nUse PascalCase for directories only containing React components:\\n````\\ncomponents/CtaButton/index.ts\\nimport 'components/CtaButton'\\n````\\nor:\\n````\\ncomponents/CtaButton/CtaButton.tsx\\nimport 'components/CtaButton/CtaButton'\\n````\\nrather than\\n````\\ncomponents/cta-button/CtaButton.tsx\\n````\\nIn all other cases, use camelCase.\\n### Positive Consequences\\n* Easier to navigate the codebase\\n* File names are more readable, and developers know what to expect\\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\\n\n\n## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n\n## Decision\n", "response": "<pad> * Provide consistency when navigating the codebasen* The earlier we decide on this, the betternChosen option: Name files after their default export. If that default export is a React Component, or a class, then thenfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoidnusing kebab-case and snake_case and make sure the name follows the default export of the file.nNaming directories should follow these guidelines:nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. island-ui instead of island-ui/core'nimport  Box  from '@island.is/island-ui/core'nUse PascalCase for directories only containing React components:nncomponents/CtaButton/index.tsnimport 'components/CtaButton'nor:nncomponents/CtaButton/CtaButton'nor:nncomponents/CtaButton/CtaButton'nrather thannncomponents/cta-button/CtaButton'nnIn all other cases, use camelCase.n#### Positive Consequencesn* Easier to navigate the codebasen* File names are more readable, and developers know what to expectn* This approach is the most common practice, and something most JS and TS developers are familiar with.n", "matched_ids": [285, 4766], "time": "0:01:32.003042", "post": "## Decision\n* Provide consistency when navigating the codebasen* The earlier we decide on this, the betternChosen option: Name files after their default export. If that default export is a React Component, or a class, then thenfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoidnusing kebab-case and snake_case and make sure the name follows the default export of the file \n Naming directories should follow these guidelines:nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. island-ui instead of island-ui/core'nimport  Box  from '@island.is/island-ui/core'nUse PascalCase for directories only containing React components:nncomponents/CtaButton/index.tsnimport 'components/CtaButton'nor:nncomponents/CtaButton/CtaButton'nor:nncomponents/CtaButton/CtaButton'nrather thannncomponents/cta-button/CtaButton'nnIn all other cases, use camelCase \n #### Positive Consequencesn* Easier to navigate the codebasen* File names are more readable, and developers know what to expectn* This approach is the most common practice, and something most JS and TS developers are familiar with \n "}
{"context": "## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n", "input": "## Context\\nThis code generation experiment is meant to be our starting point for both the API unification for ND4J and SameDiff,\\nand the multi-language support. For this reason we have to define ops, or their interface, in a language neutral way.\\nThe initial idea was to use a Language Workbench like MPS. This had to be discarded because of bugs and limitations\\nencountered while trying to define a language that would work for a few simple examples.\\nThe next idea was to use Ops defined in JSON files. This would have allowed us to define Ops as human readable data and\\nread and write those files from any programming language. However, the drawback with this approach is that writing json\\nmanually invites many problems if written manually (e.g. typos, bad structuring, having to look up the proper keys,...).\\nIn order to rectify that drawback, we would have to create custom tooling, that we would have to maintain and that\\ncontributors would have to use.\\nUsing a Java builder pattern based approach is very verbose.\\n\n## Decision\nWe use a Kotlin-based DSL to define Ops.\\n\n\n## Context\\nIn selecting a static asset pipeline, there are several factors that are important to us.\\nWe don\u2019t want to lose features that speed up developer feedback, such as a dev server with linting, auto-gen, and browser reload.\\nThe solution needs to support tests run with a single command similar to what we\u2019ve had in previous projects.\\nThe solution needs to be fast.\\nSome team members have expressed dissatisfaction with the stability of the NPM ecosystem, but pretty much everything relies on it to some extent, so the issue is probably best addressed elsewhere (possibly using a lazy NPM cache).\\nIn the past, our team has used Grunt to manage and build static assets. Various team members / designers have expressed discontent with the idea of going forward using Grunt, primarily due to inherent factors such as long compile times and configuration complexity.\\nOther teams in Azavea have had good experiences with Webpack. Webpack is not a complete replacement for Grunt, but of the difference in functionality, it is highly likely that NPM tasks will cover it if used in conjunction. Despite scattered complaints of complexity and poor documentation found while researching, I was able to set up pipeline using npm and webpack overnight without too many issues. Significant features which are built in include a dev server capable of supporting hot reloading, very fast feedback on changes, and support for all the tools we commonly use in our applications. In addition to quality of life features, Webpack also gives us a dependency graph which includes static assets and bundle splitting. Combined with NPM, it appears to be a good solution.\\nA final option that we have to consider is Gulp, which has been used on a couple of projects by the Urban Apps team. Where grunt uses configuration files, Gulp uses code. Gulp\u2019s main advantage is speed: streaming operations mean that files never touch disk between chained tasks. Other than the method of configuration and speed of operating, the basic functionality of Gulp and Grunt seem to be fairly aligned.\\n\n## Decision\nGiven the good experiences that others in the company have had with Webpack, we\u2019ve decided to run the static asset pipeline using a combination of npm and webpack. Npm will be used to manage dependencies and initiating tasks, while webpack will be used to run the dev server and build static assets.\\n\n\n## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n\n## Decision\n", "response": "<pad> We will use a Kotlin-based DSL to define Ops.n", "matched_ids": [2939, 1680], "time": "0:00:21.383581", "post": "## Decision\nWe will use a Kotlin-based DSL to define Ops \n "}
{"context": "## Context and Problem Statement\\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\\nWhich query syntax should be used for the abstract queries?\\nWhich features should the syntax support?\\n", "input": "## Context and Problem Statement\\nTo build custom dashboards we need to define queries that are too complex for the existing filter box form.\\nWe would like to use flexible search expressions based on a proper query language.\\n\n## Decision\nChosen option: \"Monocle Query Language\", because it comes out best (see below).\\n### Positive Consequences\\n- We improve the user experience by replacing the clunky filter box with a simpler search bar.\\n- We create a re-usable component.\\n### Negative Consequences\\n- We need to maintain a language toolchain.\\n\n\n## Context\\nThe existing print holdings system keeps all data in MySQL. While this allows\\nflexible querying, it is also computationally expensive and is difficult to\\nscale.\\nKnown use cases largely involve computing things specific to a clusters'\\nholdings and HathiTrust items, so one way to parallelize queries is in a\\nmap-reduce fashion - compute something about each cluster, then process the\\ndata from each cluster into a final result.\\n\n## Decision\nWe will use MongoDB for a persistent data store. Each print holdings cluster\\nwill be a document, and will contain holdings, HathiTrust items, and shared\\nprint commitments for that cluster as sub-documents.\\n\n\n## Context and Problem Statement\\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\\nWhich query syntax should be used for the abstract queries?\\nWhich features should the syntax support?\\n\n## Decision\n", "response": "<pad> Chosen option: \"Application-specific query syntax\", because it comes out best (see below).n### Positive Consequencesn- We improve the user experience by replacing the clunky filter box with a simpler search bar.n- We create a re-usable component.n### Negative Consequencesn- We need to maintain a language toolchain.n", "matched_ids": [4637, 4197], "time": "0:00:33.702359", "post": "## Decision\nChosen option: \"Application-specific query syntax\", because it comes out best (see below) \n ### Positive Consequencesn- We improve the user experience by replacing the clunky filter box with a simpler search bar \n - We create a re-usable component \n ### Negative Consequencesn- We need to maintain a language toolchain \n "}
{"context": "## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\\nGovUK services have a \"trial mode\", and a \"live mode\".\\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\\n", "input": "## Context\\nOn our platform, application instances receive traffic routed from a load balancer. Typically, an application instance runs Nginx\\nwith a server_name based Vhost per application. Each application Vhost proxies the requests to the local port\\nwhere the application listens.\\nLoad balancers use a health check to determine which instances to send traffic to. Health checks depend on the type of load\\nbalancer and protocol in use. Usually we are going to configure health checks using TCP+PORT (ELBs) or HTTP+PORT+Request_path+Response_code\\n(ALB). For more information on health checks, have a look at the following links:\\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html\\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\\nWhen we configure HTTP health checks, the load balancer sends a request without a Host header. In this case, Nginx processes that\\nrequest in the default Vhost. These requests have been configured in the default Vhost to serve an `200 OK` response code, which indicates to the\\nload balancer that the instance is healthy and ready to receive traffic. This presents a problem when we are starting or redeploying\\napplications in that instance, because the application port will be unavailable for a period of time, and Nginx will serve 5xx responses. The\\nload balancer still sees the health check responses as `200 OK`, so it continues sending traffic to that instance, even though the application\\nis not ready to receive traffic.\\nThe current ALB health check is rudimentary and does only test for the basic Nginx configuration being in place, rather than testing whether\\nall apps are properly deployed.  If an instance is terminated, this causes its replacement to be added to the target group pool prematurely which\\ncauses elevated 5xx rates.\\n\n## Decision\nWhen an instance is running more than one application, we are going to route traffic to the instance with an ALB. Each application has a dedicated\\ntarget group, with a specific health check path, following the format `/_healthcheck_<app-name>`.\\nThis health check path is configured in the default Vhost to proxy the request to the upstream application, and rewrites the request to match the\\napplication health check path that is also used in Icinga checks. This is configured in Puppet, in `govuk::app::config`, as the `health_check_path`\\nparameter.\\nThe ALB includes routing rules based on Host header, and redirects the traffic to the application target group when the Host header matches\\nthe value `<<app-name>.*`\\n\n\n## Context and Problem Statement\\nNearly all services running on MDTP need to include a link to their landing page in the\\nGOV.UK header. Exceptionally, frontend microservices may either need to omit the service name\\nor be able to configure it based on the path. For example, in the case of shared frontend\\nmicroservices (contact-frontend, accessibility-statement-frontend) or microservices that host\\nmore than one public-facing service (view-external-guidance-frontend, gform-frontend).\\nWhen integrating with play-frontend-hmrc we want to\\nminimise boilerplate for service teams while providing flexibility to cope with edge\\ncases.\\nServices using the govukLayout component in play-frontend-govuk and not overriding the `headerBlock`\\nparameter, specify the service name in the\\n`service.name` message in `conf/messages`, providing any Welsh translation in `conf/messages.cy`.\\nBased on a Github search, most services overriding the headerBlock with, for example, `hmrcHeader`\\nor `govukHeader` are passing the service name from `messages('service.name')`. However, this\\nmechanism is awkward for services not requiring a service name \u2013 they have to override with a blank message \u2013\\nand services needing more than one are unable to use this functionality. This pattern\\nassumes a 1:1 correspondence between frontend microservices and public-facing\\ntax services.\\nShould we continue to support this pattern to reduce boilerplate for the majority of frontend\\nservices, insist on explicitly passing the service name or support some combination of the above?\\n## Decision Drivers\\n* Minimising of boilerplate and lines of code needed in frontend microservices\\n* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with\\npublic-facing services.\\n* The need to keep things simple and unsurprising\\n\n## Decision\n* Minimising of boilerplate and lines of code needed in frontend microservices\\n* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with\\npublic-facing services.\\n* The need to keep things simple and unsurprising\\nChosen option 2 because reducing boilerplate for the majority of services is important, we should support\\nmore unusual services and not dictate an architecture that assumes a 1:1 correspondence\\nbetween frontend microservices and public-facing services.\\n\n\n## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\\nGovUK services have a \"trial mode\", and a \"live mode\".\\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\\n\n## Decision\n", "response": "<pad> * Minimising boilerplate and lines of code needed in frontend microservicesn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing services.n* The need to keep things simple and unsurprisingnChosen option 2 because reducing boilerplate for the majority of services is important, we should supportnmore unusual services and not dictate an architecture that assumes a 1:1 correspondancenbetween frontend microservices and public-facing services.n", "matched_ids": [4036, 559], "time": "0:00:35.000419", "post": "## Decision\n* Minimising boilerplate and lines of code needed in frontend microservicesn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with public-facing services \n * The need to keep things simple and unsurprisingnChosen option 2 because reducing boilerplate for the majority of services is important, we should supportnmore unusual services and not dictate an architecture that assumes a 1:1 correspondancenbetween frontend microservices and public-facing services \n "}
{"context": "## Context\\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\\n", "input": "## Context\\nEnsure good code quality with ease for change, integration and error correction.\\n\n## Decision\nIt will be used the [PHPUnit](https://phpunit.de/) as testing framework.\\n\n\n## Context\\nTo ensure the RODA application has the highest availability, we want\\nto have confidence that the application is resourced properly in times of\\nlow and high usage.\\n\n## Decision\ndxw have used the Skylight gem in many other projects and there is a Skylight\\naccount available for use. We have prior knowledge of using Skylight in\\nother projects.\\nSkylight provides a web interface and graphing so we can more easily identify\\npain points in our application which will cause it to become less performant\\nunder high load. It can identify slow-running parts of the service down to the\\ncontroller action level, and indicate where - for example - excessive database\\ncalls or slow queries are causing degradation in the user's experience.\\nCommon alternatives to Skylight are New Relic and DataDog. We do have some\\nexperience with DataDog at dxw, but we had more confidence within the team with\\nusing Skylight.\\n\n\n## Context\\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\\n\n## Decision\n", "response": "<pad> We will use the [PHPUnit](https://phpunit.de/) as testing framework.n", "matched_ids": [5070, 2391], "time": "0:00:18.905631", "post": "## Decision\nWe will use the [PHPUnit](https://phpunit.de/) as testing framework \n "}
{"context": "## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https://stylelint.io/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n", "input": "## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https://eslint.org/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n\n## Decision\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n\n\n## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https://eslint.org/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n\n## Decision\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n\n\n## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https://stylelint.io/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n\n## Decision\n", "response": "<pad> We will check code style using ESLint.nWe will let Prettier have precedence when ESLint and Prettier conflict in theirnstyles.nWe will use the recommended configuration for plugins where possible.nWe will run ESLint as part of the test suite.n", "matched_ids": [1404, 4224], "time": "0:00:29.190100", "post": "## Decision\nWe will check code style using ESLint \n We will let Prettier have precedence when ESLint and Prettier conflict in theirnstyles \n We will use the recommended configuration for plugins where possible \n We will run ESLint as part of the test suite \n "}
{"context": "## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n", "input": "## Context\\nWe need a convenient system for managing API keys used by the Python client. This system should give the user multiple options for\\nproviding an API key to be used when making a request to the API. These options should include:\\n* Storing API keys on the users system\\n* Reading an API key from the environment\\n* Passing an API key directly to the API request methods\\nUsers may have multiple valid API keys associated with their account at any given time. The system for storing API keys on the user's\\nsystem must accommodate this and provide a clear, deterministic way of resolving an API key for a given project.\\nWe anticipate the need to store other data related to Radiant MLHub for uses unrelated to authentication. For instance, we may have a need to\\ntrack the progress of downloads so that they can be resumed if interrupted, or we may want to specify a base URL in a config file so that\\ndevelopers can test against the staging environment. The method that we choose for storing API keys on the user's system must not preclude\\nus from storing this additional information.\\n\n## Decision\nThe Python client will resolve the API key to be used in a request in the following order:\\n1) Passing an `api_key` argument directly to the method\\n2) Setting an `MLHUB_API_KEY` environment variable\\n3) Passing a `profile` argument directly to the method. This will read the API key from the given profile (see below for details)\\n4) Setting an `MLHUB_PROFILE` environment variable. This will read the API key from the given profile (see below for details)\\n5) Using the API from the `default` profile\\nProfiles will be stored in a `.mlhub/profiles` file in the user's home directory. This file will be an INI file containing at least a\\n`[default]` section with an `api_key` value. The file may contain other sections corresponding to named profiles. Any `profile` argument\\npassed to a method must correspond to one of these section names, or it will raise an exception.\\n\n\n## Context\\nOn PFE, there are a couple of different ways workflows are routed:\\n- Navigating to `/classify`, the user sees either the default workflow for that project, a random active workflow, or the workflow stored in project preferences for logged-in volunteers.\\n- Navigating to `/classify?workflow=[workflow_id]` shows the workflow determined by the query parameter when the \"Use workflow query\" feature is enabled for the project, or if the user has the correct role (owner, collaborator, tester, or admin).\\nThis is not optimal for several reasons. Users cannot bookmark specific workflows unless they use the query parameter; there can be uncertainty over which actual workflow is being shown (particularly as the workflow name isn't shown in the UI); different workflows can be shown at the same URL at different times; and ultimately, workflows are static resources that should be routed to - that's what URLs are for.\\nAdditionally, [we currently have a requirement for CSSI CitSci to be able to route to specific subjects](https://github.com/zooniverse/front-end-monorepo/issues/806#issuecomment-495685027). It's conceivable that a project will have a requirement for routing to a specific subject set linked to a workflow as well.\\n\n## Decision\nWe adopt the following URL structure:\\n```\\n/projects/[owner]/[project]/classify/workflow/:workflow-id/subject-set/:subject-set-id/subject/:subject-id\\n```\\nThis would be facilitated by the [dynamic routing feature in Next.js 9](https://github.com/zeit/next.js/#dynamic-routing) (see [#1071](https://github.com/zooniverse/front-end-monorepo/pull/1071)).\\nWorkflow links from the home page would route directly to their workflow-specific URLs.\\n### `/classify` behaviour\\nWhen navigating to `/classify`, the page should redirect the user to the correct workflow URL in the following priority:\\n1. Workflow ID set in user project preferences by the user\\n1. Workflow ID set in user project preferences by the project\\n1. Project default workflow\\n1. Random active selection\\nThe new URL that we redirect the user to should replace the entry for `/classify` in the browser history.\\n### Error handling\\nWhen a user is unable to view a workflow at a given URL, we should provide an error page at that URL. Next.js allows you to provide a response code in `getInitialProps`, which we can use to tailor the message we show on a custom error component, so for non-existent workflows, `404`; incorrect permissions get a `401`/`403` (depending on login status) etc.\\n\n\n## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n\n## Decision\n", "response": "<pad> We adopt the following URL structure:nn/projects/[owner]/[project]/classify/workflow/:workflow-id/subject-set/:subject-set-id/subject/:subject-idnnThis would be facilitated by the [dynamic routing feature in Next.js 9](https://github.com/zeit/next.js/#dynamic-routing) (see [#1071](https://github.com/zooniverse/front-end-monorepo/pull/1071)).nWorkflow links from the home page should route directly to their workflow-specific URLs.n### classify behaviournWhen navigating to classify, the page should redirect the user to the correct workflow URL in the following priority:n1. Workflow ID set in user project preferences by the usern1. Workflow ID set in user project preferences by the projectn1. Project default workflown1. Random active selectionnThe new URL that we redirect the user to should replace the entry for /classify in the browser history.n### Error handlingnWhen a user is unable to view a workflow at a given URL, we should provide an error page at that URL. Next.js allows you to provide a response code in getInitialProps, which we can use to tailor the message we show on a custom error component, so for non-existent workflows, 404; incorrect permissions get a 401/403 (depending on login status) etc.n", "matched_ids": [702, 507], "time": "0:01:12.312713", "post": "## Decision\nWe adopt the following URL structure:nn/projects/[owner]/[project]/classify/workflow/:workflow-id/subject-set/:subject-set-id/subject/:subject-idnnThis would be facilitated by the [dynamic routing feature in Next.js 9](https://github.com/zeit/next.js/#dynamic-routing) (see [#1071](https://github.com/zooniverse/front-end-monorepo/pull/1071)) \n Workflow links from the home page should route directly to their workflow-specific URLs \n ### classify behaviournWhen navigating to classify, the page should redirect the user to the correct workflow URL in the following priority:n1. Workflow ID set in user project preferences by the usern1. Workflow ID set in user project preferences by the projectn1. Project default workflown1. Random active selectionnThe new URL that we redirect the user to should replace the entry for /classify in the browser history \n ### Error handlingnWhen a user is unable to view a workflow at a given URL, we should provide an error page at that URL. Next.js allows you to provide a response code in getInitialProps, which we can use to tailor the message we show on a custom error component, so for non-existent workflows, 404; incorrect permissions get a 401/403 (depending on login status) etc \n "}
{"context": "## Context\\nThe Data Migrate gem has caused us a number of issues in the past, it runs\\nsilently as part of a deploy, and this can result in surprising errors\\nduring a deploy. We've also had issues with the gem itself - most recently\\na bug in a new version causing strange errors in deployment.\\n", "input": "## Context\\ncljdoc's deployment story has been simplistic but effective. To recap:\\n- During CI a zip file is pushed to S3 that contains all files to run the application\\n- On the live server there is systemd service that will download an archive and run it. The\\nversion of the downloaded archive is specified via a file on the server.\\nUpdating simply required updating a file on the server and restarting the service.\\nThe issue with this approach however is that every time a new release was pushed to the server\\nthe restart of the systemd service would incur up to a minute of downtime. While this generally\\nisn't a huge deal it discourages certain development practices that may be desirable such as\\nContinuous Deployment.\\nOur existing deployment setup (and tools used by it) are poorly equipped to handle this kind of\\ndeployment scenario. A large amount of bash scripts would be required to start a new cljdoc server,\\nwait for it to become available, update nginx's upstream port, and kill old cljdoc server instances\\nin a repeatable, automated manner. Likely these bash scripts would be error-prone and turn into\\nsomething nobody likes to touch.\\n\n## Decision\nImplement a *canary deploy* mechanism for the cljdoc server application using\\n[Nomad](https://nomadproject.io) and [Traefik](https://traefik.io).\\nWhile both of these tools are probably aimed at much more complex workloads they provide the\\nfollowing benefits over the existing systemd/Nginx setup:\\n- Automatic SSL certificates via Lets Encrypt\\n- Declarative specification of jobs and their desired update semantics\\n- APIs to schedule new jobs and cycle old/new deployments\\n- Health checks to verify new deployments work as expected\\n- Machine images become much simpler since they only need Nomad, Consul, Docker\\nThis simplifies a lot of cljdoc's operational tasks while also enabling Continuous Deployment.\\n\n\n## Context\\nIt is required to create a demo app to show the component parts for DECODE\\nworking together in order to be able to prove that the system works. An\\nearlier version of this was implemented in Python using Flask and\\nFlask-socketio. This largely worked but was prone to weird socket failures,\\nand I wasn't very happy with maintaining or extending it (which may be\\nrequired).\\nThe finished service should be easy to deploy somewhere public (e.g. Heroku).\\nThe system should be amenable to being extended should we need to add any\\nmore functionality (e.g. to support logging into the dashboard).\\n\n## Decision\nWe will reimplement the demo service using Elixir/Phoenix rather than the\\nPython/Flask/Socketio version previously worked on.\\n\n\n## Context\\nThe Data Migrate gem has caused us a number of issues in the past, it runs\\nsilently as part of a deploy, and this can result in surprising errors\\nduring a deploy. We've also had issues with the gem itself - most recently\\na bug in a new version causing strange errors in deployment.\\n\n## Decision\n", "response": "<pad> We will use a stack config to configure the stack configuration.n", "matched_ids": [2610, 4000], "time": "0:00:22.486150", "post": "## Decision\nWe will use a stack config to configure the stack configuration \n "}
{"context": "## Context\\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\\n", "input": "## Context\\nSome mail templates of the core component (Newsletter, Registration, Password Recovery, Order Status mails) depend on storefront Urls to be included in the mails.\\nThose Urls are not available when shopware is used in \"headless\" mode, without the storefront bundle being installed.\\nFor some mails (Newsletter subscription, Double Opt-In, Password recovery), the Url was made configurable over the system config and over the settings inside the administration.\\nThe default values for those Urls are the ones that the storefront bundle would use.\\nThis option does not really scale well as each Url that should be used, needs to be configurable in the administration and this can grow quickly out of hand.\\nAdditionally, it is not clear when and where those configs should be used to generate the absolute Urls, as with the BusinessEvent system and the upcoming FlowBuilder, the sending of mails is not necessarily triggered by the same entry point all the times, but different trigger can lead to sending the same mails.\\n\n## Decision\nThere shouldn't be any links generated on PHP-side as that can be hard to override per sales-channel and can not easily be changed by apps, and links should be generated inside the mailTemplates with string concatenation instead of `raw_url`-twig functions, so the links can still be generated even if the route is not registered in the system.\\nTo make generation of urls inside the mail templated easier, we will add a `{{ domain }}` variable to the twig context, that contains the domain of the current salesChannelContext, of the order in question etc.\\nThe URLs we use in the core mail templates become part of the public API, and custom frontends should adhere to theme and provide routes under the same path, or create redirects so that the default URLs work for their frontend implementation.\\nThe default urls are:\\n```\\n/account/order/{deepLinkCode} -> opens the order details of the given order\\n/account/recover/password?hash={recoverHash} -> start password recovery process\\n/newsletter-subscribe?em={emailHash}&hash={subscribeHash} -> Subscribe email with given hash to the newsletter (for douple-opt in)\\n/registration/confirm?em={emailHash}&hash={subscribeHash} -> Confirm registration for user eith the given mail hash (for douple-opt in)\\n```\\nIf the custom frontends can't or don't want to use our default URLs they can use the possibility to override the existing mail templates to generate custom URLs.\\nWe will deprecate the usage of the system-config configuration values and the events thrown when the links are generated on PHP-side and remove those in the next major version.\\nTo be forward compatible we will already pass the necessary data needed for generating the links into the templates, so the urls can be already generated inside the mail templates.\\nThird party clients (like the PWA) should either adhere to our default URLs or add additional mail templates, that generate the correct urls for their client.\\nIn addition to that the third party client could extend the core mail template, rather than providing a new one, and then deciding in an `IF/ELSE` what url needs to be generated based on the salesChannel or domain.\\n\n\n## Context\\nThis application handles several important pieces of information, including the\\nrequest/user mapping, and the session state for each browser connection. By\\ndefault this information is stored in memory within the application process.\\nHowever, if the application fails and another instance begins to process\\nrequests (e.g. via a load balancer), then any login requests that are still in\\nprogress will be lost (the user will get an error and have to try again).\\nTo enable smooth failover and ensure high availability, an external data store\\noption should be provided, in which both the request/user mapping and the\\nsession state will be stored. This store would be configured like any other\\naspect of the application, via environment variables. If not so configured, the\\nexisting in-memory data store should be used; this should be the default\\nbehavior.\\n\n## Decision\nSimply storing the data in a reliable fashion is easily done using disk files.\\nHowever, if the application is deployed to several machines, and one of those\\nmachines goes down, then the disk files will be unavailable to the other\\ninstance(s), assuming the use of local disk storage.\\nAlternatively, the data can be stored on a separate system, such as a database\\nor key/value store. A database is often more complex to set up than is really\\nnecessary, and for the purpose of this application, a key/value store is more\\nthan adequate. The question then becomes, which key/value store to use. Ideally\\nit would be popular, mature, and well maintained. It would also need an actively\\nmaintained client library available as a Node package. The best solution would\\nbe one with easy setup in cloud computing services such as Azure.\\nThis application will offer support for the **Redis** key/value store. There are\\nseveral reasons for this choice:\\n1. Redis is popular, well supported, and has been maintained for more than 10 years.\\n1. Redis can be configured to use memory-only, or file-based storage.\\n1. Nearly all cloud computing providers offer readily-available Redis instances.\\n\n\n## Context\\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\\n\n## Decision\n", "response": "[ERROR]: An error occurred while generating decision. Please rate anything for this decision.HTTPSConnectionPool(host='b2w8w5mmlr342ulg.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=300)", "matched_ids": [4500, 2020], "time": "0:05:00.914410", "post": "## Decision\n[ERROR]: An error occurred while generating decision. Please rate anything for this decision.HTTPSConnectionPool(host='b2w8w5mmlr342ulg.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=300)"}
{"context": "## Context\\nWe are doing a rewrite.\\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\\n", "input": "## Context and Problem Statement\\nSylius is by design application that will be adjusted to customer needs.\\nTherefore each part of it has to be crafted with replaceability or customizability at its core.\\nNonetheless, the current state of API Platform integration requires the replacement of the whole config of the whole resource.\\nIn terms of the more complicated cases like Order or Customer, this practice may become error-prone and hard to maintain for both maintainers and Sylius users.\\n\n## Decision\nChosen option: \"Config merging\", because it allows us easily overwrite any endpoint, without getting deep into api platform resources.\\n\n\n## Context\\nThe use of TypeScript would in turn provide a more stable codebase.\\nFor this particular project since the vast majority of components will receive either strings, objects or arrays of strings as props and have no state due to the fact they just render externally provided content.\\nWe feel that in this use-case adding the complexity, additional dependencies, increased on-boarding and higher risk of incompatibilities the benefits do not out weigh the overhead, with PropTypes being sufficient given the static nature of the website.\\nTypeScript would increase the complexity of the codebase and the time needed to on-board a developer.\\n\n## Decision\nWe will use PropTypes\\n\n\n## Context\\nWe are doing a rewrite.\\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\\n\n## Decision\n", "response": "<pad> Chosen option: \"React based on a react-api config\", because it allows us to easily overwrite any endpoint, without getting deep into api platform resources.n", "matched_ids": [680, 4429], "time": "0:04:45.984495", "post": "## Decision\nChosen option: \"React based on a react-api config\", because it allows us to easily overwrite any endpoint, without getting deep into api platform resources \n "}
{"context": "## Problem\\nCurrently multiple warnings are saved in an elektra non-conforming array\\nnotation which is limited to 100 entries. The notation of `#00` is against\\nthe design [decision made](array.md).\\n", "input": "## Problem\\nCurrently, an error or warning message in elektra causes the following line to be shown:\\n```\\nIngroup: <group>\\n```\\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\\n\n## Decision\nThe `ingroup` message will be removed as it does not yield any notable benefit.\\nSee [Error concept](error_codes.md)\\n\n\n## Problem\\nOn structures like maps or [arrays](array.md) there are different\\npossibilities which keys are looked up in the KeySet and which\\nare simply iterated.\\nWithout any guidelines, applications would provide arbitrary inconsistent\\nbehavior.\\n\n## Decision\nEvery key that an application wants to use, must be looked up\\nwith `ksLookup` using a cascading lookup key.\\n- [Arrays](array.md)\\n\n\n## Problem\\nCurrently multiple warnings are saved in an elektra non-conforming array\\nnotation which is limited to 100 entries. The notation of `#00` is against\\nthe design [decision made](array.md).\\n\n## Decision\n", "response": "<pad> The #00 notation will be removed as it does not yield any notable benefit.nSee [Error concept](error_codes.md)n", "matched_ids": [1308, 1310], "time": "0:00:23.495408", "post": "## Decision\nThe #00 notation will be removed as it does not yield any notable benefit \n See [Error concept](error_codes.md)n"}
{"context": "## Context\\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\\nThere is currently no formal recommendation for a tool of choice in the technical guidance https://github.com/DFE-Digital/technical-guidance.\\nWe want a tool that provides an open and modern security standard.\\n", "input": "## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThis service has a need to send emails and SMS to users. As Hackney is approved as an organisation that can use GOV.UK Notify we are able to use this service and take advantage of the reliability and low costs.\\nHackney already use Notify for other services, although it is not yet referenced in their playbook https://github.com/LBHackney-IT/API-Playbook\\n\n## Decision\nUse Notify over a third party service such as SendGrid for the sending of emails and SMS\\n\n\n# Context\\nAs someone responsible for ensuring accurate data is published by the\\ngovernment, I need to ensure that people authoring content can only\\nview and edit things that they own, so that there are no embarrassing\\nupdates published.\\nWe need to ensure that datasets and dashboards can only be edited by\\nthe right people.\\nWe currently have signonotron2 accounts for all of the people using\\nthis application. That will continue to be in place for the future.\\nWe need a way of restricting which signon accounts have access to\\ndifferent entities within the admin application.\\nWe have tried to model this in various ways.\\nAs someone working at the DVLA, should I have the ability to edit\\nall of the DVLAs dashboards? The answer to that is no.\\nWe've had some discussions with our user population, but what is\\nactually good still isn't known at this point. So we are going to\\nincrementally do this functionality, as a series of vertical slices,\\nand accept that we will need to refactor, rework and do data\\nmigrations as we learn more about the problem space and how best to\\nmeet those needs.\\nInitially, we proposed using custom Django Managers in stagecraft to\\nimplement this. We don't think this is the right approach; it smells\\ntoo complex.\\nA simpler start point will be to define the URLs (resources) and\\nhave permissions per resource.\\n# Decision\\nDefine the matrix of URLs, and map permissions per resource.\\n# Status\\nAccepted.\\n# Consequences\\nTrading shipping a thing with future technical debt for now.\\n\n## Decision\nDefine the matrix of URLs, and map permissions per resource.\\n# Status\\nAccepted.\\n# Consequences\\nTrading shipping a thing with future technical debt for now.\\n\n\n## Context\\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\\nThere is currently no formal recommendation for a tool of choice in the technical guidance https://github.com/DFE-Digital/technical-guidance.\\nWe want a tool that provides an open and modern security standard.\\n\n## Decision\n", "response": "<pad> We will use a SecuritySet library to store the user's passwords.n", "matched_ids": [5213, 3394], "time": "0:00:22.405042", "post": "## Decision\nWe will use a SecuritySet library to store the user's passwords \n "}
{"context": "## Context and Problem Statement\\nHow to guarantee reproducibility of Jupyter Notebooks?\\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\\nNotebooks should be treated as component/service that use their own dependencies, therefore when storing notebooks,\\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\\n## Decision Drivers <!-- optional -->\\n* user prospective\\n* reproducibility\\n* traecability\\n", "input": "## Context and Problem Statement\\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`/`PUT` HTTP methods),\\nRead(`GET` HTTP method), Update(`PUT`/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\\nworkflow in a state machine) or command execution.\\n## Decision Drivers\\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\n\n## Decision\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\nThe \"Taking advantage of the `Controller` REST archetype\" should be considered as a recommended solution. All Sylius\\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\\nor new schema should be introduced for the JSON-LD structure.\\nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing is\\ndelivered with vanilla Sylius installation.\\n\n\n## Context\\nHistorically we have had \"two ways\" of configuring Verify Service Provider:\\n- Using environment variables\\n- Using a YAML file\\nWhen using environment variables the application used the verify-service-provider-env.yml\\nfile from the resources directory (so inside the jar). When using the YAML file you would\\npass the path to a different file as a command line parameter - usually people\\nwould use the example one that's contained in the repo.\\nThere were a couple of reasons for the extra complexity of managing two files, both due to\\nrestrictions with the java buildpack used by cloudfoundry:\\n- It's not possible to specify command line arguments through the java buildpack,\\nso you can't specify a path to your config file\\n- We weren't confident in the way cloudfoundry manages static files, so we didn't want\\nto rely on one.\\nThere was also a philosophical point that 12 factor applications should be configured through\\ntheir environment. This made the \"hide the configuration in the .jar and do everything through\\nenv vars\" way appealing.\\n\n## Decision\nWe will remove the verify-service-provider-env.yml file from src/main/resources\\nThe application will default to the verify-service-provider.yml\\nfile that's included in the .zip if no command line arguments are provided.\\nIf the application is started without command line arguments specifying a yml file\\nAND no environment variables have been set, startup should error gracefully and tell\\nthe user that the configuration fields have not been specified for example:\\n\"ERROR - no configuration fields found, either set environment variables or specify\\na configuration file using command line arguments ```server <path/to/verify-service-provider.yml>```\"\\nWe will establish the path to verify-service-provider.yml by asking java for the\\npath to the .jar file containing the Application class and looking in the parent\\nfolder.\\n\n\n## Context and Problem Statement\\nHow to guarantee reproducibility of Jupyter Notebooks?\\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\\nNotebooks should be treated as component/service that use their own dependencies, therefore when storing notebooks,\\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\\n## Decision Drivers <!-- optional -->\\n* user prospective\\n* reproducibility\\n* traecability\\n\n## Decision\n", "response": "<pad> * Option 1:!-- optional -->n* user prospectiven* reproducibilityn* traecabilityn", "matched_ids": [674, 4448], "time": "0:00:23.195506", "post": "## Decision\n* Option 1:!-- optional -->n* user prospectiven* reproducibilityn* traecabilityn"}
{"context": "## Context\\nWe need to record the architectural decisions that we make as we develop Ockam.\\n", "input": "## Context\\nWe need to record the architectural decisions made for our team.\\n\n## Decision\nWe will use Architecture Decision Records, as discussed in our original proposal: https://docs.google.com/document/d/1pZlYCyXcZbmQq1O-g4BNJD1uZTjluYKkk7BSu2BwOGU/edit#\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions that we make as we develop Ockam.\\n\n## Decision\n", "response": "<pad> We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisionsn", "matched_ids": [2261, 4248], "time": "0:00:23.596567", "post": "## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisionsn"}
{"context": "## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n", "input": "## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\n", "response": "<pad> The change that we're proposing or have agreed to implement.n", "matched_ids": [3544, 2025], "time": "0:00:20.003925", "post": "## Decision\nThe change that we're proposing or have agreed to implement \n "}
{"context": "## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https://apps.nrs.gov.bc.ca/int/jira/browse/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n", "input": "## Context\\nDuring the first iteration of development on the wikibase release pipeline, one of the goals was to build and run it against the master branches of MediaWiki, Wikibase and other bundled extensions ([T270133](https://phabricator.wikimedia.org/T270133)).\\nBecause of the lack of such docker images at the time the team decided to create our own, inspired by the work of the official docker images. The benefits of this decision was seen when previously untested parts of Wikibase (Multi Wiki testing client/repo ) now had some coverage on the master branch. During the development of the pipeline several issues were detected by using these custom docker images, sometimes the pipeline would breakdown days before a bug report would appear on phabricator.\\nThis can be useful but also comes with some additional drawbacks that can affect the maintainability and the quality of the releases WMDE will produce.\\n- To offer the same quality and security as the official Mediawiki docker images we now also have to maintain our own rather than building upon what already exists.\\n- Any updates or security fixes to these images are probably also more likely to be identified and patched in the official MediaWiki docker images quicker than in any image maintained by WMDE.\\n- The MediaWiki docker images are battle proven with 10+ Million downloads, our custom images are not.\\nAs the priority of the release pipeline should be to provide stable and secure releases it could make sense to revert this initial decision of building our own image.\\nThe decision to adopt parts of the testing done in the release pipeline for Wikibase CI is still pending. Depending on the outcome of [T282476](https://phabricator.wikimedia.org/T282476), custom images could then be required again and could serve as a base when used for testing in CI where the requirements for security or performance aren't as high ([T282479](https://phabricator.wikimedia.org/T282479)).\\n\n## Decision\n- Use the official MediaWiki docker images as a base for the Wikibase base and bundle images.\\n\n\n## Context and Problem Statement\\nThe [Settings Redesign project](https://jira.mozilla.com/browse/FXA-840) will be created [as a new React application](https://github.com/mozilla/fxa/blob/main/docs/adr/0011-create-new-react-app-for-settings-redesign.md) and in turn, has opened the door to assess certain pieces of our technology stack.\\n[GraphQL](https://graphql.org/), or GQL, is not a database query language, but is instead a query language for APIs. It _describes_ data requirements, and is a powerful alternative to REST. Some benefits can be gained by using it on top of existing REST architecture.\\n\"Apollo\" in this document refers to the \"Apollo client\" and \"Apollo server\" pieces of the Apollo platform\u00b9, which can be described as a unified data layer that enables applications to interact with data from data stores and APIs. In other words, it allows us to write and handle GraphQL on the client and server. Apollo also gives us many tools out of the box like caching.\\nThis ADR serves to lay out pros and cons of using GraphQL and Apollo in the Settings Redesign project as an alternative to hitting our conventional REST endpoints.\\n\u00b9Apollo also offers Apollo Graph Manager and Apollo Federation which are paid services, [read more from their docs](https://www.apollographql.com/docs/intro/platform/). We do not need to use these to use GQL with Apollo server or Apollo client.\\n## Decision Drivers\\n- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\n\n## Decision\n- Performance implications; consideration around the number of network requests and data transferred\\n- Ease of setup for client-side API calls\\n- Clarity around expected data, React integration, and developer tooling\\n- Development speed, both around initial setup and as new features roll out\\nChosen option: \"B - Layer GraphQL on top of our REST architecture\", because:\\n- GQL offers performance optimizations by allowing us to consolidate our network requests by sending data requirements up in a single request, asking for only what is needed on the client, shifting the burden of figuring out how and where to gather this data from the client onto the server. The server then compensates for overfetching by only sending back what is requested.\\n- Allowing developers to query for and expect only exactly what is needed with end-to-end typing and a more \"declarative\" way of thinking towards data requirements, along with keeping data requirement schemas close to where they're consumed, makes it painfully clear what's being sent and received between client and server.\\n- Does not preclude option C, as we can replace or supplement direct calls to the FxA auth-server down the line. This is faster for initial development, and this option over option C will also help mitigate risk around using a relatively novel piece of technology for FxA.\\n- Nice side-effects include the GQL playground, managing only a single API endpoint, and the ability to store both local state and network data in the Apollo cache.\\n\n\n## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https://apps.nrs.gov.bc.ca/int/jira/browse/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n\n## Decision\n", "response": "<pad> * Status: proposedn* Deciders: License Changen* Date: 2020-08-27nTechnical Story: [description | https://apps.nrs.gov.bc.ca/int/jira/browse/ARCH-62]>n", "matched_ids": [4464, 387], "time": "0:00:27.893615", "post": "## Decision\n* Status: proposedn* Deciders: License Changen* Date: 2020-08-27nTechnical Story: [description | https://apps \n rs.gov.bc.ca/int/jira/browse/ARCH-62]>n"}
{"context": "## **Context**\\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\\nWe will look at two options for validation:\\n- **Data Annotations**\\nThis involves \"annotating\" each class model with specific validation, such as\\n- `[Required(ErrorMessage = \"This field is required\")]`\\n- `[MaxLength(20)]`\\nThere are a number of issues with this approach:\\n- Validation is scattered throughout the codebase as attributes on data model classes\\n- Testing is not straightforward\\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\\n- Does not allow for conditional validation\\n- **Fluent Validation**\\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\\n- Lives outside of the data model classes\\n- Very easy to test validation in isolation\\n- Error messaging can be externalised using dependency injection\\n- Allows for chaining of validators and conditional validation\\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\\n", "input": "## Context\\nThe Wikidata Bridge app uses VueJs version 2 and Vuex version 3 for state management.\\nBoth within the store and in vue components the calls to `dispatch()` and `commit()` are not type safe.\\nThat means that they accept any arguments and TypeScript will still compile without error.\\nSince `dispatch()` and `commit()` are typical seams that are usually mocked during unit testing,\\nit is up to integration, end-to-end and browser tests to detect these errors.\\nThis is particularly unfortunate as the store is one of the central locations where business logic happens.\\nWe considered two options:\\n1. writing our own set of wrappers for `dispatch()` and `commit()` to get type safety\\n1. using vuex-smart-modules\\nThe advantages of doing it ourselves included us not having another dependency.\\nThe risks include that this would be yet another homebrew layer of abstraction.\\nThe advantages of using vuex-smart-modules include:\\n- it gives us proper native type safety both in the store and in components\\n- we can call actions/getters/mutations with [method style access](https://github.com/ktsn/vuex-smart-module#method-style-access-for-actions-and-mutations)\\n- we can get rid of all the `BRIDGE_SET_TARGET_VALUE` constants without having to fall back to string literals\\n- we can actually use the IDE's `go to method definition` functionality\\n- we can rely on the action's return type instead of dispatch's `Promise<any>` being used everywhere\\n- we can drop the `vuex-class` dependency as we can use vuex-smart-modules for all store access in components\\n- it is developed by a VueJs core contributor\\nThe risks include:\\n- it is still a very new project with a 0.x.y version number\\n- it is another layer on top of vuex, which means the documentation may not be as good as it could be\\n- we are the first big project to use it\\n- mocking of dependencies and nested modules in testing still seems to be not handled as diligently as one would wish\\n\n## Decision\nWe decided to rewrite our Vuex store using vuex-smart-modules version 0.3.4\\n\n\n## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When\\n`nil` is accidentally provided during the start up process it is preferable to\\nfail fast with an explicit message. Without this `nil` can be passed down\\nthrough the stack and cause strange behaviour where the code has been designed\\nwith it as a dependency. Instead of adding `nil` guards throughout the codebase\\nfor required environment variables (eg. `ENV.fetch('FOO', 'default')`, this\\nshould be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016\\nhttps://github.com/laserlemon/figaro. We should only use supported gems to\\nensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An\\neasy example of this is when we use environment variables as a feature flag\\nmechanism. We want to stub the value to test both scenarios without being\\ninfluenced by real values being loaded. Mutating the actual ENV value (eg.\\n`allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but\\nmay have unexpected consequences where more than 1 part of the same process\\nunder test uses the same variable. Figaro used to be a handy abstraction layer\\nthat we could stub eg.\\n`allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then\\nconsider how we can stub environment variables.\\n\n## Decision\nUse DotEnv to load our environment variables.\\n\n\n## **Context**\\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\\nWe will look at two options for validation:\\n- **Data Annotations**\\nThis involves \"annotating\" each class model with specific validation, such as\\n- `[Required(ErrorMessage = \"This field is required\")]`\\n- `[MaxLength(20)]`\\nThere are a number of issues with this approach:\\n- Validation is scattered throughout the codebase as attributes on data model classes\\n- Testing is not straightforward\\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\\n- Does not allow for conditional validation\\n- **Fluent Validation**\\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\\n- Lives outside of the data model classes\\n- Very easy to test validation in isolation\\n- Error messaging can be externalised using dependency injection\\n- Allows for chaining of validators and conditional validation\\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\\n\n## Decision\n", "response": "<pad> We decided to rewrite our Vuex store using vuex-smart-modules version 0.3.4n", "matched_ids": [1345, 4181], "time": "0:00:20.693746", "post": "## Decision\nWe decided to rewrite our Vuex store using vuex-smart-modules version 0.3.4n"}
{"context": "## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n", "input": "## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n\n## Decision\nCreate a distribution-aware implementation of `TaskManager`.\\n\n\n## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\\n\n## Decision\n* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\\n\n\n## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n\n## Decision\n", "response": "<pad> Create a distribution-aware implementation of TaskManager.n", "matched_ids": [2874, 2121], "time": "0:00:18.410831", "post": "## Decision\nCreate a distribution-aware implementation of TaskManager \n "}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "input": "## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard\\nin this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n", "response": "<pad> We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).n", "matched_ids": [494, 1269], "time": "0:00:25.295233", "post": "## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) \n "}
{"context": "## Context\\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\\nsandboxed environment. A Docker image has been created to help with this but this still\\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\\nto run Bitcoin miners and other compute-stealing stuff.\\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\\nbe found in the [notes on Isolation](https://github.com/martinklepsch/cljdoc/blob/72da65055ab94942f33fb63b29b732e81b559508/doc/isolation.md)\\n", "input": "## Context\\nIstio's service mesh capabilites are facilitated via sidecars injected into\\nworkload pods. These sidecars run an Istio-patched version of Envoy that is tied\\nto the version of Istio that injects them.\\nTypically when new versions of Istio are released, new versions of the sidecars\\nare released as well. Istio has been good so far about supporting older versions\\nof sidecars that were deployed before Istio was upgraded, but it is still\\n[documented best practice](https://istio.io/latest/docs/setup/upgrade/) to roll\\nall the pods after an Istio upgrade.\\nAs an additional constraint, the operators of cf-for-k8s clusters expect to be\\nable to perform upgrades in one `kapp deploy`, with no post-install hooks or\\nother bash scripts. This limits our options considerably. See this [Slack\\nthread](https://cloudfoundry.slack.com/archives/CH9LF6V1P/p1592521879117400) on\\nthat constraint.\\n\n## Decision\nWe will use the kubernetes\\n[Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/) resource\\nto run the kubectl command needed to roll workload pods, after waiting for the\\nnew Istio control plane to be up and healthy.\\nTo that end, we will add the necessary minimal `ServiceAccounts` and `Roles`\\nneeded to list resources in the `istio-system` namespace, and restart resources\\nin the configured workload namespace. We will also build and maintain a\\ncontainer image that contains the Job's logic.\\nAll istio components will be tagged with their Istio version so that the job can\\npositively determine that the correct version of control plane components are\\nalive and healthy. We will also name the job according to it's Istio version, so\\nthat we can take advantage of `Jobs` inherent immutability in cases where a\\ncf-for-k8s upgrade does not contain a new Istio version (pushing the same job\\nagain will not cause it to rerun, preventing workloads from rolling\\nunnecessarily). Subsequent jobs will clean up previous ones.\\n\n\n## Context\\nWe have the requirement of adding some resources to the base cloud instances. We currently do\\nthis via the [cloud.conf](https://github.com/alphagov/prometheus-aws-configuration/blob/375f34600e373aa0e4c66fcae032ceee361d8c21/terraform/modules/prometheus/cloud.conf) system. This presents us with some limitations, such as configuration\\nbeing limited to 16kb, duplication in each instance terraform and a lack of fast feedback testing.\\n\n## Decision\nWe have decided to move away from cloud.conf as much as possible and instead use it to instantiate\\na masterless puppet agent which will manage the resources.\\n\n\n## Context\\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\\nsandboxed environment. A Docker image has been created to help with this but this still\\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\\nto run Bitcoin miners and other compute-stealing stuff.\\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\\nbe found in the [notes on Isolation](https://github.com/martinklepsch/cljdoc/blob/72da65055ab94942f33fb63b29b732e81b559508/doc/isolation.md)\\n\n## Decision\n", "response": "<pad> We will use the [Kubernetes Docker image](https://github.com/kubernetes/kubernetes/docs/concepts/workloads/controllers/job/) resourcento run the kubectl command needed to roll workload pods, after waiting for thennew Istio control plane to be up and healthy.nTo that end, we will add the necessary minimal ServiceAccounts and Rolesnneeded to list resources in the istio-system namespace, and restart resourcesnin the configured workload namespace. We will also build and maintain ancontainer image that contains the Job's logic.nAll istio components will be tagged with their Istio version so that the job cannpositively determine that the correct version of control plane components arenalive and healthy. We will also name the job according to it's Istio version, sonthat we can take advantage of Jobs inherent immutability in cases where ancf-for-k8s upgrade does not contain a new Istio version (pushing the same jobnagain will not cause it to rerun, preventing workloads from rollingnunnecessarily). Subsequent jobs will clean up previous ones.n", "matched_ids": [2240, 1372], "time": "0:01:18.709009", "post": "## Decision\nWe will use the [Kubernetes Docker image](https://github.com/kubernetes/kubernetes/docs/concepts/workloads/controllers/job/) resourcento run the kubectl command needed to roll workload pods, after waiting for thennew Istio control plane to be up and healthy \n To that end, we will add the necessary minimal ServiceAccounts and Rolesnneeded to list resources in the istio-system namespace, and restart resourcesnin the configured workload namespace. We will also build and maintain ancontainer image that contains the Job's logic \n All istio components will be tagged with their Istio version so that the job cannpositively determine that the correct version of control plane components arenalive and healthy. We will also name the job according to it's Istio version, sonthat we can take advantage of Jobs inherent immutability in cases where ancf-for-k8s upgrade does not contain a new Istio version (pushing the same jobnagain will not cause it to rerun, preventing workloads from rollingnunnecessarily). Subsequent jobs will clean up previous ones \n "}
{"context": "## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\\nAs part of this, considerations were made around:\\n* Ease of use for creating new sites\\n* Ability to share components\\n* Ability to segregate content for specific channels (sites)\\n* Ability to share content up and down the stack where needed and appropriate\\n* Permissions model required to support this model\\n", "input": "## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\n### Options\\n####\u00a0NHS Digital BloomReach\\nWe looked at the implementation of document management functionality in BloomReach delivered as part of the work for NHS Digital - https://github.com/NHS-digital-website/hippo. This project provides a good view of what native BloomReach document management and publishing looks like, it delivers a flexible content model for publishing to HTML documents.\\n#### Office 365\\nIn addition we should look at the current usage of Office 365 to determine whether the platform would be suitable for integration with BloomReach.\\n#### FutureNHS\\nFinally looked at the FutureNHS collaboration platform to see whether it would be suitable for integration with BloomReach. Whilst the platform in its current state would be a good candidate for integration, we discovered that at the time of writing, this platform is due to be rewritten.\\n\n## Decision\nWe believe that Microsoft Office 365 and Sharepoint provide a good basis for the platforms document management capabilities and we have proved that it can be successfully integrated.\\nHaving looked at the requirements for LKS staff, particularly around the ability to support a range of document types such as spreadsheets and presentations, our belief is that we would be better placed to integrate an external document management system. The NHS Digital publishing platform provides an excellent HTML publishing model and workflow, however extending it to support a broader range of document types would be complex.\\nFutureNHS may provide a good candidate for integration in the future, however at the time of writing it is difficult to recommend as the product is in the process of being rewritten.\\n\n\n## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\\n\n## Decision\nWe will use [Quay.io](https://quay.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and/or enterprise artifacts with trail licenses).\\nWe\u2019d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we'd prefer a SaaS solution.\\nThe REST API of Quay.io allows our organization to potentially automate user provisioning/invitation and user/group management which is not available for Docker Cloud at this time.\\nAdditionally, Quay / CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product.\\nThe summary [![report of the comparison](https://img.shields.io/badge/report%20of%20the%20comparison-PRIVATE-red.svg)](https://ts.alfresco.com/share/s/mVAV1sGIReC_iqgMN0GGnQ) also contains reference links to the full investigation.\\n\n\n## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\\nAs part of this, considerations were made around:\\n* Ease of use for creating new sites\\n* Ability to share components\\n* Ability to segregate content for specific channels (sites)\\n* Ability to share content up and down the stack where needed and appropriate\\n* Permissions model required to support this model\\n\n## Decision\n", "response": "<pad> We will use [Bloomreach](https://www.bloomreach.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and/or enterprise artifacts with trail licenses).nWe\u2019d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we\u2019d prefer a SaaS solution.nThe REST API of Quay.io allows our organization to potentially automate user provisioning/invitation and user/group management which is not available for Docker Cloud at this time.nAdditionally, Quay / CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product.nThe summary [![report of the comparison](https://img.shields.io/badge/report%20of%20the%20comparison-PRIVATE-red.svg) also contains reference links to the full investigation.n", "matched_ids": [1195, 3598], "time": "0:01:10.797472", "post": "## Decision\nWe will use [Bloomreach](https://www.bloomreach.io) for the internal and protected tiers of access and use Docker Hub for public repositories (images of community versions and/or enterprise artifacts with trail licenses) \n We\u2019d like to limit the introduction of additional deployments (particularly customer-facing) that our IT staff has to maintain, so we\u2019d prefer a SaaS solution \n The REST API of Quay.io allows our organization to potentially automate user provisioning/invitation and user/group management which is not available for Docker Cloud at this time \n Additionally, Quay / CoreOS seems strongly committed to their SaaS offering while Docker seems entirely focused on their Enterprise \u2018on-prem\u2019 product \n The summary [![report of the comparison](https://img.shields.io/badge/report%20of%20the%20comparison-PRIVATE-red.svg) also contains reference links to the full investigation \n "}
{"context": "## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https://symfony.com/doc/current/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n", "input": "## Context\\nWe are currently using the [Java-based OpenAPI generator CLI](https://www.npmjs.com/package/@openapitools/openapi-generator-cli).\\nAs this generator is targeting many different programming languages in some points it does not fit our needs, e. g. handling of unique names or API files, which also differ between components.\\nIn our current approach we wrap the generated API with our own to fit those needs, but we realized, that we ended up writing many workarounds.\\nMany workarounds can currently only be covered by preprocessing/manipulating the original service definition.\\nAt the moment, some API designs cannot be realized or are not worth investing into.\\nWe have observed that due to the size of the project, there are quite a lot of known bugs, that take a long time to solve or are not solved at all.\\nThe probably biggest pain point is that the used CLI needs a Java runtime.\\nTo run the CLI we have to download the given .jar file (~24MB) and make it part of our npm package, making it much bigger in size than usual.\\nThis is necessary as the file is not downloaded on installation but on command execution, causing other errors down the line (hurts SLC-29, race conditions in async batch execution).\\n\n## Decision\nWrite our own generator, that is purely based on TypeScript and supports the API we need out of the box.\\n\n\n## Problem Statement\\nWe have a specification-first development cycle for our APIs. This means that editing our API - adding endpoints,\\nediting responses, changing functionality - starts in the YAML file that contains the API definition. From that, we use\\n`go-swagger` to read our specification and generate Go types for use in our backend.\\n**The good:** With this model, we can focus on the API design without worrying about how to convert that into usable Go\\ncode - `go-swagger` does this for us. Code is neatly organized into separate packages for each API, so they can function\\nindependently.\\n**The problem:** Our APIs are all concerned with the same data models, so even though they are _technically_\\nindependent, they are highly related. We're defining the same objects over and over again in our YAML specs. All APIs\\nhave a shipment, a move, an orders object, and the list goes on. When we make one change to these objects, we have to\\nmake changes to each and every YAML file.\\nThis means our YAML files quickly get out of sync. We've had to deal with bugs stemming from this disconnect many times.\\nThis is also hugely redundant - there are hundreds of lines that are essentially identical in each API.\\nWe have to do a lot of manual type conversions in the backend to turn the Swagger-generated Go types in our general\\nmodel types. These type conversions are also redundant, and they're another place where we can miss changes that add or\\nmodify fields. One possible negative is that having shared types between APIs would threaten their ability to function\\nindependently. However, our APIs are extremely interconnected on the backend and use many of the same services, so there\\nis a question of how independent they could possibly be regardless.\\nLastly, we struggle with maintaining the same standards in each API. Some are more resistant to change, and we don't\\nhave a good method for incrementally standardizing those APIs.\\n\n## Decision\n### Chosen Alternative: _Use the `openapi` CLI tool to compile shared API definitions (Option 3)_\\nThis looks like the most complicated solution by far. And for the initial implementation, it is. We have already\\nintroduced the `openapi` tool to the project so that we can preview our API documentation, but now we will be dependent\\non it for our development process. We will also have to work in a new folder, so all of our engineers will have to\\nacclimate to the development cycle.\\nHowever, the benefits are significant. The `openapi` compiler dictates a structure that is organized and fairly\\nintuitive, making it easy to create, find, and reference separate definition files. Like option 2, edits to one file can\\napply to all of our APIs. Furthermore, the compiler can handle our files as-is, so we can gradually split our\\ndefinitions as we move forward.\\nUnlike option 2, this method won't change the outward behavior of our APIs. External tools like Load Testing, and\\neventually the Prime integration, won't need to change the way they consume our content. This was ultimately the\\ndeciding factor because, even though this option _looks_ more complicated, the overall impact of the switch will be\\nminimal. Load Testing was also completely non-functional with option 2, and I have not yet figured out how to make it\\nwork.\\n\n\n## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https://symfony.com/doc/current/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n\n## Decision\n", "response": "<pad> ### Alternative: _Use the openapi CLI tool to compile shared API definitions (Option 3)_nThis looks like the most complicated solution by far. And for the initial implementation, it is. We have alreadynintroduced the openapi tool to the project so that we can preview our API documentation, but now we will be dependentnon it for our development process. We will also have to work in a new folder, so all of our engineers will have tonclimate to the development cycle.nHowever, the benefits are significant. The openapi compiler dictates a structure that is organized and fairlynintuitive, making it easy to create, find, and reference separate definition files. Like option 2, edits to one file cannapply to all of our APIs. Furthermore, the compiler can handle our files as-is, so we can gradually split ourndefinitions as we move forward.nUnlike option 2, this method won't change the outward behavior of our APIs. External tools like Load Testing, andneventually the Prime integration, won't need to change the way they consume our content. This was ultimately thendeciding factor because, even though this option _looks_ more complicated, the overall impact of the switch will be minimal. Load Testing was also completely non-functional with option 2, and I have not yet figured out how to make it work.n", "matched_ids": [3626, 3079], "time": "0:01:17.396165", "post": "## Decision\n### Alternative: _Use the openapi CLI tool to compile shared API definitions (Option 3)_nThis looks like the most complicated solution by far. And for the initial implementation, it is. We have alreadynintroduced the openapi tool to the project so that we can preview our API documentation, but now we will be dependentnon it for our development process. We will also have to work in a new folder, so all of our engineers will have tonclimate to the development cycle \n However, the benefits are significant. The openapi compiler dictates a structure that is organized and fairlynintuitive, making it easy to create, find, and reference separate definition files. Like option 2, edits to one file cannapply to all of our APIs. Furthermore, the compiler can handle our files as-is, so we can gradually split ourndefinitions as we move forward \n Unlike option 2, this method won't change the outward behavior of our APIs. External tools like Load Testing, andneventually the Prime integration, won't need to change the way they consume our content. This was ultimately thendeciding factor because, even though this option _looks_ more complicated, the overall impact of the switch will be minimal. Load Testing was also completely non-functional with option 2, and I have not yet figured out how to make it work \n "}
{"context": "## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n", "input": "## Context and Problem Statement\\nWe want to use a component to graphically model nodes and edges in a web application.\\n## Decision Drivers <!-- optional -->\\n* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\n\n## Decision\n* usable with Angular\\n* number of dependecies\\n* features\\n+ create Nodes\\n+ create Connections\\n* usability\\nChosen option: Choosen a Combination of ngx-graph and plain d3.js, because ngx-graph might miss some usefull features but has good usability which is worth to have a look at\\n### Positive Consequences <!-- optional -->\\n* easy to use graph modeler\\n* only a few dependencies\\n* full control over features and behavior\\n### Negative consequences <!-- optional -->\\n* needs to be implemented\\n\n\n## Context and Problem Statement\\nSeveral operations (crawl repo, deploy services, ...) are required to run in the background. Therefore, a background job architecture is required being able to launch arbitrary jobs in the background.\\n## Decision Drivers\\n* MUST be implemented in Java\\n* MUST offer a callback functionality\\n\n## Decision\n* MUST be implemented in Java\\n* MUST offer a callback functionality\\nChosen option: Java CompletableFuture, because of the possibility to chain multiple actions and the option to attach callbacks on completion of a background task.\\n### Positive Consequences\\n* Possibility to chain together multiple actions after a succesful execution of a background task.\\n* Use of FixedThreadPool, i.e., a pool of threads with a predefined number of threads which are reused. If all threads are active at a time and additional tasks are submitted, they will wait in a queue.\\n### Negative Consequences\\n* With the use of a FixedThreadPool, a background task may be delayed if all threads are active at the time of submission.This may produces time of waiting in the frontend of MICO.\\n\n\n## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n\n## Decision\n", "response": "<pad> * Simple implementationn* No assumption about the tasks trackern* Support of a set of generic fields related to a tasknChosen option: Feature Request Tracker, because it's a simple and easy to use tool.n### Positive Consequences!-- optional -->n* Simple implementationn* No assumption about the tasks trackern* Support of a set of generic fields related to a taskn", "matched_ids": [4689, 4674], "time": "0:00:36.487105", "post": "## Decision\n* Simple implementationn* No assumption about the tasks trackern* Support of a set of generic fields related to a tasknChosen option: Feature Request Tracker, because it's a simple and easy to use tool \n ### Positive Consequences!-- optional -->n* Simple implementationn* No assumption about the tasks trackern* Support of a set of generic fields related to a taskn"}
{"context": "## Context\\nOur service-operator allows service teams to provision various AWS services by\\ndeclaratively defining resources and submitting them via the kubernetes api.\\nSome of these resources require IAM to authorise how the provisioned service\\ncan be used. The types of actions that can be performed on.\\n#### Example\\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\\n```\\n---\\napiVersion: storage.govsvc.uk/v1beta1\\nkind: S3Bucket\\nmetadata:\\nname: s3-bucket-sample\\nspec:\\naws:\\nLifecycleRules:\\n- Expiration: 90days\\nVersioning:\\nEnabled: true\\n```\\nIn order to access a provisioned bucket via the the AWS SDK users will require\\nan IAM role/policy that allows access.\\nWe want things like bucket ACL, versioning configuration and lifecycle policy\\nto be defined declaratively via the resource manifest (see example above), and continuously managed\\nby the service operator.\\nWe want users of the provisioned bucket to be able to read back all\\nconfiguration, and be able to fully utilise the specific bucket for reading,\\nwriting and managing their objects within the provisioned bucket, but we want\\nto avoid giving permissions to users that could cause conflicts with the\\nproperties that are managed by the service operator's reconcile loop.\\nFor example, given the example manifest above, we would like to avoid giving\\npermissions that would allow a user to alter the Expiration LifeCycleRules,\\nsince any changes the user made would be periodically overwritten by the\\nservice operator's reconciliation.\\n", "input": "## Context\\nFollowing the retirement of gsp-local the service teams have no way of testing\\nchanges before a merge to master. So the feedback cycle is very slow, and\\npotentially risky. We need to enable devs to get more rapid feedback through use\\nof namespaces that allow for deployments from arbitrary branches or from local\\n`kubectl apply` actions.\\nIn the past service teams have made use of the Sandbox cluster for some of this\\ntesting where we allow them Cluster Admin permissions. However the primary\\nfunction of the Sandbox cluster is to test changes to the platform itself and so\\nis often down or otherwise degraded, which also slows service team development\\ndown.\\nThis presents several security problems that need to be addressed.\\nThe in-cluster concourse:\\n* must not be able to create or edit daemonsets (as tenants should not need to\\ninteract with per-node pods that typically need elevated permissions to e.g.\\nmanipulate the network stack)\\n* must not be able to create pods that use host networking, or run in privileged\\nmode\\n* must not be authorised to create any \"cluster\" scoped resources (e.g.\\nclusterrole, custom resources etc.)\\nFor harbor:\\n* a dev in one namespace must not be authorised to push, edit or delete images\\nrelating to namespaces outside their own\\n* notary signing keys must be namespace-scoped\\nFor external-dns:\\n* a namespace must not be able to hijack the DNS entries of another namespace\\nFor istio:\\n* any istio resources deployed as part of a namespace must not have any impact\\non other namespaces\\nFor CloudHSM access:\\n* connectivity only enabled via `-cluster-config`, as with \"prod\" namespaces and\\npods\\n* credentials should be different for each namespace, and not the same as those\\nused for the \"production\" instances of the application\\n\n## Decision\nWe will address the majority of the security concerns by implementing\\n[ADR043][].\\nWe will address the harbor concerns by creating namespace-scoped credentials\\nrelating to namespace-scoped harbor \"projects\" and provide these credentials via\\nsecrets in the namespace.\\nWe will address the DNS concerns by locking down each namespace instance of\\nexternal-dns to a dedicated zone.\\nWe will address the istio concerns through the use of gatekeeper constraints\\n(e.g. all istio resources that support it have `exportTo: [\".\"]` set).\\nWe will create separate CryptoUsers in the CloudHSM for each namespace that\\nrequires access. CloudHSM cryptographic keys/operations can then be scoped to a\\nsingle user (& namespace). This will allow each namespace to effectively have\\nit's own virtual slice of the CloudHSM and enable controlling access on a\\nper-namespace basis.\\n\n\n## Context and Problem Statement\\nTracking consent stores cookie consent information in a userConsent cookie as URL-encoded JSON. This information is available\\nto frontend microservices in Javascript running in a user's browser and in the request headers passed to the Scala\\nPlay controllers running on the server-side.\\nBefore frontend services can interpret the cookie, they need to decode and parse it using a Scala JSON library. They also need\\nto apply some business logic to ensure in cases of uncertainty, the user is, by default, assumed to have rejected cookies.\\nFor Javascript code, we propose to make the existing UserPreferences object available in the window object. This will mean\\nservices integrating with tracking consent will have a method available to them for parsing the cookie using the latest\\nbusiness logic. Additionally, they will have a method for subscribing and being notified about changes to cookie preferences in real-time.\\nFor Scala code, the situation is more problematic. Services will only have the raw userConsent cookie itself passed down in the `Cookie` header.\\nTo avoid teams having to copy and paste substantial quantities of Scala boilerplate replicating the existing Javascript logic\\nacross services, should we look to centralise this logic so it can be more easily maintained across the platform and if so how?\\nIdeally we would like a Scala controller to be able to do something like:\\n```scala\\nprivate val themeCookie = Cookie(name = \"theme\", value = \"dark\", maxAge = Some(365.days.toSeconds.toInt))\\nprivate def allowedCookies(implicit request: RequestHeader) =\\nif (userPreferences.preferences.settings) Seq(themeCookie) else Seq.empty\\nval termsAndConditions: Action[AnyContent] = Action { implicit request =>\\nOk(termsAndConditionsPage()).withCookies(allowedCookies: _*)\\n}\\n```\\n## Decision Drivers\\n* the need to keep repetitious boilerplate to a minimum, with changes needed of the order of a single line code change.\\n* the need to centralise the maintenance of the cookie reading logic, so it is easier to roll out changes\\n* the importance of having loosely coupled microservices and avoidance of unnecessarily introducing additional hard\\nnetworked dependencies into microservices\\n* the need for simplicity and for the fewest number of moving parts\\n* the preference for not introducing a requirement for microservices to add additional libraries\\n\n## Decision\n* the need to keep repetitious boilerplate to a minimum, with changes needed of the order of a single line code change.\\n* the need to centralise the maintenance of the cookie reading logic, so it is easier to roll out changes\\n* the importance of having loosely coupled microservices and avoidance of unnecessarily introducing additional hard\\nnetworked dependencies into microservices\\n* the need for simplicity and for the fewest number of moving parts\\n* the preference for not introducing a requirement for microservices to add additional libraries\\nChosen option 3: because it meets most if not all the above criteria.\\n### Positive Consequences\\n* Services can determine acceptance or rejection of non-essential cookies in a single line of code.\\n* Changes to the cookie parsing logic can be rolled out relatively easily by #team-plat-ui\\n* Consuming services do not rely on any additional networked dependencies\\n* No additional libraries would be needed (assuming the micro-library is added to play-frontend-hmrc or bootstrap-play)\\n### Negative Consequences\\n* Any version change to the userConsent cookie has the potential to break functionality relying on non-essential cookies\\nin consuming services. We may need to look at introducing the ability to have the concept of minor versions or only increment the version for breaking\\nchanges e.g. removal or change to the meaning of a cookie type.\\n\n\n## Context\\nOur service-operator allows service teams to provision various AWS services by\\ndeclaratively defining resources and submitting them via the kubernetes api.\\nSome of these resources require IAM to authorise how the provisioned service\\ncan be used. The types of actions that can be performed on.\\n#### Example\\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\\n```\\n---\\napiVersion: storage.govsvc.uk/v1beta1\\nkind: S3Bucket\\nmetadata:\\nname: s3-bucket-sample\\nspec:\\naws:\\nLifecycleRules:\\n- Expiration: 90days\\nVersioning:\\nEnabled: true\\n```\\nIn order to access a provisioned bucket via the the AWS SDK users will require\\nan IAM role/policy that allows access.\\nWe want things like bucket ACL, versioning configuration and lifecycle policy\\nto be defined declaratively via the resource manifest (see example above), and continuously managed\\nby the service operator.\\nWe want users of the provisioned bucket to be able to read back all\\nconfiguration, and be able to fully utilise the specific bucket for reading,\\nwriting and managing their objects within the provisioned bucket, but we want\\nto avoid giving permissions to users that could cause conflicts with the\\nproperties that are managed by the service operator's reconcile loop.\\nFor example, given the example manifest above, we would like to avoid giving\\npermissions that would allow a user to alter the Expiration LifeCycleRules,\\nsince any changes the user made would be periodically overwritten by the\\nservice operator's reconciliation.\\n\n## Decision\n", "response": "<pad> * the need to maintain a consistent set of services and servicesninstancesnto allow teams to easily test changes to the platform.n* the need to maintain a consistent set of services and servicesninstancesnto allow teams to easily test changes to the platform.nChosen option 3: because it meets most if not all the above criteria.n### Positive Consequencesn* Services can determine acceptance or rejection of non-essential cookies in a single line of code.n* Changes to the cookie parsing logic can be rolled out relatively easily by #team-plat-uin* Consuming services do not rely on any additional networked dependenciesn* No additional libraries would be needed (assuming the micro-library is added to play-frontend-hmrc or bootstrap-play)n### Negative Consequencesn* Any version change to the userConsent cookie has the potential to break functionality relying on non-essential cookies.nIn consuming services, we want to avoid giving permissions to users that could cause conflicts with the properties that are managed by the service operator's reconciliation loop.nFor example, given the example above, we would like to avoid giving permissions that would allow a user to alter the Expiration LifecycleRules,nsince any changes the user made would be periodically overwritten by the service operator's reconciliation loop.n", "matched_ids": [3895, 3523], "time": "0:01:20.294979", "post": "## Decision\n* the need to maintain a consistent set of services and servicesninstancesnto allow teams to easily test changes to the platform \n * the need to maintain a consistent set of services and servicesninstancesnto allow teams to easily test changes to the platform \n Chosen option 3: because it meets most if not all the above criteria \n ### Positive Consequencesn* Services can determine acceptance or rejection of non-essential cookies in a single line of code \n * Changes to the cookie parsing logic can be rolled out relatively easily by #team-plat-uin* Consuming services do not rely on any additional networked dependenciesn* No additional libraries would be needed (assuming the micro-library is added to play-frontend-hmrc or bootstrap-play)n### Negative Consequencesn* Any version change to the userConsent cookie has the potential to break functionality relying on non-essential cookies \n In consuming services, we want to avoid giving permissions to users that could cause conflicts with the properties that are managed by the service operator's reconciliation loop \n For example, given the example above, we would like to avoid giving permissions that would allow a user to alter the Expiration LifecycleRules,nsince any changes the user made would be periodically overwritten by the service operator's reconciliation loop \n "}
{"context": "## Context\\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\\n", "input": "## Context\\nWe need a way to provide live updates to the front-end for ingest sheet validation,\\ningest status, etc.\\n\n## Decision\nWe will use the [WebSocket API](https://www.w3.org/TR/websockets/) via [Phoenix Channels](https://hexdocs.pm/phoenix/channels.html) to enable real-time communication between the client and server.\\n\n\n## Context\\nThe tool provides a number of related commands to create\\nand manipulate architecture decision records.\\nHow can the user find out about the commands that are available?\\n\n## Decision\nThe tool defines a single command, called `adr`.\\nThe first argument to `adr` (the subcommand) specifies the\\naction to perform.  Further arguments are interpreted by the\\nsubcommand.\\nRunning `adr` without any arguments lists the available\\nsubcommands.\\nSubcommands are implemented as scripts in the same\\ndirectory as the `adr` script.  E.g. the subcommand `new` is\\nimplemented as the script `adr-new`, the subcommand `help`\\nas the script `adr-help` and so on.\\nHelper scripts that are part of the implementation but not\\nsubcommands follow a different naming convention, so that\\nsubcommands can be listed by filtering and transforming script\\nfile names.\\n\n\n## Context\\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\\n\n## Decision\n", "response": "[ERROR]: An error occurred while generating decision. Please rate anything for this decision.HTTPSConnectionPool(host='b2w8w5mmlr342ulg.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=300)", "matched_ids": [3845, 3565], "time": "0:05:00.833834", "post": "## Decision\n[ERROR]: An error occurred while generating decision. Please rate anything for this decision.HTTPSConnectionPool(host='b2w8w5mmlr342ulg.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=300)"}
{"context": "## Context and Problem Statement\\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\\n## Decision Drivers <!-- optional -->\\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible/when if needed.\\n", "input": "## Context and Problem Statement\\nIn a document, some words have to be put in quotes. How to direct latex to enquote a word?\\n## Decision Drivers\\n* Automatic correct typographical layout\\n* Less effort for the user\\n* Supported by prominent LaTeX editors (overleaf, vs.code, ...)\\n* Supported by standard LaTeX environments\\n\n## Decision\n* Automatic correct typographical layout\\n* Less effort for the user\\n* Supported by prominent LaTeX editors (overleaf, vs.code, ...)\\n* Supported by standard LaTeX environments\\nChosen option: \"Use single quote (\") to enquote text\", because resolves all forces.\\nWe accept that special hyphenation instructions such as `application\"=specific` do not work anymore.\\n<!-- markdownlint-disable-file MD013 -->\\n\n\n## Context and Problem Statement\\nWe want a message broker so the devices and other applications can communicate with the backend.\\nWhat broker and protocol should we use?\\n## Decision Drivers <!-- optional -->\\n- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\n\n## Decision\n- Applicability regarding IoT projects : low-resources clients, etc\\n- Possibility to use it to stream frames/images coming from cars cameras\\n- Ease of deployment on Kubernetes\\n- Existing knowledge of the team\\nChosen option: **[RabbitMQ](TODO) with [MQTT plugin](https://www.rabbitmq.com/mqtt.html)**, because:\\n- It is already well-known among the team\\n- It has some [existing \"official\" Helm chart](https://github.com/helm/charts/tree/master/stable/rabbitmq)\\n- It seems like a good fit to iterate fast\\nWe **do not exclude switching to another MQTT broker such as VerneMQ in the future**, depending on our ability to dsitribute it cleanly on Kubernetes.\\nWe also **do not exclude using another broker such as Kafka or NATS for appropriate use cases**.\\n\n\n## Context and Problem Statement\\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\\n## Decision Drivers <!-- optional -->\\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible/when if needed.\\n\n## Decision\n", "response": "[ERROR]: An error occurred while generating decision. Please rate anything for this decision.HTTPSConnectionPool(host='b2w8w5mmlr342ulg.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=300)", "matched_ids": [2333, 979], "time": "0:05:00.912015", "post": "## Decision\n[ERROR]: An error occurred while generating decision. Please rate anything for this decision.HTTPSConnectionPool(host='b2w8w5mmlr342ulg.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=300)"}
{"context": "## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state/model.\\n", "input": "## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nWhen it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.  It could be more sensible to only verify the state/model.\\nBut, we had a small problem described in ADR-0002.\\n\n## Decision\nWe will assert both against the session in the provider **and** against the DOM elements to make sure the application view is updating properly.\\n\n\n## Context\\nAccessing ENV directly without a wrapper is limited and can introduce problems.\\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https://github.com/laserlemon/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\\n\n## Decision\nUse DotEnv to load our environment variables.\\n\n\n## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state/model.\\n\n## Decision\n", "response": "<pad> We will assert both against the session in the provider **and** against the DOM elements to make sure the application view is updating properly.n", "matched_ids": [2814, 2390], "time": "0:01:01.045880", "post": "## Decision\nWe will assert both against the session in the provider **and** against the DOM elements to make sure the application view is updating properly \n "}
{"context": "## Context\\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\\nAdditionally Geocoder can be used to verify addresses that are manually entered.\\n- [Geocoder](https://www2.gov.bc.ca/gov/content/data/geographic-data-services/location-services/geocoder)\\n- [Data BC](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service)\\n- [API Swagger](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service/resource/40d6411e-ab98-4df9-a24e-67f81c45f6fa/view/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\\n- [Developer API Keys](https://github.com/bcgov/gwa/wiki/Developer-Guide#developer-api-keys)\\n- API Host = `https://geocoder.api.gov.bc.ca`\\n", "input": "## Context\\nGOV.UK Docker has services defined for various databases, such as PostgreSQL,\\nMongoDB and Elasticsearch. These have their data persisted in\\n[Docker volumes][volume] defined [for each service][defined-volumes].\\nWhen GOV.UK Docker updated from [MongoDB 2.4 to 3.6][mongodb-update] users\\nbegan receiving cryptic errors when trying to run applications that depended\\non MongoDB. This was because they had a volume with data structured for MongoDB\\n2.4 whereas MongoDB was expecting this to be structured for 3.6.\\nWe felt that this upgrade path was more confusing and difficult than it needed\\nbe and wanted to improve it for the future.\\n[volume]: https://docs.docker.com/storage/volumes/\\n[defined-volumes]: https://github.com/alphagov/govuk-docker/blob/cb124a3a1d3353e777d4f777d77f03f93415d415/docker-compose.yml#L5-L10\\n[mongodb-update]: https://github.com/alphagov/govuk-docker/pull/356\\n\n## Decision\nWe decided that we would rename database services, and their respective\\nvolumes, to reflect the software version number. For example the `mongo`\\nservice, and namesake volume, have been renamed to `mongo-3.6`.\\nThe precision of the version number is chosen based on the backwards\\ncompatibility between versions of the software. For example to upgrade from\\nMySQL 5.5 to 5.6 requires an upgrade script and thus would cause problems for\\nGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not. For\\nservices that aren't sensitive to minor versions, such as Elasticsearch, we\\nhave specified the service and volume with respective to a major version:\\n`elasticsearch-6`.\\n\n\n## Context\\nWe need to provide a nicely ergonomic API to programs using Converge,\\nand we wanted to adhere closely to the Clojure Way (TM).  We decided\\nto use the Atom API of swap!/reset!/deref.\\nWhen altering the value of the ConvergentRef via swap!/reset!, we need\\nto create a covering set of Ops to describe the change being made.  We\\nconsidered the following alternatives:\\n1. Using `clojure.data/diff` on old-value and new-value, with metadata\\nfor Id accounting\\n2. Using [`differ`](https://github.com/Skinney/differ) on old-value\\nand new-value, with metadata for Id accounting\\n3. Using [`editscript`](https://github.com/juji-io/editscript) on\\nold-value and new-value, with metadata for Id accounting\\n4. Implementing the Map/Vector interfaces a la\\n[Schism](https://github.com/aredington/schism/blob/master/src/schism/impl/types/nested_map.cljc)\\nand do our tracking and accounting within those implementation\\n\n## Decision\nOn initial analysis, it appears that #4 above is flawed, as\\nimplementing our own tracking/accounting nested map/list types\\nwouldn't maintain the necessary context to translate to an opset.\\nSo that left us with a strategy based on diffing/patching. Based on\\nanalysis in [this blog\\npost](https://juji.io/blog/comparing-clojure-diff-libraries/), as well\\nas our own usage testing with `clojure.data/diff`, `differ`, and\\n`editscript`, we have decided to use\\n[`editscript`](https://github.com/juji-io/editscript) for creating\\nopset patches.\\n\n\n## Context\\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\\nAdditionally Geocoder can be used to verify addresses that are manually entered.\\n- [Geocoder](https://www2.gov.bc.ca/gov/content/data/geographic-data-services/location-services/geocoder)\\n- [Data BC](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service)\\n- [API Swagger](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service/resource/40d6411e-ab98-4df9-a24e-67f81c45f6fa/view/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\\n- [Developer API Keys](https://github.com/bcgov/gwa/wiki/Developer-Guide#developer-api-keys)\\n- API Host = `https://geocoder.api.gov.bc.ca`\\n\n## Decision\n", "response": "<pad> We decided that we would rename database services, and their respectivenvolumes, to reflect the software version number. For example the mongodb-update, usersnbegan receiving cryptic errors when trying to run applications that dependednon MongoDB. This was because they had a volume with data structured for MongoDBn2.4 whereas MongoDB was expecting this to be structured for 3.6.nWe felt that this upgrade path was more confusing and difficult than it needed to be and wanted to improve it for the future.n[volume]: https://docs.docker.com/alphagov/govuk-docker/blob/cb124a3a1d3353e777d4f777d77f03f93415d415/docker-compose.yml#L5-L10n[mongodb-update]: https://github.com/alphagov/govuk-docker/pull/356n", "matched_ids": [1666, 978], "time": "0:00:58.107414", "post": "## Decision\nWe decided that we would rename database services, and their respectivenvolumes, to reflect the software version number. For example the mongodb-update, usersnbegan receiving cryptic errors when trying to run applications that dependednon MongoDB. This was because they had a volume with data structured for MongoDBn2.4 whereas MongoDB was expecting this to be structured for 3.6 \n We felt that this upgrade path was more confusing and difficult than it needed to be and wanted to improve it for the future \n [volume]: https://docs.docker.com/alphagov/govuk-docker/blob/cb124a3a1d3353e777d4f777d77f03f93415d415/docker-compose.yml#L5-L10n[mongodb-update]: https://github.com/alphagov/govuk-docker/pull/356n"}
{"context": "## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n", "input": "## Context\\nIn [adr-008][1] we agreed on focusing only on English pages.\\nWe have noticed during the last few months that this restriction is causing more\\nissues than benefits, because:\\n- We have Content Items with the same `content_id` and different `locale`s.\\n- We needed to handle edge cases when retrieving the information from Publishing API in order to work around only retrieving English language content.\\n\n## Decision\nTrack Content metrics of all Content Item regardless of their locale.\\n\n\n## Context\\nWe maintain a lot of [terraform modules] which teams use to manage AWS\\nresources for their namespaces.\\nIn the past, lettings different namespaces use different versions of terraform\\nmodules has caused problems because there has not always been a clear upgrade\\npath from a module that is several versions old, to the latest version.\\nIn these cases, it would have been easier to have upgraded every namespace when\\nthe significant change was made to the module, because at that point in time\\nthe relevant knowledge and understanding are fresh. Leaving it until later\\nmakes the whole process much more difficult.\\n\n## Decision\nWe decided to ensure that all namespaces are always using the latest version of\\nevery module.\\n\n\n## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n\n## Decision\n", "response": "<pad> The decision was taken to use the Security property to store the data.n", "matched_ids": [1862, 621], "time": "0:00:21.202053", "post": "## Decision\nThe decision was taken to use the Security property to store the data \n "}
{"context": "## Context and Problem Statement\\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\\n```\\n<xs:element name=\"take\" type=\"xs:integer\">\\n```\\nor with attributes as:\\n```\\n<xs:element name=\"take\">\\n<xs:complexType>\\n<xs:attribute name=\"value\"/>\\n</xs:complexType>\\n</xs:element>\\n```\\n## Decision Drivers\\n* consistency\\n* expressiveness\\n", "input": "## Context\\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\\n```\\nstruct tof\\n{\\nusing distance_t = uint16_t;\\nusing cb_t = stdext::inplace_function<void(distance_t)>;\\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\\nenum class mode\\n{\\ndefaultRange = 0,\\nshortRange,\\nmedRange,\\nlongRange,\\n};\\n}''\\n```\\nInterface classes would inherit from these structs:\\n```\\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\\n```\\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\\n```\\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\\n```\\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\\n\n## Decision\nEach driver interface class and any related types are to be stored in a separate namespace per driver type. This keeps related types tied together, improves our generated documentation, and provides improved names for classes and types.\\n\n\n## Context\\nAs we start pulling back data from multiple searches, we run the risk of ending up with multiple parameters (fields, targetFields, targetFields2 etc. etc.)\\nA better approach is to pass in specifications using a nested hash or similar so fields can be targeted to individual searches e.g.\\n```\\n{\\n\"genes\":[\"id\",\"name\",\"description\"],\\n\"variations\":[\"\"]\\n}\\n```\\nHowever, for simplicity, this needs to support simple field specifications too e.g.\\n```\\n[\"id\",\"name\",\"description\"]\\n```\\nor even\\n```\\n\"id\",\"name\",\"description\"\\n```\\n\n## Decision\nMixing arbitrary arrays and hashes in Java is not well supported and requires casts etc. We will instead create a specific class, `org.ensembl.genesearch.QueryOutput`, capable of representing this kind of nested structure.  `build()` methods will be provided to support parsing various specifications.\\n\n\n## Context and Problem Statement\\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\\n```\\n<xs:element name=\"take\" type=\"xs:integer\">\\n```\\nor with attributes as:\\n```\\n<xs:element name=\"take\">\\n<xs:complexType>\\n<xs:attribute name=\"value\"/>\\n</xs:complexType>\\n</xs:element>\\n```\\n## Decision Drivers\\n* consistency\\n* expressiveness\\n\n## Decision\n", "response": "<pad> * consistencyn* expressivenessn", "matched_ids": [3027, 3284], "time": "0:00:19.595969", "post": "## Decision\n* consistencyn* expressivenessn"}
{"context": "Context\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https://github.com/openedx/event-routing-backends/blob/f430d4cf58bdab01e42fcc944241898606873d82/event_routing_backends/processors/mixins/base_transformer.py#L139\\n", "input": "Context\\n-------\\n`OEP-26 <https://open-edx-proposals.readthedocs.io/en/latest/oep-0026-arch-realtime-events.html>`__\\nconsists of the following components:\\n-  Asynchronous Routing Backend\\n-  Regular-expressions based filter processor\\n-  IMS Caliper transformer\\n-  xAPI transformer\\n-  Router to forward events\\nKeeping all of these components in one repo will make the repository\\nunnecessarily tangled since these additional components are not required\\nby the core app for its functionality.\\nDecision\\n--------\\nAmong the components listed above, Asynchronous Routing Backend and the\\nregular-expressions filter will be added in the core app (i.e.\\n`event-tracking <https://github.com/openedx/event-tracking>`__) while the\\nother components, i.e. Caliper transformer backend, xAPI transformer\\nbackend and router, will be added in the current repo.\\nBy keeping the concrete backends separate from the code, we can have\\nonly the core plugin interface for event tracking in its repository.\\nConsequences\\n------------\\nThe code will be decoupled and components can be used independently if\\nrequired.\\nRejected Alternatives\\n---------------------\\n**Add the routing backends to the event-tracking repository**\\nThis idea was rejected to keep the core event-tracking repository clean\\nand independent. The core repo is functional on its own and any\\npluggable extensions should be implemented separately.\\n\n## Decision\n--------\\nAmong the components listed above, Asynchronous Routing Backend and the\\nregular-expressions filter will be added in the core app (i.e.\\n`event-tracking <https://github.com/openedx/event-tracking>`__) while the\\nother components, i.e. Caliper transformer backend, xAPI transformer\\nbackend and router, will be added in the current repo.\\nBy keeping the concrete backends separate from the code, we can have\\nonly the core plugin interface for event tracking in its repository.\\nConsequences\\n------------\\nThe code will be decoupled and components can be used independently if\\nrequired.\\nRejected Alternatives\\n---------------------\\n**Add the routing backends to the event-tracking repository**\\nThis idea was rejected to keep the core event-tracking repository clean\\nand independent. The core repo is functional on its own and any\\npluggable extensions should be implemented separately.\\n\n\n## Context and Problem Statement\\nTracking consent reads the PLAY_LANG cookie to determine whether to render the banner in Welsh. PLAY_LANG is the cookie\\nused by the Play Framework to persist a user's language preferences. It will be set to 'cy'\\nwhen a user has selected Welsh using the language toggle in MDTP services using the Play Framework.\\nTeams are increasingly setting PLAY_LANG to HttpOnly in an attempt to get green ZAP tests, even though there are no\\nknown security concerns around keeping PLAY_LANG as a normal cookie. Setting a cookie to\\nHttpOnly makes it unreadable within the client-side Javascript code that renders the tracking consent banner. The result\\nof this is that the banner will not be translated into Welsh for these services.\\nA related issue is that PLAY_LANG is not set for classic services written in Java, which means a Welsh version of the banner is not\\ncurrently available for classic services.\\nIt is worth noting that the only other known instance of our reading PLAY_LANG using Javascript is in the assets-frontend\\n[timeout dialog](https://github.com/hmrc/assets-frontend/blob/97c638289e23bee255ac30724a8572c6efa96817/assets/patterns/help-users-when-we-time-them-out-of-a-service/timeoutDialog.js#L14) timeout dialog. All the new govuk-frontend and hmrc-frontend components use data attributes instead.\\nShould we remove the reading of PLAY_LANG in tracking consent and accept a data-language attribute instead?\\n## Decision Drivers\\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\n\n## Decision\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\nChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'\\ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG into Zap tests. We also agreed that our frontend\\ncomponents should be consistent in their treatment of language until such time as we are able to provide an\\nalternative approach that works for all components.\\n### Positive Consequences\\n* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner\\n* Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings/errors\\n* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etc\\n### Negative Consequences\\n* We will need to make a small change to tracking consent and communicate this change to teams\\n* Teams will need to upgrade to the latest version of play-ui/play-frontend-hmrc to get language translations\\n* Teams not using the tracking consent helpers will need to add a data-attribute for the language\\n\n\nContext\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https://github.com/openedx/event-routing-backends/blob/f430d4cf58bdab01e42fcc944241898606873d82/event_routing_backends/processors/mixins/base_transformer.py#L139\\n\n## Decision\n", "response": "<pad> --------nThe core app will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to generate edx events.nThe edx-frontend will be a edx-frontend application that will be used to", "matched_ids": [4479, 3526], "time": "0:02:30.421029", "post": "## Decision\n--------nThe core app will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to generate edx events \n The edx-frontend will be a edx-frontend application that will be used to"}
{"context": "# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https://www.gov.uk/service-manual/making-software/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop / HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n", "input": "## Context\\nIn [ADR-0002][adr-0002] we outlined the overall technical approach for the Data\\nSubmission Service, and highlighted the components which we expect will be\\ndeveloped.\\nThis ADR focusses on the technology choice for building a portion of the\\nfeatures. In particular:\\n1. File transformation service - a small service for extracting data from an\\nuploaded file, transforming it into a useful format and storing it via the API\\n(a process we're currently calling 'ingest').\\n1. Data validation service - a service for validating the data provided by\\nsuppliers and calculating the appropriate management fee.\\n### Features of these services\\nThese services have to handle large peaks in traffic, but will be rarely used\\nfor a large part of the month.\\nThe file transformation service will need to handle multiple files, of different\\nformats, at the same time, some with well over 100,000 rows of data.\\nThe data validation service will need to perform validation and calculations on\\neach row within the data.\\nTo provide a good user-experience, we want these services to operate quickly and\\nprovide feedback to users.\\nThese services will need to apply custom rules based on the framework the report\\napplies to. For example the fields of data provided for G-Cloud 9 are different\\nfrom General Legal Services, and the process for calculating the management fee\\nis different for Rail Legal Services and Courier Services.\\n### Serverless\\nServerless technologies are designed to run code without the need to manage\\ninfrastructure, which allows it to scale easily. Generally, you are only billed\\nfor the time it takes to run the code - usually charged in fractions of a\\nsecond.\\nDeveloping these services to be hosted as a serverless function would allow us,\\nin theory, to infinitely scale the file ingest and validation parts of the\\nservice and only pay for what is actually needed.\\nEach of the main cloud providers has their own implementation of a serverless\\nhosting option including Azure Functions, Google Cloud Functions and AWS Lambda.\\n\n## Decision\nWe will use AWS Lambda for the ingest and data validation processes.\\nWe can deploy a different Lambda function for each file type we expect to\\ningest, and split the data validation scripts into easy-to-maintain functions.\\n\n\n# Context\\nAs someone responsible for ensuring accurate data is published by the\\ngovernment, I need to ensure that people authoring content can only\\nview and edit things that they own, so that there are no embarrassing\\nupdates published.\\nWe need to ensure that datasets and dashboards can only be edited by\\nthe right people.\\nWe currently have signonotron2 accounts for all of the people using\\nthis application. That will continue to be in place for the future.\\nWe need a way of restricting which signon accounts have access to\\ndifferent entities within the admin application.\\nWe have tried to model this in various ways.\\nAs someone working at the DVLA, should I have the ability to edit\\nall of the DVLAs dashboards? The answer to that is no.\\nWe've had some discussions with our user population, but what is\\nactually good still isn't known at this point. So we are going to\\nincrementally do this functionality, as a series of vertical slices,\\nand accept that we will need to refactor, rework and do data\\nmigrations as we learn more about the problem space and how best to\\nmeet those needs.\\nInitially, we proposed using custom Django Managers in stagecraft to\\nimplement this. We don't think this is the right approach; it smells\\ntoo complex.\\nA simpler start point will be to define the URLs (resources) and\\nhave permissions per resource.\\n# Decision\\nDefine the matrix of URLs, and map permissions per resource.\\n# Status\\nAccepted.\\n# Consequences\\nTrading shipping a thing with future technical debt for now.\\n\n## Decision\nDefine the matrix of URLs, and map permissions per resource.\\n# Status\\nAccepted.\\n# Consequences\\nTrading shipping a thing with future technical debt for now.\\n\n\n# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https://www.gov.uk/service-manual/making-software/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop / HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n\n## Decision\n", "response": "<pad> We will use AWS Lambda for the ingest and data validation processes.nWe can deploy a different Lambda function for each file type we expect toningest, and split the data validation scripts into easy-to-maintain functions.n# StatusnAccepted.n# ConsequencesnTrading shipping a thing with future technical debt for now.n", "matched_ids": [2046, 3394], "time": "0:00:31.972259", "post": "## Decision\nWe will use AWS Lambda for the ingest and data validation processes \n We can deploy a different Lambda function for each file type we expect toningest, and split the data validation scripts into easy-to-maintain functions \n # StatusnAccepted \n # ConsequencesnTrading shipping a thing with future technical debt for now \n "}
{"context": "## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https://www.w3.org/WAI/WCAG21/Understanding/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https://github.com/hmrc/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n", "input": "## Context and Problem Statement\\nNearly all services running on MDTP need to include a link to their landing page in the\\nGOV.UK header. Exceptionally, frontend microservices may either need to omit the service name\\nor be able to configure it based on the path. For example, in the case of shared frontend\\nmicroservices (contact-frontend, accessibility-statement-frontend) or microservices that host\\nmore than one public-facing service (view-external-guidance-frontend, gform-frontend).\\nWhen integrating with play-frontend-hmrc we want to\\nminimise boilerplate for service teams while providing flexibility to cope with edge\\ncases.\\nServices using the govukLayout component in play-frontend-govuk and not overriding the `headerBlock`\\nparameter, specify the service name in the\\n`service.name` message in `conf/messages`, providing any Welsh translation in `conf/messages.cy`.\\nBased on a Github search, most services overriding the headerBlock with, for example, `hmrcHeader`\\nor `govukHeader` are passing the service name from `messages('service.name')`. However, this\\nmechanism is awkward for services not requiring a service name \u2013 they have to override with a blank message \u2013\\nand services needing more than one are unable to use this functionality. This pattern\\nassumes a 1:1 correspondence between frontend microservices and public-facing\\ntax services.\\nShould we continue to support this pattern to reduce boilerplate for the majority of frontend\\nservices, insist on explicitly passing the service name or support some combination of the above?\\n## Decision Drivers\\n* Minimising of boilerplate and lines of code needed in frontend microservices\\n* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with\\npublic-facing services.\\n* The need to keep things simple and unsurprising\\n\n## Decision\n* Minimising of boilerplate and lines of code needed in frontend microservices\\n* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with\\npublic-facing services.\\n* The need to keep things simple and unsurprising\\nChosen option 2 because reducing boilerplate for the majority of services is important, we should support\\nmore unusual services and not dictate an architecture that assumes a 1:1 correspondence\\nbetween frontend microservices and public-facing services.\\n\n\n## Context and Problem Statement\\nTracking consent reads the PLAY_LANG cookie to determine whether to render the banner in Welsh. PLAY_LANG is the cookie\\nused by the Play Framework to persist a user's language preferences. It will be set to 'cy'\\nwhen a user has selected Welsh using the language toggle in MDTP services using the Play Framework.\\nTeams are increasingly setting PLAY_LANG to HttpOnly in an attempt to get green ZAP tests, even though there are no\\nknown security concerns around keeping PLAY_LANG as a normal cookie. Setting a cookie to\\nHttpOnly makes it unreadable within the client-side Javascript code that renders the tracking consent banner. The result\\nof this is that the banner will not be translated into Welsh for these services.\\nA related issue is that PLAY_LANG is not set for classic services written in Java, which means a Welsh version of the banner is not\\ncurrently available for classic services.\\nIt is worth noting that the only other known instance of our reading PLAY_LANG using Javascript is in the assets-frontend\\n[timeout dialog](https://github.com/hmrc/assets-frontend/blob/97c638289e23bee255ac30724a8572c6efa96817/assets/patterns/help-users-when-we-time-them-out-of-a-service/timeoutDialog.js#L14) timeout dialog. All the new govuk-frontend and hmrc-frontend components use data attributes instead.\\nShould we remove the reading of PLAY_LANG in tracking consent and accept a data-language attribute instead?\\n## Decision Drivers\\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\n\n## Decision\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\nChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'\\ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG into Zap tests. We also agreed that our frontend\\ncomponents should be consistent in their treatment of language until such time as we are able to provide an\\nalternative approach that works for all components.\\n### Positive Consequences\\n* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner\\n* Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings/errors\\n* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etc\\n### Negative Consequences\\n* We will need to make a small change to tracking consent and communicate this change to teams\\n* Teams will need to upgrade to the latest version of play-ui/play-frontend-hmrc to get language translations\\n* Teams not using the tracking consent helpers will need to add a data-attribute for the language\\n\n\n## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https://www.w3.org/WAI/WCAG21/Understanding/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https://github.com/hmrc/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n\n## Decision\n", "response": "<pad> * The need to minimise boilerplate and lines of code needed in frontend microservicesn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence withnpublic-facing services.n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, tonavoid services having to upgrade a second time.n* The preference for not advising teams to add exemptions to ZAP tests across MDTPn* The preference for consistency in language settings across our frontend componentsnChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG intonZAP tests. We also agreed that our frontendncomponents should be consistent in their treatment of language until such time as we are able to provide annalternative approach that works for all components.n### Positive Consequencesn* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner.n* Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings/errorsn* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etcn### Negative Consequencesn* We will need to make a small change to tracking consent and communicate this change to teamsn* Teams will need to upgrade to the latest version of play-ui/play-frontend-hmrc to get language translationsn* Teams not using the tracking consent helpers will need to add a data-attribute for the languagen", "matched_ids": [559, 3526], "time": "0:01:25.111041", "post": "## Decision\n* The need to minimise boilerplate and lines of code needed in frontend microservicesn* Providing flexibility for frontend microservices that do not have a 1:1 correspondence withnpublic-facing services \n * The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, tonavoid services having to upgrade a second time \n * The preference for not advising teams to add exemptions to ZAP tests across MDTPn* The preference for consistency in language settings across our frontend componentsnChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG intonZAP tests. We also agreed that our frontendncomponents should be consistent in their treatment of language until such time as we are able to provide annalternative approach that works for all components \n ### Positive Consequencesn* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner \n * Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings/errorsn* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etcn### Negative Consequencesn* We will need to make a small change to tracking consent and communicate this change to teamsn* Teams will need to upgrade to the latest version of play-ui/play-frontend-hmrc to get language translationsn* Teams not using the tracking consent helpers will need to add a data-attribute for the languagen"}
{"context": "## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n", "input": "## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n\n## Decision\nI will always focus on simplicity, readaility and the ease of understanding\\nany code or structure.\\n\n\n## Context\\nThere is an initiative about questioning the technical vocabulary we use, and avoiding some words that are widely used\\nand may offend people. This is a project that adheres to this initiative, therefore...\\n\n## Decision\nRemove all existing technical vocabulary that might be offensive, and prevent those terms to be added in the future.\\nFor instance, the use of \"master/slave\" replaced by \"main/replica\" (or similar), or \"whitelist/blacklist\" by\\n\"safelist/blocklist\" (or similar).\\n\n\n## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n\n## Decision\n", "response": "<pad> I will always focus on simplicity, readaility and the ease of understandingnany code or structure.n", "matched_ids": [1416, 1047], "time": "0:00:19.886581", "post": "## Decision\nI will always focus on simplicity, readaility and the ease of understandingnany code or structure \n "}
{"context": "## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n", "input": "## Context\\nThe key component of this project is a 'Bicycle Network Analysis' task which is run on an arbitrary, user-provided neighborhood boundary. This task performs the following actions:\\n- Import neighborhood boundary into a PostgreSQL database\\n- Download OSM extract for the provided neighborhood boundary + a buffer and import to PostgreSQL\\n- Download related census block and job data for the boundary and import to PostgreSQL\\n- Generate a network graph from the imported data\\n- Run a series of client-provided analyses on the graph and imported data\\n- Export relevant data to an external file store for archival\\n- Generate a tile set of the network graph for display on a web map\\nThe application will be configured with multiple organizations, and each organization can only run one analysis job at a time. A new analysis job triggered by a user of the organization will supersede any existing older analysis, which can be thrown away.\\nSince the analysis workflow is already a self-contained process, there are a few ways to trigger this job, and a few options for an asynchronous task queue. One option is to use Celery, a tool we are familiar with, to provide a known interface to trigger these analysis jobs. Another is to configure the analysis as an AWS ECS task, and have the application use the ECS API or Boto to start a new analysis.\\nCelery has multiple options for brokers:\\n| Broker | Advantages | Disadvantages |\\n| ------ | ---------- | ------------- |\\n| SQS | Cheap, easy to set up, now stable, provides configuration options to isolate environments | No result backend, [potential issues with result timeouts](http://docs.celeryproject.org/en/latest/getting-started/brokers/sqs.html#caveats) |\\n| Redis | Trivial to configure, can additionally be used as a results backend without further architecting | Key eviction issues, additional cost to run dedicated instance |\\nRunning the analysis via AWS Lambda was briefly considered, but the project dependencies and resources required are not conducive to that environment.\\n\n## Decision\nThe team will use Celery + SQS broker to manage the asynchronous analysis jobs. While Celery is not strictly necssary, it provides a potentially useful abstraction layer for triggering tasks, managing jobs and reporting errors. Celery also provides out of the box support for Django and allows us to write any peripheral task logic in Python. The SQS broker was chosen to keep the managed application architecture simple and reduce ongoing application stack costs. The team is familiar with an older version of the SQS broker used for the Cicero District Match project.\\n\n\n## Context and Problem Statement\\nAs part of PLATUI-1393, browser based acceptance tests were added that rely on `assets-frontend`. These acceptance tests\\nrun as part of the build in Jenkins, which means `ASSETS_FRONTEND` needs to be added as a parameter to the `build-jobs`\\njob builder `.withServiceManager` call. However, `assets-frontend` has specific logic that means all versions are started\\nvia Service Manager, that is multiple instances of `assets-frontend` will start, not just the latest release.\\nThere is logic in Service Manager to pass an `application.conf` file to start only required versions of `assets-frontend`,\\nhowever as we are not using Service Manager to start `contact-frontend` itself, this logic does not get called. Therefore,\\nall verisons of `assets-frontend` start during when running a build, either after merge or as part of the PR builder for\\n`contact-frontend`.\\n## Decision Drivers\\n* `assets-frontend` is a deprecated service - we want to ensure the functionality using it does not break, but we are\\nreluctant to invest large amounts of time in it\\n* Service Manager is a Python application used heavily across the Platform and not owned by PlatUI, and we would like to\\navoid making changes to it if possible to support our workflow.\\n* Similarly the job builders in the `build-jobs` repo are heavily shared, and we would like to avoid making changes to\\n`build-jobs` if possible\\n\n## Decision\n* `assets-frontend` is a deprecated service - we want to ensure the functionality using it does not break, but we are\\nreluctant to invest large amounts of time in it\\n* Service Manager is a Python application used heavily across the Platform and not owned by PlatUI, and we would like to\\navoid making changes to it if possible to support our workflow.\\n* Similarly the job builders in the `build-jobs` repo are heavily shared, and we would like to avoid making changes to\\n`build-jobs` if possible\\nChosen option: \"Accept that builds of `contact-frontend` will be slower due to starting all versions of\\n`assets-frontend`\", because:\\n* It is a quick solution that does not involve further investigation to support a test case that we hope to deprecate\\n* The performance impact will slow down development in terms of PR building but will not affect local development\\n* This decision can be easily revisited if performance becomes a problem, i.e. by making no changes there will be no\\nrolling back needed if we remove the test or decide to change other services\\n### Positive Consequences\\n* Allows us to add browser-based acceptance tests for forms with `contact-frontend` relying on `assets-frontend`\\n* Does not involve further development time\\n* Proven to work in the build environment\\n### Negative Consequences\\n* Will definitely cause a negative impact on the time taken to build a release, or run a PR builder\\n\n\n## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n\n## Decision\n", "response": "<pad> * assets-frontend is a deprecated service - we want to ensure the functionality using it does not break, but we arenreluctant to invest large amounts of time in itn* Service Manager is a Python application used heavily across the Platform and not owned by PlatUI, and we would like to avoid making changes to it if possible to support our workflow.n* Similarly the job builders in the build-jobs repo are heavily shared, and we would like to avoid making changes tonbuild-jobs if possiblenChosen option: \"Accept that builds of contact-frontend will be slower due to starting all versions ofnassets-frontend\", because:n* It is a quick solution that does not involve further investigation to support a test case that we hope to deprecaten* The performance impact will slow down development in terms of PR building but will not affect local developmentn* This decision can be easily revisited if performance becomes a problem, i.e. by making no changes there will be nonrolling back needed if we remove the test or decide to change other servicesn### Positive Consequencesn* Allows us to add browser-based acceptance tests for forms with contact-frontend relying on assets-frontendn* Does not involve further development timen* Proven to work in the build environmentn### Negative Consequencesn* Will definitely cause a negative impact on the time taken to build a release, or run a PR buildern", "matched_ids": [709, 1753], "time": "0:01:21.712719", "post": "## Decision\n* assets-frontend is a deprecated service - we want to ensure the functionality using it does not break, but we arenreluctant to invest large amounts of time in itn* Service Manager is a Python application used heavily across the Platform and not owned by PlatUI, and we would like to avoid making changes to it if possible to support our workflow \n * Similarly the job builders in the build-jobs repo are heavily shared, and we would like to avoid making changes tonbuild-jobs if possiblenChosen option: \"Accept that builds of contact-frontend will be slower due to starting all versions ofnassets-frontend\", because:n* It is a quick solution that does not involve further investigation to support a test case that we hope to deprecaten* The performance impact will slow down development in terms of PR building but will not affect local developmentn* This decision can be easily revisited if performance becomes a problem, i.e. by making no changes there will be nonrolling back needed if we remove the test or decide to change other servicesn### Positive Consequencesn* Allows us to add browser-based acceptance tests for forms with contact-frontend relying on assets-frontendn* Does not involve further development timen* Proven to work in the build environmentn### Negative Consequencesn* Will definitely cause a negative impact on the time taken to build a release, or run a PR buildern"}
{"context": "## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n", "input": "## Context and Problem Statement\\nSpring HATEOAS includes several classes that encapsulate domain objects, adding support for links.\\nConstructing such objects, as well as adding the desired links to them is a common operation that\\nrequires entity-specific boilerplate code.\\nHow can duplicate code in nearly all controller methods be avoided?\\n## Decision Drivers <!-- optional -->\\n* Avoid duplicate code to create HATEOAS models\\n* Decouple link creation from normal entity logic\\n\n## Decision\n* Avoid duplicate code to create HATEOAS models\\n* Decouple link creation from normal entity logic\\nSeparate model assemblers were chosen, as the former option would require us to have a deep coupling between HATEOAS types\\nand our DTO classes.\\nDue to the assembler classes being initially only used for links they all reside in the `linkassembler` package.\\n\n\n## Context and Problem Statement\\nIn the course of implementing a class-based role metadata model we have also\\nreviewed options on how to design serialization infrastructure between wire\\nformats and the class model. In an initial attempt we have implemented\\nserialization on the metadata class (see option 1), but issues with inheritance\\nand calls for more flexibility have caused us to rethink this approach.\\n## Decision Drivers\\n* A class-based role metadata model (see ADR4) requires serialization routines\\nfrom and to wire formats\\n* TUF integrators may require custom serialization implementations for custom\\nwire formats\\n* Readability and simplicity of implementation for users and maintainers\\n* Recognizability of specification\\n\n## Decision\n* A class-based role metadata model (see ADR4) requires serialization routines\\nfrom and to wire formats\\n* TUF integrators may require custom serialization implementations for custom\\nwire formats\\n* Readability and simplicity of implementation for users and maintainers\\n* Recognizability of specification\\nChosen option: \"Compromise 2\", because implementing dict conversion as methods\\non a corresponding class is idiomatic and allows for well-structured code.\\nTogether with a separated serialization interface, it provides both ease of use\\nand maintenance, and full flexibility with regards to custom serialization\\nimplementations and wire formats.\\n\n\n## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n\n## Decision\n", "response": "<pad> * High availability/Fault tolerancen* Learning curven* Maintenance overheadn* Vendor lock-innChosen option: \"Graph Database Serialization\", because it provides both ease of usenand maintenance, and full flexibility with regards to custom serialization implementations and wire formats.n", "matched_ids": [696, 4254], "time": "0:00:27.483045", "post": "## Decision\n* High availability/Fault tolerancen* Learning curven* Maintenance overheadn* Vendor lock-innChosen option: \"Graph Database Serialization\", because it provides both ease of usenand maintenance, and full flexibility with regards to custom serialization implementations and wire formats \n "}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "input": "## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard\\nin this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n", "response": "<pad> We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).n", "matched_ids": [494, 1269], "time": "0:00:25.100048", "post": "## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) \n "}
{"context": "## Context\\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\\n", "input": "## Context\\nTEET is a large web application and will contain many namespaces and different types of namespaces:\\nviews, controllers, common UI components, etc.\\nThe way to structure namespaces affects how easy it is to find and refer to a given piece of code.\\nCommon structure makes it more predictable where developers can expect to find things.\\n\n## Decision\nWe use feature based grouping with layer suffix.\\n`teet.<feature>.<feature>-<layer>`\\nExample: `teet.search.search-view` and `teet.search.search-controller`\\nWhen referring to namespaces, use the last part of the name fully, e.g. `search-view`\\n```clojure\\n(ns teet.search.search-view\\n(:require [teet.search.search-controller :as search-controller]))\\n```\\nFeatures may use implementation specific sub-namespaces as seen fit.\\nNon-feature functionality, like common UI utilities, are placed under the layer, e.g. `teet.ui.panels`.\\n\n\n## Context\\nError handling is a very important process that is needed by every crate of the original Ockam rust library, as well as any third-party crate that was designed to supplement Ockam rust library (such as transport and vault implementations).\\nThere are multiple requirements to error handling:\\n- agile enough to be used in different cases\\n- portable to work in different environments with different constraints\\n- convenient for both library developer and library user\\n\n## Decision\nIn search of balance between above-mentioned requirements it has been decided that errors are handled in native to Rust way of using Result type, Error type for such Result should be either of ockam_core::Error of implement Into<ockam_core::Error>\\n\n\n## Context\\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\\n\n## Decision\n", "response": "<pad> We use feature based grouping with layer suffix.nteet.feature>.feature>-layer>nExample: teet.search.search-view and teet.search.search-controllernExample: teet.search.search-view and teet.search.search-controllernWhen referring to namespaces, use the last part of the name fully, e.g. search-viewnclojuren(ns teet.search.search-viewn(:require [teet.search.search-controller :as search-controller]))nnFeatures may use implementation specific sub-namespaces as seen fit.nNon-feature functionality, like common UI utilities, are placed under the layer, e.g. teet.ui.panels.n", "matched_ids": [4794, 4536], "time": "0:01:06.906950", "post": "## Decision\nWe use feature based grouping with layer suffix \n teet.feature>.feature>-layer>nExample: teet.search.search-view and teet.search.search-controllernExample: teet.search.search-view and teet.search.search-controllernWhen referring to namespaces, use the last part of the name fully, e.g. search-viewnclojuren(ns teet.search.search-viewn(:require [teet.search.search-controller :as search-controller]))nnFeatures may use implementation specific sub-namespaces as seen fit \n Non-feature functionality, like common UI utilities, are placed under the layer, e.g. teet.ui.panels \n "}
{"context": "## Context\\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\\nto not overload a server.\\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\\nallows, among others:\\n- Delaying retries upon MX delivery failure to a remote site.\\n- Throttling, which could be helpful for not being considered a spammer.\\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\\ndelays, purging the queue, etc.\\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\\nto interact with all its James servers, which is not friendly in a distributed setup.\\nDistributed James relies on the following third party softwares (among other):\\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\\n- **ObjectStorage** (Swift or S3) holds byte content.\\n", "input": "## Context\\nCassandra eventual consistency is all about \"replication\", but \"denormalization\" consistency needs\\nto be handled at the applicative layer (due to the lack of transactions in a NoSQL database).\\nIn the past we did set up \"Solve inconsistency\" tasks that can be assimilated to Cassandra repairs. Such\\ntasks, after being scheduled, ensure that the according entity denormalization is correctly denormalized.\\nHowever, the inconsistencies persist between runs. We experienced inconsistencies in some production platform\\nfor both the mailbox entity, and the mailbox counter entity (whose table structure is exposed in\\n[these](0020-cassandra-mailbox-object-consistency.md), [ADRs](0023-cassandra-mailbox-counters-inconsistencies.md)).\\nMonitoring is required to detect when to run them and is time consuming for the platform administrator.\\nGiven a large dataset, it could even be impossible to run such tasks in a timely fashion.\\nAnother classic eventual consistency mechanism, that enables auto-healing is read-repair. Randomly piggy back upon reads\\nsynchronous or asynchronous consistency checks. If missed a repair is performed.\\nIn order to achieve denormalization auto-healing, we thus need to implement \"applicative read repairs\".\\n\n## Decision\nProvide a Proof of concept for \"Applicative read repairs\" for the mailbox and mailbox-counters entities.\\nThis enables read path simplification (and performance enhancements) for the mailbox object.\\nIMAP LIST should not read mailbox counters. This information is uneeded and we should avoid paying the\\nprice of read repairs for this operation.\\nProvide a comprehensive documentation page regarding \"Distributed James consistency model\".\\n\n\n## Context\\nA given mail is often written to the blob store by different components. And mail traffic is heavily duplicated (several recipients receiving similar email, same attachments). This causes a given blob to often be persisted several times.\\nCassandra was the first implementation of the blobStore. Cassandra is a heavily write optimized NoSQL database. One can assume writes to be fast on top of Cassandra. Thus we assumed we could always overwrite blobs.\\nThis usage pattern was also adopted for BlobStore on top of ObjectStorage.\\nHowever writing in Object storage:\\n- Takes time\\n- Is billed by most cloud providers\\nThus choosing a right strategy to avoid writing blob twice is desirable.\\nHowever, ObjectStorage (OpenStack Swift) `exist` method was not efficient enough to be a real cost and performance saver.\\n\n## Decision\nRely on a StoredBlobIdsList API to know which blob is persisted or not in object storage. Provide a Cassandra implementation of it.\\nLocated in blob-api for convenience, this it not a top level API. It is intended to be used by some blobStore implementations\\n(here only ObjectStorage). We will provide a CassandraStoredBlobIdsList in blob-cassandra project so that guice products combining\\nobject storage and Cassandra can define a binding to it.\\n- When saving a blob with precomputed blobId, we can check the existence of the blob in storage, avoiding possibly the expensive \"save\".\\n- When saving a blob too big to precompute its blobId, once the blob had been streamed using a temporary random blobId, copy operation can be avoided and the temporary blob could be directly removed.\\nCassandra is probably faster doing \"write every time\" rather than \"read before write\" so we should not use the stored blob projection for it\\nSome performance tests will be run in order to evaluate the improvements.\\n\n\n## Context\\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\\nto not overload a server.\\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\\nallows, among others:\\n- Delaying retries upon MX delivery failure to a remote site.\\n- Throttling, which could be helpful for not being considered a spammer.\\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\\ndelays, purging the queue, etc.\\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\\nto interact with all its James servers, which is not friendly in a distributed setup.\\nDistributed James relies on the following third party softwares (among other):\\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\\n- **ObjectStorage** (Swift or S3) holds byte content.\\n\n## Decision\n", "response": "<pad> Provide a Proof of concept for \"Applicative read repairs\" for the mailbox and mailbox-counters entities.nThis enables read path simplification (and performance enhancements) for the mailbox object.nIMAP LIST should not read mailbox counters. This information is uneeded and we should avoid paying thenprice of read repairs for this operation.nProvide a comprehensive documentation page regarding \"Distributed James consistency model\".n", "matched_ids": [2903, 2122], "time": "0:00:34.391581", "post": "## Decision\nProvide a Proof of concept for \"Applicative read repairs\" for the mailbox and mailbox-counters entities \n This enables read path simplification (and performance enhancements) for the mailbox object \n IMAP LIST should not read mailbox counters. This information is uneeded and we should avoid paying thenprice of read repairs for this operation \n Provide a comprehensive documentation page regarding \"Distributed James consistency model\" \n "}
{"context": "## Context\\nThe main data source of the API catalogue website was originally a collection of\\nmarkdown files. The build process uses the Middleman static site generator\\nconfigured by the [Tech Docs Gem](https://github.com/alphagov/tech-docs-gem)\\n('TDG').\\nThe TDG provides additional functionality including search, sidebar\\nnavigation ('Table of Contents'), the layout, and styling.\\nThe TDG is not necessarily a good fit for the API catalogue because the project\\nisn't purely documentation, and our data source is now a CSV.\\nIn particular it is difficult to override templates inherited from the gem, to\\nadjust the layout on a particular page or add page-specific JavaScript for\\nexample.\\nUsing TDG to render the Table of Contents is slow for our site because\\nby design every page is re-rendered multiple times to pull out the headings\\n(adding over a minute to build times).\\nThe TDG also requires specific dependency versions. These version\\nrestrictions prevent us being in control of version upgrades which are necessary\\nto remain on support versions and receive security patches.\\n", "input": "## Context and Problem Statement\\nThe original Fundraising Application was a classic web-based Multi-Page-Application with some JavaScript for progressive enhancement and state stored on the Server. The 2016 rewrite introduced some state on the client. The new skin we introduced in 2019 renders the template variables in an HTML data attribute, but the client-side markup is rendered with Vue. It is a \"hybrid\" Multi-Page-Application, where the client-side code still depends on the server side variables, but the server-side \"templating\" is obsolete.\\nThe current architecture has the following drawbacks:\\n* Switching pages does a full page reload\\n* Our naive client-side \"subpage\" mechanism breaks the browser history (see https://phabricator.wikimedia.org/T285046 )\\n* Components shared across pages need to be compiled into the entry point\\nfor each page.\\n* We need \"Full Stack\" Developers who are familiar with backend\\ntechnologies (PHP, Symfony, databases, PHP CI tools, Docker) and frontend technologies\\n(TypeScript, Vue, SASS, bundler and CI tools).\\n## Decision Drivers\\n* User experience - fast page load times, browser history, immediate feedback of\\nwhat's happening, keep focus on current task.\\n* Developer experience (ease-of-use, fewest dependencies possible)\\n\n## Decision\n* User experience - fast page load times, browser history, immediate feedback of\\nwhat's happening, keep focus on current task.\\n* Developer experience (ease-of-use, fewest dependencies possible)\\nA separation into an API and Single-Page-Application looks like the best\\noption, regarding our decision drivers:\\n* It's improving on an already good user experience.\\n* It won't add complexity for developers beyond the status quo\\n* It's \"open\" enough to improve our technology stack, we don't tie\\nourselves to a specific library.\\nA Server-Rendered Multi-Page-Application with progressive enhancement\\nlooks like a promising architecture for the far future. We will keep\\nobserving the available technologies.\\n\n\n## Context\\nHistorically we have had \"two ways\" of configuring Verify Service Provider:\\n- Using environment variables\\n- Using a YAML file\\nWhen using environment variables the application used the verify-service-provider-env.yml\\nfile from the resources directory (so inside the jar). When using the YAML file you would\\npass the path to a different file as a command line parameter - usually people\\nwould use the example one that's contained in the repo.\\nThere were a couple of reasons for the extra complexity of managing two files, both due to\\nrestrictions with the java buildpack used by cloudfoundry:\\n- It's not possible to specify command line arguments through the java buildpack,\\nso you can't specify a path to your config file\\n- We weren't confident in the way cloudfoundry manages static files, so we didn't want\\nto rely on one.\\nThere was also a philosophical point that 12 factor applications should be configured through\\ntheir environment. This made the \"hide the configuration in the .jar and do everything through\\nenv vars\" way appealing.\\n\n## Decision\nWe will remove the verify-service-provider-env.yml file from src/main/resources\\nThe application will default to the verify-service-provider.yml\\nfile that's included in the .zip if no command line arguments are provided.\\nIf the application is started without command line arguments specifying a yml file\\nAND no environment variables have been set, startup should error gracefully and tell\\nthe user that the configuration fields have not been specified for example:\\n\"ERROR - no configuration fields found, either set environment variables or specify\\na configuration file using command line arguments ```server <path/to/verify-service-provider.yml>```\"\\nWe will establish the path to verify-service-provider.yml by asking java for the\\npath to the .jar file containing the Application class and looking in the parent\\nfolder.\\n\n\n## Context\\nThe main data source of the API catalogue website was originally a collection of\\nmarkdown files. The build process uses the Middleman static site generator\\nconfigured by the [Tech Docs Gem](https://github.com/alphagov/tech-docs-gem)\\n('TDG').\\nThe TDG provides additional functionality including search, sidebar\\nnavigation ('Table of Contents'), the layout, and styling.\\nThe TDG is not necessarily a good fit for the API catalogue because the project\\nisn't purely documentation, and our data source is now a CSV.\\nIn particular it is difficult to override templates inherited from the gem, to\\nadjust the layout on a particular page or add page-specific JavaScript for\\nexample.\\nUsing TDG to render the Table of Contents is slow for our site because\\nby design every page is re-rendered multiple times to pull out the headings\\n(adding over a minute to build times).\\nThe TDG also requires specific dependency versions. These version\\nrestrictions prevent us being in control of version upgrades which are necessary\\nto remain on support versions and receive security patches.\\n\n## Decision\n", "response": "<pad> * User experience - fast page load times, browser history, immediate feedback ofnwhat's happening, keep focus on current task.n* Developer experience (ease-of-use, fewest dependencies possible)nA separation into an API and Single-Page-Application looks like the best option, regarding our decision drivers:n* It's improving on an already good user experience.n* It won't add complexity for developers beyond the status quon* It's \"open\" enough to improve our technology stack, we don't tienourselves to a specific library.nA Server-Rendered Multi-Page-Application with progressive enhancementnlooks like a promising architecture for the far future. We will keepnfollowing the available technologies.n", "matched_ids": [1524, 4448], "time": "0:00:45.707225", "post": "## Decision\n* User experience - fast page load times, browser history, immediate feedback ofnwhat's happening, keep focus on current task \n * Developer experience (ease-of-use, fewest dependencies possible)nA separation into an API and Single-Page-Application looks like the best option, regarding our decision drivers:n* It's improving on an already good user experience \n * It won't add complexity for developers beyond the status quon* It's \"open\" enough to improve our technology stack, we don't tienourselves to a specific library \n A Server-Rendered Multi-Page-Application with progressive enhancementnlooks like a promising architecture for the far future. We will keepnfollowing the available technologies \n "}
{"context": "## Context and Problem Statement\\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https://github.com/Semantic-Org/Semantic-UI-React/pull/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https://github.com/decentraland/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\\n- The `@artsy/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\\n- Doesn't have hooks support.\\n#### Second alternative (react-semantic-ui)\\n##### Advantages\\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\\n##### Disadvantages\\n- Bad SSR support.\\n", "input": "## Context\\nOpenHAB uses OSGi as a runtime.\\nThis runtime promotes clear separation between implementation and contract (API/implementation/SPI) packages.\\nOnce program gets launched OSGi framework such Apache Felix or Eclipse Equinox makes sure that \"implementation\" packages stay hidden.\\nOn the build tool side we do not have such strong separation because many parts of project are co-developed.\\nInternal packages and API are in the same source root, and often functionally different elements of code are included in the same bundle.\\nFor example, this means that the `org.openhab.core.items` package is in the same module as `org.openhab.core.items.internal`.\\nAs a result, during compile time we have all of the dependencies together - ones which are required by `core.items` and ones used by `core.items.internal` package.\\nWhile it might not cause major issues for this module, it might have devastating influence over callers who depend on public parts of the API.\\nDuring compilation phase they will get polluted by internal package dependencies and quite often use them.\\nSuch approach promotes tight coupling between contract and implementation.\\nMore over, it also promotes exposure of specific implementation classes via public API.\\nThe natural way to deal with such things is to address them with a build tool that includes an appropriate includes/excludes mechanism for dependencies.\\nIt would work properly, but openHAB core is a single jar which makes things even harder.\\nThis means that quite many dependencies get unnecessarily propagated to all callers of public APIs.\\nopenHAB utilizes Apache Karaf for provisioning of the application.\\nKaraf provisioning itself is capable of verifying its \"features\" based on declared modules, bundles, JAR files, etc.\\nCurrently, most of the project features depend on one of two root features, `openhab-core-base` or `openhab-runtime-base`, making no distinction on how particular parts of the framework interact with each other.\\nA tiny extension (SPI) bundle that is targeted at a specific framework feature would need to resolve one of the above features, which would then lead to the activation of the entire openhab framework.\\nPresent structure of modules / bundles is as follows:\\n```\\n[openhab thing core] <--- [openhab rest core]\\n```\\n\n## Decision\nSince openHAB core is a framework, we need to keep strong and clear separation of API and implementation packages.\\nThis will clarify module responsibilities and streamline development of extensions through smaller and easier to maintain modules.\\nThere are also significant opportunities to speed up the build processes and make better use of resolution and graph analysis between framework parts.\\nWe decided that a clear separation between API and implementation packages should be made.\\nThis means that each service intended to be interacted with through an OSGi services API should form its own contract bundle.\\nNew framework modules will separate `internal` package under a `core.<area>.core` bundle and have a distinct interface in a separate module.\\nAdditionally, functionally separate components should be placed in separate bundles to ensure that unnecessary dependencies are not pulled in to a build.\\nThis is a big change which involves a lot of code relocations and also cleans up the project dependencies which were built up over time.\\nBy preserving public package names we will be able to keep backward compatibility at an affordable level.\\n```\\n[openhab thing api] <--- [openhab rest thing]\\n^\\n|\\n[openhab thing core]\\n```\\nIdeally, such a pattern should evolve further to include a common structure used for keeping contracts and implementations separate.\\n\n\n## Context\\nThe Wikidata Bridge app uses VueJs version 2 and Vuex version 3 for state management.\\nBoth within the store and in vue components the calls to `dispatch()` and `commit()` are not type safe.\\nThat means that they accept any arguments and TypeScript will still compile without error.\\nSince `dispatch()` and `commit()` are typical seams that are usually mocked during unit testing,\\nit is up to integration, end-to-end and browser tests to detect these errors.\\nThis is particularly unfortunate as the store is one of the central locations where business logic happens.\\nWe considered two options:\\n1. writing our own set of wrappers for `dispatch()` and `commit()` to get type safety\\n1. using vuex-smart-modules\\nThe advantages of doing it ourselves included us not having another dependency.\\nThe risks include that this would be yet another homebrew layer of abstraction.\\nThe advantages of using vuex-smart-modules include:\\n- it gives us proper native type safety both in the store and in components\\n- we can call actions/getters/mutations with [method style access](https://github.com/ktsn/vuex-smart-module#method-style-access-for-actions-and-mutations)\\n- we can get rid of all the `BRIDGE_SET_TARGET_VALUE` constants without having to fall back to string literals\\n- we can actually use the IDE's `go to method definition` functionality\\n- we can rely on the action's return type instead of dispatch's `Promise<any>` being used everywhere\\n- we can drop the `vuex-class` dependency as we can use vuex-smart-modules for all store access in components\\n- it is developed by a VueJs core contributor\\nThe risks include:\\n- it is still a very new project with a 0.x.y version number\\n- it is another layer on top of vuex, which means the documentation may not be as good as it could be\\n- we are the first big project to use it\\n- mocking of dependencies and nested modules in testing still seems to be not handled as diligently as one would wish\\n\n## Decision\nWe decided to rewrite our Vuex store using vuex-smart-modules version 0.3.4\\n\n\n## Context and Problem Statement\\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https://github.com/Semantic-Org/Semantic-UI-React/pull/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https://github.com/decentraland/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\\n- The `@artsy/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\\n- Doesn't have hooks support.\\n#### Second alternative (react-semantic-ui)\\n##### Advantages\\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\\n##### Disadvantages\\n- Bad SSR support.\\n\n## Decision\n", "response": "[ERROR]: An error occurred while generating decision. Please rate anything for this decision.HTTPSConnectionPool(host='b2w8w5mmlr342ulg.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=300)", "matched_ids": [660, 1345], "time": "0:05:01.442008", "post": "## Decision\n[ERROR]: An error occurred while generating decision. Please rate anything for this decision.HTTPSConnectionPool(host='b2w8w5mmlr342ulg.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=300)"}
{"context": "## Context\\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\\nIn the future we will want to more automatically match buyers\u2019 articulations of need so as to work out which products CCS can offer to meet their need.\\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\\n", "input": "## Context\\nPartner organizations work directly with the families in need. Sometimes they will need to collect PII such as address, names and ages of family members, and other contact information. This data is not necessary for Diaperbase to operate, and there are security concerns about that PII being disclosed should there be a breach. There is also a separation of concerns between the two applications; Diaperbase is inventory management and Partnerbase is effectively a CRM. At this time, we belive that they are different enough in their purposes that they should be separate.\\n\n## Decision\nA new application, Partnerbase, will be created to handle the CRM aspects. It will communicate over a private API with Diaperbase to handle request fulfillment.\\n\n\n## Context\\nOur main source of data on prisoners and prison staff which we need for\\nallocations is NOMIS.\\nThere are now four APIs into NOMIS providing general data access, with varying\\napproaches to presenting the data and authentication. We do not want to add to\\nthis duplication.\\nThe APIs which have been developed more recently are more under the control of\\nHMPPS than the earlier ones (which were developed by a supplier). That gives us\\nmore flexibility around how we work with them and makes it possible to get\\nchanges into production more quickly. Using one of the newer APIs should mean\\nthat we are less blocked by delays around API changes than we have been on our\\nwork on Visit someone in prison.\\nIt has been agreed by the HMPPS technical community that we would like to move\\nall clients to use the Custody API in preference to the other APIs over time.\\nAlthough that work has not yet been prioritised, using the Custody API for new\\napplications will reduce the work needed in future to align our API usage.\\nThe Custody API has been designed to give a more direct view of the data in\\nNOMIS than the previous APIs have been - earlier approaches have favoured\\nimplementing specific endpoints to meet the needs of service teams rather than\\ngiving a more comprehensive view of all the data.\\n\n## Decision\nWe will use the Custody API to access the NOMIS data we need.\\nWe will work with the team in Sheffield on development of the Custody API to\\nadd support for accessing the data we need.\\n\n\n## Context\\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\\nIn the future we will want to more automatically match buyers\u2019 articulations of need so as to work out which products CCS can offer to meet their need.\\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\\n\n## Decision\n", "response": "[ERROR]: An error occurred while generating decision. Please rate anything for this decision.HTTPSConnectionPool(host='b2w8w5mmlr342ulg.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=300)", "matched_ids": [3342, 267], "time": "0:05:01.003338", "post": "## Decision\n[ERROR]: An error occurred while generating decision. Please rate anything for this decision.HTTPSConnectionPool(host='b2w8w5mmlr342ulg.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=300)"}
{"context": "## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted/PaaS CI/CD solution\\n", "input": "## Context\\nTravis is actively used for many other projects within the BEIS GitHub account.\\nWe know we will want to be using containers to run this application, so the tool we use must support Docker too.\\n\n## Decision\nUse Travis to build and test the application.\\nUse Travis to automatically deploy the applications to live.\\n\n\n## Context\\nAt least one user is currently using node js and passport. We want to provide as\\nfrictionless as possible an integration for them.\\nOther users will be using other languages and frameworks.\\n\n## Decision\nWe will initially build only a node / passport client. We will want to build\\nanother client in another language as soon as possible to make sure the API\\nis well designed.\\nUsers should also be able to interact with the API directly if we haven't built\\nan appropriate client for their use case.\\n\n\n## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted/PaaS CI/CD solution\\n\n## Decision\n", "response": "<pad> Use Travis to build and test the application.nUse Travis to automatically deploy the applications to live.n", "matched_ids": [2393, 577], "time": "0:00:34.541578", "post": "## Decision\nUse Travis to build and test the application \n Use Travis to automatically deploy the applications to live \n "}
{"context": "## Context\\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\\n", "input": "## Context\\nWe need a convenient system for managing API keys used by the Python client. This system should give the user multiple options for\\nproviding an API key to be used when making a request to the API. These options should include:\\n* Storing API keys on the users system\\n* Reading an API key from the environment\\n* Passing an API key directly to the API request methods\\nUsers may have multiple valid API keys associated with their account at any given time. The system for storing API keys on the user's\\nsystem must accommodate this and provide a clear, deterministic way of resolving an API key for a given project.\\nWe anticipate the need to store other data related to Radiant MLHub for uses unrelated to authentication. For instance, we may have a need to\\ntrack the progress of downloads so that they can be resumed if interrupted, or we may want to specify a base URL in a config file so that\\ndevelopers can test against the staging environment. The method that we choose for storing API keys on the user's system must not preclude\\nus from storing this additional information.\\n\n## Decision\nThe Python client will resolve the API key to be used in a request in the following order:\\n1) Passing an `api_key` argument directly to the method\\n2) Setting an `MLHUB_API_KEY` environment variable\\n3) Passing a `profile` argument directly to the method. This will read the API key from the given profile (see below for details)\\n4) Setting an `MLHUB_PROFILE` environment variable. This will read the API key from the given profile (see below for details)\\n5) Using the API from the `default` profile\\nProfiles will be stored in a `.mlhub/profiles` file in the user's home directory. This file will be an INI file containing at least a\\n`[default]` section with an `api_key` value. The file may contain other sections corresponding to named profiles. Any `profile` argument\\npassed to a method must correspond to one of these section names, or it will raise an exception.\\n\n\n# Context\\nScott recently added a `path_url` prompt to get the eventual url a web1 app would end up. The purpose of this property was to update the css and javascript url paths in a web app html file. The problem is that typical useage of the lifecorp-init app is to press enter through each prompt and accept the defaults. For a web1 app, there is no default for the url path because there is no standard for where the app will be placed on the lifecorp network. If this field does NOT get filled in when the app is intiated, then the developer must manually fill in this information. The purpose of lifecorp-init is to get a project skeleton up and running to a \"Hello World\" state with little to no modifications.\\n# Decision\\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n\n## Decision\nScott will add a parameter called `--require` and `-r` that will allow a string to be passed to tell lifecorp-init to require a specific prompt to not be empty for some projects, but remain empty for others.\\n# Consequences\\nThe lifcorp-init will be able to block users from skipping required parameters.\\n# Status\\nAccepted\\n\n\n## Context\\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\\n\n## Decision\n", "response": "<pad> We will use the Security API to provide a way to store the API key on the user's system.n", "matched_ids": [702, 139], "time": "0:00:24.002877", "post": "## Decision\nWe will use the Security API to provide a way to store the API key on the user's system \n "}
{"context": "## Context\\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\\n", "input": "## Context\\nAiven provides hosted Elasticsearch for the Elasticsearch backing service.\\nThe PaaS has several environments which will need to use Aiven. These\\nenvironments should be isolated from each other so that changes made in testing\\nand development environments do do not affect production users.\\nAiven provide a \"Project\" abstraction where a user can be a member of several\\nprojects. API tokens are user specific. By creating one user per project it's\\npossible to scope API tokens to a project.\\n\n## Decision\nWe'll use separate projects for separate environments, initially using the\\nfollowing Aiven projects:\\n* ci-testing (for the CI environment for the elasticsearch broker itself)\\n* paas-cf-dev\\n* paas-cf-staging\\n* paas-cf-prod\\nFor staging and prod we will use separate API tokens within the same project to\\nseparate credentials between the London and Ireland regions.\\nWe will have the following per-project users to hold API tokens:\\nthe-multi-cloud-paas-team+aiven-ci@digital.cabinet-office.gov.uk\\nthe-multi-cloud-paas-team+aiven-dev@digital.cabinet-office.gov.uk\\nthe-multi-cloud-paas-team+aiven-staging@digital.cabinet-office.gov.uk\\nthe-multi-cloud-paas-team+aiven-prod@digital.cabinet-office.gov.uk\\nThe credentials for the ci and dev users will be stored in the\\n`paas-credentials` passwordstore. staging and prod will be stored in\\n`paas-credentials-high`.\\nMembers of the PaaS team will each have their own user which will have access\\nall of the projects for management purposes.\\n\n\n## Context\\nDo we have lots of little xxx4k libraries in their own repositories?  Or have a monorepo: one big project with each xxx4k library in a subdirectory.\\nThe former decouples release cadences.\\nThe latter makes it easier to maintain a single BOM for publishing to Maven Central, perform integration testing when libraries depend on one another, and use a consistent version number across all libraries.\\n\n## Decision\nWe will have a monorepo.\\n\n\n## Context\\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\\n\n## Decision\n", "response": "<pad> We will use a sdk-repository to store dependencies.n", "matched_ids": [191, 4542], "time": "0:00:20.738052", "post": "## Decision\nWe will use a sdk-repository to store dependencies \n "}
{"context": "## Context\\nDrop FreeTFT, TFTA tokens from SDK\\n", "input": "## Context\\nNeed of using Infrastructure as code to, track what resources are running in the AWS account\\n\n## Decision\nUse Terraform\\n\n\n## Context\\nRead [Event Bus ADR](0037-eventbus.md) for context.\\nGiven several James servers, we need them to share a common EventBus.\\nThis:\\n- Ensures a better load balancing for `group mailbox listners`.\\n- Is required for correctness of notifications (like IMAP IDLE).\\n\n## Decision\nProvide a distributed implementation of the EventBus leveraging RabbitMQ.\\nEvents are emitted to a single Exchange.\\nEach group will have a corresponding queue, bound to the main exchange, with a default routing key. Each eventBus\\nwill consume this queue and execute the relevant listener, ensuring at least once execution at the cluster level.\\nRetries are managed via a dedicated exchange for each group: as we need to count retries, the message headers need to\\nbe altered and we cannot rely on rabbitMQ build in retries. Each time the execution fails locally, a new event is emitted\\nvia the dedicated exchange, and the original event is acknowledged.\\nEach eventBus will have a dedicated exclusive queue, bound to the main exchange with the `registrationKeys` used by local\\nnotification mailboxListeners (to only receive the corresponding subset of events). Errors are not retried for\\nnotifications, failures are not persisted within `DeadLetter`, achieving at most once event delivery.\\n\n\n## Context\\nDrop FreeTFT, TFTA tokens from SDK\\n\n## Decision\n", "response": "[ERROR]: An error occurred while generating decision. Please rate anything for this decision.HTTPSConnectionPool(host='b2w8w5mmlr342ulg.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=300)", "matched_ids": [1464, 2125], "time": "0:05:00.739575", "post": "## Decision\n[ERROR]: An error occurred while generating decision. Please rate anything for this decision.HTTPSConnectionPool(host='b2w8w5mmlr342ulg.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=300)"}
{"context": "## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThe original author of go-bindata delete their account and then the repo was\\nrecreated under a different owner. The dependency has shifted around enough that\\nwe have lost faith in the intention of the maintainers\\n[more details here](https://twitter.com/francesc/status/961249107020001280?lang=en)\\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\\nduring a test run or final build) made it hard to reason about.\\n", "input": "## Context\\nIstio's service mesh capabilites are facilitated via sidecars injected into\\nworkload pods. These sidecars run an Istio-patched version of Envoy that is tied\\nto the version of Istio that injects them.\\nTypically when new versions of Istio are released, new versions of the sidecars\\nare released as well. Istio has been good so far about supporting older versions\\nof sidecars that were deployed before Istio was upgraded, but it is still\\n[documented best practice](https://istio.io/latest/docs/setup/upgrade/) to roll\\nall the pods after an Istio upgrade.\\nAs an additional constraint, the operators of cf-for-k8s clusters expect to be\\nable to perform upgrades in one `kapp deploy`, with no post-install hooks or\\nother bash scripts. This limits our options considerably. See this [Slack\\nthread](https://cloudfoundry.slack.com/archives/CH9LF6V1P/p1592521879117400) on\\nthat constraint.\\n\n## Decision\nWe will use the kubernetes\\n[Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/) resource\\nto run the kubectl command needed to roll workload pods, after waiting for the\\nnew Istio control plane to be up and healthy.\\nTo that end, we will add the necessary minimal `ServiceAccounts` and `Roles`\\nneeded to list resources in the `istio-system` namespace, and restart resources\\nin the configured workload namespace. We will also build and maintain a\\ncontainer image that contains the Job's logic.\\nAll istio components will be tagged with their Istio version so that the job can\\npositively determine that the correct version of control plane components are\\nalive and healthy. We will also name the job according to it's Istio version, so\\nthat we can take advantage of `Jobs` inherent immutability in cases where a\\ncf-for-k8s upgrade does not contain a new Istio version (pushing the same job\\nagain will not cause it to rerun, preventing workloads from rolling\\nunnecessarily). Subsequent jobs will clean up previous ones.\\n\n\n## Context\\nAn Auto-scaling Group's (ASG) instances are all created using a Launch Configuration (LC). The LC manages the image used to build instances as well as any user-data, security groups or tags.\\nWhen the LC of an ASG is changed those changes are only propagated to new instances. New instances are only created if the ASG scales-out or if an existing instance goes away (e.g. is terminated or fails).\\nHistorically many of our instances are long lived (e.g. production puppetmaster, at time of writing, has an uptime of 79 days). This means that changes to the LC may take a long time to propagate fully. In the case of important changes to the LC (e.g. new security groups or tags that add Puppet facts) this is undesirable.\\nThere are several possible ways to force propagation of LC changes within the existing Terraform code were considered ([issue](https://github.com/hashicorp/terraform/issues/1552)):\\n* Manual termination of instances\\n* Embedding CloudFormation in Terraform to more tightly manage instance life-cycle ([e.g. see this comment](https://github.com/hashicorp/terraform/issues/1552#issuecomment-191847434))\\n* Creating a new ASG for every LC ([e.g. see this post](https://groups.google.com/forum/#!msg/terraform-tool/7Gdhv1OAc80/iNQ93riiLwAJ))\\n\n## Decision\nIn order to propagate LC changes we will manually terminate effected instances.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThe original author of go-bindata delete their account and then the repo was\\nrecreated under a different owner. The dependency has shifted around enough that\\nwe have lost faith in the intention of the maintainers\\n[more details here](https://twitter.com/francesc/status/961249107020001280?lang=en)\\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\\nduring a test run or final build) made it hard to reason about.\\n\n## Decision\n", "response": "[ERROR]: An error occurred while generating decision. Please rate anything for this decision.HTTPSConnectionPool(host='b2w8w5mmlr342ulg.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=300)", "matched_ids": [2240, 4048], "time": "0:05:01.122483", "post": "## Decision\n[ERROR]: An error occurred while generating decision. Please rate anything for this decision.HTTPSConnectionPool(host='b2w8w5mmlr342ulg.us-east-1.aws.endpoints.huggingface.cloud', port=443): Read timed out. (read timeout=300)"}
{"context": "## Context\\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\\n[Prettier](https://prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\\n", "input": "## Context\\nThe web frontend of time-tracker is crucial, since it is the only way to access certain important features such as user management and invoicing. It will be built as a single-page application. Web frontend development is complex nowadays, and choosing the right stack can help mitigate this complexity and speed up the development process. We are evaluating Typescript and ClojureScript.\\nTypescript is widely used and has a lot of mindshare. As such, setting up a Typescript project and making use of the Typescript/Javascript ecosystem is straightforward.\\nClojureScript offers a number of advantages over Typescript, such as programming in a functional-first style with immutable data structures, LISP syntax, seamless live code reloading, a fast REPL-driven workflow and generally less code and boilerplate. In addition to this, the [`re-frame`](http://day8.github.io/re-frame/) framework is a well thought-out approach to state management. It manages state and side effects very well and with much less boilerplate compared to popular JS/TS equivalents such as Redux. All programmers at nilenso are familiar with Clojure (if not ClojureScript already), and learning ClojureScript and getting productive shouldn't be an issue for them.\\nOne potential concern with ClojureScript is easy access to the JavaScript/NPM ecosystem of libraries and tooling features. This concern is addressed by `shadow-cljs`, a popular build tool for ClojureScript which (among other things) makes access to NPM packages as frictionless as possible.\\nSome features readily available in JS/TS such as CSS modules are tricky to set up or unusable with ClojureScript.\\n\n## Decision\n1. We will use [ClojureScript](https://clojurescript.org/) as the programming language for the web frontend.\\n2. We will use [re-frame](http://day8.github.io/re-frame/) as the main application framework.\\n3. We will use [shadow-cljs](http://shadow-cljs.org/) as the build tool for ClojureScript. For other assets such as CSS, we will use other build tooling as necessary.\\n\n\n## Context\\nAdding static types to a JavaScript project can be incredibly useful to reduce the likelihood of introducing runtime type errors.\\nWe have considered two methods of adding static types to dotcom-rendering.\\n### Flow\\nFlow makes it easy to retrofit a useful level of implicit type checking to a legacy project.\\nHowever, it produces error messages that are difficult to understand.\\nFlow requires developers to check in types for 3rd party libraries. These can be automatically generated based on the `package.json`, but developers must remember to keep these up to date.\\nIts configuration is often too coarse-grained, meaning code gets littered with opt-outs and `any` types. It doesn't feel like a natural type system, as it is only annotations on top of a dynamic language.\\nFlow's types are not exact by default.\\nThe ecosystem is still immature.\\n### TypeScript\\nTypeScript produces more understandable error messages.\\nIt allows developers to install versioned types for third party libraries.\\nTypeScript is a language in its own right, and feels more natural. It can improve JavaScript language features to make them more type-friendly. Its configuration is more fine-grained.\\nTypes are exact by default.\\nThe ecosystem is more mature.\\n\n## Decision\nUse TypeScript for static typing.\\n\n\n## Context\\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\\n[Prettier](https://prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\\n\n## Decision\n", "response": "<pad> 1. We will use [ClojureScript](https://clojurescript.org/) as the programming language for the web frontend.n2. We will use [re-frame](http://day8.github.io/re-frame/) as the main application framework.n3. We will use [shadow-cljs](http://shadow-cljs.org/) as the build tool for ClojureScript. For other assets such as CSS, we will use other build tooling as necessary.n", "matched_ids": [4921, 2652], "time": "0:01:23.396473", "post": "## Decision\n1. We will use [ClojureScript](https://clojurescript.org/) as the programming language for the web frontend \n 2. We will use [re-frame](http://day8.github.io/re-frame/) as the main application framework \n 3. We will use [shadow-cljs](http://shadow-cljs.org/) as the build tool for ClojureScript. For other assets such as CSS, we will use other build tooling as necessary \n "}
{"context": "## Context\\nWe have to make a decision on the branching strategy for development.\\n", "input": "## Context\\nWe have to make a decision, how we describe the core use cases of the application.\\n\n## Decision\nWe use [Domain Storytelling](https://domainstorytelling.org/) to describe core use cases & interactions.\\nAll user stories can be found in the [`./doc/user-stories`](./doc/user-stories) directory.\\n\n\n## Context\\nWe have to make a decision on how/where to host the DB.\\n\n## Decision\nWe use a freely managed [Postgres on Heroku](https://www.heroku.com/postgres)\\n\n\n## Context\\nWe have to make a decision on the branching strategy for development.\\n\n## Decision\n", "response": "<pad> We use [Semantic Branching](https://semantic-branching.org/) to branch out the application.n", "matched_ids": [3659, 3657], "time": "0:00:22.895256", "post": "## Decision\nWe use [Semantic Branching](https://semantic-branching.org/) to branch out the application \n "}
{"context": "## Context\\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\\n", "input": "## Context\\nWe need to add new functionality to the project - the ability to create custom NRPE checks.\\nThis means having templated configuration for adding service-specific plugins for the purposes of monitoring.\\nThe `base` module already contains NRPE specific code.\\nIt installs NRPE and ensures it is running. It also configures a set of default monitoring plugins.\\n\n## Decision\nWe considered three possible approaches to adding the new NRPE functionality:\\n- Add it to the `base::nrpe`\\n- Create a separate `nrpe_custom_checks` module and keep `base` the way it is\\n- Create a separate `nrpe_custom_checks` module and refactor `base` to use it for check creation\\nRather than extending the base NRPE class, we chose to create a standalone NRPE custom checks module and keep `base` independent.\\n\n\n## Context and Problem Statement\\nSome components in FDS would suffer from accepting a `classNames` prop. `Flex`, for\\nexample, would no longer guarantee it would work as a basic layout utility if we allowed\\narbitrary style overrides on the component.\\nWe want all components in FDS to follow a similar prop interface, so this decision should\\napply to _all_ components.\\n\n## Decision\nChosen option: \"No components accept a `className`\", because...\\n- It's too risky to introduce style overrides to low-level layout components.\\n- Style customization should happen via props, where the component can be responsible for\\nstyle changes and variations.\\n### Positive Consequences <!-- optional -->\\n- Prop API consistency across all components.\\n### Negative Consequences <!-- optional -->\\n- Consumers of FDS components must handle the breaking change. Any custom `className`\\nprops on FDS components must be moved to a direct parent or direct child of the\\ncomponent.\\n\n\n## Context\\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\\n\n## Decision\n", "response": "<pad> Chosen option: \"Add it to the base::nrpe\", because...n- It's too risky to introduce style overrides to low-level layout components.n- Style customization should happen via props, where the component can be responsible fornstyle changes and variations.n### Positive Consequences!-- optional -->n- Prop API consistency across all components.n### Negative Consequences!-- optional -->n- Consumers of FDS components must handle the breaking change. Any custom classNamenprops on FDS components must be moved to a direct parent or direct child of thencomponent.n", "matched_ids": [1268, 802], "time": "0:00:43.209924", "post": "## Decision\nChosen option: \"Add it to the base::nrpe\", because.. \n - It's too risky to introduce style overrides to low-level layout components \n - Style customization should happen via props, where the component can be responsible fornstyle changes and variations \n ### Positive Consequences!-- optional -->n- Prop API consistency across all components \n ### Negative Consequences!-- optional -->n- Consumers of FDS components must handle the breaking change. Any custom classNamenprops on FDS components must be moved to a direct parent or direct child of thencomponent \n "}
{"context": "## Context\\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\\n```\\nstruct tof\\n{\\nusing distance_t = uint16_t;\\nusing cb_t = stdext::inplace_function<void(distance_t)>;\\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\\nenum class mode\\n{\\ndefaultRange = 0,\\nshortRange,\\nmedRange,\\nlongRange,\\n};\\n}''\\n```\\nInterface classes would inherit from these structs:\\n```\\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\\n```\\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\\n```\\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\\n```\\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\\n", "input": "## Context\\nThe Kubernetes Probe Resource Structure would allow adding multiple types of Healthchecks at once. This would be:\\n* exec\\n* httpGet\\n* tcpSocket\\nHowever currently v1.18 v1.20 when one tries to add more than one Type, like:\\n```\\nreadinessProbe:\\nexec:\\ncommand:\\n- \"/usr/bin/sh\"\\n- \"-c\"\\n- \"echo Hello World\"\\nhttpGet:\\nhost: localhost\\nport: 80\\ntcpSocket:\\nhost: localhost\\nport: 80\\n```\\nkubernetes will not validate the Resource with following message:\\n```\\n# pods \"test-probes\" was not valid:\\n# * spec.containers[0].readinessProbe.httpGet: Forbidden: may not specify more than 1 handler type\\n# * spec.containers[0].readinessProbe.tcpSocket: Forbidden: may not specify more than 1 handler type\\n```\\n\n## Decision\nIt is decided to go with the same logic and ensure that only one type will be accepted, so the Operator is not supposed to handle the case more than one Types are given.\\nFurther it follows the same logic and will use and check HTTP first than TCP last. Exec healthchecks don't make sense and will be ignored completly\\n\n\n**Context**: most APIs in the GCS library have a number of optional parameters,\\nfor example, the API can use `ifMetagenerationMatch` to apply an operation only\\nif the metadata generation matches a given number. The question arose of how to\\nrepresent these parameters, as properties that we modify in the client or\\nobject, or as per-request parameters in the function used to access the API.\\nThat is, we had two proposals, one where the application would write code like\\nthis:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\nvs. code like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\n// Create a new bucket handle that applies the given optional parameters to\\n// all requests.\\nauto bucket = b.ApplyModifiers(\\nstorage::IfMetagenerationMatch(7), UserProject(\"my-project\"))\\nbucket.ApiName(foo, bar);\\n}\\n```\\n**Decision**: The parameters are passed as variadic arguments into any function\\nthat needs them. That is, all APIs look like this:\\n```C++\\nclass Bucket /* or Object as applicable */ { public:\\ntemplate <typename... Parameters>\\nReturnType ApiName(\\nFixedArgument1 a1, FixedArgument2 a2,\\nParameters&&... p);\\n```\\nand are used like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\n**Consequences**: The advantages of this approach include:\\n- It is easier to use parameters in an API, it does not require to create a new\\nbucket or object or client handle just for changing the parameters in one\\nrequest.\\nThe downsides include:\\n- All APIs become templates, we should be careful not to create massive header\\nfiles that are slow to compile.\\n- It is harder to overload APIs.\\n- It is not clear how other optional parameters of the APIs, such as timeouts,\\nfit with this structure.\\n\n## Decision\nthat needs them. That is, all APIs look like this:\\n```C++\\nclass Bucket /* or Object as applicable */ { public:\\ntemplate <typename... Parameters>\\nReturnType ApiName(\\nFixedArgument1 a1, FixedArgument2 a2,\\nParameters&&... p);\\n```\\nand are used like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\n**Consequences**: The advantages of this approach include:\\n- It is easier to use parameters in an API, it does not require to create a new\\nbucket or object or client handle just for changing the parameters in one\\nrequest.\\nThe downsides include:\\n- All APIs become templates, we should be careful not to create massive header\\nfiles that are slow to compile.\\n- It is harder to overload APIs.\\n- It is not clear how other optional parameters of the APIs, such as timeouts,\\nfit with this structure.\\n\n\n## Context\\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\\n```\\nstruct tof\\n{\\nusing distance_t = uint16_t;\\nusing cb_t = stdext::inplace_function<void(distance_t)>;\\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\\nenum class mode\\n{\\ndefaultRange = 0,\\nshortRange,\\nmedRange,\\nlongRange,\\n};\\n}''\\n```\\nInterface classes would inherit from these structs:\\n```\\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\\n```\\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\\n```\\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\\n```\\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\\n\n## Decision\n", "response": "<pad> It is decided to go with the same logic and ensure that only one type will be accepted, so the Operator is not supposed to handle the case more than one Types are given.nFurther it follows the same logic and will use and check HTTP first than TCP last. Exec healthchecks don't make sense and will be ignored completlyn", "matched_ids": [1734, 2469], "time": "0:00:29.989436", "post": "## Decision\nIt is decided to go with the same logic and ensure that only one type will be accepted, so the Operator is not supposed to handle the case more than one Types are given \n Further it follows the same logic and will use and check HTTP first than TCP last. Exec healthchecks don't make sense and will be ignored completlyn"}
{"context": "### Context\\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\\n```php\\nfunction test_something(): void {\\n// ...\\ntry {\\n// the statement that fail\\n$this->fail();\\n} catch (Exception $e) {\\n// ...\\n}\\n}\\n```\\nOr:\\n```php\\nfunction test_something(): void {\\n// ...\\n$this->expectException($exception)\\n// the statement that fail\\n}\\n```\\n### Decision\\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https://thephp.cc/company/consultants/sebastian-bergmann\\n[phpunit-exception-best-practices]: https://thephp.cc/news/2016/02/questioning-phpunit-best-practices\\n[1090]: https://github.com/infection/infection/pull/1061\\n", "input": "### Context\\nPHPUnit assertions are static methods, yet in our code base we call them with `$this` instead of\\n`self`.\\nWhilst \"incorrect\", this usage does not break anything. Besides:\\n- [PHUnit documentation][phpunit-doc] itself uses this by default\\n- `$this` is much more widely used than `self` in this context in the community\\n- all Infection code uses `$this`\\nThere is not much shortcomings from using this other than the \"incorrectness\" of using a static\\nmethod as a non-static one.\\n### Decision\\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https://phpunit.de/manual/6.5/en/appendixes.assertions.html\\n[1061]: https://github.com/infection/infection/pull/1061\\n\n## Decision\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https://phpunit.de/manual/6.5/en/appendixes.assertions.html\\n[1061]: https://github.com/infection/infection/pull/1061\\n\n\n## Context and Problem Statement\\nWe want to use [Semantic Versioning (SemVer)](https://semver.org/) when publishing our GitHub Actions, but [we cannot use SemVer ranges](https://github.community/t/semver-ranges-docs-issues/16723/2) when consuming them.\\n<details>\\n<summary><b>More Context</b></summary><br>\\nWhen we do\\n```yaml\\nuses: byu-oit/github-action-tf-plan-analyzer@v2\\n```\\nin GitHub Actions, we're telling GitHub to grab the repo at the _branch_ or _tag_ labelled `v2`.\\nThere's [currently no concept of SemVer](https://github.community/t/semver-ranges-docs-issues/16723/2) in GitHub Actions, but there are some conventions. A common convention is to cut \"releases\" (tags) using SemVer, e.g. `v2.0.1`, but that doesn't change the `v2` tag or branch by default.\\nWe would like consumers to be able to get `^2.0.0`, but currently that requires moving the `v2` tag or updating the `v2` branch.\\nGitHub for a while suggested using moving around tags like `v2`, but has [since suggested](https://docs.github.com/en/free-pro-team@latest/actions/creating-actions/about-actions#using-tags-for-release-management) either using branches or tags for that purpose. It is otherwise very unusual to move tags on GitHub, so we prefer branches to convey mutability.\\n</details>\\n## Decision Drivers\\n* Versioning is necessary to communicate changes (especially breaking changes)\\n* We don't want to break deployments\\n* Breaking somebody's ability to deploy doesn't actually bring down production\\n* We want to make it easy to roll out updates\\n* Many consumers of our GitHub Actions are unfamiliar with how they work\\n\n## Decision\n* Versioning is necessary to communicate changes (especially breaking changes)\\n* We don't want to break deployments\\n* Breaking somebody's ability to deploy doesn't actually bring down production\\n* We want to make it easy to roll out updates\\n* Many consumers of our GitHub Actions are unfamiliar with how they work\\nWhen publishing actions, use branches corresponding to major versions (e.g. `v1`, `v2`) and keep them updated. Treat updates to the major-version branches as production changes. Continue to tag new releases according to SemVer (e.g. `v2.0.1`, `v2.1.0`).\\nWhen consuming [our actions](https://github.com/search?q=org%3Abyu-oit+github-action+archived%3Afalse&type=Repositories), refer to the major version branches (e.g. `v2`). This will allow us to roll out most changes to the organization quickly and effectively.\\n\n\n### Context\\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\\n```php\\nfunction test_something(): void {\\n// ...\\ntry {\\n// the statement that fail\\n$this->fail();\\n} catch (Exception $e) {\\n// ...\\n}\\n}\\n```\\nOr:\\n```php\\nfunction test_something(): void {\\n// ...\\n$this->expectException($exception)\\n// the statement that fail\\n}\\n```\\n### Decision\\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https://thephp.cc/company/consultants/sebastian-bergmann\\n[phpunit-exception-best-practices]: https://thephp.cc/news/2016/02/questioning-phpunit-best-practices\\n[1090]: https://github.com/infection/infection/pull/1061\\n\n## Decision\n", "response": "<pad> * Versioning is necessary to communicate changes (especially breaking changes)n* We don't want to break deploymentsn* Breaking somebody's ability to deploy doesn't actually bring down productionn* We want to make it easy to roll out updatesn* Many consumers of our GitHub Actions are unfamiliar with how they worknWhen publishing actions, use branches corresponding to major versions (e.g. v1, v2) and keep them updated. Treat updates to the major-version branches (e.g. v2) as production changes. Continue to tag new releases according to SemVer (e.g. v2.0.1, v2.1.0).nWhen consuming [our actions](https://github.com/search?q=org%3Abyu-oit+github-action+archived%3Afalse&type=Repositories), refer to the major version branches (e.g. v2). This will allow us to roll out most changes to the organization quickly and effectively.n### StatusnAccepted ([#1061][1061])n[phpunit-doc]: https://phpunit.de/manual/6.5/en/appendixes.assertions.htmln[1061]: https://github.com/infection/infection/pull/1061n", "matched_ids": [66, 4073], "time": "0:01:19.415321", "post": "## Decision\n* Versioning is necessary to communicate changes (especially breaking changes)n* We don't want to break deploymentsn* Breaking somebody's ability to deploy doesn't actually bring down productionn* We want to make it easy to roll out updatesn* Many consumers of our GitHub Actions are unfamiliar with how they worknWhen publishing actions, use branches corresponding to major versions (e.g. v1, v2) and keep them updated. Treat updates to the major-version branches (e.g. v2) as production changes. Continue to tag new releases according to SemVer (e.g. v2.0.1, v2.1.0) \n When consuming [our actions](https://github.com/search?q=org%3Abyu-oit+github-action+archived%3Afalse&type=Repositories), refer to the major version branches (e.g. v2). This will allow us to roll out most changes to the organization quickly and effectively \n ### StatusnAccepted ([#1061][1061])n[phpunit-doc]: https://phpunit.de/manual/6.5/en/appendixes.assertions.htmln[1061]: https://github.com/infection/infection/pull/1061n"}
{"context": "## Context\\nWhen developing new algorithms and features for PACE-related software,\\ndevelopers often create useful demo/visualisation scripts for their own use.\\nThese scripts could be useful or interesting for other developers, and are\\nimportant for reproducibility or justifying design decisions. They should be\\nstored somewhere in version control so that they can be easily accessed by any\\ndevelopers and referred to later. However, they are not intended for general\\nuse and will not be actively maintained or tested. There are 2 main options:\\n* Store them in a `dev_scripts` directory in each separate project repository\\n* Store them in a `scripts` directory in `pace-developers`\\nIf they're in the `dev_scripts` directory for each project repository:\\n+ All in one place\\n+ Scripts will be close to the code they are used for\\n- Scripts may not work with the version of the code they are distributed with\\n- It's unclear where scripts that use more than one project would go\\n- Despite the folder being called `dev_scripts` people might expect the scripts\\nto actually work as they're in the main project repository\\nIf they're in a `scripts` directory in `pace-developers`:\\n+ They can be kept close to the decision-making developer documentation that\\nthey support\\n+ A version can be specified for any project dependencies\\n", "input": "## Context\\nWhen multiple demo instances exist in parallel, we need to avoid to step on each others toes. The scope is\\n- the manuela-gitops repository, where component deployment and configuration takes place as part of a demo run\\n- the manuela-dev repository, where coding changes take place as part of a demo run\\nOptions:\\n1. **BRANCHES** inside the existing sa-mw-dach repos, for each manuela demo env there would be a branch (e.g. name based on a convention).\\nPros:\\n- Easiest to set up since it's literally just cloning the central repos.\\n- only need to adjust branch names for new demo instance (however the number of required adjustments is likely the same)\\nCons:\\n- need to coordinate the creation/assignment of branches\\n- all demoers need write access to the repos\\n- Danger of cluttering the repo with \"abandoned\" branches\\n2. **FORKS**: The \"owner\" of the new installation forks all repos in GitHub or creates new repositories from scratch. The changing remote URL needs to be adjusted when setting up the demo. Preferably, there would be somewhere one central configuration (GIT_BASE_URL) defining from which fork to pull from.\\nPros:\\n- demo instances can be set up without any coordination\\n- No conflicts when branches are used for e.g. production staging approvals\\nCons:\\n- need to adjust\\n- pipeline configs (pointing to forks of manuela-dev/manuela-gitops)\\n- argoCD applications (poingint to forks of manuela-gitops)\\n3. **DEDICATED GIT**: We could deploy a dedicated GIT(LAB?) to a namespace, place the manuela-gitops +  manuela-dev repos there. As long as we can use only cluster-internal URLs these will not change across instances.\\nPros:\\n- Consistent URLs at least in a single-cluster scenario, no need to adapt anything\\nCons:\\n- Will require similar adaptations as **FORKS** in a multi-cluster scenario, since we will need to use external URLs for cross-cluster communication\\n\n## Decision\nUse **FORKS**: this requires least coordination, access rights or additional components.\\n\n\n## Context and Problem Statement\\nAs part of PLATUI-1393, browser based acceptance tests were added that rely on `assets-frontend`. These acceptance tests\\nrun as part of the build in Jenkins, which means `ASSETS_FRONTEND` needs to be added as a parameter to the `build-jobs`\\njob builder `.withServiceManager` call. However, `assets-frontend` has specific logic that means all versions are started\\nvia Service Manager, that is multiple instances of `assets-frontend` will start, not just the latest release.\\nThere is logic in Service Manager to pass an `application.conf` file to start only required versions of `assets-frontend`,\\nhowever as we are not using Service Manager to start `contact-frontend` itself, this logic does not get called. Therefore,\\nall verisons of `assets-frontend` start during when running a build, either after merge or as part of the PR builder for\\n`contact-frontend`.\\n## Decision Drivers\\n* `assets-frontend` is a deprecated service - we want to ensure the functionality using it does not break, but we are\\nreluctant to invest large amounts of time in it\\n* Service Manager is a Python application used heavily across the Platform and not owned by PlatUI, and we would like to\\navoid making changes to it if possible to support our workflow.\\n* Similarly the job builders in the `build-jobs` repo are heavily shared, and we would like to avoid making changes to\\n`build-jobs` if possible\\n\n## Decision\n* `assets-frontend` is a deprecated service - we want to ensure the functionality using it does not break, but we are\\nreluctant to invest large amounts of time in it\\n* Service Manager is a Python application used heavily across the Platform and not owned by PlatUI, and we would like to\\navoid making changes to it if possible to support our workflow.\\n* Similarly the job builders in the `build-jobs` repo are heavily shared, and we would like to avoid making changes to\\n`build-jobs` if possible\\nChosen option: \"Accept that builds of `contact-frontend` will be slower due to starting all versions of\\n`assets-frontend`\", because:\\n* It is a quick solution that does not involve further investigation to support a test case that we hope to deprecate\\n* The performance impact will slow down development in terms of PR building but will not affect local development\\n* This decision can be easily revisited if performance becomes a problem, i.e. by making no changes there will be no\\nrolling back needed if we remove the test or decide to change other services\\n### Positive Consequences\\n* Allows us to add browser-based acceptance tests for forms with `contact-frontend` relying on `assets-frontend`\\n* Does not involve further development time\\n* Proven to work in the build environment\\n### Negative Consequences\\n* Will definitely cause a negative impact on the time taken to build a release, or run a PR builder\\n\n\n## Context\\nWhen developing new algorithms and features for PACE-related software,\\ndevelopers often create useful demo/visualisation scripts for their own use.\\nThese scripts could be useful or interesting for other developers, and are\\nimportant for reproducibility or justifying design decisions. They should be\\nstored somewhere in version control so that they can be easily accessed by any\\ndevelopers and referred to later. However, they are not intended for general\\nuse and will not be actively maintained or tested. There are 2 main options:\\n* Store them in a `dev_scripts` directory in each separate project repository\\n* Store them in a `scripts` directory in `pace-developers`\\nIf they're in the `dev_scripts` directory for each project repository:\\n+ All in one place\\n+ Scripts will be close to the code they are used for\\n- Scripts may not work with the version of the code they are distributed with\\n- It's unclear where scripts that use more than one project would go\\n- Despite the folder being called `dev_scripts` people might expect the scripts\\nto actually work as they're in the main project repository\\nIf they're in a `scripts` directory in `pace-developers`:\\n+ They can be kept close to the decision-making developer documentation that\\nthey support\\n+ A version can be specified for any project dependencies\\n\n## Decision\n", "response": "<pad> * assets-frontend is a deprecated service - we want to ensure the functionality using it does not break, but we arenreluctant to invest large amounts of time in itn* Service Manager is a Python application used heavily across the Platform and not owned by PlatUI, and we would like to avoid making changes to it if possible to support our workflow.n* Similarly the job builders in the build-jobs repo are heavily shared, and we would like to avoid making changes tonbuild-jobs if possiblenChosen option: \"Accept that builds of assets-frontend will be slower due to starting all versions ofnassets-frontend\", because:n* It is a quick solution that does not involve further investigation to support a test case that we hope to deprecaten* The performance impact will slow down development in terms of PR building but will not affect local developmentn* This decision can be easily revisited if performance becomes a problem, i.e. by making no changes there will be nonrolling back needed if we remove the test or decide to change other servicesn### Positive Consequencesn* Allows us to add browser-based acceptance tests for forms with facebook-frontend relying on assets-frontendn* Does not involve further development timen* Proven to work in the build environmentn### Negative Consequencesn* Will definitely cause a negative impact on the time taken to build a release, or run a PR buildern", "matched_ids": [2971, 1753], "time": "0:01:56.705717", "post": "## Decision\n* assets-frontend is a deprecated service - we want to ensure the functionality using it does not break, but we arenreluctant to invest large amounts of time in itn* Service Manager is a Python application used heavily across the Platform and not owned by PlatUI, and we would like to avoid making changes to it if possible to support our workflow \n * Similarly the job builders in the build-jobs repo are heavily shared, and we would like to avoid making changes tonbuild-jobs if possiblenChosen option: \"Accept that builds of assets-frontend will be slower due to starting all versions ofnassets-frontend\", because:n* It is a quick solution that does not involve further investigation to support a test case that we hope to deprecaten* The performance impact will slow down development in terms of PR building but will not affect local developmentn* This decision can be easily revisited if performance becomes a problem, i.e. by making no changes there will be nonrolling back needed if we remove the test or decide to change other servicesn### Positive Consequencesn* Allows us to add browser-based acceptance tests for forms with facebook-frontend relying on assets-frontendn* Does not involve further development timen* Proven to work in the build environmentn### Negative Consequencesn* Will definitely cause a negative impact on the time taken to build a release, or run a PR buildern"}
{"context": "**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\\nfor all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https://en.cppreference.com/w/cpp/io/ios_base/iostate\\n[bigtable-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/bigtable\\n[gcs-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/storage\\n[survey-link]: https://isocpp.org/blog/2018/03/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n", "input": "**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https://github.com/googleapis/google-cloud-cpp/issues/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n\n## Decision\nwill not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https://github.com/googleapis/google-cloud-cpp/issues/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n\n\n**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https://github.com/googleapis/google-cloud-cpp/issues/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n\n## Decision\nwill not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https://github.com/googleapis/google-cloud-cpp/issues/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n\n\n**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\\nfor all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https://en.cppreference.com/w/cpp/io/ios_base/iostate\\n[bigtable-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/bigtable\\n[gcs-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/storage\\n[survey-link]: https://isocpp.org/blog/2018/03/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n\n## Decision\n", "response": "<pad> **Consequences**: This decision will shield from endorsing any particular binary package management system. Other individuals who want to build and maintain anbinary distribution of our code may do so without our permission or knowledge. If they need small and reasonable tweaks from us to support their binaryndistributions, they may file an issue and we'll do our best to accommodate them within reason.n", "matched_ids": [5061, 2462], "time": "0:00:41.979233", "post": "## Decision\n**Consequences**: This decision will shield from endorsing any particular binary package management system. Other individuals who want to build and maintain anbinary distribution of our code may do so without our permission or knowledge. If they need small and reasonable tweaks from us to support their binaryndistributions, they may file an issue and we'll do our best to accommodate them within reason \n "}
{"context": "## Context and Problem Statement\\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\\n", "input": "## Context\\nDeploying hosted 3Bot takes a lot of time because of cloning and installation time.\\nIt can make use of the new flist build by the CI to be always up to date and reduce the installation time.\\n\n## Decision\nUpdate the flist with the up to date one\\n\n\n## Context\\nTravisCI is currently used for building feature branches, pull requests and releases.\\nThe way releases, however, are performed is a bit convoluted and not flexible. By default minor version is updated\\nand it's not clear how to do a major or patch release. Due to the way TravisCI builds feature branches vs pull requests\\nit requires hacks in travis config.\\n\n## Decision\nSolution implemented in https://github.com/ifrost/starterkit have worked quite well so far. It's based on adding\\nlabels to commit messages when merging a pull request to master.\\nIt's way easier to do major/minor/patch or hotfix release and publish to npm separately.\\n\n\n## Context and Problem Statement\\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\\n\n## Decision\n", "response": "<pad> We will use a stack plugin to build the stack module.n", "matched_ids": [5201, 5106], "time": "0:00:25.899036", "post": "## Decision\nWe will use a stack plugin to build the stack module \n "}
{"context": "## Context and Problem Statement\\nIn order to simplify the tracking consent build and deploy process and\\nmake integrating with tracking consent less surprising, should\\nthe configuration of the GTM container used by tracking consent be via\\ndata attributes rather than separate bundles?\\n## Decision Drivers\\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\n", "input": "## Context and Problem Statement\\nA solution for isolating third-party payment widgets from the rest of account\\nmanagement can include building a separate web app on its own dedicated domain.\\nAs a side effect in building that app, we have the opportunity to choose\\ntechnologies for building it that don't necessarily follow the rest of FxA.\\n## Decision Drivers\\n* Opportunity for a fresh start with tech stack without rewriting\\n* Security in dealing with payment transactions\\n* Developer ergonomics\\n* Code quality & testing\\n* Subscription services deadlines\\n\n## Decision\n* Opportunity for a fresh start with tech stack without rewriting\\n* Security in dealing with payment transactions\\n* Developer ergonomics\\n* Code quality & testing\\n* Subscription services deadlines\\nChosen option: \"React, Redux, Typescript\", because\\n* Chance to start with a fresh stack\\n* More vibrant ecosystem\\n* Better tooling & developer ergonomics\\n\n\n## Context and Problem Statement\\nIn the course of implementing a class-based role metadata model we have also\\nreviewed options on how to design serialization infrastructure between wire\\nformats and the class model. In an initial attempt we have implemented\\nserialization on the metadata class (see option 1), but issues with inheritance\\nand calls for more flexibility have caused us to rethink this approach.\\n## Decision Drivers\\n* A class-based role metadata model (see ADR4) requires serialization routines\\nfrom and to wire formats\\n* TUF integrators may require custom serialization implementations for custom\\nwire formats\\n* Readability and simplicity of implementation for users and maintainers\\n* Recognizability of specification\\n\n## Decision\n* A class-based role metadata model (see ADR4) requires serialization routines\\nfrom and to wire formats\\n* TUF integrators may require custom serialization implementations for custom\\nwire formats\\n* Readability and simplicity of implementation for users and maintainers\\n* Recognizability of specification\\nChosen option: \"Compromise 2\", because implementing dict conversion as methods\\non a corresponding class is idiomatic and allows for well-structured code.\\nTogether with a separated serialization interface, it provides both ease of use\\nand maintenance, and full flexibility with regards to custom serialization\\nimplementations and wire formats.\\n\n\n## Context and Problem Statement\\nIn order to simplify the tracking consent build and deploy process and\\nmake integrating with tracking consent less surprising, should\\nthe configuration of the GTM container used by tracking consent be via\\ndata attributes rather than separate bundles?\\n## Decision Drivers\\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\n\n## Decision\n", "response": "<pad> * The need to keep things simple for service developersn* The need to improve the operability of tracking consentnChosen option: \"Option 1\", because it provides both ease of usenand maintenance, and provides a clearer understanding of thenoptions.n", "matched_ids": [372, 4254], "time": "0:00:28.499045", "post": "## Decision\n* The need to keep things simple for service developersn* The need to improve the operability of tracking consentnChosen option: \"Option 1\", because it provides both ease of usenand maintenance, and provides a clearer understanding of thenoptions \n "}
{"context": "## Context\\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\\n", "input": "## Context and Problem Statement\\nWhen creating and updating pattern languages and patterns, the structure should be defined so that users can implement their own renderers for a specific pattern language.\\n## Decision Drivers\\n* Pattern languages can be very different, so we should restrict their properties as little as possible.\\n\n## Decision\n* Pattern languages can be very different, so we should restrict their properties as little as possible.\\nA pattern consists of\\n* name (string), e.g. \"Elastic Infrastructure\"\\n* type (string), e.g. <https://purl.org/patternpedia/cloudcomputingpatterns#CloudComputingPattern>\\n* uri (string, e.g. <https://purl.org/patternpedia/cloudcomputingpatterns/elasticinfrastructure#ElasticInfrastructure>)\\n* a Map for the section properties of its pattern language (Map<section, string | string[]>),\\ne.g. for the section  https://purl.org/patternpedia/cloudcomputingpatterns#hasLogo we can obtain the corresponding value  \"https://www.cloudcomputingpatterns.org/img/book.png\" or an array of strings\\nTo render the pattern properties best, section should contain information about the value type, e.g. xsd:anyURI, xsd:string. This will allow us to display properties like \"https://www.cloudcomputingpatterns.org/img/book.png\" as links/pictures and not only as text.\\n\n\n## Context\\nIt is often discussed whether to work with plain SQL or with the Data Abstraction Layer.\\n\n## Decision\nIn the following application layers, the DAL should be used for the following reasons:\\n* In the Store API\\n* Data selected and returned via Store API must be extensible by third party developers.\\n* Requests against the Store API should always allow additional data to be loaded.\\n* Data retrieval and encoding must be secured by ACL.\\n* Storefront page loader and controller\\n* Data passed from the storefront to the Twig templates must be extensible by third party developers.\\n* Since our templates are customized by many developers, we cannot provide only a minimal offset of the actual data.\\n* On admin API level\\n* Data selected and returned via admin API must be extensible by third party developers.\\n* Requests that go against the Store API should always allow additional data to be loaded.\\n* Data retrieval and encoding must be secured by ACL.\\n* When writing data\\n* The DAL has a validation, event and indexing system which is used for the write process. Therefore, it is mandatory to ensure data integrity, that write processes take place exclusively via the DAL.\\n* The entity indexers are an exception here, see below.\\nIn the following application layers you should work with plain SQL because of the following reasons:\\n* In the entity indexers\\n* The entity indexers are located behind the entity repository layer, so it only makes sense that they do not work with the repositories but with the database connection directly.\\n* the entity indexers must be able to re-index all data after a versions update. To avoid as much hydration and event overhead as possible, they should work directly with the connection.\\n* The entity indexers are not an extension point of shopware. The queries that are executed there are only used for internal processing of data and should never be rewritten.\\n* In Core Components\\n* Core components like the theme compiler, request transformer, etc. are not places where a third party developer should be able to load additional data. The data loaded here is for pure processing only and should never be rewritten.\\n* Deep processes like theme compiling should not be affected by plugin entity schemas, because plugins are an optional part of the system and might be in an unstable state during an update process.\\n\n\n## Context\\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\\n\n## Decision\n", "response": "<pad> * In the following application layers, the DAL should be used for the following reasons:n* In the Store APIn* Data selected and returned via Store API must be extensible by third party developers.n* Data retrieval and encoding must be secured by ACL.n* Storefront page loader and controllern* Data passed from the storefront to the Twig templates must be extensible by third party developers.n* Since our templates are customized by many developers, we cannot provide only a minimal offset of the actual data.n* On admin API leveln* Data selected and returned via Store API must be extensible by third party developers.n* Data retrieval and encoding must be secured by ACL.n* When writing datan* The DAL has a validation, event and indexing system which is used for the write process. Therefore, it is mandatory to ensure data integrity, that write processes take place exclusively via the DAL.n* The entity indexers are an exception here, see below.nIn the following application layers you should work with plain SQL because of the following reasons:n* In the entity indexersn* The entity indexers are located behind the entity repository layer, so it only makes sense that they do not work with the repositories but with the database connection directly.n* The entity indexers must be able to re-index all data after a versions update. To avoid as much hydration and event overhead as possible, they should work directly with the connection.n* The entity indexers are not an extension point of shopware. The queries that are executed there are only used for internal processing of data and should never be rewritten.n* In Core Componentsn* Core components like the theme compiler, request transformer, etc. are not places where a third party developer should be able to load additional data. The data loaded here is for pure processing only and should never be rewritten.n* Deep processes like theme compiling should not be affected by plugin entity schemas, because plugins are an optional part of the system and might be in an unstable state during an update process.n", "matched_ids": [347, 4511], "time": "0:01:39.019542", "post": "## Decision\n* In the following application layers, the DAL should be used for the following reasons:n* In the Store APIn* Data selected and returned via Store API must be extensible by third party developers \n * Data retrieval and encoding must be secured by ACL \n * Storefront page loader and controllern* Data passed from the storefront to the Twig templates must be extensible by third party developers \n * Since our templates are customized by many developers, we cannot provide only a minimal offset of the actual data \n * On admin API leveln* Data selected and returned via Store API must be extensible by third party developers \n * Data retrieval and encoding must be secured by ACL \n * When writing datan* The DAL has a validation, event and indexing system which is used for the write process. Therefore, it is mandatory to ensure data integrity, that write processes take place exclusively via the DAL \n * The entity indexers are an exception here, see below \n In the following application layers you should work with plain SQL because of the following reasons:n* In the entity indexersn* The entity indexers are located behind the entity repository layer, so it only makes sense that they do not work with the repositories but with the database connection directly \n * The entity indexers must be able to re-index all data after a versions update. To avoid as much hydration and event overhead as possible, they should work directly with the connection \n * The entity indexers are not an extension point of shopware. The queries that are executed there are only used for internal processing of data and should never be rewritten \n * In Core Componentsn* Core components like the theme compiler, request transformer, etc. are not places where a third party developer should be able to load additional data. The data loaded here is for pure processing only and should never be rewritten \n * Deep processes like theme compiling should not be affected by plugin entity schemas, because plugins are an optional part of the system and might be in an unstable state during an update process \n "}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "input": "## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard\\nin this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n", "response": "<pad> We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).n", "matched_ids": [494, 1269], "time": "0:00:21.788380", "post": "## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) \n "}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "input": "## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard\\nin this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n", "response": "<pad> We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).n", "matched_ids": [494, 1269], "time": "0:00:24.983712", "post": "## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) \n "}
{"context": "## Context\\nI need to implement two special pages, the privacy policy page and the about me page.\\nThey can't be in the map, I don't want them there because the user would\\nbe forced to read them when scanning the spiral.\\n", "input": "## Context\\nWe need to allow a number of users to sign in to the service in order to use it.\\nIn order to implement this quickly, we'll use Auth0 to manage this.\\nAs Auth0's authentication uses OAuth2, it should be straightforward to migrate\\nto another service, if BEIS have a preference for something else.\\nAuth0 provides views for authentication that we can use in our user journeys.\\nThere are two versions of these views 'Classic' and 'New'.\\nClassic uses JavaScript and is not progressively enhanced. New uses language\\nthat results in a poor user experience.\\n\n## Decision\nWe will use the free tier and 'Classic' views of Auth0 for the private beta.\\n\n\n## Context\\nIn [adr-008][1] we agreed on focusing only on English pages.\\nWe have noticed during the last few months that this restriction is causing more\\nissues than benefits, because:\\n- We have Content Items with the same `content_id` and different `locale`s.\\n- We needed to handle edge cases when retrieving the information from Publishing API in order to work around only retrieving English language content.\\n\n## Decision\nTrack Content metrics of all Content Item regardless of their locale.\\n\n\n## Context\\nI need to implement two special pages, the privacy policy page and the about me page.\\nThey can't be in the map, I don't want them there because the user would\\nbe forced to read them when scanning the spiral.\\n\n## Decision\n", "response": "<pad> I will use security_policy and about_me to implement these pages.n", "matched_ids": [2405, 1862], "time": "0:00:20.812769", "post": "## Decision\nI will use security_policy and about_me to implement these pages \n "}
{"context": "## Context\\nThe current request builder APIs are not able to handle some odata requests like:\\n- query navigation properties `GET /People('scottketchum')/Friends`\\n- getting \"raw value\" of a property `/People('scottketchum')/$value`\\n", "input": "## Context\\nIn the light client (or any other client), the user may want to **subscribe to\\na subset of transactions** (rather than all of them) using `/subscribe?event=X`. For\\nexample, I want to subscribe for all transactions associated with a particular\\naccount. Same for fetching. The user may want to **fetch transactions based on\\nsome filter** (rather than fetching all the blocks). For example, I want to get\\nall transactions for a particular account in the last two weeks (`tx's block time >= '2017-06-05'`).\\nNow you can't even subscribe to \"all txs\" in Tendermint.\\nThe goal is a simple and easy to use API for doing that.\\n![Tx Send Flow Diagram](img/tags1.png)\\n\n## Decision\nABCI app return tags with a `DeliverTx` response inside the `data` field (_for\\nnow, later we may create a separate field_). Tags is a list of key-value pairs,\\nprotobuf encoded.\\nExample data:\\n```json\\n{\\n\"abci.account.name\": \"Igor\",\\n\"abci.account.address\": \"0xdeadbeef\",\\n\"tx.gas\": 7\\n}\\n```\\n### Subscribing for transactions events\\nIf the user wants to receive only a subset of transactions, ABCI-app must\\nreturn a list of tags with a `DeliverTx` response. These tags will be parsed and\\nmatched with the current queries (subscribers). If the query matches the tags,\\nsubscriber will get the transaction event.\\n```\\n/subscribe?query=\"tm.event = Tx AND tx.hash = AB0023433CF0334223212243BDD AND abci.account.invoice.number = 22\"\\n```\\nA new package must be developed to replace the current `events` package. It\\nwill allow clients to subscribe to a different types of events in the future:\\n```\\n/subscribe?query=\"abci.account.invoice.number = 22\"\\n/subscribe?query=\"abci.account.invoice.owner CONTAINS Igor\"\\n```\\n### Fetching transactions\\nThis is a bit tricky because a) we want to support a number of indexers, all of\\nwhich have a different API b) we don't know whenever tags will be sufficient\\nfor the most apps (I guess we'll see).\\n```\\n/txs/search?query=\"tx.hash = AB0023433CF0334223212243BDD AND abci.account.owner CONTAINS Igor\"\\n/txs/search?query=\"abci.account.owner = Igor\"\\n```\\nFor historic queries we will need a indexing storage (Postgres, SQLite, ...).\\n### Issues\\n- https://github.com/tendermint/tendermint/issues/376\\n- https://github.com/tendermint/tendermint/issues/287\\n- https://github.com/tendermint/tendermint/issues/525 (related)\\n\n\n## Context\\nIn the light client (or any other client), the user may want to **subscribe to\\na subset of transactions** (rather than all of them) using `/subscribe?event=X`. For\\nexample, I want to subscribe for all transactions associated with a particular\\naccount. Same for fetching. The user may want to **fetch transactions based on\\nsome filter** (rather than fetching all the blocks). For example, I want to get\\nall transactions for a particular account in the last two weeks (`tx's block time >= '2017-06-05'`).\\nNow you can't even subscribe to \"all txs\" in Tendermint.\\nThe goal is a simple and easy to use API for doing that.\\n![Tx Send Flow Diagram](img/tags1.png)\\n\n## Decision\nABCI app return tags with a `DeliverTx` response inside the `data` field (_for\\nnow, later we may create a separate field_). Tags is a list of key-value pairs,\\nprotobuf encoded.\\nExample data:\\n```json\\n{\\n\"abci.account.name\": \"Igor\",\\n\"abci.account.address\": \"0xdeadbeef\",\\n\"tx.gas\": 7\\n}\\n```\\n### Subscribing for transactions events\\nIf the user wants to receive only a subset of transactions, ABCI-app must\\nreturn a list of tags with a `DeliverTx` response. These tags will be parsed and\\nmatched with the current queries (subscribers). If the query matches the tags,\\nsubscriber will get the transaction event.\\n```\\n/subscribe?query=\"tm.event = Tx AND tx.hash = AB0023433CF0334223212243BDD AND abci.account.invoice.number = 22\"\\n```\\nA new package must be developed to replace the current `events` package. It\\nwill allow clients to subscribe to a different types of events in the future:\\n```\\n/subscribe?query=\"abci.account.invoice.number = 22\"\\n/subscribe?query=\"abci.account.invoice.owner CONTAINS Igor\"\\n```\\n### Fetching transactions\\nThis is a bit tricky because a) we want to support a number of indexers, all of\\nwhich have a different API b) we don't know whenever tags will be sufficient\\nfor the most apps (I guess we'll see).\\n```\\n/txs/search?query=\"tx.hash = AB0023433CF0334223212243BDD AND abci.account.owner CONTAINS Igor\"\\n/txs/search?query=\"abci.account.owner = Igor\"\\n```\\nFor historic queries we will need a indexing storage (Postgres, SQLite, ...).\\n### Issues\\n- https://github.com/tendermint/tendermint/issues/376\\n- https://github.com/tendermint/tendermint/issues/287\\n- https://github.com/tendermint/tendermint/issues/525 (related)\\n\n\n## Context\\nThe current request builder APIs are not able to handle some odata requests like:\\n- query navigation properties `GET /People('scottketchum')/Friends`\\n- getting \"raw value\" of a property `/People('scottketchum')/$value`\\n\n## Decision\n", "response": "<pad> ABCI app return tags with a DeliverTx response inside the data field (_fornnow, later we may create a separate field_). Tags is a list of key-value pairs,nprotobuf encoded.nExample data:njsonnn\"abci.account.name\": \"Igor\",n\"abci.account.address\": \"0xdeadbeef\",n\"tx.gas\": 7nn### Subscribing for transactions eventsnIf the user wants to receive only a subset of transactions, ABCI-app mustnreturn a list of tags with a DeliverTx response inside the data field (_fornnow, later we may create a separate field_). Tags is a list of key-value pairs,nprotobuf encoded.nExample data:njsonnn\"abci.account.name\": \"Igor\",n\"abci.account.invoice.number = 22\"n/subscribe?query=\"abci.account.invoice.owner = Igor\"nnFor historic queries we will need a indexing storage (Postgres, SQLite,...).n### Issuesn- https://github.com/tendermint/tendermint/issues/376n- https://github.com/tendermint/tendermint/issues/525 (related)n", "matched_ids": [1941, 4106], "time": "0:01:21.310658", "post": "## Decision\nABCI app return tags with a DeliverTx response inside the data field (_fornnow, later we may create a separate field_). Tags is a list of key-value pairs,nprotobuf encoded \n Example data:njsonnn\"abci.account \n ame\": \"Igor\",n\"abci.account.address\": \"0xdeadbeef\",n\"tx.gas\": 7nn### Subscribing for transactions eventsnIf the user wants to receive only a subset of transactions, ABCI-app mustnreturn a list of tags with a DeliverTx response inside the data field (_fornnow, later we may create a separate field_). Tags is a list of key-value pairs,nprotobuf encoded \n Example data:njsonnn\"abci.account \n ame\": \"Igor\",n\"abci.account.invoice \n umber = 22\"n/subscribe?query=\"abci.account.invoice.owner = Igor\"nnFor historic queries we will need a indexing storage (Postgres, SQLite,...) \n ### Issuesn- https://github.com/tendermint/tendermint/issues/376n- https://github.com/tendermint/tendermint/issues/525 (related)n"}
{"context": "## Context\\nFrequency Capping allows Content Managers to limit the number of\\nimpressions or interactions users have with content. Is a widely\\navailable tool in Publishing Platforms.\\nIt's usually developed on the server side where the system can decide\\nhow many times to serve the content to the requesting users which we\\ncall \"Global Frequency Capping\". Additionally the system may be able\\nto limit the number of impressions per user which we call \"Local\" or\\n\"User Frequency Capping\".\\nFor example a Content Piece can be set to 1,000,000 Global Impressions\\nand 1 Impression per User, thus indirectly driving 1,000,000 different\\nusers to this Content.\\nThis functionality has been lacking from the Snippet Service due to\\ntechnical limitations imposed by the way metrics were collected and\\ncontent selection was handled on the client side. The latest\\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\\nPipeline unblock this capability. [0]\\n", "input": "## Context\\nWe need to persist the user data to the backend. Rather than building our\\nfrontend to be tightly coupled to an API, we want a layer that can sit between\\nthe application and the API.\\nWe know that we will need to support offline use of these forms. We also want\\nusers to be able to recover unsubmitted forms if they haven't gone online\\nbetween sessions.\\nWe also know that some forms will require the upload of images.\\nIndexedDB is a [well supported](https://caniuse.com/#feat=indexeddb) browser\\ndatabase that persists data across sessions. It supports storing blob data (such\\nas images), and has decent storage limits.\\n\n## Decision\nWe will support IndexedDB as a data store layer, sitting between the application\\nand the API.\\n\n\n## Context\\nThe Cloud Platform was originally set up to have a single ingress controller to\\nmanage all ingresses in the cluster. So, every new ingress added a config block\\nto one, large nginx config file, and all traffic to all services hosted on the\\nCloud Platform is routed through a single AWS load balancer.\\nAlthough this was both easy to manage, and saved us some money on load\\nbalancers (approx. $25/month per ingress), it has become unsustainable. We\\nusually have 6 replicas of the ingress controller pod, and we have started to\\nsee instances of several of these pods crash-looping (usually because they have\\nrun out of shared memory, which cannot be increased in kubernetes. See [this\\nissue] for more information).\\nWe believe this is because the nginx config has become so large (over 100K\\nlines), that sometimes pods fail to reload it when it is changed, or the pod is\\nmoved.\\n\n## Decision\nWe will create a separate AWS load balancer and ingress-controller for every\\nnamespace in the cluster. An \"ingress class\" annotation will cause traffic for\\na particular ingress to be routed through the appropriate AWS load balancer and\\ningress-controller. See our [module repository] for more details.\\n\"System\" ingresses (e.g. those used for concourse, grafana, etc.) will continue\\nto use the default ingress-controller. There should only ever be a handful of\\nthese, compared with hundreds of team ingresses, so the load on the default\\ningress-controller should stay within acceptable limits.\\n\n\n## Context\\nFrequency Capping allows Content Managers to limit the number of\\nimpressions or interactions users have with content. Is a widely\\navailable tool in Publishing Platforms.\\nIt's usually developed on the server side where the system can decide\\nhow many times to serve the content to the requesting users which we\\ncall \"Global Frequency Capping\". Additionally the system may be able\\nto limit the number of impressions per user which we call \"Local\" or\\n\"User Frequency Capping\".\\nFor example a Content Piece can be set to 1,000,000 Global Impressions\\nand 1 Impression per User, thus indirectly driving 1,000,000 different\\nusers to this Content.\\nThis functionality has been lacking from the Snippet Service due to\\ntechnical limitations imposed by the way metrics were collected and\\ncontent selection was handled on the client side. The latest\\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\\nPipeline unblock this capability. [0]\\n\n## Decision\n", "response": "<pad> We will create a separate AWS load balancer and ingress-controller for everynnamespace in the cluster. An \"ingress class\" annotation will cause traffic forna particular ingress to be routed through the appropriate AWS load balancer andningress-controller. See our [module repository] for more details.n\"System\" ingresses (e.g. those used for concourse, grafana, etc.) will continue to use the default ingress-controller. There should only ever be a handful ofnthese, compared with hundreds of team ingresses, so the load on the defaultningress-controller should stay within acceptable limits.n", "matched_ids": [5257, 620], "time": "0:00:43.286751", "post": "## Decision\nWe will create a separate AWS load balancer and ingress-controller for everynnamespace in the cluster. An \"ingress class\" annotation will cause traffic forna particular ingress to be routed through the appropriate AWS load balancer andningress-controller. See our [module repository] for more details \n \"System\" ingresses (e.g. those used for concourse, grafana, etc.) will continue to use the default ingress-controller. There should only ever be a handful ofnthese, compared with hundreds of team ingresses, so the load on the defaultningress-controller should stay within acceptable limits \n "}
{"context": "## Context\\nThe application is about finding services closest to the search point. All\\nservices have a co-ordinate in lat/lon format. Currently the application\\nrequests either an out-code or a postcode as the search point. The submitted\\npostcode needs to be resolved to a lat/lon that can be used to query against.\\n", "input": "## Context\\nIn a multitenancy setup, there are multiple commercetools projects in one notification module instance.\\nWhen a notification comes from Adyen, the notification module needs to find the correct commercetools project for the notification.\\nThere are 2 possible options to tackle this problem:\\n1. The commercetools project name is a part of the notification callback URL (e.g. URL path) that is set up in Adyen Admin.\\n1. The commercetools project name is an attribute in the notification itself.\\nThe solution should be as less work as possible for the user of the integration. Additionally, it should work in a serverless environment where the URL to the function could be randomly generated.\\n\n## Decision\n- We will have an attribute in the notification itself. Its value will be the commercetools project name that the notification belongs to.\\n- In order to have the commercetools project name in every notification, we will use [`metadata` attribute](https://docs.adyen.com/api-explorer/#/CheckoutService/v66/post/payments__reqParam_metadata). This attribute will be set in the extension module in every request to Adyen.\\n- The value for `metadata` attribute will be taken from a custom field `custom.fields.commercetoolsProjectKey`. Its value will be set by the user of the integration.\\n\n\n## Context\\nExample: take principles from an asciidoc source.\\nFor sync, having uuid might be an issue unless we use a \"source\" optional\\nattribute. That might make things more complex...\\nA source can be empty (when created from GUI) or set to the FQCN of the\\nprovisioner.\\nEven with a source attribute, having UUID does not make the automatic\\nidentification of a category or a type, based on name only, easy.\\nHence the need for a semantic id.\\nRisks of collision for provisioners remains and provisioned items will have\\nto prepend the id of their provisioner to their id.\\nA qualified source attribute still make sense to identify items which\\ncannot be edited. To fit with the usual way of Ontrack, the source must\\nbe a FQCN which maps to an `Extension`.\\n\n## Decision\n* do not use UUID for indicator items (categories, types & protfolios)\\n* for provisioned items, prepend the id of the provisioner to the item id\\n* introduce a source attribute and an indicator source model\\n\n\n## Context\\nThe application is about finding services closest to the search point. All\\nservices have a co-ordinate in lat/lon format. Currently the application\\nrequests either an out-code or a postcode as the search point. The submitted\\npostcode needs to be resolved to a lat/lon that can be used to query against.\\n\n## Decision\n", "response": "<pad> We will have an attribute in the notification itself. Its value will be the commercetools project name that the notification belongs to.nIn order to have the commercetools project name in every notification, we will use [metadata attribute](https://docs.adyen.com/api-explorer/#/CheckoutService/v66/post/payments__reqParam_metadata). This attribute will be set in the extension module in every request to Adyen.nThe value for metadata attribute will be taken from a custom field custom.fields.commercetoolsProjectKey. Its value will be set by the user of the integration.n", "matched_ids": [1716, 5121], "time": "0:00:43.690095", "post": "## Decision\nWe will have an attribute in the notification itself. Its value will be the commercetools project name that the notification belongs to \n In order to have the commercetools project name in every notification, we will use [metadata attribute](https://docs.adyen.com/api-explorer/#/CheckoutService/v66/post/payments__reqParam_metadata). This attribute will be set in the extension module in every request to Adyen \n The value for metadata attribute will be taken from a custom field custom.fields.commercetoolsProjectKey. Its value will be set by the user of the integration \n "}
{"context": "## Context or problem to solve\\n* How should git histories look like?\\n* What format should we use to write commit messages?\\n", "input": "## Context and Problem Statement\\n- How do we call the folder for assets?\\n- What is the pattern of the filename where an ADR is stored?\\n- How do we write titles in the ADR documents?\\n\n## Decision\n### Asset folder\\nChosen option: `resources`\\n### Filename\\nChosen option: `docs/ADR-NNNN-title-using-dashes.md` and `docs/resources/ADR-NNNN/{filename}`, because\\n- Adding `arch` or `adr` folders is redundant.\\n- `NNNN` provides a unique number, which can be used for referencing using an immutable number (and generating permalinks if we render the repository to a static site).\\n- The creation time of an ADR is of historical interest only, if it gets updated somehow. It can be embeded in the file or extracted from git history.\\n- Having no spaces in filenames eases working in the command line.\\n- Prefixing with `docs/ADR-` enables future new kinds of documents.\\n- Calling a folder `docs/resources/ADR-NNNN/{filename}` (omiting the `title-with-dashes`) helps in refactors or changes in the file names.\\n### Title (markdown)\\nChosen option: `# Plain title` because:\\n- At the moment of writing the ADR the writer may not know the final ADR number.\\n- It can be embeded by rendering tools\\n- It would duplicate data from the filename\\n\n\n## Context and Problem Statement\\nShould we consider and address security requirements?\\n\n## Decision\nFor the initial start of the project we do not consider security as a must have.\\n\n\n## Context or problem to solve\\n* How should git histories look like?\\n* What format should we use to write commit messages?\\n\n## Decision\n", "response": "<pad> ### docsnChosen option: docs/ADR-NNNN-title-using-dashes.md and docs/resources/ADR-NNNN/filename, becausen- Adding arch or adr folders is redundant.n- NNNN provides a unique number, which can be used for referencing using an immutable number (and generating permalinks if we render the repository to a static site).n- The creation time of an ADR is of historical interest only, if it gets updated somehow. It can be embeded in the file or extracted from git history.n### Title (markdown)nChosen option: # Plain title because:n- At the moment of writing the ADR the writer may not know the final ADR number.n- It can be embeded by rendering toolsn- It would duplicate data from the filenamen### Title (markdown)nChosen option: # Plain title because:n- At the moment of writing the ADR the writer may not know the final ADR number.n- It can be embeded by rendering toolsn- It would duplicate data from the filenamen", "matched_ids": [4619, 4681], "time": "0:01:01.205031", "post": "## Decision\n### docsnChosen option: docs/ADR-NNNN-title-using-dashes.md and docs/resources/ADR-NNNN/filename, becausen- Adding arch or adr folders is redundant \n - NNNN provides a unique number, which can be used for referencing using an immutable number (and generating permalinks if we render the repository to a static site) \n - The creation time of an ADR is of historical interest only, if it gets updated somehow. It can be embeded in the file or extracted from git history \n ### Title (markdown)nChosen option: # Plain title because:n- At the moment of writing the ADR the writer may not know the final ADR number \n - It can be embeded by rendering toolsn- It would duplicate data from the filenamen### Title (markdown)nChosen option: # Plain title because:n- At the moment of writing the ADR the writer may not know the final ADR number \n - It can be embeded by rendering toolsn- It would duplicate data from the filenamen"}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "input": "## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard\\nin this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n", "response": "<pad> We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).n", "matched_ids": [494, 1269], "time": "0:00:22.006647", "post": "## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) \n "}
{"context": "## Context\\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\\n", "input": "## Context and Problem Statement\\nWhen creating and updating pattern languages and patterns, the structure should be defined so that users can implement their own renderers for a specific pattern language.\\n## Decision Drivers\\n* Pattern languages can be very different, so we should restrict their properties as little as possible.\\n\n## Decision\n* Pattern languages can be very different, so we should restrict their properties as little as possible.\\nA pattern consists of\\n* name (string), e.g. \"Elastic Infrastructure\"\\n* type (string), e.g. <https://purl.org/patternpedia/cloudcomputingpatterns#CloudComputingPattern>\\n* uri (string, e.g. <https://purl.org/patternpedia/cloudcomputingpatterns/elasticinfrastructure#ElasticInfrastructure>)\\n* a Map for the section properties of its pattern language (Map<section, string | string[]>),\\ne.g. for the section  https://purl.org/patternpedia/cloudcomputingpatterns#hasLogo we can obtain the corresponding value  \"https://www.cloudcomputingpatterns.org/img/book.png\" or an array of strings\\nTo render the pattern properties best, section should contain information about the value type, e.g. xsd:anyURI, xsd:string. This will allow us to display properties like \"https://www.cloudcomputingpatterns.org/img/book.png\" as links/pictures and not only as text.\\n\n\n## Problem\\nA standard way of describing REST APIs offered by tools and plugins for Elektra is required to ease development for and usage of these. Because many good standards for describing APIs are out there already, an existing one shall be used.\\n\n## Decision\nThe decision is to use [API blueprints](https://apiblueprint.org/) together with additional tools from its ecosystem.\\n\n\n## Context\\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\\n\n## Decision\n", "response": "<pad> * We should restrict the properties of the pattern language as little as possible.n* A pattern consists ofn* name (string), e.g. \"Elastic Infrastructure\"n* type (string), e.g. https://purl.org/patternpedia/cloudcomputingpatterns#cloudcomputingPattern>n* uri (string, e.g. https://purl.org/patternpedia/cloudcomputingpatterns#elasticinfrastructure#ElasticInfrastructure>)n* a Map for the section properties of its pattern language (Mapsection, string | string[]>),ne.g. for the section https://purl.org/patternpedia/cloudcomputingpatterns#hasLogo we can obtain the corresponding value \"https://www.cloudcomputingpatterns.org/img/book.png\" or an array of stringsnTo render the pattern properties best, section should contain information about the value type, e.g. xsd:anyURI, xsd:string. This will allow us to display properties like \"https://www.cloudcomputingpatterns.org/img/book.png\" as links/pictures and not only as text.n", "matched_ids": [347, 1298], "time": "0:01:07.200007", "post": "## Decision\n* We should restrict the properties of the pattern language as little as possible \n * A pattern consists ofn* name (string), e.g. \"Elastic Infrastructure\"n* type (string), e.g. https://purl.org/patternpedia/cloudcomputingpatterns#cloudcomputingPattern>n* uri (string, e.g. https://purl.org/patternpedia/cloudcomputingpatterns#elasticinfrastructure#ElasticInfrastructure>)n* a Map for the section properties of its pattern language (Mapsection, string | string[]>),ne.g. for the section https://purl.org/patternpedia/cloudcomputingpatterns#hasLogo we can obtain the corresponding value \"https://www.cloudcomputingpatterns.org/img/book.png\" or an array of stringsnTo render the pattern properties best, section should contain information about the value type, e.g. xsd:anyURI, xsd:string. This will allow us to display properties like \"https://www.cloudcomputingpatterns.org/img/book.png\" as links/pictures and not only as text \n "}
{"context": "## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\\n", "input": "## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we should standardize on a base Docker Image for Java-based services that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use.\\nWhile there are a few popular choices and de facto standards such as OpenJDK, we need to consider the support and legal implications of that choice, particularly since we could be considered as \u2018distributing\u2019 all layers in that image, including the OS and Java binaries.\\nWe could potentially workaround being the distributors of sensitive layers using techniques like init containers, composite containers, or foreign layers.\\n\n## Decision\nWe will start from the official CentOS 7 Docker image and add the Oracle server JRE binaries for the first iteration of the Alfresco base Java image.\\nThis allows us to start from a popular OS and Java runtime which are already in the supported ACS and APS stacks and QuickStart AMIs, and closer to the most popular customer OS, RHEL, than others.  We also have precedence in distributing the Oracle JRE in our current installer.\\nThe CentOS image also contains very few known vulnerabilities which we can easily patch at the moment.\\nWhile Alpine is an attractive choice due to its simplicity, it may be more difficult to debug and support, and the majority of our components are not yet architected to take advantage of the smaller size and lower resource consumption.\\nWhile distribution workarounds mentioned above were considered, it was felt that they are not common practice and are likely to produce more problems than they solve.\\nThe summary [![report of the comparison](https://img.shields.io/badge/report%20of%20the%20comparison-PRIVATE-red.svg)](https://ts.alfresco.com/share/s/bqDcnHWpSrSGybJhMxf93A) contains more details.\\nThe base image Dockerfile is [here](https://github.com/Alfresco/alfresco-docker-base-java/blob/master/Dockerfile).\\n\n\n## Context\\nDeploying to the cloud is a fundamental part of increasing software delivery\\nfrequency.  However once the frequency is increased, it becomes increasingly\\nneeded to manage the provisioning and configuration of those cloud resources\\nlest a step is forgotten, a security vulnerability is found, or an instance is\\ncorrupted.  Following patterns found in software development, such as managing\\ninfrastructure as code (IaC) and tracking changes in source control, are\\nwidely regarded as good practice today.\\nMany cloud providers have bespoke tooling that helps manage the creation of\\nresources on their infrastructure.  While this tooling is hugely powerful, the\\ncontext of PCMT is that our cloud resources should be kept as simple as\\npossible, in part due to our constraint to allow for on-prem deployments.\\nFurther an organization that chooses to deploy PCMT as a SaaS may not use the\\nsame cloud provider as the one the PCMT project uses.\\n\n## Decision\nWe will use Terraform to provision cloud resources:  compute instances,\\nstorage, DNS, etc.\\n\n\n## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\\n\n## Decision\n", "response": "<pad> We will start from the official CentOS 7 Docker image and add the Oracle server JRE binaries for the first iteration of the Alfresco base Java image.nThis allows us to start from a popular OS and Java runtime which are already in the supported ACS and APS stacks and QuickStart AMIs, and closer to the most popular customer OS, RHEL, than others. We also have precedence in distributing the Oracle JRE in our current installer.nThe CentOS image also contains very few known vulnerabilities which we can easily patch at the moment.nWhile Alpine is an attractive choice due to its simplicity, it may be more difficult to debug and support, and the majority of our components are not yet architected to take advantage of the smaller size and lower resource consumption.nWhile distribution workarounds mentioned above were considered, it was felt that they are not common practice and are likely to produce more problems than they solve.nThe summary [![report of the comparison](https://img.shields.io/badge/report%20of%20the%20comparison-PRIVATE-red.svg)](https://ts.alfresco.com/share/s/bqDcnHWpSrSGybJhMxf93A) contains more details.nThe base image Dockerfile is [here](https://github.com/Alfresco/alfresco-docker-base-java/blob/master/Dockerfile).n", "matched_ids": [3603, 2949], "time": "0:01:14.296894", "post": "## Decision\nWe will start from the official CentOS 7 Docker image and add the Oracle server JRE binaries for the first iteration of the Alfresco base Java image \n This allows us to start from a popular OS and Java runtime which are already in the supported ACS and APS stacks and QuickStart AMIs, and closer to the most popular customer OS, RHEL, than others. We also have precedence in distributing the Oracle JRE in our current installer \n The CentOS image also contains very few known vulnerabilities which we can easily patch at the moment \n While Alpine is an attractive choice due to its simplicity, it may be more difficult to debug and support, and the majority of our components are not yet architected to take advantage of the smaller size and lower resource consumption \n While distribution workarounds mentioned above were considered, it was felt that they are not common practice and are likely to produce more problems than they solve \n The summary [![report of the comparison](https://img.shields.io/badge/report%20of%20the%20comparison-PRIVATE-red.svg)](https://ts.alfresco.com/share/s/bqDcnHWpSrSGybJhMxf93A) contains more details \n The base image Dockerfile is [here](https://github.com/Alfresco/alfresco-docker-base-java/blob/master/Dockerfile) \n "}
{"context": "## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n", "input": "## Context and Problem Statement\\nSeveral operations (crawl repo, deploy services, ...) are required to run in the background. Therefore, a background job architecture is required being able to launch arbitrary jobs in the background.\\n## Decision Drivers\\n* MUST be implemented in Java\\n* MUST offer a callback functionality\\n\n## Decision\n* MUST be implemented in Java\\n* MUST offer a callback functionality\\nChosen option: Java CompletableFuture, because of the possibility to chain multiple actions and the option to attach callbacks on completion of a background task.\\n### Positive Consequences\\n* Possibility to chain together multiple actions after a succesful execution of a background task.\\n* Use of FixedThreadPool, i.e., a pool of threads with a predefined number of threads which are reused. If all threads are active at a time and additional tasks are submitted, they will wait in a queue.\\n### Negative Consequences\\n* With the use of a FixedThreadPool, a background task may be delayed if all threads are active at the time of submission.This may produces time of waiting in the frontend of MICO.\\n\n\n## Context and Problem Statement\\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\\n## Decision Drivers <!-- optional -->\\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible/when if needed.\\n\n## Decision\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible/when if needed.\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\nStill evaluating, leaning torwards some light refactoring to enable parent component, option 1 to keep things simple.\\n<!-- ### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026 -->\\n\n\n## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n\n## Decision\n", "response": "<pad> * readable API documentationn* effort of manually creating client servicesnChosen option: \"Separate API documentation\"nbecause [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force |... | comes out best (see below)]. -->nStill evaluating, leaning torwards some light refactoring to enable parent component, option 1 to keep things simple.n!-- ### Positive Consequencesn* [e.g., improvement of quality attribute satisfaction, follow-up decisions required,...]n*...n### Negative consequencesn* [e.g., compromising quality attribute, follow-up decisions required,...]n*... -->n", "matched_ids": [4674, 3187], "time": "0:00:45.986357", "post": "## Decision\n* readable API documentationn* effort of manually creating client servicesnChosen option: \"Separate API documentation\"nbecause [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force |... | comes out best (see below)]. -->nStill evaluating, leaning torwards some light refactoring to enable parent component, option 1 to keep things simple \n !-- ### Positive Consequencesn* [e.g., improvement of quality attribute satisfaction, follow-up decisions required,...]n*.. \n ### Negative consequencesn* [e.g., compromising quality attribute, follow-up decisions required,...]n*... -->n"}
{"context": "## Context and Problem Statement\\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\\nand was only noticed some days after when the Project Manager needed to\\nexport the data. Upon investigation, it was discovered that:\\n* Error logging was inactive on the application. (Now fixed)\\n* We can\u2019t debug using the responses our system returned to PayPal as\\nwe don\u2019t have access to the IPN log.\\nThis led to a situation where we couldn't get the information required\\nto debug the error. It was suggested we queue all incoming requests from\\nPayPal on our own system for processing by our system.\\n## Decision Drivers\\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\n", "input": "## Context\\n1. We have a fixed deadline\\n1. We will have our first users week beginning 7 June\u00a02021\\n1. For our Delius integration, we rely on an intricate set of reference data\\n1. We are uncertain if there are any issues with the reference data for all possible combinations of offenders\\n1. We rely on these integrations to set up \"container\" data in nDelius. If they fail, we need to recover before providers book appointments\\nWe want to ensure when updates fail, we have a way to resume that retries the interaction.\\nHowever, **we do not have retry mechanisms in place yet**.\\nWe could add an admin task/script to retry a certain event (we use Spring Application Events that don't\\ncreate side effects, so we could retrigger those).\\nWe feel this would create an overhead that would be too much administrative burden together with\\nthe anticipated noise of most users starting the service at the same time.\\n\n## Decision\nDue to the uncertainties and lack of convenient retry mechanism,\\nwe favour **consistency over availabilty** in the short term.\\n\n\n## Context\\nThe Cloud Platform was originally set up to have a single ingress controller to\\nmanage all ingresses in the cluster. So, every new ingress added a config block\\nto one, large nginx config file, and all traffic to all services hosted on the\\nCloud Platform is routed through a single AWS load balancer.\\nAlthough this was both easy to manage, and saved us some money on load\\nbalancers (approx. $25/month per ingress), it has become unsustainable. We\\nusually have 6 replicas of the ingress controller pod, and we have started to\\nsee instances of several of these pods crash-looping (usually because they have\\nrun out of shared memory, which cannot be increased in kubernetes. See [this\\nissue] for more information).\\nWe believe this is because the nginx config has become so large (over 100K\\nlines), that sometimes pods fail to reload it when it is changed, or the pod is\\nmoved.\\n\n## Decision\nWe will create a separate AWS load balancer and ingress-controller for every\\nnamespace in the cluster. An \"ingress class\" annotation will cause traffic for\\na particular ingress to be routed through the appropriate AWS load balancer and\\ningress-controller. See our [module repository] for more details.\\n\"System\" ingresses (e.g. those used for concourse, grafana, etc.) will continue\\nto use the default ingress-controller. There should only ever be a handful of\\nthese, compared with hundreds of team ingresses, so the load on the default\\ningress-controller should stay within acceptable limits.\\n\n\n## Context and Problem Statement\\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\\nand was only noticed some days after when the Project Manager needed to\\nexport the data. Upon investigation, it was discovered that:\\n* Error logging was inactive on the application. (Now fixed)\\n* We can\u2019t debug using the responses our system returned to PayPal as\\nwe don\u2019t have access to the IPN log.\\nThis led to a situation where we couldn't get the information required\\nto debug the error. It was suggested we queue all incoming requests from\\nPayPal on our own system for processing by our system.\\n## Decision Drivers\\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\n\n## Decision\n", "response": "<pad> We will use a Security config file to store the config for eachnnamespace in the cluster.nThe Security config file will be stored in a separate Security folder.n", "matched_ids": [1799, 620], "time": "0:00:27.807799", "post": "## Decision\nWe will use a Security config file to store the config for eachnnamespace in the cluster \n The Security config file will be stored in a separate Security folder \n "}
{"context": "## Context\\nWe need to deploy the Broker somewhere.\\nThe Broker implements the service broker API to generate per-application syslog\\ndrain URLs (Adapter URLs).\\nThe Adapter is written in Go.\\nThe Broker is written in Go.\\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\\n", "input": "## Context and Problem Statement\\nWe want to have a Source-to-Image workflow to import services based on a GitHub repository. It should run inside our Kubernetes cluster, however currently Kubernetes doesn't have a resource build-in that is able to build container images. Therefore another technology is required.\\n## Decision Drivers\\n* MUST run on our Kubernetes cluster\\n* MUST run completely in userspace (no root access required)\\n* MUST be sufficient to provide a single URL to a GitHub repository (with included Dockerfile)\\n* SHOULD be independent of any cloud service provider\\n\n## Decision\n* MUST run on our Kubernetes cluster\\n* MUST run completely in userspace (no root access required)\\n* MUST be sufficient to provide a single URL to a GitHub repository (with included Dockerfile)\\n* SHOULD be independent of any cloud service provider\\nChosen option: *Knative Build*, because it meets all of our criterion decision drivers. It allows us to implement a Source-to-Image workflow on our Kubernetes cluster independently to any cloud service provider.\\n### Positive Consequences\\n* By using *Knative Build* we have the choice to use different kinds of `Builders`, follow-up decision is required: [Building OCI images](./0015-building-oci-images.md)\\n### Negative consequences\\n* *Nothing known*\\n\n\n## Context\\nThe application needs to be deployed somewhere.\\nThere are many container orchestration platforms for configuring a cluster of machines running Docker containers and managing the deployment of images on to these containers. Setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines.\\nHeroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https://devcenter.heroku.com/articles/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.\\nHeroku has several pricing tiers for machines that the application will run on, including a free tier.\\nHeroku provides a free hosted PostgreSQL option. It will handle setting a \"DATABASE_URL\" environment variable, containing the information required to connect to this database. The free tier database is limited to 10,000 rows.\\nWe want the setup process to be as simple as possible.\\n\n## Decision\nWe will host the application on Heroku, using their Container Registry solution with a hosted PostgreSQL database.\\n\n\n## Context\\nWe need to deploy the Broker somewhere.\\nThe Broker implements the service broker API to generate per-application syslog\\ndrain URLs (Adapter URLs).\\nThe Adapter is written in Go.\\nThe Broker is written in Go.\\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\\n\n## Decision\n", "response": "<pad> * MUST run on our Kubernetes clustern* MUST run completely in userspace (no root access required)n* MUST be sufficient to provide a single URL to a GitHub repository (with included Dockerfile)n* SHOULD be independent of any cloud service providernChosen option: *Knative Build*, because it meets all of our criterion decision drivers. It allows us to implement a Source-to-Image workflow on our Kubernetes cluster independently to any cloud service provider.n## Positive Consequencesn* By using *Knative Build* we have the choice to use different kinds of Builders, follow-up decision is required: [Building OCI images](./0015-building-oci-images.md)n### Negative consequencesn* *Nothing known*n", "matched_ids": [4683, 718], "time": "0:00:51.298319", "post": "## Decision\n* MUST run on our Kubernetes clustern* MUST run completely in userspace (no root access required)n* MUST be sufficient to provide a single URL to a GitHub repository (with included Dockerfile)n* SHOULD be independent of any cloud service providernChosen option: *Knative Build*, because it meets all of our criterion decision drivers. It allows us to implement a Source-to-Image workflow on our Kubernetes cluster independently to any cloud service provider \n ## Positive Consequencesn* By using *Knative Build* we have the choice to use different kinds of Builders, follow-up decision is required: [Building OCI images](./0015-building-oci-images.md)n### Negative consequencesn* *Nothing known*n"}
