{"context": "## Context and Problem Statement\\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\\n## Decision Drivers <!-- optional -->\\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\\n## Decision Drivers <!-- optional -->\\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\n<end_of_turn>\n<start_of_turn>model\n", "response": "* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\nChosen option: 2, because it is not supposed to become the bottleneck, SQL locking is required for pessimistic\\nlocking which can be quite expensive (and we want it performed only for small duration, so, not many rows)\\nand may be a source of hidden complexity to manage.\\n\nmodel\n* Performance - decision should support high throughput scenarios", "time": "0:00:03.940514", "post": "## Decision\n* Performance - decision should support high throughput scenarios \n * Maintainability - amount of code needed to write should be minimized \n Chosen option: 2, because it is not supposed to become the bottleneck, SQL locking is required for pessimistic \n locking which can be quite expensive (and we want it performed only for small duration, so, not many rows) \n and may be a source of hidden complexity to manage. \n \nmodel\n* Performance - decision should support high throughput scenarios"}
{"context": "## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https://www.postgresql.org/docs/9.5/static/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https://www.postgresql.org/docs/9.5/static/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https://www.postgresql.org/docs/9.5/static/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https://www.postgresql.org/docs/9.5/static/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We have decided to propose to the PostgreSQL community [enhancement proposal #762](https://www.postgresql.org/docs/9.5/static/sql-alterdefaultprivileges.html) which would grant the new default privileges of `SELECT`, `INSERT`, `UPDATE`, `DELETE`, and `TRUNCATE` on all tables (not assets like sequences or views) at all parents adding up to the directory hierarchy from the current schema. Landings exist for releasing 9", "time": "0:00:03.923257", "post": "## Decision\nWe have decided to propose to the PostgreSQL community [enhancement proposal #762](https://www.postgresql.org/docs/9.5/static/sql-alterdefaultprivileges.html) which would grant the new default privileges of `SELECT`, `INSERT`, `UPDATE`, `DELETE`, and `TRUNCATE` on all tables (not assets like sequences or views) at all parents adding up to the directory hierarchy from the current schema. Landings exist for releasing 9"}
{"context": "## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Although a surprising variety of PostgreSQL containers exist, the decision was made to adopt the official image provided by PostgreSQL itself: \\n```\\nFROM postgres:12.3-alpine\\n```\\nMultiple choices motivate the shift.\\nWhile alternative suggestions (like the one from moby) were made, some important pieces of functionality seem to be missing: e.g., the CLI tools available from the official image. Further, there are several pull requests open, that might not be addressed", "time": "0:00:03.958679", "post": "## Decision\nAlthough a surprising variety of PostgreSQL containers exist, the decision was made to adopt the official image provided by PostgreSQL itself:  \n ``` \n FROM postgres:12.3-alpine \n ``` \n Multiple choices motivate the shift. \n While alternative suggestions (like the one from moby) were made, some important pieces of functionality seem to be missing: e.g., the CLI tools available from the official image. Further, there are several pull requests open, that might not be addressed"}
{"context": "## Context\\nWe need a system to manage the lifecycle of a grant application.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe need a system to manage the lifecycle of a grant application.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Guidewire was chosen as the system for managing the lifecycle of GCRF grant application\\nGuidewire was selected as it is tool proven to have been used in similar project\\n\nmodel\nWe will use the [GCRF Support Service grant application management system](https://support.lancs.ac.uk/kb/home/expenses-other/grant-applications)\\n\nmodel\nWe will use the [GCRF Support Service grant application management system](https://", "time": "0:00:03.853746", "post": "## Decision\nGuidewire was chosen as the system for managing the lifecycle of GCRF grant application \n Guidewire was selected as it is tool proven to have been used in similar project \n \nmodel\nWe will use the [GCRF Support Service grant application management system](https://support.lancs.ac.uk/kb/home/expenses-other/grant-applications) \n \nmodel\nWe will use the [GCRF Support Service grant application management system](https://"}
{"context": "## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n<end_of_turn>\n<start_of_turn>model\n", "response": "- Use `ProtoProofOp` for merkle proof in `abci.ResponseQuery`\\n- Sequence proof with any `ProofOperator` that support `Apply` method\\n- Provided the validation method `ValidateProof` in protobuf\\n- Developers can implement their own Merkle proof in this interface\\n\nmodel\n- Use `ProtoProofOp` for merkle proof in `abci.ResponseQuery`\\n- Sequence proof with any `ProofOperator`", "time": "0:00:03.875314", "post": "## Decision\n- Use `ProtoProofOp` for merkle proof in `abci.ResponseQuery` \n - Sequence proof with any `ProofOperator` that support `Apply` method \n - Provided the validation method `ValidateProof` in protobuf \n - Developers can implement their own Merkle proof in this interface \n \nmodel\n- Use `ProtoProofOp` for merkle proof in `abci.ResponseQuery` \n - Sequence proof with any `ProofOperator`"}
{"context": "## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n", "input": "<bos><start_of_turn>user\n## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "SEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n\nmodel\nAzure Active Directory. This decision was agreed as it is directly integrated with the backend services both Dynamics CRM & Cloud Services offering a minimal disruption to CCMS API adoption with an opportunity to deprecate the existing identity provider and flag mechanism in the future offering a single place for oAuth items.\\n\nmodel\nAzure Active Directory.", "time": "0:00:03.870459", "post": "## Decision\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability. \n \nmodel\nAzure Active Directory. This decision was agreed as it is directly integrated with the backend services both Dynamics CRM & Cloud Services offering a minimal disruption to CCMS API adoption with an opportunity to deprecate the existing identity provider and flag mechanism in the future offering a single place for oAuth items. \n \nmodel\nAzure Active Directory."}
{"context": "## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n", "input": "<bos><start_of_turn>user\n## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Pick a monolith over a microservices architecture, because it will be easier\\nto find specific parts of the code.\\n\nmodel\nPick a monolith over a microservices architecture, because it will be easier\\nto find specific parts of the code.\\n\nmodel\nPick a monolith over a microservices architecture, because it will be easier\\nto find specific parts of the code.\\n\nmodel\nPick a monolith over a microservices architecture,", "time": "0:00:03.833827", "post": "## Decision\nPick a monolith over a microservices architecture, because it will be easier \n to find specific parts of the code. \n \nmodel\nPick a monolith over a microservices architecture, because it will be easier \n to find specific parts of the code. \n \nmodel\nPick a monolith over a microservices architecture, because it will be easier \n to find specific parts of the code. \n \nmodel\nPick a monolith over a microservices architecture,"}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe need to record the architectural decisions made on this project.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n", "time": "0:00:02.432814", "post": "## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions). \n "}
{"context": "## Context\\nThe rig defined at [Bookit Infrastructure](https://github.com/buildit/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\\nThe only way to capture that is via branches which can be hard to discover.\\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\\n", "input": "<bos><start_of_turn>user\n## Context\\nThe rig defined at [Bookit Infrastructure](https://github.com/buildit/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\\nThe only way to capture that is via branches which can be hard to discover.\\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will create paper specifications for rigs.  Our first will be the AWS Bare Metal Rig. An instantiation of the will be delivered via a separate repo matching the name of the AWS Bare Metal Rig as 1.0.0, so acf-aws-bare-metal-rig-1.0.0 what ever features it specifies or is implemented with.  Each subsequent replenishment will be a different git tag.  The rig repo will point to the production instance, i.e", "time": "0:00:03.907148", "post": "## Decision\nWe will create paper specifications for rigs.  Our first will be the AWS Bare Metal Rig. An instantiation of the will be delivered via a separate repo matching the name of the AWS Bare Metal Rig as 1.0.0, so acf-aws-bare-metal-rig-1.0.0 what ever features it specifies or is implemented with.  Each subsequent replenishment will be a different git tag.  The rig repo will point to the production instance, i.e"}
{"context": "## Context\\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\\nPHP [appears to be on a downwards trend as a language](https://pypl.github.io/PYPL.html?country=GB),\\nespecially in contrast with Python. It's likely it will become increasingly difficult\\nto find good PHP developers in future.\\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\\nlanguages of similar pedigree, such as Python.\\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\\nA mix of contractors and developers working on the code base over several years has\\nresulted in a mix of styles and approaches. While we have already cleared out a lot\\nof unused and/or broken code, there is likely to be more we haven't found yet.\\nWe are also lagging behind the latest Design System guidelines, as our application was one\\nof the first to go live, before the current iteration of the Design System existed.\\nThis means that any changes to design have to be done piecemeal and manually: we can't\\nsimply import the newest version of the design system and have everything magically update.\\nThis combination of factors means that the code base can be difficult to work with:\\nresistant to change and easy to break.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\\nPHP [appears to be on a downwards trend as a language](https://pypl.github.io/PYPL.html?country=GB),\\nespecially in contrast with Python. It's likely it will become increasingly difficult\\nto find good PHP developers in future.\\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\\nlanguages of similar pedigree, such as Python.\\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\\nA mix of contractors and developers working on the code base over several years has\\nresulted in a mix of styles and approaches. While we have already cleared out a lot\\nof unused and/or broken code, there is likely to be more we haven't found yet.\\nWe are also lagging behind the latest Design System guidelines, as our application was one\\nof the first to go live, before the current iteration of the Design System existed.\\nThis means that any changes to design have to be done piecemeal and manually: we can't\\nsimply import the newest version of the design system and have everything magically update.\\nThis combination of factors means that the code base can be difficult to work with:\\nresistant to change and easy to break.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We want to [build Release Management DB](https://docs.google.com/document/d/1vYx0yDAbFz3NZ2oOg05aPMHbNqOthq3AlGUtf9FBE9c/edit?usp=sharing) from\\nthe ground up. We should take the opportunity to create a new application, rather than try to\\ntranform the existing one.\\nThis would allow us to:\\n*", "time": "0:00:03.956343", "post": "## Decision\nWe want to [build Release Management DB](https://docs.google.com/document/d/1vYx0yDAbFz3NZ2oOg05aPMHbNqOthq3AlGUtf9FBE9c/edit?usp=sharing) from \n the ground up. We should take the opportunity to create a new application, rather than try to \n tranform the existing one. \n This would allow us to: \n *"}
{"context": "## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n", "input": "<bos><start_of_turn>user\n## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Use touch events only knowledge to detect touch events and create touch listeners that support multiple touch points. Avoid previous coordinate tracking and use geometry calculations on touch information to determine direction and magnitude of movement between touches.\\n\nmodel\nInstead of relying on a non-standard approach provided by DevTools protocol, our initial design will be synchronous, always tightly constrained to the currently selected CSS element.\\nSince asynchronous evaluations will be implemented inside of Hanami MAUI application code that may be optimized across multiple runs", "time": "0:00:03.842766", "post": "## Decision\nUse touch events only knowledge to detect touch events and create touch listeners that support multiple touch points. Avoid previous coordinate tracking and use geometry calculations on touch information to determine direction and magnitude of movement between touches. \n \nmodel\nInstead of relying on a non-standard approach provided by DevTools protocol, our initial design will be synchronous, always tightly constrained to the currently selected CSS element. \n Since asynchronous evaluations will be implemented inside of Hanami MAUI application code that may be optimized across multiple runs"}
{"context": "## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n", "input": "<bos><start_of_turn>user\n## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "As the version of davrd and dapr-cli will diverge quickly as we increase the frequency of minor releases, we anticipate that this will cause tension between the CLI and the runtime. To reduce the risk of subclassing the runtime when users try to install/update a newer version of components, we decided to uncouple the CLI version from runtime version.\\nIf a user executes dapr upgrade --upgradeAllComponents, We will only allow update the components to the latest one suppor \u578b runtime of version", "time": "0:00:03.980355", "post": "## Decision\nAs the version of davrd and dapr-cli will diverge quickly as we increase the frequency of minor releases, we anticipate that this will cause tension between the CLI and the runtime. To reduce the risk of subclassing the runtime when users try to install/update a newer version of components, we decided to uncouple the CLI version from runtime version. \n If a user executes dapr upgrade --upgradeAllComponents, We will only allow update the components to the latest one suppor \u578b runtime of version"}
{"context": "## Problem\\nMost of the credentials/information of the environment (like a service address) are only available as soon the application is deployed.\\n", "input": "<bos><start_of_turn>user\n## Problem\\nMost of the credentials/information of the environment (like a service address) are only available as soon the application is deployed.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "The approach taken here is the 2nd one, as these credentials are not stored in some directory that could be leaked or accessed without permission.\\n\nmodel\n\"Instead putting all the information in the build, use external environments that will hold not sensitive service bindings (like external file systems) or GCP projects with service-accounts and binder among others\".\\nNot all the environment variables of hdx-serving should be stored, only the ones that cannot be configured through the configuration files", "time": "0:00:03.883659", "post": "## Decision\nThe approach taken here is the 2nd one, as these credentials are not stored in some directory that could be leaked or accessed without permission. \n \nmodel\n\"Instead putting all the information in the build, use external environments that will hold not sensitive service bindings (like external file systems) or GCP projects with service-accounts and binder among others\". \n Not all the environment variables of hdx-serving should be stored, only the ones that cannot be configured through the configuration files"}
{"context": "## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno / minimal costs.\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno / minimal costs.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno / minimal costs.\\nWe had some play time with Auth0 on the Common Room project, and as such\\nwe felt it would be quicker to use that over building our own from\\nscraps, and just pay the monthly cost of the solution. The only core\\nfeature this project requires which", "time": "0:00:03.936819", "post": "## Decision\n* Ease of initial deployment and configuration of the solution. \n * Ongoing hosting and maintenance costs of the solution. \n * Availability of core features to satisfy the project requirements with \n no / minimal costs. \n We had some play time with Auth0 on the Common Room project, and as such \n we felt it would be quicker to use that over building our own from \n scraps, and just pay the monthly cost of the solution. The only core \n feature this project requires which"}
{"context": "## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n", "input": "<bos><start_of_turn>user\n## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Bookit will use AWS hosted Aurora MySql RDBMS.\\n\nmodel\nBookit will use AWS hosted Aurora MySql RDBMS.\\n\nmodel\nBookit will use AWS hosted Aurora MySql RDBMS.\\n\nmodel\nBookit will use AWS hosted Aurora MySql RDBMS.\\n\nmodel\nBookit will use AWS hosted Aurora MySql RDBMS.\\n\nmodel\nBookit will use AWS hosted Aurora MySql RDBMS.\\n\n", "time": "0:00:03.909902", "post": "## Decision\nBookit will use AWS hosted Aurora MySql RDBMS. \n \nmodel\nBookit will use AWS hosted Aurora MySql RDBMS. \n \nmodel\nBookit will use AWS hosted Aurora MySql RDBMS. \n \nmodel\nBookit will use AWS hosted Aurora MySql RDBMS. \n \nmodel\nBookit will use AWS hosted Aurora MySql RDBMS. \n \nmodel\nBookit will use AWS hosted Aurora MySql RDBMS. \n "}
{"context": "## Context\\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\\n", "input": "<bos><start_of_turn>user\n## Context\\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Zero Knowledge cryptography is to be used to ensure correctness of off-chain data is always able to be proven on-chain.\\n\nmodel\nVote to remove competition mode. Approved.\\nLagging node synchronization is a problem that targets smaller node operators, thus it defeats the principals of decentralisation.</br>\\nOnce the proof of record updates finish improvement to poor node synchronization performance aim to be provided.\\n\nmodel\nIDS deployment in a weekly mode was found to be acceptable", "time": "0:00:04.049484", "post": "## Decision\nZero Knowledge cryptography is to be used to ensure correctness of off-chain data is always able to be proven on-chain. \n \nmodel\nVote to remove competition mode. Approved. \n Lagging node synchronization is a problem that targets smaller node operators, thus it defeats the principals of decentralisation.</br> \n Once the proof of record updates finish improvement to poor node synchronization performance aim to be provided. \n \nmodel\nIDS deployment in a weekly mode was found to be acceptable"}
{"context": "## Problem\\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\\n(as they are designed to do so). Limitations of other storage plugins are\\ne.g., that not every structure of configuration is allowed.\\nSome of these limitations were documented `infos/status`, others were not.\\n", "input": "<bos><start_of_turn>user\n## Problem\\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\\n(as they are designed to do so). Limitations of other storage plugins are\\ne.g., that not every structure of configuration is allowed.\\nSome of these limitations were documented `infos/status`, others were not.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We replace strictness of invariant checking by configuration\\nmechanical checks of signals (in an intermediate configuration).\\nWe don't maintain a list of storage types for every signal structure.\\n\nmodel\nWe replace strictness of invariant checking by configuration\\nmechanical checks of signals (in an intermediate configuration).\\nWe don't maintain a list of storage types for every signal structure.\\n\nmodel\nWe replace strictness of invariant checking by configuration\\nmechanical checks of", "time": "0:00:04.169304", "post": "## Decision\nWe replace strictness of invariant checking by configuration \n mechanical checks of signals (in an intermediate configuration). \n We don't maintain a list of storage types for every signal structure. \n \nmodel\nWe replace strictness of invariant checking by configuration \n mechanical checks of signals (in an intermediate configuration). \n We don't maintain a list of storage types for every signal structure. \n \nmodel\nWe replace strictness of invariant checking by configuration \n mechanical checks of"}
{"context": "## Context\\n[Command-query separation](https://martinfowler.com/bliki/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n", "input": "<bos><start_of_turn>user\n## Context\\n[Command-query separation](https://martinfowler.com/bliki/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Such separation will benefit the whole application, not only in disease schemas but everywhere.\\n### Changes to queries\\nThis principle, in order to be followed, required some major changes to the queries in the `menu-generation` application.\\nThe main goal is to have methods that always return data. That means that every side-effect should be avoided, and\\nin order to notify developers about that we changed the naming of the queries from `command` to `query`. Also, in order", "time": "0:00:03.865287", "post": "## Decision\nSuch separation will benefit the whole application, not only in disease schemas but everywhere. \n ### Changes to queries \n This principle, in order to be followed, required some major changes to the queries in the `menu-generation` application. \n The main goal is to have methods that always return data. That means that every side-effect should be avoided, and \n in order to notify developers about that we changed the naming of the queries from `command` to `query`. Also, in order"}
{"context": "## Context\\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We use and accept the time defined by the client of the api, but display date and time information in UTC with a user-friendly representation.\\n\nmodel\nWe use and accept the time defined by the client of the api, but display date and time information in UTC with a user-friendly representation.\\n\nmodel\nWe use and accept the time defined by the client of the api, but display date and time information in UTC with a user-friendly representation.\\n", "time": "0:00:03.989308", "post": "## Decision\nWe use and accept the time defined by the client of the api, but display date and time information in UTC with a user-friendly representation. \n \nmodel\nWe use and accept the time defined by the client of the api, but display date and time information in UTC with a user-friendly representation. \n \nmodel\nWe use and accept the time defined by the client of the api, but display date and time information in UTC with a user-friendly representation. \n "}
{"context": "## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n", "input": "<bos><start_of_turn>user\n## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "It is decided to develop the software as a collection of Gradle modules making up the Code Generation Software project.\\n\nmodel\nIt is decided to develop the software as a collection of Gradle modules making up the Code Generation Software project.\\n\nmodel\nIt is decided to develop the software as a collection of Gradle modules making up the Code Generation Software project.\\n\nmodel\nIt is decided to develop the software as a collection of Gradle modules making up", "time": "0:00:03.852307", "post": "## Decision\nIt is decided to develop the software as a collection of Gradle modules making up the Code Generation Software project. \n \nmodel\nIt is decided to develop the software as a collection of Gradle modules making up the Code Generation Software project. \n \nmodel\nIt is decided to develop the software as a collection of Gradle modules making up the Code Generation Software project. \n \nmodel\nIt is decided to develop the software as a collection of Gradle modules making up"}
{"context": "## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n", "input": "<bos><start_of_turn>user\n## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "The relationship modeled in data-model is `RelationshipStart`, a UMLS code, the start_date, and the end_date.\\nNothing is linking this entity to subject_of relationship. This is left for the implementation where we would either create a \"thread\" with a `subject_of` linking to `RelationshipStart` entity in the specified dates for simplification, or we implement fully entity relationship modeling for proper relationships.\\nFor delivery, the `subject_of` will be Pren", "time": "0:00:03.877335", "post": "## Decision\nThe relationship modeled in data-model is `RelationshipStart`, a UMLS code, the start_date, and the end_date. \n Nothing is linking this entity to subject_of relationship. This is left for the implementation where we would either create a \"thread\" with a `subject_of` linking to `RelationshipStart` entity in the specified dates for simplification, or we implement fully entity relationship modeling for proper relationships. \n For delivery, the `subject_of` will be Pren"}
{"context": "## Context\\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Use Crowd for Admin Users.  We will use the project's GitLab ID for a Crowd group\\n\nmodel\nUse Crowd for Admin Users.  We will use the project's GitLab ID for a Crowd group\\n\nmodel\nUse Crowd for Admin Users.  We will use the project's GitLab ID for a Crowd group\\n\nmodel\nUse Crowd for Admin Users.  We will use the project's GitLab", "time": "0:00:03.902715", "post": "## Decision\nUse Crowd for Admin Users.  We will use the project's GitLab ID for a Crowd group \n \nmodel\nUse Crowd for Admin Users.  We will use the project's GitLab ID for a Crowd group \n \nmodel\nUse Crowd for Admin Users.  We will use the project's GitLab ID for a Crowd group \n \nmodel\nUse Crowd for Admin Users.  We will use the project's GitLab"}
{"context": "## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n", "input": "<bos><start_of_turn>user\n## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "[Security](https://docs.google.com/document/u/3/d/1iEAewWFHt-JR9T6vPeLccsl_sTLXcr9hHSGx_MGxLdE/offline) and CI/CD considerations mean this code and pipeline is best left in the hands that already built it.\\n\nmodel\nThe decision is still to be made. Regardless of the conversion path, we are currently reviewing identities across the", "time": "0:00:03.908408", "post": "## Decision\n[Security](https://docs.google.com/document/u/3/d/1iEAewWFHt-JR9T6vPeLccsl_sTLXcr9hHSGx_MGxLdE/offline) and CI/CD considerations mean this code and pipeline is best left in the hands that already built it. \n \nmodel\nThe decision is still to be made. Regardless of the conversion path, we are currently reviewing identities across the"}
{"context": "## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n", "input": "<bos><start_of_turn>user\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "The change that we're proposing or have agreed to implement.\\n\nmodel\nWe will [change X to Y] because [Z]. We will do this [how/when].\\n### Consequences\\nWe expect the following consequences, both positive and negative:\\n* [positive consequence 1]\\n* [negative consequence 1]\\nIn the future, we may need to [future action 1, e.g., revisit this decision] if [condition ", "time": "0:00:03.864620", "post": "## Decision\nThe change that we're proposing or have agreed to implement. \n \nmodel\nWe will [change X to Y] because [Z]. We will do this [how/when]. \n ### Consequences \n We expect the following consequences, both positive and negative: \n * [positive consequence 1] \n * [negative consequence 1] \n In the future, we may need to [future action 1, e.g., revisit this decision] if [condition"}
{"context": "## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n", "input": "<bos><start_of_turn>user\n## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will run Kafkarator locally in each application that consumes Kafka.\\n\nmodel\nWe will run Kafkarator locally in each application that consumes Kafka.\\n\nmodel\nWe will run Kafkarator locally in each application that consumes Kafka.\\n\nmodel\nWe will run Kafkarator locally in each application that consumes Kafka.\\n\nmodel\nWe will run Kafkarator locally in each application that consumes Kafka.\\n\nmodel\nWe", "time": "0:00:03.893858", "post": "## Decision\nWe will run Kafkarator locally in each application that consumes Kafka. \n \nmodel\nWe will run Kafkarator locally in each application that consumes Kafka. \n \nmodel\nWe will run Kafkarator locally in each application that consumes Kafka. \n \nmodel\nWe will run Kafkarator locally in each application that consumes Kafka. \n \nmodel\nWe will run Kafkarator locally in each application that consumes Kafka. \n \nmodel\nWe"}
{"context": "## Context and Problem Statement\\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Chosen option: The Azure Front Door, because\\n- we own services are expected in our data center\\n- pstt.ecloud.newslabs edition is private by default (not better because Microsoft is our tenant and internal private is not activated),\\n- Azure is our cloud provider\\nAlso because we did not want to manage a new nginx server, nginx's authentication quota is far far too small, Azure Front door provided URL patterns seem fine and we did some tests whereas we", "time": "0:00:03.865227", "post": "## Decision\nChosen option: The Azure Front Door, because \n - we own services are expected in our data center \n - pstt.ecloud.newslabs edition is private by default (not better because Microsoft is our tenant and internal private is not activated), \n - Azure is our cloud provider \n Also because we did not want to manage a new nginx server, nginx's authentication quota is far far too small, Azure Front door provided URL patterns seem fine and we did some tests whereas we"}
{"context": "## Context\\nPIMS requires a database to store all property information.\\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\\nAdditionally it must be supported by Entity Framework Core 3.1.\\n", "input": "<bos><start_of_turn>user\n## Context\\nPIMS requires a database to store all property information.\\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\\nAdditionally it must be supported by Entity Framework Core 3.1.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "The decision is to use Microsoft SQL Server. Running as a docker container with a Linux image.\\n\nmodel\nThe decision is to use Microsoft SQL Server. Running as a docker container with a Linux image.\\n\nmodel\nMicrosoft SQL Server\\n\nmodel\nMicrosoft SQL Server. Running as a docker container with a Linux image.\\n\nmodel\nMicrosoft SQL Server. Running as a docker container with a Linux image.\\n\nmodel\n", "time": "0:00:03.861930", "post": "## Decision\nThe decision is to use Microsoft SQL Server. Running as a docker container with a Linux image. \n \nmodel\nThe decision is to use Microsoft SQL Server. Running as a docker container with a Linux image. \n \nmodel\nMicrosoft SQL Server \n \nmodel\nMicrosoft SQL Server. Running as a docker container with a Linux image. \n \nmodel\nMicrosoft SQL Server. Running as a docker container with a Linux image. \n \nmodel"}
{"context": "## Context and Problem Statement\\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\\ninterfaces\" - possibly leading to the impression that there is such a thing as\\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\\nthe only question is who manages this thread-safety. Interfaces which are not\\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\\nhidden in the generated code and therefore not obvious to the casual reader.\\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\\nmanaging their own locking internally. This ADR proposes that uniffi forces\\ncomponent authors to explicitly manage that locking in all cases - or to put\\nthis in Rust terms, that all structs supported by uniffi must already be\\n`Send+Sync`\\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\\nany misunderstandings that might be caused by using the somewhat broad and\\ngeneric \"Threadsafe\".\\n## Decision Drivers\\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https://github.com/mozilla-mobile/fenix/issues/17086)\\n(with more details available in [this JIRA ticket](https://jira.mozilla.com/browse/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https://github.com/mozilla/uniffi-rs/issues/417) or to\\n[pass interface references over the FFI](https://github.com/mozilla/uniffi-rs/issues/419).\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\\ninterfaces\" - possibly leading to the impression that there is such a thing as\\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\\nthe only question is who manages this thread-safety. Interfaces which are not\\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\\nhidden in the generated code and therefore not obvious to the casual reader.\\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\\nmanaging their own locking internally. This ADR proposes that uniffi forces\\ncomponent authors to explicitly manage that locking in all cases - or to put\\nthis in Rust terms, that all structs supported by uniffi must already be\\n`Send+Sync`\\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\\nany misunderstandings that might be caused by using the somewhat broad and\\ngeneric \"Threadsafe\".\\n## Decision Drivers\\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https://github.com/mozilla-mobile/fenix/issues/17086)\\n(with more details available in [this JIRA ticket](https://jira.mozilla.com/browse/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https://github.com/mozilla/uniffi-rs/issues/417) or to\\n[pass interface references over the FFI](https://github.com/mozilla/uniffi-rs/issues/419).\\n<end_of_turn>\n<start_of_turn>model\n", "response": "* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https://github.com/mozilla-mobile/fenix/issues/17086)\\n(with more details available in [this JIRA ticket](https://jira", "time": "0:00:03.961742", "post": "## Decision\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make \n them `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally \n having method calls unexpectedly block for long periods, such as \n [this Fenix bug](https://github.com/mozilla-mobile/fenix/issues/17086) \n (with more details available in [this JIRA ticket](https://jira"}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe need to record the architectural decisions made on this project.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\nmodel\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\nmodel\nWe will use Architecture", "time": "0:00:03.946167", "post": "## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions). \n \nmodel\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions). \n \nmodel\nWe will use Architecture"}
{"context": "## Context and Problem Statement\\nWe found that without a standardised format our javascript files ended up with different\\nformats in different files or even multiple formats in the same file.  We also found that\\nour IDEs had different configurations which meant that using an autoformat tool would give\\ndifferent results when each of us do it.\\n## Decision Drivers\\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\nWe found that without a standardised format our javascript files ended up with different\\nformats in different files or even multiple formats in the same file.  We also found that\\nour IDEs had different configurations which meant that using an autoformat tool would give\\ndifferent results when each of us do it.\\n## Decision Drivers\\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\n<end_of_turn>\n<start_of_turn>model\n", "response": "* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\nWe adopted Prettier and decided to become consistent for the entire project with the\\nbases in the project with the settings written in a `.prettierrc` and `.pretti", "time": "0:00:03.995101", "post": "## Decision\n* We wanted to spend less time doing manual formatting \n * We wanted to spend less time undoing autoformatting which had been applied to unchanged lines \n * We wanted to see easily which lines had actually changed when reviewing PRs \n * We wanted to avoid discussions about individual's preferences for particular \n We adopted Prettier and decided to become consistent for the entire project with the \n bases in the project with the settings written in a `.prettierrc` and `.pretti"}
{"context": "## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n", "input": "<bos><start_of_turn>user\n## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We use string constants for type names.\\n\nmodel\nWe use string constants for type names.\\n\nmodel\nWe use string constants for type names.\\nOperational semantics and object definition uses constant string values with distinct case distinctions.\\n\nmodel\nWe use string constants for type names.\\nObject definition uses constant string values with distinct case distinction.\\n\nmodel\nWe use string constants for type names.\\nObject definition uses constant string values with distinct", "time": "0:00:03.881725", "post": "## Decision\nWe use string constants for type names. \n \nmodel\nWe use string constants for type names. \n \nmodel\nWe use string constants for type names. \n Operational semantics and object definition uses constant string values with distinct case distinctions. \n \nmodel\nWe use string constants for type names. \n Object definition uses constant string values with distinct case distinction. \n \nmodel\nWe use string constants for type names. \n Object definition uses constant string values with distinct"}
{"context": "## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n", "input": "<bos><start_of_turn>user\n## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We propose to build Alfa in TypeScript. TypeScript sources will be transpiled to plain JavaScript files that the browser can understand. This provides us with type checking and static analysis which we believe will be a powerful weapon against many types of bugs.\\n\nmodel\nWe propose to build Alfa in TypeScript. TypeScript sources will be transpiled to plain JavaScript files that the browser can understand. This provides us with type checking and static analysis which we believe will be a powerful weapon against many types of", "time": "0:00:04.006093", "post": "## Decision\nWe propose to build Alfa in TypeScript. TypeScript sources will be transpiled to plain JavaScript files that the browser can understand. This provides us with type checking and static analysis which we believe will be a powerful weapon against many types of bugs. \n \nmodel\nWe propose to build Alfa in TypeScript. TypeScript sources will be transpiled to plain JavaScript files that the browser can understand. This provides us with type checking and static analysis which we believe will be a powerful weapon against many types of"}
{"context": "## Context and Problem Statement\\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\\nWhich query syntax should be used for the abstract queries?\\nWhich features should the syntax support?\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\\nWhich query syntax should be used for the abstract queries?\\nWhich features should the syntax support?\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Chosen option: \"[Lucene JSON-like] (Option #1)\", because the most users will be happy, the costs of implementation / maintenance are acceptable and it the Parse Tree can be used to add new features at any time and the additional production for teaching \"LSH Search Builder\".\\n### Positive Consequences <!-- optional -->\\n* the query syntax is well known for advanced users\\n* the query syntax can be visually represented\\n* search builders can be generated easily\\n###", "time": "0:00:03.850419", "post": "## Decision\nChosen option: \"[Lucene JSON-like] (Option #1)\", because the most users will be happy, the costs of implementation / maintenance are acceptable and it the Parse Tree can be used to add new features at any time and the additional production for teaching \"LSH Search Builder\". \n ### Positive Consequences <!-- optional --> \n * the query syntax is well known for advanced users \n * the query syntax can be visually represented \n * search builders can be generated easily \n ###"}
{"context": "## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\\nGovUK services have a \"trial mode\", and a \"live mode\".\\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\\n", "input": "<bos><start_of_turn>user\n## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\\nGovUK services have a \"trial mode\", and a \"live mode\".\\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "GovUK Notify instances have not been set up in trial mode.\\n\nmodel\nGovUK Notify instances have not been set up in trial mode.\\n\nmodel\nGovUK Notify instances have not been set up in trial mode.\\n\nmodel\nGovUK Notify instances have not been set up in trial mode.\\n\nmodel\nGovUK Notify instances have not been set up in trial mode.\\n\nmodel\nGovUK Notify instances have", "time": "0:00:03.839985", "post": "## Decision\nGovUK Notify instances have not been set up in trial mode. \n \nmodel\nGovUK Notify instances have not been set up in trial mode. \n \nmodel\nGovUK Notify instances have not been set up in trial mode. \n \nmodel\nGovUK Notify instances have not been set up in trial mode. \n \nmodel\nGovUK Notify instances have not been set up in trial mode. \n \nmodel\nGovUK Notify instances have"}
{"context": "## Context\\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\\n", "input": "<bos><start_of_turn>user\n## Context\\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will use Microsoft Application Insights to achieve this as is in-line with the MOJ's telemetry strategy.\\n\nmodel\nWe will use Microsoft Application Insights to achieve this as is in-line with the MOJ's telemetry strategy.\\n\nmodel\nWe will use Microsoft Application Insights to achieve this as is in-line with the MOJ's telemetry strategy.\\n</details>\\n\nmodel\nWe will use Microsoft Application Insights to achieve this", "time": "0:00:03.831808", "post": "## Decision\nWe will use Microsoft Application Insights to achieve this as is in-line with the MOJ's telemetry strategy. \n \nmodel\nWe will use Microsoft Application Insights to achieve this as is in-line with the MOJ's telemetry strategy. \n \nmodel\nWe will use Microsoft Application Insights to achieve this as is in-line with the MOJ's telemetry strategy. \n </details> \n \nmodel\nWe will use Microsoft Application Insights to achieve this"}
{"context": "## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https://stylelint.io/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https://stylelint.io/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will use Stylelint to lint our CSS code and will run it as part of our test\\nsuite.\\n\nmodel\nWe will use Stylelint to lint our CSS code and will run it as part of our test\\nsuite.\\n\nmodel\nWe will use Stylelint to lint our CSS code and will run it as part of our test\\nsuite.\\n\nmodel\nWe will use Stylelint to lint our CSS code and will run", "time": "0:00:03.914142", "post": "## Decision\nWe will use Stylelint to lint our CSS code and will run it as part of our test \n suite. \n \nmodel\nWe will use Stylelint to lint our CSS code and will run it as part of our test \n suite. \n \nmodel\nWe will use Stylelint to lint our CSS code and will run it as part of our test \n suite. \n \nmodel\nWe will use Stylelint to lint our CSS code and will run"}
{"context": "## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n", "input": "<bos><start_of_turn>user\n## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Due to their radically different purposes, blacklights-authorities is of no help to us in sorting out who can do what to\\nwhich work.\\nTrying to combine permission results returns from different sources is out of scope of [origin 2021:\\nSupporting customized inheritance with a Permission Service](https://docs.google.com/document/d/1n6lpsBLy8ProQvGxjJ638zU5eaUlvrwC8", "time": "0:00:03.886277", "post": "## Decision\nDue to their radically different purposes, blacklights-authorities is of no help to us in sorting out who can do what to \n which work. \n Trying to combine permission results returns from different sources is out of scope of [origin 2021: \n Supporting customized inheritance with a Permission Service](https://docs.google.com/document/d/1n6lpsBLy8ProQvGxjJ638zU5eaUlvrwC8"}
{"context": "## Context\\nThe Data Migrate gem has caused us a number of issues in the past, it runs\\nsilently as part of a deploy, and this can result in surprising errors\\nduring a deploy. We've also had issues with the gem itself - most recently\\na bug in a new version causing strange errors in deployment.\\n", "input": "<bos><start_of_turn>user\n## Context\\nThe Data Migrate gem has caused us a number of issues in the past, it runs\\nsilently as part of a deploy, and this can result in surprising errors\\nduring a deploy. We've also had issues with the gem itself - most recently\\na bug in a new version causing strange errors in deployment.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will use a task runner (karma) to ensure that our rake tasks can always be run from the command line,\\nfrom anywhere.\\n\nmodel\nWe will use a task runner (karma) to ensure that our rake tasks can always be run from the command line,\\nfrom anywhere.\\n\nmodel\nWe will use a infrastructure testing tool (policy_test_infrastructure) to run against a test policy that\\n(placeholder for the test_policy_name", "time": "0:00:05.387604", "post": "## Decision\nWe will use a task runner (karma) to ensure that our rake tasks can always be run from the command line, \n from anywhere. \n \nmodel\nWe will use a task runner (karma) to ensure that our rake tasks can always be run from the command line, \n from anywhere. \n \nmodel\nWe will use a infrastructure testing tool (policy_test_infrastructure) to run against a test policy that \n (placeholder for the test_policy_name"}
{"context": "## Context\\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Extend `DownloadManager_Impl.download()` this method which target `DownloadManagerDelegate`, which delegate task down to `DownloadManagerTask_Factory` by using a `Registry` as interface to be ignored by the manager itself.\\nUsing `Task_Factory` has its own life cycle and criteria for executing tasks and tasks is only created when they are executed, will help decoupling reliance on system services specially `DownloadManager`.\\n\nmodel\nExtend `DownloadManager_Impl.", "time": "0:00:03.866948", "post": "## Decision\nExtend `DownloadManager_Impl.download()` this method which target `DownloadManagerDelegate`, which delegate task down to `DownloadManagerTask_Factory` by using a `Registry` as interface to be ignored by the manager itself. \n Using `Task_Factory` has its own life cycle and criteria for executing tasks and tasks is only created when they are executed, will help decoupling reliance on system services specially `DownloadManager`. \n \nmodel\nExtend `DownloadManager_Impl."}
{"context": "## Context\\nWe are doing a rewrite.\\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe are doing a rewrite.\\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "The decision is to use hexagonal, to taste.\\n\nmodel\nThe sedicion is to use hexagnal, to taste.\\n\nmodel\nA single global state, with immutable parts.\\nIn the initial review and in this ADR some clarification was done on \"global state\":\\n* It is a **single** state object, not a mechanism that lets all actors access existing objects in isolation.\\n* This way it's as minmal as possible,", "time": "0:00:03.896629", "post": "## Decision\nThe decision is to use hexagonal, to taste. \n \nmodel\nThe sedicion is to use hexagnal, to taste. \n \nmodel\nA single global state, with immutable parts. \n In the initial review and in this ADR some clarification was done on \"global state\": \n * It is a **single** state object, not a mechanism that lets all actors access existing objects in isolation. \n * This way it's as minmal as possible,"}
{"context": "## Problem\\nCurrently multiple warnings are saved in an elektra non-conforming array\\nnotation which is limited to 100 entries. The notation of `#00` is against\\nthe design [decision made](array.md).\\n", "input": "<bos><start_of_turn>user\n## Problem\\nCurrently multiple warnings are saved in an elektra non-conforming array\\nnotation which is limited to 100 entries. The notation of `#00` is against\\nthe design [decision made](array.md).\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Restructure complete warning store into a map with all warnings `#nn`\\ndefined there.\\nAll warnings are collected as soon as the kdb is read.\\nSave information associated to warning messages in `elektraKey*` and\\ndeletter it while printing.\\nIn case a saved warning is outside current valid key range, it is\\nconsidered invalid and deleted.\\nThe removed warning is printed.\\n consternomically the formatting might be a disaster, without\\n", "time": "0:00:03.880843", "post": "## Decision\nRestructure complete warning store into a map with all warnings `#nn` \n defined there. \n All warnings are collected as soon as the kdb is read. \n Save information associated to warning messages in `elektraKey*` and \n deletter it while printing. \n In case a saved warning is outside current valid key range, it is \n considered invalid and deleted. \n The removed warning is printed. \n  consternomically the formatting might be a disaster, without \n "}
{"context": "## Context\\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\\nThere is currently no formal recommendation for a tool of choice in the technical guidance https://github.com/DFE-Digital/technical-guidance.\\nWe want a tool that provides an open and modern security standard.\\n", "input": "<bos><start_of_turn>user\n## Context\\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\\nThere is currently no formal recommendation for a tool of choice in the technical guidance https://github.com/DFE-Digital/technical-guidance.\\nWe want a tool that provides an open and modern security standard.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We have decided to use the Passwordless service as a supported service. This service manages the lifecycle of the signing user name and manages the Signs-in to the account.\\n\nmodel\n* Use DSI\\n\nmodel\n* Use DSI\\n\nmodel\n* Use DSI\\n\nmodel\nWe have decided to use the Passwordless service as a supported service. This service manages the lifecycle of the signing user name and manages the", "time": "0:00:03.859765", "post": "## Decision\nWe have decided to use the Passwordless service as a supported service. This service manages the lifecycle of the signing user name and manages the Signs-in to the account. \n \nmodel\n* Use DSI \n \nmodel\n* Use DSI \n \nmodel\n* Use DSI \n \nmodel\nWe have decided to use the Passwordless service as a supported service. This service manages the lifecycle of the signing user name and manages the"}
{"context": "## Context and Problem Statement\\nHow to guarantee reproducibility of Jupyter Notebooks?\\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\\nNotebooks should be treated as component/service that use their own dependencies, therefore when storing notebooks,\\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\\n## Decision Drivers <!-- optional -->\\n* user prospective\\n* reproducibility\\n* traecability\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\nHow to guarantee reproducibility of Jupyter Notebooks?\\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\\nNotebooks should be treated as component/service that use their own dependencies, therefore when storing notebooks,\\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\\n## Decision Drivers <!-- optional -->\\n* user prospective\\n* reproducibility\\n* traecability\\n<end_of_turn>\n<start_of_turn>model\n", "response": "* user prospective\\n* reproducibility\\n* traecability\\n1. Any new notebook will be managed using the `pipenv` tool created by the amazing folks at `pipenv`\\n1. Each new notebook repository will have a `pipfile` and a `pip-lock` where capture dependencies and their exact versions,\\nin order to reproduce notebooks or their logic across environments.\\n1. We will not create images on every single push to git, but instead create image", "time": "0:00:03.860183", "post": "## Decision\n* user prospective \n * reproducibility \n * traecability \n 1. Any new notebook will be managed using the `pipenv` tool created by the amazing folks at `pipenv` \n 1. Each new notebook repository will have a `pipfile` and a `pip-lock` where capture dependencies and their exact versions, \n in order to reproduce notebooks or their logic across environments. \n 1. We will not create images on every single push to git, but instead create image"}
{"context": "## Context\\nWe need to record the architectural decisions that we make as we develop Ockam.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe need to record the architectural decisions that we make as we develop Ockam.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will use Architecture Decision Records, as described by Michael Nygard, in this article first published on codingthearchitecture.com: [http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions][0]\\n\nmodel\nWe will use Architecture Decision Records, as described by Michael Nygard, in this article first published on codingthearchitecture.com: [http://thinkrelevance.com/blog/20", "time": "0:00:03.895131", "post": "## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard, in this article first published on codingthearchitecture.com: [http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions][0] \n \nmodel\nWe will use Architecture Decision Records, as described by Michael Nygard, in this article first published on codingthearchitecture.com: [http://thinkrelevance.com/blog/20"}
{"context": "## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n", "input": "<bos><start_of_turn>user\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will... This section contains the decision and is formatted as a declarative sentence.\\n\nmodel\nChosen option: \"[option 2]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\", "time": "0:00:03.951531", "post": "## Decision\nWe will... This section contains the decision and is formatted as a declarative sentence. \n \nmodel\nChosen option: \"[option 2]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. \n ### Positive Consequences <!-- optional --> \n * [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] \n * \u2026\\"}
{"context": "## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https://apps.nrs.gov.bc.ca/int/jira/browse/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n", "input": "<bos><start_of_turn>user\n## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https://apps.nrs.gov.bc.ca/int/jira/browse/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its", "time": "0:00:03.943032", "post": "## Decision\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases. \n * Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning. \n * Oracle extended the public updates of its"}
{"context": "## **Context**\\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\\nWe will look at two options for validation:\\n- **Data Annotations**\\nThis involves \"annotating\" each class model with specific validation, such as\\n- `[Required(ErrorMessage = \"This field is required\")]`\\n- `[MaxLength(20)]`\\nThere are a number of issues with this approach:\\n- Validation is scattered throughout the codebase as attributes on data model classes\\n- Testing is not straightforward\\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\\n- Does not allow for conditional validation\\n- **Fluent Validation**\\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\\n- Lives outside of the data model classes\\n- Very easy to test validation in isolation\\n- Error messaging can be externalised using dependency injection\\n- Allows for chaining of validators and conditional validation\\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\\n", "input": "<bos><start_of_turn>user\n## **Context**\\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\\nWe will look at two options for validation:\\n- **Data Annotations**\\nThis involves \"annotating\" each class model with specific validation, such as\\n- `[Required(ErrorMessage = \"This field is required\")]`\\n- `[MaxLength(20)]`\\nThere are a number of issues with this approach:\\n- Validation is scattered throughout the codebase as attributes on data model classes\\n- Testing is not straightforward\\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\\n- Does not allow for conditional validation\\n- **Fluent Validation**\\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\\n- Lives outside of the data model classes\\n- Very easy to test validation in isolation\\n- Error messaging can be externalised using dependency injection\\n- Allows for chaining of validators and conditional validation\\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\\n<end_of_turn>\n<start_of_turn>model\n", "response": "- We've decided to use Fluent Validation.\\n- _Decision also applies to new projects_, see README.md.\\n\nmodel\n- We've decided to use Fluent Validation.\\n- _Decision also applies to new projects_, see README.md.\\n\nmodel\n- We've decided to use Fluent Validation.\\n- _Decision also applies to new projects_, see README.md.\\n\n", "time": "0:00:03.604914", "post": "## Decision\n- We've decided to use Fluent Validation. \n - _Decision also applies to new projects_, see README.md. \n \nmodel\n- We've decided to use Fluent Validation. \n - _Decision also applies to new projects_, see README.md. \n \nmodel\n- We've decided to use Fluent Validation. \n - _Decision also applies to new projects_, see README.md. \n "}
{"context": "## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n", "input": "<bos><start_of_turn>user\n## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will need a new `DistributedTaskManagerService` to store and manage dependencies between `Task`s.\\n\nmodel\nWe will need a new `DistributedTaskManagerService` to store and manage dependencies between `Task`s.\\n\nmodel\nWe will need a new `DistributedTaskManagerService` to store and manage dependencies between `Task`s.\\n\nmodel\nWe will need a new `DistributedTaskManagerService` to store and manage dependencies", "time": "0:00:03.831464", "post": "## Decision\nWe will need a new `DistributedTaskManagerService` to store and manage dependencies between `Task`s. \n \nmodel\nWe will need a new `DistributedTaskManagerService` to store and manage dependencies between `Task`s. \n \nmodel\nWe will need a new `DistributedTaskManagerService` to store and manage dependencies between `Task`s. \n \nmodel\nWe will need a new `DistributedTaskManagerService` to store and manage dependencies"}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe need to record the architectural decisions made on this project.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\nmodel\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\nmodel\nWe will use Architecture", "time": "0:00:03.872050", "post": "## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions). \n \nmodel\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions). \n \nmodel\nWe will use Architecture"}
{"context": "## Context\\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\\nsandboxed environment. A Docker image has been created to help with this but this still\\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\\nto run Bitcoin miners and other compute-stealing stuff.\\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\\nbe found in the [notes on Isolation](https://github.com/martinklepsch/cljdoc/blob/72da65055ab94942f33fb63b29b732e81b559508/doc/isolation.md)\\n", "input": "<bos><start_of_turn>user\n## Context\\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\\nsandboxed environment. A Docker image has been created to help with this but this still\\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\\nto run Bitcoin miners and other compute-stealing stuff.\\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\\nbe found in the [notes on Isolation](https://github.com/martinklepsch/cljdoc/blob/72da65055ab94942f33fb63b29b732e81b559508/doc/isolation.md)\\n<end_of_turn>\n<start_of_turn>model\n", "response": "The results of the test are important, and if the original goal was to build some kind of CI\\nservice for untrusted code, CircleCI is easy to deploy which is nice.\\nFor the Clojure plugin's planned features it would be useful to allow people to configure\\nhow tests are run so it's probably best to _not_ build a Docker image of Clojure and run it\\nin isolation. Doing so would also involve a learning curve for itself in unloading and", "time": "0:00:03.900491", "post": "## Decision\nThe results of the test are important, and if the original goal was to build some kind of CI \n service for untrusted code, CircleCI is easy to deploy which is nice. \n For the Clojure plugin's planned features it would be useful to allow people to configure \n how tests are run so it's probably best to _not_ build a Docker image of Clojure and run it \n in isolation. Doing so would also involve a learning curve for itself in unloading and"}
{"context": "## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\\nAs part of this, considerations were made around:\\n* Ease of use for creating new sites\\n* Ability to share components\\n* Ability to segregate content for specific channels (sites)\\n* Ability to share content up and down the stack where needed and appropriate\\n* Permissions model required to support this model\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\\nAs part of this, considerations were made around:\\n* Ease of use for creating new sites\\n* Ability to share components\\n* Ability to segregate content for specific channels (sites)\\n* Ability to share content up and down the stack where needed and appropriate\\n* Permissions model required to support this model\\n<end_of_turn>\n<start_of_turn>model\n", "response": "* \"Separate channels\" approach picked to first answer some qustions.\\n* Set to be tested out to work out how this (separate channels approach) underpins features required and if this approach scales as a design. \\n* This way can keep platform (in other parts for now) more familiar\\n* Should hopefully answer what is possible at MVP/T1, and what therefore would be required to handle speciality sites (lights-out).\\n* Contract CMS [T77", "time": "0:00:03.874863", "post": "## Decision\n* \"Separate channels\" approach picked to first answer some qustions. \n * Set to be tested out to work out how this (separate channels approach) underpins features required and if this approach scales as a design.  \n * This way can keep platform (in other parts for now) more familiar \n * Should hopefully answer what is possible at MVP/T1, and what therefore would be required to handle speciality sites (lights-out). \n * Contract CMS [T77"}
{"context": "## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https://symfony.com/doc/current/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https://symfony.com/doc/current/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "* The unit tests for each valid object will include validation tests.\\n* The logic for each validation will be self-describing, eg. We will use appropriate vocabulary.\\nAs a consequence of communicating the validation logic in our tests, we will be able to verify the validation claims.\\n\nmodel\n* The unit tests for each valid object will include validation tests.\\n* The logic for each validation will be self-describing, eg. We will use appropriate vocabulary.\\nAs a", "time": "0:00:03.853581", "post": "## Decision\n* The unit tests for each valid object will include validation tests. \n * The logic for each validation will be self-describing, eg. We will use appropriate vocabulary. \n As a consequence of communicating the validation logic in our tests, we will be able to verify the validation claims. \n \nmodel\n* The unit tests for each valid object will include validation tests. \n * The logic for each validation will be self-describing, eg. We will use appropriate vocabulary. \n As a"}
{"context": "## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n<end_of_turn>\n<start_of_turn>model\n", "response": "* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\nChosen option: Squash the Commits\\n### Positive Consequences\\n* Less implementation when planning using release branches.\\n* Possibly more branches which can enhance maintainability.\\n* Less computations.\\n### Negative Consequences\\n* It ignores history related to releases.\\n\nmodel\n* Simple implementation\\n* No assumption about the tasks tracker\\", "time": "0:00:03.946872", "post": "## Decision\n* Simple implementation \n * No assumption about the tasks tracker \n * Support of a set of generic fields related to a task \n Chosen option: Squash the Commits \n ### Positive Consequences \n * Less implementation when planning using release branches. \n * Possibly more branches which can enhance maintainability. \n * Less computations. \n ### Negative Consequences \n * It ignores history related to releases. \n \nmodel\n* Simple implementation \n * No assumption about the tasks tracker\\"}
{"context": "## Context\\nOur service-operator allows service teams to provision various AWS services by\\ndeclaratively defining resources and submitting them via the kubernetes api.\\nSome of these resources require IAM to authorise how the provisioned service\\ncan be used. The types of actions that can be performed on.\\n#### Example\\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\\n```\\n---\\napiVersion: storage.govsvc.uk/v1beta1\\nkind: S3Bucket\\nmetadata:\\nname: s3-bucket-sample\\nspec:\\naws:\\nLifecycleRules:\\n- Expiration: 90days\\nVersioning:\\nEnabled: true\\n```\\nIn order to access a provisioned bucket via the the AWS SDK users will require\\nan IAM role/policy that allows access.\\nWe want things like bucket ACL, versioning configuration and lifecycle policy\\nto be defined declaratively via the resource manifest (see example above), and continuously managed\\nby the service operator.\\nWe want users of the provisioned bucket to be able to read back all\\nconfiguration, and be able to fully utilise the specific bucket for reading,\\nwriting and managing their objects within the provisioned bucket, but we want\\nto avoid giving permissions to users that could cause conflicts with the\\nproperties that are managed by the service operator's reconcile loop.\\nFor example, given the example manifest above, we would like to avoid giving\\npermissions that would allow a user to alter the Expiration LifeCycleRules,\\nsince any changes the user made would be periodically overwritten by the\\nservice operator's reconciliation.\\n", "input": "<bos><start_of_turn>user\n## Context\\nOur service-operator allows service teams to provision various AWS services by\\ndeclaratively defining resources and submitting them via the kubernetes api.\\nSome of these resources require IAM to authorise how the provisioned service\\ncan be used. The types of actions that can be performed on.\\n#### Example\\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\\n```\\n---\\napiVersion: storage.govsvc.uk/v1beta1\\nkind: S3Bucket\\nmetadata:\\nname: s3-bucket-sample\\nspec:\\naws:\\nLifecycleRules:\\n- Expiration: 90days\\nVersioning:\\nEnabled: true\\n```\\nIn order to access a provisioned bucket via the the AWS SDK users will require\\nan IAM role/policy that allows access.\\nWe want things like bucket ACL, versioning configuration and lifecycle policy\\nto be defined declaratively via the resource manifest (see example above), and continuously managed\\nby the service operator.\\nWe want users of the provisioned bucket to be able to read back all\\nconfiguration, and be able to fully utilise the specific bucket for reading,\\nwriting and managing their objects within the provisioned bucket, but we want\\nto avoid giving permissions to users that could cause conflicts with the\\nproperties that are managed by the service operator's reconcile loop.\\nFor example, given the example manifest above, we would like to avoid giving\\npermissions that would allow a user to alter the Expiration LifeCycleRules,\\nsince any changes the user made would be periodically overwritten by the\\nservice operator's reconciliation.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "#### Decision\\nWe have decided that the service-operator will empower IAM resources abstracting away users, groups via aliases, archetypes and polymorphism:\\n- Aliases\\nA service operator should require a user or group in order for any provisioning of configuration to be performed (i.e. in order to transform from a template to a manifest).\\nTo prevent over-engineering the solution we will use the existing MFA enabled Platform Accounts.\\nUsers or groups must have an appropriately delegated administrative permission where", "time": "0:00:03.962744", "post": "## Decision\n#### Decision \n We have decided that the service-operator will empower IAM resources abstracting away users, groups via aliases, archetypes and polymorphism: \n - Aliases \n A service operator should require a user or group in order for any provisioning of configuration to be performed (i.e. in order to transform from a template to a manifest). \n To prevent over-engineering the solution we will use the existing MFA enabled Platform Accounts. \n Users or groups must have an appropriately delegated administrative permission where"}
{"context": "## Context\\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\\n", "input": "<bos><start_of_turn>user\n## Context\\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "The autocomplete button functionality is to be provided by suggestions api for potential values of a field.\\n\nmodel\nThe autocomplete button functionality is to be provided by suggestions api for potential values of a field.\\n\nmodel\nThe autocomplete button functionality is to be provided by suggestions api for potential values of a field.\\n\nmodel\nThe autocomplete button functionality is to be provided by suggestions api for potential values of a field.\\n\nmodel\nThe autocomplete button functionality", "time": "0:00:03.843265", "post": "## Decision\nThe autocomplete button functionality is to be provided by suggestions api for potential values of a field. \n \nmodel\nThe autocomplete button functionality is to be provided by suggestions api for potential values of a field. \n \nmodel\nThe autocomplete button functionality is to be provided by suggestions api for potential values of a field. \n \nmodel\nThe autocomplete button functionality is to be provided by suggestions api for potential values of a field. \n \nmodel\nThe autocomplete button functionality"}
{"context": "## Context and Problem Statement\\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\\n## Decision Drivers <!-- optional -->\\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible/when if needed.\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\\n## Decision Drivers <!-- optional -->\\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible/when if needed.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible/when if needed.\\nChosen option: Using the event-based pattern, because\\n* It's symmetric.\\n* Reduces sharing of common state allowing more independent components making component testing easier.\\n* Makes it more likely future modifications won't break other async loops when expanding the app.\\n* \"Makes it", "time": "0:00:03.915770", "post": "## Decision\n* Simple and straightforward way to reason around passing data between components \n * Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible/when if needed. \n Chosen option: Using the event-based pattern, because \n * It's symmetric. \n * Reduces sharing of common state allowing more independent components making component testing easier. \n * Makes it more likely future modifications won't break other async loops when expanding the app. \n * \"Makes it"}
{"context": "## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state/model.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state/model.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "I think [Prismic's Architecture](https://prismic.io/docs/core-concepts/what-is-something) is worth being read twice!\\nIn their view, the model contains the intended logic and contains no side-effects. Initiative-driven but with a separate API. This seems interesting and worth considering.\\n\nmodel\nIn this view, because it [authenticate and secured](https://our.sagradae.hr/docs/eu_union", "time": "0:00:03.921698", "post": "## Decision\nI think [Prismic's Architecture](https://prismic.io/docs/core-concepts/what-is-something) is worth being read twice! \n In their view, the model contains the intended logic and contains no side-effects. Initiative-driven but with a separate API. This seems interesting and worth considering. \n \nmodel\nIn this view, because it [authenticate and secured](https://our.sagradae.hr/docs/eu_union"}
{"context": "## Context\\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\\nAdditionally Geocoder can be used to verify addresses that are manually entered.\\n- [Geocoder](https://www2.gov.bc.ca/gov/content/data/geographic-data-services/location-services/geocoder)\\n- [Data BC](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service)\\n- [API Swagger](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service/resource/40d6411e-ab98-4df9-a24e-67f81c45f6fa/view/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\\n- [Developer API Keys](https://github.com/bcgov/gwa/wiki/Developer-Guide#developer-api-keys)\\n- API Host = `https://geocoder.api.gov.bc.ca`\\n", "input": "<bos><start_of_turn>user\n## Context\\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\\nAdditionally Geocoder can be used to verify addresses that are manually entered.\\n- [Geocoder](https://www2.gov.bc.ca/gov/content/data/geographic-data-services/location-services/geocoder)\\n- [Data BC](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service)\\n- [API Swagger](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service/resource/40d6411e-ab98-4df9-a24e-67f81c45f6fa/view/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\\n- [Developer API Keys](https://github.com/bcgov/gwa/wiki/Developer-Guide#developer-api-keys)\\n- API Host = `https://geocoder.api.gov.bc.ca`\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Used Geocoder to feed into the lookup for address suggestions\\nWe will use the Delimited Result Format\\nWe found the propery id length variable from [Environment](https://github.com/bcgov/AAE_POC/blob/main/encora-environment.ts#L346-L347)\\ntailored to our needs.\\n\nmodel\nAfter 30 minutes of wrestling with the interaction resulted in a poorly documented and Dub", "time": "0:00:03.903610", "post": "## Decision\nUsed Geocoder to feed into the lookup for address suggestions \n We will use the Delimited Result Format \n We found the propery id length variable from [Environment](https://github.com/bcgov/AAE_POC/blob/main/encora-environment.ts#L346-L347) \n tailored to our needs. \n \nmodel\nAfter 30 minutes of wrestling with the interaction resulted in a poorly documented and Dub"}
{"context": "## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "The challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n\nmodel\nThe rule will be kept simple to avoid the need of an API/Custom content handler. The url `https://www.koshvani.gov.in/>` will be used to extract the table.<br>\\n#### Constraints\\n- The `Financial Releases` page of the mentioned url should automatically navigate to year wise releases for the month selected.<br", "time": "0:00:03.856471", "post": "## Decision\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted. \n \nmodel\nThe rule will be kept simple to avoid the need of an API/Custom content handler. The url `https://www.koshvani.gov.in/>` will be used to extract the table.<br> \n #### Constraints \n - The `Financial Releases` page of the mentioned url should automatically navigate to year wise releases for the month selected.<br"}
{"context": "## Context and Problem Statement\\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\\n```\\n<xs:element name=\"take\" type=\"xs:integer\">\\n```\\nor with attributes as:\\n```\\n<xs:element name=\"take\">\\n<xs:complexType>\\n<xs:attribute name=\"value\"/>\\n</xs:complexType>\\n</xs:element>\\n```\\n## Decision Drivers\\n* consistency\\n* expressiveness\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\\n```\\n<xs:element name=\"take\" type=\"xs:integer\">\\n```\\nor with attributes as:\\n```\\n<xs:element name=\"take\">\\n<xs:complexType>\\n<xs:attribute name=\"value\"/>\\n</xs:complexType>\\n</xs:element>\\n```\\n## Decision Drivers\\n* consistency\\n* expressiveness\\n<end_of_turn>\n<start_of_turn>model\n", "response": "* consistency\\n* expressiveness\\nChosen option: \"Represent operations as attributes of the element\", because this choice is more consistent in the hypothesis that technical elements can be used as a sub-entity of a meta-model.\\n\nmodel\n* consistency\\n* expressiveness\\nChosen option: \"Represent operations as attributes of the element\", because this choice is more consistent in the hypothesis that technical elements can be used as a sub-entity of a meta-model.\\", "time": "0:00:03.887017", "post": "## Decision\n* consistency \n * expressiveness \n Chosen option: \"Represent operations as attributes of the element\", because this choice is more consistent in the hypothesis that technical elements can be used as a sub-entity of a meta-model. \n \nmodel\n* consistency \n * expressiveness \n Chosen option: \"Represent operations as attributes of the element\", because this choice is more consistent in the hypothesis that technical elements can be used as a sub-entity of a meta-model.\\"}
{"context": "Context\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https://github.com/openedx/event-routing-backends/blob/f430d4cf58bdab01e42fcc944241898606873d82/event_routing_backends/processors/mixins/base_transformer.py#L139\\n", "input": "<bos><start_of_turn>user\nContext\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https://github.com/openedx/event-routing-backends/blob/f430d4cf58bdab01e42fcc944241898606873d82/event_routing_backends/processors/mixins/base_transformer.py#L139\\n<end_of_turn>\n<start_of_turn>model\n", "response": "--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper),", "time": "0:00:03.865705", "post": "## Decision\n-------- \n #. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers. \n #. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception. \n #. If a key, deemed optional by the specification (xAPI or Caliper),"}
{"context": "# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https://www.gov.uk/service-manual/making-software/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop / HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n", "input": "<bos><start_of_turn>user\n# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https://www.gov.uk/service-manual/making-software/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop / HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n\nmodel\nIt has been agreed that we will use AWS RDS for our database.\\nThis was done collaboratively. Some of the challenges with the induction and\\nextensive documentation process were:\\", "time": "0:00:03.859749", "post": "## Decision\nWe will use MongoDB. We are comfortable operating it in production, \n it will allow unstructured data (in the form of JSON documents) and we can \n apply structure at query time. \n # Status \n Accepted. \n # Consequences \n Use MongoDB with an appropriate replica-set configuration. \n \nmodel\nIt has been agreed that we will use AWS RDS for our database. \n This was done collaboratively. Some of the challenges with the induction and \n extensive documentation process were:\\"}
{"context": "## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https://www.w3.org/WAI/WCAG21/Understanding/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https://github.com/hmrc/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https://www.w3.org/WAI/WCAG21/Understanding/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https://github.com/hmrc/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for", "time": "0:00:03.895468", "post": "## Decision\n* The need to minimise code changes for service teams other than a library upgrade. \n * The avoidance of requiring service teams to add any additional routing rules. \n * The avoidance of requiring service teams to add any additional configuration. \n * The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and \n any other library or service. \n * The preference for maintaining loose coupling of frontend services and avoiding adding single points \n of failure between frontend microservices. \n * The need for"}
{"context": "## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n", "input": "<bos><start_of_turn>user\n## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We decided to stay with the PSR adopters standards.\\nWe were curious about HHVM, but we don't have any HHVM engineer\\nenough for adopting it right now.\\n\nmodel\nWe decided to stay with the PSR adopters standards.\\nWe were curious about HHVM, but we don't have any HHVM engineer\\nenough for adopting it right now.\\n\nmodel\nWe decided to stay with the PSR adopters standards.\\n", "time": "0:00:03.898775", "post": "## Decision\nWe decided to stay with the PSR adopters standards. \n We were curious about HHVM, but we don't have any HHVM engineer \n enough for adopting it right now. \n \nmodel\nWe decided to stay with the PSR adopters standards. \n We were curious about HHVM, but we don't have any HHVM engineer \n enough for adopting it right now. \n \nmodel\nWe decided to stay with the PSR adopters standards. \n "}
{"context": "## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n", "input": "<bos><start_of_turn>user\n## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We have decided to go with SWF, primarily because\\n* we are no longer required to implement S3-based storage changes in order to obtain a HA cluster (AWS HA clusters have become less expensive)\\n* hetzner servers [could still be acceptable](https://docs.aws.amazon.com/amazonswf/latest/developerguide/emodb-asg-hetzner.html) so we do not need to invest in ES and EP stores\\nThe next", "time": "0:00:03.846942", "post": "## Decision\nWe have decided to go with SWF, primarily because \n * we are no longer required to implement S3-based storage changes in order to obtain a HA cluster (AWS HA clusters have become less expensive) \n * hetzner servers [could still be acceptable](https://docs.aws.amazon.com/amazonswf/latest/developerguide/emodb-asg-hetzner.html) so we do not need to invest in ES and EP stores \n The next"}
{"context": "## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n<end_of_turn>\n<start_of_turn>model\n", "response": "* High availability/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\nOption 2 MicroSoft Azure Cosmos DB\\n### Positive Consequences <!-- optional -->\\n* Cosmos DB will provide scalable storage for RDF. This scales automatically up to as high as 10 OLTRPs.\\n* A MongoDB API is provided for the case in which RDF computation fails or is unavailable. This is a major improvement:\\nthe projected development for the Cosmos DB", "time": "0:00:03.900282", "post": "## Decision\n* High availability/Fault tolerance \n * Learning curve \n * Maintenance overhead \n * Vendor lock-in \n Option 2 MicroSoft Azure Cosmos DB \n ### Positive Consequences <!-- optional --> \n * Cosmos DB will provide scalable storage for RDF. This scales automatically up to as high as 10 OLTRPs. \n * A MongoDB API is provided for the case in which RDF computation fails or is unavailable. This is a major improvement: \n the projected development for the Cosmos DB"}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe need to record the architectural decisions made on this project.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\nmodel\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n", "time": "0:00:03.640442", "post": "## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions). \n \nmodel\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions). \n "}
{"context": "## Context\\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\\n", "input": "<bos><start_of_turn>user\n## Context\\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "The studio provides access to these background tasks in the form of CLI commands using the [Bus](https://github.com/zooniverse/bus) npm package. These CLI commands are passed tasks that are executed in a [node runner](https://github.com/zooniverse/bus/tree/master/packages/task-runners/node). [Bus](https://github.com/zooniverse/bus) was chosen as a work-in-progress prior to the", "time": "0:00:03.829204", "post": "## Decision\nThe studio provides access to these background tasks in the form of CLI commands using the [Bus](https://github.com/zooniverse/bus) npm package. These CLI commands are passed tasks that are executed in a [node runner](https://github.com/zooniverse/bus/tree/master/packages/task-runners/node). [Bus](https://github.com/zooniverse/bus) was chosen as a work-in-progress prior to the"}
{"context": "## Context\\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\\nto not overload a server.\\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\\nallows, among others:\\n- Delaying retries upon MX delivery failure to a remote site.\\n- Throttling, which could be helpful for not being considered a spammer.\\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\\ndelays, purging the queue, etc.\\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\\nto interact with all its James servers, which is not friendly in a distributed setup.\\nDistributed James relies on the following third party softwares (among other):\\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\\n- **ObjectStorage** (Swift or S3) holds byte content.\\n", "input": "<bos><start_of_turn>user\n## Context\\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\\nto not overload a server.\\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\\nallows, among others:\\n- Delaying retries upon MX delivery failure to a remote site.\\n- Throttling, which could be helpful for not being considered a spammer.\\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\\ndelays, purging the queue, etc.\\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\\nto interact with all its James servers, which is not friendly in a distributed setup.\\nDistributed James relies on the following third party softwares (among other):\\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\\n- **ObjectStorage** (Swift or S3) holds byte content.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "\\nGive the technical background explained above, the decision is to inject\u00a0**Apache Cassandra**\u00a0main object store\\nfor mail content to Community. By doing so, we will ensure scale ability through a very known and proven object store\\nproviding also ease administration with a JSON message format understanding the RabbitMQ methods APIs by\\nexploring the `writeAheadLog` mechanism to empower the queue in Cassandra while leveraging distributed\\ntechnology on the content, we will be able to achieve an", "time": "0:00:03.910901", "post": "## Decision\n \n Give the technical background explained above, the decision is to inject\u00a0**Apache Cassandra**\u00a0main object store \n for mail content to Community. By doing so, we will ensure scale ability through a very known and proven object store \n providing also ease administration with a JSON message format understanding the RabbitMQ methods APIs by \n exploring the `writeAheadLog` mechanism to empower the queue in Cassandra while leveraging distributed \n technology on the content, we will be able to achieve an"}
{"context": "## Context\\nThe main data source of the API catalogue website was originally a collection of\\nmarkdown files. The build process uses the Middleman static site generator\\nconfigured by the [Tech Docs Gem](https://github.com/alphagov/tech-docs-gem)\\n('TDG').\\nThe TDG provides additional functionality including search, sidebar\\nnavigation ('Table of Contents'), the layout, and styling.\\nThe TDG is not necessarily a good fit for the API catalogue because the project\\nisn't purely documentation, and our data source is now a CSV.\\nIn particular it is difficult to override templates inherited from the gem, to\\nadjust the layout on a particular page or add page-specific JavaScript for\\nexample.\\nUsing TDG to render the Table of Contents is slow for our site because\\nby design every page is re-rendered multiple times to pull out the headings\\n(adding over a minute to build times).\\nThe TDG also requires specific dependency versions. These version\\nrestrictions prevent us being in control of version upgrades which are necessary\\nto remain on support versions and receive security patches.\\n", "input": "<bos><start_of_turn>user\n## Context\\nThe main data source of the API catalogue website was originally a collection of\\nmarkdown files. The build process uses the Middleman static site generator\\nconfigured by the [Tech Docs Gem](https://github.com/alphagov/tech-docs-gem)\\n('TDG').\\nThe TDG provides additional functionality including search, sidebar\\nnavigation ('Table of Contents'), the layout, and styling.\\nThe TDG is not necessarily a good fit for the API catalogue because the project\\nisn't purely documentation, and our data source is now a CSV.\\nIn particular it is difficult to override templates inherited from the gem, to\\nadjust the layout on a particular page or add page-specific JavaScript for\\nexample.\\nUsing TDG to render the Table of Contents is slow for our site because\\nby design every page is re-rendered multiple times to pull out the headings\\n(adding over a minute to build times).\\nThe TDG also requires specific dependency versions. These version\\nrestrictions prevent us being in control of version upgrades which are necessary\\nto remain on support versions and receive security patches.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "The site will render the index page and any extra CSS/JS required to render any\\npage in Markup from Start Kit, all index.mb.json files will be merged together\\nto be passed to Sprockets, Start Kit will maintain the site's root layout markup.\\nThe markup from the main.js.erb file (originally created by TDG) will be\\nremoved.\\n\nmodel\nThe site will render the index page and any extra CSS/JS", "time": "0:00:03.858919", "post": "## Decision\nThe site will render the index page and any extra CSS/JS required to render any \n page in Markup from Start Kit, all index.mb.json files will be merged together \n to be passed to Sprockets, Start Kit will maintain the site's root layout markup. \n The markup from the main.js.erb file (originally created by TDG) will be \n removed. \n \nmodel\nThe site will render the index page and any extra CSS/JS"}
{"context": "## Context and Problem Statement\\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https://github.com/Semantic-Org/Semantic-UI-React/pull/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https://github.com/decentraland/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\\n- The `@artsy/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\\n- Doesn't have hooks support.\\n#### Second alternative (react-semantic-ui)\\n##### Advantages\\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\\n##### Disadvantages\\n- Bad SSR support.\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https://github.com/Semantic-Org/Semantic-UI-React/pull/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https://github.com/decentraland/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\\n- The `@artsy/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\\n- Doesn't have hooks support.\\n#### Second alternative (react-semantic-ui)\\n##### Advantages\\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\\n##### Disadvantages\\n- Bad SSR support.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "* We tried and used an alternative for the `Responsive` component in the `Responsive` component (`@artsy/fresnel`), but we have to face the fact that it has queries by default that we don't want.\\n* Faced the same problems, finding the second alternative as the key solution. The only disadvantages from the `@tgriesser/frontend` implementation is that it requires a consumer component.\\n\nmodel\nChosen option: \"Keep `@artsy", "time": "0:00:03.918855", "post": "## Decision\n* We tried and used an alternative for the `Responsive` component in the `Responsive` component (`@artsy/fresnel`), but we have to face the fact that it has queries by default that we don't want. \n * Faced the same problems, finding the second alternative as the key solution. The only disadvantages from the `@tgriesser/frontend` implementation is that it requires a consumer component. \n \nmodel\nChosen option: \"Keep `@artsy"}
{"context": "## Context\\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\\nIn the future we will want to more automatically match buyers\u2019 articulations of need so as to work out which products CCS can offer to meet their need.\\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\\n", "input": "<bos><start_of_turn>user\n## Context\\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\\nIn the future we will want to more automatically match buyers\u2019 articulations of need so as to work out which products CCS can offer to meet their need.\\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Each supplier advert will have one list of questions, since they do not related to CCS commercial agreements. It will contain one list of datafields which will include the details to be recorded as well as one for outcome fields, since an outcome field may contain the same information as a data field but relate so specifically to our outcomes that its title should be different.\\nBuyer question categories and lists will be maintained within Framework agreement templates.\\nFMW will have one additional category of datafields but offer both buyer and", "time": "0:00:03.878022", "post": "## Decision\nEach supplier advert will have one list of questions, since they do not related to CCS commercial agreements. It will contain one list of datafields which will include the details to be recorded as well as one for outcome fields, since an outcome field may contain the same information as a data field but relate so specifically to our outcomes that its title should be different. \n Buyer question categories and lists will be maintained within Framework agreement templates. \n FMW will have one additional category of datafields but offer both buyer and"}
{"context": "## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted/PaaS CI/CD solution\\n", "input": "<bos><start_of_turn>user\n## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted/PaaS CI/CD solution\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Utilize AWS CodePipeline and CodeBuild at the State and Local level as our CI/CD platform\\nEJH will have local CI/CD, but the deployment will still utilize PaaS service and their specific CI/CD, should it be Terraform Cloud, Hypeerfast, GitLab, or something else\\nConsult with each LOP's Handeler and CDO about where the integration will take place in their pipeline.  Should it be in Logic Apps, an Azure Function", "time": "0:00:03.848942", "post": "## Decision\nUtilize AWS CodePipeline and CodeBuild at the State and Local level as our CI/CD platform \n EJH will have local CI/CD, but the deployment will still utilize PaaS service and their specific CI/CD, should it be Terraform Cloud, Hypeerfast, GitLab, or something else \n Consult with each LOP's Handeler and CDO about where the integration will take place in their pipeline.  Should it be in Logic Apps, an Azure Function"}
{"context": "## Context\\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\\n", "input": "<bos><start_of_turn>user\n## Context\\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\\n<end_of_turn>\n<start_of_turn>model\n", "response": "The options one and two, although simple have massive limitations:\\n- The owner of the account that the API writes to S3 has total administration access (including deleting, downloading) over whatever they upload. Although client applications have no access to these files, this could present a risk if an API is compromised\\n- From a security point of view this seems like a dangerous security bypass for anyone with a key for that account\\n- The BLOB we want to send is large and we have", "time": "0:00:03.884196", "post": "## Decision\nThe options one and two, although simple have massive limitations: \n - The owner of the account that the API writes to S3 has total administration access (including deleting, downloading) over whatever they upload. Although client applications have no access to these files, this could present a risk if an API is compromised \n - From a security point of view this seems like a dangerous security bypass for anyone with a key for that account \n - The BLOB we want to send is large and we have"}
{"context": "## Context\\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\\n", "input": "<bos><start_of_turn>user\n## Context\\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We decide to use mono repo solution because it allows us to deliver products through one repository which for the end users leads to easier integration and maintenance.\\nhttps://docs.npmjs.com/files/packagejson\\nhttps://github.com/zeit/next.js#manual-eject\\n\nmodel\nWe decide to use mono repo solution because it allows us to deliver products through one repository which for the end users leads to easier integration and maintenance.\\nhttps://docs", "time": "0:00:04.480284", "post": "## Decision\nWe decide to use mono repo solution because it allows us to deliver products through one repository which for the end users leads to easier integration and maintenance. \n https://docs.npmjs.com/files/packagejson \n https://github.com/zeit/next.js#manual-eject \n \nmodel\nWe decide to use mono repo solution because it allows us to deliver products through one repository which for the end users leads to easier integration and maintenance. \n https://docs"}
{"context": "## Context\\nDrop FreeTFT, TFTA tokens from SDK\\n", "input": "<bos><start_of_turn>user\n## Context\\nDrop FreeTFT, TFTA tokens from SDK\\n<end_of_turn>\n<start_of_turn>model\n", "response": "1. Drop FreeTFT, TFTA tokens from SDK and debugger JS client\\n1. Remove TFTA, Tft tokens injected into the widget\\n1. Change tftToken.json to include all tokens (metadata about Tft tokens, TFta and FrE tokens)\\n1. Convert to SQL based database tables\\n\nmodel\n1. Drop FreeTFT, TFTA tokens from SDK and debugger JS client\\n1. Remove TFTA, Tft tokens", "time": "0:00:03.835784", "post": "## Decision\n1. Drop FreeTFT, TFTA tokens from SDK and debugger JS client \n 1. Remove TFTA, Tft tokens injected into the widget \n 1. Change tftToken.json to include all tokens (metadata about Tft tokens, TFta and FrE tokens) \n 1. Convert to SQL based database tables \n \nmodel\n1. Drop FreeTFT, TFTA tokens from SDK and debugger JS client \n 1. Remove TFTA, Tft tokens"}
{"context": "## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThe original author of go-bindata delete their account and then the repo was\\nrecreated under a different owner. The dependency has shifted around enough that\\nwe have lost faith in the intention of the maintainers\\n[more details here](https://twitter.com/francesc/status/961249107020001280?lang=en)\\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\\nduring a test run or final build) made it hard to reason about.\\n", "input": "<bos><start_of_turn>user\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThe original author of go-bindata delete their account and then the repo was\\nrecreated under a different owner. The dependency has shifted around enough that\\nwe have lost faith in the intention of the maintainers\\n[more details here](https://twitter.com/francesc/status/961249107020001280?lang=en)\\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\\nduring a test run or final build) made it hard to reason about.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Go-bindata dependency has been removed.\\nSome of the files were originally fetched using the go-bindata\\ninstead of distributed .zip files instead, to increase reusability and\\nspeed-up in the client. Updated client now uses the .zip.\\nIn previous documentation, we mentioned that:\\n- go-bindata package was actively maintained\\n- go-bindata package was actively maintained\\nfrom a variety of sources (like docker images)\\nWe will", "time": "0:00:03.839591", "post": "## Decision\nGo-bindata dependency has been removed. \n Some of the files were originally fetched using the go-bindata \n instead of distributed .zip files instead, to increase reusability and \n speed-up in the client. Updated client now uses the .zip. \n In previous documentation, we mentioned that: \n - go-bindata package was actively maintained \n - go-bindata package was actively maintained \n from a variety of sources (like docker images) \n We will"}
{"context": "## Context\\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\\n[Prettier](https://prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\\n", "input": "<bos><start_of_turn>user\n## Context\\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\\n[Prettier](https://prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will use Prettier to format source code.\\n\nmodel\nWe will use Prettier to format source code.\\n\nmodel\nWe will use Prettier to format source code.\\n\nmodel\nWe will use Prettier to format source code.\\n\nmodel\nWe will use Prettier to format source code.\\n\nmodel\nWe will use Prettier to format source code.\\n\nmodel\nWe will use Pret", "time": "0:00:03.952720", "post": "## Decision\nWe will use Prettier to format source code. \n \nmodel\nWe will use Prettier to format source code. \n \nmodel\nWe will use Prettier to format source code. \n \nmodel\nWe will use Prettier to format source code. \n \nmodel\nWe will use Prettier to format source code. \n \nmodel\nWe will use Prettier to format source code. \n \nmodel\nWe will use Pret"}
{"context": "## Context\\nWe have to make a decision on the branching strategy for development.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe have to make a decision on the branching strategy for development.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will follow the [GitFlow](http://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow) branching strategy. This comes with a lot of best practices for release management, plus it provides a solid structure for how, and when, to create branches and when to merge them. It also defines sensible set of branches.\\nThe strategy will be used by all developers on the team.\\n\nmodel\nWe will follow the [Git", "time": "0:00:03.848284", "post": "## Decision\nWe will follow the [GitFlow](http://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow) branching strategy. This comes with a lot of best practices for release management, plus it provides a solid structure for how, and when, to create branches and when to merge them. It also defines sensible set of branches. \n The strategy will be used by all developers on the team. \n \nmodel\nWe will follow the [Git"}
{"context": "## Context\\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We provide explicit combination of configurations and their relations without changing the behavior of the decorator for single configuration, so we do not introduce unexpected side effects on transformer loading.\\n- Option C\\n\nmodel\nWe provide explicit combination of configurations and their relations without changing the behavior of the decorator for single configuration, so we do not introduce unexpected side effects on transformer loading.\\n- Option C\\n\nmodel\nWe provide explicit combination of configurations and their relations without changing the behavior of", "time": "0:00:03.828752", "post": "## Decision\nWe provide explicit combination of configurations and their relations without changing the behavior of the decorator for single configuration, so we do not introduce unexpected side effects on transformer loading. \n - Option C \n \nmodel\nWe provide explicit combination of configurations and their relations without changing the behavior of the decorator for single configuration, so we do not introduce unexpected side effects on transformer loading. \n - Option C \n \nmodel\nWe provide explicit combination of configurations and their relations without changing the behavior of"}
{"context": "## Context\\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\\n```\\nstruct tof\\n{\\nusing distance_t = uint16_t;\\nusing cb_t = stdext::inplace_function<void(distance_t)>;\\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\\nenum class mode\\n{\\ndefaultRange = 0,\\nshortRange,\\nmedRange,\\nlongRange,\\n};\\n}''\\n```\\nInterface classes would inherit from these structs:\\n```\\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\\n```\\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\\n```\\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\\n```\\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\\n", "input": "<bos><start_of_turn>user\n## Context\\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\\n```\\nstruct tof\\n{\\nusing distance_t = uint16_t;\\nusing cb_t = stdext::inplace_function<void(distance_t)>;\\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\\nenum class mode\\n{\\ndefaultRange = 0,\\nshortRange,\\nmedRange,\\nlongRange,\\n};\\n}''\\n```\\nInterface classes would inherit from these structs:\\n```\\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\\n```\\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\\n```\\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\\n```\\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will abandon any approach of using `struct`s to derive generic interfaces, instead generating them via enums like we do for our drivers.\\n\nmodel\nEnum\\n\nmodel\nEnum\\n\nmodel\nEnum\\n\nmodel\nEnum\\n\nmodel\nWe will reduce the generated enums to constants, change the method name from `as_enum` to `value_or_error()`, and make the call made with the", "time": "0:00:03.993411", "post": "## Decision\nWe will abandon any approach of using `struct`s to derive generic interfaces, instead generating them via enums like we do for our drivers. \n \nmodel\nEnum \n \nmodel\nEnum \n \nmodel\nEnum \n \nmodel\nEnum \n \nmodel\nWe will reduce the generated enums to constants, change the method name from `as_enum` to `value_or_error()`, and make the call made with the"}
{"context": "### Context\\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\\n```php\\nfunction test_something(): void {\\n// ...\\ntry {\\n// the statement that fail\\n$this->fail();\\n} catch (Exception $e) {\\n// ...\\n}\\n}\\n```\\nOr:\\n```php\\nfunction test_something(): void {\\n// ...\\n$this->expectException($exception)\\n// the statement that fail\\n}\\n```\\n### Decision\\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https://thephp.cc/company/consultants/sebastian-bergmann\\n[phpunit-exception-best-practices]: https://thephp.cc/news/2016/02/questioning-phpunit-best-practices\\n[1090]: https://github.com/infection/infection/pull/1061\\n", "input": "<bos><start_of_turn>user\n### Context\\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\\n```php\\nfunction test_something(): void {\\n// ...\\ntry {\\n// the statement that fail\\n$this->fail();\\n} catch (Exception $e) {\\n// ...\\n}\\n}\\n```\\nOr:\\n```php\\nfunction test_something(): void {\\n// ...\\n$this->expectException($exception)\\n// the statement that fail\\n}\\n```\\n### Decision\\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https://thephp.cc/company/consultants/sebastian-bergmann\\n[phpunit-exception-best-practices]: https://thephp.cc/news/2016/02/questioning-phpunit-best-practices\\n[1090]: https://github.com/infection/infection/pull/1061\\n<end_of_turn>\n<start_of_turn>model\n", "response": "As recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#", "time": "0:00:03.956533", "post": "## Decision\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in \n [this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is \n necessary, the decision taken is to leverage the `expectException*()` API when possible. \n A pull request to fix this practice in the whole codebase may be done but has not been made \n mandatory. New pull requests though should stick to this practice. \n ### Status \n Accepted ([#"}
{"context": "## Context\\nWhen developing new algorithms and features for PACE-related software,\\ndevelopers often create useful demo/visualisation scripts for their own use.\\nThese scripts could be useful or interesting for other developers, and are\\nimportant for reproducibility or justifying design decisions. They should be\\nstored somewhere in version control so that they can be easily accessed by any\\ndevelopers and referred to later. However, they are not intended for general\\nuse and will not be actively maintained or tested. There are 2 main options:\\n* Store them in a `dev_scripts` directory in each separate project repository\\n* Store them in a `scripts` directory in `pace-developers`\\nIf they're in the `dev_scripts` directory for each project repository:\\n+ All in one place\\n+ Scripts will be close to the code they are used for\\n- Scripts may not work with the version of the code they are distributed with\\n- It's unclear where scripts that use more than one project would go\\n- Despite the folder being called `dev_scripts` people might expect the scripts\\nto actually work as they're in the main project repository\\nIf they're in a `scripts` directory in `pace-developers`:\\n+ They can be kept close to the decision-making developer documentation that\\nthey support\\n+ A version can be specified for any project dependencies\\n", "input": "<bos><start_of_turn>user\n## Context\\nWhen developing new algorithms and features for PACE-related software,\\ndevelopers often create useful demo/visualisation scripts for their own use.\\nThese scripts could be useful or interesting for other developers, and are\\nimportant for reproducibility or justifying design decisions. They should be\\nstored somewhere in version control so that they can be easily accessed by any\\ndevelopers and referred to later. However, they are not intended for general\\nuse and will not be actively maintained or tested. There are 2 main options:\\n* Store them in a `dev_scripts` directory in each separate project repository\\n* Store them in a `scripts` directory in `pace-developers`\\nIf they're in the `dev_scripts` directory for each project repository:\\n+ All in one place\\n+ Scripts will be close to the code they are used for\\n- Scripts may not work with the version of the code they are distributed with\\n- It's unclear where scripts that use more than one project would go\\n- Despite the folder being called `dev_scripts` people might expect the scripts\\nto actually work as they're in the main project repository\\nIf they're in a `scripts` directory in `pace-developers`:\\n+ They can be kept close to the decision-making developer documentation that\\nthey support\\n+ A version can be specified for any project dependencies\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Some decision context here, to set the scene. We're considering whether to do\\nthing A or thing B, or maybe thing C, and we've identified the pros and cons of\\neach option. In the end we've decided that _[thing which has been decided]_ - you\\ncan find more details under the Change Log.  Any changes to README should be laid\\nout separately.\\n\nmodel\nSome decision context here, to set the", "time": "0:00:03.856626", "post": "## Decision\nSome decision context here, to set the scene. We're considering whether to do \n thing A or thing B, or maybe thing C, and we've identified the pros and cons of \n each option. In the end we've decided that _[thing which has been decided]_ - you \n can find more details under the Change Log.  Any changes to README should be laid \n out separately. \n \nmodel\nSome decision context here, to set the"}
{"context": "**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\\nfor all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https://en.cppreference.com/w/cpp/io/ios_base/iostate\\n[bigtable-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/bigtable\\n[gcs-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/storage\\n[survey-link]: https://isocpp.org/blog/2018/03/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n", "input": "<bos><start_of_turn>user\n**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\\nfor all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https://en.cppreference.com/w/cpp/io/ios_base/iostate\\n[bigtable-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/bigtable\\n[gcs-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/storage\\n[survey-link]: https://isocpp.org/blog/2018/03/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n<end_of_turn>\n<start_of_turn>model\n", "response": "**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\\nfor all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library", "time": "0:00:03.984549", "post": "## Decision\n**Context**: We know there will be users of these C++ libraries who want to use \n C++ exceptions as well as those who are not able to. Our C++ libraries must work \n for all of our users, regardless of their ability to use exceptions. \n **Decision**: None of our APIs will throw exceptions to indicate errors. \n Instead, our APIs will typically report errors to callers by returning a \n `Status` or `StatusOr<T>` object, unless the library"}
{"context": "## Context and Problem Statement\\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Option 1 is the best option due to our development team. We have lots of different tech level developers and some of them are not very good at using Gradle DSL.\\nChosen option: \"Option 1: Turn off commands/tasks in order to decrease build times for all developers\"\\nI also add that we should move some plugins to runtime.\\n\nmodel\nOption 1 is the best option due to our development team. We have lots of different tech level developers and", "time": "0:00:03.900702", "post": "## Decision\nOption 1 is the best option due to our development team. We have lots of different tech level developers and some of them are not very good at using Gradle DSL. \n Chosen option: \"Option 1: Turn off commands/tasks in order to decrease build times for all developers\" \n I also add that we should move some plugins to runtime. \n \nmodel\nOption 1 is the best option due to our development team. We have lots of different tech level developers and"}
{"context": "## Context and Problem Statement\\nIn order to simplify the tracking consent build and deploy process and\\nmake integrating with tracking consent less surprising, should\\nthe configuration of the GTM container used by tracking consent be via\\ndata attributes rather than separate bundles?\\n## Decision Drivers\\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\nIn order to simplify the tracking consent build and deploy process and\\nmake integrating with tracking consent less surprising, should\\nthe configuration of the GTM container used by tracking consent be via\\ndata attributes rather than separate bundles?\\n## Decision Drivers\\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\n<end_of_turn>\n<start_of_turn>model\n", "response": "* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\nChosen option: \"configuration via data attributes\", because it will reduce\\nthe cognitive load on service developers and, it is believed, reduce the\\namount of boilerplate JavaScript that needs to be added in order to use\\ntracking consent.\\n### Positive Consequences\\n* The bundle `tracking-consent-config.js` is no longer required, therefore\\", "time": "0:00:03.809166", "post": "## Decision\n* The need to keep things simple for service developers \n * The need to improve the operability of tracking consent \n Chosen option: \"configuration via data attributes\", because it will reduce \n the cognitive load on service developers and, it is believed, reduce the \n amount of boilerplate JavaScript that needs to be added in order to use \n tracking consent. \n ### Positive Consequences \n * The bundle `tracking-consent-config.js` is no longer required, therefore\\"}
{"context": "## Context\\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\\n", "input": "<bos><start_of_turn>user\n## Context\\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "This project will use arrow functions (syntactically optional [ternary operator =>](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Bitwise_OR_operator#valueof_condition_in_the_ternary_operator_useful_as_short_if_but_nullish_coalescing_shortcircuit_util)) where possible.\\nFunction declarations will only be used when teaching.\\n\n", "time": "0:00:03.879261", "post": "## Decision\nThis project will use arrow functions (syntactically optional [ternary operator =>](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Bitwise_OR_operator#valueof_condition_in_the_ternary_operator_useful_as_short_if_but_nullish_coalescing_shortcircuit_util)) where possible. \n Function declarations will only be used when teaching. \n "}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe need to record the architectural decisions made on this project.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\nmodel\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\nmodel\n", "time": "0:00:03.916320", "post": "## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions \n \nmodel\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions \n \nmodel"}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe need to record the architectural decisions made on this project.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\nmodel\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\nmodel\n", "time": "0:00:03.824248", "post": "## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions \n \nmodel\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions \n \nmodel"}
{"context": "## Context\\nI need to implement two special pages, the privacy policy page and the about me page.\\nThey can't be in the map, I don't want them there because the user would\\nbe forced to read them when scanning the spiral.\\n", "input": "<bos><start_of_turn>user\n## Context\\nI need to implement two special pages, the privacy policy page and the about me page.\\nThey can't be in the map, I don't want them there because the user would\\nbe forced to read them when scanning the spiral.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "I decided to implement two components for every two, in order to manage\\ndimensions and keep the dimensions constant, they will be unfolded and displayed\\nwhen touched.\\nThe two touched actions should send a signal that communicates accessible component.\\n\nmodel\nI decided to implement two components for the two pages, in order to be able to maintain\\nconstant dimensions. Instead of putting them separate from the components in a separate library\\nbecause they are quite simple, I decided to", "time": "0:00:03.842833", "post": "## Decision\nI decided to implement two components for every two, in order to manage \n dimensions and keep the dimensions constant, they will be unfolded and displayed \n when touched. \n The two touched actions should send a signal that communicates accessible component. \n \nmodel\nI decided to implement two components for the two pages, in order to be able to maintain \n constant dimensions. Instead of putting them separate from the components in a separate library \n because they are quite simple, I decided to"}
{"context": "## Context\\nThe current request builder APIs are not able to handle some odata requests like:\\n- query navigation properties `GET /People('scottketchum')/Friends`\\n- getting \"raw value\" of a property `/People('scottketchum')/$value`\\n", "input": "<bos><start_of_turn>user\n## Context\\nThe current request builder APIs are not able to handle some odata requests like:\\n- query navigation properties `GET /People('scottketchum')/Friends`\\n- getting \"raw value\" of a property `/People('scottketchum')/$value`\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Option 2: Expand the AllRequestBuilder in order to implement the new non-query methods.\\n- provide a set of methods like `GetRawValue` for getting the rawValue of the data.\\n- fetch keys, values, headers, raw. this set of methods will be supported in all ReadOptions (AllReadOptions).\\n- add the `Link` property to Option and ReadOptions so that we know that everything else is supported to protect against inappropriate usage outside of Keys.\\n", "time": "0:00:03.886855", "post": "## Decision\nOption 2: Expand the AllRequestBuilder in order to implement the new non-query methods. \n - provide a set of methods like `GetRawValue` for getting the rawValue of the data. \n - fetch keys, values, headers, raw. this set of methods will be supported in all ReadOptions (AllReadOptions). \n - add the `Link` property to Option and ReadOptions so that we know that everything else is supported to protect against inappropriate usage outside of Keys. \n "}
{"context": "## Context\\nFrequency Capping allows Content Managers to limit the number of\\nimpressions or interactions users have with content. Is a widely\\navailable tool in Publishing Platforms.\\nIt's usually developed on the server side where the system can decide\\nhow many times to serve the content to the requesting users which we\\ncall \"Global Frequency Capping\". Additionally the system may be able\\nto limit the number of impressions per user which we call \"Local\" or\\n\"User Frequency Capping\".\\nFor example a Content Piece can be set to 1,000,000 Global Impressions\\nand 1 Impression per User, thus indirectly driving 1,000,000 different\\nusers to this Content.\\nThis functionality has been lacking from the Snippet Service due to\\ntechnical limitations imposed by the way metrics were collected and\\ncontent selection was handled on the client side. The latest\\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\\nPipeline unblock this capability. [0]\\n", "input": "<bos><start_of_turn>user\n## Context\\nFrequency Capping allows Content Managers to limit the number of\\nimpressions or interactions users have with content. Is a widely\\navailable tool in Publishing Platforms.\\nIt's usually developed on the server side where the system can decide\\nhow many times to serve the content to the requesting users which we\\ncall \"Global Frequency Capping\". Additionally the system may be able\\nto limit the number of impressions per user which we call \"Local\" or\\n\"User Frequency Capping\".\\nFor example a Content Piece can be set to 1,000,000 Global Impressions\\nand 1 Impression per User, thus indirectly driving 1,000,000 different\\nusers to this Content.\\nThis functionality has been lacking from the Snippet Service due to\\ntechnical limitations imposed by the way metrics were collected and\\ncontent selection was handled on the client side. The latest\\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\\nPipeline unblock this capability. [0]\\n<end_of_turn>\n<start_of_turn>model\n", "response": "With the opportunities present in Fenix web rendering and its combined\\nclient-server nature to easily make frequencies capable we will build\\na replacement for the Message Frequency Capping Service to do Global\\nand Local Frequency Capping via the current Distribution API mechanisms.\\n[1]\\n\nmodel\nWith the opportunities present in Fenix web rendering and its combined\\nclient-server nature to easily make frequencies capable we will build\\na replacement for the Message Frequency Capping Service to do", "time": "0:00:03.926426", "post": "## Decision\nWith the opportunities present in Fenix web rendering and its combined \n client-server nature to easily make frequencies capable we will build \n a replacement for the Message Frequency Capping Service to do Global \n and Local Frequency Capping via the current Distribution API mechanisms. \n [1] \n \nmodel\nWith the opportunities present in Fenix web rendering and its combined \n client-server nature to easily make frequencies capable we will build \n a replacement for the Message Frequency Capping Service to do"}
{"context": "## Context\\nThe application is about finding services closest to the search point. All\\nservices have a co-ordinate in lat/lon format. Currently the application\\nrequests either an out-code or a postcode as the search point. The submitted\\npostcode needs to be resolved to a lat/lon that can be used to query against.\\n", "input": "<bos><start_of_turn>user\n## Context\\nThe application is about finding services closest to the search point. All\\nservices have a co-ordinate in lat/lon format. Currently the application\\nrequests either an out-code or a postcode as the search point. The submitted\\npostcode needs to be resolved to a lat/lon that can be used to query against.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "A geocoding service will be used to resolve postcodes to lat/lon and any other\\nSearch Point Coordinate type in future.\\nAny deviations to this decision shall be documented herein.\\n\nmodel\nWe will rely on Google Maps API for external geocoding of postcodes to get the\\nexact `lat/lon` before querying. We will use the validator library to identify\\nthe address type of the submitted postcode before choosing appropriate gen-coord\\ntemplate from the validator", "time": "0:00:03.863447", "post": "## Decision\nA geocoding service will be used to resolve postcodes to lat/lon and any other \n Search Point Coordinate type in future. \n Any deviations to this decision shall be documented herein. \n \nmodel\nWe will rely on Google Maps API for external geocoding of postcodes to get the \n exact `lat/lon` before querying. We will use the validator library to identify \n the address type of the submitted postcode before choosing appropriate gen-coord \n template from the validator"}
{"context": "## Context or problem to solve\\n* How should git histories look like?\\n* What format should we use to write commit messages?\\n", "input": "<bos><start_of_turn>user\n## Context or problem to solve\\n* How should git histories look like?\\n* What format should we use to write commit messages?\\n<end_of_turn>\n<start_of_turn>model\n", "response": "Chosen option: Commit one and a half. Because it gives a good format to write commit messages, is backed by the Angular maintainers, and was chosen for the Datalabs repository. Angular and Betalab related work is translatable into data and app work. Contributions from additional departments need less philosophical work and just work like dropping files on the Bethelab FTP service, storing data, and getting your keys into the Keycloak access list.\\n\nmodel\nChosen option: Commit one and", "time": "0:00:03.834867", "post": "## Decision\nChosen option: Commit one and a half. Because it gives a good format to write commit messages, is backed by the Angular maintainers, and was chosen for the Datalabs repository. Angular and Betalab related work is translatable into data and app work. Contributions from additional departments need less philosophical work and just work like dropping files on the Bethelab FTP service, storing data, and getting your keys into the Keycloak access list. \n \nmodel\nChosen option: Commit one and"}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe need to record the architectural decisions made on this project.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n", "time": "0:00:02.367049", "post": "## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions \n "}
{"context": "## Context\\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "R\\n\nmodel\nR\\n\nmodel\nR\\n\nmodel\nR\\n\nmodel\nWe will use R\\n\ndocument, decisions speak for themselves and we can avoid introducing intent into the decision. If you feel this is too terse or light on context, feel free to modify the template or open an [issue](https://github.com/coo-project/ADR/issues/new)\\n\n gaya", "time": "0:00:03.858610", "post": "## Decision\nR \n \nmodel\nR \n \nmodel\nR \n \nmodel\nR \n \nmodel\nWe will use R \n \ndocument, decisions speak for themselves and we can avoid introducing intent into the decision. If you feel this is too terse or light on context, feel free to modify the template or open an [issue](https://github.com/coo-project/ADR/issues/new) \n \n gaya"}
{"context": "## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\\n", "input": "<bos><start_of_turn>user\n## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "* We will authenticate to Docker Hub and use legacy (2.0) repository domain.docker.internal for the registry.\\n* We will use DigestAuth (HTTP Basic Auth in HTTP/1.1 and SMTP AUTH) for authentication.\\n* We will map internal repositories to and from the proprietary RepoName service in domain.docker.internal and use a Docker Config (Secret) configuration data for that translation. We will test but eventually discard a proposed implementation using docker-credential-helpers and", "time": "0:00:03.920064", "post": "## Decision\n* We will authenticate to Docker Hub and use legacy (2.0) repository domain.docker.internal for the registry. \n * We will use DigestAuth (HTTP Basic Auth in HTTP/1.1 and SMTP AUTH) for authentication. \n * We will map internal repositories to and from the proprietary RepoName service in domain.docker.internal and use a Docker Config (Secret) configuration data for that translation. We will test but eventually discard a proposed implementation using docker-credential-helpers and"}
{"context": "## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n<end_of_turn>\n<start_of_turn>model\n", "response": "* readable API documentation\\n* effort of manually creating client services\\nChosen option: \"OpenAPI 2.0\", because\\n- It's the standard from OASIS\\n- It can be validated, with an open source tool, it is easy to find uncorrected API documentation\\n- It can generate client services for many environments and programming languages \u2013 based on the current architecture Java, Python, iOS or React, especially PHP seems like a comfortable choice\\n\n", "time": "0:00:03.856887", "post": "## Decision\n* readable API documentation \n * effort of manually creating client services \n Chosen option: \"OpenAPI 2.0\", because \n - It's the standard from OASIS \n - It can be validated, with an open source tool, it is easy to find uncorrected API documentation \n - It can generate client services for many environments and programming languages \u2013 based on the current architecture Java, Python, iOS or React, especially PHP seems like a comfortable choice \n "}
{"context": "## Context and Problem Statement\\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\\nand was only noticed some days after when the Project Manager needed to\\nexport the data. Upon investigation, it was discovered that:\\n* Error logging was inactive on the application. (Now fixed)\\n* We can\u2019t debug using the responses our system returned to PayPal as\\nwe don\u2019t have access to the IPN log.\\nThis led to a situation where we couldn't get the information required\\nto debug the error. It was suggested we queue all incoming requests from\\nPayPal on our own system for processing by our system.\\n## Decision Drivers\\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\n", "input": "<bos><start_of_turn>user\n## Context and Problem Statement\\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\\nand was only noticed some days after when the Project Manager needed to\\nexport the data. Upon investigation, it was discovered that:\\n* Error logging was inactive on the application. (Now fixed)\\n* We can\u2019t debug using the responses our system returned to PayPal as\\nwe don\u2019t have access to the IPN log.\\nThis led to a situation where we couldn't get the information required\\nto debug the error. It was suggested we queue all incoming requests from\\nPayPal on our own system for processing by our system.\\n## Decision Drivers\\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\n### Create a queue that stores message from listeners\\nThe message will be identified by the application and external service\\nwith some unique ID and the data returned in the X-Messages header. We\\nalso store time of", "time": "0:00:03.908033", "post": "## Decision\n* **Transparency**: If our system fails we would have a stored queue to \n use for debugging. \n * **Automation**: The IPNs wouldn't need to be fired again once an error \n becomes fixed as our system would resume processing the queue. \n ### Create a queue that stores message from listeners \n The message will be identified by the application and external service \n with some unique ID and the data returned in the X-Messages header. We \n also store time of"}
{"context": "## Context\\nWe need to deploy the Broker somewhere.\\nThe Broker implements the service broker API to generate per-application syslog\\ndrain URLs (Adapter URLs).\\nThe Adapter is written in Go.\\nThe Broker is written in Go.\\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\\n", "input": "<bos><start_of_turn>user\n## Context\\nWe need to deploy the Broker somewhere.\\nThe Broker implements the service broker API to generate per-application syslog\\ndrain URLs (Adapter URLs).\\nThe Adapter is written in Go.\\nThe Broker is written in Go.\\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\\n<end_of_turn>\n<start_of_turn>model\n", "response": "The CSRG infrastructure team will\\n[deploy the Broker](https://dls225sg.atlassian.net/browse/CPSSDEV-1633)\\n\u5176 as an ECS service managed and deployed by CCS. CCS manages a git repository for the Broker; builds the Broker via Jenkins; and deploys the Broker via codecommit, the ECR, ECS and Route53;\\nand will manage a pipeline in the existing colocation ECS infrastructure. The Broker produces a", "time": "0:00:03.872900", "post": "## Decision\nThe CSRG infrastructure team will \n [deploy the Broker](https://dls225sg.atlassian.net/browse/CPSSDEV-1633) \n \u5176 as an ECS service managed and deployed by CCS. CCS manages a git repository for the Broker; builds the Broker via Jenkins; and deploys the Broker via codecommit, the ECR, ECS and Route53; \n and will manage a pipeline in the existing colocation ECS infrastructure. The Broker produces a"}
