{"context": "## Context and Problem Statement\\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\\n## Decision Drivers <!-- optional -->\\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nIn non-trivial projects with complex business rules, it is often a challenge to come up with the model that scales well\\nfor reads and writes at the same time in regard to performance and maintainability.\\n\n## Decision\nWe will adopt Command Query Responsibility Segregation architectural style where there may exist 2 models for same\\ndomain entities each respectively on the Command (write) and Query (read) sides\\n\n\n## Context and Problem Statement\\nIn order to maintain the serverless nature of the project, a DynamoDB table was\\nchosen as main persistence mechanism, which means that a schedule is primarily\\nstored in DynamoDB and then distributed to other components, which in turn\\ngoes back to DynamoDB to update state. So, DynamoDB holds both state and\\nhistorical data.\\nThe problem here is that both ~~warmer~~ `task-1minute-enqueuer` and ~~poller~~ `task-1minute-sqs2sns` will concur by Dynamo resources and probably will be throttled (it's easy to reproduce this behavior only by setting Dynamo's read and write capacity to 1 and trying to send some hundreds of schedules while some other are ~~moving from *WARM* state~~ being enqueued in delayer queue).\\n## Decision Drivers\\n*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\n\n## Decision\n*   Solution must kept as simple as possible\\n*   Even it could delay the problem, increase read and write capacity of\\nDynamoDB is not an architectural solution\\nTake the decision for the use of the DynamoDB introduced a new concept for the entire architecture: the layered concern.\\nThe `delayer-aws` solution aims to provide a way to schedule future operations reliably. It's not part of this system store or ingest or even present information about these schedules. In this sense, the use of DynamoDB is needed only because there's a need of store schedules that could not be posted in delayer queue, and there's only 2 options for those records: or they are in the delayer queue, or they're not. That's why the \"state\" field is needed, but it will not hold the *entire* lifecycle of a schedule.\\nWith this in mind, we realize that all 3 options will be considered, but in different contexts:\\n-   Present data of scheduler is not `delayer-aws`'s concern, but it will be needed. So all the data events should be published by `delayer-aws` to be consumed by another \"view\" platform - this is a kind of *event driven approach*.\\n-   In this sense, if another system will ingest all of this published data, state and historical data will be stored in different persistent stores;\\n### How it should work\\nWhen a schedule is inserted, if their timeframe was greater then the timeframe of the delayer queue, the schedule is stored in DynamoDB with a *currentStatus* marked as `NEW` and a TTL of 1 hour after schedule time.\\nWhen `task-1minute-enqueuer` runs and finds this scheduler, it will be updated to `PROCESSED`. After that, no more updates are made in DynamoDB.\\nFor each event that occurs with this schedule, a message should be posted in the correspondent topic.\\n### Don't use state as a field in database\\nThe state of a schedule is noted by currentStatus/currentState attribute in\\nDynamoDB. The proposal of this optional is to treat state as part of the\\ngeneral flow of application:\\n1.  When scheduled, it's COLD\\n1.  When *warmer* puts it on SQS, it's WARM\\n1.  When *poller* publishes it on SNS is DOING\\n1.  When *actions* execute\\n*   Good, simplified update policy\\n*   Good, reduced DynamoDB costs\\n*   Bad, no information about past events\\n\n\n## Context\\nThe issue motivatina# [short title of solved problem and solution]\\nWhat is the issue that we're seeing that is motivating this decision or change?\\n* Status: [proposed | rejected | accepted | deprecated | ... | superseded by [ADR-0005](0005-example.md)] <!-- optional -->\\n* Deciders: [list everyone involved in the decision] <!-- optional -->\\n* Date: [YYYY-MM-DD when the decision was last updated] <!-- optional -->\\nTechnical Story: [description | ticket/issue URL] <!-- optional -->\\n## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\n\n## Decision\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | ... | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, ...]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, ...]\\n* ...\\nWhat is the change that we're proposing and/or doing?\\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context and Problem Statement\\nServices currently have a static network of dependencies. This makes it impossible to have a service that depends on a SQL based database use a different database that is compatible with the service.\\n## Decision Drivers\\n* Should fit in the current domain model\\n* Easy to add afterwards\\n* Should support existing services with their static dependencies\\n\n## Decision\n* Should fit in the current domain model\\n* Easy to add afterwards\\n* Should support existing services with their static dependencies\\nTo be decided.\\nThis adr purely documents possible decisions for this problem.\\n\n\n## Context and Problem Statement\\nWe need to store data internally in a structured way to gain performance.\\n## Decision Drivers\\n* Easy to integrate\\n* Easy to use\\n* Common technology\\n\n## Decision\n* Easy to integrate\\n* Easy to use\\n* Common technology\\nChosen option: \"H2 Database Engine\", because it was straight-forward to use.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\\n## Decision Drivers <!-- optional -->\\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\n\n## Decision\n", "response": "## Decision\nTo address the problem of concurrent changes to data in the Volley Management system, we will adopt the Optimistic Concurrency Control (OCC) strategy. \n\nChosen option: \"Optimistic Concurrency Control\", because it supports high throughput scenarios while being less intrusive in terms of code complexity as compared to Pessimistic Concurrency Control. OCC allows reads and writes to proceed without locking resources, only checking for conflicts at the time of commit, thus aligning well with the decision drivers of performance and maintainability.\n\n### Positive Consequences\n* **Performance**: High throughput is maintained as the system avoids locking mechanisms that could bottleneck operations.\n* **Maintainability**: The code complexity is reduced since OCC typically involves less boilerplate and is managed largely by the database management system.\n* **Scalability**: It scales well in distributed systems where network latency might otherwise cause issues with locking mechanisms.\n\n### Negative Consequences\n* **Conflict Resolution**: Additional logic may be needed to handle conflicts that are detected at commit time, with potential retries increasing latency in specific cases.\n* **Increased complexity**: Some complexity may shift to handling retries and managing application logic to gracefully handle concurrency violations.", "matched_ids": [1031, 2791, 4800, 4690, 4747], "time": "0:00:17.937924", "input_tokens": 1676, "output_tokens": 241}
{"context": "## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https://www.postgresql.org/docs/9.5/static/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https://www.postgresql.org/docs/9.5/static/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need to provide tenants with the ability to provision databases for use in\\ntheir applications. Our first iteration of this will be using RDS.\\nWe investigated some implementations of a service broker which supported RDS\\n- [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\\n- [18F](https://github.com/18F/rds-service-broker)\\n\n## Decision\nWe will use the [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\\nrds broker. As this is not a supported product, we will fork this and maintain\\nthis and implement new features ourselves.\\n\n\n## Context\\nDigiDeps uses a Postgres database to store persistent information. Since the project was first set up, we have been using an Amazon RDS hosted instance of Postgres for this. However, this hosting option lacks scalability so we have to pay for a full database host 24/7 for each environment.\\nWe have several environments which do not need to be highly available. This includes \"ephemeral\" development environments, the \"main\" environment used in CI, and the training environment. We do not need to run a database for these environments outside of working hours, and often inside of them too.\\n\n## Decision\nWe will use Amazon Aurora Serverless for environments which do not need to always be on. Aurora automatically scales with usage, including pausing completely if the database isn't in use.\\n\n\n## Context\\nWe need to decide whether to run PostgreSQL on the Puppetmaster itself or use\\nRDS from the off.\\nCurrently our Puppetmaster uses a local PostgreSQL instance as its database. In moving to AWS we have the opportunity to replace this with an RDS instance.\\nThis may remove a reasonable amount of puppet code used to configure the database and associated tasks (e.g. backups). It would also require an amount of Terraform work to provision the RDS instance and some updates to the Puppetmaster code to use that instance.\\n\n## Decision\nThe Puppetmaster will continue to use a local instance of PostgreSQL.\\n\n\n## Context\\nOur service operator will be responsible for providing Postgres along any other\\nservices to our end users.\\nIn this case, the ask is specifically Postgres, meaning we can pick our\\nsolution for providing the instance as long as it exposes the correct APIs.\\nA reasonable set of candidates are:\\n- AWS RDS Postgres\\n- AWS RDS Aurora Postgres\\n### AWS RDS Postgres\\nA solution most comonly used across the board for databases. Is managed and\\nmaintained by AWS, provides Backups and Snapshots.\\n### AWS RDS Aurora Postgres\\nHas the benefits of AWS RDS Postgres, and some more benefits. Such as:\\n- Scalable persistance storage\\n- IAM authentication\\n- Automatic Failover with multi AZs\\n- Faster read loads\\n- Slightly cheaper\\nAlso has few downsides:\\n- Not recommended for heavy write systems (Twitter big)\\n- Slightly behind version wise\\n\n## Decision\nWe will continue with Aurora as we we don't have any specific requirements not\\nto and can benefit from the solution.\\n\n\n## Context\\nCurrently we are using a traditional RDBMS, PostgreSQL as the storage of all the CI/CD state.\\nThe DB is the one and only state of the whole CI/CD cluster, storing all of the pipeline, resource, artifact definitions, runs, logs etc. For all of these, tracking historical changes is of utmost importance.\\nUsing a traditional CRUD workflow with PostgeSQL raises the following issues:\\n- Analytics are really hard as the Storage and Querying is coupled and a heavy query slows the whole cluster down\\n- Central locking is a severe impediment to scale\\n- When a change occurs in the pipeline, resource provider or artifact store definition its quite difficult to track these changes across time for audit and rollback needs\\n- To debug failures we need to have extended historical logging and is difficult to trace back in an update-in place system\\n- The tabular structure comes in our way with rigidity and is not simple to introduce schema changes\\n- Need of elaborate migration strategies which is further complicated with distributed state\\n\n## Decision\nBased on the above facts the following is decided:\\n- Use [XTDB](https://xtdb.com) as the temporal, document store for the following reasons:\\n- Though being newer compared to [Datomic](https://www.datomic.com/) its free and open source and ready for production use\\n- Has a quite unbundled design and uses a variety of storage backends and is transactor free\\n- Is [bi-temporal](https://xtdb.com/articles/bitemporality.html) and thereby offering more systematic analytical queries across time\\n- Has an HTTP interface for non JVM clients\\n- Use it with JDBC/PostgreSQL backend which is quite readily available and managed in all popular cloud providers\\n- Remove the CRUD way of doing things, expose the DB too via API for more powerful, direct analytical querying\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https://www.postgresql.org/docs/9.5/static/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https://www.postgresql.org/docs/9.5/static/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n\n## Decision\n", "response": "## Decision\n\nWe will implement a role-based access control (RBAC) scheme in PostgreSQL to manage permissions more effectively across applications bound to tenant databases. Our approach will consist of the following steps:\n\n1. **Create a Shared Role for Ownership**: We will create a shared role (e.g., `db_shared_owner`) that will own all tables, sequences, and functions. All actions such as table creation and modification will be executed under this role. This role will be the single point of truth for table ownership, facilitating easier management of permissions.\n\n2. **Grant Required Privileges**: New users will be granted membership to this shared role. This way, they inherit the necessary rights to `ALTER`, `DROP`, or `MODIFY` the tables without depending on individual ownership.\n\n3. **Manage Privileges via Scripts**: Since RDS does not support DDL event triggers, we will handle ownership transfers using scripts executed periodically or triggered by deployment pipelines to reassess and transfer ownership of new database objects to the `db_shared_owner` role.\n\n4. **Standardized Deployment Process**: Enforce a standardized deployment process for applications where all schema changes are made using a predefined role associated with `db_shared_owner`.\n\n5. **Custom Schemas**: Ensure that any application-specific requirements for custom schemas are designed to utilize the `db_shared_owner` role, guaranteeing consistency and avoiding privilege issues.\n\nThis RBAC approach provides more flexibility and maintains secure, scalable permissions management across different applications, ensuring stable database operations and consistent access control.", "matched_ids": [186, 4411, 4061, 3897, 4285], "time": "0:00:06.751239", "input_tokens": 2077, "output_tokens": 310}
{"context": "## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context and Problem Statement\\nWe want to deploy our application in docker containers that can be easily updated\\n\n## Decision\nChosen option: \"OpenShift\", because\\n* Built on Kubernetes.\\nThe bank has experience on it.\\nProvides a lot of added value tools for CI/CD, automated builds.\\nIs supported by RedHat and we have a great support contract for it.\\n\n\n## Context\\nThe application needs to be deployed somewhere.\\nThere are many container orchestration platforms for configuring a cluster of machines running Docker containers and managing the deployment of images on to these containers. Setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines.\\nHeroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https://devcenter.heroku.com/articles/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.\\nHeroku has several pricing tiers for machines that the application will run on, including a free tier.\\nHeroku provides a free hosted PostgreSQL option. It will handle setting a \"DATABASE_URL\" environment variable, containing the information required to connect to this database. The free tier database is limited to 10,000 rows.\\nWe want the setup process to be as simple as possible.\\n\n## Decision\nWe will host the application on Heroku, using their Container Registry solution with a hosted PostgreSQL database.\\n\n\n## Context\\nWe need a way for the service to automatically and regularly run a task.\\nWe already have Redis available as our caching layer and Sidekiq works great with it.\\nAn alternative could be to use Cron for scheduled tasks and a postgres backed asynchronous jobs, perhaps even run inline. We know how to get Sidekiq running with Docker and reusing Redis (rather than Postgres) for job data that is ephemeral feels a better fit given we already have Redis.\\n\n## Decision\nUse Sidekiq for processing asynchronous tasks.\\n\n\n## Context\\nDigiDeps uses a Postgres database to store persistent information. Since the project was first set up, we have been using an Amazon RDS hosted instance of Postgres for this. However, this hosting option lacks scalability so we have to pay for a full database host 24/7 for each environment.\\nWe have several environments which do not need to be highly available. This includes \"ephemeral\" development environments, the \"main\" environment used in CI, and the training environment. We do not need to run a database for these environments outside of working hours, and often inside of them too.\\n\n## Decision\nWe will use Amazon Aurora Serverless for environments which do not need to always be on. Aurora automatically scales with usage, including pausing completely if the database isn't in use.\\n\n\n## Context\\nOur service operator will be responsible for providing Postgres along any other\\nservices to our end users.\\nIn this case, the ask is specifically Postgres, meaning we can pick our\\nsolution for providing the instance as long as it exposes the correct APIs.\\nA reasonable set of candidates are:\\n- AWS RDS Postgres\\n- AWS RDS Aurora Postgres\\n### AWS RDS Postgres\\nA solution most comonly used across the board for databases. Is managed and\\nmaintained by AWS, provides Backups and Snapshots.\\n### AWS RDS Aurora Postgres\\nHas the benefits of AWS RDS Postgres, and some more benefits. Such as:\\n- Scalable persistance storage\\n- IAM authentication\\n- Automatic Failover with multi AZs\\n- Faster read loads\\n- Slightly cheaper\\nAlso has few downsides:\\n- Not recommended for heavy write systems (Twitter big)\\n- Slightly behind version wise\\n\n## Decision\nWe will continue with Aurora as we we don't have any specific requirements not\\nto and can benefit from the solution.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n\n## Decision\n", "response": "## Decision\nWe will set up a Docker Compose configuration to manage PostgreSQL lifecycle for service tests, because\\n* Docker Compose allows for easy management of containerized services, making it ideal for ensuring PostgreSQL is up and running when needed.\\n* It provides a simple way to define and run multi-container applications, making it easy to manage dependencies.\\n* We can configure it to automatically start the PostgreSQL container when running tests.\\n* This approach helps maintain consistency across different development environments, ensuring everyone uses the same PostgreSQL setup.\\n", "matched_ids": [4096, 718, 1260, 4411, 3897], "time": "0:00:04.002877", "input_tokens": 958, "output_tokens": 109}
{"context": "## Context\\nWe need a system to manage the lifecycle of a grant application.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need a workflow to build our appliation and components.\\n\n## Decision\nWe use Storybook only for building new self contained components.\\n\n\n## Context\\nIn the coming months we will be building this application. The first order of business is to come up with a sensible data model and application structure.\\n\n## Decision\nWe discussed a database model that is based around a small number of tables with a lot of columns. In particular:\\n- `Candidate`, primarily used for authentication\\n- `ApplicationForm`, which belongs to a Candidate, and holds the\\ninformation that is common to all of the applications that the user\\nmakes (in the \"Apply 1\" phase, they can apply for up to three courses).\\n- `ApplicationChoice`, which holds the information that is course specific,\\nlike the status of the application and the personal statement. It holds a reference to the \"course choice\", which is the combination of course, provider and the training location.\\nThere will be some other models that hold references, work histories and qualifications.\\nThe eventual table structure will look something like this:\\n![data model](0003-initial-datamodel.png)\\n\n\n## Context\\nBased on ADRs and developer need, we need to create a system that is light weight, fully managed and to integrate into.\\n\n## Decision\nTo run in a fully managed AWS cloud environment using Terraform to manage it.\\n\n\n## Context\\nWe need a database in order to store the data provided by the user in the different forms.\\n\n## Decision\nTo reduce the amount of integrations and complexity, we've decided to use one of the databases that Firebase offers.\\nUnder the Firebase umbrella are Real-time database and Firestore which are similar in many features.\\nGoogle recommends Firestore for new developers, as it is built upon \u201cthe successes of Real-time database\u201d. Also, for our purposes, Firestore is a better choice because:\\n- It offers offline support for iOS, Android and Web clients whereas real-time DB supports only iOS and Android clients.\\n- The one-big-JSON-tree structure of real-time can become difficult to maintain as the DB grows. Document-based Firestore offers better organisation provisions using collections.\\n- Scaling on firestore is automatic whereas scaling on real-time database requires sharding.\\n\n\n## Context\\nWe need to have the mechanism for supporting and validating common architecture principles in all application.\\n\n## Decision\nArchitecture tests with using Archunit should be implemented.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe need a system to manage the lifecycle of a grant application.\\n\n## Decision\n", "response": "## Decision\nWe have decided to implement a grant management system using a modular software architecture. This system will consist of the following components:\\n- `ApplicationSubmission`, which provides a user interface for applicants to submit their grant proposals and required documents.\\n- `ApplicationReview`, which facilitates the review process by allowing reviewers to access, evaluate, and score applications.\\n- `GrantAwarding`, which proceeds with the awarding process and generates award notifications and agreements for successful applicants.\\n- `FinancialTracking`, which monitors disbursements and budgets for each awarded grant.\\n- `ReportingDashboard`, which offers an integrated view of key performance indicators and outcomes for grant programs.\\nThese components will interact through RESTful APIs to ensure maintainability and flexibility. For persistence, a relational database such as PostgreSQL will be used to support complex queries and maintain data integrity.", "matched_ids": [3722, 4021, 1545, 2488, 2502], "time": "0:00:09.997348", "input_tokens": 626, "output_tokens": 173}
{"context": "## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nABCI-server could expose its own RPC-server and act as a proxy to Tendermint.\\nThe idea was for the Tendermint RPC to just be a transparent proxy to the app.\\nClients need to talk to Tendermint for proofs, unless we burden all app devs\\nwith exposing Tendermint proof stuff. Also seems less complex to lock down one\\nserver than two, but granted it makes querying a bit more kludgy since it needs\\nto be passed as a `Query`. Also, **having a very standard rpc interface means\\nthe light-client can work with all apps and handle proofs**. The only\\napp-specific logic is decoding the binary data to a more readable form (eg.\\njson). This is a huge advantage for code-reuse and standardization.\\n\n## Decision\nWe dont expose an RPC server on any of our ABCI-apps.\\n\n\n## Context\\nCurrently the Tendermint ABCI application hash is equal to the consensus state\\nroot for a specific height. In order to allow additional uses, like proving to\\nlight clients that specific events have been emitted in a block, we should make\\nthe application hash be derivable from potentially different kinds of roots.\\n\n## Decision\nThe proposed design is to derive the Tendermint ABCI application hash by hashing\\nall the different roots as follows:\\n```\\nAppHash := H(Context || Root_0 || ... || Root_n)\\n```\\nWhere:\\n- `H` is the SHA-512/256 hash function.\\n- `Context` is the string `oasis-core/tendermint: roots`.\\n- `Root_i` is the fixed-size SHA-512/256 root hash of the specified root.\\nCurrently, the only root would be the existing consensus state root at index 0.\\nTo implement this change the following modifications would be required:\\n- Update the ABCI multiplexer's `Commit` method to calculate and return the\\napplication hash using the scheme specified above.\\n- Update the consensus API `SignedHeader` response to include the\\n`UntrustedStateRoot` (the untrusted prefix denotes that the user must verify\\nthat the state root corresponds to `AppHash` provided in the signed header in\\n`Meta`).\\nWhen new roots will be added in the future, both `Block` and `SignedHeader`\\nwill need to include them all.\\n\n\n## Context\\nCurrently, the messages exchanged between tendermint and a (potentially remote) signer/validator,\\nnamely votes, proposals, and heartbeats, are encoded as a JSON string\\n(e.g., via `Vote.SignBytes(...)`) and then\\nsigned . JSON encoding is sub-optimal for both, hardware wallets\\nand for usage in ethereum smart contracts. Both is laid down in detail in [issue#1622].\\nAlso, there are currently no differences between sign-request and -replies. Also, there is no possibility\\nfor a remote signer to include an error code or message in case something went wrong.\\nThe messages exchanged between tendermint and a remote signer currently live in\\n[privval/socket.go] and encapsulate the corresponding types in [types].\\n[privval/socket.go]: https://github.com/tendermint/tendermint/blob/d419fffe18531317c28c29a292ad7d253f6cafdf/privval/socket.go#L496-L502\\n[issue#1622]: https://github.com/tendermint/tendermint/issues/1622\\n[types]: https://github.com/tendermint/tendermint/tree/master/types\\n\n## Decision\n- restructure vote, proposal, and heartbeat such that their encoding is easily parseable by\\nhardware devices and smart contracts using a  binary encoding format ([amino] in this case)\\n- split up the messages exchanged between tendermint and remote signers into requests and\\nresponses (see details below)\\n- include an error type in responses\\n### Overview\\n```\\n+--------------+                      +----------------+\\n|              |     SignXRequest     |                |\\n|Remote signer |<---------------------+  tendermint    |\\n| (e.g. KMS)   |                      |                |\\n|              +--------------------->|                |\\n+--------------+    SignedXReply      +----------------+\\nSignXRequest {\\nx: X\\n}\\nSignedXReply {\\nx: X\\nsig: Signature // []byte\\nerr: Error{\\ncode: int\\ndesc: string\\n}\\n}\\n```\\nTODO: Alternatively, the type `X` might directly include the signature. A lot of places expect a vote with a\\nsignature and do not necessarily deal with \"Replies\".\\nStill exploring what would work best here.\\nThis would look like (exemplified using X = Vote):\\n```\\nVote {\\n// all fields besides signature\\n}\\nSignedVote {\\nVote Vote\\nSignature []byte\\n}\\nSignVoteRequest {\\nVote Vote\\n}\\nSignedVoteReply {\\nVote SignedVote\\nErr  Error\\n}\\n```\\n**Note:** There was a related discussion around including a fingerprint of, or, the whole public-key\\ninto each sign-request to tell the signer which corresponding private-key to\\nuse to sign the message. This is particularly relevant in the context of the KMS\\nbut is currently not considered in this ADR.\\n[amino]: https://github.com/tendermint/go-amino/\\n### Vote\\nAs explained in [issue#1622] `Vote` will be changed to contain the following fields\\n(notation in protobuf-like syntax for easy readability):\\n```proto\\n// vanilla protobuf / amino encoded\\nmessage Vote {\\nVersion       fixed32\\nHeight        sfixed64\\nRound         sfixed32\\nVoteType      fixed32\\nTimestamp     Timestamp         // << using protobuf definition\\nBlockID       BlockID           // << as already defined\\nChainID       string            // at the end because length could vary a lot\\n}\\n// this is an amino registered type; like currently privval.SignVoteMsg:\\n// registered with \"tendermint/socketpv/SignVoteRequest\"\\nmessage SignVoteRequest {\\nVote vote\\n}\\n//  amino registered type\\n// registered with \"tendermint/socketpv/SignedVoteReply\"\\nmessage SignedVoteReply {\\nVote      Vote\\nSignature Signature\\nErr       Error\\n}\\n// we will use this type everywhere below\\nmessage Error {\\nType        uint  // error code\\nDescription string  // optional description\\n}\\n```\\nThe `ChainID` gets moved into the vote message directly. Previously, it was injected\\nusing the [Signable] interface method `SignBytes(chainID string) []byte`. Also, the\\nsignature won't be included directly, only in the corresponding `SignedVoteReply` message.\\n[Signable]: https://github.com/tendermint/tendermint/blob/d419fffe18531317c28c29a292ad7d253f6cafdf/types/signable.go#L9-L11\\n### Proposal\\n```proto\\n// vanilla protobuf / amino encoded\\nmessage Proposal {\\nHeight            sfixed64\\nRound             sfixed32\\nTimestamp         Timestamp         // << using protobuf definition\\nBlockPartsHeader  PartSetHeader     // as already defined\\nPOLRound          sfixed32\\nPOLBlockID        BlockID           // << as already defined\\n}\\n// amino registered with \"tendermint/socketpv/SignProposalRequest\"\\nmessage SignProposalRequest {\\nProposal proposal\\n}\\n// amino registered with \"tendermint/socketpv/SignProposalReply\"\\nmessage SignProposalReply {\\nProp   Proposal\\nSig    Signature\\nErr    Error     // as defined above\\n}\\n```\\n### Heartbeat\\n**TODO**: clarify if heartbeat also needs a fixed offset and update the fields accordingly:\\n```proto\\nmessage Heartbeat {\\nValidatorAddress Address\\nValidatorIndex   int\\nHeight           int64\\nRound            int\\nSequence         int\\n}\\n// amino registered with \"tendermint/socketpv/SignHeartbeatRequest\"\\nmessage SignHeartbeatRequest {\\nHb Heartbeat\\n}\\n// amino registered with \"tendermint/socketpv/SignHeartbeatReply\"\\nmessage SignHeartbeatReply {\\nHb     Heartbeat\\nSig    Signature\\nErr    Error     // as defined above\\n}\\n```\\n\n\n## Context\\nWe must standardize our method for encoding public keys and signatures on chain.\\nCurrently we amino encode the public keys and signatures.\\nThe reason we are using amino here is primarily due to ease of support in\\nparsing for other languages.\\nWe don't need its upgradability properties in cryptosystems, as a change in\\nthe crypto that requires adapting the encoding, likely warrants being deemed\\na new cryptosystem.\\n(I.e. using new public parameters)\\n\n## Decision\n### Public keys\\nFor public keys, we will continue to use amino encoding on the canonical\\nrepresentation of the pubkey.\\n(Canonical as defined by the cryptosystem itself)\\nThis has two significant drawbacks.\\nAmino encoding is less space-efficient, due to requiring support for upgradability.\\nAmino encoding support requires forking protobuf and adding this new interface support\\noption in the language of choice.\\nThe reason for continuing to use amino however is that people can create code\\nmore easily in languages that already have an up to date amino library.\\nIt is possible that this will change in the future, if it is deemed that\\nrequiring amino for interacting with Tendermint cryptography is unnecessary.\\nThe arguments for space efficiency here are refuted on the basis that there are\\nfar more egregious wastages of space in the SDK.\\nThe space requirement of the public keys doesn't cause many problems beyond\\nincreasing the space attached to each validator / account.\\nThe alternative to using amino here would be for us to create an enum type.\\nSwitching to just an enum type is worthy of investigation post-launch.\\nFor reference, part of amino encoding interfaces is basically a 4 byte enum\\ntype definition.\\nEnum types would just change that 4 bytes to be a variant, and it would remove\\nthe protobuf overhead, but it would be hard to integrate into the existing API.\\n### Signatures\\nSignatures should be switched to be `[]byte`.\\nSpatial efficiency in the signatures is quite important,\\nas it directly affects the gas cost of every transaction,\\nand the throughput of the chain.\\nSignatures don't need to encode what type they are for (unlike public keys)\\nsince public keys must already be known.\\nTherefore we can validate the signature without needing to encode its type.\\nWhen placed in state, signatures will still be amino encoded, but it will be the\\nprimitive type `[]byte` getting encoded.\\n#### Ed25519\\nUse the canonical representation for signatures.\\n#### Secp256k1\\nThere isn't a clear canonical representation here.\\nSignatures have two elements `r,s`.\\nThese bytes are encoded as `r || s`, where `r` and `s` are both exactly\\n32 bytes long, encoded big-endian.\\nThis is basically Ethereum's encoding, but without the leading recovery bit.\\n\n\n## Context\\nOne of the biggest challenges this project faces is to proof that the\\nimplementations of the specifications are correct, much like we strive to\\nformaly verify our alogrithms and protocols we should work towards high\\nconfidence about the correctness of our program code. One of those is the core\\nof Tendermint - Consensus - which currently resides in the `consensus` package.\\nOver time there has been high friction making changes to the package due to the\\nalgorithm being scattered in a side-effectful container (the current\\n`ConsensusState`). In order to test the algorithm a large object-graph needs to\\nbe set up and even than the non-deterministic parts of the container makes will\\nprevent high certainty. Where ideally we have a 1-to-1 representation of the\\n[spec](https://github.com/tendermint/spec), ready and easy to test for domain\\nexperts.\\nAddresses:\\n- [#1495](https://github.com/tendermint/tendermint/issues/1495)\\n- [#1692](https://github.com/tendermint/tendermint/issues/1692)\\n\n## Decision\nTo remedy these issues we plan a gradual, non-invasive refactoring of the\\n`consensus` package. Starting of by isolating the consensus alogrithm into\\na pure function and a finite state machine to address the most pressuring issue\\nof lack of confidence. Doing so while leaving the rest of the package in tact\\nand have follow-up optional changes to improve the sepration of concerns.\\n### Implementation changes\\nThe core of Consensus can be modelled as a function with clear defined inputs:\\n* `State` - data container for current round, height, etc.\\n* `Event`- significant events in the network\\nproducing clear outputs;\\n* `State` - updated input\\n* `Message` - signal what actions to perform\\n```go\\ntype Event int\\nconst (\\nEventUnknown Event = iota\\nEventProposal\\nMajority23PrevotesBlock\\nMajority23PrecommitBlock\\nMajority23PrevotesAny\\nMajority23PrecommitAny\\nTimeoutNewRound\\nTimeoutPropose\\nTimeoutPrevotes\\nTimeoutPrecommit\\n)\\ntype Message int\\nconst (\\nMeesageUnknown Message = iota\\nMessageProposal\\nMessageVotes\\nMessageDecision\\n)\\ntype State struct {\\nheight      uint64\\nround       uint64\\nstep        uint64\\nlockedValue interface{} // TODO: Define proper type.\\nlockedRound interface{} // TODO: Define proper type.\\nvalidValue  interface{} // TODO: Define proper type.\\nvalidRound  interface{} // TODO: Define proper type.\\n// From the original notes: valid(v)\\nvalid       interface{} // TODO: Define proper type.\\n// From the original notes: proposer(h, r)\\nproposer    interface{} // TODO: Define proper type.\\n}\\nfunc Consensus(Event, State) (State, Message) {\\n// Consolidate implementation.\\n}\\n```\\nTracking of relevant information to feed `Event` into the function and act on\\nthe output is left to the `ConsensusExecutor` (formerly `ConsensusState`).\\nBenefits for testing surfacing nicely as testing for a sequence of events\\nagainst algorithm could be as simple as the following example:\\n``` go\\nfunc TestConsensusXXX(t *testing.T) {\\ntype expected struct {\\nmessage Message\\nstate   State\\n}\\n// Setup order of events, initial state and expectation.\\nvar (\\nevents = []struct {\\nevent Event\\nwant  expected\\n}{\\n// ...\\n}\\nstate = State{\\n// ...\\n}\\n)\\nfor _, e := range events {\\nsate, msg = Consensus(e.event, state)\\n// Test message expectation.\\nif msg != e.want.message {\\nt.Fatalf(\"have %v, want %v\", msg, e.want.message)\\n}\\n// Test state expectation.\\nif !reflect.DeepEqual(state, e.want.state) {\\nt.Fatalf(\"have %v, want %v\", state, e.want.state)\\n}\\n}\\n}\\n```\\nHeight           int64\\nRound            int\\nBlockID          BlockID\\n}\\ntype TriggerTimeout struct {\\nHeight           int64\\nRound            int\\nDuration         Duration\\n}\\ntype RoundStep int\\nconst (\\nRoundStepUnknown RoundStep = iota\\nRoundStepPropose\\nRoundStepPrevote\\nRoundStepPrecommit\\nRoundStepCommit\\n)\\ntype State struct {\\nHeight           int64\\nRound            int\\nStep             RoundStep\\nLockedValue      BlockID\\nLockedRound      int\\nValidValue       BlockID\\nValidRound       int\\nValidatorId      int\\nValidatorSetSize int\\n}\\nfunc proposer(height int64, round int) int {}\\nfunc getValue() BlockID {}\\nfunc Consensus(event Event, state State) (State, Message, TriggerTimeout) {\\nmsg = nil\\ntimeout = nil\\nswitch event := event.(type) {\\ncase EventNewHeight:\\nif event.Height > state.Height {\\nstate.Height = event.Height\\nstate.Round = -1\\nstate.Step = RoundStepPropose\\nstate.LockedValue = nil\\nstate.LockedRound = -1\\nstate.ValidValue = nil\\nstate.ValidRound = -1\\nstate.ValidatorId = event.ValidatorId\\n}\\nreturn state, msg, timeout\\ncase EventNewRound:\\nif event.Height == state.Height and event.Round > state.Round {\\nstate.Round = eventRound\\nstate.Step = RoundStepPropose\\nif proposer(state.Height, state.Round) == state.ValidatorId {\\nproposal = state.ValidValue\\nif proposal == nil {\\nproposal = getValue()\\n}\\nmsg =  MessageProposal { state.Height, state.Round, proposal, state.ValidRound }\\n}\\ntimeout = TriggerTimeout { state.Height, state.Round, timeoutPropose(state.Round) }\\n}\\nreturn state, msg, timeout\\ncase EventProposal:\\nif event.Height == state.Height and event.Round == state.Round and\\nevent.Sender == proposal(state.Height, state.Round) and state.Step == RoundStepPropose {\\nif event.POLRound >= state.LockedRound or event.BlockID == state.BlockID or state.LockedRound == -1 {\\nmsg = MessageVote { state.Height, state.Round, event.BlockID, Prevote }\\n}\\nstate.Step = RoundStepPrevote\\n}\\nreturn state, msg, timeout\\ncase TimeoutPropose:\\nif event.Height == state.Height and event.Round == state.Round and state.Step == RoundStepPropose {\\nmsg = MessageVote { state.Height, state.Round, nil, Prevote }\\nstate.Step = RoundStepPrevote\\n}\\nreturn state, msg, timeout\\ncase Majority23PrevotesBlock:\\nif event.Height == state.Height and event.Round == state.Round and state.Step >= RoundStepPrevote and event.Round > state.ValidRound {\\nstate.ValidRound = event.Round\\nstate.ValidValue = event.BlockID\\nif state.Step == RoundStepPrevote {\\nstate.LockedRound = event.Round\\nstate.LockedValue = event.BlockID\\nmsg = MessageVote { state.Height, state.Round, event.BlockID, Precommit }\\nstate.Step = RoundStepPrecommit\\n}\\n}\\nreturn state, msg, timeout\\ncase Majority23PrevotesAny:\\nif event.Height == state.Height and event.Round == state.Round and state.Step == RoundStepPrevote {\\ntimeout = TriggerTimeout { state.Height, state.Round, timeoutPrevote(state.Round) }\\n}\\nreturn state, msg, timeout\\ncase TimeoutPrevote:\\nif event.Height == state.Height and event.Round == state.Round and state.Step == RoundStepPrevote {\\nmsg = MessageVote { state.Height, state.Round, nil, Precommit }\\nstate.Step = RoundStepPrecommit\\n}\\nreturn state, msg, timeout\\ncase Majority23PrecommitBlock:\\nif event.Height == state.Height {\\nstate.Step = RoundStepCommit\\nstate.LockedValue = event.BlockID\\n}\\nreturn state, msg, timeout\\ncase Majority23PrecommitAny:\\nif event.Height == state.Height and event.Round == state.Round {\\ntimeout = TriggerTimeout { state.Height, state.Round, timeoutPrecommit(state.Round) }\\n}\\nreturn state, msg, timeout\\ncase TimeoutPrecommit:\\nif event.Height == state.Height and event.Round == state.Round {\\nstate.Round = state.Round + 1\\n}\\nreturn state, msg, timeout\\n}\\n}\\nfunc ConsensusExecutor() {\\nproposal = nil\\nvotes = HeightVoteSet { Height: 1 }\\nstate = State {\\nHeight:       1\\nRound:        0\\nStep:         RoundStepPropose\\nLockedValue:  nil\\nLockedRound:  -1\\nValidValue:   nil\\nValidRound:   -1\\n}\\nevent = EventNewHeight {1, id}\\nstate, msg, timeout = Consensus(event, state)\\nevent = EventNewRound {state.Height, 0}\\nstate, msg, timeout = Consensus(event, state)\\nif msg != nil {\\nsend msg\\n}\\nif timeout != nil {\\ntrigger timeout\\n}\\nfor {\\nselect {\\ncase message := <- msgCh:\\nswitch msg := message.(type) {\\ncase MessageProposal:\\ncase MessageVote:\\nif msg.Height == state.Height {\\nnewVote = votes.AddVote(msg)\\nif newVote {\\nswitch msg.Type {\\ncase Prevote:\\nprevotes = votes.Prevotes(msg.Round)\\nif prevotes.WeakCertificate() and msg.Round > state.Round {\\nevent = EventNewRound { msg.Height, msg.Round }\\nstate, msg, timeout = Consensus(event, state)\\nstate = handleStateChange(state, msg, timeout)\\n}\\nif blockID, ok = prevotes.TwoThirdsMajority(); ok and blockID != nil {\\nif msg.Round == state.Round and hasBlock(blockID) {\\nevent = Majority23PrevotesBlock { msg.Height, msg.Round, blockID }\\nstate, msg, timeout = Consensus(event, state)\\nstate = handleStateChange(state, msg, timeout)\\n}\\nif proposal != nil and proposal.POLRound == msg.Round and hasBlock(blockID) {\\nevent = EventProposal {\\nHeight: state.Height\\nRound:  state.Round\\nBlockID: blockID\\nPOLRound: proposal.POLRound\\nSender: message.Sender\\n}\\nstate, msg, timeout = Consensus(event, state)\\nstate = handleStateChange(state, msg, timeout)\\n}\\n}\\nif prevotes.HasTwoThirdsAny() and msg.Round == state.Round {\\nevent = Majority23PrevotesAny { msg.Height, msg.Round, blockID }\\nstate, msg, timeout = Consensus(event, state)\\nstate = handleStateChange(state, msg, timeout)\\n}\\ncase Precommit:\\n}\\n}\\n}\\ncase timeout := <- timeoutCh:\\ncase block := <- blockCh:\\n}\\n}\\n}\\nfunc handleStateChange(state, msg, timeout) State {\\nif state.Step == Commit {\\nstate = ExecuteBlock(state.LockedValue)\\n}\\nif msg != nil {\\nsend msg\\n}\\nif timeout != nil {\\ntrigger timeout\\n}\\n}\\n```\\n### Implementation roadmap\\n* implement proposed implementation\\n* replace currently scattered calls in `ConsensusState` with calls to the new\\n`Consensus` function\\n* rename `ConsensusState` to `ConsensusExecutor` to avoid confusion\\n* propose design for improved separation and clear information flow between\\n`ConsensusExecutor` and `ConsensusReactor`\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n\n## Decision\n", "response": "## Decision\n\nIntroduce a new `ProofOperator` interface to handle Merkle proofs in the `abci.ResponseQuery`. By encapsulating proof logic within this interface, we achieve the following:\n\n1. **Layering Proofs**: Each `ProofOperator` will manage a specific type of Merkle proof, allowing seamless layering without the need for repeated decoding and reencoding.\n\n2. **Chaining Proofs**: `ProofOperator` instances can be combined in a sequence to create complex, multi-layered proof structures, supporting a flexible proof verification process.\n\n3. **Extensibility**: Define a clear protocol for creating and registering custom `ProofOperator` types. This ensures that third-party developers can introduce their own proof formats to accommodate diverse use cases.\n\n### Implementation Details\n\n- **ProofOperator Interface**: Define methods like `Apply` and `Verify` that participants must implement, ensuring consistency and interoperability between different proof formats.\n\n- **Integration with `abci.ResponseQuery`**: Adapt the ResponseQuery mechanism to accept a list of `ProofOperator` instances, enabling the flexible management of proof layers.\n\n- **Documentation and Examples**: Provide comprehensive documentation and examples on how to implement custom `ProofOperator` types and how to integrate them within an application's proof verification flow.\n\nBy implementing this approach, we enhance the flexibility, scalability, and extensibility of Merkle proof handling in our system, paving the way for more sophisticated proof architectures.", "matched_ids": [1985, 4354, 4097, 1962, 1967], "time": "0:00:06.676415", "input_tokens": 5108, "output_tokens": 291}
{"context": "## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need to describe an approach to authentication that supports the use of multiple authentication providers. Azure AD provides the ability to configure direct federation with a range of authentication providers and includes the ability to provide managed authorisation for users without the ability to authenticate with Azure AD\\n- https://docs.microsoft.com/en-us/azure/active-directory/b2b/delegate-invitations\\n\n## Decision\nWe will choose an authentication provider/platform that supports the federation of authentication.\\n\n\n## Context\\nWe need to allow a number of users to sign in to the service in order to use it.\\nIn order to implement this quickly, we'll use Auth0 to manage this.\\nAs Auth0's authentication uses OAuth2, it should be straightforward to migrate\\nto another service, if BEIS have a preference for something else.\\nAuth0 provides views for authentication that we can use in our user journeys.\\nThere are two versions of these views 'Classic' and 'New'.\\nClassic uses JavaScript and is not progressively enhanced. New uses language\\nthat results in a poor user experience.\\n\n## Decision\nWe will use the free tier and 'Classic' views of Auth0 for the private beta.\\n\n\n## Context\\nAt the moment and for the short term future Hackney do not have a clear candidate for a single sign-on provider. They are reviewing the situation and will decide on a future solution before the end of 2019.\\nWe have so far been using Basic Authentication.\\nAs with other decisions we have attempted to follow the path set by the Repairs Hub which is another Hackney Rails app that is more mature. We asked what they were using and the answer was Microsoft Azure Active Directory using OpenID Connect (OAuth2). We believed we could get the exact same provisioning when the time came for Report a Defect to integrate with a single sign-on provider, however when the time came we learnt that it wasn't supported by Hackney and should be thought of as experimental.\\nAs we had 2 weeks left we discussed with Hackney Architect Keith Gatt the possibility of using Auth0 as a temporary option to enable us to meet the user need of providing agent names within the service, as well as security needs through the use of a logged authentication service that provided one set of credentials per person, rather than a single set for basic auth.\\n\n## Decision\nUse Auth0 as the single sign-on provider, and remove basic auth from production.\\n\n\n## Context\\nOur user facing applications will need to authenticate users.\\nIn the long-term, we want to use the standard user sign-on services in CCS for\\neach appropriate user group.\\nFor suppliers, this is the [Supplier Registration Service (SRS)][service-srs].\\nFor CCS staff it is their G-Suite account.\\nUsing a single authentication service will reduce administration overheads\\nbecause we won't have to manage a JML (Joiners Movers Leavers) process. For\\nusers it will also mean they don't have to remember yet another username and\\npassword combination.\\nHowever, we want to get moving quickly, and integration with SRS/CCS G-Suite\\nwill take time. For now, we want a simple sign-in service that we can integrate\\nwith quickly, to enable us to test and iterate the application.\\n\n## Decision\nWe will use [Auth0][service-auth0] for authentication on a temporary basis.\\nAuth0 is an authentication service that uses the OAuth protocol. It provides\\nand simple integration path, and is free for several thousand active users.\\nWe acknowledge that we are incurring technical debt by not integrating\\nimmediately with SRS and G-Suite, however we believe that this will let us move\\nmore quickly during development.\\nThis decision will also allow us to gain a better understanding of the user\\nneeds around authentication before a decision around the final authentication\\nsolution is made.\\nWe will replace Auth0 with the proper authentication service later in the\\ndevelopment process.\\n\n\n## Context\\nWe need an identity provider for our platform. This could be a host (SaaS) solution vs a self hosted solution. While the Cost & Lockin is higher at scale with a SaaS the self hosted has a higher Management Effort & Less Security & Expensive Bootstraping. From an evolutionary architecture perspective it is not core to value generation and does not change rapidly[source](https://www.youtube.com/watch?v=8bEsNT7jdC4&t=112s&index=57&list=WL) - identity will mostlikly the same (OpenIDConnect, Login page ...) but it will be hard to change in the future if you decided for one provider.\\n\n## Decision\nWe will use a SaaS solution as we don't have the skill to host our own solution in the efficient quality. And from a evolutionary architecture perspective the system is commondity or support.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n\n## Decision\n", "response": "## Decision\nWe will use Azure Active Directory for authentication and single sign-on for the frontend application. By leveraging Azure's identity management system, we can ensure seamless integration with existing SEPA internal account management, enhancing security and usability through a unified login experience. This decision aligns with our goal to maximize vendor reuse and interoperability with the Azure Cloud Services and Microsoft Dynamics 365.", "matched_ids": [1207, 2405, 5211, 2041, 3421], "time": "0:00:03.952674", "input_tokens": 1262, "output_tokens": 74}
{"context": "## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nEnsure good code quality with ease for change, integration and error correction.\\n\n## Decision\nIt will be used the [PHPUnit](https://phpunit.de/) as testing framework.\\n\n\n## Context\\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\\n\n## Decision\nWe will use [Standard.rb](https://github.com/testdouble/standard) and run the standard.rb rake task to lint the code as part of the test suite.\\n\n\n## Context\\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\\n\n## Decision\nWe will use [Standard.rb](https://github.com/testdouble/standard) and run the standard.rb rake task to lint the code as part of the test suite.\\n\n\n## Context\\nWe need to make sure we adhere to our own designated code quality best practices.\\n\n## Decision\nUse a code analysis tool (specifically, [credo](http://credo-ci.org/) for Elixir\\nand [prettier](https://prettier.io/) for JavaScript).\\n\n\n## Context\\nIt is easy to use different terms meaning the same thing in code. A common aphorism is that the most difficult thing in coding is naming.\\n\n## Decision\nA `Terminology.md` document will be created, that will be used to collect the official terms for the project. If naming diverts from this terminology, it should be considerd a bug.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n\n## Decision\n", "response": "## Decision\nWe will adopt the use of clear and consistent code documentation practices including inline comments where necessary, a comprehensive README at the repository level, and detailed architectural diagrams for complex systems. In addition, we will employ [Doxygen](http://www.doxygen.nl/) for generating documentation from annotated code and [Structurizr](https://structurizr.com/) for creating visual architecture diagrams to assist in understanding the overall structure of applications and systems.", "matched_ids": [5070, 2382, 3386, 3853, 2270], "time": "0:00:03.779292", "input_tokens": 530, "output_tokens": 90}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n", "response": "## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n", "matched_ids": [4699, 4428, 4408, 3119, 1436], "time": "0:00:02.900530", "input_tokens": 410, "output_tokens": 42}
{"context": "## Context\\nThe rig defined at [Bookit Infrastructure](https://github.com/buildit/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\\nThe only way to capture that is via branches which can be hard to discover.\\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nAWS Bare Metal rig gives you the choice between EC2 hosting or FARGATE for compute.\\n\n## Decision\nFor the Twig riglet, we will use FARGATE.  Primary driver for this decision is to have a reference the uses FARGATE instead of EC2, and we are in the process of updating the Twig riglet.\\n\n\n## Context\\nWe need to create a riglet for our new bookit project so that we practice what we preach.\\n\n## Decision\nWe will use the AWS Bare Metal Riglet from bookit-riglet as a starting point for our riglet.  We will keep the previous bookit-riglet and create a new bookit-infrastructure project/repo.\\nTechnologies:\\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\\n* Deployment Mechanism: Docker images\\n* Build: Travis\\n\n\n## Context\\nNeed of using Infrastructure as code to, track what resources are running in the AWS account\\n\n## Decision\nUse Terraform\\n\n\n## Context\\nTwig has been running on an older riglet flavor.  There is a desire to move to a newer/simpler riglet flavor\\nand put newer approaches to the test.\\n\n## Decision\n* Use AWS CodePipeline and CodeBuild instead of Jenkins, at least in the beginning.\\n* We will aim to create a new Pipeline/Build and potentially execution environment per branch.\\n* This will be manual at first and later could be automated via webhooks and lambda functions\\n\n\n## Context\\nA fully configured EKS cluster requires many AWS resources and a lot of configuration. Defining each resource in our own Terraform module maximises flexibility but also requires a significant level of effort. We could instead make use of the [existing Terraform registry EKS module](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest) to optimise for speed of delivery.\\n\n## Decision\nAdopt the [existing Terraform registry EKS module](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest).\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nThe rig defined at [Bookit Infrastructure](https://github.com/buildit/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\\nThe only way to capture that is via branches which can be hard to discover.\\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\\n\n## Decision\n", "response": "## Decision\nEstablish a centralized AWS Bare Metal Rig repository that serves as the canonical source for all the latest features and configurations. This will involve:\n\n1. **Modularization**: Split the rig into modular components (Compute, RDS, Networking, etc.) that can be individually versioned and updated.\n\n2. **Branch Strategy**: Implement a clear branch strategy for feature development and releases. Utilize tags or branches for various configuration options (such as ECS EC2, Fargate, EKS) to provide clarity and discoverability.\n\n3. **Documentation**: Create comprehensive documentation for the repository, detailing options and configurations available, how to instantiate a new rig instance, and the process for contributing updates.\n\n4. **CI/CD Integration**: Set up continuous integration and delivery pipelines to automate testing and validation of different configurations. This will ensure that all branches and features are kept up to date and are working as expected.\n\n5. **Community Contributions**: Encourage contributions from various project teams who use the AWS Bare Metal Rig, to incorporate their updates and improvements into the central repository.\n\n6. **Release Management**: Regularly merge stable and tested feature branches into a `main` or `release` branch, representing the latest recommended version of the rig.\n\nBy adopting this approach, we aim to streamline the instantiation of AWS Bare Metal Rigs for new projects, ensuring they benefit from the latest configurations and best practices.", "matched_ids": [348, 1738, 1464, 350, 3610], "time": "0:00:07.601971", "input_tokens": 842, "output_tokens": 287}
{"context": "## Context\\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\\nPHP [appears to be on a downwards trend as a language](https://pypl.github.io/PYPL.html?country=GB),\\nespecially in contrast with Python. It's likely it will become increasingly difficult\\nto find good PHP developers in future.\\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\\nlanguages of similar pedigree, such as Python.\\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\\nA mix of contractors and developers working on the code base over several years has\\nresulted in a mix of styles and approaches. While we have already cleared out a lot\\nof unused and/or broken code, there is likely to be more we haven't found yet.\\nWe are also lagging behind the latest Design System guidelines, as our application was one\\nof the first to go live, before the current iteration of the Design System existed.\\nThis means that any changes to design have to be done piecemeal and manually: we can't\\nsimply import the newest version of the design system and have everything magically update.\\nThis combination of factors means that the code base can be difficult to work with:\\nresistant to change and easy to break.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\n[context]: #context\\nThe current frontend is in a bad state.\\nThe following issues need to be solved to change the situation:\\n- **Maintainability**\\nDue to the untyped programming language JavaScript most types of code refactoring are risky to realize.\\nThis gets emphasized by external dependencies that easily can break the web application.\\nOften a simple dependency update leads to multiple hours of fighting the dependency hell.\\nUnfortunately a handful of bugs don't occur during the refactoring process but during the\\nruntime (because JS can't check correctness at compile time).\\n- **Complexity**\\nThe current complexity of the codebase is grown due to a lot of dirty quick fixes.\\nThis not only makes it hard for the community to contribute to the project\\nbut it also makes it costly to introduce new features.\\n- **Tests**\\nThere are few tests but most of the code is untested.\\nThis makes it even harder to refactor and maintain the project.\\n- **File size**\\nThe packed JavaScript bundle size was 5.3 MB some time ago.\\nIn the meanwhile its shrunken to 1.5 MB but that's still a bunch of bytes that\\nneeds to be downloaded at once.\\n\n## Decision\n[decision]: #decision\\nWe will port the whole frontend to [Rust](https://rust-lang.org). The migration\\ncan be divided into the following stages:\\n- Port the existing admin views to WASM as the first use case. All new admin views\\nwill be implemented with the new technology. This first stage serves as test balloon\\nto verify the feasibility of the plan.\\n- Create a read-only view of the regular Kvm front-end that allows to browse\\nthe map as well as searching and selecting entries.\\n- Complete the front-end by addint the editing functionality, i.e. to create and\\nmodify entries. This may include includes extension like additional actions for\\nlogged in users depending on their role.\\nThe result will be a [WASM](https://en.wikipedia.org/wiki/WebAssembly) module.\\nWe made this decision based on following reasons.\\nWe expect to achieve\\n- a much better **maintainability** because:\\n- Rust is statically typed\\n- refactoring is easy (the compiler does a very good job)\\n- the package manager [Cargo](https://doc.rust-lang.org/stable/cargo/) provides\\na reliable dependency management\\n- most mistakes will popup at compile-time\\n- a reduced **complexity** because we can get rid of most legacy code\\n- a more efficient **testing** because we can focus on business logic\\n(instead of testing trivial things like it is required in JavaScript)\\n- a mobile friendly **file size** because the app is packed in a binary format\\n(instead of text like it is with JavaScript)\\n- a better API integration because we can **share** modules with the backend (OpenFairDB)\\nthat is also written in Rust.\\n\n\n## Context\\nWhen developing a new system, there are many possible language choices available.\\n-CCS uses a mixture of different languages for existing systems including:\\n- PHP for the Supplier Registration Service and Contracts Finder\\n- C# for MISO\\n- Java for the CCS ID and DigiTS services\\n- Python for the Digital Marketplace\\nOther services in government use a mixture of PHP, Java, Python, Ruby, Scala and GO.\\nPicking one standard language for all CCS services could restrict the pool of potential suppliers we could interact with when building new services. Conversely, allowing an unlimited selection of languages can make future management and support of services more complex and expensive.\\nAgreeing a selection of acceptable languages will allow the greatest flexibility in supplier choice, but also helps minimise future ongoing support costs.\\nAny decision we make about languages and frameworks should consider:\\n- Skills available in the current team(s) - what are the current team(s) comfortable using? What do they have experience in?\\n- Skills of a future team - is there a large enough pool of suppliers and contractors who could support and maintain the services in the future?\\n- External toolkits and modules - are there useful toolkits and modules which could reduce development effort (eg the GOV.UK Frontend Toolkit)\\n- Cost - what is the cost of developing in this way? Are there licence costs?\\n- Hosting - would picking a language restrict hosting options?\\nWe should also consider the Service Manual guide on [choosing technology](https://www.gov.uk/service-manual/technology/choosing-technology-an-introduction).\\n\n## Decision\nNew services developed by CCS should use the following languages:\\n- Java\\n- Python\\n- Ruby\\n- Javascript\\nDigital services should also use HTML5, CSS3 and JSON as required.\\nSalesforce work may continue to use the Salesforce Apex language, but where possible, we should use external web service components to reduce the volume of Apex in use.\\nNew services which do not use these languages will require an Architectural Decision to explain why.\\nAs the software development industry changes quickly, we will re-evaluate this decision in 6 months, taking account of current market trends and industry moves.\\nThis decision does not apply to services comprised entirely of Software as a Service (SaaS) solutions, and does not affect systems which already exist. It also does not cover the development of native mobile applications.\\n\n\n## Context\\nCode base is becomes too complicated.\\nMaintaining and supporting multiple API versions is painful and expensive.\\n\n## Decision\nWe will avoid API versioning.\\nWe will not introduce breaking changes.\\n\n\n## Context and Problem Statement\\nThe original Fundraising Application was a classic web-based Multi-Page-Application with some JavaScript for progressive enhancement and state stored on the Server. The 2016 rewrite introduced some state on the client. The new skin we introduced in 2019 renders the template variables in an HTML data attribute, but the client-side markup is rendered with Vue. It is a \"hybrid\" Multi-Page-Application, where the client-side code still depends on the server side variables, but the server-side \"templating\" is obsolete.\\nThe current architecture has the following drawbacks:\\n* Switching pages does a full page reload\\n* Our naive client-side \"subpage\" mechanism breaks the browser history (see https://phabricator.wikimedia.org/T285046 )\\n* Components shared across pages need to be compiled into the entry point\\nfor each page.\\n* We need \"Full Stack\" Developers who are familiar with backend\\ntechnologies (PHP, Symfony, databases, PHP CI tools, Docker) and frontend technologies\\n(TypeScript, Vue, SASS, bundler and CI tools).\\n## Decision Drivers\\n* User experience - fast page load times, browser history, immediate feedback of\\nwhat's happening, keep focus on current task.\\n* Developer experience (ease-of-use, fewest dependencies possible)\\n\n## Decision\n* User experience - fast page load times, browser history, immediate feedback of\\nwhat's happening, keep focus on current task.\\n* Developer experience (ease-of-use, fewest dependencies possible)\\nA separation into an API and Single-Page-Application looks like the best\\noption, regarding our decision drivers:\\n* It's improving on an already good user experience.\\n* It won't add complexity for developers beyond the status quo\\n* It's \"open\" enough to improve our technology stack, we don't tie\\nourselves to a specific library.\\nA Server-Rendered Multi-Page-Application with progressive enhancement\\nlooks like a promising architecture for the far future. We will keep\\nobserving the available technologies.\\n\n\n## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n\n## Decision\nI will always focus on simplicity, readaility and the ease of understanding\\nany code or structure.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\\nPHP [appears to be on a downwards trend as a language](https://pypl.github.io/PYPL.html?country=GB),\\nespecially in contrast with Python. It's likely it will become increasingly difficult\\nto find good PHP developers in future.\\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\\nlanguages of similar pedigree, such as Python.\\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\\nA mix of contractors and developers working on the code base over several years has\\nresulted in a mix of styles and approaches. While we have already cleared out a lot\\nof unused and/or broken code, there is likely to be more we haven't found yet.\\nWe are also lagging behind the latest Design System guidelines, as our application was one\\nof the first to go live, before the current iteration of the Design System existed.\\nThis means that any changes to design have to be done piecemeal and manually: we can't\\nsimply import the newest version of the design system and have everything magically update.\\nThis combination of factors means that the code base can be difficult to work with:\\nresistant to change and easy to break.\\n\n## Decision\n", "response": "## Decision\nWe will gradually refactor and migrate the legacy PHP codebase to a more modern stack. Our chosen path involves the following steps:\n\n1. **Refactor Codebase**: Initially refactor the existing PHP code to improve consistency, remove unused or broken components, and enhance maintainability. This step involves introducing coding standards, establishing a clear code style, and implementing automated testing practices.\n\n2. **Migrate to Python**: Begin migrating sections of the PHP application to Python, starting with modules that are standalone and least dependent on the core logic. Python's rising popularity and cleaner syntax will not only attract more developers but will also facilitate easier integration with modern tools and libraries.\n\n3. **Introduce a Modular Architecture**: Adopt a modular architecture to replace monolithic components. This will help in isolating features, making the migration process and future updates easier, while providing options for microservices and containerization in the future.\n\n4. **Incrementally Update the Design System**: Gradually transition the front-end components to align with the latest Design System guidelines. This process should involve systematic updates, starting with high-impact areas to ensure that immediate user experiences are improved.\n\n5. **Leverage Modern Libraries and Frameworks**: Use modern libraries and frameworks that are well-supported and actively maintained to facilitate the migration and enhancement process. Consider frameworks such as Django or Flask for back-end development in Python.\n\nThis decision was made considering the long-term viability, increased developer satisfaction, maintainability, and ability to keep pace with technological evolution. Regular reviews will be conducted as the migration progresses to ensure alignment with project goals and to adapt to any new opportunities in the development landscape.", "matched_ids": [1872, 1000, 3534, 1524, 1416], "time": "0:00:07.194563", "input_tokens": 2361, "output_tokens": 333}
{"context": "## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need a way to send analytics data to Ophan (internal analytics analysis tool). This includes at its MVP level, screen views and events. This needs to work across iOS and Android.\\n\n## Decision\n```sequence\\nOphan Analytics Library->Multiplatform Ophan: Thrift interface\\nMultiplatform Ophan->Native React Native Layer: Native iOS and Android modules\\nNative React Native Layer->Typescript Layer: Type definitions to mimic modules\\nTypescript Layer->App Code: Consume! \ud83c\udf54\\nApp Code-->Ophan Analytics Library: Sends events\\n```\\n### Key\\n**Ophan Analytics Library** - This is the current library that provides an interface to log events in Ophan.\\n**Multplatform Ophran** - Written in Kotlin, this uses the Ophan Analytics Library via its Thrift interface and exposes a subset of methods required by the apps.\\n**Native React Native Layer** - Takes the module created by Multiplatform Ophan and makes it \"React Nativey\" to expose methods to the JS.\\n**Typescript Layer** - Takes the React Native JS methods exposed and provides type definitions for them. This is what the App will use and it means we can capture errors at build time.\\n**App Code** - Consumes the typescript library where its needed.\\n\n\n## Context\\nWe need an application platform that meets the following requirements:\\n- Cross-platform: The app may be used to enter data on phone or tablet in real time or through a computer at a later stage.\\n- Works offline: Given the environmental conditions on a boat in the middle of the ocean, consistent internet connection is not a given. Thus, our tech stack must provide offline capabilities with minimal-to-no data loss.\\n### Option 1 - Cache First Web App (PWA)\\nBy creating a web app that meets certain criteria, the app will be downloaded to cache on first use. This means that the app will be accessible offline. This option would also allow us to create a single application which could be used on any device with a modern web browser.\\n### Option 2 - Native application\\nAnother option would be to develop a native application for each platform which may be used (iOS, macOS, Android, Windows). These apps could be developed more rapidly using something like React Native which would allow some components to be shared. This method would give us greater control and potentially give a better user experience.\\n\n## Decision\nThe decision was made to create a web app with offline capability due to the flexibility and speed it offers. Creating native applications for each of the required platforms would not be feasible with the available time and resources.\\n### PWA Issues\\nSince selecting a web app as the platform for this project a number of issues have been discovered:\\n- Updating the app to the latest version is inconsistent between browsers and devices.\\n- Refreshing while offline may show a 'No internet connection' page until the app is closed and reopened.\\n- Offline behaviour on iOS in particular seems inconsistent with other platforms.\\n\n\n## Context\\nLike so many other accessibility tools, such as the [Accessibility Developer Tools by Google](https://github.com/GoogleChrome/accessibility-developer-tools), [aXe by Deque](https://github.com/dequelabs/axe-core), and [HTML_CodeSniffer by Squiz](https://github.com/squizlabs/HTML_CodeSniffer) to name a few, our proprietary accessibility conformance testing engine at Siteimprove runs within the context of a browser. The reason why this seems to be the de facto way of implementing an accessibility tool is obvious: The browser is the tool used to consume your website, so why not test directly within that very tool? Through the APIs exposed by the browser, we get access to all the information needed in order to assess the accessibility of a website; the structure we can access and inspect through the DOM, information about styling can be gained through the CSSOM, and soon we also get our hands on a standardised accessibility tree through the [AOM](https://wicg.github.io/aom/).\\nHowever, not all is good in the land of browsers. Rendering a website is an inherently non-deterministic process and the timing of network requests, script execution, the content of request headers, and much more, all play a role in what the final result will look like. In most cases, this will directly affect the assessment of a tool that runs within the browser and will become very apparent at scale. At Siteimprove, we feel the effect of this on a daily basis; a customer asking us why we came up with a certain result and us having little to no clue because we cannot replicate the exact circumstances that led to that result. This is a frustrating experience for both our customers and ourselves as it makes it difficult to reason about our tool.\\nWe want to fix this and we want to fix it for good. To do so, we must ensure that we have the ability to exactly replicate the results of a given accessibility assessment. Ideally, as many unknown browser variables as possible should be taken out of the equation and the browser only be used for what is absolutely necessary.\\n\n## Decision\nWe will abandon any sort of dynamic analysis within the context of a browser. The input to Alfa will be static data and any assessment must be made based on that data alone. A browser may or may not be involved in the construction of the data, but the browser will not be required for any further assessment thereof.\\nIf additional data is needed by a given accessibility rule, we will adjust the data format to meet the needs of the rule. We will also carefully consider the extent of the data format as to not bloat it with information that could otherwise be inferred from existing data. Ideally, the size of the data when serialised and stored on disk will not be much larger than the size of the original source code on which the data is based.\\n\n\n## Context\\nWhile signals/front-end is a React web app, the client wants to provide it to mobile app users as well.\\nFurthermore, some changes are requested specifically for the 'app version':\\n- The application header should be hidden\\n- The button 'Doe een melding' should be replaced with a 'Sluit venster' button which closes the window\\n- The possibility to initialize the map center location\\nThe above requirements are described in more detail in [Jira](https://datapunt.atlassian.net/browse/SIG-3933).\\nAt this moment, rebuilding the frontend in a native app architecture is not possible due to time and financial constraints.\\n\n## Decision\nAdd a feature flag `appMode` to the application configuration, and deploy the application with `appMode=true` to a separate domain.\\nUsing the feature flag, the application conditionally applies the app-specific requirements.\\nThe mobile app embeds the web app (e.g. using [React Native WebView](https://github.com/react-native-webview/react-native-webview)),\\nand provides additional information via query parameters (e.g. map coordinates).\\nConversely, communication from the web-app to the mobile app occurs via the `postMessage()` API, but its use should be limited.\\nBecause an embedded application cannot close itself, the `postMessage()` can be used to notify the app that it should be closed.\\n\n\n## Context\\n- We need to keep a record of activities taken by users so that we can gather quantitative research data\\n- We need to avoid third-party tracking services, since these carry with them privacy concerns, lack of clarity around retention rules, additional user consent, and the need for additional approvals\\n\n## Decision\nStore all records of activities taken by users in-app.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n\n## Decision\n", "response": "## Decision\nDue to the limited support for the PointerEvents API in Safari and iOS, we will implement a hybrid event handling approach by using both the PointerEvents API and the traditional MouseEvents API. On platforms where PointerEvents API functions correctly, we will leverage it for a unified event handling mechanism. However, for Safari and iOS where support is lacking, we will fallback to using the MouseEvents API combined with manual calculation of movement distance by tracking the previous coordinates. This will ensure compatibility across all browsers and platforms while still providing a robust solution for capturing event data on desktop environments where it is primarily utilized. For future-proofing, we will also periodically reassess the support status for PointerEvents API in Safari to potentially revert to a cleaner implementation once full support is achieved.", "matched_ids": [669, 2490, 3156, 3051, 1257], "time": "0:00:04.842922", "input_tokens": 1876, "output_tokens": 156}
{"context": "## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nAs we are building out v7 of the CLI, we need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need a simple way to manage our package version.\\n\n## Decision\nWe use versioneer to do this for us.\\n\n\n## Context\\nThis record descibes how to safely release new dapr binaries and the corresponding configurations without any blockers to users.\\n\n## Decision\n### Integration build release\\nIntegration build refers to the build from `master` branch once we merge PullRequest to master branch. This build will be used for development purposes and must not be released to users and impact their environments.\\n### Official build release\\n#### Pre-release build\\nPre-release build will be built from `release-<major>.<minor>` branch and versioned by git version tag suffix e.g. `-alpha.0`, `-alpha.1`, etc. This build is not released to users who use the latest stable version.\\n**Pre-release process**\\n1. Create branch `release-<major>.<minor>` from master and push the branch. e.g. `release-0.1`\\n2. Add pre-release version tag(with suffix -alpha.0 e.g. v0.1.0-alpha.0) and push the tag\\n```\\n$ git tag \"v0.1.0-alpha.0\" -m \"v0.1.0-alpha.0\"\\n$ git push --tags\\n```\\n3. CI creates new build and push the images with only version tag\\n4. Test and validate the functionalities with the specific version\\n5. If there are regressions and bugs, fix them in release-* branch and merge back to master\\n6. Create new pre-release version tag(with suffix -alpha.1, -alpha.2, etc)\\n7. Repeat from 4 to 6 until all bugs are fixed\\n#### Release the stable version to users\\nOnce all bugs are fixed, we will create the release note under [./docs/release_notes](https://github.com/dapr/dapr/tree/master/docs/release_notes) and run CI release manually in order to deliver the stable version to users.\\n### Release Patch version\\nWe will work on the existing `release-<major>.<minor>` branch to release patch version. Once all bugs are fixed, we will add new patch version tag, such as `v0.1.1-alpha.0`, and then release the build manually.\\n\n\n## Context\\nThis ADR presents a technical plan for creation of a 2.0 version of edgex-cli which supports the new V2 REST APIs developed as part of the Ireland release of EdgeX.\\n\n## Decision\n1. Use standardized command-line args/flags\\n| Argument/Flag      | Description |\\n| ----------- | ----------- |\\n| `-d`, `--debug`      | show additional output for debugging purposes (e.g. REST URL, request JSON, \u2026). This command-line arg will replace -v, --verbose and will no longer trigger output of the response JSON (see -j, --json).       |\\n| `-j`, `--json`   | output the raw JSON response returned by the EdgeX REST API and *nothing* else. This output mode is used for script-based usage of the client.    |\\n| `--version`   | output the version of the client and if available, the version of EdgeX installed on the system (using the version of the metadata data service)   |\\n2. Restructure the Go code hierarchy to follow the [most recent recommended guidelines](https://github.com/golang-standards/project-layout). For instance /cmd should just contain the main application for the project, not an implementation for each command - that should be in /internal/cmd\\n3. Take full advantage of the features of the underlying command-line library, [Cobra](https://github.com/spf13/cobra), such as tab-completion of commands.\\n4. Allow overlap of command names across services by supporting an argument to specify the service to use: `-m/--metadata`, `-c/--command`, `-n/--notification`, `-s/--scheduler` or `--data` (which is the default). Examples:\\n- `edgex-cli ping --data`\\n- `edgex-cli ping -m`\\n- `edgex-cli version -c`\\n5. Implement all required V2 endpoints for core services\\n**Core Command**\\n- **`edgex-cli command`** `read | write | list`\\n**Core Data**\\n- **`edgex-cli event`** `add | count | list | rm | scrub**`\\n- **`edgex-cli reading`** `count | list`\\n**Metadata**\\n- **`edgex-cli device`**  `add | adminstate | list | operstate | rm | update`\\n- **`edgex-cli deviceprofile`**  `add | list | rm | update`\\n- **`edgex-cli deviceservice`** ` add | list | rm | update`\\n- **`edgex-cli provisionwatcher`**  `add | list | rm | update`\\n**Support Notifications**\\n- **`edgex-cli notification`** `add | list | rm`\\n- **`edgex-cli subscription`** `add | list | rm`\\n**Support Scheduler**\\n- **`edgex-cli interval`** `add | list | rm | update`\\n**Common endpoints in all services**\\n- **`edgex-cli version`**\\n- **`edgex-cli ping`**\\n- **`edgex-cli metrics`**\\n- **`edgex-cli status`**\\nThe commands will support arguments as appropriate. For instance:\\n- `event list` using `/event/all` to return all events\\n- `event list --device {name}` using `/event/device/name/{name}` to return the events sourced from the specified device.\\n6.  Currently, some commands default to always displaying GUIDs in objects when they're not really needed. Change this so that by default GUIDs aren't displayed, but add a flag which causes them to be displayed.\\n7. **scrub** may not work with Redis being secured by default. That might also apply to the top-level `db` command (used to wipe the entire db). If so, then the commands will be disabled in secure mode, but permitted in non-secure mode.\\n8. Have built-in defaults with port numbers for all core services and allow overrides, avoiding the need for static configuration file or configuration provider.\\n9. *(Stretch)* implement a `-o`/`--output` argument which could be used to customize the pretty-printed objects (i.e. non-JSON).\\n10. *(Stretch)* Implement support for use of the client via the API Gateway, including being able to connect to a remote EdgeX instance. This might require updates in go-mod-core-contracts.\\n\n\n## Context and Problem Statement\\nWe want to use [Semantic Versioning (SemVer)](https://semver.org/) when publishing our GitHub Actions, but [we cannot use SemVer ranges](https://github.community/t/semver-ranges-docs-issues/16723/2) when consuming them.\\n<details>\\n<summary><b>More Context</b></summary><br>\\nWhen we do\\n```yaml\\nuses: byu-oit/github-action-tf-plan-analyzer@v2\\n```\\nin GitHub Actions, we're telling GitHub to grab the repo at the _branch_ or _tag_ labelled `v2`.\\nThere's [currently no concept of SemVer](https://github.community/t/semver-ranges-docs-issues/16723/2) in GitHub Actions, but there are some conventions. A common convention is to cut \"releases\" (tags) using SemVer, e.g. `v2.0.1`, but that doesn't change the `v2` tag or branch by default.\\nWe would like consumers to be able to get `^2.0.0`, but currently that requires moving the `v2` tag or updating the `v2` branch.\\nGitHub for a while suggested using moving around tags like `v2`, but has [since suggested](https://docs.github.com/en/free-pro-team@latest/actions/creating-actions/about-actions#using-tags-for-release-management) either using branches or tags for that purpose. It is otherwise very unusual to move tags on GitHub, so we prefer branches to convey mutability.\\n</details>\\n## Decision Drivers\\n* Versioning is necessary to communicate changes (especially breaking changes)\\n* We don't want to break deployments\\n* Breaking somebody's ability to deploy doesn't actually bring down production\\n* We want to make it easy to roll out updates\\n* Many consumers of our GitHub Actions are unfamiliar with how they work\\n\n## Decision\n* Versioning is necessary to communicate changes (especially breaking changes)\\n* We don't want to break deployments\\n* Breaking somebody's ability to deploy doesn't actually bring down production\\n* We want to make it easy to roll out updates\\n* Many consumers of our GitHub Actions are unfamiliar with how they work\\nWhen publishing actions, use branches corresponding to major versions (e.g. `v1`, `v2`) and keep them updated. Treat updates to the major-version branches as production changes. Continue to tag new releases according to SemVer (e.g. `v2.0.1`, `v2.1.0`).\\nWhen consuming [our actions](https://github.com/search?q=org%3Abyu-oit+github-action+archived%3Afalse&type=Repositories), refer to the major version branches (e.g. `v2`). This will allow us to roll out most changes to the organization quickly and effectively.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n\n## Decision\n", "response": "## Decision\nWe will decouple CLI versions from runtime versions. This decision is made to allow independent development, testing, and release cycles for the CLI and the runtime components. By decoupling the versions, we can ensure that updates and new features in the CLI or runtime can be released without being dependent on each other's schedules. \n\nKey considerations:\n1. **Flexibility in Release Cycles**: Decoupling allows each component to evolve at its own pace. Critical bug fixes or enhancements can be released more swiftly without waiting for a coordinated release.\n\n2. **Backward Compatibility**: The CLI will ensure backward compatibility with multiple versions of the runtime where feasible. Clear documentation will guide users on version compatibility.\n\n3. **Simplified Development**: Developers can focus on the specific needs and improvements pertinent to either the CLI or runtime, reducing the complexity involved in aligning their development cycles.\n\n4. **Versioning Strategy**: The CLI and runtime will each follow their semantic versioning (`semver`) schemes. For instance, CLI could be at version 1.0.0 while the runtime is at 1.5.0.\n\n5. **User Communication**: We will ensure effective communication around the compatibility of various CLI and runtime versions to avoid confusion among users.\n\nBy decoupling, Dapr can respond more quickly to user needs and technological advancements in a more agile manner.", "matched_ids": [3650, 241, 81, 957, 4073], "time": "0:00:04.803076", "input_tokens": 2286, "output_tokens": 278}
{"context": "## Problem\\nMost of the credentials/information of the environment (like a service address) are only available as soon the application is deployed.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context and Problem Statement\\nThis addresses the fact that we could not configure different environments to point to different API URLs as necessary (across Int, Test, and Live envs in Cosmos).\\nAt the point of deployment, `npm run build` is called in Jenkins to build the static  bundle and minify etc. Environment config (`REACT_APP_SERVER_URL` etc. variables declared in `.env`) get baked into the bundle and obfuscated at this point.\\nWe would need different Jenkins jobs or parameters to bake in the environment config (eg. the correct server URL) multiple times, which goes against the idea of having an _unconfigured / environment agnostic_ RPM in Cosmos.\\n## Decision Drivers <!-- optional -->\\n* Avoiding too much extra code overhead\\n* Trying to avoid security converns with proxy solutions (injection / DB manipulations)\\n* Avoiding having to re-build in Cosmos or on the box\\n\n## Decision\n* Avoiding too much extra code overhead\\n* Trying to avoid security converns with proxy solutions (injection / DB manipulations)\\n* Avoiding having to re-build in Cosmos or on the box\\nThis is a common problem with create-react-app and the need to configure it. We came across [this solution](https://github.com/facebook/create-react-app/issues/578#issuecomment-277843310) which detailed a simple way of consuming config with the `window` global.\\nIn source code this remains localhost:8080 as a local development fallback and it is passed in via Cosmos environment config.\\n\n\n## Context\\nIn some environments the port and other variables are only available on startup, and will have to be overriden. See https://github.com/servian/TechChallengeApp/issues/21\\n\n## Decision\nAdd environment variables overrides back\\n\n\n## Context\\nWe need a way of deploying our services to our various hosted environments (dev, staging, prod, etc.). We would\\nprefer these deployments to be automated with minimal to zero human interaction.\\n\n## Decision\nThe DIT infrastructure already has a lot of tooling and infrastructure around deployments so we will utilise this. We\\nwill use Jenkins to automatically deploy from dedicated git branches - these will be:\\n- development\\n- staging\\n- uat\\nWe will have 5 Jenkins jobs in total - these will be:\\n- trade-access-program-backoffice\\n- trade-access-program-frontend\\n- trade-access-program-polling-dev\\n- trade-access-program-polling-staging\\n- trade-access-program-polling-uat\\nThe role of the \"polling\" jobs are to watch a related git branch for any commit changes. Once a change is detected then\\nit will trigger the `trade-access-program-backoffice` and `trade-access-program-frontend` jobs with a set of\\nenvironment parameters triggering the deployment to one of our three environments.\\nThis allows us to simply merge or push to one of the three dedicated git branches above and a full automated deployment\\nwill occur for that environment.\\n\n\n## Context\\nWe need a way to deploy our application to AWS.\\n\n## Decision\nWe will use [Terraform] to deploy our application. More specifically,\\nwe will use [Chalice] to create the build artifact for the source\\ncode (.zip) and then use [Terraform] to manage the infrastructure\\nresources.\\n\ud83d\udc49 **Note:** Ideally, we would publish the build artifacts to S3 before\\ndeployment. That way the deployment process could happen independently\\nof the build process.\\n\ud83d\udc49 **Note:** To begin with, we'll just convert the JSON data created\\nby [Chalice] to [HCL] and add any other resources needed. Going forward,\\nwe will use modules to create reusable components.\\n\n\n## Context and Problem Statement\\nWe want to deploy our application in docker containers that can be easily updated\\n\n## Decision\nChosen option: \"OpenShift\", because\\n* Built on Kubernetes.\\nThe bank has experience on it.\\nProvides a lot of added value tools for CI/CD, automated builds.\\nIs supported by RedHat and we have a great support contract for it.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Problem\\nMost of the credentials/information of the environment (like a service address) are only available as soon the application is deployed.\\n\n## Decision\n", "response": "## Decision\nTo handle this scenario, we will implement a solution using **environment variables** and **runtime configuration**. Instead of bundling these credentials and information at build time, the application will be designed to read necessary environment variables at startup. This decision allows us to harness dynamically injected configuration information, adapting to the actual deployment environment.\n\n- Use a configuration service (e.g., HashiCorp Consul, AWS Parameter Store, Azure App Configuration) to manage and inject configuration values at runtime.\n- Leverage a startup script to source and export these environment variables just before the application initiates.\n- Implement a health check or initialization routine in the application that verifies the necessary configuration is available, logging any missing or malformed data for easier debugging.\n- This approach ensures that the application is adaptable to different environments without needing to restart or rebuild, greatly improving flexibility and reducing deployment time.", "matched_ids": [4165, 4584, 5013, 4358, 4096], "time": "0:00:14.355153", "input_tokens": 991, "output_tokens": 175}
{"context": "## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno / minimal costs.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context and Problem Statement\\nBFI's IIIF Univeral Viewer auditing platform requires that each event\\nwhich is subject to long term persistence is captured in a manner that\\nis secure, robust and performant.\\n## Decision Drivers\\n* Ability for internal BFI staff to leverage their existing experience\\nwith managing and maintaining the RDMS.\\n* Ease of deployment and configuration.\\n* Ease of ongoing maintenance, patching and support.\\n* Open source, mature and in active support / maintenance.\\n\n## Decision\n* Ability for internal BFI staff to leverage their existing experience\\nwith managing and maintaining the RDMS.\\n* Ease of deployment and configuration.\\n* Ease of ongoing maintenance, patching and support.\\n* Open source, mature and in active support / maintenance.\\nMySQL is selected as the solution as it aligns provides BFI with an RDMS\\nwith which they already have experience and are able to support and\\nmaintaining in the long term.\\n### Positive Consequences\\n* No initial or ongoing licensing costs associated with the use of\\nMySQL.\\n* Ease of ongoing maintenance for BFI owning to their existing\\nfamiliarity with MySQL.\\n* Availability of multiple storage engines (InnoDB, MyISAM, etc) and\\ndistributions (MySQL Community Edition, MariaDB, PerconaDB, etc).\\n* Simple deployment and configuration (e.g., MySQL publish official\\nDocker images).\\n### Negative Consequences\\n* Certain SQL features are not available (although it is not expected\\nthat these will be required).\\n\n\n## Context\\nWe need an identity provider for our platform. This could be a host (SaaS) solution vs a self hosted solution. While the Cost & Lockin is higher at scale with a SaaS the self hosted has a higher Management Effort & Less Security & Expensive Bootstraping. From an evolutionary architecture perspective it is not core to value generation and does not change rapidly[source](https://www.youtube.com/watch?v=8bEsNT7jdC4&t=112s&index=57&list=WL) - identity will mostlikly the same (OpenIDConnect, Login page ...) but it will be hard to change in the future if you decided for one provider.\\n\n## Decision\nWe will use a SaaS solution as we don't have the skill to host our own solution in the efficient quality. And from a evolutionary architecture perspective the system is commondity or support.\\n\n\n## Context\\nAt the moment and for the short term future Hackney do not have a clear candidate for a single sign-on provider. They are reviewing the situation and will decide on a future solution before the end of 2019.\\nWe have so far been using Basic Authentication.\\nAs with other decisions we have attempted to follow the path set by the Repairs Hub which is another Hackney Rails app that is more mature. We asked what they were using and the answer was Microsoft Azure Active Directory using OpenID Connect (OAuth2). We believed we could get the exact same provisioning when the time came for Report a Defect to integrate with a single sign-on provider, however when the time came we learnt that it wasn't supported by Hackney and should be thought of as experimental.\\nAs we had 2 weeks left we discussed with Hackney Architect Keith Gatt the possibility of using Auth0 as a temporary option to enable us to meet the user need of providing agent names within the service, as well as security needs through the use of a logged authentication service that provided one set of credentials per person, rather than a single set for basic auth.\\n\n## Decision\nUse Auth0 as the single sign-on provider, and remove basic auth from production.\\n\n\n## Context\\nA mechanism for providing end-user authentication of MIT users is required for\\ncore aspects of this application.\\nMIT officially only supports Shibboleth / Touchstone which traditionally is enabled via an Apache httpd module.\\nThe [MIT OpenID Pilot](https://mitlibraries.github.io/oauth.html) was determined\\nunacceptable for this project.\\nIn order to use Shibboleth we intended to containerize the application. However,\\nwhile that process ended in a successful proof-of-concept, it used SAML and not\\nmod_shib. Once we went down the path of not using the officially MIT supported\\nmod_shib solution, we felt using SAML in-app should be explored.\\n\n## Decision\nWe will use an in-app Touchstone SP using ruby-saml as documented in our\\n[Developer Documentation](https://mitlibraries.github.io/touchstone_saml.html).\\n\n\n## Context\\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\\nThere is currently no formal recommendation for a tool of choice in the technical guidance https://github.com/DFE-Digital/technical-guidance.\\nWe want a tool that provides an open and modern security standard.\\n\n## Decision\nWe are going to use DSI as our single sign-on provider using the OIDC standard.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno / minimal costs.\\n\n## Decision\n", "response": "## Decision\nWe will use Keycloak as our identity and access management solution. Keycloak is an open-source solution that provides all core features such as user creation and registration, management, authentication, and authorization. It is relatively easy to deploy and configure, supports various authentication standards like OIDC and SAML, and offers a comprehensive admin console to manage users and roles efficiently. The fact that it is open source means there are no licensing costs, thus addressing the project's requirements of minimal costs for the platform. Additionally, Keycloak's active community and regular updates ensure the solution remains reliable and secure over time.", "matched_ids": [4854, 3421, 5211, 2582, 1252], "time": "0:00:04.220525", "input_tokens": 1379, "output_tokens": 121}
{"context": "## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nOur service operator will be responsible for providing Postgres along any other\\nservices to our end users.\\nIn this case, the ask is specifically Postgres, meaning we can pick our\\nsolution for providing the instance as long as it exposes the correct APIs.\\nA reasonable set of candidates are:\\n- AWS RDS Postgres\\n- AWS RDS Aurora Postgres\\n### AWS RDS Postgres\\nA solution most comonly used across the board for databases. Is managed and\\nmaintained by AWS, provides Backups and Snapshots.\\n### AWS RDS Aurora Postgres\\nHas the benefits of AWS RDS Postgres, and some more benefits. Such as:\\n- Scalable persistance storage\\n- IAM authentication\\n- Automatic Failover with multi AZs\\n- Faster read loads\\n- Slightly cheaper\\nAlso has few downsides:\\n- Not recommended for heavy write systems (Twitter big)\\n- Slightly behind version wise\\n\n## Decision\nWe will continue with Aurora as we we don't have any specific requirements not\\nto and can benefit from the solution.\\n\n\n## Context\\nWe needed some persistent storage of audit results.\\nWe considered:\\n### Schemaless - DynamoDb\\nThis would be the most obvious choice for a lambda based\\nservice.\\nThe dynamo data model is tables of key value pairs.\\nThe main problem with Dynamo is the limit of I think 4K\\nper value.\\nOne of the things we wanted to do was briefly cache API\\nresponses which could easily breach that 4K limit.\\nWith Dynamo the access control is via IAM which would be\\nrelatively easy to manage and encryption at rest can\\nbe easily configured.\\n### Schemaless - MongoDb\\nMongo was a better fit for our service, saving JSON\\nrepresentations of API responses and resources.\\nThe problem with Mongo is it's not AWS native so we'd\\nhave to provision a server, manage the access controls,\\nmaintenance and availability.\\n### Relational - RDS MySQL / PostgreSQL\\nRDS gives us the benefit of running a managed service so\\nAWS are responsible for backups and patching minor\\nversion.\\nRelational databases give us the ability to do on-the-fly\\nanalysis more easily.\\nWe can store JSON as blob data although not ideal.\\nIf we want to re-use the database instance as a shared\\nresource across multiple services RDS is more capable.\\nIt's not unlikely that a future development may require\\ndjango.\\nPostgreSQL seems to be the general direction of travel\\nin GDS and is much more capable for a wider range of\\nuse-cases where we don't know what we're building in the\\nfuture.\\n\n## Decision\nWhilst none of the options were perfect we decided that\\na PostgreSQL RDS was the best option given the\\ninformation available to give us an infrastructure to\\nsupport multiple tools and services.\\n\n\n## Context\\nData storage and management is a key component of the tta smarthub platform. At a fundamental level, a relational as well as NoSQL systems were considered. Because the platform's data is mainly structured  and with a need for frequent queries and reports, a relational database management system was viewed as more suitable. With that in mind we looked at MySQL and PostreSQL (Postgres) both open source and popular choices.\\n\n## Decision\nWhile both databases provide adequate storage and management, especially with updates provided by version 8 of MySQL, in the end Postgres was chosen. The main advantages of Postgres are implementations leading to better concurrency, specifically the MVCC (Multiversion Concurrency Control) without the need to use read locks, transactional ACID (Atomicity, Consistency, Isolation, Durability) support making the system less vulnerable to data corruption. Postgres also supports partial indexes, can create indexes in a non-blocking way, multiple cores, GIN/GIST indexing accelerating full-text searches.\\n\n\n#### Context and Problem Statement\\nA relational database is required to persist the applications data.\\n#### Considered Options\\n- Microsoft SQL\\n- MySQL\\n- PostgreSQL\\n- SQLite\\n#### Decision Outcome\\nChosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n\n## Decision\nChosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n\n\n## Context and Problem Statement\\nBFI's IIIF Univeral Viewer auditing platform requires that each event\\nwhich is subject to long term persistence is captured in a manner that\\nis secure, robust and performant.\\n## Decision Drivers\\n* Ability for internal BFI staff to leverage their existing experience\\nwith managing and maintaining the RDMS.\\n* Ease of deployment and configuration.\\n* Ease of ongoing maintenance, patching and support.\\n* Open source, mature and in active support / maintenance.\\n\n## Decision\n* Ability for internal BFI staff to leverage their existing experience\\nwith managing and maintaining the RDMS.\\n* Ease of deployment and configuration.\\n* Ease of ongoing maintenance, patching and support.\\n* Open source, mature and in active support / maintenance.\\nMySQL is selected as the solution as it aligns provides BFI with an RDMS\\nwith which they already have experience and are able to support and\\nmaintaining in the long term.\\n### Positive Consequences\\n* No initial or ongoing licensing costs associated with the use of\\nMySQL.\\n* Ease of ongoing maintenance for BFI owning to their existing\\nfamiliarity with MySQL.\\n* Availability of multiple storage engines (InnoDB, MyISAM, etc) and\\ndistributions (MySQL Community Edition, MariaDB, PerconaDB, etc).\\n* Simple deployment and configuration (e.g., MySQL publish official\\nDocker images).\\n### Negative Consequences\\n* Certain SQL features are not available (although it is not expected\\nthat these will be required).\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n\n## Decision\n", "response": "## Decision\n\nWe will proceed with AWS RDS Aurora MySQL for Bookit's persistence mechanism. This decision is based on the following factors:\n- **Enhanced Performance and Availability:** Aurora MySQL offers improved performance and up to the minute backups with no degradation, which is beneficial even if the current usage does not require it, as there's always a need for high availability.\n- **Hands-off Management:** Aurora provides a managed service that simplifies maintenance with its high availability features, automated backups, and scalability.\n- **Cost-Effectiveness:** While there is a minimal cost increase compared to standard MySQL on RDS, Aurora MySQL's feature set justifies this small additional expense.\n- **AWS Integration:** As Bookit is operating within AWS, using an AWS service like Aurora ensures seamless integration and reduces complexity compared to setting up an entirely new database environment.\n\nThis choice aligns with the project\u2019s requirement for a hosted/PaaS SQL/RDBMS solution that maintains open-source principles while addressing the essentials of database performance and reliability.", "matched_ids": [3897, 418, 1167, 1661, 4854], "time": "0:00:06.946780", "input_tokens": 1811, "output_tokens": 204}
{"context": "## Context\\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nOff-chain data is accessible via REST API. Currently all system components uses direct REST calls in various places making unit test hard.\\n\n## Decision\nCreate client library and use it as dependency in components that want to read the off-chain data. Include the mocked version of the service so unit-tests does not have to rely on the implementation.\\n\n\n## Context\\nWe need to store persistent data in a form that is easy to query and need to select the\\nappropriate data store. We consider the choice to be between a relational database or a\\ndocument database.\\n\n## Decision\nWe have decided to use [MongoDB](https://www.mongodb.com/) as our database for datalabs\\ndata. We felt that the data model for the parts of the system known at this time, a\\ndocument structure provided more flexibility and easier integration with our Node.js\\napplication.\\nMongoDB also provides the ability to run in cluster providing the option for greater\\nresilience. For the time being we are opting to run a single node storing data to\\na mounted GlusterFS volume. This gives data resilience through node failure but obviously\\ndoes not give service resilience or time based backups.\\n\n\n## Context\\nAll the basic data types needs to be efficiently serialized into binary format before saving in KV store or sending to network.\\n\n## Decision\n`protobuf` is used for data serialization both for storing and network communication.\\n`protobuf` is used widely in entire Cosmos ecosystem, and we would need to use it anyways.\\n\n\n## Context\\nSome of the data that is required by the application is only needed in specific modules. Till now, `redux` has been relied on heavily and most of times for good reason. Some data, however, is only needed in specific parts of the application, but is still stored in the global store or is kept in a reducer on a per-component basis.\\nDifferent parts of the application have their own saga, reducer, actions and selectors which makes the application more difficult to understand, error prone and maintenance harder to keep up.\\nStoring all data in the global store requires a lot of (duplicate) boilerplate code, tests and mocking.\\n\n## Decision\nThe structure of the application's state needs to reflect the data that is globally required. If specific data is only needed in specific parts of the application, that application's part should provide the data through a reducer and a context provider and not make use of the global (`redux`) store.\\nEssentially, the entire application's state can be provided by a context provider, but for now we'll take the bottom-up approach and gradually refactor and introduce the reducer/context approach in favour of `redux` complexity.\\n\n\n## Context\\nThe API for CHIP needs some sort of persistence for storing its data. Although the data needs for the app are pretty lightweight, it's likely that they will grow in the future as new features are desired or more data is acquired. To this end, sticking with a simple, powerful, reliable, and flexible technology that is widely understood makes sense.\\n\n## Decision\nA RDBMS fits this bill and PostgreSQL does so even more snugly. It's a top contender, it's free, it's currently the best supported traditional database for the Elixir/Erlang ecosystem and it's unparalleled at its geospatial capabilities.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\\n\n## Decision\n", "response": "## Decision\nImplement a two-phase commit protocol to ensure consistency between off-chain and on-chain data stores. The first phase will prepare the transactions on both storage types and verify that they can succeed. The second phase will commit the transactions. If any failure is detected during the prepare phase, the system will abort the transaction and roll back any changes. Additionally, use consistent hashing or a timestamp mechanism to keep track of the transaction sequence, ensuring that references are only accessible once all related data is properly committed and synchronized. Regular audits and consistency checks will be put in place to detect and correct any potential discrepancies proactively.", "matched_ids": [3692, 730, 3746, 314, 4859], "time": "0:00:03.251358", "input_tokens": 861, "output_tokens": 121}
{"context": "## Problem\\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\\n(as they are designed to do so). Limitations of other storage plugins are\\ne.g., that not every structure of configuration is allowed.\\nSome of these limitations were documented `infos/status`, others were not.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Problem\\nTo make storage-plugins suitable for `spec` they need to be able to store\\nall the metadata as specified in [METADATA.ini](/doc/METADATA.ini).\\nMost file formats do not have support for that.\\nIf metadata is merged from different namespaces, e.g., `spec:` and `user:`,\\nmetadata from one namespace might end up in keys of other namespaces, e.g.,\\nmetadata from `spec:` might end up in `user:`.\\n\n## Decision\nDo not store metadata unrelated to the configuration file structure in any namespace except in `spec:/`.\\n- Trying to store any other metadata in any other namespace leads to an error.\\nE.g. `kdb set-meta user:/data metadata_not_suitable_for_storage_plugins something` would fail\\n(validated by `spec` plugin).\\n- Metadata that is designed to be stored by storage plugins to preserve configuration file structure.\\nE.g. `comment` or `order`, might be stored in any namespace.\\nSometimes, the same metadata can be used in several namespaces but with different meanings\\nand ways of serialization, e.g. `type` and `array`:\\n- In `spec:/` the metadata `array=` (empty value) means \"this is an array\".\\nIf you give it a value e.g. `array=#4` it means \"this is an array with default size X\" (e.g. `#4` = size 5).\\n- In any other namespace `array=` means \"this is an empty array\" and e.g. `array=#4` means \"this is an array with max index #4\".\\n`array=#4` is not stored literally but inferred.\\n- Either the storage plugin does not support arrays, then the metadata will be discarded\\non `kdbSet` but `spec` will keep on adding it for every `kdbGet`.\\n- Or, if the storage plugin supports arrays, the data will be serialized as array\\n(even if the metadata comes from `spec`) and as such available in the next `kdbGet`\\nfrom the storage plugin to be validated by `spec`.\\nUse different storage plugins, or plugins with different configurations,\\nfor the `spec:/` namespace:\\n- `ni`\\n- TOML with `meta` configuration\\nThe `kdb mount` tool will add the `meta` plugin configuration when mounting a storage plugin to `spec:/`.\\n- [Spec Expressiveness](spec_expressiveness.md)\\n- [Arrays](array.md)\\n\n\n## Problem\\nAn empty KeySet is passed to kdbSet(). What is the correct persistent\\nrepresentation?\\n\n## Decision\nRemove files on empty KeySet.\\n\n\n## Problem\\nOn structures like maps or [arrays](array.md) there are different\\npossibilities which keys are looked up in the KeySet and which\\nare simply iterated.\\nWithout any guidelines, applications would provide arbitrary inconsistent\\nbehavior.\\n\n## Decision\nEvery key that an application wants to use, must be looked up\\nwith `ksLookup` using a cascading lookup key.\\n- [Arrays](array.md)\\n\n\n## Problem\\nWhen doing kdbGet() possible more keys are returned which might be\\nconfusing. When doing a second kdbGet() with a new keyset\\nno keys might be returned, because it is up-to-date.\\nWhen doing kdbSet() a deep duplication is needed.\\nIdea: keep a duplicated keyset internally. Return (non-deep?)\\nduplications exactly of the parts as requested.\\n\n## Decision\nNot yet decided.\\n- [Global Validation](global_validation.md)\\n\n\n## Problem\\n- locking is not reset when ref counting again gets 0 (adding to keyset and\\npop again) #2202\\n- C++ API for KeySet and Key has unexpected differences: also use ref counting\\nfor KeySets (also suggested in #1332)\\n\n## Decision\n- add second counter to Key\\n- One counter is for references, the other one is for locking the keyname. The keyname is thereby protected with a re-entrant lock.\\n- introduce reference counter for KeySets (for external keyset references, e.g. in bindings)\\n- limit number of references to `UINT16_MAX - 1` and use `UINT16_MAX` as an error value\\n- return error on reference count overflow\\n- no error on underflow (decrement when zero), instead stay at zero\\n- use fixed sized types (`uint16_t`) for reference counters\\n- increment/decrement references before/after passing instances to plugins\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Problem\\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\\n(as they are designed to do so). Limitations of other storage plugins are\\ne.g., that not every structure of configuration is allowed.\\nSome of these limitations were documented `infos/status`, others were not.\\n\n## Decision\n", "response": "## Decision\n- Ensure that limitations of all storage plugins are clearly documented in `infos/status` to provide transparency and avoid unexpected behavior.\n- Introduce a validation mechanism that checks a KeySet against the documented limitations before using a storage plugin. This will prevent attempts to store unsupported configurations.\n- Develop a standard interface for querying the capability and limitations of each storage plugin programmatically. This allows applications to adapt to different storage backends dynamically.\n- Encourage the use of `dump` and `quickdump` for use cases requiring full representation capability of any KeySet and reserve other plugins for scenarios where such comprehensive support is unnecessary and known limitations are acceptable.\n- Consider enhancing existing storage plugins incrementally by extending their capabilities, where feasible, to bridge gaps in their representation abilities and improve overall usability.", "matched_ids": [1278, 1271, 1310, 1289, 1309], "time": "0:00:04.595127", "input_tokens": 1188, "output_tokens": 157}
{"context": "## Context\\n[Command-query separation](https://martinfowler.com/bliki/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nThe theory of the CQRS and the CQS principle says that we should not return any information as the result of Command processing. The result should be always \"void\". However, sometimes we need to return some data immediately as part of the same request.\\n\n## Decision\nWe decided to allow in some cases return results after command processing. Especially, when we create something and we need to return the ID of created object or don't know if request is Command or Query (like Authentication).\\n\n\n## Context\\nFor each module, when we process particular Command or Query, we need to resolve a graph of objects. We need to decide how dependencies of objects will be resolved.\\n\n## Decision\nSolution number 2 - IoC Container per module</br>\\nIoC Container per module supports the autonomy of the module and louse coupling so this is a more important aspect for us than duplicated code in some places.\\n\n\n## Context\\nIn [ADR 5](0005-use-cqrs-architectural-style.md) we decided to adopt CQRS architectural style. Therefore, we need a\\nstrategy, or a guideline for handling commands issued to admin sub-system. Couple of things we can take into the\\naccount:\\n- Admin tool is intended for \"internal use\" only, i.e. it won't be exposed to the wild (customers)\\n- There is no strict requirement about predictable and stable latency\\n\n## Decision\nCommands issued to admin will be handled synchronously by default. An asynchronous mode is still allowed though for\\nexceptional cases only.\\n\n\n## Context\\nApplication architecture is commonly expressed as a set of guidelines (written or not) that the development team needs to follow. Although essential, architectural guidelines are rarely reviewed or\\nenforced appropriately. The typical result is that architecture degrades through time.\\nIn some cases, application use component ([ADR-0003 - CQRS and Event Sourcing (ES) for Applications](0003-cqrs-and-event-sourcing-for-applications.md)) and\\ndesign ([ADR-0004 - Hexagonal Architecture for Applications](0004-hexagonal-architecture-for-applications.md)) architectural patterns that promote architectural guidelines, but usually there is\\nnothing to verify them.\\nThe hexagonal architecture provides a well-defined placeholder for every significant application artifact. But there are also some rules regarding dependencies between those artifacts. It is not\\nallowed that each class or interface access anything that it wants. When you add additional CQRS/ES aspects, there are even more rules to follow.\\nWe want to ensure that rules will not be broken and that developers new to the hexagonal architecture and CQRS/ES can comfortably work with them without breaking anything. It will help if we\\nhave in place tests that verify all architectural invariants.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n## Decision\n**We will use architectural testing for verifying architectural constraints.**\\nBuilding on top of the [ArchUnit](https://www.archunit.org/) library, Klokwrk provides DSL for specifying hexagonal architecture layers for CQRS/ES applications. There is support for several subtypes\\nof CQRS/ES flavored hexagonal architecture corresponding to the commandside, projections, and queryside aspects.\\nFor more insight and details, take a look at [\"Behavior and architectural testing\"](../../article/modules-and-packages/modulesAndPackages.md#behavior-and-architectural-testing). There is also a video\\n[\"Project Klokwrk: how it helps defining software architecture and solves integration\"](https://www.youtube.com/watch?v=35oUxjXWNYU) that, besides other things, talks about architectural testing in\\n`klokwrk-project`.\\n\n\n## Context\\nAdopting the [Domain-driven design](0002-adopt-ddd-approach.md) approach requires isolating domain elements (i.e.\\naggregates and services) from the infrastructure (i.e. application clients and persistence).\\n\n## Decision\n`menu-generation` application will adopt [hexagonal architecture](https://en.wikipedia.org/wiki/Hexagonal_architecture_(software)),\\nas it aims to provide this separation.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\n[Command-query separation](https://martinfowler.com/bliki/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n\n## Decision\n", "response": "## Decision\nWe will fully embrace the Command-query separation principle in the `menu-generation` application, ensuring that methods are distinctly categorized as either commands or queries. This separation will enhance our ability to confidently use queries without unintended side effects, while maintaining clear orchestration and management of commands that alter system state. Existing methods will be reviewed and refactored if necessary to align with this principle, supporting a more robust and maintainable architecture.", "matched_ids": [890, 899, 1040, 4898, 643], "time": "0:00:02.952271", "input_tokens": 1159, "output_tokens": 87}
{"context": "## Context\\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nThe system is composed of a number of API and related datastores. There are currently a number of differing date formats in use.\\n\n## Decision\nWe will use the ISO 8601 format for dates: yyyy-mm-dd and times.\\n\n\n## Context\\nIt is important to ensure for each team member the same environment configuration in which the application will operate.\\n\n## Decision\nDocker\\n\n\n## Context\\nThe app should be able to build in any basic environment, without the need to worry about environmental differences\\n\n## Decision\nThe app will use Docker for production deployment.\\n\n\n## Context\\nApplications need to be configured differently depending on where they are running. For example, the backend running locally will have different configuration then the backend running in production.\\n\n## Decision\nWe will use environment variables to configure applications.\\n\n\n## Context\\nWe want to have an application wide logging infrastructure in place so that we can trace user requests through the\\napplication. It should be possible to trace each request through the application layers so that we can see what outgoing\\ncalls were made for incoming requests.\\n\n## Decision\nThe use of `monolog/monolog` to provide configurable logging levels throughout the application. Unique tracing\\ninformation is already provided by the amazon loadbalancers so this should be made available to the logging library and\\nattached to the logged information.\\n* Logging of service code to be main source of log information. If needed handlers can also be logged.\\n* Most logging to be done at an _info_ level.\\n* User actions that need monitoring (e.g. authentication failures) at _notice_ level\\n* Personally Identifiable Information (PII) to **not** be logged anywhere but _debug_\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\\n\n## Decision\n", "response": "## Decision\nWe will embed versioning information in the application metadata and log the current version at application startup. For capturing the application's view of time, logs will include timestamps in UTC format along with each log entry. This ensures that both the deployed version and accurate timing information are readily accessible for debugging purposes.", "matched_ids": [2173, 3060, 3264, 1173, 4852], "time": "0:00:03.881296", "input_tokens": 549, "output_tokens": 61}
{"context": "## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nDo we have lots of little xxx4k libraries in their own repositories?  Or have a monorepo: one big project with each xxx4k library in a subdirectory.\\nThe former decouples release cadences.\\nThe latter makes it easier to maintain a single BOM for publishing to Maven Central, perform integration testing when libraries depend on one another, and use a consistent version number across all libraries.\\n\n## Decision\nWe will have a monorepo.\\n\n\n## Context\\nI want to be able to mix and match different implementations\\nof the ddd sample application, without needing to pull in\\na large collection of unnecessary dependencies.\\nI want all of the code to be together in one place; which\\nis to say, I want to treat the entire project as a mono-repo.\\nI can't be bothered to maven install/maven deploy each\\nlittle piece to propagate the necessary changes between\\nisolated libraries.\\n\n## Decision\nUse a maven reactor project to track the dependencies between\\ndifferent libraries within the project\\n\n\n## Context\\nModern software needs to be easy to build.\\nFor this reason most software uses a build system to specify how the software is build and which dependencies it needs to work.\\nPopular build systems in the Java world are Ant, Maven and Gradle.\\nAnt is pretty flexible but lacks dependency management and is also very rarely used these days.\\nMaven is more rigid than Ant but supports dependency management and is still widely used.\\nGradle is the newest build system. It can be programmed in Groovy/Kotlin and is the most flexible build system.\\nGradle is also the only way to build for the Android platform.\\nGradle enjoys widespread usage and the authors are most proficient in it.\\n\n## Decision\nWe will use Gradle as build system.\\n\n\n## Context\\nAs the project is an example of a more advanced monolith architecture, it is necessary to save all architectural decisions in one place.\\n\n## Decision\nFor all architectural decisions Architecture Decision Log (ADL) is created. All decisions will be recorded as Architecture Decision Records (ADR).\\nEach ADR will be recorded using [Michael Nygard template](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions), which contains following sections: Status, Context, Decision and Consequences.\\n\n\n## Context\\nAlmost every software project needs to interact with the 3rd party libraries and frameworks to some extent.\\nGenerally, the less coupled the code is with the 3rd party libraries and frameworks (usually thanks to additional abstraction\\nlayers on top of them), more flexible the software becomes in regard to technology replacements in the future.\\nHowever, sometimes it's quite viable to compromise this flexibility in favor of practical benefits that particular\\ntechnology brings, in this case Spring Boot/Framework. Especially that it is designed with the abstraction and\\nextensibility in mind, so introducing another layer of abstraction is senseless.\\n\n## Decision\nWe accept direct coupling with Spring Boot/Framework code without any abstraction layers on top of it.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n\n## Decision\n", "response": "## Decision\nWe will modularize the software into smaller, well-defined Gradle subprojects. This will help maintain clear API boundaries, improve code quality, and allow for easier reuse of modules outside of this project. Each module will be responsible for a specific functionality and can be integrated with others as needed, maintaining both ease of integration and potential for reuse.", "matched_ids": [4542, 127, 2633, 900, 1035], "time": "0:00:03.504491", "input_tokens": 856, "output_tokens": 71}
{"context": "## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nDeprecated design:\\n- For each resource, we extended the existing resource by creating a new table in the OpenMRS database called nhs_{resource} (e.g. nhs_patient and nhs_practitioner).\\nThe problem:\\n- When we identified a one-to-many relationship between a resource and a data field (between the Practitioner and the SDS Role Profile Id field), the current solution would have required making further changes to the OpenMRS model.\\n- These changes would have included creating a new table to hold instances of the SDS Role Profile Id field.\\n- However, this would have made the Practitioner resource more complex.\\*\\n\\* For context, a Practitioner resource in FHIR is equivalent to a provider in OpenMRS.\\n\n## Decision\n- We decided to use the existing OpenMRS model for creating attributes for each resource.\\n- There is a one-to-many relationship between a resource and its attributes.\\n- Each attribute has an attribute type, which is stored in the attribute_type table.\\n- For each new piece of data to be added to a resource:\\n- Create a new attribute_type that describes the data type - this should be included in the seed data.\\n- Create a new attribute with the attribute_type that you just created and link this new attribute to your resource.\\n- Update the translator for the resource (from FHIR to OpenMRS and vice versa) to ensure that the new piece of data is populated.\\n- Please see [the addition of the SDS Role Profile Id field to the Practitioner resource](https://github.com/Open-GP/openmrs-module-gpconnect/commit/f0ff30c18f0d386e338d87f22a847b0dc3dff5ff) to see an implementation of the following pattern:\\n![Provider Attribute Design](../images/ProviderAttributeDesign2.png)\\n\n\n## Context\\nIn the coming months we will be building this application. The first order of business is to come up with a sensible data model and application structure.\\n\n## Decision\nWe discussed a database model that is based around a small number of tables with a lot of columns. In particular:\\n- `Candidate`, primarily used for authentication\\n- `ApplicationForm`, which belongs to a Candidate, and holds the\\ninformation that is common to all of the applications that the user\\nmakes (in the \"Apply 1\" phase, they can apply for up to three courses).\\n- `ApplicationChoice`, which holds the information that is course specific,\\nlike the status of the application and the personal statement. It holds a reference to the \"course choice\", which is the combination of course, provider and the training location.\\nThere will be some other models that hold references, work histories and qualifications.\\nThe eventual table structure will look something like this:\\n![data model](0003-initial-datamodel.png)\\n\n\n## Context\\nOur application should handle 2 types of requests - reading and writing. </br>\\nFor now, it looks like:</br>\\n- for reading, we need data model in relational form to return data in tabular/flattened way (tables, lists, dictionaries).\\n- for writing, we need to have a graph of objects to perform more sophisticated work like validations, business rules checks, calculations.\\n\n## Decision\nWe applied the CQRS architectural style/pattern for each business module. Each module will have a separate model for reading and writing. For now, it will be the simplest CQRS implementation when the read model is immediate consistent. This kind of separation is useful even in simple modules like User Access.\\n\n\n## Context\\nThe Childrens Social Care Placement service is in Alpha and needs to explore solutions to the domain problem. Given this is still very much an exploratory phase in the Software Delivery LifeCycle (SDLC) it seems appropriate to optimise around developer productivity so that the service can flex to the most appropriate fit.\\n\n## Decision\nThe decision is to use the [Gov.UK PaaS].\\n**What does Gov.UK PaaS offer?**\\nGov.UK PaaS is a fully managed platform service which has central government accredition for Offical data and provides teams with:-\\n* Fast application delivery, with short cycles\\n* Flexible development tools\\n* Simplified DevOps\\n* Seamless scalability\\n* Integrated data services\\n* A secure, trusted platform\\n* 24x7 support\\nIt is also important to note that Ruby on Rails is a first class citizen on Gov.UK as one of its core buildpack languages and frameworks.\\n**Why not DfE Azure?**\\nThere are two primary reasons why; simplified DevOps and increased developer productivity.\\n**Is it secure?**\\nOne of the main drivers for Departmental Platforms is security assurance. Gov.UK PaaS is accredited by for Official information.\\nSome useful URLs for security in general on Gov.UK PaaS are [here](https://www.cloud.service.gov.uk/security/) and the [security principles](https://www.cloud.service.gov.uk/cloud-security-principles/).\\nAnd for the DfE there is a specific [paper](https://docs.google.com/document/d/1_81bqO2snLDn95p9W5OyMXwqzUqX5qcHSYJvV5CUeeQ/edit#heading=h.du8tl2830srf) from MBS.\\n\n\n## Context\\nPartner organizations work directly with the families in need. Sometimes they will need to collect PII such as address, names and ages of family members, and other contact information. This data is not necessary for Diaperbase to operate, and there are security concerns about that PII being disclosed should there be a breach. There is also a separation of concerns between the two applications; Diaperbase is inventory management and Partnerbase is effectively a CRM. At this time, we belive that they are different enough in their purposes that they should be separate.\\n\n## Decision\nA new application, Partnerbase, will be created to handle the CRM aspects. It will communicate over a private API with Diaperbase to handle request fulfillment.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n\n## Decision\n", "response": "## Decision\nWe decided to implement a relationship management model using a graph-based approach where each individual (mother, child, head of household, etc.) is a node, and the relationships (e.g., parent-child, family head-dependent) are edges connecting these nodes. \n\nKey features of the model:\n- **Node Entity:** Each individual (mother, child, head of household, etc.) is represented as a node.\n- **Edge Entity:** Each relationship (e.g., parent-child, head-dependent) is represented as an edge with attributes defining the type of relationship.\n- **Graph Database Integration:** Utilize a graph database to efficiently manage and query relationships, allowing for easy navigation and data retrieval during delivery and PNC visits.\n- **Flexible Relationship Types:** Define a set of standard relationship types to capture various familial connections while allowing the addition of custom relationship types for future requirements.\n- **UI/UX Design for Easy Switching:** Implement a user interface that allows healthcare workers to easily navigate between related individuals, enhancing productivity and reducing input errors during visits.\n\nBy adopting this graph-based relationship model, we ensure scalability and flexibility, accommodating current and future family relationship use cases while also improving the user experience for healthcare providers.", "matched_ids": [781, 4021, 888, 920, 3342], "time": "0:00:05.355110", "input_tokens": 1572, "output_tokens": 242}
{"context": "## Context\\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need an authentication mechanism for users. It must be secure and\\nfrontend application independent.\\n\n## Decision\nWe will use OpenID Connect. Open Lobby Server will provide all the hard stuff\\nfor a frontend applications. Ideally over the GraphQL API.\\n\n\n## Context\\nWe currently have two separate models for Administrators (Admin) and Nurses (User)\\nto distinguish users of the dashboard and the app respectively. But there is an increasing\\noverlap between the two models.\\n- Some Administrators are using the app\\n- Access to dashboard needs to be audited similarly to syncing\\n\n## Decision\nCombine User and Admin into a single User model, and have different authentication mechanism for\\nboth these models.\\n- Authenticate syncing api using phone number authentication\\n- Authenticate dashboard access using email authentication\\n\n\n## Context\\nAs with any modern web system there is a need to secure the application with\\nstrong authentication and Role Based Access Control (RBAC). RBAC allows us to\\ncreate permissions, apply them to roles and associates roles with users. From\\nthis we can restrict what users can do and see within the application based on\\ntheir role. In addition to this we also had to investigate where to store role\\ninformation and define teams to support our use of RBAC.\\n\n## Decision\nAfter some investigation and discussion into various ways of implementing RBAC\\nwithin Ruby we have decided to implement the Pundit Ruby gem within the Verify\\nSelf Service application. This gem provides us with a way of creating policies\\nwhich apply to the app as a whole and to individual classes within the\\napplication.\\nWe have chosen pundit for a number of reasons. The first is that it seems\\nrelatively easy to implement within our application. A proof of concept was\\ncreated in a few hours which restricted creating new components within the\\napplication to those who hold the `gds` or `dev` roles. Policies and permissions\\nare defined in code as plain Ruby objects which are easy to understand and\\nsimple to implement. This ease of implementation and an appearance of providing\\nthe fuctionality we are seeking we believe makes pundit a good candidate for\\nimplementing RBAC within our application.\\nIn addition to this we are aware of prior-art use within GDS on the GOV.UK\\nproject. Finally we have found a lot of documentation and articles about how to\\nimplement and use pundit.\\nRole information we have decided to store in a custom field within our cognito\\ninstance. This will take the form of a comma seperated list which will be split\\ninto an array of roles for use within the application. This method allows us to\\nkeep all use information together within AWS Cognito and means that the only way\\nrole information can be changed is via SDK calls to AWS Cognito.\\nFinally we understand there is a need to define teams within our application. We\\nhave decided that a team is analogous to an AWS Cognito group. This allows a\\nuser to be part of one more groups and for a group to hold one or more users. We\\ncan easily query group membership using the AWS Cognito SDK and check for a\\nusers membership to a specific group.\\n\n\n## Context\\nWe need a way to protect our app. Only a small number of people\\nshould be able to access the application. This includes the\\ndevelopers and the intended end users (i.e. you).\\n\n## Decision\nTo protect our application, we will require all requests to include\\nan `Authorization` header containing a JWT. Any request that is missing\\nthis header will be rejected. Futhermore, the JWT will include an\\nexpiry so we can control the time period in which users can access\\nthe application.\\nThe authentication process will be implemented as an additional AWS\\nLambda function. In [Chalice], this is referred to as a Custom Authorizer\\n\n\n## Context\\nThe service will be used by a variety of different types of users, and we need\\nto ensure that those users are only able to access the parts of the service that\\nwe want them to.\\nOur current chosen authentication provider, Auth0, has support for assigning\\nroles to users, but this couples the service tightly to their service, so we\\nshould avoid this.\\n\n## Decision\nWe will use the 'Pundit' gem to manage users' access to the service. The\\npermissions will be grouped into roles that can then be assigned to users\\nrequiring a particular level of access.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\\n\n## Decision\n", "response": "## Decision\nWe will implement authentication and management of Link Platform Administrators using an OAuth 2.0 provider, such as AWS Cognito or Auth0. This approach allows us to securely authenticate administrators while providing management capabilities for their sessions. Administrators will log in using their credentials, and their access tokens will be used to manage data and configuration within their Link Instances. The system will also include role-based access control to ensure administrators have the appropriate permissions for their responsibilities. This solution provides a secure and scalable method for managing administrative access across different instances.", "matched_ids": [478, 1699, 3950, 4362, 2397], "time": "0:00:06.634451", "input_tokens": 1086, "output_tokens": 110}
{"context": "## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need to deploy the Broker somewhere.\\nThe Broker implements the service broker API to generate per-application syslog\\ndrain URLs (Adapter URLs).\\nThe Adapter is written in Go.\\nThe Broker is written in Go.\\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\\n\n## Decision\nWe will deploy the Broker as an AWS Lambda\\n\n\n## Context\\nWe need to provide tenants with the ability to provision databases for use in\\ntheir applications. Our first iteration of this will be using RDS.\\nWe investigated some implementations of a service broker which supported RDS\\n- [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\\n- [18F](https://github.com/18F/rds-service-broker)\\n\n## Decision\nWe will use the [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\\nrds broker. As this is not a supported product, we will fork this and maintain\\nthis and implement new features ourselves.\\n\n\n## Context\\nThe early beta was originally hosted on dxw's Heroku to get delivering quickly whilst access to GPaaS could be set up.\\nAccess to GPaaS with approved billing has now been confirmed so we are migrating the service to its longer term home.\\n\n## Decision\n- Move the service from Heroku to GPaaS for all environments except the ephemeral pull request review environments.\\n- Use Terraform to define the infrastructure as code.\\n\n\n## Context\\nAs a government entity, BEIS want to host their service on [GOVUK\\nPaaS](https://www.cloud.service.gov.uk/). This was always the service teams\\nintention.\\n\n## Decision\nHost all environments excpet local development on BEIS own GOVUK PaaS account.\\n\n\n## Context\\nIn [this issue](https://github.com/raft-tech/TANF-app/issues/609) we propose tasks to automate the provisioning of Cloud.gov-brokered services via our CI pipeline.\\nThis project plans to use Cloud.gov-brokered S3 buckets for TANF data file storage (sensitive PII data) and for Django Admin static assets (public), and a Cloud.gov-brokered RDS PostgreSQL service. Currently, there are no scripts to provision and secure these S3 buckets and RDS instance, and no dev documentation on how to deploy them, secure them, and verify that they are configured correctly. Additionally, this same initial provisioning must be done again for all additional target environments.\\n\n## Decision\n[Terraform](https://www.terraform.io/) is a tool for building, changing, and versioning infrastructure safely and efficiently, and was proposed as a solution for managing our persistent Cloud.gov-brokered infrastructure. We can closely model what is done in [another HHS project](https://github.com/HHS/Head-Start-TTADP) and create per-environment infrastructure configurations which are leveraged by Terraform in CircleCI, with environment-specific settings read directly from Cloud.gov during our CI process. Note that this workflow was a [recommendation from Cloud.gov](https://www.youtube.com/watch?v=86wfgNK_0o4), and they themselves use Terraform to provision their own infrastructure.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n\n## Decision\n", "response": "## Decision\nWe will continue to host and deploy the broker using the Tech Ops multi-tenant Concourse pipeline, as it aligns with the existing infrastructure and collaboration practices. We will ensure that the broker remains compliant with Cyber Security requirements, and we will perform regular audits and updates as needed. Any modifications or enhancements to the broker will be managed through the `alphagov/tech-ops` repository under a structured branching and review process to maintain code integrity.", "matched_ids": [3295, 186, 1250, 2410, 3794], "time": "0:00:03.271764", "input_tokens": 966, "output_tokens": 92}
{"context": "## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nTo simplify the implementation for drivers, we will create a \"unified\" GPIO base class, which provides the standard interfaces that can be used to set the mode (input, output, special function). This will provide a single class to implement for GPIO support, rather than one class for each mode.\\nIdeally, we will find a way to zero-overhead create wrapper classes that constrain the interface appropriately, so we can mark pins as `GPIOInput` or `GPIOOutput` and have the compiler report an error if we try to do an invalid operation for the given type.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nThe change that we're proposing or have agreed to implement.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\n", "response": "## Decision\nIn order to address the issue and accommodate the constraints, we will adopt microservices architecture for our application. This decision is based on the necessity to increase scalability, improve fault isolation, and facilitate easier deployment. Microservices will allow us to design our application as a collection of loosely coupled services, each responsible for specific functionalities. This will enable teams to work on different services independently, thus speeding up development and fostering innovation. Additionally, it will allow us to utilize different technology stacks according to the needs of each microservice, thereby improving flexibility and efficiency.", "matched_ids": [3544, 2025, 3021, 3820, 1145], "time": "0:00:03.792140", "input_tokens": 416, "output_tokens": 112}
{"context": "## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is to buy hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n\n## Decision\nKafkarator will only deal with the future solution using Aiven, and not work for on-premise Kafka.\\n\n\n## Context\\nWhen application developers wants to interact with Kafkarator, they need an API. We have previously been vague about how that API should look, should it be one CRD, multiple CRDs, piggyback on existing NAIS CRDs etc.\\nWe need to make a decision, so that we can proceed with detailing how the API looks, and what can be expected from it. It is also needed so that we can actually start implementing Kafkarator in earnest.\\nFrom various discussions, we have a few findings that guide our decision:\\n- When doing NAIS deploy, it is possible for developers to supply multiple resources to be applied to the cluster\\n- We have two separate concerns that needs two separate configurations\\n\n## Decision\n- We will define one new CRD object to configure topics and access to this\\n- App developers will create this in the cluster when deploying their application\\n- Kafkarator will watch this CRD and take needed actions\\n- App developers will add configuration to their Application resource listing kafka pools they need access to\\n\n\n## Context\\nA lot of options exist to create applications nowadays - serverless, PaaS, SaaS etc. The purpose of this project is to hone Kubernetes and Docker skills in particular which may mean use of these technologies seem over engineered.\\n\n## Decision\n* Use Kubernetes and Docker\\n* Use Amazon EKS\\n\n\n## Context\\nFollowing the [rollout of AWS EKS in London](https://aws.amazon.com/about-aws/whats-new/2019/02/amazon-eks-available-in-mumbai--london--and-paris-aws-regions/) it is now an attractive alternative to the hand-rolled kubernetes installation that was created as a result of [ADR003](ADR003-container-orchestration.md). This will bring numerous benefits:\\n* reducing the amount of infrastructure we manage in-house, by offloading it to AWS\\n* better alignment with [Technology & Operations Strategic Principle #3 - \"Use fully managed cloud services by default\"](https://reliability-engineering.cloudapps.digital/documentation/strategy-and-principles/re-principles.html#3-use-fully-managed-cloud-services-by-default)\\nAs of 1.12, EKS supports what we need (e.g. Istio, kiam etc.).\\n\n## Decision\nWe will host the GDS Supported Platform on AWS EKS.\\n\n\n## Context\\nWe plan to use AWS as a development/staging environment and as a production environment, on ECS.\\n\n## Decision\nUse CircleCI for CI, and deploy docker images to AWS ECR for use in deployments.\\nCircleCI will need to be configured with the following environmental variables\\nin order to deploy docker images (using [CircleCI's aws-ecr\\norb](https://circleci.com/developer/orbs/orb/circleci/aws-ecr)):\\n- DEV_AWS_ACCESS_KEY_ID\\n- DEV_AWS_SECRET_ACCESS_KEY\\n- PROD_AWS_ACCESS_KEY_ID\\n- PROD_AWS_SECRET_ACCESS_KEY\\n- AWS_REGION\\n- AWS_ECR_ACCOUNT_URL\\nThis project's Dockerfile requires access to a privately hosted github project\\n(openfido-utils). Use [Buildkit](https://docs.docker.com/develop/develop-images/build_enhancements/) on CircleCI to grant SSH access to the docker build processes.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n\n## Decision\n", "response": "## Decision\nKafkarator will be designed specifically to support the hosted Kafka service from Aiven, ensuring seamless integration with cloud infrastructure. On-premise Kafka support will not be implemented, as the focus will be solely on the cloud solution to align with NAV's migration strategy.", "matched_ids": [863, 2834, 3118, 3911, 2471], "time": "0:00:03.436728", "input_tokens": 1019, "output_tokens": 56}
{"context": "## Context and Problem Statement\\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nIt will be necessary to access the servers that form the Datalabs network but we do not\\nwant to expose any services outside of the JASMIN tenancy that we do not have to.\\n\n## Decision\nWe have decided that all access to the cluster will be via a Bastion server over an SSH\\nconnection on port 22. We will restrict access through the firewall to known IP address\\nranges including the development workstations, the STFC VPN and the Tessella public IP\\naddress.\\nThis excludes public facing services that should be available over HTTPS on port 443 via\\na different route.\\n\n\n## Context and Problem Statement\\nMaking a node wrapper for Mozilla Deepseepch, in `deepspeech-node-wrapper`.\\nUsing Mozilla node `deepspeech` module and their example [`DeepSpeech/examples/nodejs_wav`](https://github.com/mozilla/DeepSpeech-examples/tree/r0.6/nodejs_wav) as a starting point.\\nThe question this ADR explore is, how to package the the STT models in this npm module?\\n## Decision Drivers <!-- optional -->\\n- Easy to reason around\\n- Avoid adding large binaries to git repository\\n- Avoid adding large binaries to NPM\\n- ease of use and setup\\n- considerate of slow internet connections\\n- \u2026 <!-- numbers of drivers can vary -->\\n\n## Decision\n- Easy to reason around\\n- Avoid adding large binaries to git repository\\n- Avoid adding large binaries to NPM\\n- ease of use and setup\\n- considerate of slow internet connections\\n- \u2026 <!-- numbers of drivers can vary -->\\n<!-- Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n_TBC_\\nLeaning torwards option 3.\\n<!-- ### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n-->\\n\n\n## Context and Problem Statement\\nSDR is evolving into an (increasing) set of (largely) single-purpose services, with communication between these services occurring via synchronous HTTP. This architecture is fragile as it scales under load and as the number of services increases because:\\n* Each service must have knowledge about many other services.\\n* Each service depends on many other services to perform its job; any of these services being unavailable can bring down the service or hamper performance.\\n* Synchronous HTTP can have a performance penalty for operations in which a response is not required (e.g., reporting events).\\n* The Stanford network is notoriously unreliable (especially at night); significant coding effort is required to account for this.\\nIn addition, there are already places within SDR were we have re-invented a message broker and/or jerry-rigged around not having a message broker (e.g., polling via HTTP to determine if a long-running job has completed).\\nIntroducing a message broker to support asynchronous communication where appropriate will allow decoupling of services, making SDR more resilient and scalable.\\n\n## Decision\nTBD.\\n### Positive Consequences\\n* Increased ability to scale number of services.\\n* Increased ability to scale under load.\\n* Increased resilience to service failures.\\n* Provide mechanism for notification of the completion of long-running jobs.\\n### Negative Consequences\\n* Message broker must be supported by Ops.\\n* Services become dependent on the Message Broker. (However, Message Brokers are typically designed to be depended on.)\\n* The evolution work to begin using the Message Broker.\\n\n\n## Context\\nThe solution has to be portable and lightweight and work without special infrastructure.\\n\n## Decision\nDocker-compose is used as multi-container solution.\\n\n\n## Context\\nWe need to define the API specifications of the services we're going to implement.\\n\n## Decision\nWe use the [OpenAPI 2.0](https://swagger.io/specification/) specification (aka Swagger spec) as standard for our REST API definitions.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\\n\n## Decision\n", "response": "## Decision\nWe will integrate with the Newslabs PSTT service by implementing a secure HTTPS-based API client within our application. This client will be responsible for handling authentication using OAuth 2.0 to ensure secure access to the STT service. We will utilize access tokens to manage sessions and maintain security over the communications. The API client will abstract the Newslabs PSTT API calls allowing for ease of integration and future adaptability if the service or its endpoints change. Additionally, the client will include error handling to manage service unavailability or request failures, providing fallbacks or retries as necessary.", "matched_ids": [745, 3340, 786, 4798, 1180], "time": "0:00:03.511899", "input_tokens": 1004, "output_tokens": 119}
{"context": "## Context\\nPIMS requires a database to store all property information.\\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\\nAdditionally it must be supported by Entity Framework Core 3.1.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need a database in order to store the data provided by the user in the different forms.\\n\n## Decision\nTo reduce the amount of integrations and complexity, we've decided to use one of the databases that Firebase offers.\\nUnder the Firebase umbrella are Real-time database and Firestore which are similar in many features.\\nGoogle recommends Firestore for new developers, as it is built upon \u201cthe successes of Real-time database\u201d. Also, for our purposes, Firestore is a better choice because:\\n- It offers offline support for iOS, Android and Web clients whereas real-time DB supports only iOS and Android clients.\\n- The one-big-JSON-tree structure of real-time can become difficult to maintain as the DB grows. Document-based Firestore offers better organisation provisions using collections.\\n- Scaling on firestore is automatic whereas scaling on real-time database requires sharding.\\n\n\n## Context\\nData storage and management is a key component of the tta smarthub platform. At a fundamental level, a relational as well as NoSQL systems were considered. Because the platform's data is mainly structured  and with a need for frequent queries and reports, a relational database management system was viewed as more suitable. With that in mind we looked at MySQL and PostreSQL (Postgres) both open source and popular choices.\\n\n## Decision\nWhile both databases provide adequate storage and management, especially with updates provided by version 8 of MySQL, in the end Postgres was chosen. The main advantages of Postgres are implementations leading to better concurrency, specifically the MVCC (Multiversion Concurrency Control) without the need to use read locks, transactional ACID (Atomicity, Consistency, Isolation, Durability) support making the system less vulnerable to data corruption. Postgres also supports partial indexes, can create indexes in a non-blocking way, multiple cores, GIN/GIST indexing accelerating full-text searches.\\n\n\n## Context\\nMany ASP.NET applications use [Entity Framework (EF)](https://docs.microsoft.com/en-us/ef/), an Object Relational Mapper (ORM) that helps access data stored in database.\\nEF allows data in a database to be accessed by extending a DbContext class and adding properties to this extending class of type DbSet. DbContext and DbSet provide methods for performing basic CRUD operations against entities in a database that are defined in model classes. These model classes contain annotations that define the table name, columns and relationships with other entities. When a query is performed, EF handles creating instances of model classes and filling them with the received data.\\nSome properties are lazily loaded, with the queries related to fetching the required data only being run when thoses properties are accessed. This approach is commonly used when accessing a property representing a relationship with another entity.\\nA DbContext by default tracks changes to entities returned as the result of queries, with changes being saved when a call is made to a DbContext's SaveChanges or SaveChangesAsync methods.\\nThe DbContext and DbSet classes provide methods that can be used to fetch data, with the ability to apply limitations on what data is returned. EF will generate the required query, execute it, parse the response data and return the appropriate entity model instances.\\nEF supports migrations written as classes with Up and Down methods, to support upgrading and rolling back, respectively. These methods are implemented by adding calls to a provided MigrationBuilder instance.\\nDapper is a library that is commonly referred to as a \"micro-ORM\". It provides methods to support executing SQL queries and parsing the results to create instances of particular model classes. Unlike EF, Dapper does not support the tracking of changes and queries must be written using SQL.\\nDapper was developed for the StackOverflow website to address performance issues, as outlined in [this blog post](https://samsaffron.com/archive/2011/03/30/How+I+learned+to+stop+worrying+and+write+my+own+ORM).\\n\n## Decision\nWe will use Dapper with the [repository pattern](http://blog.mantziaris.eu/blog/2016/10/24/the-repository-and-unit-of-work-pattern/) to access data stored in the database.\\n\n\n## Context\\nWe need to store persistent data in a form that is easy to query and need to select the\\nappropriate data store. We consider the choice to be between a relational database or a\\ndocument database.\\n\n## Decision\nWe have decided to use [MongoDB](https://www.mongodb.com/) as our database for datalabs\\ndata. We felt that the data model for the parts of the system known at this time, a\\ndocument structure provided more flexibility and easier integration with our Node.js\\napplication.\\nMongoDB also provides the ability to run in cluster providing the option for greater\\nresilience. For the time being we are opting to run a single node storing data to\\na mounted GlusterFS volume. This gives data resilience through node failure but obviously\\ndoes not give service resilience or time based backups.\\n\n\n## Context\\nWe need [dependency injection](https://stackoverflow.com/questions/130794/what-is-dependency-injection) in order to build decoupled software units.\\nAlso we need access on a relational database.\\n\n## Decision\nSpring offers a lot of convenience features for those purposes.\\nApache Wicket already provides a Spring integration.\\nWith the help of the [wicket-spring-boot](https://github.com/MarcGiffing/wicket-spring-boot) the convenience feature of Spring Boot can be applied on top of that.\\nThis makes the development faster and easier than using the alternative of dependency injection with [Google Guice](http://software.danielwatrous.com/wicket-guice-including-unittests/).\\nFurther usage of Spring enables potential useage of the various Spring sub-projects for advanced requirements like the integration of other software systems.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nPIMS requires a database to store all property information.\\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\\nAdditionally it must be supported by Entity Framework Core 3.1.\\n\n## Decision\n", "response": "## Decision\nWe have decided to use PostgreSQL as the database for PIMS. PostgreSQL is an open-source, highly stable, and robust relational database management system that supports complex queries, data constraints, and ensures data integrity, which is essential for handling property information.\n\nKey reasons for choosing PostgreSQL include:\n\n- **Compatibility with Entity Framework Core 3.1:** PostgreSQL offers strong support for Entity Framework Core 3.1, which will simplify the integration of the database with our application and allow the use of familiar object-relational mapping patterns.\n  \n- **Containerization and Scalability:** PostgreSQL is well-suited to run within a Linux Docker container, providing a seamless deployment experience on OpenShift. Its robust architecture also supports efficient container scaling, which aligns well with OpenShift's orchestration capabilities.\n\n- **Advanced Concurrency and Data Constraints:** PostgreSQL supports Multi-Version Concurrency Control (MVCC) without read locks, ensuring high performance and efficient transaction handling. Its support for ACID transactions and advanced data constraints will help maintain data integrity.\n\n- **Community and Ecosystem:** PostgreSQL has a strong community and a broad ecosystem of extensions and tools. This ensures continuous support, regular updates, and the possibility to extend its capabilities if future requirements evolve.\n\nThis decision aligns with our technical requirements and ensures a reliable, performant, and future-proof database solution for PIMS.", "matched_ids": [2488, 1167, 727, 730, 1758], "time": "0:00:10.068731", "input_tokens": 1408, "output_tokens": 277}
{"context": "## Context and Problem Statement\\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\\ninterfaces\" - possibly leading to the impression that there is such a thing as\\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\\nthe only question is who manages this thread-safety. Interfaces which are not\\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\\nhidden in the generated code and therefore not obvious to the casual reader.\\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\\nmanaging their own locking internally. This ADR proposes that uniffi forces\\ncomponent authors to explicitly manage that locking in all cases - or to put\\nthis in Rust terms, that all structs supported by uniffi must already be\\n`Send+Sync`\\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\\nany misunderstandings that might be caused by using the somewhat broad and\\ngeneric \"Threadsafe\".\\n## Decision Drivers\\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https://github.com/mozilla-mobile/fenix/issues/17086)\\n(with more details available in [this JIRA ticket](https://jira.mozilla.com/browse/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https://github.com/mozilla/uniffi-rs/issues/417) or to\\n[pass interface references over the FFI](https://github.com/mozilla/uniffi-rs/issues/419).\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nThe issue motivatina# [short title of solved problem and solution]\\nWhat is the issue that we're seeing that is motivating this decision or change?\\n* Status: [proposed | rejected | accepted | deprecated | ... | superseded by [ADR-0005](0005-example.md)] <!-- optional -->\\n* Deciders: [list everyone involved in the decision] <!-- optional -->\\n* Date: [YYYY-MM-DD when the decision was last updated] <!-- optional -->\\nTechnical Story: [description | ticket/issue URL] <!-- optional -->\\n## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\n\n## Decision\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | ... | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, ...]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, ...]\\n* ...\\nWhat is the change that we're proposing and/or doing?\\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context\\nThe `p2p` package responsible for peer-to-peer networking is rather old and has a number of weaknesses, including tight coupling, leaky abstractions, lack of tests, DoS vulnerabilites, poor performance, custom protocols, and incorrect behavior. A refactor has been discussed for several years ([#2067](https://github.com/tendermint/tendermint/issues/2067)).\\nInformal Systems are also building a Rust implementation of Tendermint, [Tendermint-rs](https://github.com/informalsystems/tendermint-rs), and plan to implement P2P networking support over the next year. As part of this work, they have requested adopting e.g. [QUIC](https://datatracker.ietf.org/doc/draft-ietf-quic-transport/) as a transport protocol instead of implementing the custom application-level `MConnection` stream multiplexing protocol that Tendermint currently uses.\\nThis ADR summarizes recent discussion with stakeholders on the scope of a P2P refactor. Specific designs and implementations will be submitted as separate ADRs.\\n\n## Decision\nThe P2P stack will be refactored and improved iteratively, in several phases:\\n* **Phase 1:** code and API refactoring, maintaining protocol compatibility as far as possible.\\n* **Phase 2:** additional transports and incremental protocol improvements.\\n* **Phase 3:** disruptive protocol changes.\\nThe scope of phases 2 and 3 is still uncertain, and will be revisited once the preceding phases have been completed as we'll have a better sense of requirements and challenges.\\n\n\n## Context\\nIn this ADR, we provide recommendations for implementing the IBC\\nhandlers within the `ibc` (modules) crate.\\n\n## Decision\nConcepts are introduced in the order given by a topological sort of their dependencies on each other.\\n### Events\\nIBC handlers must be able to emit events which will then be broadcasted via the node's pub/sub mechanism,\\nand eventually picked up by the IBC relayer.\\nAn event has an arbitrary structure, depending on the handler that produces it.\\nHere is the [list of all IBC-related events][events], as seen by the relayer.\\nNote that the consumer of these events in production would not be the relayer directly\\n(instead the consumer is the node/SDK where the IBC module executes),\\nbut nevertheless handlers will reuse these event definitions.\\n[events]: https://github.com/informalsystems/hermes/blob/bf84a73ef7b3d5e9a434c9af96165997382dcc9d/modules/src/events.rs#L15-L43\\n```rust\\npub enum IBCEvent {\\nNewBlock(NewBlock),\\nCreateClient(ClientEvents::CreateClient),\\nUpdateClient(ClientEvents::UpdateClient),\\nClientMisbehavior(ClientEvents::ClientMisbehavior),\\nOpenInitConnection(ConnectionEvents::OpenInit),\\nOpenTryConnection(ConnectionEvents::OpenTry),\\n//     ...\\n}\\n```\\n### Logging\\nIBC handlers must be able to log information for introspectability and ease of debugging.\\nA handler can output multiple log records, which are expressed as a pair of a status and a\\nlog line. The interface for emitting log records is described in the next section.\\n```rust\\npub enum LogStatus {\\nSuccess,\\nInfo,\\nWarning,\\nError,\\n}\\npub struct Log {\\nstatus: LogStatus,\\nbody: String,\\n}\\nimpl Log {\\nfn success(msg: impl Display) -> Self;\\nfn info(msg: impl Display) -> Self;\\nfn warning(msg: impl Display) -> Self;\\nfn error(msg: impl Display) -> Self;\\n}\\n```\\n### Handler output\\nIBC handlers must be able to return arbitrary data, together with events and log records, as described above.\\nAs a handler may fail, it is necessary to keep track of errors.\\nTo this end, we introduce a type for the return value of a handler:\\n```rust\\npub type HandlerResult<T, E> = Result<HandlerOutput<T>, E>;\\npub struct HandlerOutput<T> {\\npub result: T,\\npub log: Vec<Log>,\\npub events: Vec<Event>,\\n}\\n```\\nWe introduce a builder interface to be used within the handler implementation to incrementally build a `HandlerOutput` value.\\n```rust\\nimpl<T> HandlerOutput<T> {\\npub fn builder() -> HandlerOutputBuilder<T> {\\nHandlerOutputBuilder::new()\\n}\\n}\\npub struct HandlerOutputBuilder<T> {\\nlog: Vec<String>,\\nevents: Vec<Event>,\\nmarker: PhantomData<T>,\\n}\\nimpl<T> HandlerOutputBuilder<T> {\\npub fn log(&mut self, log: impl Into<Log>);\\npub fn emit(&mut self, event: impl Into<Event>);\\npub fn with_result(self, result: T) -> HandlerOutput<T>;\\n}\\n```\\nWe provide below an example usage of the builder API:\\n```rust\\nfn some_ibc_handler() -> HandlerResult<u64, Error> {\\nlet mut output = HandlerOutput::builder();\\n// ...\\noutput.log(Log::info(\"did something\"))\\n// ...\\noutput.log(Log::success(\"all good\"));\\noutput.emit(SomeEvent::AllGood);\\nOk(output.with_result(42));\\n}\\n```\\n### IBC Submodule\\nThe various IBC messages and their processing logic, as described in the IBC specification,\\nare split into a collection of submodules, each pertaining to a specific aspect of\\nthe IBC protocol, eg. client lifecycle management, connection lifecycle management,\\npacket relay, etc.\\nIn this section we propose a general approach to implement the handlers for a submodule.\\nAs a running example we will use a dummy submodule that deals with connections, which should not\\nbe mistaken for the actual ICS 003 Connection submodule.\\n#### Reader\\nA typical handler will need to read data from the chain state at the current height,\\nvia the private and provable stores.\\nTo avoid coupling between the handler interface and the store API, we introduce an interface\\nfor accessing this data. This interface, called a `Reader`, is shared between all handlers\\nin a submodule, as those typically access the same data.\\nHaving a high-level interface for this purpose helps avoiding coupling which makes\\nwriting unit tests for the handlers easier, as one does not need to provide a concrete\\nstore, or to mock one.\\n```rust\\npub trait ConnectionReader\\n{\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd>;\\n}\\n```\\nA production implementation of this `Reader` would hold references to both the private and provable\\nstore at the current height where the handler executes, but we omit the actual implementation as\\nthe store interfaces are yet to be defined, as is the general IBC top-level module machinery.\\nA mock implementation of the `ConnectionReader` trait could looks as follows:\\n```rust\\nstruct MockConnectionReader {\\nconnection_id: ConnectionId,\\nconnection_end: Option<ConnectionEnd>,\\nclient_reader: MockClientReader,\\n}\\nimpl ConnectionReader for MockConnectionReader {\\nfn connection_end(&self, connection_id: &ConnectionId) -> Option<ConnectionEnd> {\\nif connection_id == &self.connection_id {\\nself.connection_end.clone()\\n} else {\\nNone\\n}\\n}\\n}\\n```\\n#### Keeper\\nOnce a handler executes successfully, some data will typically need to be persisted in the chain state\\nvia the private/provable store interfaces. In the same vein as for the reader defined in the previous section,\\na submodule should define a trait which provides operations to persist such data.\\nThe same considerations w.r.t. to coupling and unit-testing apply here as well.\\n```rust\\npub trait ConnectionKeeper {\\nfn store_connection(\\n&mut self,\\nclient_id: ConnectionId,\\nclient_type: ConnectionType,\\n) -> Result<(), Error>;\\nfn add_connection_to_client(\\n&mut self,\\nclient_id: ClientId,\\nconnection_id: ConnectionId,\\n) -> Result<(), Error>;\\n}\\n```\\n#### Submodule implementation\\nWe now come to the actual definition of a handler for a submodule.\\nWe recommend each handler to be defined within its own Rust module, named\\nafter the handler itself. For example, the \"Create Client\" handler of ICS 002 would\\nbe defined in `modules::ics02_client::handler::create_client`.\\n##### Message type\\nEach handler must define a datatype which represent the message it can process.\\n```rust\\npub struct MsgConnectionOpenInit {\\nconnection_id: ConnectionId,\\nclient_id: ClientId,\\ncounterparty: Counterparty,\\n}\\n```\\n##### Handler implementation\\nIn this section we provide guidelines for implementing an actual handler.\\nWe divide the handler in two parts: processing and persistence.\\n###### Processing\\nThe actual logic of the handler is expressed as a pure function, typically named\\n`process`, which takes as arguments a `Reader` and the corresponding message, and returns\\na `HandlerOutput<T, E>`, where `T` is a concrete datatype and `E` is an error type which defines\\nall potential errors yielded by the handlers of the current submodule.\\n```rust\\npub struct ConnectionMsgProcessingResult {\\nconnection_id: ConnectionId,\\nconnection_end: ConnectionEnd,\\n}\\n```\\nThe `process` function will typically read data via the `Reader`, perform checks and validation, construct new\\ndatatypes, emit log records and events, and eventually return some data together with objects to be persisted.\\nTo this end, this `process` function will create and manipulate a `HandlerOutput` value like described in\\nthe corresponding section.\\n```rust\\npub fn process(\\nreader: &dyn ConnectionReader,\\nmsg: MsgConnectionOpenInit,\\n) -> HandlerResult<ConnectionMsgProcessingResult, Error>\\n{\\nlet mut output = HandlerOutput::builder();\\nlet MsgConnectionOpenInit { connection_id, client_id, counterparty, } = msg;\\nif reader.connection_end(&connection_id).is_some() {\\nreturn Err(Kind::ConnectionAlreadyExists(connection_id).into());\\n}\\noutput.log(\"success: no connection state found\");\\nif reader.client_reader.client_state(&client_id).is_none() {\\nreturn Err(Kind::ClientForConnectionMissing(client_id).into());\\n}\\noutput.log(\"success: client found\");\\noutput.emit(IBCEvent::ConnectionOpenInit(connection_id.clone()));\\nOk(output.with_result(ConnectionMsgProcessingResult {\\nconnection_id,\\nclient_id,\\ncounterparty,\\n}))\\n}\\n```\\n###### Persistence\\nIf the `process` function specified above succeeds, the result value it yielded is then\\npassed to a function named `keep`, which is responsible for persisting the objects constructed\\nby the processing function. This `keep` function takes the submodule's `Keeper` and the result\\ntype defined above, and performs side-effecting calls to the keeper's methods to persist the result.\\nBelow is given an implementation of the `keep` function for the \"Create Connection\" handlers:\\n```rust\\npub fn keep(\\nkeeper: &mut dyn ConnectionKeeper,\\nresult: ConnectionMsgProcessingResult,\\n) -> Result<(), Error>\\n{\\nkeeper.store_connection(result.connection_id.clone(), result.connection_end)?;\\nkeeper.add_connection_to_client(result.client_id, result.connection_id)?;\\nOk(())\\n}\\n```\\n##### Submodule dispatcher\\n> This section is very much a work in progress, as further investigation into what\\n> a production-ready implementation of the `ctx` parameter of the top-level dispatcher\\n> is required. As such, implementers should feel free to disregard the recommendations\\n> below, and are encouraged to come up with amendments to this ADR to better capture\\n> the actual requirements.\\nEach submodule is responsible for dispatching the messages it is given to the appropriate\\nmessage processing function and, if successful, pass the resulting data to the persistence\\nfunction defined in the previous section.\\nTo this end, the submodule should define an enumeration of all messages, in order\\nfor the top-level submodule dispatcher to forward them to the appropriate processor.\\nSuch a definition for the ICS 003 Connection submodule is given below.\\n```rust\\npub enum ConnectionMsg {\\nConnectionOpenInit(MsgConnectionOpenInit),\\nConnectionOpenTry(MsgConnectionOpenTry),\\n...\\n}\\n```\\nThe actual implementation of a submodule dispatcher is quite straightforward and unlikely to vary\\nmuch in substance between submodules. We give an implementation for the ICS 003 Connection module below.\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: Msg) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ConnectionReader + ConnectionKeeper,\\n{\\nmatch msg {\\nMsg::ConnectionOpenInit(msg) => {\\nlet HandlerOutput {\\nresult,\\nlog,\\nevents,\\n} = connection_open_init::process(ctx, msg)?;\\nconnection::keep(ctx, result)?;\\nOk(HandlerOutput::builder()\\n.with_log(log)\\n.with_events(events)\\n.with_result(()))\\n}\\nMsg::ConnectionOpenTry(msg) => // omitted\\n}\\n}\\n```\\nIn essence, a top-level dispatcher is a function of a message wrapped in the enumeration introduced above,\\nand a \"context\" which implements both the `Reader` and `Keeper` interfaces.\\n### Dealing with chain-specific datatypes\\nThe ICS 002 Client submodule stands out from the other submodules as it needs\\nto deal with chain-specific datatypes, such as `Header`, `ClientState`, and\\n`ConsensusState`.\\nTo abstract over chain-specific datatypes, we introduce a trait which specifies\\nboth which types we need to abstract over, and their interface.\\nFor the ICS 002 Client submodule, this trait looks as follow:\\n```rust\\npub trait ClientDef {\\ntype Header: Header;\\ntype ClientState: ClientState;\\ntype ConsensusState: ConsensusState;\\n}\\n```\\nThe `ClientDef` trait specifies three datatypes, and their corresponding interface, which is provided\\nvia a trait defined in the same submodule.\\nA production implementation of this interface would instantiate these types with the concrete\\ntypes used by the chain, eg. Tendermint datatypes. Each concrete datatype must be provided\\nwith a `From` instance to lift it into its corresponding `Any...` enumeration.\\nFor the purpose of unit-testing, a mock implementation of the `ClientDef` trait could look as follows:\\n```rust\\nstruct MockHeader(u32);\\nimpl Header for MockHeader {\\n// omitted\\n}\\nimpl From<MockHeader> for AnyHeader {\\nfn from(mh: MockHeader) -> Self {\\nSelf::Mock(mh)\\n}\\n}\\nstruct MockClientState(u32);\\nimpl ClientState for MockClientState {\\n// omitted\\n}\\nimpl From<MockClientState> for AnyClientState {\\nfn from(mcs: MockClientState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockConsensusState(u32);\\nimpl ConsensusState for MockConsensusState {\\n// omitted\\n}\\nimpl From<MockConsensusState> for AnyConsensusState {\\nfn from(mcs: MockConsensusState) -> Self {\\nSelf::Mock(mcs)\\n}\\n}\\nstruct MockClient;\\nimpl ClientDef for MockClient {\\ntype Header = MockHeader;\\ntype ClientState = MockClientState;\\ntype ConsensusState = MockConsensusState;\\n}\\n```\\nSince the actual type of client can only be determined at runtime, we cannot encode\\nthe type of client within the message itself.\\nBecause of some limitations of the Rust type system, namely the lack of proper support\\nfor existential types, it is currently impossible to define `Reader` and `Keeper` traits\\nwhich are agnostic to the actual type of client being used.\\nWe could alternatively model all chain-specific datatypes as boxed trait objects (`Box<dyn Trait>`),\\nbut this approach runs into a lot of limitations of trait objects, such as the inability to easily\\nrequire such trait objects to be Clonable, or Serializable, or to define an equality relation on them.\\nSome support for such functionality can be found in third-party libraries, but the overall experience\\nfor the developer is too subpar.\\nWe thus settle on a different strategy: lifting chain-specific data into an `enum` over all\\npossible chain types.\\nFor example, to model a chain-specific `Header` type, we would define an enumeration in the following\\nway:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] // TODO: Add Eq\\npub enum AnyHeader {\\nMock(mocks::MockHeader),\\nTendermint(tendermint::header::Header),\\n}\\nimpl Header for AnyHeader {\\nfn height(&self) -> Height {\\nmatch self {\\nSelf::Mock(header) => header.height(),\\nSelf::Tendermint(header) => header.height(),\\n}\\n}\\nfn client_type(&self) -> ClientType {\\nmatch self {\\nSelf::Mock(header) => header.client_type(),\\nSelf::Tendermint(header) => header.client_type(),\\n}\\n}\\n}\\n```\\nThis enumeration dispatches method calls to the underlying datatype at runtime, while\\nhiding the latter, and is thus akin to a proper existential type without running\\ninto any limitations of the Rust type system (`impl Header` bounds not being allowed\\neverywhere, `Header` not being able to be treated as a trait objects because of `Clone`,\\n`PartialEq` and `Serialize`, `Deserialize` bounds, etc.)\\nOther chain-specific datatypes, such as `ClientState` and `ConsensusState` require their own\\nenumeration over all possible implementations.\\nOn top of that, we also need to lift the specific client definitions (`ClientDef` instances),\\ninto their own enumeration, as follows:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Eq)]\\npub enum AnyClient {\\nMock(mocks::MockClient),\\nTendermint(tendermint::TendermintClient),\\n}\\nimpl ClientDef for AnyClient {\\ntype Header = AnyHeader;\\ntype ClientState = AnyClientState;\\ntype ConsensusState = AnyConsensusState;\\n}\\n```\\nMessages can now be defined generically over the `ClientDef` instance:\\n```rust\\n#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]\\npub struct MsgCreateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub client_type: ClientType,\\npub consensus_state: CD::ConsensusState,\\n}\\npub struct MsgUpdateClient<CD: ClientDef> {\\npub client_id: ClientId,\\npub header: CD::Header,\\n}\\n```\\nThe `Keeper` and `Reader` traits are defined for any client:\\n```rust\\npub trait ClientReader {\\nfn client_type(&self, client_id: &ClientId) -> Option<ClientType>;\\nfn client_state(&self, client_id: &ClientId) -> Option<AnyClientState>;\\nfn consensus_state(&self, client_id: &ClientId, height: Height) -> Option<AnyConsensusState>;\\n}\\npub trait ClientKeeper {\\nfn store_client_type(\\n&mut self,\\nclient_id: ClientId,\\nclient_type: ClientType,\\n) -> Result<(), Error>;\\nfn store_client_state(\\n&mut self,\\nclient_id: ClientId,\\nclient_state: AnyClientState,\\n) -> Result<(), Error>;\\nfn store_consensus_state(\\n&mut self,\\nclient_id: ClientId,\\nconsensus_state: AnyConsensusState,\\n) -> Result<(), Error>;\\n}\\n```\\nThis way, only one implementation of the `ClientReader` and `ClientKeeper` trait is required,\\nas it can delegate eg. the serialization of the underlying datatypes to the `Serialize` bound\\nof the `Any...` wrapper.\\nBoth the `process` and `keep` function are defined to take a message generic over\\nthe actual client type:\\n```rust\\npub fn process(\\nctx: &dyn ClientReader,\\nmsg: MsgCreateClient<AnyClient>,\\n) -> HandlerResult<CreateClientResult<AnyClient>, Error>;\\npub fn keep(\\nkeeper: &mut dyn ClientKeeper,\\nresult: CreateClientResult<AnyClient>,\\n) -> Result<(), Error>;\\n```\\nSame for the top-level dispatcher:\\n```rust\\npub fn dispatch<Ctx>(ctx: &mut Ctx, msg: ClientMsg<AnyClient>) -> Result<HandlerOutput<()>, Error>\\nwhere\\nCtx: ClientReader + ClientKeeper;\\n```\\nWith this boilerplate out of way, one can write tests using a mock client, and associated mock datatypes\\nin a fairly straightforward way, taking advantage of the `From` instance to lift concerete mock datatypes\\ninto the `Any...` enumeration:\\n```rust\\n#[test]\\nfn test_create_client_ok() {\\nlet client_id: ClientId = \"mockclient\".parse().unwrap();\\nlet reader = MockClientReader {\\nclient_id: client_id.clone(),\\nclient_type: None,\\nclient_state: None,\\nconsensus_state: None,\\n};\\nlet msg = MsgCreateClient {\\nclient_id,\\nclient_type: ClientType::Tendermint,\\nconsensus_state: MockConsensusState(42).into(), // lift into `AnyConsensusState`\\n};\\nlet output = process(&reader, msg.clone());\\nmatch output {\\nOk(HandlerOutput {\\nresult,\\nevents,\\nlog,\\n}) => {\\n// snip\\n}\\nErr(err) => {\\npanic!(\"unexpected error: {}\", err);\\n}\\n}\\n}\\n```\\n\n\n## Context and Problem Statement\\nThis is part of a body of work necessary to support the use of Nimbus within web applications.\\nThe current Nimbus SDK is written in such a way that it supports client-oriented experimentation \u2014 experiments are downloaded, evaluated, and stored on the client, and a feature store is exposed with the experiment branches applied.\\nIn previous decisions (not in this repository), we've already decided that in order to support web experimentation the Nimbus SDK will need to be updated to be stateless, to support a more statically defined set of helper methods, and to have additional support for Python.\\nUltimately, the problem we're trying to solve can be boiled down to one question \u2014 how can we update the Nimbus SDK to support web applications while continuing to support the existing clients?\\nAs an example of what the Cirrus API might look like, we can likely expect endpoints to perform the following:\\n* Enroll a user into available experiment(s)\\n* This would return the enrolled experiments as well as the feature values given the enrollments\\n* Fetch the default feature values\\n* Fetch the feature manifest\\n* Fetch a specific feature value given enrolled experiments\\n## Decision Drivers\\n* Architecture decisions for Nimbus on the web[<sup>[1.i]</sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\n\n## Decision\n* Architecture decisions for Nimbus on the web[<sup>[1.i]</sup>](#links)\\n* The core logic of the existing SDK (Rust) will be re-used for the Nimbus web service (Cirrus)\\n* The SDK needs to support Python via UniFFI\\n* The SDK needs to be stateless\\nWe have decided to move forward with option number 2, \"Cargo features, one library\".\\nThis option, like the others mentioned, meets the key decision drivers.\\nWe believe using this option will be the most maintainable long-term, despite the added complexity of using cargo features.\\nIn addition, implementing this option has a similarly short timeline and amount of work necessary as compared to the \"Cargo features, separate library\" option, without the additional overhead of more complex typing.\\n### Positive Consequences\\n* Meets all decision drivers.\\n* Small amount of work necessary with a very limited amount of code churn.\\n### Negative Consequences\\n* It will be difficult to draw boundaries between Cirrus and Nimbus code.\\n* We could run into unexpected issues with UniFFI, as multiple UDL files in a single crate has not yet been tested.\\n2. [Cargo Features](https://doc.rust-lang.org/cargo/reference/features.html)\\n2. Examples\\n1. [Multi-module typing example](https://github.com/jeddai/application-services/pull/1/files#diff-02305e8e02a7900352e67be1fd2eef0b5a7c7cf91f4cc3e4559668c124d34e88R11-R19)\\n\n\n## Context and Problem Statement\\nThe original goal for the reference implementation refactor was to provide an\\nimplementation which is both an aid to understanding the specification and a\\ngood architecture for other implementations to mimic.\\nDuring refactoring efforts on the metadata API and ngclient, several friction\\npoints have arisen where a safe object-oriented API would result in a less\\ndirect mapping to the [Document formats] in the specification.\\nThe archetypal example friction point is that [Timestamp] lists snapshot _only_\\nin a `meta` dictionary of `METAPATH` -> attribute fields. The dictionary will\\nonly ever contain one value and creates an extra level of indirection for\\nimplementations which try to map to the file format.\\nWhen presented with such cases, we have considered multiple options:\\n* Strict mapping to the [Document formats]\\n* Simple and safe API in preference to mapping to the [Document formats]\\n* Strict mapping to the [Document formats] with additional convenience API\\nwhich is documented as the preferred interface for users\\nSo far implementation has tended towards the final option, but this is\\nunsatisfying because:\\n* the API contains traps for the unsuspecting users\\n* two code paths to achieve the same goal is likely to result in inconsistent\\nbehaviour and bugs\\nTherefore, we would like to define our primary purpose so that we can make\\nconsistent decisions.\\n[Document formats]: https://theupdateframework.github.io/specification/latest/#document-formats\\n[Timestamp]: https://theupdateframework.github.io/specification/latest/#file-formats-timestamp\\n## Decision Drivers\\n* The reference implementation is often the starting point for new\\nimplementations, porting architecture of the reference implementation to new\\nlanguages/frameworks\\n* Reading reference implementation code is a common way to learn about TUF\\n* The TUF formats include non-intuitive JSON object formats when mapping to OOP\\nobjects\\n* Multiple code paths/API for the same feature is a common source of bugs\\n\n## Decision\n* The reference implementation is often the starting point for new\\nimplementations, porting architecture of the reference implementation to new\\nlanguages/frameworks\\n* Reading reference implementation code is a common way to learn about TUF\\n* The TUF formats include non-intuitive JSON object formats when mapping to OOP\\nobjects\\n* Multiple code paths/API for the same feature is a common source of bugs\\nPrimary purpose of the reference implementation is as an exemplary reference:\\nproviding a safe, consistent API for users and a good architecture for other\\nimplementations to mimic.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\\ninterfaces\" - possibly leading to the impression that there is such a thing as\\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\\nthe only question is who manages this thread-safety. Interfaces which are not\\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\\nhidden in the generated code and therefore not obvious to the casual reader.\\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\\nmanaging their own locking internally. This ADR proposes that uniffi forces\\ncomponent authors to explicitly manage that locking in all cases - or to put\\nthis in Rust terms, that all structs supported by uniffi must already be\\n`Send+Sync`\\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\\nany misunderstandings that might be caused by using the somewhat broad and\\ngeneric \"Threadsafe\".\\n## Decision Drivers\\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https://github.com/mozilla-mobile/fenix/issues/17086)\\n(with more details available in [this JIRA ticket](https://jira.mozilla.com/browse/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https://github.com/mozilla/uniffi-rs/issues/417) or to\\n[pass interface references over the FFI](https://github.com/mozilla/uniffi-rs/issues/419).\\n\n## Decision\n", "response": "## Decision\nAll structs supported by uniffi must already be `Send+Sync`. This decision eliminates the hidden complexity and potential performance issues associated with automatically wrapping interfaces in a mutex. It ensures that component authors manage thread-safety explicitly, making the code's thread-safety guarantees clear and maintainable.\n\n### Positive Consequences\n* Clear and explicit thread-safety guarantees, reducing confusion and potential bugs.\n* Avoids unintended blocking behavior due to hidden mutexes, improving performance and responsiveness.\n* Facilitates future growth of uniffi in supporting alternative method receivers and passing interface references over FFI, as these features rely on a clear understanding of struct thread-safety.\n\n### Negative Consequences\n* Increased responsibility on component authors to ensure their structs are `Send+Sync`, potentially increasing the complexity of implementation.\n* Authors may need additional education or support to successfully implement `Send+Sync` capabilities correctly.\n\nThis approach aligns with the Rust philosophy of explicitness and safety, while ensuring uniffi's evolution can meet practical needs in library development.", "matched_ids": [4800, 1979, 4656, 1819, 4252], "time": "0:00:05.357699", "input_tokens": 7340, "output_tokens": 209}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael\\nNygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n", "response": "## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n", "matched_ids": [867, 463, 4823, 1833, 3988], "time": "0:00:02.494098", "input_tokens": 411, "output_tokens": 42}
{"context": "## Context and Problem Statement\\nWe found that without a standardised format our javascript files ended up with different\\nformats in different files or even multiple formats in the same file.  We also found that\\nour IDEs had different configurations which meant that using an autoformat tool would give\\ndifferent results when each of us do it.\\n## Decision Drivers\\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https://prettier.io/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https://github.com/prettier/prettier/network/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n## Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n\n\n## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https://prettier.io/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https://github.com/prettier/prettier/network/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n## Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n\n\n## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https://prettier.io/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https://github.com/prettier/prettier/network/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n## Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n\n\n## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https://prettier.io/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https://github.com/prettier/prettier/network/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n## Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n\n\n## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https://prettier.io/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https://github.com/prettier/prettier/network/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, including React itself, and has become a standard.\\n\n## Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\nWe found that without a standardised format our javascript files ended up with different\\nformats in different files or even multiple formats in the same file.  We also found that\\nour IDEs had different configurations which meant that using an autoformat tool would give\\ndifferent results when each of us do it.\\n## Decision Drivers\\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\n\n## Decision\n", "response": "## Decision\n\nWe will adopt a standardized code formatter to ensure consistent formatting across all JavaScript files. We will use [Prettier](https://prettier.io/), as it is widely adopted and supports the JavaScript ecosystem comprehensively. \n\nWe will configure all IDEs and editors in the team to use Prettier as their default formatter. \n\nTo enforce this standard, we will implement Git hooks to automatically format code with Prettier before any changes are committed. \n\nAdditionally, our continuous integration (CI) pipeline will reject any pull requests that do not conform to the Prettier formatting rules. \n\nThis approach will reduce manual formatting efforts, maintain code consistency, and streamline code reviews by highlighting only meaningful code changes.", "matched_ids": [5255, 1408, 4226, 3760, 454], "time": "0:00:02.999446", "input_tokens": 1231, "output_tokens": 148}
{"context": "## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nNot all inputs or args (= parameters) are always required.\\nOften there are sensible defaults available. We want to be able to make those defaults explicit where possible.\\nEven though some parameters may be optional, they might become required in the presence of other optional parameters.\\nWe need a way to explicitly define what combinations are possible.\\n\n## Decision\nWe drop the `optional` property on parameters. Instead, parameters get an additional property `defaultValue`. It can be\\nset to either a fixed literal value (e.g. `7`, `\"something\"`, `null`), an Arg, or it may reference the specific methods\\n`shape()` and `dataType()` on inputs and outputs. Parameters with `defaultValue` specified are treated as optional.\\nTo be able to deal with languages that do not support default values for arguments, Signatures will be specified.\\nSignatures are specified using a `Signature(a,b,c){ \"signature specific documentation\" }` section for each signature.\\nWith the signature specific documentation being optional.\\nSignatures making use of outputs will only be generated for NDArray programming mode, not in SameDiff mode. This also\\nmeans that parameters with a `defaultValue` based on an output will be treated as required in SameDiff mode.\\nIf signatures are specified, only the specified signatures will be generated.\\nIf no signatures are explicitly specified, only the \"all-arg\" and \"no-optional-arg\" signatures will be generated. In\\nNDArray programming mode, the default signatures also include a variant that includes the output.\\n\n\n## Context\\nIt is easy to use different terms meaning the same thing in code. A common aphorism is that the most difficult thing in coding is naming.\\n\n## Decision\nA `Terminology.md` document will be created, that will be used to collect the official terms for the project. If naming diverts from this terminology, it should be considerd a bug.\\n\n\n## Context\\nSeveral parts of the pipeline execute operations on specific columns.\\n\n## Decision\nWhere there was an easy way to do so, we gave users the options to supply variable names:\\n* checking whether all expected columns are present\\n* coercing data types.\\nIn other sections of the pipeline, variable names had to be hardcoded:\\n* cleaning variables (replacing missing values with NA)\\n* deriving variables\\n* deriving row quality and flagging duplicates (optional)\\n\n\n## Context\\nThere are slight inconsistencies in naming between existing op class definitions and factory methods. For example a\\nfactory method called `bernoulli` in the `random` namespace with a corresponding op class called\\n`BernoulliDistribution`.\\nTwo possible solutions where suggested:\\n1. Add an additional property that provides us with the correct class name\\n2. Rename classes in ND4J to ensure consistency and provide backwards compatibility via deprecated subclasses\\n\n## Decision\nFor now we will introduce a `javaOpClass` property which in cases of inconsistency provides us with the correct class\\nname.\\n\n\n## Context\\n\n## Decision\nThe project is build with the MERN Stack.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n\n## Decision\n", "response": "## Decision\nWe will introduce an enumeration (`enum`) to represent the modes for these ordinal parameters. Each mode will be given a descriptive name within the enum. This approach will improve code readability and maintainability by providing clear, self-documenting names for each mode. Documentation will be updated to reflect the use of these enums, and code examples will demonstrate their usage. Additionally, existing code will be refactored to replace ordinal values with the corresponding enum constants wherever applicable.", "matched_ids": [2940, 2270, 3256, 2934, 4797], "time": "0:00:03.004504", "input_tokens": 793, "output_tokens": 94}
{"context": "## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nThe approach I have chosen (see docs/model.md and docs/algorithm.md) leads to\\nmany small unit-testable components and a main function that ties them together.\\nThe options I can see for the main function are:\\n1. Write it with unit tests, heavily mocking the dependencies\\n2. Use a type system to check that the plumbing matches up, don't unit test the\\nmain function\\n3. Don't test the main function, don't use types. My guess is I'll make some\\nmistake along the way and have to debug it.\\nIn all cases I'll run integration tests, but without (1) or (2) I won't know\\nwhere bugs are when the integration test fails.\\nMy guess is that (2) will be more efficient than (1), and either of them will be\\nmore efficient than (3).\\nFor JavaScript static typing, I'm familiar with TypeScript.\\n\n## Decision\nUse TypeScript.\\n\n\n## Context\\nAdding static types to a JavaScript project can be incredibly useful to reduce the likelihood of introducing runtime type errors.\\nWe have considered two methods of adding static types to dotcom-rendering.\\n### Flow\\nFlow makes it easy to retrofit a useful level of implicit type checking to a legacy project.\\nHowever, it produces error messages that are difficult to understand.\\nFlow requires developers to check in types for 3rd party libraries. These can be automatically generated based on the `package.json`, but developers must remember to keep these up to date.\\nIts configuration is often too coarse-grained, meaning code gets littered with opt-outs and `any` types. It doesn't feel like a natural type system, as it is only annotations on top of a dynamic language.\\nFlow's types are not exact by default.\\nThe ecosystem is still immature.\\n### TypeScript\\nTypeScript produces more understandable error messages.\\nIt allows developers to install versioned types for third party libraries.\\nTypeScript is a language in its own right, and feels more natural. It can improve JavaScript language features to make them more type-friendly. Its configuration is more fine-grained.\\nTypes are exact by default.\\nThe ecosystem is more mature.\\n\n## Decision\nUse TypeScript for static typing.\\n\n\n## Context\\nWe want to be confident about the code we write, and for it to be\\nself-documenting as much as possible.\\n[TypeScript](https://www.typescriptlang.org/) is a compiled language with\\noptional typing. It's a superset of JavaScript, so is familiar to developers who\\nknow JavaScript. It has wide editor support.\\nAs of writing, TypeScript is used by over\\n[1.4 million repositories](https://github.com/microsoft/TypeScript/network/dependents?package_id=UGFja2FnZS01MTE3ODUxNjg%3D)\\non GitHub.\\n\n## Decision\nWe will use TypeScript.\\n\n\n## Context\\nWe must allow developers to use new JavaScript syntax and features without excluding older execution environments. Code must be automatically checked against common development mistakes and optimised for download and execution.\\n\n## Decision\nWe acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.\\n\n\n## Context\\nWe need a way to describe the structure of Javascript objects for readers of the code to be able to understand the structure.\\nAnd we need a way to detect null references at build time Javascript.\\nOur Javascript passes though a build step.\\nOur Javascript is maintained by people who did not write the code.\\n\n## Decision\nWe will use Typescript as a tool to describe object structure.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n\n## Decision\n", "response": "## Decision\nWe will use TypeScript for developing the Alfa code base. TypeScript is a superset of JavaScript with a strong type system that can enforce API contracts and catch common bugs like `undefined` or `null` values during the development phase. It integrates smoothly with existing JavaScript and runs in the browser, ensuring compatibility with browser-based tools such as our Chrome extension. By adopting TypeScript, we leverage its robust type-checking capabilities while maintaining the flexibility and ubiquity of JavaScript for browser environments.", "matched_ids": [2290, 2652, 3762, 2687, 1515], "time": "0:00:03.312584", "input_tokens": 1274, "output_tokens": 105}
{"context": "## Context and Problem Statement\\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\\nWhich query syntax should be used for the abstract queries?\\nWhich features should the syntax support?\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context and Problem Statement\\nTo build custom dashboards we need to define queries that are too complex for the existing filter box form.\\nWe would like to use flexible search expressions based on a proper query language.\\n\n## Decision\nChosen option: \"Monocle Query Language\", because it comes out best (see below).\\n### Positive Consequences\\n- We improve the user experience by replacing the clunky filter box with a simpler search bar.\\n- We create a re-usable component.\\n### Negative Consequences\\n- We need to maintain a language toolchain.\\n\n\n## Context\\nAs our users base is growing and we have more and more search queries, we would like to improve current search form to:\\n* Be able to search on newly added fields (e.g. created date, modified date, author, licence, etc)\\n* Allow more complex searches on existing and new fields such as:\\n* \"and\" queries to search on multiple tags\\n* \"or\" queries to search on an exclusive list of tags\\n* \"startsWith\" and \"contains\" search instead of current exact match searches\\n* synonyms\\n\n## Decision\nGiven our needs it seems pretty clear that we need a search engine, as databases search capabilities are limited and would cost more and more as data is growing.\\nWe will use Elastic Search as our search engine as it is the de facto standard and is based on battle-tested search engine\\nLucene. Our current microservices will now evolve to this schema:\\n![](microservices-architecture-with-es.png)\\nNote: ES indexes do not need to be updated in real-time when a new image/gif is uploaded, in fact we can easily\\ntolerate a few minutes of delay between an upload and its actual availability for search. All we need is to be\\neventually consistent.\\n\n\n## Context\\nFollowing user research a more comprehensive text search engine was needed in order to be able to\\nuse more complex searches (and/or), full phrase search, stemming as well as geolocation searches.\\n\n## Decision\nA review of ElasticSearch and Funnelback was undertaken [here](https://docs.google.com/document/d/10xK0on4gzrB1ImZHk3Z_PoCr7V_aUpiieuLdQDg4OwQ/edit)\\nand [ElasticSearch](https://www.elastic.co/products/elasticsearch) was chosen as a candidate due to the fact that it was\\nopen source and had comprehensive documentation with an established community around it.\\n(Funnelback)[https://www.funnelback.com/] was discounted due to it being closed-source and for\\nit's lack of community support.\\nA  Docker image of GP data ([profiles-db-elastic](https://hub.docker.com/r/nhsuk/profiles-db-elastic/))\\nrunning an ElasticSearch instance has been created for use by this application and possibly by other applications.\\n\n\n## Context and Problem Statement\\nThe study holds query and library entries that could be replaced respectively with complex query and fetcher instances.\\nThis poses the question: should the study remain a pure DTO object or should it contain direct object instances?\\n\n## Decision\nChosen option: \"Keep study as DTO and use transformators\", because comes out best (see below).\\n\n\n## Context\\nWe need a search engine to which we will send user queries, and from which we will receive their results.\\nMoreover, we need an engine that can scale with our data and be resilient to faults (network, filesystem, etc.).\\n\n## Decision\nThe current UniProt website uses lucene as the search engine. This is very fast. However, the drawback is that it does not easily scale.\\nThis can be provided by Solrcloud. We have used this for the Proteins API with success, and therefore have 4 years of experience with it.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\\nWhich query syntax should be used for the abstract queries?\\nWhich features should the syntax support?\\n\n## Decision\n", "response": "## Decision\nChosen option: \"Unified Search Query Language (USQL)\" because it provides a balance between human readability and machine parsing, and can be easily translated to different library-specific syntaxes with minimal ambiguity.\n\n### Positive Consequences\n- USQL offers a simplified yet powerful syntax that can handle most complex queries an end-user might want to perform.\n- The language includes support for logical operations (AND, OR, NOT), range queries, wildcards, and grouping, which are essential for a comprehensive search capability.\n- It simplifies the parsing process into an AST while retaining the flexibility to adapt to different library-specific search implementations.\n- By having a common syntax, it increases the usability for users needing to interact with multiple libraries.\n\n### Negative Consequences\n- Implementing the translation layer for each specific library will require initial development effort.\n- Users familiar with a particular library's native syntax might face an initial learning curve when transitioning to USQL.\n- Continuous maintenance is needed to ensure that any updates or extensions to the libraries' native syntaxes are reflected in the USQL translation layer.", "matched_ids": [4637, 2513, 3472, 4741, 1476], "time": "0:00:05.317945", "input_tokens": 1048, "output_tokens": 216}
{"context": "## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\\nGovUK services have a \"trial mode\", and a \"live mode\".\\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\n\n## Decision\nIt was decided to pass the entire Keycloak message into the GovUK Notify template via the `keycloakMessage` personalisation parameter, rather than having a mix of text and parameters in the Keycloak template.\\nThis is because the Keycloak message is passed in as a complete constructed string. It would be very difficult to extract particular portions of this string, for example a link for use within the GovUK Notify template.\\nThis approach does mean it is harder for non-developers to make changes to the email messages. The decision may be revisited in the future if this proves to be causing issues.\\n\n\n## Context\\nIt was decided to set the GovUK Notify Staging service to go live: [0015 GovUK Notify Staging Configuration](0015-govuuk-notify-staging-configuration.md)\\n\n## Decision\nThe GovUK Notify team recommended that usually they do not go live with non-production services.\\nIt was decided to not go live with the GovUK Notify Staging services following this advice from the GovUK Notify team.\\nIt is still possible to send emails to users participating with UAT via adding their email addresses to the service's guest list.\\nThe Staging service is sufficiently consistent with the Production service if it were to remain in \"trial mode\".\\nFor full details of the \"trial mode\" for GovUK Notify services see here: https://www.notifications.service.gov.uk/using-notify/trial-mode\\n\n\n## Context\\nMost public-facing digital services have a need to notify its users about something at a critical business process, or just to manage their expectations to prevent incoming contact.\\nThe notifications can, for example, be:\\n* Telling a supplier they have a task to complete,\\n* Telling a supplier that a deadline is approaching,\\n* Telling a supplier that a buyer has been shortlisted for a procurement,\\n* Providing a receipt,\\n* Providing status updates,\\n* Etc...\\nGenerally, the notifications will be email, but may also need to be SMS in some cases.\\nTo reduce the need for each organisation or service team to build its own infrastructure for sending email and SMS based messages, the Government Digital Service (GDS) has built GOV.UK Notify.\\nGOV.UK Notify can send notifications either via an API call, or by uploading a CSV to the website. Sending email is free, and the service provides feedback on what messages have failed to be delivered.\\nThe service is already used in various parts of CCS, including MISO, Report MI, DigiTS etc.\\nUsing GOV.UK Notify instead of other solutions (eg using the BCC field in an email) also reduces the potential for an accidental data breach.\\n\n## Decision\nWe will use GOV.UK Notify to send notifications to users of any newly built CCS digital services.\\nThis will make sure Notify offers the same user experience across all CCS digital services, and it is currently the most cost-effective (having no-cost for emails) bulk messaging 3rd party solution on the market.\\nThe dashboard to create new Notify templates and send bulk emails is very easy, and new Service Teams at CCS can easily be onboarded to use it, with minimal/no training.\\nThis ADR does not apply to generic Email Newsletter communication, for which other, more market specific tools may be more appropriate.\\n\n\n## Context\\nThe Data Submission Service will need to send notifications to suppliers at\\nvarious stages during the submission process.\\nThe notifications will, for example, include:\\n- Telling a supplier they have a task to complete\\n- Telling a supplier that a deadline is approaching\\n- Telling a supplier that their task is overdue\\n- Providing a receipt for a submission\\nInitially, the notifications will be email, but may also need to be SMS based in\\nfuture.\\nTo reduce the need for each organisation or service team to build it's own\\ninfrastructure for sending email and SMS based messages, the Government Digital\\nService (GDS) has built [GOV.UK Notify][service-notify].\\nGOV.UK Notify can send notifications either via an API call, or by uploading a\\nCSV to the website. Sending email is free.\\nThe service is already used in various parts of CCS.\\n\n## Decision\nWe will use GOV.UK notify to send notifications for Data Submission Service.\\nFor MVP we may use the CSV upload function to send notifications, but this will\\nbe replaced by automated API calls later in development.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThis service has a need to send emails and SMS to users. As Hackney is approved as an organisation that can use GOV.UK Notify we are able to use this service and take advantage of the reliability and low costs.\\nHackney already use Notify for other services, although it is not yet referenced in their playbook https://github.com/LBHackney-IT/API-Playbook\\n\n## Decision\nUse Notify over a third party service such as SendGrid for the sending of emails and SMS\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\\nGovUK services have a \"trial mode\", and a \"live mode\".\\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\\n\n## Decision\n", "response": "## Decision\nIt was decided to use the \"live mode\" for each TDR environment's separate GovUK Notify service. This decision was made to ensure that there are no restrictions on who can receive emails and that the volume of emails is not limited, which is critical for the functioning of various TDR environments. By opting for \"live mode,\" we can provide a consistent and fully functional notification experience across all environments without the constraints imposed by \"trial mode\". This approach allows each environment to operate independently with its respective audience, aligning with the multi-environment configuration strategy already in place. However, careful monitoring and management of each environment's use of the service will be necessary to maintain compliance with GovUK Notify's operational guidelines.", "matched_ids": [1786, 1767, 996, 2050, 5213], "time": "0:00:05.199120", "input_tokens": 1421, "output_tokens": 145}
{"context": "## Context\\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need to be able to monitor network services separately.\\n\n## Decision\nWe will use _akka_ for managing the task to monitor a particular network service.\\n\n\n## Context\\nA core part of the Modernisation Platform is its network topology and configuration and we naturally want confidence that it exists in its desired state to meet security, compliance and best-practise requirements. We want to use the right products and tools that offer networking monitoring and testing capabilities to provide us with this confidence.\\n\n## Decision\n[VPC flow flows](https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html) contain information about data coming into and out of attached network interfaces. As such, flow log data could be collected, aggregated, analysed and visualised in order to provide insights into the traffic flowing (or not) through VPCs. VPC flow logs are already collected in environment accounts and at the platform-level in the core-logging account, within CloudWatch log groups.\\n\n\n## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n\n## Decision\nWe will use Prometheus to monitor and alert on the state of the application.\\n\n\n## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n\n## Decision\nWe will use Prometheus to monitor and alert on the state of the application.\\n\n\n## Context\\nWe want to provide metrics about the status of a monitored service.\\n\n## Decision\nWe will use _Dropwizard_ for creating the metrics.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\\n\n## Decision\n", "response": "## Decision\nWe will implement Datadog to provide continuous monitoring for the TTA Smart Hub. Datadog offers a comprehensive suite for monitoring application performance, infrastructure, and logs which allows for real-time performance tracking and proactive error detection, thereby ensuring uptime and facilitating quick error resolution.", "matched_ids": [1624, 3150, 3469, 4400, 1623], "time": "0:00:03.260813", "input_tokens": 569, "output_tokens": 57}
{"context": "## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https://stylelint.io/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https://eslint.org/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript and React though plugins.\\n\n## Decision\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n\n\n## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https://eslint.org/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript and React though plugins.\\n\n## Decision\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n\n\n## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https://eslint.org/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n\n## Decision\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n\n\n## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [ESLint](https://eslint.org/) is the standard linter for modern\\nJavaScript, and has good support for TypeScript though plugins.\\n\n## Decision\nWe will check code style using ESLint.\\nWe will let Prettier have precedence when ESLint and Prettier conflict in their\\nstyles.\\nWe will use the recommended configuration for plugins where possible.\\nWe will run ESLint as part of the test suite.\\n\n\n### Context\\n- linting for CSS\\n- After running it, it didn't pick up a single error\\n### Decision\\nDon't start using stylelint\\n\n## Decision\nDon't start using stylelint\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https://stylelint.io/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n\n## Decision\n", "response": "## Decision\nWe will enforce code style consistency using Stylelint.\\nWe will adopt the standard Stylelint configuration, extending it as necessary to accommodate our CSS/SASS preferences.\\nWe will integrate Stylelint into our development workflow and ensure it runs as part of our continuous integration pipeline.\\nWe will periodically review and update the Stylelint configuration to match current best practices.\\n", "matched_ids": [3330, 4173, 1404, 4224, 5084], "time": "0:00:03.594084", "input_tokens": 812, "output_tokens": 75}
{"context": "## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nAs with any modern web system there is a need to secure the application with\\nstrong authentication and Role Based Access Control (RBAC). RBAC allows us to\\ncreate permissions, apply them to roles and associates roles with users. From\\nthis we can restrict what users can do and see within the application based on\\ntheir role. In addition to this we also had to investigate where to store role\\ninformation and define teams to support our use of RBAC.\\n\n## Decision\nAfter some investigation and discussion into various ways of implementing RBAC\\nwithin Ruby we have decided to implement the Pundit Ruby gem within the Verify\\nSelf Service application. This gem provides us with a way of creating policies\\nwhich apply to the app as a whole and to individual classes within the\\napplication.\\nWe have chosen pundit for a number of reasons. The first is that it seems\\nrelatively easy to implement within our application. A proof of concept was\\ncreated in a few hours which restricted creating new components within the\\napplication to those who hold the `gds` or `dev` roles. Policies and permissions\\nare defined in code as plain Ruby objects which are easy to understand and\\nsimple to implement. This ease of implementation and an appearance of providing\\nthe fuctionality we are seeking we believe makes pundit a good candidate for\\nimplementing RBAC within our application.\\nIn addition to this we are aware of prior-art use within GDS on the GOV.UK\\nproject. Finally we have found a lot of documentation and articles about how to\\nimplement and use pundit.\\nRole information we have decided to store in a custom field within our cognito\\ninstance. This will take the form of a comma seperated list which will be split\\ninto an array of roles for use within the application. This method allows us to\\nkeep all use information together within AWS Cognito and means that the only way\\nrole information can be changed is via SDK calls to AWS Cognito.\\nFinally we understand there is a need to define teams within our application. We\\nhave decided that a team is analogous to an AWS Cognito group. This allows a\\nuser to be part of one more groups and for a group to hold one or more users. We\\ncan easily query group membership using the AWS Cognito SDK and check for a\\nusers membership to a specific group.\\n\n\n## Context\\nIn Armadillo:\\n* A cohort is an OpenID application\\n* The researcher needs a `ROLE_<STUDY>_RESEARCHER` on the application to read data\\nin shared folder `<study>`\\nRequirements on who manages what:\\n* User and permission management is done by representatives of the consortium and cohort\\n* The identity of the user is federated to their local institution\\n* Representatives of the cohort administer the study researchers\u2019 admission to their cohort\\n* Representatives of the consortium can see which users have which roles on which cohort\\n\n## Decision\nImplement this in Fusion Auth, plus a (modest) administration UI.\\n* The users register themselves.\\n* A cohort representative explicitly creates a registration for the researcher in the application,\\nwith one or more roles.\\n### Administration\\nApplication-specific permissions are not available in Fusion Auth.\\nIn other products they tend to be rather complicated to configure.\\nCreate an edge service that enforces the permissions and uses\\nan api key to administer them through the Fusion Auth API.\\n\n\n## Context\\nSome modules have components which require many acl privileges. Examples\\nare the rule builder or the media manager. Therefore, you need all privileges\\nin each module which have these components. Also you do not want to add the\\nmodule to the dependency section because then the user has full access to module\\nin the administration.\\n\n## Decision\nTo avoid duplication of these privileges we use a helper function. These\\nfunction returns all privileges from the other module dynamically. You can\\nuse it directly in the privileges:\\n```js\\nShopware.Service('privileges')\\n.addPrivilegeMappingEntry({\\ncategory: 'permissions',\\nparent: null,\\nkey: 'promotion',\\nroles: {\\nviewer: {\\nprivileges: ['promotion:read',],\\ndependencies: []\\n},\\neditor: {\\nprivileges: [\\n'promotion:update',\\nShopware.Service('privileges').getPrivileges('rule.creator')\\n],\\ndependencies: [\\n'promotion.viewer'\\n]\\n}\\n}\\n});\\n```\\n\n\n## Context\\nWhen designing APIs to handle metadata about datasets, editions and versions\\nno particular limitations were placed on which order these documents should be\\nedited in.\\nIn the existing publishing workflow, a single item can be added to only one\\ncollection at a time. As the APIs have separated formerly joined concepts, it\\nis necessary to understand whether datasets can be edited or published in\\nisolation from versions.\\n\n## Decision\nAdding a version to a collection does not prevent the dataset being edited in a different collection\\n\n\n## Context\\nThe OpenFIDO project will mediate access to a workflow service that runs\\nGridLabD jobs. The frontend React application will need access to these\\nresources, but only if they have permission to access via an organization.\\n\n## Decision\nCreate a Flask Rest service - since the other microservices in this suite of\\nservices are also Flask based, keep this using the same kind of infrastructure\\nso that common utilities can be shared (openfido-utils) and the maintenance will\\nbe simplified.\\nOrganize the database logic into a simplified CQRS-inspired style code\\nstructure. Since we anticipate many conceptual resources, each resource will\\nhave its own module:\\n* app/RESOURCE/models.py - contains all models for RESOURCE.\\n* app/RESOURCE/routes.py - contains all Rest routes specific to RESOURCE.\\n* app/RESOURCE/schemas.py - contains all Marshmallow schemas specific to routes of RESOURCE.\\n* app/RESOURCE/services.py - all db commands that modify database state.\\n* app/RESOURCE/queries.py - all db queries to the database.\\nAdditional libraries we anticipate using:\\n* [marshmallow](https://marshmallow.readthedocs.io/en/stable/) will be used since there are many rest api endpoints that will take nested bodies.\\n* [alembic](https://alembic.sqlalchemy.org/en/latest/) to manage database schema migrations.\\nOther thoughts on design:\\n* Because HTTP errors are intended for human use via the OpenFIDO client, messages should be verbose - showing both a generic message and specific field validation messages where appropriate.\\nUse the structure of the [openfido-workflow-service](https://github.com/slacgismo/openfido-workflow-service) project as a reference.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n\n## Decision\n", "response": "## Decision\nImplement a unified access control service that will check both author-based permissions and ACLs for a given resource. This service will be responsible for collating permissions from both sources and making a final determination on access rights. The design of the service will follow these steps:\n\n1. **Permission Aggregation**: Develop a middleware component which intercepts access requests and consults both the author assignment data and the ACLs.\n2. **Prioritization Logic**: Implement a logic layer that first checks explicit permissions by the resource author (depositor or proxy) and then supplements with ACL-derived permissions. If author permissions grant or deny access, prioritize these over ACLs.\n3. **Cache Results**: Utilize caching for frequently accessed resources to minimize repeated checks of static ACL entries and author permissions, maintaining a balanced load on the authorization system.\n4. **Auditing and Logging**: Maintain an audit trail for access checks to facilitate troubleshooting and analysis. Log each check, noting whether permissions were derived from author roles or ACL entries.\n5. **User and Group Management**: Integrate with existing user and group management systems to keep ACLs in sync with organizational roles and changes to contributor lists.\n\nBy consolidating author-level access and ACL-derived permissions into one cohesive system, we can ensure efficient and accurate permission management. Additionally, comprehensive documentation and API support will be provided to ease integration into existing resource management workflows.", "matched_ids": [3950, 4292, 4525, 5114, 2470], "time": "0:00:08.293016", "input_tokens": 1652, "output_tokens": 283}
{"context": "## Context\\nThe Data Migrate gem has caused us a number of issues in the past, it runs\\nsilently as part of a deploy, and this can result in surprising errors\\nduring a deploy. We've also had issues with the gem itself - most recently\\na bug in a new version causing strange errors in deployment.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe noticed that deployments to staging were silently failing since a gem update that succeeded in Travis did not succeed in our docker environment, where docker is used on our live environment.\\ndxw have a default stance to test with containers where we host with containers: https://github.com/dxw/tech-team-rfcs/blob/main/rfc-013-use-docker-to-deploy-and-run-applications-in-containers.md\\n\n## Decision\nTravis uses Docker containers to test the application on every build and deploy.\\n\n\n## Context\\nSee also ADR 0002 \"Database migrations are standalone releases\"\\nAs our system is designed for zero downtime, we have to be careful that\\nwe don't change the database in a way that causes production issues\\n\n## Decision\nWhere possible, we should avoid database migrations that will lock the database\\nfor any significant amount of time.  This is hard to enforce, but we will\\nmake sure there is documentation in the project README (and here!) on ways\\nto achieve this.\\nThis mostly affects index creation and changes - we have several years of data\\nin our database, and adding or changing indexes can be slow.  In general,\\nyou should use the `CREATE INDEX CONCURRENTLY` option to let indexes be\\ncreated in a non-blocking way.  See https://www.postgresql.org/docs/current/static/sql-createindex.html\\nIf you want to `ALTER INDEX` or `REINDEX`, they can't be concurrent - in this\\ncase you'll need to look at stopping the Event Recorder lambda, allowing messages\\nto queue up while the index change is made.  *BEWARE* however that SQS queues\\nonly allow 100,000 messages, and at peak load we have historically sent 75,000\\nmessages an hour, so you have a somewhat limited amount of time to run such a change.\\nIf you have a very complex change, you should consider:\\n- Dropping the index then running `CREATE INDEX CONCURRENTLY` rather than\\naltering indexes - generally our reports run intermittently, so it is safe to have\\nno indexes for a period of time, data will still be appended with no problems\\n- Performance testing the change - we have a large fake dataset available that\\ncan be used to simulate a production database in a test environment\\n- Duplicating the database - you could apply the change to a new database containing\\na copy of production data, then switch databases, and migrate any missed changes\\nfrom the old database to the new.\\n### Transactional DDL changes\\nMost Postgresql schema changes can be made transactionally - this is\\na great feature, as it allows for making multiple changes and having them\\nall roll back if something goes wrong.  For example:\\n```\\nBEGIN;\\nALTER TABLE fizzbuzz RENAME COLUMN foo TO bar;\\nUPDATE TABLE fizzbuzz set foo = 'splat';\\nCOMMIT;\\n```\\nIn this case the `UPDATE` will fail, so the column rename will be reverted.\\n*However* note that `CREATE INDEX CONCURRENTLY` does not work in a transaction -\\nit depends on being able to change the table incrementally, which doesn't fit\\nthe transaction model.  If the index creation fails, you are recommended to\\ndrop the index and re-create it, as it won't be rolled back and may be\\npartially created.\\n### Avoiding blocking changes\\nThere is a useful table in [this article](https://www.citusdata.com/blog/2018/02/15/when-postgresql-blocks/) which I've reproduced below (as the article may disappear):\\n| Runs concurrently with           | SELECT | INSERT UPDATE DELETE | CREATE INDEX CONC VACUUM ANALYZE | CREATE INDEX | CREATE TRIGGER | ALTER TABLE DROP TABLE TRUNCATE VACUUM FULL |\\n|----------------------------------|--------|----------------------|----------------------------------|--------------|----------------|---------------------------------------------|\\n| SELECT                           | Y      | Y                    | Y                                | Y            | Y              | No                                          |\\n| INSERT UPDATE DELETE             | Y      | Y                    | Y                                | No           | No             | No                                          |\\n| CREATE INDEX CONC VACUUM ANALYZE | Y      | Y                    | No                               | No           | No             | No                                          |\\n| CREATE INDEX                     | Y      | No                   | No                               | Y            | No             | No                                          |\\n| CREATE TRIGGER                   | Y      | No                   | No                               | No           | No             | No                                          |\\n| ALTER TABLE etc                  | No     | No                   | No                               | No           | No             | No                                          |\\nOur reports should only use SELECT so most operations won't block them.\\nThe Event Recorder however needs to insert data, so you can't run\\nany of the index modification changes (apart from `CREATE INDEX CONCURRENTLY`) without risking blocking.\\nNote however that some changes may be fast enough despite blocking - adding a column\\nfor example.  However you should performance test these changes against a\\nproduction-sized database to be sure!\\n\n\n## Context\\nOn January 20, 2020, the team became aware that a data migration that needed to be executed along with a prior\\nproduction deployment was never run in production. In order to rectify this, the team ran the required data migration on\\nthe evening of January 20. Since the task was CPU (and perhaps memory) intensive, the task ran for ~8 hours overnight,\\nand required constant monitoring from the team to ensure that the task wasn't affecting the regular performance of the\\nSimple Server. After the task was completed, it was noted that the Simple Server _was_ impacted by the task.\\nReferences\\n* [Slack conversation](https://simpledotorg.slack.com/archives/CFHMC60P5/p1579524825012500)\\nThe problems that this ADR aims to tackle are the following:\\n* A required data migration was not run after the corresponding production deployment\\n* Data migrations require constant supervision to monitor server performance and correct failures\\n* The Simple Server's performance was degraded during the execution of the data migration\\n\n## Decision\n### Enforcing data migration execution\\nIn order to better enforce the execution of data migrations, we will invoke them using the\\n[`data-migrate`](https://github.com/ilyakatz/data-migrate) gem. This will ensure that data migrations are run with the\\nsame reliability as database migrations, while still allowing database migrations to be run or rolled back independently\\nif necessary. For example, we may ship a data migration that looks like this.\\n```ruby\\nclass UpdateNilensoBPs < ActiveRecord::Migration[5.1]\\ndef up\\nBPUpdater.perform(facility: \"Nilenso\") # An associated service class that performs the data migration\\nend\\ndef down\\nRails.logger.info(\"This data migration will not be reversed. Good luck!\")\\nend\\nend\\n```\\nWe will need to take care with our file management. [See below](#consequences).\\n### Reliable hands-off execution\\nWe will use background jobs to execute data migrations in an asynchronous, atomic, and repeatable fashion. Ideally, the\\ndata migration will be split up into as small atomic parts as possible, and each part will be executed in its own\\nbackground job. For example, a task to update 1000 records will be executed using 1000 Sidekiq jobs, one for each\\nrecord.\\n```ruby\\n# good\\naffected_records.each do |record|\\nUpdateJob.perform_later(record)\\nend\\n# bad - not atomic\\nBulkUpdateJob.perform_later(affected_records)\\n# bad - not asynchronous\\nUpdater.perform_right_now(affected_records)\\n```\\nUsing atomic background jobs will have several advantages.\\n* Failed jobs will be retried automatically with sensible exponential back-off\\n* We avoid the use of very-long-running processes on the server (which could also result in very-long-running database\\nconnections)\\n* It becomes easier to monitor the progress of the migration through the Sidekiq dashboard\\n* The execution of the data migration can be easily throttled, paused, or halted\\nWe will need to take care of our Sidekiq queues. [See below](#consequences).\\n### Other beneficial traits\\n**Idempotence:** Ideally, the data migration task should be repeatable indefinitely without changing the end result.\\nIdempotent tasks will ensure that even if we have to abandon the migration in a half-finished state (eg. Redis falls\\ndown and background jobs are lost), we should be able to re-run the data migration without requiring any code changes.\\n```ruby\\n# good\\nBloodPressure.where(recorded_at: nil).each do |bp|\\nbp.touch(:recorded_at)\\nend\\n# bad - not idempotent\\nBloodPressure.where(id: affected_ids).each do |bp|\\nbp.update(systolic: bp.systolic + 10)\\nend\\n```\\n\n\n## Context\\nIn order to allow apps to ship independently, we need to ensure that\\nsplit names defined in different apps don't collide. The strategy until\\nnow has been for developers to run all the apps that might have\\nconflicting splits in local dev, but this doesn't scale well as they may\\nnot have the latest version of each app at all times and a\\nfully-upgraded local testtrack server to detect conflicts that might\\narise in production.\\nWe need to make sure that migrations don't get jammed up in production\\neven if a developer only has their own app downloaded locally.\\n\n## Decision\nNew migration runners will be expected to prefix their split names with\\ntheir app names and a dot. Legacy runners will be grandfathered out of\\nthis constraint, but we will begin soft enforcing that any new split\\nname conform to using at most a single dot, and that the text before\\nthat dot be the app name of the split's owner_app.\\n\n\n## Context\\nGOV.UK Docker has services defined for various databases, such as PostgreSQL,\\nMongoDB and Elasticsearch. These have their data persisted in\\n[Docker volumes][volume] defined [for each service][defined-volumes].\\nWhen GOV.UK Docker updated from [MongoDB 2.4 to 3.6][mongodb-update] users\\nbegan receiving cryptic errors when trying to run applications that depended\\non MongoDB. This was because they had a volume with data structured for MongoDB\\n2.4 whereas MongoDB was expecting this to be structured for 3.6.\\nWe felt that this upgrade path was more confusing and difficult than it needed\\nbe and wanted to improve it for the future.\\n[volume]: https://docs.docker.com/storage/volumes/\\n[defined-volumes]: https://github.com/alphagov/govuk-docker/blob/cb124a3a1d3353e777d4f777d77f03f93415d415/docker-compose.yml#L5-L10\\n[mongodb-update]: https://github.com/alphagov/govuk-docker/pull/356\\n\n## Decision\nWe decided that we would rename database services, and their respective\\nvolumes, to reflect the software version number. For example the `mongo`\\nservice, and namesake volume, have been renamed to `mongo-3.6`.\\nThe precision of the version number is chosen based on the backwards\\ncompatibility between versions of the software. For example to upgrade from\\nMySQL 5.5 to 5.6 requires an upgrade script and thus would cause problems for\\nGOV.UK Docker users. However, upgrading from 5.5.58 to 5.5.59 does not. For\\nservices that aren't sensitive to minor versions, such as Elasticsearch, we\\nhave specified the service and volume with respective to a major version:\\n`elasticsearch-6`.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nThe Data Migrate gem has caused us a number of issues in the past, it runs\\nsilently as part of a deploy, and this can result in surprising errors\\nduring a deploy. We've also had issues with the gem itself - most recently\\na bug in a new version causing strange errors in deployment.\\n\n## Decision\n", "response": "## Decision\nWe will decouple data migrations from the deployment process to gain more control and visibility over their execution. This will involve the following actions:\n\n1. **Separate Data Migration Pipeline:** Implement a separate pipeline or process for data migrations, allowing them to be executed manually before or after application code deployments. This will help isolate issues related to data changes and prevent them from affecting the deployment process.\n\n2. **Enable Logging and Notifications:** Enhance logging and notification mechanisms for data migration tasks to ensure any issues encountered are promptly and clearly reported. This will improve our ability to diagnose and address problems quickly.\n\n3. **Version Control of Gem:** Pin the Data Migrate gem version to a stable and thoroughly tested release to prevent any inadvertent issues arising from problematic updates. Review new versions carefully before considering upgrading.\n\n4. **Review Alternative Libraries:** Evaluate other libraries or tools that might offer more predictable and controlled behavior for data migrations. Consider switching if a more reliable alternative is found.\n\n5. **Manual Verification and Rollback Plans:** Implement a plan for manual verification of critical data migrations and establish rollback procedures in case errors affect the integrity of our data. This will ensure confidence in the correctness of data modifications.", "matched_ids": [2398, 317, 1713, 4757, 1666], "time": "0:00:06.276172", "input_tokens": 2707, "output_tokens": 239}
{"context": "## Context\\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nApplication delegate download task to `DownloadManager` system service and get an `unique id` for this task.\\nApplication need to store this Id to do further stuff with this task:\\n- Query status\\n- Get the completed state\\n- Get error details\\n- Cancel a download task\\n\n## Decision\nCreating a `DownloadingRepository` to manage downloading tasks\\n\n\n## Context\\n- Download feature does not work when targeting into SDK 29 and above\\n- From Android 10 and above, the [scoped storage](https://developer.android.com/training/data-storage#scoped-storage) has enabled that makes the access into external storage is limited. So, we need to adapt with this change.\\n\n## Decision\n- Target to Android 10 (SDK 29) first, then adapt to Android 11 (SDK 30) later.\\n- Temporarily [opt-out of scoped storage](https://developer.android.com/training/data-storage/use-cases#opt-out-scoped-storage) in Android 10:\\n```xml\\n<manifest ... >\\n<!-- This attribute is \"false\" by default on apps targeting\\nAndroid 10 or higher. -->\\n<application android:requestLegacyExternalStorage=\"true\" ... >\\n...\\n</application>\\n</manifest>\\n```\\n```groovy\\ndefaultConfig {\\n...\\ntargetSdkVersion 29\\n...\\n}\\n```\\n\n\n## Context\\nAt this time, we used `android.net.Uri` to extract information and open an `InputStream` to upload.\\nBut, an `Uri` has `UriPermission` protect the data which is represented.\\nThis permission will be revoked by shared application when the received Activity no-longer run.\\nIt is not critical to Android 7, but with Android 9 we always get `Permission Denied` when deliver Uri to Worker to execute.\\n\n## Decision\n- We extract all requirement information of the Document at the time we receive the Intent.\\n- Instead of using Uri directly, we create a temporary file to store the file which Uri represent\\n- We deliver temporary file path to Worker\\n\n\n## Context\\nIn LinShare Flutter application, we can download files one by one from \"My Space\", \"Received Shares\" or \"Shared Spaces\" with the library `flutter_downloader` by adding it to the queue :\\n```\\nTaskId enqueue(String url, String saveDir, String header...)\\n```\\nWe can also clear or cancel file queue with dedicated functions.\\n\n## Decision\nWe decided that LinShare could currently download a file one by one.\\n\n\n## Context\\nAndroid and iOS have different folder architectures.\\nTherefore we need to provide *path folder* according to user's plateform.\\n`path-provider` library only provide *Download Folder* for Android Version.\\n\n## Decision\nWe decided to use `path-provider` if the user is on Android, and use an other library (TBD) if the user is on iOS.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\\n\n## Decision\n", "response": "## Decision\nWe decided to leverage the `DownloadManager` system service for handling background downloads. This approach simplifies the implementation by utilizing the built-in capabilities of `DownloadManager` to handle HTTP interactions, retries, and download persistence across connectivity changes and system reboot. Additionally, we will implement a broadcast receiver to listen for download progress, failure, and completion events, ensuring that the application can react appropriately to the download lifecycle.", "matched_ids": [1645, 3300, 1643, 3297, 3301], "time": "0:00:03.250172", "input_tokens": 905, "output_tokens": 84}
{"context": "## Context\\nWe are doing a rewrite.\\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nFor the Go based prototype we used IPFS' Kademlia DHT implementations with edits.\\nSince we are switching over to Rust, we can't easily continue using that code base.\\nMore importantly, there are too many Holochain specific additions to a vanilla Kademlia DHT, as well as other possible implementations of achieving entry resilience, routing, our forthcoming World Model, CRDT handling, gossip and our graph properties via linking, so it does not make sense to change existing implementations but instead roll our own.\\n\n## Decision\nWe will build our own DHT implementation in Rust from scratch.\\n\n\n## Context\\nFor Holo, we need to have user agent's keys and source chain on the client machine and the rest of the Holochain agent (the DHT shard etc.) be held by HoloPorts.\\nIn February 2018, Arthur, Eric and Nico discussed this during a co-creative session in London and made the assumption to have the Holochain agent be split up into two pieces, called front-house and back-house (prior left and right hemisphere). The front-house was meant to entail the source chain and private key management as well as a ribosome to run the app and provide interfaces for the UI and in the non-Holo case for bridging - everything that is user/agent facing. The back-house should have been the DHT shard, routing table, metrics, etc. Basically everything that is network facing.\\nWith this separation, the reasoning in February was to rewrite (only) the front-house in JS to be able to run this piece in a web browser, as needed for Holo.\\nEric and Nico continued to map out the specifics of how these two modules would interface. In that process it became apparent that dividing an agent in these two pieces and have them run on different remote machines has a huge problem:\\n* every network communication (including the world model which happens on an ongoing basis) that the back-house is doing has to be signed with the agent's keys\\n* the agent's keys are by definition part of the front-house\\n* **-> the back-house can't live up to its main accountability without communicating with the front-house and requesting a signature for every packet of communication that might be even triggered from the network/DHT while the user is offline and keys not accessible**\\nFurther conversation including Arthur on May 17th 2018 makes it clear that thinking in terms of two different kinds of agency seems appropriate. We discussed separating the authoring or creative agency from the part that runs validations and holds DHT shards and world model information, and allows the later to proxy for the former, **with separate keys**.\\n\n## Decision\nWe decide to **not** emulate a single agency (as represented by one key) across two remote devices. In other words: we decide to not try to implement distributed agents.\\nInstead, we solve the initial Holo front-end problem by establishing **two sub-agencies with distinct keys**, where the Holo client's authoring agency explicitly grants proxy rights to a HoloPort's DHT agency.\\nIn other words: the Holo user uses their local key to sign a statement that grants another agent (the HoloPort) to act on their behalf for all the cases needed to have the HoloPort carry the DHT weight for this agent. But technically, it is another agent with its own key.\\n\n\n## Context\\nGoing forward with the rust implementation, we recognize that some 32bit architectures exist that people may want to run Holochain on.  Supporting 32bit architectures may have particular consequences in the realm of cryptography.  We have limited resources.\\n\n## Decision\nFor now we will assume availability 64bit CPUs and not use our resources testing against 32bit targets.\\n\n\n## Context\\nWith the opportunity to design and implement the peer-to-peer stack from\\nscratch in the context of the Tendermint implementation in Rust, a lot of the\\nlearnings of the shortcomings of the original Go implementation can be used to\\nprevent certain mistakes. Namely two:\\n* Leakage of physical concerns into the core domain\\n* Flexibility to adopt different wire protocols for transport of messages\\nFor that, the first set of newly introduced concepts will attempt to be generic\\nover which transport is used to connect and converse with other peers. Given\\nstrongly tailored abstract interfaces, concrete implementations will be easy to\\nspin up and plug into the machinery which lifts bytes from the wire into the\\ncore domain and transports messages into the rest of the system.\\n\n## Decision\n### Transport\\nWrapping the design is the `Transport`. Modelled with the properties of\\na physical network endpoint in mind, which can be bound and stopped. It should\\nstrongly correspond to the acquisition and lifecycle management of network\\nresources on the system.\\n``` rust\\npub trait Transport {\\ntype Connection: Connection;\\ntype Endpoint: Endpoint<Connection = <Self as Transport>::Connection>;\\ntype Incoming: Stream<Item = Result<<Self as Transport>::Connection>> + Send + Sync;\\nasync fn bind(self, bind_info: BindInfo) -> Result<(Self::Endpoint, Self::Incoming)>;\\n}\\n```\\nAfter the successful bind the caller holds an `Endpoint` as well as a stream of\\nincoming `Connection`s. Which is a standardised way to connect to new peers and\\nreact to newly connected ones respectively.\\n``` rust\\npub trait Endpoint: Send + Sync {\\ntype Connection;\\nasync fn connect(&self, info: ConnectInfo) -> Result<Self::Connection>;\\nfn listen_addrs(&self) -> Vec<SocketAddr>;\\n}\\n```\\nCenterpiece of the whole shebang is the `Connection`. It represents a connected\\npeer and provides the primitives to get data and send data from a peer. It is\\ndesigned with the outlook to support stream based transports down the road.\\nWhile being open to enable feature parity with current production installations\\nbased on tendermint-go's `MConn`.\\n``` rust\\npub trait StreamSend: Send + Sync {\\nasync fn send<B: AsRef<[u8]>>(msg: B) -> Result<()>;\\n}\\npub trait Connection: Send + Sync {\\ntype Error: std::error::Error + Send + Sync + 'static;\\ntype StreamRead: Stream<Item = Result<Vec<u8>>> + Send;\\ntype StreamSend: StreamSend;\\nfn advertised_addrs(&self) -> Vec<SocketAddr>;\\nasync fn close(&self) -> Result<()>;\\nfn local_addr(&self) -> SocketAddr;\\nasync fn open_bidirectional(\\n&self,\\nstream_id: StreamId,\\n) -> Result<(Self::StreamRead, Self::StreamSend), Self::Error>;\\nfn public_key(&self) -> PublicKey;\\nfn remote_addr(&self) -> SocketAddr;\\n}\\n```\\n### Peer\\nGiven a correct implementation of a `Transport` and its `Connection` newly\\nestablished ones will be wrapped with a `Peer`. Which is in charge of setting\\nup the correct streams on the `Connection` and multiplex messages - incoming\\nand outgoing alike - efficiently. It's also an attempt to enforce\\ncorrect-by-construction constraints on the state machine of the peer. To avoid\\nmisuse or unexpected transitions. The only way to construct is, is from an\\nexisting connection which gives the caller a connected peer. When invoking run\\non that one a fully function peer is \"returned\". Therefore the states look\\nlike: `Connected -> Running -> Stopped`.\\n``` rust\\nimpl<Conn> Peer<Connected<Conn>>\\nwhere\\nConn: Connection,\\n{\\npub async fn run(self, stream_ids: Vec<StreamId>) -> Result<Peer<Running<Conn>>> {\\n// ...\\n}\\nasync fn stop(self) -> Result<Peer<Stopped>> {\\n// ...\\n}\\n}\\nimpl<Conn> Peer<Running<Conn>>\\nwhere\\nConn: Connection,\\n{\\npub async fn send(&self, message: message::Send) -> Result<()> {\\n// ...\\n}\\npub async fn stop(self) -> Result<Peer<Stopped>> {\\n// ...\\n}\\n}\\n```\\nWhile sending messages is done through a method on a running peer, getting hold\\nof incoming messages can be achieved by draining the `Receiver` part of the\\nrunning state.\\n### Supervisor\\nThe `Supervisor` is the main entry point to the p2p package giving higher-level\\ncomponents access to a unified stream of peer events and messages as well as\\nthe ability to control peer lifecycle (connect, disconnect, etc.).\\n``` rust\\npub enum Command {\\nAccept,\\nConnect(SocketAddr),\\nDisconnect(node::Id),\\nMsg(node::Id, message::Send),\\n}\\npub enum Event {\\nConnected(node::Id, Direction),\\nDisconnected(node::Id, Report),\\nMessage(node::Id, message::Receive),\\nUpgraded(node::Id),\\nUpgradeFailed(node::Id, Report),\\n}\\nstruct CommandHandle;\\nimpl CommandHandle {\\nfn instruct(command: Command) -> Result<()> {\\n// ..\\n}\\n}\\nimpl Supervisor {\\npub fn new<T>(transport: T) -> Self\\nwhere\\nT: transport::Transport + Send + 'static,\\n{\\n// ..\\n}\\npub handle(&self) -> CommandHandle {\\n// ..\\n}\\npub subscribe(&self) -> Receiver<Event> {\\n// ..\\n}\\npub async fn run<T>(self) -> Result<()> {\\n// ...\\n}\\n}\\n```\\n\n\n## Context\\nRust as a language, in my opinion, does not lend itself to rapid prototyping. Writing efficient Rust code, while absolutely possible, is a problem domain unto itself, and takes time. E.g. Is it worth returning borrowed references in this case to save memory? Am I able to craft the lifetimes appropriately given all the other api usages of my struct? What about synchronization? These questions are implementation details, not architectural. If we are still working out the details with the underlying architecture, we don't want to be spending brainpower on * how * we accomplish our experiments.\\nIn this prototyping / proof-of-concept phase, I don't want to take the time to write efficient rust code, but I don't want to leave us with a bunch of unmaintainable inefficient spaghetti if we *do* decide to go forward with the prototype.\\n\n## Decision\nAbstract the p2p library at the process level.\\n- The P2P process will host a [ZeroMQ](http://zeromq.org/) ROUTER socket. This process can be in any language that supports zmq, and using any of the transports it supports. (Likely start with unix domain sockets for their high throughput).\\n- The holochain rust code will connect to the P2P process with a ROUTER socket using the [zmq](https://crates.io/crates/zmq) crate.\\n- The holochain rust code will access this ipc abstraction through the [network-abstraction](0007-abstraction-for-network-layer.md) framework allowing the option to implement, for example, the rust version of libp2p both internally, and as a separate process.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe are doing a rewrite.\\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\\n\n## Decision\n", "response": "## Decision\nWe will implement a centralized state management solution within each Holochain agent in the Rust rewrite. This will be achieved by creating a single global state repository that is accessed and mutated through a defined interface. This approach will reduce implicit dependencies and enhance maintainability by clearly delineating state interactions.\n\n- **State Management**: Introduce a global state management module that serves as the canonical source of truth for any agent state, providing controlled and predictable access to state data.\n- **Access Control**: Employ a well-defined API to interact with the global state, providing mechanisms for state queries, updates, and event-driven state changes, further reducing complexity and implicit dependencies.\n- **Concurrency**: Utilize Rust's inherent concurrency model and ownership system to handle state access and mutations safely, leveraging tools such as locks or atomic primitives as needed.\n- **Scalability and Modularity**: Design the state management solution to be modular and scalable, ensuring that it can accommodate future expansions and enhancements in the network architecture.\n\nThis architectural choice simplifies the system by centralizing state management while still respecting the distributed network nature of Holochain agents, ultimately making the codebase more manageable and less prone to errors.", "matched_ids": [1494, 1500, 1496, 4313, 1501], "time": "0:00:05.751104", "input_tokens": 2636, "output_tokens": 239}
{"context": "## Problem\\nCurrently multiple warnings are saved in an elektra non-conforming array\\nnotation which is limited to 100 entries. The notation of `#00` is against\\nthe design [decision made](array.md).\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Problem\\nMemory Consumption in Elektra is quite high as the key names\\nare long and stored twice in memory.\\n\n## Decision\nOnly store the unescaped key name, suitable for comparing/searching/iterating over name, i.e.:\\n- Remove the escaped name from `struct _Key` and use it only when necessary.\\n- Clarify and reduce [terminology](/doc/help/elektra-glossary.md).\\n- API Changes:\\n- `keyNew (const char*, size_t)`\\n- `keyName` returns the unescaped name\\n- remove `keyUnescapedName`, `keyGetUnescapedNameSize`.\\n- reverse terminology: with \"key name\" we will refer to the unescaped (base) name,\\nthe escaped name will be explicitly called \"escaped key name\".\\n- escaped name will be outside the core for tooling\\n- `keyDup(.., int)` with options to filter which parts are copied\\n(to allow copy of keys where only the key name is copied)\\n\n\n## Problem\\nInconsistent use of booleans in various parts of Elektra.\\n\n## Decision\nOnly the strings `0` and `1` are allowed in the `KeySet` for `type = boolean`, for both values and defaults.\\nEverything else should lead to errors in checkers (in `kdbSet`).\\nA spec with `type = boolean` without a specified default should be interpreted as `default = 0`.\\nExample for an implementation in C in an application:\\n```c\\nif (k != NULL && strcmp(keyString(k), \"1\") == 0) {/*true*/} else {/*false*/}\\n```\\nStorage plugins are allowed any representation as suitable, e.g., a JSON plugin might render `1` as `true`.\\nThe type checker plugin should allow\\n- non-presence\\n- the string \"0\"\\n- the string \"1\"\\n\n\n## Problem\\nWhile we have a classification of errors and warnings, it remains\\nunclear when plugins actually should emit errors and warnings.\\n\n## Decision\nProvide guidelines in the form as tutorials, covering:\\n- prefer errors to warnings\\n- that any not understood metadata (e.g. types), should lead to an error\\n- that wrong specifications, like `kdb meta-set /tests/ipaddr/ipv4 check/ipaddr ipv8` should be rejected\\n- if the value does not confirm **exactly** to the specified type, an error should be emitted\\n(e.g. only `0` or `1` as boolean)\\n- anything else that is beyond the capabilities of a plugin (not implemented),\\nshould lead to an error\\nViolations against these guidelines can be reported as bug and then either:\\n- the bug gets fixed\\n- the plugin get a worse `infos/status` but still get shipped with 1.0\\n- the plugin gets removed\\n- [Metadata in Spec Namespace](spec_metadata.md)\\n- [Capabilities](capabilities.md)\\n- [Boolean](boolean.md)\\n\n\n## Problem\\nCurrently, an error or warning message in elektra causes the following line to be shown:\\n```\\nIngroup: <group>\\n```\\nIts main purpose is to show the user if the error occurred in either `kdb`, `module` or `plugin`.\\nThe problem is that this message is of no value for the user and increases the verbosity of the message.\\n\n## Decision\nThe `ingroup` message will be removed as it does not yield any notable benefit.\\nSee [Error concept](error_codes.md)\\n\n\n## Problem\\nThe current error concept has disadvantages in following regards:\\n- A lot of redundant errors\\nAt the moment, each new plugin introduces new error codes which led to about 210+ error codes. Many of those errors\\nare duplicated because developers did not know or search for a similar error which is already present. This concept should\\ngroup similar errors together so that there is one coherent and consistent state again.\\n- Hard to manage specification file\\nSince every developer adds its own error individually, a lot of merge conflicts happen which makes contributing to the codebase\\nunpleasant. Additionally, if you want to reuse any error you have to scrape to the whole file with ~1300+ lines. As there is no\\nsenseful ordering or scheme behind the errors (since they grew by time), it is a hassle to find the correct error code.\\nThe new concept should standardize errors, making it easy to categorize errors from new plugins and avoid merge conflicts.\\n- No senseful way for application developers to use error codes from elektra\\nIf developers of plugins/ external tools using elektra want to react to errors, they have to be very specific. At the moment there is\\nno possibility to catch all errors easily which force a certain behavior. Eg. if there happens a temporary recoverable error, developers have to\\ncatch for every specific error code rather than a general hierarchical error. The new concept should make it easy to react to errors as they are\\nsensefully grouped together and are hierarchically structured.\\n\n## Decision\nAll \"fatal\" errors will be converted to \"errors\" as the distinction is not relevant.\\nUnused errors will be removed from the specification.\\nErrors will be categorized into logical groups with subgroups.\\nEach error will be made up of 5 characters, where the first 2 character indicate the highest level\\nand character 3 to 5 will be used for subgrouping. Errors are prepended with the letter `C` which\\nis the abbreviation for \"Code\".\\n- Permanent errors C01000\\n- Resource C01100\\n- Out of Memory C01110\\n- Installation C01200\\n- Logical C01300\\n- Internal C01310\\n- Interface C01320\\n- Plugin Misbehavior C01330\\n- Conflicting State C02000\\n- Validation C03000\\n- Syntactic C03100\\n- Semantic C03200\\nTo see an explanation of the categories along with a guideline on how to categorize please\\nsee the [Error Codes Guideline](../dev/error-categorization.md)\\n- [Error Message Format](error_message_format.md)\\nShows the new format of the error message\\n- [Error Codes Guideline](../dev/error-categorization.md)\\nShows how to categorize errors\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Problem\\nCurrently multiple warnings are saved in an elektra non-conforming array\\nnotation which is limited to 100 entries. The notation of `#00` is against\\nthe design [decision made](array.md).\\n\n## Decision\n", "response": "## Decision\nMigrate the warning storage system to utilize Elektra's conforming array notation, which uses `#_` where `_` represents a numerical value, without a leading zero. This removes the limitation of 100 entries as well as adhering to the design decisions in [array.md]. The changes include:\n- Modify the underlying codebase to support the new conforming notation format. This involves updating array handling logic to interpret `#_` as an Elektra array index.\n- Update any existing instances of warnings stored in the `#00` format to the new `#_` format using a migration script or during Elektra initialization if necessary.\n- Ensure that any plugins or modules that interact with warning arrays are consistent with the new notation and perform validation to ensure that arrays comply with the new standard.\n- Provide backwards compatibility by warning users if outdated notations are encountered, guiding them to upgrade their configurations.\n- Update documentation to reflect these changes and educate users on the proper array notation for storing multiple warnings.", "matched_ids": [1291, 1293, 1302, 1308, 1319], "time": "0:00:07.072542", "input_tokens": 1509, "output_tokens": 207}
{"context": "## Context\\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\\nThere is currently no formal recommendation for a tool of choice in the technical guidance https://github.com/DFE-Digital/technical-guidance.\\nWe want a tool that provides an open and modern security standard.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nOur user facing applications will need to authenticate users.\\nIn the long-term, we want to use the standard user sign-on services in CCS for\\neach appropriate user group.\\nFor suppliers, this is the [Supplier Registration Service (SRS)][service-srs].\\nFor CCS staff it is their G-Suite account.\\nUsing a single authentication service will reduce administration overheads\\nbecause we won't have to manage a JML (Joiners Movers Leavers) process. For\\nusers it will also mean they don't have to remember yet another username and\\npassword combination.\\nHowever, we want to get moving quickly, and integration with SRS/CCS G-Suite\\nwill take time. For now, we want a simple sign-in service that we can integrate\\nwith quickly, to enable us to test and iterate the application.\\n\n## Decision\nWe will use [Auth0][service-auth0] for authentication on a temporary basis.\\nAuth0 is an authentication service that uses the OAuth protocol. It provides\\nand simple integration path, and is free for several thousand active users.\\nWe acknowledge that we are incurring technical debt by not integrating\\nimmediately with SRS and G-Suite, however we believe that this will let us move\\nmore quickly during development.\\nThis decision will also allow us to gain a better understanding of the user\\nneeds around authentication before a decision around the final authentication\\nsolution is made.\\nWe will replace Auth0 with the proper authentication service later in the\\ndevelopment process.\\n\n\n## Context\\nWe need to allow a number of users to sign in to the service in order to use it.\\nIn order to implement this quickly, we'll use Auth0 to manage this.\\nAs Auth0's authentication uses OAuth2, it should be straightforward to migrate\\nto another service, if BEIS have a preference for something else.\\nAuth0 provides views for authentication that we can use in our user journeys.\\nThere are two versions of these views 'Classic' and 'New'.\\nClassic uses JavaScript and is not progressively enhanced. New uses language\\nthat results in a poor user experience.\\n\n## Decision\nWe will use the free tier and 'Classic' views of Auth0 for the private beta.\\n\n\n## Context\\nAt the moment and for the short term future Hackney do not have a clear candidate for a single sign-on provider. They are reviewing the situation and will decide on a future solution before the end of 2019.\\nWe have so far been using Basic Authentication.\\nAs with other decisions we have attempted to follow the path set by the Repairs Hub which is another Hackney Rails app that is more mature. We asked what they were using and the answer was Microsoft Azure Active Directory using OpenID Connect (OAuth2). We believed we could get the exact same provisioning when the time came for Report a Defect to integrate with a single sign-on provider, however when the time came we learnt that it wasn't supported by Hackney and should be thought of as experimental.\\nAs we had 2 weeks left we discussed with Hackney Architect Keith Gatt the possibility of using Auth0 as a temporary option to enable us to meet the user need of providing agent names within the service, as well as security needs through the use of a logged authentication service that provided one set of credentials per person, rather than a single set for basic auth.\\n\n## Decision\nUse Auth0 as the single sign-on provider, and remove basic auth from production.\\n\n\n## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno / minimal costs.\\n\n## Decision\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno / minimal costs.\\nAuth0 is selected as the solution, due to its managed nature requiring\\nzero deployment and effort. After an evaluation of features compared\\nagainst the project requirements, it has been determined that Auth0's\\nfree tier will suffice and as a result there are no ongoing costs\\neither.\\n### Positive Consequences\\n* As a managed solution, there are no initial deployment or ongoing\\nhosting / infrastructure costs.\\n* Patching and maintenance of the solution is provided by Auth0 at no\\nadditional cost / interruption to the customer.\\n* It is expected that (at least initially) the free tier will suffice to\\nmeet the requirements of the project.\\n### Negative Consequences\\n* The features and customisations available are limited without\\nupgrading to a paid tier:\\n* Email templates cannot be customised on the free plan.\\n* A custom domain cannot be used to serve the Auth0 tenant.\\n* Enterprise integrations (AD, LDAP, etc) are not available.\\n* Multifactor authentication is not available.\\n* Auth0 hosted UI's can be customised but only to a limited extent.\\n* Limit of 7000 monthly active users.\\n\n\n## Context\\nWe need to protect the allocation API with authentication, but we'd rather not\\nhave to come up with an approach to do that ourselves from scratch.\\nThe new [NOMIS OAuth2 server](https://github.com/ministryofjustice/nomis-oauth2-server)\\nis already being used in production for authentication on almost all of the\\nNOMIS APIs and some other APIs built in Sheffield. We will need to use it to\\nauthenticate with the Custody API, and the other services which may need to use\\nthe allocation API are very likely to already be using this authentication\\nmethod for the other APIs they use.\\nClients can use one token (of a particular grant type) to authenticate with all\\nAPIs which use the NOMIS OAuth2 server, which makes things simpler for all\\nthose services - they don't have to work with multiple different authentication\\napproaches.\\nThe NOMIS OAuth2 server uses JWTs signed with a private key, so relying\\nservices can verify the integrity and authenticity of tokens presented by\\nclients using the corresponding public key.\\nWe've decided that the allocation manager will be entirely responsible for user\\naccess control and will call other APIs directly, and the allocation API will\\nbe a smaller interface onto its data (see [ADR 0010](0010-allocation-api-has-less-responsibility.md)).\\nThat means that the allocation API doesn't need to know which user it's\\nreturning data for, and we can use a system-to-system approach to\\nauthentication.\\nWe don't know of any other shared approaches to API authentication which are\\nused in the prison space.\\n\n## Decision\nWe will use the NOMIS OAuth2 server for authentication on the allocation API.\\nWe will use the client credentials OAuth2 grant type for authentication on the\\nallocation API.\\nWe will verify signatures on presented tokens in the allocation API.\\nWe will respect expiration times on presented tokens in the allocation API.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\\nThere is currently no formal recommendation for a tool of choice in the technical guidance https://github.com/DFE-Digital/technical-guidance.\\nWe want a tool that provides an open and modern security standard.\\n\n## Decision\n", "response": "## Decision\nWe will proceed with implementing the authentication for trusted school buying professionals using the DfE Sign-in (DSI) service as mandated by the DfE governance review. Although the service does not require the additional school and user information stored within DSI, we will integrate with DSI to comply with governance guidelines and ensure consistency across DfE services. \n\nTo address the preference for a password-less authentication approach, we will explore integrating an open and modern security standard such as WebAuthn or OAuth2 in conjunction with DSI, to evaluate if it can be supplemented for users who prefer a password-less experience. \n\nWe acknowledge that while adopting DSI may initially lead to additional overhead as it holds more information than currently needed, it will ensure we meet the required security and governance standards, provide a recognized user authentication flow, and leverage existing technical infrastructure within the Department for Education services. \n\nFurthermore, adhering to DSI could facilitate future needs that may arise regarding access to school and user information, should requirements evolve to necessitate such data.", "matched_ids": [2041, 2405, 5211, 4857, 257], "time": "0:00:05.975150", "input_tokens": 1843, "output_tokens": 211}
{"context": "## Context and Problem Statement\\nHow to guarantee reproducibility of Jupyter Notebooks?\\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\\nNotebooks should be treated as component/service that use their own dependencies, therefore when storing notebooks,\\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\\n## Decision Drivers <!-- optional -->\\n* user prospective\\n* reproducibility\\n* traecability\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context and Problem Statement\\nWe need a way to deploy specific versions of our bounded contexts, having\\nreproducible releases of our software in testing and production .\\n## Decision Drivers\\n* We want to have frequent releases of the software, adding new features\\netc.\\n* We want to keep our dependencies up to date on a monthly basis, using\\n`composer update`\\n* We want to have reproducible versions of our software - `composer\\ninstall` must install the same code on the CI machine, each\\ndevelopers machine, the user acceptance environment and the production\\nenvironment.\\n* We want to have a trunk-based development process, where the current\\nmaster of each repository is always working and we can deploy at any time.\\n* We regularly update the `wmde/fundraising-frontend-content` dependency.\\n\n## Decision\n* We want to have frequent releases of the software, adding new features\\netc.\\n* We want to keep our dependencies up to date on a monthly basis, using\\n`composer update`\\n* We want to have reproducible versions of our software - `composer\\ninstall` must install the same code on the CI machine, each\\ndevelopers machine, the user acceptance environment and the production\\nenvironment.\\n* We want to have a trunk-based development process, where the current\\nmaster of each repository is always working and we can deploy at any time.\\n* We regularly update the `wmde/fundraising-frontend-content` dependency.\\nWe will use semantic versioning for 3 months (until 2020-08-14). After\\nthat period we'll evaluate the actual benefits and drawbacks of semantic\\nversioning. If the drawbacks outweigh the benefits and we can't fix them\\nthrough other means (automation, CI), we'll abandon semantic\\nversioning and return to trunk-based dependencies.\\n\n\n## Context\\nTraceability of a Docker image to a source code repository is valuable to debug any problem that comes up during testing and deployment.\\nDependency pinning on project images pinning a particular version of the base image they are using is also valuable for build reproducibility.\\n\n## Decision\nWe will tag every new, tested version of an image using the commit SHA value that produced it.\\n\n\n## Context\\nWhen bundling components for usage in consuming applications, should we require consuming applications to have the same dependencies as this repository? Or should exported components be packaged assuming no dependencies?\\n\n## Decision\nWe'll package components with their own inline styles. No external dependencies needed.\\n\n\n## Context and Problem Statement\\nWe needed to clarify the relationship between the DPE Client repository, where the components we are using to populate the Storybook repo have already been written, and the Storybook repository.\\n- From which repo would components be published?\\n- Which repos would consume components from NPM?\\n- Should the Storybook live inside the Client repo?\\n## Decision Drivers\\nN/A\\n\n## Decision\nN/A\\nChosen option: Option 2, because this allows us to refactor components' code and preview changes within the Storybook locally \u2014 before publishing the component to the hosted Storybook and NPM.\\nThis means that our workflow for populating the Storybook and refactoring the Client code is as follows:\\n1. Duplicate component code to Storybook repo\\n2. Publish completed components to NPM\\n3. Remove the original component code from the Client and import via NPM\\n### Positive Consequences\\n### Negative consequences\\nCaveat: If more than one person is working on the Storybook and DPE Client, they'll need to sync up to ensure that details in code refactors are not lost due to overlapping work.\\nIf possible, also avoid having people working simultaneously on a component that consumes / is consumed by another component (i.e., one person working on a card component and another person working on a list component that consumes card components).\\n\n\n## Context\\nReproducing results from a program is challenging when operating systems, language versions, and dependency versions can vary.\\nFor this codebase, we will focus on consistent Python versions and dependency versions.\\n\n## Decision\nManage multiple Python versions via [pyenv](https://github.com/pyenv/pyenv).\\nManage dependencies via [poetry](https://python-poetry.org/).\\nDefine tests via [nox](https://nox.thea.codes/en/stable/).\\nRun tests in automation/CI via [Github Actions](https://github.com/features/actions).\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\nHow to guarantee reproducibility of Jupyter Notebooks?\\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\\nNotebooks should be treated as component/service that use their own dependencies, therefore when storing notebooks,\\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\\n## Decision Drivers <!-- optional -->\\n* user prospective\\n* reproducibility\\n* traecability\\n\n## Decision\n", "response": "## Decision\nWe will use [conda](https://docs.conda.io/en/latest/) to manage the environment dependencies for each Jupyter Notebook. Each notebook will be accompanied by an `environment.yml` file that specifies both direct and transitive dependencies, ensuring the same environment setup across various systems.\n\nFor additional reproducibility and sharing, we will use [Docker](https://www.docker.com/) to containerize the Jupyter Notebooks. A `Dockerfile` will be provided for each notebook, which will use the `environment.yml` to create a consistent runtime image. This ensures that the notebooks can be shared and executed with identical behavior regardless of the host environment.\n\nSteps:\n1. Define all dependencies in an `environment.yml` file.\n2. Create a `Dockerfile` that installs these dependencies within a Jupyter-enabled image.\n3. Store the `Dockerfile` and `environment.yml` alongside the notebook in version control for traceability.\n\nThis approach ensures that users can recreate the analysis in the notebook on any system, maintaining security, reproducibility, and traceability.", "matched_ids": [1531, 1503, 1631, 36, 535], "time": "0:00:06.652080", "input_tokens": 1284, "output_tokens": 219}
{"context": "## Context\\nWe need to record the architectural decisions that we make as we develop Ockam.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need to record the architectural decisions made for our team.\\n\n## Decision\nWe will use Architecture Decision Records, as discussed in our original proposal: https://docs.google.com/document/d/1pZlYCyXcZbmQq1O-g4BNJD1uZTjluYKkk7BSu2BwOGU/edit#\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions.\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael\\nNygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe need to record the architectural decisions that we make as we develop Ockam.\\n\n## Decision\n", "response": "## Decision\nWe will use Architecture Decision Records to document our architectural decisions as we develop Ockam, following the guidelines outlined by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions.", "matched_ids": [2261, 546, 867, 463, 4823], "time": "0:00:03.041246", "input_tokens": 441, "output_tokens": 55}
{"context": "## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nThe change that we're proposing or have agreed to implement.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\n", "response": "## Decision\nWe will adopt a microservices architecture to enhance the scalability and flexibility of our system, allowing individual services to be developed, deployed, and scaled independently. This decision is driven by the need for rapid development and deployment of new features while ensuring system reliability and maintainability.", "matched_ids": [639, 2026, 4804, 3820, 1145], "time": "0:00:06.380999", "input_tokens": 310, "output_tokens": 56}
{"context": "## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https://apps.nrs.gov.bc.ca/int/jira/browse/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we should standardize on a base Docker Image for Java-based services that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use.\\nWhile there are a few popular choices and de facto standards such as OpenJDK, we need to consider the support and legal implications of that choice, particularly since we could be considered as \u2018distributing\u2019 all layers in that image, including the OS and Java binaries.\\nWe could potentially workaround being the distributors of sensitive layers using techniques like init containers, composite containers, or foreign layers.\\n\n## Decision\nWe will start from the official CentOS 7 Docker image and add the Oracle server JRE binaries for the first iteration of the Alfresco base Java image.\\nThis allows us to start from a popular OS and Java runtime which are already in the supported ACS and APS stacks and QuickStart AMIs, and closer to the most popular customer OS, RHEL, than others.  We also have precedence in distributing the Oracle JRE in our current installer.\\nThe CentOS image also contains very few known vulnerabilities which we can easily patch at the moment.\\nWhile Alpine is an attractive choice due to its simplicity, it may be more difficult to debug and support, and the majority of our components are not yet architected to take advantage of the smaller size and lower resource consumption.\\nWhile distribution workarounds mentioned above were considered, it was felt that they are not common practice and are likely to produce more problems than they solve.\\nThe summary [![report of the comparison](https://img.shields.io/badge/report%20of%20the%20comparison-PRIVATE-red.svg)](https://ts.alfresco.com/share/s/bqDcnHWpSrSGybJhMxf93A) contains more details.\\nThe base image Dockerfile is [here](https://github.com/Alfresco/alfresco-docker-base-java/blob/master/Dockerfile).\\n\n\n## Context\\nThe issue motivatina# [short title of solved problem and solution]\\nWhat is the issue that we're seeing that is motivating this decision or change?\\n* Status: [proposed | rejected | accepted | deprecated | ... | superseded by [ADR-0005](0005-example.md)] <!-- optional -->\\n* Deciders: [list everyone involved in the decision] <!-- optional -->\\n* Date: [YYYY-MM-DD when the decision was last updated] <!-- optional -->\\nTechnical Story: [description | ticket/issue URL] <!-- optional -->\\n## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\n\n## Decision\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | ... | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, ...]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, ...]\\n* ...\\nWhat is the change that we're proposing and/or doing?\\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context and Problem Statement <!-- required -->\\nFedora 3 is unsupported and has been unsupported for four years; it is unlikely to be supported in the foreseeable future. It also requires a difficult-to-support version of the Java Virtual Machine. And yet, Fedora 3 is the cornerstone of our management \"repository,\" in which all SDR content is managed and from which said content flows to access and preservation environments. At the same time, there is a dwindling number of organizations in the cultural heritage community who are still using Fedora 3.\\n## Decision Drivers <!-- optional -->\\n* Fedora 3 is unsupported and unlikely to be supported\\n* Fedora 3 will be harder to install on newer operating systems\\n* The Fedora 3 data model is not inherently validatable\\n* The Fedora 3 community is disappearing, so we are increasingly going it alone\\n* Fedora 3 is a critical piece of SDR infrastructure and represents an enormous risk\\n* Samvera software that supports Fedora 3 is outdated and maintained/supported only through our own efforts, preventing us from using mainstream Samvera software\\n* We have (unverified) concerns about the scalability of Fedora 3\\n\n## Decision\n* Fedora 3 is unsupported and unlikely to be supported\\n* Fedora 3 will be harder to install on newer operating systems\\n* The Fedora 3 data model is not inherently validatable\\n* The Fedora 3 community is disappearing, so we are increasingly going it alone\\n* Fedora 3 is a critical piece of SDR infrastructure and represents an enormous risk\\n* Samvera software that supports Fedora 3 is outdated and maintained/supported only through our own efforts, preventing us from using mainstream Samvera software\\n* We have (unverified) concerns about the scalability of Fedora 3\\nNo decision made yet. See status field above.\\n\n\n## Context\\n*This section describes the forces at play, including technological, political, social, and project local.\\nThese forces are probably in tension, and should be called out as such. The language in this section is value-neutral.\\nIt is simply describing facts.*\\nThe Office of Development Transformation (ODT) is focused on helping CAD learn and adopt lean, agile, and devOps techniques and\\nprocesses. ODT will be initialy focus on Green Field Developent. The Online Bank is a sample app used in the ODT University classes.  The Purpose of theapplication is to provide a\\nsample app as a context for the training courses.  We believe most people are familiar with online\\nbanking, and won't need to spend a lot of time learning the problem domain.\\nThe purpose of this document is to determine the technology stack for the sample application.\\n**NOTE:** While we are initially focused on a Java Web stack, we may elect to develop out an example in the Microsoft Stack.\\nIf you would like to help, please contact us.\\n\n## Decision\n*This section describes our response to these forces. It is stated in full sentences, with active voice.\\n\"We will ...\"*\\nWe will be focusing on the modern web java stack.\\n### UI Technology Stack:\\n- Angular 2\\n- Bootstrap\\n- protractor\\n- jasmine\\n### Server Side Technology Stack:\\n- Java 1.8+\\n- SpringBoot 1.4+\\n- Restful\\n- MySQL\\n- Spring Data JPA (Hibernate)\\n- JUnit 5 / Spock\\n### Development Technology Stack:\\n- Local Development\\n- SCM: GIT\\n- Intellij\\n- Favorite Editor\\n- Gradle\\n- Angular Quickstart\\n- Concourse CI (CI/CD)\\n- Jira\\n- Sonar\\n#### Deployment:\\n- Pivotal Cloud Foundry (Java and Static Build Packs)\\n\n\n## Context and Problem Statement\\nIt is the offical policy of the Digital Iceland and stated in the Techical Direction that it is to be implemented as free and open source. Open source software by definition is open to anyone to use, modify, distribute and study. These permissions are enforced an open source license. There are a number of well-known and widely used open source licenses available and we need to choose a license that best fits the goals of digital iceland.\\nThere are two main types of open source licences:  more permissive licences that confer broad freedoms and minimal obligations (e.g., the MIT, BSD and the Apache 2.0 licences); and sharealike licences that require licensing adaptations with the same licence if they distribute them (e.g., the GNU GPL).\\nDevelopment for Digital Iceland will be open and free with minimum complications for development for all involved. Reuse and transparency will be promoted.\\n## Decision Drivers\\n* The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.\\n* It is important to build on the experience of similar government led inititives in other countries.\\n* Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.\\n* It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.\\n\n## Decision\n* The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.\\n* It is important to build on the experience of similar government led inititives in other countries.\\n* Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.\\n* It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.\\nThe MIT license was chosen, for the following reasons:\\n* It is the least restrictive of the licenses.\\n* It is very consise, simple and easy to understand and therefore should be clear to users and developers.\\n* Digital Iceland does not require protection of patents or existing intelletual property.\\n* Well known government lead initiatives like uk.gov and X-Road use the MIT license.\\n* The MIT license is the best known and most widely used free and open-source license in the world.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https://apps.nrs.gov.bc.ca/int/jira/browse/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n\n## Decision\n", "response": "## Decision\nIn order to respond to the shift in Oracle's licensing model and ensure the security and future-proofing of IITD's Java applications, the following decisions have been made:\n\n1. **Adopt OpenJDK:** IITD will officially adopt OpenJDK as the standard JDK for all IITD hosted Java applications. This decision ensures continued access to free and open-source updates and security patches for Java without incurring additional subscription costs associated with Oracle Java SE.\n\n2. **Target JDK Version:** All IITD Java applications will be upgraded or migrated to at least JDK 11 as soon as possible. JDK 11 is an LTS (Long-Term Support) release, providing long-term stability and security updates. This aligns with the security recommendations and provides a future-ready platform.\n\n3. **Transition Plan:** Applications currently running on Oracle JDK versions 8 and earlier will begin the transition to OpenJDK, with a prioritization strategy based on security and business criticality. A timeline will be developed to ensure full migration by a defined deadline.\n\n4. **Compatibility Assurance:** Conduct compatibility testing thorough for all applications moving from Oracle JDK to OpenJDK to ensure smooth transition without any functional disruptions.\n\n5. **Regular Updates Commitment:** IITD Architecture will monitor and stay updated on JDK developments, ensuring the Java environment remains secure and compliant with modern standards.\n\nThis decision aligns with the broader industry shift towards JDK distributions that assure cost-effectiveness and open-source alignment while securing IITD's operational java environment.", "matched_ids": [3603, 4800, 796, 1665, 1994], "time": "0:00:07.299617", "input_tokens": 2956, "output_tokens": 308}
{"context": "## **Context**\\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\\nWe will look at two options for validation:\\n- **Data Annotations**\\nThis involves \"annotating\" each class model with specific validation, such as\\n- `[Required(ErrorMessage = \"This field is required\")]`\\n- `[MaxLength(20)]`\\nThere are a number of issues with this approach:\\n- Validation is scattered throughout the codebase as attributes on data model classes\\n- Testing is not straightforward\\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\\n- Does not allow for conditional validation\\n- **Fluent Validation**\\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\\n- Lives outside of the data model classes\\n- Very easy to test validation in isolation\\n- Error messaging can be externalised using dependency injection\\n- Allows for chaining of validators and conditional validation\\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https://symfony.com/doc/current/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n\n## Decision\nFor each use case we write a validator class that checks the `Request` value object of that use case. The validator class must ensure that the use case can create valid domain objects from the request object. The validator class uses simple `if` checks and no external framework.\\nWe return result data structures from validation classes. The result data structures that have some way of communicating to the framework layer what input caused the validation error. If necessary, one input can have more than one validation error.\\nValidation error names are language-independent unique strings in `snake_case`. When we need to translate those error codes, we put the translations in the file [`validations.js`](https://github.com/wmde/fundraising-frontend-content/blob/test/i18n/de_DE/messages/validations.json) in the [content repository](https://github.com/wmde/fundraising-frontend-content). We don't  map every error to a translation, we can write frontend layer code that summarizes the errors or maps them in a different way.\\n\n\n## Context\\nMany ASP.NET applications use [Entity Framework (EF)](https://docs.microsoft.com/en-us/ef/), an Object Relational Mapper (ORM) that helps access data stored in database.\\nEF allows data in a database to be accessed by extending a DbContext class and adding properties to this extending class of type DbSet. DbContext and DbSet provide methods for performing basic CRUD operations against entities in a database that are defined in model classes. These model classes contain annotations that define the table name, columns and relationships with other entities. When a query is performed, EF handles creating instances of model classes and filling them with the received data.\\nSome properties are lazily loaded, with the queries related to fetching the required data only being run when thoses properties are accessed. This approach is commonly used when accessing a property representing a relationship with another entity.\\nA DbContext by default tracks changes to entities returned as the result of queries, with changes being saved when a call is made to a DbContext's SaveChanges or SaveChangesAsync methods.\\nThe DbContext and DbSet classes provide methods that can be used to fetch data, with the ability to apply limitations on what data is returned. EF will generate the required query, execute it, parse the response data and return the appropriate entity model instances.\\nEF supports migrations written as classes with Up and Down methods, to support upgrading and rolling back, respectively. These methods are implemented by adding calls to a provided MigrationBuilder instance.\\nDapper is a library that is commonly referred to as a \"micro-ORM\". It provides methods to support executing SQL queries and parsing the results to create instances of particular model classes. Unlike EF, Dapper does not support the tracking of changes and queries must be written using SQL.\\nDapper was developed for the StackOverflow website to address performance issues, as outlined in [this blog post](https://samsaffron.com/archive/2011/03/30/How+I+learned+to+stop+worrying+and+write+my+own+ORM).\\n\n## Decision\nWe will use Dapper with the [repository pattern](http://blog.mantziaris.eu/blog/2016/10/24/the-repository-and-unit-of-work-pattern/) to access data stored in the database.\\n\n\n## Context\\nAs the code rapidly grows we need a common structure and approach that will keep the code tidy and coherent.\\nThere are various aspects, the initial concern was the data access separation. The issue #140 initially talked about how this could be separated in terms of an ORM (Object-Relational Mapping) or a DAL (Data Access Layer).\\nHowever, this discussion also soon covered the structure of the code in general and how to structure it for ease of development and best practise.\\n### Rejected Options\\nWe considered the following options:\\n* The Pubsweet Way - This was to define a base class \"Model\" and extend this for each entity. This was rejected as its another self made ORM but and has validations that we don't want.\\n* Other ORMs : Waterline, sequelize, bookshelf - General purpose ORMs were rejected based on the experience that the time spent going up the learning curve and sifting through the docs when things go wrong is greater then simply rolling your own.\\n* Query Builders : Knex.js -These were not entirely ruled out, and were deemed compatible with a good code structure that would allow the database access to be encapsulated in a way that would allow a query builder to be used where necessary.\\n\n## Decision\nIn summary the server-side code will be structured below the `server` folder as follows:\\n* **entities** - This folder will contain subfolders relating to named entities in the system. See [\"Data Model Specification\"](https://docs.google.com/document/d/1KU-DLMNhPxjQF2j8HVlJvenvttPLxgtbTHo8Sy_PNRc/).\\n* **entities/\\<entity\\>** - The example below shows `Mansuscript` as an example entity, this folder will contain a common set of files that describe the entity's behaviour. Each of these have a particular purpose explained below:\\n* index.js - The main business logic, e.g. for `Manuscript` this could contain `getAuthor()`\\n* typedefs.js - Contains the GraphQL types pertinent to this entity.\\n* resolvers.js - Contains the GraphQL interface specified in terms of `Query` and `Mutation` 's for this entity. These in general should map onto the exposed functions from index.js. For example, `Manuscript` may contain a `Query` for `allManuscripts` and a `Mutation` for `createManuscript`.\\n* data-access.js - Contains the functions for saving to and loading from the database. All references to the database and SQL that is written should remain private to this file.\\n* **manuscript-state-machine** - It is anticipated that this is where the most orchestration of the entites will take place. This has been created as a separate folder so that it can expand to implement the state transitions currently defined for the manuscript.\\n### Example\\n```\\nserver\\n|\\n\u251c\u2500\u2500 entities\\n|   |\\n\u2502   \u251c\u2500\u2500 manuscript\\n\u2502   \u2502   \u251c\u2500\u2500 typedefs.js\\n\u2502   \u2502   \u251c\u2500\u2500 resolvers.js\\n\u2502   \u2502   \u251c\u2500\u2500 data-access.js\\n\u2502   \u2502   \u2514\u2500\u2500 index.js\\n|   |\\n\u2502   \u2514\u2500\u2500 <entity>\\n\u2502       \u251c\u2500\u2500 typedefs.js\\n\u2502       \u251c\u2500\u2500 resolvers.js\\n\u2502       \u251c\u2500\u2500 data-access.js\\n\u2502       \u2514\u2500\u2500 index.js\\n|\\n\u2514\u2500\u2500 manuscript-state-machine\\n\u2514\u2500\u2500 index.js\\n```\\n\n\n## Context\\nBuilding forms in Rails that are compliant with the GOVUK Design System involve\\nmanually declaring the correct HTML structure, class names and ARIA attributes,\\nwhich is time-consuming and hard to get right.\\nAdditionally, our validation errors currently use Rails' default pattern, rather\\nthan the one recommended for use as part of the design system, which is designed\\nwith accessibility in mind.\\n\n## Decision\nWe will use DfE's `govuk_design_system_formbuilder` to simplify the creation of\\nGOV.UK Design System-compliant forms.\\nAs we are currently using Simple Form rather than Rails' default form builder\\nfor our other forms, the two form builders can co-exist for the time being,\\nwhilst we transition the forms over.\\n\n\n#### Context and Problem Statement\\nA technology stack is required for the projects data access application programming interface (API).\\n#### Considered Options\\n- ASP.NET Web API (C#)\\n#### Decision Outcome\\nChosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n\n## Decision\nChosen option: \"ASP.NET Web API (C#)\", because\\n- This can target .Net Core 2\\n- Dapper Object Relational Mapper (ORM) can be used, leveraging reuse of skills already within Trade Me.\\n- A repository pattern can be applied facilitating Unit Testing.\\n([back](README.md))\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## **Context**\\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\\nWe will look at two options for validation:\\n- **Data Annotations**\\nThis involves \"annotating\" each class model with specific validation, such as\\n- `[Required(ErrorMessage = \"This field is required\")]`\\n- `[MaxLength(20)]`\\nThere are a number of issues with this approach:\\n- Validation is scattered throughout the codebase as attributes on data model classes\\n- Testing is not straightforward\\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\\n- Does not allow for conditional validation\\n- **Fluent Validation**\\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\\n- Lives outside of the data model classes\\n- Very easy to test validation in isolation\\n- Error messaging can be externalised using dependency injection\\n- Allows for chaining of validators and conditional validation\\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\\n\n## Decision\n", "response": "## Decision\nWe will use Fluent Validation for handling validation within our APIs. This choice is due to several compelling advantages. Fluent Validation offers a centralized location for validation logic, separate from our data model classes, leading to a cleaner and more maintainable codebase. The ease of testing validation rules in isolation enhances the robustness of our code by ensuring reliable validation processes. Moreover, its support for chaining validators and performing conditional validation aligns well with complex business logic needs. The ability to externalize error messages using dependency injection allows for flexible and customizable error handling, which can be adapted to different contexts or languages. Additionally, the minimal setup required in `Startup.cs` makes integration seamless without significantly impacting our existing code structure. Thus, Fluent Validation provides a robust and flexible framework to meet our validation requirements effectively.", "matched_ids": [1532, 727, 3776, 2386, 1660], "time": "0:00:07.815962", "input_tokens": 2633, "output_tokens": 158}
{"context": "## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n\n## Decision\nCreate a distribution-aware implementation of `TaskManager`.\\n\n\n## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need to have a way to describe the `Task` to be executed and serialize it in order to be able to store it in the `Created` event. Which will be persisted in the Event Store, and will be send in the event bus.\\nAt this point in time a `Task` can contain any arbitrary code. It's not an element of a finite set of actions.\\n\n## Decision\n* Create a `Factory` for one `Task`\\n* Inject a `Factory` `Registry` via a Guice Module\\n* The `Task` `Serialization` will be done in JSON, We will get inspired by `EventSerializer`\\n* Every `Task`s should have a specific integration test demonstrating that serialization works\\n* Each `Task` is responsible of eventually dealing with the different versions of the serialized information\\n\n\n## Context\\nBy switching the task manager to a distributed implementation, we need to be able to execute a `Task` on any node of the cluster.\\nWe need a way for nodes to be signaled of any termination event so that we can notify blocking clients.\\n\n## Decision\n* Creating a `RabbitMQEventHandler` which publish `Event`s pushed to the task manager's event system to RabbitMQ\\n* All the events which end a `Task` (`Completed`, `Failed`, and `Canceled`) have to be transmitted to other nodes\\n\n\n## Context\\nBy switching the task manager to a distributed implementation, we need to be able to `list` all `Task`s running on the cluster.\\n\n## Decision\n* Read a Cassandra projection to get all `Task`s and their `Status`\\n\n\n## Context\\nAt the time being James use the scala programming language in some parts of its code base, particularily for implementing the Distributed Task Manager,\\nwhich uses the event sourcing modules.\\nThe module `event-store-memory` already uses Scala.\\n\n## Decision\nWhat is proposed here, is to convert in Scala the event sourcing modules.\\nThe modules concerned by this change are:\\n-  `event-sourcing-core`\\n-  `event-sourcing-pojo`\\n-  `event-store-api`\\n-  `event-store-cassandra`\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n\n## Decision\n", "response": "## Decision\nImplement a centralized coordination mechanism for `TaskManager` to facilitate cluster-wide task orchestration. This includes:\n\n- Introducing a shared `TaskRepository` that can store and retrieve task details across the cluster.\n- Developing a communication protocol using a message broker like RabbitMQ or Kafka to synchronize `TaskManager` instances across nodes.\n- Implementing a global task registry that allows each node to be aware of tasks running on other nodes and their statuses.\n- Creating APIs for querying and managing tasks at the cluster level, ensuring visibility and control over all tasks in the distributed system.", "matched_ids": [2874, 2864, 2121, 2140, 2862], "time": "0:00:03.412607", "input_tokens": 888, "output_tokens": 116}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael\\nNygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n", "response": "## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n", "matched_ids": [867, 463, 4823, 1833, 3988], "time": "0:00:03.516054", "input_tokens": 411, "output_tokens": 42}
{"context": "## Context\\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\\nsandboxed environment. A Docker image has been created to help with this but this still\\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\\nto run Bitcoin miners and other compute-stealing stuff.\\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\\nbe found in the [notes on Isolation](https://github.com/martinklepsch/cljdoc/blob/72da65055ab94942f33fb63b29b732e81b559508/doc/isolation.md)\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWhile most of cljdoc's publishing pipeline could be ran with\\n\"job-runners\" (e.g. CircleCI, AWS Lambda) the isolation provided by\\nthose isn't required for many parts of the process.\\nFurthermore some API to trigger documentation builds will be required\\nat some point and this API will need to hold some credentials, e.g.\\nAWS keys for S3 and CircleCI API tokens. Exposing access to job-runners\\ndirectly would make it harder to implement things like rate-limiting etc.\\nAn API may also be useful to expose further utility endpoints, like\\nways to list available versions or download the metadata for a given\\nnamespace or artifact.\\n\n## Decision\nWe will implement a server component which has the following responsibilties:\\n- be a central API to end users to request documentation builds\\n- hold credentials for various services which should not be exposed in analysis sandbox\\n- manage flow of documentation builds\\n- trigger analysis of jars\\n- receive webhooks about completed analysis\\n- populate Grimoire store using analysis results and a project's Git repository\\n- build HTML documentation based on data in Grimoire store\\n- deploy HTML documentation to S3/Cloudfront\\n\n\n## Context\\ncljdoc uses CircleCI as a sandbox to run analysis on projects as\\noutlined in [ADR #0008](0008-use-circleci-as-analysis-sandbox.md).\\nThe process currenlty works like this:\\n1. cljdoc queues a build\\n1. CircleCI eventually runs the build\\n1. cljdoc is notified via a webhook\\nThis approach is nice because we don't need to constantly check the status\\nof a build until it eventually finishes but it also has a few drawbacks:\\n- Testing anything involving webhooks in a local development\\nenvironment is a serious pain\\n- Code related to running the analysis and storing it's result are\\nspread accross multiple places making the code harder to follow and\\nmaintain\\n\n## Decision\nMove to a model where the respective analysis service (CircleCI or Local)\\nexposes a blocking interface for running the analysis.\\nFor CircleCI this blocking interface will simply poll the CircleCI build until\\nit reached [lifecycle `\"finished\"`](https://circleci.com/docs/api/v1-reference/#build).\\n\n\n## Context\\ncljdoc's deployment story has been simplistic but effective. To recap:\\n- During CI a zip file is pushed to S3 that contains all files to run the application\\n- On the live server there is systemd service that will download an archive and run it. The\\nversion of the downloaded archive is specified via a file on the server.\\nUpdating simply required updating a file on the server and restarting the service.\\nThe issue with this approach however is that every time a new release was pushed to the server\\nthe restart of the systemd service would incur up to a minute of downtime. While this generally\\nisn't a huge deal it discourages certain development practices that may be desirable such as\\nContinuous Deployment.\\nOur existing deployment setup (and tools used by it) are poorly equipped to handle this kind of\\ndeployment scenario. A large amount of bash scripts would be required to start a new cljdoc server,\\nwait for it to become available, update nginx's upstream port, and kill old cljdoc server instances\\nin a repeatable, automated manner. Likely these bash scripts would be error-prone and turn into\\nsomething nobody likes to touch.\\n\n## Decision\nImplement a *canary deploy* mechanism for the cljdoc server application using\\n[Nomad](https://nomadproject.io) and [Traefik](https://traefik.io).\\nWhile both of these tools are probably aimed at much more complex workloads they provide the\\nfollowing benefits over the existing systemd/Nginx setup:\\n- Automatic SSL certificates via Lets Encrypt\\n- Declarative specification of jobs and their desired update semantics\\n- APIs to schedule new jobs and cycle old/new deployments\\n- Health checks to verify new deployments work as expected\\n- Machine images become much simpler since they only need Nomad, Consul, Docker\\nThis simplifies a lot of cljdoc's operational tasks while also enabling Continuous Deployment.\\n\n\n## Context\\nWe need to be able to search (documented) projects, whether\\nthey come from Clojars or Maven Central.\\nSee https://github.com/cljdoc/cljdoc/issues/85\\n\n## Decision\nImplement our own search, using direct integration with Lucene. Download and\\nindex artifact list from Clojars and \"org.clojure\" artifacts from Maven Central.\\nWe will use Lucene as that is the absolutely prevailing solution for search in\\nJava. Direct Java interop is quite idiomatic in Clojure; it isn't too much work as we\\nonly need to implement the parts relevant for us and not a generic Lucene wrapper.\\nWe avoid the risk of depending on incomplete and potentially abandoned library\\n(as happend to Clojars with clucy). And to be able to use Lucene efficiently we\\nneed to understand it sufficiently anyway.\\n\n\n## Context\\nThere is a lot of things to learn, not only the ecosystem of clojure, we what create a high scalable application deploy in cloud, with autoscale, resilience, geographic distribution, multyples clouds providers, etc.\\n\n## Decision\nWe start with the most simple solution, a console application, after that we try to evolve to an API, Web Page, Microservices, etc.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\\nsandboxed environment. A Docker image has been created to help with this but this still\\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\\nto run Bitcoin miners and other compute-stealing stuff.\\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\\nbe found in the [notes on Isolation](https://github.com/martinklepsch/cljdoc/blob/72da65055ab94942f33fb63b29b732e81b559508/doc/isolation.md)\\n\n## Decision\n", "response": "## Decision\nWe will leverage a continuous integration (CI) service, such as CircleCI or GitHub Actions, to run the analysis of untrusted Clojure code, effectively using the CI environment as a sandbox. This approach provides several benefits:\n\n- **Isolation**: The CI service provides an isolated environment for each job, reducing the risk of malicious code affecting our infrastructure.\n- **Rate Limiting and Quotas**: CI services inherently offer rate limiting and quotas, protecting us against abuse like cryptocurrency mining.\n- **Resource Management**: Offloading execution to a CI service transfers the responsibility of resource allocation and management, simplifying our operational overhead.\n- **Scalability**: We can leverage the scalability of the CI provider to handle varying workloads without requiring additional infrastructure investments on our part.\n\nWe will also implement monitoring and alerts for job failures and unusual resource usage patterns to quickly identify and mitigate possible abuse.", "matched_ids": [2605, 2606, 2610, 2609, 108], "time": "0:00:03.718372", "input_tokens": 1552, "output_tokens": 182}
{"context": "## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\\nAs part of this, considerations were made around:\\n* Ease of use for creating new sites\\n* Ability to share components\\n* Ability to segregate content for specific channels (sites)\\n* Ability to share content up and down the stack where needed and appropriate\\n* Permissions model required to support this model\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\n### Options\\n####\u00a0NHS Digital BloomReach\\nWe looked at the implementation of document management functionality in BloomReach delivered as part of the work for NHS Digital - https://github.com/NHS-digital-website/hippo. This project provides a good view of what native BloomReach document management and publishing looks like, it delivers a flexible content model for publishing to HTML documents.\\n#### Office 365\\nIn addition we should look at the current usage of Office 365 to determine whether the platform would be suitable for integration with BloomReach.\\n#### FutureNHS\\nFinally looked at the FutureNHS collaboration platform to see whether it would be suitable for integration with BloomReach. Whilst the platform in its current state would be a good candidate for integration, we discovered that at the time of writing, this platform is due to be rewritten.\\n\n## Decision\nWe believe that Microsoft Office 365 and Sharepoint provide a good basis for the platforms document management capabilities and we have proved that it can be successfully integrated.\\nHaving looked at the requirements for LKS staff, particularly around the ability to support a range of document types such as spreadsheets and presentations, our belief is that we would be better placed to integrate an external document management system. The NHS Digital publishing platform provides an excellent HTML publishing model and workflow, however extending it to support a broader range of document types would be complex.\\nFutureNHS may provide a good candidate for integration in the future, however at the time of writing it is difficult to recommend as the product is in the process of being rewritten.\\n\n\n## Context and Problem Statement\\nisland.is will be maintaining and publishing content from many different government agencies and institutions. Their technical skill may vary a great deal, the content skill may also be lacking, therefore it is paramount for the system to be user friendly and intuitive.\\nAgencies and institutions should have enough autonomy with regards to editing content they are responsible for, to minimise the manual labour required by the island.is editors.\\nWhich CMS system would best suit the needs of island.is?\\n## Decision Drivers\\n- Content needs to be editable by non technical users\\n- Content needs to be accessible across multiple domains and platforms\\n- Setup should be simple for developers new to the project\\n- The system should manage flexible content structures to limit systems impact on design\\n- The system should be user friendly and easy to use for a non technical person\\n- The system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute\\n\n## Decision\n- Content needs to be editable by non technical users\\n- Content needs to be accessible across multiple domains and platforms\\n- Setup should be simple for developers new to the project\\n- The system should manage flexible content structures to limit systems impact on design\\n- The system should be user friendly and easy to use for a non technical person\\n- The system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute\\nDevs narrowed the choice down to two options Contentful and Contentstack.\\nBoth systems meet the required featureset.\\nA decision from management was made to use Contentful.\\nContentful is deemed to have a larger presence in the Icelandic dev community.\\nContentful is also believed to have a stronger funding base.\\nContentful is already implemented in some of our projects.\\n\n\n## Context and Problem Statement\\nisland.is will be maintaining and publishing content from many different government agencies and institutions. Their technical skill may vary a great deal, the content skill may also be lacking, therefore it is paramount for the system to be user friendly and intuitive.\\nAgencies and institutions should have enough autonomy with regards to editing content they are responsible for, to minimise the manual labour required by the island.is editors.\\nWhich CMS system would best suit the needs of island.is?\\n## Decision Drivers\\n- Content needs to be editable by non technical users\\n- Content needs to be accessible across multiple domains and platforms\\n- Setup should be simple for developers new to the project\\n- The system should manage flexible content structures to limit systems impact on design\\n- The system should be user friendly and easy to use for a non technical person\\n- The system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute\\n\n## Decision\n- Content needs to be editable by non technical users\\n- Content needs to be accessible across multiple domains and platforms\\n- Setup should be simple for developers new to the project\\n- The system should manage flexible content structures to limit systems impact on design\\n- The system should be user friendly and easy to use for a non technical person\\n- The system needs to offer a suitable workflow option to ease content management once multiple agencies start to contribute\\nDevs narrowed the choice down to two options Contentful and Contentstack.\\nBoth systems meet the required featureset.\\nA decision from management was made to use Contentful.\\nContentful is deemed to have a larger presence in the Icelandic dev community.\\nContentful is also believed to have a stronger funding base.\\nContentful is already implemented in some of our projects.\\n\n\n## Context\\nThe cloud platforms team has had a number of discussions about where to hold documentation. We have tried using confluence for technical documentation but it has largely gone stale through lack of updates.\\nFor the development of the new platform we want to keep _technical_ documentation close to the code that implements that documentation. An example of this is the [kubernetes investigations](https://github.com/ministryofjustice/kubernetes-investigations) repo which holds our experiments into the use of kubernetes.\\nPutting technical documentation in GitHub has additional benefits:\\n* Using pull requests gives us a clear review and approval process\\n* It is part of the same workflow as other activities that we do on a day to day basis (e.g. writing code)\\n* The information can be held in the open and viewed by anyone\\n\n## Decision\n1. Our architecture decision log for the new cloud platform will be held in GitHub\\n\n\n## Context\\nWe need to design the building blocks of the Digital Citizenship platform. We will make decisions about the overall architecture and the building blocks we're going to use to satisfy the functional and non-functional requirements.\\nWhen deciding what technologies to use and how to satisfy those requirements, we're going to evaluate whether we want to build certain components from scratch, use open-source solutions managed by us or rely on existing public cloud components, managed by 3rd party providers.\\nAs the world of cloud technologies continues to accelerate, we should absorb new developments into how we work. Leading technology organisations are rapidly embracing new tools like \"serverless\" computing.\\nAt the infrastructure and application level we should expect our applications to be resilient, flexible and API-driven. We should have the tools and practices in place to manage and secure a distributed range of tools accessed over the internet.\\nDecision criteria will also include:\\n* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\n\n## Decision\n* elasticity and resilience\\n* pay-as-you-go pricing\\n* exploiting economies of scale\\n* exploiting falling costs of infrastructure over time\\n* quality of management tools\\n* best of breed security\\n* flexibility and opportunity costs\\nWe decide to build the components of the Digital Citizenship initiative on top of scalable, managed and cost-effective components provided by the leading public cloud providers.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\\nAs part of this, considerations were made around:\\n* Ease of use for creating new sites\\n* Ability to share components\\n* Ability to segregate content for specific channels (sites)\\n* Ability to share content up and down the stack where needed and appropriate\\n* Permissions model required to support this model\\n\n## Decision\n", "response": "## Decision\nAfter evaluating the options, we have decided to integrate Bloomreach's 'Channel' concept for managing our document management capabilities and multiple sites, leveraging its native strengths. The 'Channel' concept will facilitate easier creation and management of new sites while ensuring component sharing and content segregation across different channels. \n\nThe ability to share and segregate content effectively aligns with our need to manage both common and unique content across various domains. Bloomreach also offers a robust permissions model to support these requirements, allowing us to control access and editing rights meticulously.\n\nThe decision to utilize Bloomreach natively is based on its alignment with the project's need for flexibility, scalability, and security. We believe this approach offers a cohesive solution with minimized complexity compared to integrating an external document management platform.", "matched_ids": [1195, 2000, 4770, 626, 1182], "time": "0:00:04.028341", "input_tokens": 1935, "output_tokens": 153}
{"context": "## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https://symfony.com/doc/current/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## **Context**\\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\\nWe will look at two options for validation:\\n- **Data Annotations**\\nThis involves \"annotating\" each class model with specific validation, such as\\n- `[Required(ErrorMessage = \"This field is required\")]`\\n- `[MaxLength(20)]`\\nThere are a number of issues with this approach:\\n- Validation is scattered throughout the codebase as attributes on data model classes\\n- Testing is not straightforward\\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\\n- Does not allow for conditional validation\\n- **Fluent Validation**\\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\\n- Lives outside of the data model classes\\n- Very easy to test validation in isolation\\n- Error messaging can be externalised using dependency injection\\n- Allows for chaining of validators and conditional validation\\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\\n\n## Decision\n**Fluent Validation**\\nFluent Valdation is widely used, offers a lot of flexibility and allows for a clean, customisable and testable approach to validation.\\n\n\n## Context\\nMost of the JavaScript Code on the FundraisingFrontend already has the [[ https://facebook.github.io/flux/docs/in-depth-overview.html#content | Flux architecture ]] with its one-way data flow (using [[ https://redux.js.org/ | Redux ]]), but it does not use one of the popular reactive \"component\" frameworks like Vue or React. Instead, it uses self-written \"components\" (two-way binding with handling of DOM events) and \"view handlers\" (one-way binding DOM manipulators) that are tied to the markup of the page via jQuery, in an attempt to do progressive enhancement and decouple the markup from the functionality.\\nThe current JavaScript code has several drawbacks:\\n* **JavaScript resource size.** The components and stores are built as one big \"library\" file (called `wmde.js`, with a global object called `WMDE`), instead of having separate \"entry points\" for the different pages (donation, membership). The current \"entry points\", `donationForm.js` and `membershipForm`, add one more JavaScript resource that the browsers needs to download, adding HTTP overhead and latency.\\n* **Hard to understand.** While the [[ https://github.com/wmde/FundraisingFrontend/blob/master/doc/HOWTO_Create_a_form.md | architecture is documented ]], the setup lacks the in-depth explanations, code snippets and tutorials that common frameworks come with. Also, the code itself does not use modern ECMAScript features (classes, spread operator, arrow functions). Instead it uses  custom, factory-function based \"classes\", making the code harder to understand.\\n* **Hard to extend.** Where Vue and React have ecosystems attached to them, all code for the current JavaScript - asynchronous validation, connecting the store to the views, etc - is custom, adding to the accidental complexity and maintenance burden.\\n* **Hard to reuse.** While the self-written components are quite flexible and \"pluggable\", the entry point scripts are very long and hard to understand, since there is no hierarchy of elements. Instead, they are a big factory that initializes all the classes. That initialization code is duplicated across scripts.\\n\n## Decision\nGoing forward, we will use Vue to render the frontend. We chose it for the following reasons:\\n* It's a mature, tested, well-documented widely used framework with an open source license and an active ecosystem\\n* There is already some knowledge about Vue in the developer and UX teams\\n* Vue is used in wikidata, making knowledge sharing easier.\\nWe will write all new features in Vue and ECMAScript 2015 (and later), refactoring and cleaning up the existing code base. The redux \"reducers\" and \"state aggregators\" will move to Vuex modules, while the Twig templates, \"view handlers\" and \"components\" of the old code base become Vue components.\\n\n\n## Context\\nWe need to have the mechanism for supporting and validating common architecture principles in all application.\\n\n## Decision\nArchitecture tests with using Archunit should be implemented.\\n\n\n## Context\\nWe decided to use the Clean Architecture ([ADR #10](0010-use-clean-architecture-for-writes.md)) and create Rich Domain Models ([ADR #11](0011-create-rich-domain-models.md)) for each module. We need to define or use some construction elements / building blocks to implement our architecture and business logic.\\n\n## Decision\nWe decided to use **Domain-Driven Design** tactical patterns. They focus on the Domain Model implementation. Especially we will use the following building blocks:\\n- Command - public method on Aggregate (behavior)\\n- Domain Event - the immutable class which represents important fact occurred on a special point of time (behavior)\\n- Entity - class with identity (identity cannot change) with mutable attributes which represents concept from domain\\n- Value Object - immutable class without an identity which represents concept from domain\\n- Aggregate - cluster of domain objects (Entities, Value Objects) with one class entry point (Entity as Aggregate Root) which defines the boundary of transaction/consistency and protects business rules and invariants\\n- Repository - collection-like abstraction to persist and load particular Aggregate\\n- Domain Service - stateless service to execute some business logic which does not belong to any of Entity/Value Object\\n\n\n## Context\\nAs the code rapidly grows we need a common structure and approach that will keep the code tidy and coherent.\\nThere are various aspects, the initial concern was the data access separation. The issue #140 initially talked about how this could be separated in terms of an ORM (Object-Relational Mapping) or a DAL (Data Access Layer).\\nHowever, this discussion also soon covered the structure of the code in general and how to structure it for ease of development and best practise.\\n### Rejected Options\\nWe considered the following options:\\n* The Pubsweet Way - This was to define a base class \"Model\" and extend this for each entity. This was rejected as its another self made ORM but and has validations that we don't want.\\n* Other ORMs : Waterline, sequelize, bookshelf - General purpose ORMs were rejected based on the experience that the time spent going up the learning curve and sifting through the docs when things go wrong is greater then simply rolling your own.\\n* Query Builders : Knex.js -These were not entirely ruled out, and were deemed compatible with a good code structure that would allow the database access to be encapsulated in a way that would allow a query builder to be used where necessary.\\n\n## Decision\nIn summary the server-side code will be structured below the `server` folder as follows:\\n* **entities** - This folder will contain subfolders relating to named entities in the system. See [\"Data Model Specification\"](https://docs.google.com/document/d/1KU-DLMNhPxjQF2j8HVlJvenvttPLxgtbTHo8Sy_PNRc/).\\n* **entities/\\<entity\\>** - The example below shows `Mansuscript` as an example entity, this folder will contain a common set of files that describe the entity's behaviour. Each of these have a particular purpose explained below:\\n* index.js - The main business logic, e.g. for `Manuscript` this could contain `getAuthor()`\\n* typedefs.js - Contains the GraphQL types pertinent to this entity.\\n* resolvers.js - Contains the GraphQL interface specified in terms of `Query` and `Mutation` 's for this entity. These in general should map onto the exposed functions from index.js. For example, `Manuscript` may contain a `Query` for `allManuscripts` and a `Mutation` for `createManuscript`.\\n* data-access.js - Contains the functions for saving to and loading from the database. All references to the database and SQL that is written should remain private to this file.\\n* **manuscript-state-machine** - It is anticipated that this is where the most orchestration of the entites will take place. This has been created as a separate folder so that it can expand to implement the state transitions currently defined for the manuscript.\\n### Example\\n```\\nserver\\n|\\n\u251c\u2500\u2500 entities\\n|   |\\n\u2502   \u251c\u2500\u2500 manuscript\\n\u2502   \u2502   \u251c\u2500\u2500 typedefs.js\\n\u2502   \u2502   \u251c\u2500\u2500 resolvers.js\\n\u2502   \u2502   \u251c\u2500\u2500 data-access.js\\n\u2502   \u2502   \u2514\u2500\u2500 index.js\\n|   |\\n\u2502   \u2514\u2500\u2500 <entity>\\n\u2502       \u251c\u2500\u2500 typedefs.js\\n\u2502       \u251c\u2500\u2500 resolvers.js\\n\u2502       \u251c\u2500\u2500 data-access.js\\n\u2502       \u2514\u2500\u2500 index.js\\n|\\n\u2514\u2500\u2500 manuscript-state-machine\\n\u2514\u2500\u2500 index.js\\n```\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https://symfony.com/doc/current/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n\n## Decision\n", "response": "## Decision\n\nWe will implement our own custom validation logic in the use case layer. This decision aligns with the following considerations:\n\n- **Decoupling from External Libraries**: By writing our own validation logic, we prevent binding our domain layer to any specific library. This decouples our architecture from external dependencies and any issues arising from their maintenance cycles.\n  \n- **Simplicity and Clarity**: Given that most of our validation requirements involve simple checks (e.g., required fields), implementing our custom logic avoids the potential complexity introduced by external libraries, keeping our implementation straightforward and easy to understand.\n\n- **Maintainability**: Our validators can be custom-tuned to our business requirements, ensuring that the logic can adapt quickly to any changes in business rules without depending on third-party updates.\n\n- **Learning Curve**: Developers can focus on mastering our own API for validation rather than learning an external library's API, facilitating easier onboarding and flexibility within the development team.\n\n- **Validation Logic in Use Cases**: By incorporating validation logic within the use case layer, we ensure that validation is an integral part of the business logic execution flow, validating inputs directly where they are utilized, which aligns with the principles of Clean Architecture. This allows for coherent validation within the context of each specific use case and keeps the domain entities free from validation concerns.\n\nThis approach will encourage a coherent design of validation logic that directly reflects the business rules and lifecycle of the use cases, improving maintainability and clarity.", "matched_ids": [2307, 1530, 2502, 898, 3776], "time": "0:00:06.074840", "input_tokens": 2473, "output_tokens": 297}
{"context": "## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context and Problem Statement\\nWe need to store changes data (Pull Requests and Reviewes) in a scalable way. The stored data must be easily accessible in an intelligible manner.\\n\n## Decision\nChosen option: \"ElasticSearch\".\\nBecause it fits better our need regarding the style of data we expect to store and how we expect to query the data.\\n\n\n## Context\\nEarlier we went with a [materialized view approach](https://github.com/simpledotorg/simple-server/blob/master/doc/arch/008-materialized-views-for-dashboards.md) for dashboard reporting data. This approach has worked out well for the current reports but has limitations in terms of scaling and complexity.\\nWe currently have a need for a new set of dashboard reports that rely on similar sets of data, and anticipate more needs for summary data for monthly and quarterly reports. We'd like to try to find a simpler, more stable, and more scalable approach to making that data easily accessible.\\n\n## Decision\nWe will create rollup summary tables for to capture the \"most recent blood pressure per patient per period\". These rollup tables will be updated in real-time as blood pressures are captured, removing the need for recurring batch jobs. We will denormalize as much data as we need to to make reporting queries simple and cheap to execute, to prevent the need for any complicated SQL queries or additional data layer caching.\\nOnce the current period has completed, we will treat the data in these rollup tables for that period as immutable. This will let us and our users have more confidence in reports from past periods, as they will not change due to patients moving to different facilities or changes in how we calculate things.\\nWe will focus on the District controlled blood pressure trend report as a first test case for this sort of approach, as it is a key report and one we'd like to gather feedback on as quickly as possible.\\n\n\n## Context\\nWe adopted the `PublicActivity` gem (as documented in [0019](https://github.com/UKGovernmentBEIS/beis-report-official-development-assistance/blob/731b91f5773c4306f919433af81976d29c5feef1/doc/architecture/decisions/0019-use-public-activity-gem.md)), but the information we record has never been used, or displayed to the user.\\nNow we're looking at surfacing information about changes to end users it's becoming increasingly clear that is `PublicActivity` is not fit for our needs, and would be easier to build our own approach to logging what changed were made by what user.\\n\n## Decision\nWe have decided to introduce a `HistoricalEvent` model to record changes to models, starting with `Activity`s.\\nOnce we have applied `HistoricalEvents` to all the models we need to track, we can remove all the calls to `PublicActivity` and remove the gem entirely.\\n\n\n## Context\\nWe'd like to be able to script metrics reporting to third party consumers of\\nRaster Foundry. Currently we manually assemble metrics when people ask for\\nthem. However, enough time has passed and we have consistent enough obligations\\nthat we know a few that would be scriptable. This ADR seeks to answer the\\nquestion of how we ought to script production of a particular subset of\\nmetrics, specifically, tile server requests by user.\\n\n## Decision\nWe should start with the second option, being mindful of the potential later\\nneed to target configurable metrics sinks, but not designing for that\\nparticular problem from the outset. We\u2019ll start with tile metrics and a\\ndatabase table that holds just enough to get us the metrics we want.\\nThe reasons the second option is likely to be safer are that it avoids\\nincreasing infrastructure complexity and that the marginal cost of its lock-in\\nis lower than the marginal cost of additional vendor lock-in. We bear the\\nburden of someday migrating everything from http4s and Postgres if we need to,\\nwhile additional AWS lock-in is a cost that either makes it more difficult for\\nanyone to deploy a bare metal solution or makes the cost of paying us to do so\\nmore expensive.\\nThe second option is also more extensible, since storing this information in\\nthe database allows us to expose it via an API and to use the same information\\nfor authorization that we use elsewhere.\\nThe downside of choosing the second option is that it couples metrics updates\\nto our release cycle, while we could have avoided that coupling with the AWS\\noption by throwing every piece of information we have into a data lake. This\\ncost is acceptable given the other benefits and is in line with a strategy of\\nnot collecting data for the sake of collecting data.\\n\n\n## Context\\nWe need to know what the application is doing in a more light weight way than\\nscraping logs. We need to be able to monitor KPIs of the application in order\\nto understand the health of the application. This will allow us to react and\\npotentially pro-actively initiate measures as to ensure the application's\\nhealth if sound. Ultimately providing a better service for our users.\\n\n## Decision\nWe will use Prometheus to monitor and alert on the state of the application.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n\n## Decision\n", "response": "## Decision\nWe will implement a change metrics system using a tagging approach. Each task will be associated with a set of standardized tags (e.g., \"Feature Request\", \"Bug Fix\"), which can be applied in any task tracker system. This tagging system will allow us to categorize changes and compute metrics on the ratios of different types of changes.\n\nThis approach is chosen because it allows for a straightforward implementation without making any assumptions about the specific task tracking system in use, as tags are a common feature in most systems. It also offers flexibility to support a range of generic fields by simply adding more standardized tags as needed. Additionally, this approach can be integrated into various reporting and analytics tools to generate insights as required.", "matched_ids": [4643, 1701, 2403, 1685, 3469], "time": "0:00:03.584176", "input_tokens": 1313, "output_tokens": 142}
{"context": "## Context\\nOur service-operator allows service teams to provision various AWS services by\\ndeclaratively defining resources and submitting them via the kubernetes api.\\nSome of these resources require IAM to authorise how the provisioned service\\ncan be used. The types of actions that can be performed on.\\n#### Example\\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\\n```\\n---\\napiVersion: storage.govsvc.uk/v1beta1\\nkind: S3Bucket\\nmetadata:\\nname: s3-bucket-sample\\nspec:\\naws:\\nLifecycleRules:\\n- Expiration: 90days\\nVersioning:\\nEnabled: true\\n```\\nIn order to access a provisioned bucket via the the AWS SDK users will require\\nan IAM role/policy that allows access.\\nWe want things like bucket ACL, versioning configuration and lifecycle policy\\nto be defined declaratively via the resource manifest (see example above), and continuously managed\\nby the service operator.\\nWe want users of the provisioned bucket to be able to read back all\\nconfiguration, and be able to fully utilise the specific bucket for reading,\\nwriting and managing their objects within the provisioned bucket, but we want\\nto avoid giving permissions to users that could cause conflicts with the\\nproperties that are managed by the service operator's reconcile loop.\\nFor example, given the example manifest above, we would like to avoid giving\\npermissions that would allow a user to alter the Expiration LifeCycleRules,\\nsince any changes the user made would be periodically overwritten by the\\nservice operator's reconciliation.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\n[Amazon announced](https://aws.amazon.com/blogs/opensource/aws-service-operator-kubernetes-available/) an AWS Service Operator for Kubernetes in October 2018.\\nThere is a need for one or more users to provision SQS queues for their application to interact with.\\nThe AWS Service Operator consists of a container that sits in the Kubernetes cluster and monitors for custom resource types which it maps to the appropriate CloudFormation resources and deploys.\\nIt supports the following resources:\\n* CloudFormation Template\\n* DynamoDB\\n* ECR Repository\\n* ElastiCache\\n* S3 Bucket\\n* SNS Subscription\\n* SNS Topic\\n* SQS Queue\\n\n## Decision\nWe have included AWS Service Operator as part of GSP so that users can provision SQS queues.\\n\n\n## Context\\nAmazon Web Services (AWS) are our current Infrastructure as a Service (IaaS)\\nprovider. Our deployment tooling (Concourse, Terraform, BOSH, etc.) and\\nCloud Foundry components (Cloud Controller, RDS broker, blobstore clients,\\netc.) use the APIs to manage or access IaaS resources.\\nThe most common mechanism for authenticating the API calls is to create an\\nIdentify and Access Management (IAM) user with the appropriate permissions,\\ngenerate an Access Key ID and Secret Access Key for that user, and export\\nthose as environment variables. `AWS_ACCESS_KEY_ID` and\\n`AWS_SECRET_ACCESS_KEY` are the standard environment variable names used by\\nmost utilities and libraries.\\nThe problem with this approach is that it's very easy to accidentally leak\\nthe plain text keys. They can appear in output from your shell, which you\\nmight copy+paste into a gist or email when debugging a problem. You might\\nadd them to your shell configuration or include them in a script, which can\\nbe pushed to a public code repository.\\nOur team have leaked keys like this on more than one occasion. It's worth\\nnoting that even if you realise that you've done this, delete the commit and\\nrevoke the keys, they may have already been used maliciously because\\nautomated bots monitor sites like GitHub using the [events firehose][] to\\ndetect any credentials.\\n[events firehose]: https://developer.github.com/v3/activity/events/\\nAs an alternative to using pre-generated keys, AWS recommends that you use\\n[IAM roles and instance profiles][] when accessing the API from EC2\\ninstances. You delegate permissions to the EC2 instance and temporary\\ncredentials are made available from the instance metadata service. Most\\ntools and libraries automatically support this. The credentials are\\nregularly rotated and never need to be stored in configuration files.\\n[IAM roles and instance profiles]: http://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#use-roles-with-ec2\\n\n## Decision\nTo reduce the likelihood of us leaking AWS keys we will use IAM roles and\\ninstance profiles for all operations that run from EC2 instances. This\\nincludes everything that happens within Concourse and Cloud Foundry.\\nTo reduce the impact of us leaking AWS keys we will use an IAM policy with\\nan [`aws:SourceIp` condition][condition] to\\nenforce that IAM accounts for team members are only used from the office IP\\naddresses.\\n[condition]: http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_examples.html#iam-policy-example-deny-source-ip-address\\nThe IAM roles, profiles, and policies will be managed by our\\n[aws-account-wide-terraform][] repo.\\n[aws-account-wide-terraform]: https://github.digital.cabinet-office.gov.uk/government-paas/aws-account-wide-terraform\\n\n\n## Context and Problem Statement\\nMost operators out there operate in one of these two modes:\\n1. Cluster-wide operator. Can manage resources in all namespaces, with cluster-wide admin privilege. A single operator running on the cluster.\\n2. Namespaced operator. Can manage resources in the namespace it's deployed in, with admin permissions in that namespace. Several operators can be running in different namespaces.\\nThe first option (cluster-wide single operator) has some major drawbacks:\\n* it does not scale well with the number of clusters\\n* it requires elevated permissions on the cluster\\nThe second option (namespace operator) also has some major drawbacks:\\n* it does not play well with cross-namespace features (a single enterprise license pool for multiple clusters in multiple namespaces, cross-cluster search and replication on clusters across namespaces)\\n* to deploy 5 clusters in 5 different namespaces, it requires 5 operators running. A single one could have been technically enough.\\n## Decision Drivers\\n* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\n\n## Decision\n* Scalability (down): must be able to scale down to single-cluster deployments without being overly complex to manage.\\n* Scalability (up): Must be able to scale up with large k8s installations and manage tens of thousands of clusters simultaneously.\\n* In any sufficiently large installation with clusters under load there is going to be high variance between response times for different ES API calls, and one clusters responsiveness should not be able to negatively affect the operations of any other cluster.\\n* Security: The solution should have an easy to understand story around credentials, RBAC permissions and service accounts.\\n* As far as possible, adhere to the principle of least amount of access: we should not require more permissions than strictly necessary for the operators to accomplish what they need to.\\nChosen option: option 2 (configurable operator), because it gives us more flexibility on the deployment strategy, and allows restricting RBAC permissions to a finer-grained level.\\n### Positive Consequences\\n* Much more flexibility to cover various deployment scenarios\\n* a single cluster-wide operator\\n* one operator per namespace\\n* one operator for all production namespaces, another one for all staging namespaces\\n* and so on\\n* We don't have to require cluster-level permissions to handle enterprise licensing\\n* A single operator concept, no namespace/global/ecosystem vocabulary madness\\n### Negative Consequences\\n* Too many options can lead to confusion, we need proper documentation\\n* Increased yaml complexity: need to develop a tool to generate yaml specifications\\n* The controller-runtime is not ready yet for multi-namespace watches\\n\n\n## Context\\nThe service will be used by a variety of different types of users, and we need\\nto ensure that those users are only able to access the parts of the service that\\nwe want them to.\\nOur current chosen authentication provider, Auth0, has support for assigning\\nroles to users, but this couples the service tightly to their service, so we\\nshould avoid this.\\n\n## Decision\nWe will use the 'Pundit' gem to manage users' access to the service. The\\npermissions will be grouped into roles that can then be assigned to users\\nrequiring a particular level of access.\\n\n\n## Context\\nAs part of the ACS with AWS Services initiative, we are looking to use more Amazon services around our Kubernetes deployment. To do this we have to find a way to provision Amazon services like S3 buckets, EKS Cluster, Amazon MQ, Aurora DB which are outside of our current helm deployment. We have investigated 3 options for doing this provisioning.\\nThe first option would be to use CloudFormation templates to do the provisioning.\\nCloudFormation would be in alignment with our AWS First company direction and it can allow us to provision all types of Amazon resources needed.\\nAn additional plus is that we have experience working with this tool within our team.\\nHowever, CloudFormation locks us to Amazon only services and makes us have separated tools for provisioning Alfresco Content Services and the adjacent resources.\\nThe second option is having Terraform as an outside of AWS provisioner.\\nTerraform allows us to provision and make use of services from different cloud providers in our solutions as well as totally unrelated services like Github, Consul, PagerDuty and more importantly Bare Metal on-prem provisioning. Terraform is also abstracting away a good part of the required metadata needed for the provisioning of resources.\\nHowever, we have limited experience in using terraform.\\nThe final option is using kubernetes controllers to deploy Amazon resources as part of the helm deployment for acs.\\nImplementing kubernetes controllers for dynamically provisioning resources along with the usual kubernetes deployment for Alfresco Content Services would make us more consistent in how we deploy our applications and would ease up maintenance in the future.\\nHowever, we would still need another way for provisioning the actual kubernetes cluster and our experience in developing custom resource definitions used in kubernetes controllers is inexistent.\\n\n## Decision\nWe will use Amazon CloudFormation templates as it is in alignment with Alfresco's AWS First direction. Also, we have experience in developing and using this tool within the company. It also brings us closer to potentially having a quickstart template for deploying the Alfresco Digital Business Platform.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nOur service-operator allows service teams to provision various AWS services by\\ndeclaratively defining resources and submitting them via the kubernetes api.\\nSome of these resources require IAM to authorise how the provisioned service\\ncan be used. The types of actions that can be performed on.\\n#### Example\\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\\n```\\n---\\napiVersion: storage.govsvc.uk/v1beta1\\nkind: S3Bucket\\nmetadata:\\nname: s3-bucket-sample\\nspec:\\naws:\\nLifecycleRules:\\n- Expiration: 90days\\nVersioning:\\nEnabled: true\\n```\\nIn order to access a provisioned bucket via the the AWS SDK users will require\\nan IAM role/policy that allows access.\\nWe want things like bucket ACL, versioning configuration and lifecycle policy\\nto be defined declaratively via the resource manifest (see example above), and continuously managed\\nby the service operator.\\nWe want users of the provisioned bucket to be able to read back all\\nconfiguration, and be able to fully utilise the specific bucket for reading,\\nwriting and managing their objects within the provisioned bucket, but we want\\nto avoid giving permissions to users that could cause conflicts with the\\nproperties that are managed by the service operator's reconcile loop.\\nFor example, given the example manifest above, we would like to avoid giving\\npermissions that would allow a user to alter the Expiration LifeCycleRules,\\nsince any changes the user made would be periodically overwritten by the\\nservice operator's reconciliation.\\n\n## Decision\n", "response": "## Decision\n\nTo balance flexibility for the users with the integrity of the configurations managed by the service operator, we will adopt a policy that separates the responsibilities between user permissions and operator control:\n\n1. **IAM Roles and Policies**\n   - We will create predefined IAM roles and policies for users accessing the provisioned S3 buckets. These roles will provide permissions for users to read and write objects within the buckets and manage their own objects without interfering with the configurations managed by the service operator.\n   - The IAM policies will explicitly deny actions related to modifying bucket-level configurations (e.g., lifecycle rules, versioning, and ACLs) that are declared and managed by the operator. This exclusion ensures that user changes do not conflict with the operator's reconciliation process.\n\n2. **Separation of Concerns**\n   - Users will be empowered to manage object-level permissions, uploads, downloads, and metadata setting, but they will not have permissions to modify configurations that are subject to the service operator's lifecycle management.\n   - Operator-managed configurations will remain exclusively the responsibility of the service operator, ensuring consistency and integrity for these critical settings.\n\n3. **Audit and Logging**\n   - Implement robust auditing and logging mechanisms to track user actions on the bucket contents. This helps in identifying any unauthorized attempts to access or modify configurations managed by the operator.\n   - Logs will provide an audit trail that helps in maintaining compliance and understanding access patterns for further refinements in IAM policies.\n\n4. **Continuous Communication and Documentation**\n   - Clearly document the roles, permissions, and restrictions for both users and operators. This documentation will ensure that users are well-informed about the boundaries of their access and the reasons behind these boundaries.\n   - Regularly communicate with users to gather feedback and address any potential issues arising from these permissions, facilitating a collaborative approach to managing access.\n\nBy implementing these strategies, we will provide users with the necessary access to utilize their provisioned resources effectively while maintaining control over critical configuration settings that are essential for operational stability and compliance.", "matched_ids": [3881, 211, 4707, 2397, 4221], "time": "0:00:08.769599", "input_tokens": 2743, "output_tokens": 400}
{"context": "## Context\\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nAs our users base is growing and we have more and more search queries, we would like to improve current search form to:\\n* Be able to search on newly added fields (e.g. created date, modified date, author, licence, etc)\\n* Allow more complex searches on existing and new fields such as:\\n* \"and\" queries to search on multiple tags\\n* \"or\" queries to search on an exclusive list of tags\\n* \"startsWith\" and \"contains\" search instead of current exact match searches\\n* synonyms\\n\n## Decision\nGiven our needs it seems pretty clear that we need a search engine, as databases search capabilities are limited and would cost more and more as data is growing.\\nWe will use Elastic Search as our search engine as it is the de facto standard and is based on battle-tested search engine\\nLucene. Our current microservices will now evolve to this schema:\\n![](microservices-architecture-with-es.png)\\nNote: ES indexes do not need to be updated in real-time when a new image/gif is uploaded, in fact we can easily\\ntolerate a few minutes of delay between an upload and its actual availability for search. All we need is to be\\neventually consistent.\\n\n\n## Context\\nTo make it easy for users to interact with our API we need a frontend that can be used by modern devices.\\n\n## Decision\nThe frontend will be build with [React](https://reactjs.org/) and [Snowpack](https://www.snowpack.dev) will be used as build tool.\\nThe application will be styled with [Tailwind CSS](https://tailwindcss.com)\\nWe also considered the following alternative solutions:\\n* Vanilla HTML would work for the scope of this application.\\n* Vanilla CSS would work, but take more time to generate nice layouts.\\n\n\n## Context\\nFollowing user research a more comprehensive text search engine was needed in order to be able to\\nuse more complex searches (and/or), full phrase search, stemming as well as geolocation searches.\\n\n## Decision\nA review of ElasticSearch and Funnelback was undertaken [here](https://docs.google.com/document/d/10xK0on4gzrB1ImZHk3Z_PoCr7V_aUpiieuLdQDg4OwQ/edit)\\nand [ElasticSearch](https://www.elastic.co/products/elasticsearch) was chosen as a candidate due to the fact that it was\\nopen source and had comprehensive documentation with an established community around it.\\n(Funnelback)[https://www.funnelback.com/] was discounted due to it being closed-source and for\\nit's lack of community support.\\nA  Docker image of GP data ([profiles-db-elastic](https://hub.docker.com/r/nhsuk/profiles-db-elastic/))\\nrunning an ElasticSearch instance has been created for use by this application and possibly by other applications.\\n\n\n## Context\\nComponents should be well documented so that the users of the library will be able to quickly start using the component.\\n\n## Decision\nFor now, we will create a README.md file inside each component folder, adjacent to the code for the component. The readme file should have the same structure and highlight some use cases, code examples and have api reference.\\n\n\n## Context\\nThe front end needs to provide a rich user experience, be modular and performant with large community support.\\n\n## Decision\nReact.js is used as the frontend framework for building the new website. React.js is used with TypeScript, which is a superset of JavaScript.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\\n\n## Decision\n", "response": "## Decision\nTo implement autocomplete functionality, we will use [Algolia](https://www.algolia.com/) as our search-as-a-service solution. Algolia provides powerful instant search capabilities and a robust infrastructure for handling autocomplete efficiently. This service will integrate seamlessly with our existing architecture and provide a responsive user experience by using client-side JavaScript libraries like [Autocomplete.js](https://www.algolia.com/doc/ui-libraries/autocomplete/introduction/what-is-autocomplete/) for the front-end. In case we need to support advanced customizations or have resources to manage our own infrastructure, we can consider [ElasticSearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters.html) to implement suggestions and autocomplete feature which is hosted in-house.", "matched_ids": [2513, 847, 3472, 1152, 2011], "time": "0:00:06.177682", "input_tokens": 878, "output_tokens": 153}
{"context": "## Context and Problem Statement\\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\\n## Decision Drivers <!-- optional -->\\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible/when if needed.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nThe state of data and UI will grow in the application as the development goes on. There needs to be good state management mechanism to handle this, particularly when components will require interaction across the component hierarchy.\\n\n## Decision\n[Redux](https://redux.js.org/) will be used for state management. It is a centralised state management container that can handle data and UI state.\\n\n\n## Context and Problem Statement\\nAs part of the Paper-edit view\\n![Screen Shot 2019-04-18 at 13 54 26](https://user-images.githubusercontent.com/4661975/56362368-8fd2bf00-61e1-11e9-9e87-edf71c0da030.png)\\n> As a user, I want to be able to get (watch or listen to) a preview of the paper-edit/program script so that I can see how my program's paper-cuts (transcripts text selections) will \"render\" as audio and/or video.\\nIn the sketch, this would be the \"canvas\" - _borrowing terminology from to the NLE video editors distinction between preview and canvas_\\nA React component that takes a sequence of audio or video clips as an input, with start and end time, and displays a player that can\\n- [ ] Seamlessly play this EDL (edit decision list) without buffer or load time in between clips\\n- [ ] Has player controls, such as\\n- [ ] progress bar\\n- [ ] play/pause\\n- [ ] stop\\nNote that the component should be able to generate the preview if the sequence is made of audio, video or a mix of audio and video files.\\nIn first version it could just deal with sequences of media, but it be good if in subsequent versions it could handle things like text, titles (as a possible way to display place holders for voice over text?) - this second part needs to be flashed out more. But the general thing to keep in mind is the extensibility of the elements to display, if that makes sense.\\n## Decision Drivers <!-- optional -->\\n* easy to reason around\\n* performant\\n\n## Decision\n* easy to reason around\\n* performant\\nThere are other examples and indications in the repo on how to enable, play, pause, and a progress bar.\\nI think under the hood, Video Context, used the HTML5 canvas to concat the videos in the sequence and then provides a unified interface to treat it as a single media.\\nChosen option: option 5 `bbc/VideoContext`, because it seems to be a performant way to display an EDL/playlist of clips with start and end times options. It is also currently being mantained by BBC R&D.\\n\n\n## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nThe issue is how often to save to local storage when a user types, previous implementation saved every 5 characters. But that caused issues [#86](https://github.com/bbc/react-transcript-editor/issues/86)\\n## Decision Drivers <!-- optional -->\\n<!-- * [driver 1, e.g., a force, facing concern, \u2026] -->\\n* A simple and straight forward way to save to local storage\\n* saving on a good frequency\\n* without introducing performance issues especially on less performant devices\\n* if possible without introducing third party dependencies\\n\n## Decision\n<!-- * [driver 1, e.g., a force, facing concern, \u2026] -->\\n* A simple and straight forward way to save to local storage\\n* saving on a good frequency\\n* without introducing performance issues especially on less performant devices\\n* if possible without introducing third party dependencies\\nChosen option: **using a js timer**.\\nIt uses a timer that can be consolidated into one final one rather then having a lot of saves being delayed, we just have one final save once after user has stopped typing for more then 5 seconds.\\nThe timer is cleared before being called so that there is only the final one left. Leaving only one final save at the end. As a performance optimization.\\n```js\\nif (this.saveTimer!== undefined) {\\nclearTimeout(this.saveTimer);\\n}\\nthis.saveTimer = setTimeout(() => {\\nthis.localSave(this.props.mediaUrl);\\n}, 5000);\\n```\\n<!-- because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n<!--\\n### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n\n\n## Context and Problem Statement\\n<!-- [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] -->\\nTo build a timed text editor, for editing audio or video transcriptions it can be useful to use an open source text editor and add functionalities to it.\\n## Decision Drivers <!-- optional -->\\n<!-- * [driver 1, e.g., a force, facing concern, \u2026] -->\\n* A simple and straight forward way to keep words and time in sync, during playback and text editing.\\n\n## Decision\n<!-- * [driver 1, e.g., a force, facing concern, \u2026] -->\\n* A simple and straight forward way to keep words and time in sync, during playback and text editing.\\nChosen option: **Draft.js**, because there is previous work in this space that has explored and stressed tests this editor.\\nAltho Quilljs has a straight forward API, despite [a quircky data model internal  representation](https://quilljs.com/docs/delta). And has been used to quickly make [text based prototypes](https://github.com/pietrop/annotated_article_generator) (see [demo](http://pietropassarelli.com/annotated_article_generator/)) even with multi user collaboration support [through togetherjs](https://togetherjs.com/).\\nIt's hard  to tell whether Quilljs is going to be around long term, while draft.js being core part of facebook is starting to see more of a community around it,  and a growing number of plugins.\\nThere also other more advanced features, like adding speakers label, timed text, and other non timed text in the transcription that has workarounds in Draftjs but is not immediatly obvious how it would be implemented in Quilljs.\\n<!-- because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. -->\\n<!--\\n### Positive Consequences\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]\\n* \u2026\\n### Negative consequences\\n* [e.g., compromising quality attribute, follow-up decisions required, \u2026]\\n* \u2026\\n\n\n## Context and Problem Statement\\nI need a way to document design decisions.\\n\n## Decision\nChosen option: Smaller markdown files in Git of format MADR, no cmd tool, IDEA is sufficient.\\nPositive Consequences:\\n* Choices and reasoning are persisted and versioned\\nNegative consequences:\\n* Having to document\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\\n## Decision Drivers <!-- optional -->\\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible/when if needed.\\n\n## Decision\n", "response": "## Decision\nChosen option: **Context API**, because it offers a simple and straightforward way to share data across components without prop drilling and is natively supported in React. \n\nUsing the React Context API allows us to create a context for shared state and enables components to subscribe to this context for receiving updates, thereby reducing boilerplate code and improving maintainability.\n\nFurthermore, the Context API is extensible enough to allow for advanced future use cases, such as integrating with Redux for more complex state management scenarios if needed, ensuring that internal information can be accessed seamlessly by various parts of the Transcript Editor when deployed in diverse environments.\n\n### Positive Consequences\n* Enhances code clarity by minimizing prop drilling.\n* Simplifies state management for global state across components.\n* Provides a clear and React-native solution that is easily understandable by developers.\n\n### Negative Consequences\n* Could lead to unnecessary re-renders if not carefully implemented.\n* May become complex if overused for deeply nested components without consideration of performance implications.", "matched_ids": [2014, 4170, 3181, 3188, 160], "time": "0:00:05.621618", "input_tokens": 1793, "output_tokens": 199}
{"context": "## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state/model.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nWhen it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.  It could be more sensible to only verify the state/model.\\nBut, we had a small problem described in ADR-0002.\\n\n## Decision\nWe will assert both against the session in the provider **and** against the DOM elements to make sure the application view is updating properly.\\n\n\n## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https://jestjs.io) is the standard, recommended test framework for React\\napps.\\n\n## Decision\nWe will use Jest as our testing framework.\\n\n\n## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https://jestjs.io) is the standard, recommended test framework for React\\napps.\\n\n## Decision\nWe will use Jest as our testing framework.\\n\n\n## Context\\nWe want a test framework that has good support for React and TypeScript.\\n[Jest](https://jestjs.io) is the standard, recommended test framework for React\\napps.\\n\n## Decision\nWe will use Jest as our testing framework.\\n\n\n## Context\\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\\nOne of the benefits of writing our infrastructure in a fully fledged programming language is the ability to write reusable components which can be tested. This library defines those components, both in the form of constructs and patterns (see [001-constructs-and-patterns](./001-constructs-and-patterns.md) for more details).\\nThere are two main strategies that can be used to unit test these components.\\nSnapshot tests synthesise a given stack and compare the output to a previous version that has been verified by the user and checked into the repository. If there are any changes, the test fails and the user is displayed the diff to either fix or update the stored snapshot.\\n[+] Quick and easy to write\\n[+] Pick up any changes to the output cloudformation (particularly useful for unintended side effects)\\n[-] A change to the component may cause many unrelated tests to also fail\\n[-] When testing different permutations of a component, a number of snapshots will be created, each of which contains a certain amount of information which is irrelevant to the particular test. This adds some extra effort to understand what is relevant to the test which you're looking at\\n[-] Snapshots are easy to update which, especially when multiple are affected by a change, makes it easy to accidentally update them incorrectly. Further to this, as the snapshot (which is essentially the assertion) is stored in a different file, it's not immediately obvious if the assertions are valid\\nDirect Assertions use the different assertions provided by the test framework to test specific functionality. For example, asserting that a property exists or that an array contains a particular value.\\n[+] Each test only contains the relevant assertions for the test, making it easier to understand the consequence of settings certain props\\n[+] Changes will only fail tests that cover that particular area\\n[-] More complex and time consuming to write\\n\n## Decision\n<!-- What is the change that we're proposing and/or doing? -->\\nUse direct assertions for constructs and snapshots for patterns (and stacks)\\n_This decision is a recommendation for the general approach. There may be some cases where using a different approach is more applicable for a given test._\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state/model.\\n\n## Decision\n", "response": "## Decision\nWe will primarily use `react-testing-library` to focus on testing the behavior of the UI through user interactions and the resulting DOM changes. While we trust the view to render the model correctly, we will ensure integration tests assert against the relevant DOM elements and user interactions to verify that the application behaves as expected. In certain scenarios, we might also verify the underlying state/model when it provides additional confidence in the application\u2019s functionality.", "matched_ids": [2814, 4223, 3323, 5252, 1192], "time": "0:00:03.763734", "input_tokens": 1089, "output_tokens": 87}
{"context": "## Context\\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\\nAdditionally Geocoder can be used to verify addresses that are manually entered.\\n- [Geocoder](https://www2.gov.bc.ca/gov/content/data/geographic-data-services/location-services/geocoder)\\n- [Data BC](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service)\\n- [API Swagger](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service/resource/40d6411e-ab98-4df9-a24e-67f81c45f6fa/view/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\\n- [Developer API Keys](https://github.com/bcgov/gwa/wiki/Developer-Guide#developer-api-keys)\\n- API Host = `https://geocoder.api.gov.bc.ca`\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nThe application has been using https://postcodes.io/. Postcodes.io is available\\nfor free and does not require any relationship between consumer and supplier.\\nOne of the drawbacks to this is that although support is available when using\\nthe free tier, there is no provision for any specific support relationship.\\nMore formal relationships are available but this is through the paid product\\nhttps://ideal-postcodes.co.uk/.  Given the number of requests and the fact the\\nfree tier was supplying the information required, it had previously been\\ndecided there was no need to migrate to a paid product. However, the strategic\\ndirection has been set such that high value APIs, usable across many products\\nshould be supplied in-house. To that end, there is now an in-house postcode\\nsearch available.\\n\n## Decision\nThe decision is to replace the use of postcodes.io with the in-house postcode\\nlookup, as per the strategic direction.\\n\n\n## Context\\nThe application is about finding services closest to the search point. All\\nservices have a co-ordinate in lat/lon format. Currently the application\\nrequests either an out-code or a postcode as the search point. The submitted\\npostcode needs to be resolved to a lat/lon that can be used to query against.\\n\n## Decision\n[Postcodes.io](https://postcodes.io/) provides both an out-code and a postcode\\nlookup. The results of which return latitude and longitude. The service is\\nfree, [supported](https://postcodes.io/about),\\n[monitored](https://status.ideal-postcodes.co.uk/) and contains the full set of\\nactive postcodes for the UK. Supplied by the\\n[ONS](https://data.gov.uk/dataset/national-statistics-postcode-lookup-uk).\\n\n\n## Context\\nThe Registraties project will need to contain a page that will show data per address (or verblijfsobject) from existing APIs, like [Basisregistraties adressen en gebouwen (BAG)](https://api.data.amsterdam.nl/bag/), [Basisregistratie kadaster (BRK)](https://api.data.amsterdam.nl/brk/) and [Handelsregister (HR)](https://api.data.amsterdam.nl/handelsregister). All required data is readily available.\\n\n## Decision\nThe project's MVP, which will contain a simple search field with auto-suggest functionality, get its data from existing API endpoints and therefore doesn't need its own back-end setup with API. In the future it might, but for the initial phase, the front-end code is capable of retrieving, formatting and showing the data from [api.data.amsterdam.nl](https://api.data.amsterdam.nl).\\nThe application will have container components that have sagas injected into them. Each saga is responsible for retrieving data from a single endpoint. If the situation calls for it, a saga can retrieve data from more than one endpoint.\\n\n\n## Context\\nThe New Build Team need to be able to perform basic search over properties for finding properties to either report defects against or to manage existing defects.\\n\n## Decision\nTo implement search we will use the built in Postgres Search rather than adding a new dependency on another service like ElasticSearch.\\n\n\n## Context\\nThe primary (only) consuming application for this API needs to show both open\\nand nearby services on separate pages (and more of them). Previously the\\napplication had shown a mix of open and nearby services within a\\nsingle page.\\nHaving the API so closely aligned to the needs of the consumer is not ideal.\\nThere is scope to increase the flexibility of the API along with increasing the\\nease with which it can be used both by the current and future consumers.\\n\n## Decision\nThe decision is to add a new endpoint i.e. `/open` alongside the current\\n`/nearby` endpoint. The former endpoint will return only services that are open\\nwhere the latter will be refactored to return only services that are nearby\\nregardless of their opening state.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\\nAdditionally Geocoder can be used to verify addresses that are manually entered.\\n- [Geocoder](https://www2.gov.bc.ca/gov/content/data/geographic-data-services/location-services/geocoder)\\n- [Data BC](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service)\\n- [API Swagger](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service/resource/40d6411e-ab98-4df9-a24e-67f81c45f6fa/view/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\\n- [Developer API Keys](https://github.com/bcgov/gwa/wiki/Developer-Guide#developer-api-keys)\\n- API Host = `https://geocoder.api.gov.bc.ca`\\n\n## Decision\n", "response": "## Decision\n\nThe decision is to integrate the Data BC Geocoder API into the inventory system for automating the setting of latitude and longitude values. By using the Geocoder API, the system will automatically retrieve accurate GIS coordinates for property addresses, eliminating manual entry and improving the accuracy of location data. Additionally, the Geocoder API will be utilized to verify manually entered addresses, enhancing data integrity and user experience. Implementing this integration will require the acquisition of Developer API Keys and adherence to the API Swagger documentation for successful connection and usage.", "matched_ids": [4276, 2204, 1122, 5217, 482], "time": "0:00:03.621704", "input_tokens": 1437, "output_tokens": 106}
{"context": "## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context and Problem Statement\\nNumerous sub-sections from Expenditure were selected to be scraped and analysed. A phased approach will be employed to make sustainable jobs for each section.\\n## Decision Drivers\\n* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\n\n## Decision\n* Use the data from differnt sections to impute accounting head information\\n* The information from various sections will be used to create data checks\\n* Long term project with 3 year pipeline to create a sustainable archirtecture\\nSet a priority list and weekly targets foe the data that needs to be scraped from Koshvani.\\n\n\n## Context and Problem Statement\\nSections of the Koshvani platfrom to be considered in scope for scoping and analysing the data.\\n- Expenditure\\n- Receipts\\n\n## Decision\nExplore both sections to identify sub-sections from which data needs to be extracted and analysed.\\n\n\n## Context and Problem Statement\\nThe data extraction w/ transaction timestamp from Koshvani platform is a challenging task.\\n- The temporal data directly available on the platform is not easy to extract\\n- The validation of temporal data to ensure accuracy depends on factors\\n- Districts/Treasuries which have updated their data\\n- Data/Time of data upload vs extraction from departments\\n## Decision Drivers\\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n\n## Decision\nThe platform shares the data updation status for each treasury at a daily level. But due to the factors stated above, an in-depth check and analysis is required to extract the correct data.\\n- Do a daily level data extraction from the platform\\n- Conduct analysis to identify daily, weekly and monthly expenditure numbers\\n- Select the accurate time period and mention methodology for the same\\n- Ignore previous time period missed in this analysis.\\nConduct the analysis on accurate data collection at in November, 2020 to take decision.\\n\n\n## Context and Problem Statement\\nSource of data and information to be decided based on the requirements for **OBI: Phase 2** and **HAQ: GEWST** projects at CivicDataLab.\\n**_Platform 1_**\\n**Name** : Budget - Government of Uttar Pradesh\\n**URL** : http://budget.up.nic.in/\\n**Usage** : Analyse budget documents and understand accounting information available.\\n**_Platform 2_**\\n**Name** : Koshvani web -- A Gateway to Finance Activities in the State of Uttar Pradesh\\n**URL** : http://koshvani.up.nic.in/\\n**Usage** : Scrape, analyse and publish open budget data for public consumption.\\n## Decision Drivers\\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\n\n## Decision\n- Create better underdstanding Uttar Pradesh fiscal data\\n- Scrape data to be consumed in machine readable format\\nExplore both platforms to understand the open fiscal data available and techinical requirements.\\n\n\n## Context and Problem Statement\\nDecision required on the framework to be used to scrape and store data from the Koshvani platform in machine readable format.\\n\n## Decision\n- Structure of the Koshvani platform\\n- Platfrom links do not reflect selection criteria\\n- Automation job requirements for data scraping\\n- Periodical jobs and access to new data\\nUse [Selenium](https://www.selenium.dev/) to create robust, browser-based automation.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n\n## Decision\n", "response": "## Decision\n- Proceed with extracting data from the `Grant-wise expenditure` section now that site stability issues have been resolved.\n- Prioritize `Grant-wise expenditure` data extraction due to its hierarchical importance and comprehensive coverage.\n- Maintain existing extraction from the `DDO-wise expenditure` as a backup and cross-reference to ensure data accuracy.\n- Develop monitoring mechanisms to quickly detect and address any recurring site performance issues during extraction.", "matched_ids": [443, 448, 442, 446, 445], "time": "0:00:03.101758", "input_tokens": 1016, "output_tokens": 84}
{"context": "## Context and Problem Statement\\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\\n```\\n<xs:element name=\"take\" type=\"xs:integer\">\\n```\\nor with attributes as:\\n```\\n<xs:element name=\"take\">\\n<xs:complexType>\\n<xs:attribute name=\"value\"/>\\n</xs:complexType>\\n</xs:element>\\n```\\n## Decision Drivers\\n* consistency\\n* expressiveness\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context and Problem Statement\\nHandling non-CRUD operation over REST API is not trivial. Once operations are beyond Create(`POST`/`PUT` HTTP methods),\\nRead(`GET` HTTP method), Update(`PUT`/`PATCH` HTTP methods), Delete(`DELETE` HTTP method) there is no clear recommendation\\nhow to map others actions. These actions include, but are not limited to, changes of states (described in the form of\\nworkflow in a state machine) or command execution.\\n## Decision Drivers\\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\n\n## Decision\n* Solution should not be limited by its convention. We may need to support two different actions of the same name,\\nbut with different business logic. E.g., one object may have two transitions with the same name but described by\\ntwo different state machines.\\n* Solution should allow for an easy understanding of what is expected behavior of its execution.\\n* Solution should provide easy to implement a way of exposing the next possible actions, according to HATEOAS paradigm.\\n* Solution should provide a clean way of adding additional fields during the execution of requested operations.\\nThe \"Taking advantage of the `Controller` REST archetype\" should be considered as a recommended solution. All Sylius\\nusers are already familiar with it, and it is easy to understand expected behavior. Linked data references should provide\\nthe discoverability of the new endpoints. The possible operation may be sent in the `Link` header\\nor new schema should be introduced for the JSON-LD structure.\\nOption 2: \"Defining custom operations in the style of command pattern\" may be useful once async data processing is\\ndelivered with vanilla Sylius installation.\\n\n\n## Context\\nThe issue motivatina# [short title of solved problem and solution]\\nWhat is the issue that we're seeing that is motivating this decision or change?\\n* Status: [proposed | rejected | accepted | deprecated | ... | superseded by [ADR-0005](0005-example.md)] <!-- optional -->\\n* Deciders: [list everyone involved in the decision] <!-- optional -->\\n* Date: [YYYY-MM-DD when the decision was last updated] <!-- optional -->\\nTechnical Story: [description | ticket/issue URL] <!-- optional -->\\n## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\n\n## Decision\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | ... | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, ...]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, ...]\\n* ...\\nWhat is the change that we're proposing and/or doing?\\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context\\nGenerally, every neural network file format defines a sequence of operations\\nto execute mathematical operations that comprises a neural network.\\nEach element in the sequence is a node that contains information such as the\\ndesired operation, and a set of attributes that represent parameters\\nin to the mathematical function to execute.\\nIn order to write import/export for different frameworks, we need to adapt\\nan attribute based format from various popular deep learning frameworks.\\nNd4j  has a different list based format for operation execution arguments.\\nIn the [previous ADR](./Import_IR.md), we added an IR which makes it easier to\\ninterop with other frameworks.\\nIn this ADR, this work is extended to add a file format for\\ndescribing lists of operations as MappingRules which allow transformations\\nfrom one framework to another.\\nThese transformations manipulate protobuf as input and output Nd4j's\\nnew OpDescriptor format as output.\\n\n## Decision\nWe implement a mapping process framework that defines transforms on an input file format.\\nA MappingProcess defines a list of MappingRules which represent a sequence of transformations\\non each attribute of an op definition.\\nTo assist in mapping, a mapping context with needed information like rule arguments\\nfor transformation, current node, and whole graph are used as input.\\nThe input is a protobuf file for a specific framework and the output is an op descriptor\\ndescribed [here](./0003-Import_IR.md).\\nA MappingRule converts 1 or more attributes in to 1 more or arg definitions. A potential definition\\ncan be found in Appendix E.\\nAttributes are named values supporting a wide variety of types from floats/doubles\\nto lists of the same primitive types. See Appendix C for a theoretical definition.\\nArg Definitions are the arguments for an OpDescriptor described in [the import IR ADR.](./0003-Import_IR.md)\\nSee Appendix D for a potential definition of arg definitions.\\nAll of this together describes how to implement a framework agnostic\\ninterface to convert between a target deep learning framework and the nd4j format.\\n\n\n## Context and Problem Statement\\nIn the course of implementing a class-based role metadata model we have also\\nreviewed options on how to design serialization infrastructure between wire\\nformats and the class model. In an initial attempt we have implemented\\nserialization on the metadata class (see option 1), but issues with inheritance\\nand calls for more flexibility have caused us to rethink this approach.\\n## Decision Drivers\\n* A class-based role metadata model (see ADR4) requires serialization routines\\nfrom and to wire formats\\n* TUF integrators may require custom serialization implementations for custom\\nwire formats\\n* Readability and simplicity of implementation for users and maintainers\\n* Recognizability of specification\\n\n## Decision\n* A class-based role metadata model (see ADR4) requires serialization routines\\nfrom and to wire formats\\n* TUF integrators may require custom serialization implementations for custom\\nwire formats\\n* Readability and simplicity of implementation for users and maintainers\\n* Recognizability of specification\\nChosen option: \"Compromise 2\", because implementing dict conversion as methods\\non a corresponding class is idiomatic and allows for well-structured code.\\nTogether with a separated serialization interface, it provides both ease of use\\nand maintenance, and full flexibility with regards to custom serialization\\nimplementations and wire formats.\\n\n\n## Context and Problem Statement\\nSome [Enterprise Integration Patterns](https://www.enterpriseintegrationpatterns.com) have a complex structure where parts of the behaviour can be implemented generically while some parts need to be modifiable by the end user (in our case the system admin using MICO).\\nWe have already [decided to use a FaaS platform](0023-faas.md) to provide this modifiability in form of code as configuration.\\nWhile this works well for most patterns, for some of the more complex patterns it is not easy to allow modifiability via FaaS.\\nThis is especially the case if the user want to write as little code as possible meaning the [generic part](0025-generic-component.md) of the component has to be implemented by the MICO team.\\n## Decision Drivers\\n* Modifiability of the patterns must be provided via a FaaS function\\n* The function should only have to contain as little code as possible\\n* Generic code for the pattern should be provided by the MICO platform either as a library to import into the FaaS function or in the component that calls said function\\n\n## Decision\n* Modifiability of the patterns must be provided via a FaaS function\\n* The function should only have to contain as little code as possible\\n* Generic code for the pattern should be provided by the MICO platform either as a library to import into the FaaS function or in the component that calls said function\\n**Where to implement logic**: To be decided\\n**State of configuration channels**: To be decided\\n**Stateful functions**: To be decided\\n**Routing**: We will support custom routing decisions in the FaaS function by always interpreting a routing slip if it is present.\\nThe routing slip has to support multiple destinations for one routing step.\\nThis will also allow us to make more patterns possible (everything that is not stateful) with a single generic kafka to FaaS connector.\\n**Intermediate requests**: To be decided\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\\n```\\n<xs:element name=\"take\" type=\"xs:integer\">\\n```\\nor with attributes as:\\n```\\n<xs:element name=\"take\">\\n<xs:complexType>\\n<xs:attribute name=\"value\"/>\\n</xs:complexType>\\n</xs:element>\\n```\\n## Decision Drivers\\n* consistency\\n* expressiveness\\n\n## Decision\n", "response": "## Decision\nChosen option: \"Use attributes to define operations where appropriate\", because this approach provides a balance between consistency and expressiveness, allowing more complex descriptions and additional metadata to be captured far more efficiently.\n\n### Positive Consequences\n* **Consistency**: Using attributes for defining operations aligns with XML best practices, where attributes are used for metadata or properties of elements. This keeps the XML structure predictable and consistent across different operations.\n* **Expressiveness**: It allows the schema to be more expressive by enabling additional constraints/metadata through attributes. Attributes can provide more descriptive semantics about the operation being defined.\n\n### Negative Consequences\n* **Complexity**: Introducing attributes can increase complexity in processing, as parsers would need to differentiate between elements and attributes. It may require additional handling during XML serialization/deserialization.\n* **Less Flexibility in Types**: Attributes are limited to simple data types, so schema designers would need to use elements where complex data structures are necessary.\n\nThe decision is to integrate operations in the FDML using attributes, with the ability to provide supplementary attributes around primary operations when needed. This method promotes a structured approach for easier understanding and maintenance. This direction will result in more expressive schema documents, while maintaining consistency across the operation definitions.", "matched_ids": [674, 4800, 2943, 4254, 4669], "time": "0:00:04.744419", "input_tokens": 2292, "output_tokens": 250}
{"context": "Context\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https://github.com/openedx/event-routing-backends/blob/f430d4cf58bdab01e42fcc944241898606873d82/event_routing_backends/processors/mixins/base_transformer.py#L139\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\nContext\\n-------\\nEvent transformers may undergo modification in future in response to consumer request, change in specification, bug fixes etc.\\nDecision\\n--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https://w3id.org/xapi/openedx/extension/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n\n## Decision\n--------\\n#. Versions of event transformers will be maintained and emitted as part of xAPI and Caliper events.\\n#. The \"transformer version\" will be a concatenation of the name of the transformer (\"event-routing-backends\"), an @ symbol, and the symantic version of the event-routing-backends package used to generate the event.\\n#. This combined version \"event-routing-backends@v(X.Y.Z)\" can be found in ``context [ extensions [ https://w3id.org/xapi/openedx/extension/transformer-version ] ]`` for xAPI statement and in ``extensions [ transformerVersion ]`` for Caliper event.\\n#. Transformer version number will be the semantic version of the event-routing-backends package.\\n#. The event-routing-backends major version will be incremented when:\\n#. Transformer is changed due to update in original specification (xAPI or Caliper).\\n#. A key is removed from or renamed in the existing transformer.\\n#. Value of a key is updated in the existing transformer.\\n#. The event-routing-backends minor version will be incremented when:\\n#. A key is added to an existing transformer.\\n#. A new event is added for transformation.\\n#. Minor version (Z) will be incremented when:\\n#. A bug is fixed.\\n#. Change logs of transformers will be maintained for both xAPI and Caliper.\\nChangelog\\n---------\\n- Updated 2023-02-28 to change the format of the transformer version.\\n- The previous version named the key \"eventVersion\", but the actual implementation used URL key pointing to the event-routing-backends docs. It was decided that since the version number represents the actual version of the event-routing-backends package and not the version of a specific event, this rename could tackle both issues.\\n\n\nContext\\n-------\\nWe can develop event transformers either using the \u201cBackend\u201d architecture\\nor \u201cProcessor\u201d architecture. Making the transformers \u201cbackends\u201d will result\\nin relatively more nesting in the configurations as this \u201cbackend\u201d will have\\nits own configurations.\\nIf we decide to develop the event transformers as \u201cprocessors\u201d, it will result\\nin less complexity in the code since the transformer can be easily appended in\\nany backend\u2019s (router or logger) processors\u2019 pipeline.\\nDecision\\n--------\\nTransformers will be developed as event processors that can be added in\\nany backend\u2019s pipeline. Then the transformed events can be used for any purpose,\\neither for simply logging using the LoggerBackend or to route events using\\nEventRoutingBackend.\\nConsequences\\n------------\\nDeveloping transformers as processors will result in relatively less complex\\nconfigurations and it would provide us wider range of use cases for the transformers.\\n\n## Decision\n--------\\nTransformers will be developed as event processors that can be added in\\nany backend\u2019s pipeline. Then the transformed events can be used for any purpose,\\neither for simply logging using the LoggerBackend or to route events using\\nEventRoutingBackend.\\nConsequences\\n------------\\nDeveloping transformers as processors will result in relatively less complex\\nconfigurations and it would provide us wider range of use cases for the transformers.\\n\n\nContext\\n-------\\nWe need to decide some approach to encapsulate and structure the Caliper\\nevents. We have the following options for binding caliper events:\\n-  Make our own data structure to store a caliper event (e.g. a python dict)\\n-  Use IMS\u2019 `caliper-python <https://github.com/IMSGlobal/caliper-python>`__ library\\nDecision\\n--------\\nWe will be using python dictionaries for processing and encapsulation of\\ntransformed events to keep things simple.\\nConsequences\\n------------\\nPython dictionaries will be used for binding Caliper events. These\\nevents stored as python dictionaries can be converted into JSON later\\nand sent to interested consumers.\\nRejected Alternatives\\n---------------------\\n**Use IMS\u2019 caliper-python library**\\nThe library under discussion is not published on\\n`PyPI <https://pypi.org/search/?q=caliper>`__. We\u2019d have to fork the\\nrepo and add its dependency on our app. Therefore we won\u2019t be using this\\nlibrary.\\n\n## Decision\n--------\\nWe will be using python dictionaries for processing and encapsulation of\\ntransformed events to keep things simple.\\nConsequences\\n------------\\nPython dictionaries will be used for binding Caliper events. These\\nevents stored as python dictionaries can be converted into JSON later\\nand sent to interested consumers.\\nRejected Alternatives\\n---------------------\\n**Use IMS\u2019 caliper-python library**\\nThe library under discussion is not published on\\n`PyPI <https://pypi.org/search/?q=caliper>`__. We\u2019d have to fork the\\nrepo and add its dependency on our app. Therefore we won\u2019t be using this\\nlibrary.\\n\n\nContext\\n-------\\n`OEP-26 <https://open-edx-proposals.readthedocs.io/en/latest/oep-0026-arch-realtime-events.html>`__\\nconsists of the following components:\\n-  Asynchronous Routing Backend\\n-  Regular-expressions based filter processor\\n-  IMS Caliper transformer\\n-  xAPI transformer\\n-  Router to forward events\\nKeeping all of these components in one repo will make the repository\\nunnecessarily tangled since these additional components are not required\\nby the core app for its functionality.\\nDecision\\n--------\\nAmong the components listed above, Asynchronous Routing Backend and the\\nregular-expressions filter will be added in the core app (i.e.\\n`event-tracking <https://github.com/openedx/event-tracking>`__) while the\\nother components, i.e. Caliper transformer backend, xAPI transformer\\nbackend and router, will be added in the current repo.\\nBy keeping the concrete backends separate from the code, we can have\\nonly the core plugin interface for event tracking in its repository.\\nConsequences\\n------------\\nThe code will be decoupled and components can be used independently if\\nrequired.\\nRejected Alternatives\\n---------------------\\n**Add the routing backends to the event-tracking repository**\\nThis idea was rejected to keep the core event-tracking repository clean\\nand independent. The core repo is functional on its own and any\\npluggable extensions should be implemented separately.\\n\n## Decision\n--------\\nAmong the components listed above, Asynchronous Routing Backend and the\\nregular-expressions filter will be added in the core app (i.e.\\n`event-tracking <https://github.com/openedx/event-tracking>`__) while the\\nother components, i.e. Caliper transformer backend, xAPI transformer\\nbackend and router, will be added in the current repo.\\nBy keeping the concrete backends separate from the code, we can have\\nonly the core plugin interface for event tracking in its repository.\\nConsequences\\n------------\\nThe code will be decoupled and components can be used independently if\\nrequired.\\nRejected Alternatives\\n---------------------\\n**Add the routing backends to the event-tracking repository**\\nThis idea was rejected to keep the core event-tracking repository clean\\nand independent. The core repo is functional on its own and any\\npluggable extensions should be implemented separately.\\n\n\n# Context\\nAs someone responsible for ensuring accurate data is published by the\\ngovernment, I need to ensure that people authoring content can only\\nview and edit things that they own, so that there are no embarrassing\\nupdates published.\\nWe need to ensure that datasets and dashboards can only be edited by\\nthe right people.\\nWe currently have signonotron2 accounts for all of the people using\\nthis application. That will continue to be in place for the future.\\nWe need a way of restricting which signon accounts have access to\\ndifferent entities within the admin application.\\nWe have tried to model this in various ways.\\nAs someone working at the DVLA, should I have the ability to edit\\nall of the DVLAs dashboards? The answer to that is no.\\nWe've had some discussions with our user population, but what is\\nactually good still isn't known at this point. So we are going to\\nincrementally do this functionality, as a series of vertical slices,\\nand accept that we will need to refactor, rework and do data\\nmigrations as we learn more about the problem space and how best to\\nmeet those needs.\\nInitially, we proposed using custom Django Managers in stagecraft to\\nimplement this. We don't think this is the right approach; it smells\\ntoo complex.\\nA simpler start point will be to define the URLs (resources) and\\nhave permissions per resource.\\n# Decision\\nDefine the matrix of URLs, and map permissions per resource.\\n# Status\\nAccepted.\\n# Consequences\\nTrading shipping a thing with future technical debt for now.\\n\n## Decision\nDefine the matrix of URLs, and map permissions per resource.\\n# Status\\nAccepted.\\n# Consequences\\nTrading shipping a thing with future technical debt for now.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\nContext\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https://github.com/openedx/event-routing-backends/blob/f430d4cf58bdab01e42fcc944241898606873d82/event_routing_backends/processors/mixins/base_transformer.py#L139\\n\n## Decision\n", "response": "Decision\n--------\n1. An accessor method will be developed to get values from Open edX events for a specified key. Access to information within the Open edX event will be exclusively managed using this accessor method across both base transformers and event transformers.\n2. If a key, mandated by the specification (xAPI or Caliper), is absent in the Open edX event, this method will raise an exception.\n3. If a key, considered optional by the specification, is not found in the Open edX event, this method will log the occurrence and return None.\n4. The effectiveness of fuzzy comparison between keys in transformed events and potential PII keys will be assessed in the future and implemented if deemed beneficial.\n\nBenefits\n---------\n1. The use of the accessor will concentrate PII checks to specific code instances that need to be reviewed to mitigate potential PII leakage risks.", "matched_ids": [4483, 4478, 4480, 4479, 3394], "time": "0:00:04.743030", "input_tokens": 3010, "output_tokens": 175}
{"context": "# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https://www.gov.uk/service-manual/making-software/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop / HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe needed some persistent storage of audit results.\\nWe considered:\\n### Schemaless - DynamoDb\\nThis would be the most obvious choice for a lambda based\\nservice.\\nThe dynamo data model is tables of key value pairs.\\nThe main problem with Dynamo is the limit of I think 4K\\nper value.\\nOne of the things we wanted to do was briefly cache API\\nresponses which could easily breach that 4K limit.\\nWith Dynamo the access control is via IAM which would be\\nrelatively easy to manage and encryption at rest can\\nbe easily configured.\\n### Schemaless - MongoDb\\nMongo was a better fit for our service, saving JSON\\nrepresentations of API responses and resources.\\nThe problem with Mongo is it's not AWS native so we'd\\nhave to provision a server, manage the access controls,\\nmaintenance and availability.\\n### Relational - RDS MySQL / PostgreSQL\\nRDS gives us the benefit of running a managed service so\\nAWS are responsible for backups and patching minor\\nversion.\\nRelational databases give us the ability to do on-the-fly\\nanalysis more easily.\\nWe can store JSON as blob data although not ideal.\\nIf we want to re-use the database instance as a shared\\nresource across multiple services RDS is more capable.\\nIt's not unlikely that a future development may require\\ndjango.\\nPostgreSQL seems to be the general direction of travel\\nin GDS and is much more capable for a wider range of\\nuse-cases where we don't know what we're building in the\\nfuture.\\n\n## Decision\nWhilst none of the options were perfect we decided that\\na PostgreSQL RDS was the best option given the\\ninformation available to give us an infrastructure to\\nsupport multiple tools and services.\\n\n\n## Context\\nGOV.UK uses MongoDB as the backing database for a number of apps. In a generic\\nGOV.UK environment, there are 5 MongoDB clusters: one for Licensify (a legacy\\nservice), one for performance monitoring, one for router and another per cloud\\nprovider (i.e. AWS and Carrenza) for all other apps (e.g. assets, imminence,\\nmaslow, content, publisher and short url manager).\\nOne architectural strategy principle of TechOps is to make use as much as\\npossible AWS managed services in order to reduce the burden on GOV.UK teams to\\nmanage Commercial off-the-shelf (COTS) functionality/services.\\n\n## Decision\nThe approaches taken for migrating from MongoDB to AWS DocumentDB are:\\n1. new apps that are being migrated from legacy cloud providers (e.g UKCloud and\\nCarrenza) to AWS and use MongoDB should be evaluated for compatibility with\\nAWS DocumentDB during the migration phase. Based on this evaluation, a\\ndecision should be made based on cost benefit whether a particular migrating\\napp should be using DocumentDB or MongoDB.\\n2. there are some apps (e.g. content store and imminence) that have already been\\nmigrated to AWS and uses the MongoDB cluster there. These apps should be\\nevaluated for compatibility with AWS DocumentDB and prioritised accordingly\\nfor migration to AWS DocumentDB.\\n\n\n## Context\\nWe need to store persistent data in a form that is easy to query and need to select the\\nappropriate data store. We consider the choice to be between a relational database or a\\ndocument database.\\n\n## Decision\nWe have decided to use [MongoDB](https://www.mongodb.com/) as our database for datalabs\\ndata. We felt that the data model for the parts of the system known at this time, a\\ndocument structure provided more flexibility and easier integration with our Node.js\\napplication.\\nMongoDB also provides the ability to run in cluster providing the option for greater\\nresilience. For the time being we are opting to run a single node storing data to\\na mounted GlusterFS volume. This gives data resilience through node failure but obviously\\ndoes not give service resilience or time based backups.\\n\n\n#### Context and Problem Statement\\nA relational database is required to persist the applications data.\\n#### Considered Options\\n- Microsoft SQL\\n- MySQL\\n- PostgreSQL\\n- SQLite\\n#### Decision Outcome\\nChosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n\n## Decision\nChosen option: \"Microsoft SQL\", because\\n- This is in-line with Trade Me's technical stack.\\n- Microsoft SQL is already widely and almost exclusively used at Trade Me.\\n\n\n## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n\n## Decision\nUse AWS RDS Aurora MySql implemntation for our database persistence mechansim.  Create via Riglet CloudFormation stacks.  Production will run with 1 primary and 1 replica for HA.  Staging & Integration will run with only 1 Primary to keep costs down.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https://www.gov.uk/service-manual/making-software/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop / HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n\n## Decision\n", "response": "## Decision\nWe will use MongoDB for the Performance Platform. Given our existing experience with operating MongoDB in production, it aligns well with the requirement to handle unstructured data in the form of JSON documents. MongoDB's ability to apply structure at query time suits our need to provide flexibility in data processing. As the data volume is small and we do not require ACID properties, MongoDB is an appropriate choice considering our initial scope. \n\n## Status\nAccepted.\n\n## Consequences\nImplement MongoDB using a replica-set configuration to ensure data availability and resilience. Regularly review the data schema and query performance as the project evolves to ensure that the database continues to meet the evolving needs of the Performance Platform.", "matched_ids": [418, 4047, 730, 1661, 1740], "time": "0:00:06.996822", "input_tokens": 2156, "output_tokens": 142}
{"context": "## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https://www.w3.org/WAI/WCAG21/Understanding/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https://github.com/hmrc/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context and Problem Statement\\nWithin contact-frontend, for the standalone pages, two routes into the page exist. The first requires a tax service\\nuser to be logged in, and redirects to login if the service user is not logged in. The second, on a different URL\\nsuffixed with \"-unauthenticated\", does not require login, but serves the same page. After discussion, the PlatUI team\\ndecided that the requirement for a user to be logged in was making a worse experience for the service end user, adding\\nthe requirement of login,when all the same functionality is also available without logging in via a different URL.\\n## Decision Drivers\\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\n\n## Decision\n* Current functionality with login leads to a possibly bad tax service user journey, in particular in the use case where\\na user is signed out in the background whilst trying to report an issue, given that all this functionality is\\navailable without login\\n* If a tax service user is logged in, but has clicked on a link to the unauthenticated version of the form,\\ncontact-frontend currently doesn't even attempt to look up their enrolments, meaning potentially less information is\\npersisted to Deskpro agents\\n* Requiring login for any of these technical problem report forms makes them less accessible and therefore makes the\\nsite less likely to receive valuable user feedback in particular from users with additional accessibility needs\\n* From a development perspective, maintaining logged-in and non-logged in versions of the pages adds to complexity in\\nthe codebase, making our development process slower and our testing time longer\\nChosen option: \"Remove login redirect from all pages and keep existing endpoints for now\", because:\\n* The benefits to end users in terms of accessibility feel significant enough to warrant the change\\n* Information about logged in users will still be persisted to Deskpro but there will not be a redirect for users\\nwho have been logged out in the background\\n* Codebase is significantly simplified for developers working on contact-frontend\\n* However, deleting the \"-unauthenticated\" routes will be a major breaking change across the Platform, and should be\\ndecoupled from this\\n### Positive Consequences\\n* Capturing information about all logged in users, not just the ones who have clicked on a contact-frontend route not\\nending in \"-unauthenticated\"\\n* More accessible experience for tax platform end users who are not forced to log in to report technical problems\\n* Significant simplification of codebase for developers\\n### Negative Consequences\\n* There is a risk that without forcing users to log in to particular implementations of the contact-frontend forms,\\nthere will be less information sent to DCST in certain cases. On the other hand, for certain users, the login\\nrequirement might lock them out of the contact-frontend forms completely, e.g. if they are trying to report a problem\\nwith login\\n\n\n## Context and Problem Statement\\nTracking consent stores cookie consent information in a userConsent cookie as URL-encoded JSON. This information is available\\nto frontend microservices in Javascript running in a user's browser and in the request headers passed to the Scala\\nPlay controllers running on the server-side.\\nBefore frontend services can interpret the cookie, they need to decode and parse it using a Scala JSON library. They also need\\nto apply some business logic to ensure in cases of uncertainty, the user is, by default, assumed to have rejected cookies.\\nFor Javascript code, we propose to make the existing UserPreferences object available in the window object. This will mean\\nservices integrating with tracking consent will have a method available to them for parsing the cookie using the latest\\nbusiness logic. Additionally, they will have a method for subscribing and being notified about changes to cookie preferences in real-time.\\nFor Scala code, the situation is more problematic. Services will only have the raw userConsent cookie itself passed down in the `Cookie` header.\\nTo avoid teams having to copy and paste substantial quantities of Scala boilerplate replicating the existing Javascript logic\\nacross services, should we look to centralise this logic so it can be more easily maintained across the platform and if so how?\\nIdeally we would like a Scala controller to be able to do something like:\\n```scala\\nprivate val themeCookie = Cookie(name = \"theme\", value = \"dark\", maxAge = Some(365.days.toSeconds.toInt))\\nprivate def allowedCookies(implicit request: RequestHeader) =\\nif (userPreferences.preferences.settings) Seq(themeCookie) else Seq.empty\\nval termsAndConditions: Action[AnyContent] = Action { implicit request =>\\nOk(termsAndConditionsPage()).withCookies(allowedCookies: _*)\\n}\\n```\\n## Decision Drivers\\n* the need to keep repetitious boilerplate to a minimum, with changes needed of the order of a single line code change.\\n* the need to centralise the maintenance of the cookie reading logic, so it is easier to roll out changes\\n* the importance of having loosely coupled microservices and avoidance of unnecessarily introducing additional hard\\nnetworked dependencies into microservices\\n* the need for simplicity and for the fewest number of moving parts\\n* the preference for not introducing a requirement for microservices to add additional libraries\\n\n## Decision\n* the need to keep repetitious boilerplate to a minimum, with changes needed of the order of a single line code change.\\n* the need to centralise the maintenance of the cookie reading logic, so it is easier to roll out changes\\n* the importance of having loosely coupled microservices and avoidance of unnecessarily introducing additional hard\\nnetworked dependencies into microservices\\n* the need for simplicity and for the fewest number of moving parts\\n* the preference for not introducing a requirement for microservices to add additional libraries\\nChosen option 3: because it meets most if not all the above criteria.\\n### Positive Consequences\\n* Services can determine acceptance or rejection of non-essential cookies in a single line of code.\\n* Changes to the cookie parsing logic can be rolled out relatively easily by #team-plat-ui\\n* Consuming services do not rely on any additional networked dependencies\\n* No additional libraries would be needed (assuming the micro-library is added to play-frontend-hmrc or bootstrap-play)\\n### Negative Consequences\\n* Any version change to the userConsent cookie has the potential to break functionality relying on non-essential cookies\\nin consuming services. We may need to look at introducing the ability to have the concept of minor versions or only increment the version for breaking\\nchanges e.g. removal or change to the meaning of a cookie type.\\n\n\n## Context\\nThe service-front component, written in PHP, uses the default Redis save\\nhandler for persisting session data. In certain situations, the\\napplication may request a resource *A* which takes significant time to deliver,\\nsuch as LPA statuses via the Sirius data API. If resource *A*\\nis requested via an Ajax request, it's possible that the client\\nwill request a new resource *B* before *A* is fully processed. If processing for\\n*B* then completes before processing for *A*, the process for *A* can erroneously\\noverwrite session data added by *B*, resulting in loss of session data required\\nby *A*.\\nThis causes particular problems for CSRF tokens, as shown by this typical sequence\\non service-front:\\n1.  dashboard page loads in browser, triggering client-side Ajax request to statuses controller\\n2.  statuses controller reads session data **S** and initiates (slow) request to Sirius\\nAPI to get LPA statuses\\n3.  meanwhile, user goes to replacement-attorney page; the Ajax request is now redundant, as the\\nuser isn't on the dashboard page any more, but the statuses controller doesn't know this\\n4.  replacement-attorney controller reads session data **S**\\n5.  statuses controller continues processing Sirius response, unaware of new data about to be added to\\nsession by replacement-attorney...\\n6.  replacement-attorney adds CSRF data to session, creating **S'**\\n7.  replacement-attorney page renders form with CSRF token, associated with data in **S'**\\n8.  replacement-attorney writes **S'** to session, including CSRF data\\n9.  statuses page finishes processing, unaware of **S'**; it assumes it has\\nthe correct data **S** and writes it to the session, losing the delta between\\n**S** and **S'** (including the CSRF token!)\\n10. user submits form to replacement-attorney controller with CSRF token in the form\\n11. replacement-attorney controller loads again, but retrieves **S** from session (just written by\\nstatuses controller in 9); this doesn't have the CSRF token (which was in **S'**)\\nfor comparison with the value in the form submitted by the user; CSRF validation fails!\\n\n## Decision\nUse a custom save handler to prevent certain Ajax requests from writing data to the session.\\nThis will still use Redis as the storage back-end.\\nThe approach is to send some Ajax requests with a custom `X-SessionReadOnly: true` header,\\nimplying that the controller they invoke should only read from the session and never write to it.\\nThe save handler inspects the header on the incoming request and ignores any requests to write\\nthe session if accompanied by this header.\\nPHP 7+ provides a mechanism to only read from the session, via:\\n```\\nsession_start(array('read_and_close' => true))\\n```\\nHowever, the complexity of the processing in the Laminas stack, which does its own session\\nmanagement, overrides any attempts to call this function. Consequently, the pragmatic\\nsolution is to move down the stack to the lower-level save handler, and implement the read-only\\nbehaviour there for requests we know to be problematic.\\n\n\n## Context and Problem Statement\\nTracking consent reads the PLAY_LANG cookie to determine whether to render the banner in Welsh. PLAY_LANG is the cookie\\nused by the Play Framework to persist a user's language preferences. It will be set to 'cy'\\nwhen a user has selected Welsh using the language toggle in MDTP services using the Play Framework.\\nTeams are increasingly setting PLAY_LANG to HttpOnly in an attempt to get green ZAP tests, even though there are no\\nknown security concerns around keeping PLAY_LANG as a normal cookie. Setting a cookie to\\nHttpOnly makes it unreadable within the client-side Javascript code that renders the tracking consent banner. The result\\nof this is that the banner will not be translated into Welsh for these services.\\nA related issue is that PLAY_LANG is not set for classic services written in Java, which means a Welsh version of the banner is not\\ncurrently available for classic services.\\nIt is worth noting that the only other known instance of our reading PLAY_LANG using Javascript is in the assets-frontend\\n[timeout dialog](https://github.com/hmrc/assets-frontend/blob/97c638289e23bee255ac30724a8572c6efa96817/assets/patterns/help-users-when-we-time-them-out-of-a-service/timeoutDialog.js#L14) timeout dialog. All the new govuk-frontend and hmrc-frontend components use data attributes instead.\\nShould we remove the reading of PLAY_LANG in tracking consent and accept a data-language attribute instead?\\n## Decision Drivers\\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\n\n## Decision\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\nChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'\\ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG into Zap tests. We also agreed that our frontend\\ncomponents should be consistent in their treatment of language until such time as we are able to provide an\\nalternative approach that works for all components.\\n### Positive Consequences\\n* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner\\n* Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings/errors\\n* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etc\\n### Negative Consequences\\n* We will need to make a small change to tracking consent and communicate this change to teams\\n* Teams will need to upgrade to the latest version of play-ui/play-frontend-hmrc to get language translations\\n* Teams not using the tracking consent helpers will need to add a data-attribute for the language\\n\n\n## Context and Problem Statement\\nNearly all services running on MDTP need to include a link to their landing page in the\\nGOV.UK header. Exceptionally, frontend microservices may either need to omit the service name\\nor be able to configure it based on the path. For example, in the case of shared frontend\\nmicroservices (contact-frontend, accessibility-statement-frontend) or microservices that host\\nmore than one public-facing service (view-external-guidance-frontend, gform-frontend).\\nWhen integrating with play-frontend-hmrc we want to\\nminimise boilerplate for service teams while providing flexibility to cope with edge\\ncases.\\nServices using the govukLayout component in play-frontend-govuk and not overriding the `headerBlock`\\nparameter, specify the service name in the\\n`service.name` message in `conf/messages`, providing any Welsh translation in `conf/messages.cy`.\\nBased on a Github search, most services overriding the headerBlock with, for example, `hmrcHeader`\\nor `govukHeader` are passing the service name from `messages('service.name')`. However, this\\nmechanism is awkward for services not requiring a service name \u2013 they have to override with a blank message \u2013\\nand services needing more than one are unable to use this functionality. This pattern\\nassumes a 1:1 correspondence between frontend microservices and public-facing\\ntax services.\\nShould we continue to support this pattern to reduce boilerplate for the majority of frontend\\nservices, insist on explicitly passing the service name or support some combination of the above?\\n## Decision Drivers\\n* Minimising of boilerplate and lines of code needed in frontend microservices\\n* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with\\npublic-facing services.\\n* The need to keep things simple and unsurprising\\n\n## Decision\n* Minimising of boilerplate and lines of code needed in frontend microservices\\n* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with\\npublic-facing services.\\n* The need to keep things simple and unsurprising\\nChosen option 2 because reducing boilerplate for the majority of services is important, we should support\\nmore unusual services and not dictate an architecture that assumes a 1:1 correspondence\\nbetween frontend microservices and public-facing services.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https://www.w3.org/WAI/WCAG21/Understanding/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https://github.com/hmrc/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n\n## Decision\n", "response": "## Decision\nTo address the issue of users unnecessarily being signed out when accessing MDTP services using multiple browser tabs or windows, we have decided to implement a solution that meets the decision drivers outlined above.\n\nChosen option: Implement a centralized session management endpoint that works in conjunction with the existing hmrcTimeoutDialog component.\n\n### Implementation Details:\n1. **Centralized Session Management Endpoint**: Introduce a common session management endpoint within the frontend microservices architecture which hmrcTimeoutDialog can query to obtain the actual time remaining on a user's active session. This endpoint should be designed to be lightweight and must not participate in the session management process (i.e., it should not refresh or modify the session).\n\n2. **Library Upgrade**: Make the necessary enhancements to the hmrcTimeoutDialog library to integrate with this centralized session endpoint using a simple, standardized API call. Service teams should be able to integrate these enhancements by merely upgrading their library, with no additional configuration or routing changes required.\n\n3. **Decoupling and Resilience**: As this endpoint forms part of every service's internal endpoints, each service would maintain its instance of this session state logic. This avoids single points of failure and ensures that frontend microservices remain loosely coupled and independently deployable.\n\n4. **Non-Disruptiveness**: Ensure that the querying action of this endpoint does not extend or modify the user's session, preserving existing session timeout policies. The endpoint should only provide observational data related to session status.\n\n### Positive Consequences\n- **Minimal Impact on Service Teams**: Service teams will only need to conduct a library upgrade, significantly reducing the development effort and avoiding complex code refactoring.\n- **Consistency**: By centralizing session timeout understanding, we ensure that there is no duplication of session management logic or knowledge, promoting maintainability.\n- **User Experience**: Users will have a more consistent and accurate session timeout experience across multiple tabs and windows, improving accessibility and compliance with WCAG 2.1 criteria.\n- **Scalability**: The solution remains scalable across a diverse set of frontend microservices without central points of failure.\n\n### Negative Consequences\n- **Initial Development**: Initial setup of the centralized session endpoint will require careful planning and development to ensure it aligns seamlessly with existing session management across services.\n- **Dependency Management**: Requiring the library update means that service teams need to incorporate this change into their release cycles, which may require orchestration.\n\nThis decision balances the need for a robust, minimal-impact solution with the necessity of improving user experience and maintaining compliance with accessibility standards.", "matched_ids": [1754, 3523, 1654, 3526, 559], "time": "0:00:10.171675", "input_tokens": 4222, "output_tokens": 516}
{"context": "## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\\n\n## Decision\nWe will use [Standard.rb](https://github.com/testdouble/standard) and run the standard.rb rake task to lint the code as part of the test suite.\\n\n\n## Context\\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\\n\n## Decision\nWe will use [Standard.rb](https://github.com/testdouble/standard) and run the standard.rb rake task to lint the code as part of the test suite.\\n\n\n## Context\\nEnsure good code quality with ease for change, integration and error correction.\\n\n## Decision\nIt will be used the [PHPUnit](https://phpunit.de/) as testing framework.\\n\n\n## Context\\nWe need to make sure we adhere to our own designated code quality best practices.\\n\n## Decision\nUse a code analysis tool (specifically, [credo](http://credo-ci.org/) for Elixir\\nand [prettier](https://prettier.io/) for JavaScript).\\n\n\n## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n\n## Decision\nI will always focus on simplicity, readaility and the ease of understanding\\nany code or structure.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n\n## Decision\n", "response": "## Decision\nWe will adopt industry-recognized coding standards and utilize automated tools such as ESLint for JavaScript, RuboCop for Ruby, and PEP 8 for Python to ensure adherence. These tools will be integrated into our CI/CD pipeline to maintain consistent code quality.", "matched_ids": [2382, 3386, 5070, 3853, 1416], "time": "0:00:02.680915", "input_tokens": 510, "output_tokens": 57}
{"context": "## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nThe key component of this project is a 'Bicycle Network Analysis' task which is run on an arbitrary, user-provided neighborhood boundary. This task performs the following actions:\\n- Import neighborhood boundary into a PostgreSQL database\\n- Download OSM extract for the provided neighborhood boundary + a buffer and import to PostgreSQL\\n- Download related census block and job data for the boundary and import to PostgreSQL\\n- Generate a network graph from the imported data\\n- Run a series of client-provided analyses on the graph and imported data\\n- Export relevant data to an external file store for archival\\n- Generate a tile set of the network graph for display on a web map\\nThe application will be configured with multiple organizations, and each organization can only run one analysis job at a time. A new analysis job triggered by a user of the organization will supersede any existing older analysis, which can be thrown away.\\nSince the analysis workflow is already a self-contained process, there are a few ways to trigger this job, and a few options for an asynchronous task queue. One option is to use Celery, a tool we are familiar with, to provide a known interface to trigger these analysis jobs. Another is to configure the analysis as an AWS ECS task, and have the application use the ECS API or Boto to start a new analysis.\\nCelery has multiple options for brokers:\\n| Broker | Advantages | Disadvantages |\\n| ------ | ---------- | ------------- |\\n| SQS | Cheap, easy to set up, now stable, provides configuration options to isolate environments | No result backend, [potential issues with result timeouts](http://docs.celeryproject.org/en/latest/getting-started/brokers/sqs.html#caveats) |\\n| Redis | Trivial to configure, can additionally be used as a results backend without further architecting | Key eviction issues, additional cost to run dedicated instance |\\nRunning the analysis via AWS Lambda was briefly considered, but the project dependencies and resources required are not conducive to that environment.\\n\n## Decision\nThe team will use Celery + SQS broker to manage the asynchronous analysis jobs. While Celery is not strictly necssary, it provides a potentially useful abstraction layer for triggering tasks, managing jobs and reporting errors. Celery also provides out of the box support for Django and allows us to write any peripheral task logic in Python. The SQS broker was chosen to keep the managed application architecture simple and reduce ongoing application stack costs. The team is familiar with an older version of the SQS broker used for the Cicero District Match project.\\n\n\n## Context\\nThe first prototype of AutoFlow used [Apache Airflow](https://airflow.apache.org/) (as used in FlowETL) to define and execute workflows. However, this proved to be problematic in some respects - Airflow has limited support for parametrising DAG runs and sharing data between tasks, and re-running a DAG for an execution date for which it has already run is complicated.\\n[Prefect Core](https://docs.prefect.io/) is an alternative open-source workflow engine, which allows DAGs to be parametrised and run simultaneously for multiple sets of parameters, and allows data exchange between tasks. Prefect also allows the creation of dynamically-generated tasks mapped over the outputs from running another task, which makes it easier for AutoFlow to spawn multiple runs of a workflow when the sensor finds multiple days of data for which the workflow has not previously run.\\n\n## Decision\nAutoFlow will use Prefect to define and run workflows.\\n\n\n## Context\\nIn ADR0003, we described the 'Bicycle Network Analysis' task to be run via an asynchronous task queue. Since then, Amazon Web Services (AWS) released a new service simply named 'Batch'. This service provides a managed task queue, with Boto and HTTP API interfaces for creating queues and jobs, and triggering new jobs. Each job in AWS Batch is configured to run a Docker container provided to the job configuration. AWS Batch manages ordering and execution of tasks in the queue. In almost every way, AWS Batch is a superior choice to the strategy outlined in ADR 0003, for a few key reasons:\\n- AWS Batch manages the queue and task autoscaling without any management from the parent application. The service can be trivially configured to scale up or down on a few different resource considerations. If there are no jobs in the queue, the pool of workers will automatically scale to zero, saving on hosting costs.\\n- AWS Batch, in comparison with a manually managed stack of celery workers + broker + result backend, is easy to configure, as it only requires defining a \"worker\" stack via a JSON cofiguration.\\n- Switching from a Celery and ECS task based solution will be easy, as AWS Batch workers are configured with Docker containers in the same way as ECS tasks would be\\n- It will be easier to trigger jobs from Django using AWS Batch, since direct calls can be made via Boto, rather than having to write some management layer to trigger ECS tasks or work with the Celery API.\\n\n## Decision\nThe team will build the Bicycle Network Analysis task queue on AWS Batch. The reduction in manual task queue management and ease of configuration should vastly outweigh having to learn how to develop applications using an unfamiliar service. While relatively new, AWS Batch has support in both Boto and via HTTP API and manual setup of a Batch stack was relatively straightforward.\\n\n\n# Context\\nAkvo DASH is a system that allows the user to connect to different _data\\nsources_ and import the data. After that user must be able to transform\\nit (clean, aggregate, etc).\\nWe have been looking at different open source ETL frameworks like:\\n* [Pentaho Data\\nIntegration](http://www.pentaho.com/product/data-integration)\\n* [Clover ETL](http://www.cloveretl.com/)\\n* [Onyx Platform](http://www.onyxplatform.org/about.html)\\n* [Bubbles](http://bubbles.databrewery.org/)\\n* [Kiba ETL](http://www.kiba-etl.org/)\\nSome of them provide a GUI to build transformations, others require\\ncoding.\\nThere are other ones that are based on Hadoop ecosystem, which is really\\ntoo much for our current needs:\\n* [Luigi](https://luigi.readthedocs.org/en/stable/)\\n* [Oozie](https://oozie.apache.org/)\\n* [Azkaban](https://azkaban.github.io/)\\n# Decision\\nBased on the skills of the team (Clojure expertise) and the fact Clojure\\nexcels at data transformation. We have decided that a small ad-hoc\\nfunctions for handling the import and transformation is enough for our\\ncurrent needs.\\nDepending on requirements we'll use a scheduling library like\\n[Quarzite](https://github.com/michaelklishin/quartzite) for scheduling\\nimports.\\n# Status\\nAccepted\\n# Consequences\\n* The current approach will be to create ad-hoc functions to handle\\nimports _(extract)_ from different data sources\\n* If we need a HA setup, there is an easy transition to Onyx Platform.\\nSee the\\n[conversation](https://gist.github.com/iperdomo/7af984b9f32c117678de) with Onyx author\\n\n## Decision\nBased on the skills of the team (Clojure expertise) and the fact Clojure\\nexcels at data transformation. We have decided that a small ad-hoc\\nfunctions for handling the import and transformation is enough for our\\ncurrent needs.\\nDepending on requirements we'll use a scheduling library like\\n[Quarzite](https://github.com/michaelklishin/quartzite) for scheduling\\nimports.\\n# Status\\nAccepted\\n# Consequences\\n* The current approach will be to create ad-hoc functions to handle\\nimports _(extract)_ from different data sources\\n* If we need a HA setup, there is an easy transition to Onyx Platform.\\nSee the\\n[conversation](https://gist.github.com/iperdomo/7af984b9f32c117678de) with Onyx author\\n\n\n## Context\\nThe Middleware team has strived to practice Infrastructure-As-Code, meaning DLP's AWS account should be reflected entirely within GitHub.\\nThis has mostly been accomplished but Ansible playbooks are currently run in Ansible Core on individual laptops of the Middleware team.\\nThe eventual goal should be GitOps, automated infrastructure changes triggered by a commit.\\nFor that purpose we will need a platform capable of running playbooks automatically.\\nThis platform would need to be able to run the playbooks on demand or scheduled.\\nRedHat provides two solutions: AWX and Ansible Tower.\\nAWX is the upline of Ansible Tower, meaning it is a more advanced version that lacks official support.\\n\n## Decision\nThe Middleware team will use AWX to manage DLP's infrastructure automation needs.\\nA different tool may be used for application automation.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n\n## Decision\n", "response": "## Decision\n\nThe team will use Airflow to manage asynchronous tasks and workflows for Raster Foundry. The main reasons for this choice are its maturity, stability, and the fact that it fits well with our execution model. Airflow's excellent UI will provide us with robust monitoring capabilities and make it easier to track the progress of workflows. Although it requires managing the scheduler and a cache, and also relies on Celery, the benefits of using a mature and stable tool with strong workflow capabilities outweigh these drawbacks. Additionally, as Airflow is written in Python, it aligns with the skill set of our team. This choice will facilitate better management of both ad hoc workflows and scheduled imports, while also enabling task progress monitoring, failure retries, and personnel notifications.", "matched_ids": [709, 5053, 707, 2803, 1729], "time": "0:00:03.222451", "input_tokens": 2654, "output_tokens": 150}
{"context": "## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context and Problem Statement\\nWe need to store data internally in a structured way to gain performance.\\n## Decision Drivers\\n* Easy to integrate\\n* Easy to use\\n* Common technology\\n\n## Decision\n* Easy to integrate\\n* Easy to use\\n* Common technology\\nChosen option: \"H2 Database Engine\", because it was straight-forward to use.\\n\n\n## Context\\nWe need to store persistent data in a form that is easy to query and need to select the\\nappropriate data store. We consider the choice to be between a relational database or a\\ndocument database.\\n\n## Decision\nWe have decided to use [MongoDB](https://www.mongodb.com/) as our database for datalabs\\ndata. We felt that the data model for the parts of the system known at this time, a\\ndocument structure provided more flexibility and easier integration with our Node.js\\napplication.\\nMongoDB also provides the ability to run in cluster providing the option for greater\\nresilience. For the time being we are opting to run a single node storing data to\\na mounted GlusterFS volume. This gives data resilience through node failure but obviously\\ndoes not give service resilience or time based backups.\\n\n\n## Context and Problem Statement\\nBFI's IIIF Univeral Viewer auditing platform requires that each event\\nwhich is subject to long term persistence is captured in a manner that\\nis secure, robust and performant.\\n## Decision Drivers\\n* Ability for internal BFI staff to leverage their existing experience\\nwith managing and maintaining the RDMS.\\n* Ease of deployment and configuration.\\n* Ease of ongoing maintenance, patching and support.\\n* Open source, mature and in active support / maintenance.\\n\n## Decision\n* Ability for internal BFI staff to leverage their existing experience\\nwith managing and maintaining the RDMS.\\n* Ease of deployment and configuration.\\n* Ease of ongoing maintenance, patching and support.\\n* Open source, mature and in active support / maintenance.\\nMySQL is selected as the solution as it aligns provides BFI with an RDMS\\nwith which they already have experience and are able to support and\\nmaintaining in the long term.\\n### Positive Consequences\\n* No initial or ongoing licensing costs associated with the use of\\nMySQL.\\n* Ease of ongoing maintenance for BFI owning to their existing\\nfamiliarity with MySQL.\\n* Availability of multiple storage engines (InnoDB, MyISAM, etc) and\\ndistributions (MySQL Community Edition, MariaDB, PerconaDB, etc).\\n* Simple deployment and configuration (e.g., MySQL publish official\\nDocker images).\\n### Negative Consequences\\n* Certain SQL features are not available (although it is not expected\\nthat these will be required).\\n\n\n## Context and Problem Statement\\nWe need a persistent state for the system. Previously I've used Azure SQL but it is quite pricey but has it's own set of features including relational model. But given that system is not very complex I can model persistence mechanism for NoSQL model.\\n## Decision Drivers\\n* Cost\\n* Backup capabilities\\n\n## Decision\n* Cost\\n* Backup capabilities\\nChosen option: \"Azure Table Storage\", because cost.\\n### Positive Consequences <!-- optional -->\\n* Persistence bill should be down from ~$5/month to less than $1/month\\n### Negative Consequences <!-- optional -->\\n* Students in IT Academy won't be exposed to a relational model, which at this moment dominates work they will have to be doing. We will have to come up with a strategy to get them good experience.\\n\n\n## Context\\nWe needed some persistent storage of audit results.\\nWe considered:\\n### Schemaless - DynamoDb\\nThis would be the most obvious choice for a lambda based\\nservice.\\nThe dynamo data model is tables of key value pairs.\\nThe main problem with Dynamo is the limit of I think 4K\\nper value.\\nOne of the things we wanted to do was briefly cache API\\nresponses which could easily breach that 4K limit.\\nWith Dynamo the access control is via IAM which would be\\nrelatively easy to manage and encryption at rest can\\nbe easily configured.\\n### Schemaless - MongoDb\\nMongo was a better fit for our service, saving JSON\\nrepresentations of API responses and resources.\\nThe problem with Mongo is it's not AWS native so we'd\\nhave to provision a server, manage the access controls,\\nmaintenance and availability.\\n### Relational - RDS MySQL / PostgreSQL\\nRDS gives us the benefit of running a managed service so\\nAWS are responsible for backups and patching minor\\nversion.\\nRelational databases give us the ability to do on-the-fly\\nanalysis more easily.\\nWe can store JSON as blob data although not ideal.\\nIf we want to re-use the database instance as a shared\\nresource across multiple services RDS is more capable.\\nIt's not unlikely that a future development may require\\ndjango.\\nPostgreSQL seems to be the general direction of travel\\nin GDS and is much more capable for a wider range of\\nuse-cases where we don't know what we're building in the\\nfuture.\\n\n## Decision\nWhilst none of the options were perfect we decided that\\na PostgreSQL RDS was the best option given the\\ninformation available to give us an infrastructure to\\nsupport multiple tools and services.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n\n## Decision\n", "response": "## Decision\nGiven the requirement to store SKOS taxonomies which are best suited to a graph database structure, and the decision drivers outlined, we have chosen to use Neo4j as our graph database solution.\n\nNeo4j offers native support for storing and querying RDF data, eliminating the need for complex serialization/deserialization processes. It also provides high availability and fault tolerance through its clustering capabilities, which aligns with our high availability requirements. While there is a learning curve associated with graph database query languages like Cypher, resources and community support make it manageable.\n\nIn terms of maintenance overhead, Neo4j offers robust management tools and active community support, reducing the burden on our team. While vendor lock-in is a consideration, the benefits provided, in this case, outweigh the risks, and there are emerging industry standards that support data portability across graph databases.\n\n### Positive Consequences\n* Native support for RDF and graph-based model reduces complexity.\n* High availability features ensure resilience.\n* Active community and resources to aid learning and support.\n\n### Negative Consequences\n* Initial learning curve with the Cypher query language.\n* Some level of vendor lock-in with Neo4j.", "matched_ids": [4747, 730, 4854, 2520, 418], "time": "0:00:05.767523", "input_tokens": 1388, "output_tokens": 232}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n", "response": "## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n", "matched_ids": [4699, 4428, 4408, 3119, 1436], "time": "0:00:02.494582", "input_tokens": 410, "output_tokens": 42}
{"context": "## Context\\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need a workflow to build our appliation and components.\\n\n## Decision\nWe use Storybook only for building new self contained components.\\n\n\n## Context\\nThe prototype kit application will need to operate similarly to a live service, depending on the level of fidelity required for testing. This means that a server is required. To make the prototype kit easy to use for different levels of coding experience we want to implement auto routing (or route matching) for template files, the simplest way to achieve this this dynamically is to have an application running on a server.\\n\n## Decision\nWe will use Node.js with the Express framework to run a Node.js server for the prototype kit on both local development environments and production environment when published on Heroku. Node.js is written in javascript and is therefore one the most common languages between designers who have some coding experience and software engineers. It is also the easiest runtime to install on both Mac and PC.\\n\n\n## Context\\nThe app should be able to build in any basic environment, without the need to worry about environmental differences\\n\n## Decision\nThe app will use Docker for production deployment.\\n\n\n## Context\\nThe project needs a way to be built and deployed. It needs to be quick and easy to use so has to be able to pick up changes to the git repo. It should be easy for anyone recreate the workflow used here and flexible enough to handle building multiple languages. At the end of successful build the artifact should get deployed.\\n\n## Decision\nAs this is project is benchmarking AWS Lambdas it makes sense to use the services AWS has for building (AWS CodeBuild) and deploying (AWS CodePipeline) code.\\n\n\n## Context\\nWe need to build the how part of the application\\n\n## Decision\nWe will use Springs ecosystem to implement how our application works.\\nMore explicitly we will use Spring Boot, Spring Data and Spring WebMvc.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\\n\n## Decision\n", "response": "## Decision\nWe will use npm scripts to automate these tasks. npm scripts provide an easy and standardized way to handle automation for tasks such as installing dependencies, moving files, and running the application. Since Node.js and npm are already used in our development stack, leveraging npm scripts allows us to maintain consistency and simplicity while handling the build and setup processes.", "matched_ids": [3722, 3681, 3264, 1568, 3533], "time": "0:00:03.427912", "input_tokens": 568, "output_tokens": 70}
{"context": "## Context\\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\\nto not overload a server.\\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\\nallows, among others:\\n- Delaying retries upon MX delivery failure to a remote site.\\n- Throttling, which could be helpful for not being considered a spammer.\\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\\ndelays, purging the queue, etc.\\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\\nto interact with all its James servers, which is not friendly in a distributed setup.\\nDistributed James relies on the following third party softwares (among other):\\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\\n- **ObjectStorage** (Swift or S3) holds byte content.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nJames offers several implementation for MailQueue, a component allowing asynchronous mail processing upon smtp mail\\nreception. These includes:\\n- Default embedded ActiveMQ mail queue implementation, leveraging the JMS APIs and using the filesystem.\\n- RabbitMQMailQueue allowing several James instances to share their MailQueue content.\\n- And FileMailQueue directly leveraging the file system.\\nWe introduced a junit5 test contract regarding management features, concurrency issues, and FileMailQueue do not meet this\\ncontract. This results in some tests being disabled and in an unstable test suite.\\nFileMailQueue tries to implement a message queue within James code, which does not really makes sense as some other projects\\nalready provides one.\\n\n## Decision\nDeprecate FileMailQueue components.\\nDisable FileMailQueue tests.\\nTarget a removal as part of 3.6.0.\\n\n\n## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n\n## Decision\nImplement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n\n\n## Context\\nJames exposes a simple BlobStore API for storing raw data. However such raw data often vary in size and access patterns.\\nAs an example:\\n- Mailbox message headers are expected to be small and frequently accessed\\n- Mailbox message body are expected to have sizes ranging from small to big but are unfrequently accessed\\n- DeletedMessageVault message headers are expected to be small and unfrequently accessed\\nThe access pattern of some of these kind of blobs does not fit Object Storage characteristics: good at storing big blobs, but\\nit induces high latencies for reading small blobs. We observe latencies of around 50-100ms while Cassandra latency is of 4ms.\\nThis gets some operations slow (for instance IMAP FETCH headers, or listing JMAP messages).\\n\n## Decision\nImplement a write through cache to have better read latency for smaller objects.\\nSuch a cache needs to be distributed in order to be more efficient.\\nGiven that we don't want to introduce new technologies, we will implement it using Cassandra.\\nThe cache should be implemented as a key-value table on a dedicated 'cache' keyspace, with a replication factor of 1,\\nand be queried with a consistency level of ONE.\\nWe will leverage a configurable TTL as an eviction policy. Cache will be populated upon writes and missed read, if the\\nblob size is below a configurable threashold. We will use the TimeWindow compaction strategy.\\nFailure to read the cache, or cache miss will result in a read in the object storage.\\n\n\n## Context\\nRead [Distributed Mail Queue](0031-distributed-mail-queue.md) for full context.\\n**enqueuedMailsV3** and **deletedMailsV2** is never cleaned up and the corresponding blobs are always referenced. This is not\\nideal both from a privacy and space storage costs point of view.\\nNote that **enqueuedMailsV3** and **deletedMailsV2** rely on timeWindowCompactionStrategy.\\n\n## Decision\nAdd a new `contentStart` table referencing the point in time from which a given mailQueue holds data, for each mail queue.\\nThe values contained between `contentStart` and `browseStart` can safely be deleted.\\nWe can perform this cleanup upon `browseStartUpdate`: once finished we can browse then delete content of **enqueuedMailsV3**\\nand **deletedMailsV2** contained between `contentStart` and the new `browseStart` then we can safely set `contentStart`\\nto the new `browseStart`.\\nContent before `browseStart` can safely be considered deletable, and is applicatively no longer exposed. We don't need an\\nadditional grace period mechanism for `contentStart`.\\nFailing cleanup will lead to the content being eventually updated upon next `browseStart` update.\\nWe will furthermore delete blobStore content upon dequeue, also when the mail had been deleted or purged via MailQueue\\nmanagement APIs.\\n\n\n## Context\\nRead [Event Bus ADR](0037-eventbus.md) for context.\\nGiven several James servers, we need them to share a common EventBus.\\nThis:\\n- Ensures a better load balancing for `group mailbox listners`.\\n- Is required for correctness of notifications (like IMAP IDLE).\\n\n## Decision\nProvide a distributed implementation of the EventBus leveraging RabbitMQ.\\nEvents are emitted to a single Exchange.\\nEach group will have a corresponding queue, bound to the main exchange, with a default routing key. Each eventBus\\nwill consume this queue and execute the relevant listener, ensuring at least once execution at the cluster level.\\nRetries are managed via a dedicated exchange for each group: as we need to count retries, the message headers need to\\nbe altered and we cannot rely on rabbitMQ build in retries. Each time the execution fails locally, a new event is emitted\\nvia the dedicated exchange, and the original event is acknowledged.\\nEach eventBus will have a dedicated exclusive queue, bound to the main exchange with the `registrationKeys` used by local\\nnotification mailboxListeners (to only receive the corresponding subset of events). Errors are not retried for\\nnotifications, failures are not persisted within `DeadLetter`, achieving at most once event delivery.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\\nto not overload a server.\\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\\nallows, among others:\\n- Delaying retries upon MX delivery failure to a remote site.\\n- Throttling, which could be helpful for not being considered a spammer.\\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\\ndelays, purging the queue, etc.\\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\\nto interact with all its James servers, which is not friendly in a distributed setup.\\nDistributed James relies on the following third party softwares (among other):\\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\\n- **ObjectStorage** (Swift or S3) holds byte content.\\n\n## Decision\n", "response": "## Decision\n\nImplement a hybrid MailQueue solution leveraging RabbitMQ for efficient message queuing and ObjectStorage for holding email content. \n\n- **RabbitMQ** will continue to be used for managing the primary message queue and handling asynchronous mail processing due to its efficiency in message queuing and delivery. However, we will augment it by integrating a management layer that facilitates advanced administrative operations like `browse`, `getSize`, and `arbitrary mail removal`. This will involve maintaining metadata for each message in the queue.\n\n- **Cassandra** will be used to store the metadata associated with each message in RabbitMQ. This metadata will include information necessary for browsing and administrating the queue. We will use a careful table design to avoid anti-patterns such as those involving tombstones. We can achieve this by focusing on immutable metadata updates and employing time-based cleanup strategies.\n\n- **ObjectStorage** (Swift or S3) will hold the byte content of emails, allowing us to separate the metadata from the larger email data. This separation will ensure that our system remains scalable and that processing remains efficient across distributed systems.\n\n- A consolidated administration interface will be developed to seamlessly manage the distributed MailQueue across all James servers, allowing administrators to easily monitor and manage email traffic, apply throttling, reset delays, and purge queues as required.\n\nBy using this hybrid architecture, we can maintain fast and efficient mail processing while also enabling robust and user-friendly administration capabilities across distributed systems.", "matched_ids": [2888, 2896, 2145, 2880, 2125], "time": "0:00:08.104822", "input_tokens": 2100, "output_tokens": 290}
{"context": "## Context\\nThe main data source of the API catalogue website was originally a collection of\\nmarkdown files. The build process uses the Middleman static site generator\\nconfigured by the [Tech Docs Gem](https://github.com/alphagov/tech-docs-gem)\\n('TDG').\\nThe TDG provides additional functionality including search, sidebar\\nnavigation ('Table of Contents'), the layout, and styling.\\nThe TDG is not necessarily a good fit for the API catalogue because the project\\nisn't purely documentation, and our data source is now a CSV.\\nIn particular it is difficult to override templates inherited from the gem, to\\nadjust the layout on a particular page or add page-specific JavaScript for\\nexample.\\nUsing TDG to render the Table of Contents is slow for our site because\\nby design every page is re-rendered multiple times to pull out the headings\\n(adding over a minute to build times).\\nThe TDG also requires specific dependency versions. These version\\nrestrictions prevent us being in control of version upgrades which are necessary\\nto remain on support versions and receive security patches.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\n[PaaS product pages] have been reviewed and a number of accessibility issues\\nhave been identified. To resolve those we, would need to make an upgrade and\\nand review if any additional changes are needed to align with the GOV.UK Design System.\\nAs those pages are built in Ruby and in [ADR024] we've made the decision\\nto develop our user-facing applications on Node, it's a good opportunity to\\nlook at rebuilding the product pages.\\nWe've discussed user needs and it emerged that:\\n* anyone in the team (developer and non-developer) should be able to update pages\\nwith less effort\\n* pages should be performant for end users\\n* pages should be rendered by the server\\n* keeping pages up to date with GOV.UK Design System releases should be quicker and easier\\n* alignment of technologies for our user-facing web products should provide better\\ndeveloper experience and give us the option to have shared component libraries\\nWith the above in mind we researched options. Our admin interface is built in React,\\nso we narrowed the scope to React-based static site generators.\\nWe ended up comparing two: [NextJS] with static page export and [GatsbyJS]\\nwhich exports static pages by default.\\nFor page content we agreed that writing pages in [Markdown] is a good option,\\nso we tested both with [MDX] which can also embed React components inside content pages.\\n[NextJS] and [GatsbyJS] have different approaches to development and there are minor\\nperformance differences between them.\\nOur use case for now is narrow enough, and with the primary need of anyone in the team\\nbeing able to update pages, [NextJS] marginally gets more votes as Gatsby cannot be installed and run on\\nnon-developer machines.\\n\n## Decision\nWe will use [NextJS] together with [MDX] to author PaaS product pages content in\\n[Markdown] and deliver them to users as static pages.\\n\n\n## Context\\nI wanted to create a website blog using [Markdown][1], a [static site generator][2] and [HapiJS][3] to serve the files. Static site generators I tried, in order, were -\\n### [11ty][4]\\nPros\\n- Easy setup\\n- Predictable static file output\\n- Relatively small codebase\\n- Supporst different template engines\\nCons\\n- The frontmatter parser [didn't work as I expected][7]\\n- Template errors were not reported and would only fail on build\\n- Needs some attention to make production ready e.g. SEO, images, favicons etc\\n### [Gatsby][5]\\nPros\\n- Modern [JAMstack][8] project with React\\n- Batteries included by default - includes helpers for favicons, image loading, SEO etc.\\n- Opportunity to use GraphQL\\nCons\\n- Doesn't output static files - [totally depends on clientside JavaScript][9]\\n- It relies on service workers to cache responses which causes flakey e2e tests and unpredictable behaviour\\n- Developing static files based on API was difficult because of caching and unexpected error handling\\n- Relatively complex project with separate build, develop steps, config, plugins etc\\n### [Hugo][6]\\nPros\\n- Fast and mature static site generator\\n- Small footprint\\nCons\\n- Written in Go - I prefer fullstack JavaScript for this site\\n\n## Decision\nIn the context of using several static site generators each of them had with their own downsides. And facing the concern of having blog that I can easily maintain and customise I decided to build my blog with HapiJS to achieve a smaller blog footprint, predictable behaviour and an opportunity to learn something new. I accept that I'm re-inventing the wheel.\\n\n\n## Context\\nA consistent platform is required for user documentation from the PACE projects: Brille, Euphonic and Horace/Herbert.\\nTwo platforms support, are well used for this service:\\n- [Read the Docs](https://readthedocs.org/)\\n- [GitHub pages](https://pages.github.com/)\\nBoth platforms will display documentation built by [Sphinx](https://www.sphinx-doc.org/) from reStructuredText source files.\\nBrille includes compiled C libraries that contribute APIs to the project. The Read the Docs build/deploy pipeline does not support inclusion of documentation generated from C source.\\n\n## Decision\nWe will use GitHub pages for all PACE project documentation.\\n\n\n## Context\\nWe want the data to be in one place, be it internal (in the static assets) or external (served by an API or an S3 bucket...)\\nWe want the project to be data-driven, so that the document is re-rendered if any data changes.\\nWe want the data being spread across the Components using the latest technologies.\\n\n## Decision\nThe source of data is kept internally. It will stay in the webapge, as a static asset from now on.\\nBut the project must be kept easily switchable to an external data source.\\nWe spread the data across Components using React Context API.\\n\n\n# Context\\nGranary will be a standalone deployable open source application. Users' primary\\nway of interacting with Granary will be through its REST API, rather than\\nthrough a UI, at least for the foreseeable future. This focus puts a larger\\nburden than usual on the quality of our API documentation.\\nWe'd like to evaluate different ways to keep API documentation up-to-date. For\\neach strategy, we want to know:\\n- What are the general pros and cons?\\n- How do we find out that our docs have drifted from the API?\\n- How can we document different versions of the API at the same time?\\n\n## Decision\nWe should use `tapir` for automatically generating API documentation. The ADT\\nsupport and straightforward API (inputs use `.in`, outputs use `.out`, auth\\nextractors use `auth`) will flatten out the learning curve, and we'll have a\\nstable and correct reference point for API documentation that users setting up\\ntheir own deployments can refer to. We can call this out at the beginning of\\nthe README and hopefully save ourselves from having to answer an entire\\ncategory of questions.\\n# Consequences\\n- The first routes added to the API will be slightly more difficult, because\\nthey'll include writing API routes with a new library for the first time.\\n- The README should be updated to point to the location of the API\\ndocumentation.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nThe main data source of the API catalogue website was originally a collection of\\nmarkdown files. The build process uses the Middleman static site generator\\nconfigured by the [Tech Docs Gem](https://github.com/alphagov/tech-docs-gem)\\n('TDG').\\nThe TDG provides additional functionality including search, sidebar\\nnavigation ('Table of Contents'), the layout, and styling.\\nThe TDG is not necessarily a good fit for the API catalogue because the project\\nisn't purely documentation, and our data source is now a CSV.\\nIn particular it is difficult to override templates inherited from the gem, to\\nadjust the layout on a particular page or add page-specific JavaScript for\\nexample.\\nUsing TDG to render the Table of Contents is slow for our site because\\nby design every page is re-rendered multiple times to pull out the headings\\n(adding over a minute to build times).\\nThe TDG also requires specific dependency versions. These version\\nrestrictions prevent us being in control of version upgrades which are necessary\\nto remain on support versions and receive security patches.\\n\n## Decision\n", "response": "## Decision\n\nWe will migrate the API catalogue website from using the Tech Docs Gem with Middleman to a more flexible and performant static site generator or framework that better accommodates our data source needs and provides easier template customization. Considering our shift from markdown to CSV data, we should explore options like Next.js or Gatsby to leverage their ability to handle different data sources and allow for server-side rendering and dynamic page generation. This change will provide better control over version upgrades, improve build times by optimizing the rendering process, and support easier template and layout customization for specific pages. We will phase out the reliance on TDG to ensure our project remains well-supported and secure with the latest dependency updates.", "matched_ids": [177, 1599, 5172, 3945, 3549], "time": "0:00:04.581358", "input_tokens": 1912, "output_tokens": 135}
{"context": "## Context and Problem Statement\\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https://github.com/Semantic-Org/Semantic-UI-React/pull/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https://github.com/decentraland/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\\n- The `@artsy/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\\n- Doesn't have hooks support.\\n#### Second alternative (react-semantic-ui)\\n##### Advantages\\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\\n##### Disadvantages\\n- Bad SSR support.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n\n## Decision\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript jsx\\n// Good:\\n<Box padding\"small\" />\\n<Box padding={{xs: 'small', md: 'medium'}} />\\n<Input large />\\n<Text preset=\"heading3\" as=\"p\" />\\n// Bad:\\n<Box className={customLayout} />\\n<Input style={{ height: 50, padding: 16 }} />\\n<Text className={styles.heading} />\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n\n\n## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n\n## Decision\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript\\n// Good:\\n<Box padding\"small\" />\\n<Box padding={{xs: 'small', md: 'medium'}} />\\n<Input large />\\n<Text preset=\"heading3\" as=\"p\" />\\n```\\n```typescript\\n// Bad:\\n<Box className={customLayout} />\\n<Input style={{ height: 50, padding: 16 }} />\\n<Text className={styles.heading} />\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from Braid, which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n\n\n## Context and Problem Statement\\nWe're building websites and web applications that share a common design system with reusable components. How do we write CSS styles in a way that is performant and safe?\\n## Decision Drivers\\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles/classes.\\n- We want a pattern for responsive props with atomic layout components.\\n\n## Decision\n- Should be performant, with code splitting, caching and minimal runtime overhead.\\n- Needs to have easy access to our design system constants. These should optimally be shared with JS logic.\\n- Should be type-safe to catch issues when refactoring.\\n- Reusable components should be closed, not accepting arbitrary styles/classes.\\n- We want a pattern for responsive props with atomic layout components.\\nChosen option: Treat, because it combines the best of both worlds from CSS-in-JS and CSS modules.\\nWe'll create shared components that have responsive props, but are otherwise closed for modifications. Theme variables are defined in a shared library with TypeScript.\\nExample:\\n```typescript jsx\\n// Good:\\n<Box padding\"small\" />\\n<Box padding={{xs: 'small', md: 'medium'}} />\\n<Input large />\\n<Text preset=\"heading3\" as=\"p\" />\\n// Bad:\\n<Box className={customLayout} />\\n<Input style={{ height: 50, padding: 16 }} />\\n<Text className={styles.heading} />\\n```\\n### Positive Consequences\\n- Treat is statically extracted at build time, so it has minimal runtime.\\n- Styles load in parallel with JS, also when code splitting.\\n- Styles are written in TypeScript which gives us type safety when referring to shared variables, styles and helpers.\\n- Styles are in special files, separate from markup and components giving us clear separation with good visibility into the rendered markup.\\n- We can pull in responsive layout component patterns from [Braid], which gives us a good base to lay out components and pages.\\n### Negative Consequences\\n- We are choosing a pretty new framework, so it may 1) have bugs or issues, 2) be an obstacle for new developers or 3) be discontinued.\\n- When we're generating responsive styles at build time we need to be mindful at how many variations we allow (eg media queries, columns, whitespace), since they can easily bloat our CSS with unused styles.\\n\n\n## Context\\nWe want:\\n- Write all simple React components in one repository\\n- Document the React components with code comments\\n- Add additional markdown documentation for components when necessary\\n- Allow both designers and engineers to edit the docs\\n- Run the tool locally to make component development and testing easier\\n- Build into a hostable web app, which can be used by anyone to learn our style, pick an appropriate component, and edit the component on the page\\n- Be able to style/theme anything about the style guide app as a whole to match our other docs\\n### Options\\n[React Storybook](https://storybook.js.org/)\\n[React Styleguidist](https://react-styleguidist.js.org/)\\n\n## Decision\nUse Styleguidist. They way it is built from markdown is more user-friendly for designers to edit vs. React Storybook. Also, it is more aimed at generating a living style guide, whereas Storybook is more of a developer's tool.\\n\n\n## Context and Problem Statement\\nViskuausan is proving to be more complex and larger platform than just a simple documentation site from static content. Which React framework provides the most out-of-the-box features that we need?\\n## Decision Drivers\\n- Should use NodeJS and React as outlined in [S\u00cd technical direction](../../technical-direction.md)\\n- Should be able to support markdown content rendered to HTML\\n- Should be open source\\n- Should be customizable to island.is UI design\\n\n## Decision\n- Should use NodeJS and React as outlined in [S\u00cd technical direction](../../technical-direction.md)\\n- Should be able to support markdown content rendered to HTML\\n- Should be open source\\n- Should be customizable to island.is UI design\\nChosen option: NextJS + NestJS\\nNextJS is the chosen web framework for all island.is websites needing server side rendering. As Viskuausan will probably be merged with island.is main website, creating it using same frameworks makes it easy to merge later on. It is easier to reuse Island UI components using NextJS over Docusaurus. Docusaurus main advantage over Next is out-of-the-box markdown support but it is easy to add markdown support in NextJS using [Remark](https://github.com/remarkjs/remark) library.\\nNestJS is used to create backend services and Viskuausan needs few backend services related to the X-Road and API GW integrations. Provides functionalities like ORM, dependency injection, unit testing.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https://github.com/Semantic-Org/Semantic-UI-React/pull/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https://github.com/decentraland/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\\n- The `@artsy/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\\n- Doesn't have hooks support.\\n#### Second alternative (react-semantic-ui)\\n##### Advantages\\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\\n##### Disadvantages\\n- Bad SSR support.\\n\n## Decision\n", "response": "## Decision\n\nAfter evaluating both alternatives, we have decided to adopt the `react-responsive` library as the replacement for the removed `Responsive` component from `react-semantic-ui`. \n\n### Rationale\n1. **Non-coupling Dependency**: Unlike `@artsy/fresnel`, `react-responsive` does not require a ContextProvider to wrap the entire application, which can simplify integration across different apps and libraries.\n\n2. **Hooks and Components**: `react-responsive` offers both hooks and component-based APIs for handling media queries, making it a versatile choice that fits into various scenarios where different device size renderings are needed. This supports a gradual adoption across diverse codebases.\n\n3. **Ease of Refactoring**: The choice of `react-responsive` allows a smoother transition by offering an intuitive API, making refactoring tasks and the update process more straightforward for development teams.\n\n4. **Community and Support**: `react-responsive` is widely adopted and has a larger user community, which can offer more community-driven support and resources during the transition.\n\n### Implementation Plan\n- **Phase 1**: Immediately patch the existing library by copying the removed `Responsive` component from its old version to prevent breaking changes across the board while the transition is planned.\n  \n- **Phase 2**: Implement `react-responsive` as the new media query solution in the shared UI repository. Update documentation to guide development teams in adopting the new library.\n\n- **Phase 3**: Provide training and examples to get development teams on board with the new solution, ensuring everyone understands the transition path and benefits.\n\n- **Phase 4**: Remove the patched `Responsive` component once all dependent projects have migrated to `react-responsive`.\n\n- **Consideration for SSR**: Given the poor server-side rendering (SSR) support in `react-responsive`, invest in testing and possible workarounds to minimize any performance issues in applications that need SSR, ensuring the solution is robust for all parts of the ecosystem.\n\n### Positive Consequences\n- Facilitates a smoother upgrade to React 17 across all UI apps and libraries.\n- Unifies the handling of media queries under a more modern and flexible library.\n- Reduces coupling, making future updates and maintenance easier.\n\n### Negative Consequences\n- Requires temporary maintenance of legacy code for the `Responsive` component until a full transition occurs.\n- May necessitate additional effort to address SSR issues for applications with specific rendering requirements.", "matched_ids": [4762, 1108, 1992, 3035, 1112], "time": "0:00:10.642696", "input_tokens": 3000, "output_tokens": 493}
{"context": "## Context\\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\\nIn the future we will want to more automatically match buyers\u2019 articulations of need so as to work out which products CCS can offer to meet their need.\\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need to establish a domain structure which\\n* Is product agnostic\\n* Is consistent across the opg-data service\\n\n## Decision\nWe will adopt the pattern:\\n[pull-request-id].[account-stage].[microservice-domain].api.opg.service.justive.gov.uk\\nWhere [pull-request-id] is for ephemeral dev environments.\\nWhere [account-stage] is the stage in our path to live/accounts i.e. dev,pre and empty for prod services.\\nWhere [microservice-domain] is the scoped domain of the microservice/integration in in question.\\n* hey\\n### examples\\n#### root:\\n* https://api.opg.service.justice.gov.uk\\n#### integration:\\n* https://deputy-reporting.api.opg.service.justice.gov.uk\\n#### environments per integration:\\n* https://pre.deputy-reporting.api.opg.service.justice.gov.uk\\n* https://dev.deputy-reporting.api.opg.service.justice.gov.uk\\n#### pr raised on an environment per integration:\\n* https://pr-1234.dev.deputy-reporting.api.opg.service.justice.gov.uk\\n\n\n## Context\\nWe are building a new digital service for CCS.\\n[CCS-ADR-0002][ccs-adr-0002] says that all new services should have a common\\nuser experience, based on the GOV.UK Design System.\\nThese patterns should be amended to use the CCS colours and brand, but should\\nremain consistent with GOV.UK where possible.\\n\n## Decision\nWe will follow this CCS Architectural Decision and use the GOV.UK Design System\\nand GOV.UK Frontend, amended to use CCS colours and brand.\\nThe service will be hosted on a subdomain of `crowncommercial.gov.uk`.\\n\n\n## Context\\nThe services need to look and feel the same. They need to comply with [adr 0002 - Use a consistent user experience](0002-use-a-consistent-user-experience.md).\\nNQC\u2019s SRS and DPS services are already built. They comply with GOV.UK styles to a certain extent. They should be partially exempted from this decision for the moment\\nGDS have released their new [Design System](https://design-system.service.gov.uk). It does not yet support non-GOV.UK designs but that is on the [roadmap](https://design-system.service.gov.uk/roadmap/).\\n[CCS Website](https://www.crowncommercial.gov.uk/s/) has complied with, and contributed to, GDS patterns, but has a divergent colour palette.\\n\n## Decision\n- All new services should share a common front end toolkit\\n- There may be different modules per language, sharing the same HTML, CSS, JavaScript assets\\n- SRS stylesheets should comply with this kit as closely as practicable. We should revisit this decision as new phases of work are considered for transparency and supplier registration including DPS\\n- Comply with GDS Design System except where CCS styling requires differences; document the differences\\n- Use a checkout of the Frontend toolkit as the base for front end resources\\n- Add a new repository for style and other resource overrides to meet CCS needs,\\n- Periodically pull new versions of Frontend toolkit and test\\n- Work with GDS to help them support non-GOV.UK styles\\n\n\n## Context\\nWe would benefit from having a central repository of integrated data from multiple sources that stores current and historical information, and use this data to create analytical reports and performance indicators to support the publishing workflow within GOV.UK.\\nThis is actually difficult to achieve as we have the information dispersed across different applications that are currently designed to support transactional operations and not analytical reporting of integrated data.\\n\n## Decision\nBuild a data warehouse(*) that maintains a copy of the information of the transactional systems.\\n(*) We will be using a PostgreSQL database in the first iteration, we will be exploring other existing solutions for data warehouses in future iterations once we have validated this approach.\\n\n\n## Context\\n* Procurement Operations team uses a shared mailbox hosted in DfE's Exchange Online to communicate with School Buying Professionals.\\n* A key requirement from case workers is to record all the email interactions regarding a case in the case management system , so there is a full view of a case and all associated interactions\\n\n## Decision\n* Use Microsoft Graph for to integrate with Microsoft Exchange Online.\\n* Use Microsoft Identity [OAuth 2.0 client credentials grant flow](https://github.com/microsoftgraph/microsoft-graph-docs/blob/main/azure/active-directory/develop/v2-oauth2-client-creds-grant-flow) to set up application's permissions for using MS Graph API with restricted permissions only to access shared mailbox.\\n* Microsoft Graph is recommended by Microsoft and [Outlook API has been deprecated](https://docs.microsoft.com/en-us/previous-versions/office/office-365-api/api/version-2.0/use-outlook-rest-api)\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\\nIn the future we will want to more automatically match buyers\u2019 articulations of need so as to work out which products CCS can offer to meet their need.\\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\\n\n## Decision\n", "response": "## Decision\n- Implement a centralized question library that can house common questions and patterns, allowing different agreements to pull relevant questions as needed.\n- Develop a modular data storage architecture that distinguishes between buyer journey data and finalized agreement details. \n  - Use a NoSQL database like MongoDB to store buyer journey data for flexibility and ease of capturing semi-structured data.\n  - Store finalized agreement details in a structured relational database like PostgreSQL for reliable transactional performance and consistency.\n- Introduce a recommendation engine that utilizes machine learning to analyze buyer responses and suggest appropriate CCS products that meet their identified needs. This engine will gradually be improved to increase automation in matching buyer needs with available products.\n- Ensure that integration layers between the question library, data storage, and recommendation engine are API-driven to facilitate seamless data exchange and future scalability.\n- Regularly review and update question patterns, buyer data models, and recommendation algorithms to stay aligned with emerging buyer needs and CCS capability enhancements.", "matched_ids": [2183, 2047, 995, 1863, 1267], "time": "0:00:05.770983", "input_tokens": 1355, "output_tokens": 191}
{"context": "## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted/PaaS CI/CD solution\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nTravis is actively used for many other projects within the BEIS GitHub account.\\nWe know we will want to be using containers to run this application, so the tool we use must support Docker too.\\n\n## Decision\nUse Travis to build and test the application.\\nUse Travis to automatically deploy the applications to live.\\n\n\n## Context\\nTravisCI is currently used for building feature branches, pull requests and releases.\\nThe way releases, however, are performed is a bit convoluted and not flexible. By default minor version is updated\\nand it's not clear how to do a major or patch release. Due to the way TravisCI builds feature branches vs pull requests\\nit requires hacks in travis config.\\n\n## Decision\nSolution implemented in https://github.com/ifrost/starterkit have worked quite well so far. It's based on adding\\nlabels to commit messages when merging a pull request to master.\\nIt's way easier to do major/minor/patch or hotfix release and publish to npm separately.\\n\n\n## Context\\nTo automated your development you need a CICD platform that automated the build, test and deploy steps.\\n\n## Decision\nI use Azure Pipeline to build, test and deploy.\\n\n\n## Context\\nWe need a way of deploying our services to our various hosted environments (dev, staging, prod, etc.). We would\\nprefer these deployments to be automated with minimal to zero human interaction.\\n\n## Decision\nThe DIT infrastructure already has a lot of tooling and infrastructure around deployments so we will utilise this. We\\nwill use Jenkins to automatically deploy from dedicated git branches - these will be:\\n- development\\n- staging\\n- uat\\nWe will have 5 Jenkins jobs in total - these will be:\\n- trade-access-program-backoffice\\n- trade-access-program-frontend\\n- trade-access-program-polling-dev\\n- trade-access-program-polling-staging\\n- trade-access-program-polling-uat\\nThe role of the \"polling\" jobs are to watch a related git branch for any commit changes. Once a change is detected then\\nit will trigger the `trade-access-program-backoffice` and `trade-access-program-frontend` jobs with a set of\\nenvironment parameters triggering the deployment to one of our three environments.\\nThis allows us to simply merge or push to one of the three dedicated git branches above and a full automated deployment\\nwill occur for that environment.\\n\n\n## Context\\nIn the previous iteration, build and deploy were consolodated into one pipeline. The result of this being that any type of configuration change required a full rebuild to deploy.  This could become unwieldy with projects that have a long-running build step.\\n\n## Decision\nPipeline has been split into distinct build and deploy pipelines.  The build pipeline, in addition to the image that it uploads to ECR, exports artifacts build.json and src.zip.  Src.zip is required still required in the deploy pipeline to run integration tests.  In the deploy pipeline, either the artifacts supplied by the build pipeline OR new app.yaml templates will trigger the pipeline.  Consequently, a config change may be made by uploading a new app.yaml, without having to re-build the Docker image.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted/PaaS CI/CD solution\\n\n## Decision\n", "response": "## Decision\nConsider switching to GitHub Actions for CI/CD. GitHub Actions offers a flexible and customizable workflow configuration, allowing for more complex and nuanced pipeline stages. Additionally, it provides seamless integration with GitHub repositories as well as a robust ecosystem of pre-built actions, which can greatly simplify the setup process. This platform aligns with the need for a hosted/PaaS solution while overcoming the stage limitation issues faced with Travis.", "matched_ids": [2393, 5106, 3717, 5013, 351], "time": "0:00:03.921023", "input_tokens": 818, "output_tokens": 85}
{"context": "## Context\\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nOur original plan was to upload files to S3 using signed URLs from S3, and checking the files' headers to determine whether or not it was, in fact, a text file. Further research has revealed that there is no fool proof way of doing this and a malicious user would be able to spoof a virus as a text file.\\n\n## Decision\nInstead of using a signed URL and sending the file directly to S3, we will instead send the file to the backend and scan it with Clam AV before sending it to S3. In the event there is a virus, we will destroy the file on the backend immediately and return an error to the frontend.\\nBy using the [ClamAV REST server](https://github.com/ajilaag/clamav-rest) implementation we are able to scan files for viruses and malicious behavior. Additionally, Anti-Virus definitions are kept up to date automatically by use of the included [freshclam](https://www.clamav.net/documents/signature-testing-and-management#freshclam) tool which automatically downloads and updates an internal database of virus signatures using the official ClamAV source.\\nIn addition to this, the frontend is able to reliably determine if a file is a binary file. The client will check submitted files for this and immediately return an error to the user. These files will not be sent to the backend.\\n\n\n## Context\\nOne of the tenants of Twelve Factor application design is that applications\\nshould be stateless, which includes not relying on local file storage to be\\npersistent. As such, this project needs a cloud based object store.\\nSee [3. Follow Twelve Factor methodology](0003-follow-twelve-factor-methodology.md)\\nAmazon Simple Storage Service (S3) is a secure, durable, and scalable object\\nstorage solution to use in the cloud with which we have established an existing\\npayment relationship.\\nAmazon provides official SDKs for various programming languages to interact\\nwith S3.\\n\n## Decision\nWe will use Amazon S3 for our object store.\\n\n\n## Context\\n* Minio already has a UI and an S3 API that allow administration of files and buckets.\\n* There are existing client libraries for the S3 API.\\n* It is nontrivial to proxy large file uploads through the armadillo server.\\n\n## Decision\nThe Armadillo client will be written as a high-level library on top of an existing S3 API.\\n\n\n## Context\\nWe are using POST HTTP requests to communicate with the local server.\\nTo pass multiple parameters we would use requests with `multipart/form-data` or `text/plain` type.\\nCreating correctly encoded multipart requests is tricky.\\n\n## Decision\nWe will use json for scratch to local server communication.\\nI.e. responses from server to scratch will be json objects similar to this:\\n````json\\n{\\n\"status\" : \"\", | \"SUCCESS\" or \"FAILED\"\\n\"errorMessage\" : \"\", | null when no error happened, otherwise an error message\\n\"result\" : \"\" | null when there is no output (e.g. in case of an error), otherwise the result\\n}\\n````\\n\n\n## Context\\nEach data source will need to upload one or more files to S3 in order to trigger processing. S3 events, which will drive Lambda execution (See [5. Use AWS Lambda](0005-use-aws-lambda.md)), are configured at the bucket level. We may or may not have much control over the environment which is sending data to S3, for example, if it came directly from a vendor. At minimum we must be able to specify a bucket, but we should not assume we will have much more control than this.\\nEach data source will also need different processing. This implies the need to identify which source a data file came from.\\n\n## Decision\nUse one S3 bucket per data source.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\\n\n## Decision\n", "response": "## Decision\nUse the S3 Presigned URL functionality for direct uploads to S3. This approach minimizes server load and bandwidth usage since the files do not have to be uploaded twice (first to the API, then to S3). It also reduces the processing needed on the server, allowing clients to directly upload to S3 securely. By generating presigned URLs, the client can be temporarily granted permission to upload files directly to S3 without needing access to AWS credentials, thereby maintaining security. Additionally, it offloads the compilation and file size limitations to S3, which is designed to handle large files efficiently.", "matched_ids": [3797, 3417, 547, 2639, 3416], "time": "0:00:05.975727", "input_tokens": 1115, "output_tokens": 122}
{"context": "## Context\\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nSmarthub SDK contracts are designed to be upgradable by abstracting proxy, logic and storage to separate contracts. This approach leads to maintaining 3 separate Solidity files per contract.\\n\n## Decision\nUse OpenZeppelin implementation based on generalized proxy, logic and storage to remove the need of keeping 3 separate custom implemented contracts.\\n\n\n## Context\\nWe need a simple way to manage our package version.\\n\n## Decision\nWe use versioneer to do this for us.\\n\n\n## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n\n## Decision\nWe will use Dependabot to monitor dependency updates.\\n\n\n## Context\\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\\n\n## Decision\nWe will use Dependabot to monitor dependency updates.\\n\n\n## Context\\nWe don't want to create an SDK that requires customers to add a slew of\\nexternal dependencies.\\n\n## Decision\nWe will avoid external runtime dependencies. For instance, we will use the core\\nApple networking libraries (e.g. URLSession) instead of introducing a\\ndependency on something like [Alamofire][1].\\nExternal development dependencies (those necessary for contributing, but which\\nare not bundled with the SDK) will be added as necessary.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\\n\n## Decision\n", "response": "## Decision\nWe will implement a monorepo setup using a tool like Lerna or Nx to manage the multiple packages within the Smarthub SDK. This will facilitate automatic updates of dependent packages whenever a dependency is changed by utilizing shared configurations and streamlined workflows.", "matched_ids": [3686, 241, 3328, 4225, 4195], "time": "0:00:02.794849", "input_tokens": 466, "output_tokens": 52}
{"context": "## Context\\nDrop FreeTFT, TFTA tokens from SDK\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nScratch supports images/sprites.\\nThe ftduino has no way to show images/sprites.\\n\n## Decision\nScratch images/sprites and image/sprite related blocks are not supported.\\n\n\n## Context\\nWe don't want to create an SDK that requires customers to add a slew of\\nexternal dependencies.\\n\n## Decision\nWe will avoid external runtime dependencies. For instance, we will use the core\\nApple networking libraries (e.g. URLSession) instead of introducing a\\ndependency on something like [Alamofire][1].\\nExternal development dependencies (those necessary for contributing, but which\\nare not bundled with the SDK) will be added as necessary.\\n\n\n## Context\\nThere was much discussion over whether the SDK should implement the\\nFirebase-related services for token refresh and notification handling, or leave\\nthat up to the consuming app. If the SDK handles it, it's less setup for the\\nconsumer; however, this comes at the cost of much flexiblity.\\n\n## Decision\nIn the end, we decided that the lack of flexibility (e.g. for supporting both\\n\"register on first launch\" and \"register/unregister on log in and log out\"\\nmodels) warranted removing the services from the SDK. They will be implemented\\nin the demo application instead.\\n\n\n## Context\\nScratch supports sounds.\\nThe ftduino has no way to play sound.\\n\n## Decision\nScratch sounds and sound related blocks are not supported.\\n\n\n## Context\\nContext here...\\n\n## Decision\n\u4f7f\u7528 Github \u8fdb\u884c Code Review\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nDrop FreeTFT, TFTA tokens from SDK\\n\n## Decision\n", "response": "## Decision\nFreeTFT and TFTA tokens will be removed from the SDK. This decision allows us to streamline the SDK by reducing unnecessary components, focusing on more widely used features and ensuring ease of maintenance and future updates. Users requiring these tokens will need to implement their own support outside the SDK.", "matched_ids": [2637, 4195, 5042, 2632, 2416], "time": "0:00:02.513594", "input_tokens": 432, "output_tokens": 60}
{"context": "## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThe original author of go-bindata delete their account and then the repo was\\nrecreated under a different owner. The dependency has shifted around enough that\\nwe have lost faith in the intention of the maintainers\\n[more details here](https://twitter.com/francesc/status/961249107020001280?lang=en)\\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\\nduring a test run or final build) made it hard to reason about.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nTo simplify the implementation for drivers, we will create a \"unified\" GPIO base class, which provides the standard interfaces that can be used to set the mode (input, output, special function). This will provide a single class to implement for GPIO support, rather than one class for each mode.\\nIdeally, we will find a way to zero-overhead create wrapper classes that constrain the interface appropriately, so we can mark pins as `GPIOInput` or `GPIOOutput` and have the compiler report an error if we try to do an invalid operation for the given type.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n\n## Decision\nThe change that we're proposing or have agreed to implement.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThe original author of go-bindata delete their account and then the repo was\\nrecreated under a different owner. The dependency has shifted around enough that\\nwe have lost faith in the intention of the maintainers\\n[more details here](https://twitter.com/francesc/status/961249107020001280?lang=en)\\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\\nduring a test run or final build) made it hard to reason about.\\n\n## Decision\n", "response": "## Decision\nWe will transition away from using go-bindata for bundling assets into our Go applications. Instead, we will explore other maintained and stable alternatives such as go:embed which provides native embedding of resources in Go as of version 1.16, ensuring better reliability and maintainability. This transition aims to simplify the bundling process, enhance security by reducing dependency on untrusted maintainers, and improve the transparency of what is included in builds and test runs. We will also conduct an audit of all indirectly affected parts of our codebase to ensure compatibility with the new approach.", "matched_ids": [3544, 2025, 3021, 639, 2026], "time": "0:00:03.801451", "input_tokens": 628, "output_tokens": 117}
{"context": "## Context\\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\\n[Prettier](https://prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https://prettier.io/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https://github.com/prettier/prettier/network/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n## Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n\n\n## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https://prettier.io/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https://github.com/prettier/prettier/network/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n## Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n\n\n## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https://prettier.io/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https://github.com/prettier/prettier/network/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n## Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n\n\n## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https://prettier.io/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https://github.com/prettier/prettier/network/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, and has become a standard.\\n\n## Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n\n\n## Context\\nWe want to ensure we're all using one code style, that is familiar across\\nprojects. [Prettier](https://prettier.io/) is an opinionated code formatter with\\nsupport for most, if not all, of the languages in the JavaScript ecosystem. As\\nof writing, it is used by over\\n[1 million repositories](https://github.com/prettier/prettier/network/dependents?package_id=UGFja2FnZS00OTAwMTEyNTI%3D)\\non GitHub, including React itself, and has become a standard.\\n\n## Decision\nWe will enforce that everything supported by Prettier has its style enforced by\\nit.\\nWe will set up Git hooks to automatically run the formatter before committing.\\nWe will set continuous integration up to reject commits that are not correctly\\nformatted.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\\n[Prettier](https://prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\\n\n## Decision\n", "response": "## Decision\n\nWe will adopt Prettier as our primary code formatting tool across all Kyma projects. This will ensure a consistent code style that enhances code readability and maintainability. The following steps will be implemented:\n\n1. Prettier will be configured with a unified configuration file that defines the formatting rules for all supported languages used in Kyma projects, including JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON.\n\n2. We will integrate Prettier as a plugin in supported IDEs to provide instant feedback on code formatting during development.\n\n3. Prettier will be set up as a pre-commit `git` hook to automatically format code before it is committed. This will enforce consistent formatting at the commit level and prevent formatting issues in the codebase.\n\n4. The existing CI/CD pipelines will be updated to include checks for code formatting compliance. Any code that does not adhere to the Prettier configuration will be rejected, ensuring the codebase remains consistently formatted.\n\n5. Documentation will be provided to developers, detailing how to install and configure Prettier for their development environment, and how to use it effectively within the Kyma projects. \n\nBy following these practices, we aim to minimize style-related discussions and focus on writing clean, functional code.", "matched_ids": [5255, 1408, 4226, 3760, 454], "time": "0:00:04.335563", "input_tokens": 1583, "output_tokens": 258}
{"context": "## Context\\nWe have to make a decision on the branching strategy for development.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need a strategy for how to git and name branches\\n\n## Decision\nTasks can be found in [Projects](https://github.com/kits-ab/LogLady/projects/1).\\nAssign one to yourself then create branch from develop and name it like this: feature/whetever-you-are-doing-#corresponding-task-ID\\nExample: `feature/branchName-#5`\\n**Note:** _feature_ is an example, it could be:\\n- feature\\n- hotfix\\n- release\\nCommit messages should start with task ID.\\nExample: `#5 update README. Explained naming.`\\nPush when done. Make pull request where you add task ID to beginning of comment.\\n\n\n## Context\\nFor the project a deployment and development mechanism is need to be chosen and adopted.\\n\n## Decision\n- [Kubernetes](https://kubernetes.io/) will be the deployment technology of choice.\\nThe project will be deployed into a kubernetes cluster using [helm](https://helm.sh/).\\n- [Docker](https://www.docker.com/) will be the container building technology of choice.\\nEvery thing will be directed through docker containers, from development to deployment. CI Pipeline will run inside a docker container as well.\\n\n\n## Context\\nWe need to record the architectural decisions made for our team.\\n\n## Decision\nWe will use Architecture Decision Records, as discussed in our original proposal: https://docs.google.com/document/d/1pZlYCyXcZbmQq1O-g4BNJD1uZTjluYKkk7BSu2BwOGU/edit#\\n\n\n## Context\\nWe need to choose how we will deploy our app.\\n\n## Decision\nWe use Docker because it allows us to isolate app with a container and easily deploy to the server with Terraform.\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe have to make a decision on the branching strategy for development.\\n\n## Decision\n", "response": "## Decision\nWe will adopt the \"GitFlow\" branching strategy to manage our development workflow. The strategy will include the following branches:\n- **master**: this branch will always reflect a production-ready state.\n- **develop**: an integration branch for features; all feature branches will be merged here.\n- **feature/**: derived from `develop`, each feature branch will be named after the feature being developed, e.g., `feature/user-authentication`.\n- **release/**: branch created from `develop` when we reach a point that is ready for release. No new features are added here, only bug fixes, documentation generation, and other release-oriented tasks.\n- **hotfix/**: branched from `master` whenever needed to quickly address issues in the production environment. After completing a hotfix, it should be merged back into both `master` and `develop`.\n\nMerge pull requests will require a code review, and feature access may be restricted based on team roles to ensure code quality and stability.", "matched_ids": [1851, 1061, 2261, 1239, 546], "time": "0:00:06.386769", "input_tokens": 561, "output_tokens": 203}
{"context": "## Context\\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n\n## Decision\nUse Simplecov with RSpec to monitor coverage changes on every test run\\n\n\n## Context\\nWe want to keep our test coverage as high as possible without having to run\\nmanual checks as these take time and are easy to forget.\\n\n## Decision\nUse Simplecov with RSpec to monitor coverage changes on every test run\\n\n\n## Context\\nWe want to keep our test coverage as high as possible without having to run manual checks as these take time.\\n\n## Decision\nUse the free tier of Coveralls to give us statistics and to give our pull requests feedback.\\n\n\n## Context\\nWe need to choose test framework.\\n\n## Decision\nWe use JUnit because it is a standard framework for JVM, and we don't have any advanced requirements yet.\\n\n\n## Context\\nWe need an easy way to integrate and test out code that is fast and reliable.\\n\n## Decision\nWe choose Bitrise because it came from a suggestion from more senior devs and provides an easy interface to manage the workflows with good support for Android apps and testing.\\nIt also allows us to notify users in a easy way and with different roles.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\\n\n## Decision\n", "response": "## Decision\nUtilize dependency injection to pass configuration settings selectively to parts of the application that require them, allowing for partial configuration loading. By injecting only the necessary configuration for specific test cases, we reduce side effects and enable more focused testing in projects using `konfetti`. This approach also aligns with the flexibility `konfetti` provides in managing configurations.", "matched_ids": [4179, 1258, 2556, 1241, 4868], "time": "0:00:03.304156", "input_tokens": 423, "output_tokens": 72}
{"context": "## Context\\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\\n```\\nstruct tof\\n{\\nusing distance_t = uint16_t;\\nusing cb_t = stdext::inplace_function<void(distance_t)>;\\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\\nenum class mode\\n{\\ndefaultRange = 0,\\nshortRange,\\nmedRange,\\nlongRange,\\n};\\n}''\\n```\\nInterface classes would inherit from these structs:\\n```\\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\\n```\\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\\n```\\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\\n```\\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nUnder the initial design of the `embvm-core`, the following types stored a name in the base class:\\n- Driver\\n- Processor\\n- Hardware Platform\\n- Platform\\nIn some cases, like drivers, this data was duplicated (through the driver registry) or unnecessary (because you use the default name and provide the system name in the driver registry). We don't need to store two different instances of the same name!\\nIn other cases, like the processor and platforms, these names were never used.\\n\n## Decision\nThese names will no longer be required by the base classes.\\n\n\n## Context\\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\\nThis project contains a large number of classes, making up the various constructs and patterns. The project is intended to be used as a component library and, therefore, used by a number of people who don't have extensive knowledge of either the CDK or this library. It is therefore important to make the experience of using these classes as intuitive as possible.\\n\n## Decision\n<!-- What is the change that we're proposing and/or doing? -->\\nConstructors should follow the following rules for consistency.\\n1. The first parameter should be a `scope` of type `GuStack`:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack) {\\n...\\n}\\n}\\n```\\nThe construct/pattern will then have a static `id` as it will never change, for example the `Stage` parameter.\\n2. They can also take a `props` object which should be correctly typed:\\n:white_check_mark: Valid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: Stack, props: object) {\\n...\\n}\\n}\\n```\\nThe construct/pattern will then derive `id` from `props` as it will never change, for example `InstanceTypeFor${props.app}`.\\n3. They can also take an `id` of type string and a `props` object which should be correctly typed\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {...}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: any, props: object) {\\n...\\n}\\n}\\n```\\n4. Where all `props` are optional, the `props` object should be optional as a whole\\n:white_check_mark: Valid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props?: MyConstructProps) {\\n...\\n}\\n}\\n```\\n:x: Invalid\\n```ts\\ninterface MyConstructProps {\\nprop1?: string;\\nprop2?: string\\n}\\nclass MyConstruct {\\nconstructor(scope: GuStack, id: string, props: MyConstructProps) {\\n...\\n}\\n}\\n```\\n\n\n## Context\\nThe issue motivatina# [short title of solved problem and solution]\\nWhat is the issue that we're seeing that is motivating this decision or change?\\n* Status: [proposed | rejected | accepted | deprecated | ... | superseded by [ADR-0005](0005-example.md)] <!-- optional -->\\n* Deciders: [list everyone involved in the decision] <!-- optional -->\\n* Date: [YYYY-MM-DD when the decision was last updated] <!-- optional -->\\nTechnical Story: [description | ticket/issue URL] <!-- optional -->\\n## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\n\n## Decision\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | ... | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, ...]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, ...]\\n* ...\\nWhat is the change that we're proposing and/or doing?\\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context\\nCode base is becomes too complicated.\\nMaintaining and supporting multiple API versions is painful and expensive.\\n\n## Decision\nWe will avoid API versioning.\\nWe will not introduce breaking changes.\\n\n\n## Context\\nIt is easy to use different terms meaning the same thing in code. A common aphorism is that the most difficult thing in coding is naming.\\n\n## Decision\nA `Terminology.md` document will be created, that will be used to collect the official terms for the project. If naming diverts from this terminology, it should be considerd a bug.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\\n```\\nstruct tof\\n{\\nusing distance_t = uint16_t;\\nusing cb_t = stdext::inplace_function<void(distance_t)>;\\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\\nenum class mode\\n{\\ndefaultRange = 0,\\nshortRange,\\nmedRange,\\nlongRange,\\n};\\n}''\\n```\\nInterface classes would inherit from these structs:\\n```\\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\\n```\\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\\n```\\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\\n```\\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\\n\n## Decision\n", "response": "## Decision\nWe will refactor the codebase to utilize `namespace` instead of struct inheritance to manage driver interface types and associated constants. By using namespaces, we aim to simplify our inheritance chains and make the code more intuitive and organized. \n\n- Define the types and constants directly within a dedicated namespace, rather than within structs:\n  ```cpp\n  namespace embvm\n  {\n      namespace tof\n      {\n          using distance_t = uint16_t;\n          using cb_t = stdext::inplace_function<void(distance_t)>;\n          constexpr distance_t INVALID_RANGE = UINT16_MAX;\n\n          enum class mode\n          {\n              defaultRange = 0,\n              shortRange,\n              medRange,\n              longRange,\n          };\n      }\n  }\n  ```\n\n- Interface classes should use the namespace directly:\n  ```cpp\n  class TimeOfFlight : public embvm::DriverBase\n  {\n  public:\n      virtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\n      // Other virtual member functions as necessary\n  };\n  ```\n\nThis approach enhances readability and maintainability by leveraging C++'s native namespacing capabilities, avoiding unnecessary complications in the form of an extended inheritance architecture.", "matched_ids": [3023, 1193, 4800, 3534, 2270], "time": "0:00:06.381244", "input_tokens": 1763, "output_tokens": 252}
{"context": "### Context\\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\\n```php\\nfunction test_something(): void {\\n// ...\\ntry {\\n// the statement that fail\\n$this->fail();\\n} catch (Exception $e) {\\n// ...\\n}\\n}\\n```\\nOr:\\n```php\\nfunction test_something(): void {\\n// ...\\n$this->expectException($exception)\\n// the statement that fail\\n}\\n```\\n### Decision\\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https://thephp.cc/company/consultants/sebastian-bergmann\\n[phpunit-exception-best-practices]: https://thephp.cc/news/2016/02/questioning-phpunit-best-practices\\n[1090]: https://github.com/infection/infection/pull/1061\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n### Context\\nPHPUnit assertions are static methods, yet in our code base we call them with `$this` instead of\\n`self`.\\nWhilst \"incorrect\", this usage does not break anything. Besides:\\n- [PHUnit documentation][phpunit-doc] itself uses this by default\\n- `$this` is much more widely used than `self` in this context in the community\\n- all Infection code uses `$this`\\nThere is not much shortcomings from using this other than the \"incorrectness\" of using a static\\nmethod as a non-static one.\\n### Decision\\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https://phpunit.de/manual/6.5/en/appendixes.assertions.html\\n[1061]: https://github.com/infection/infection/pull/1061\\n\n## Decision\nSince there is no clear benefits of adopting `self` over `$this` and given the context of its usage,\\nthe decision is to keep the usage of `$this` over `self` in the codebase.\\n### Status\\nAccepted ([#1061][1061])\\n[phpunit-doc]: https://phpunit.de/manual/6.5/en/appendixes.assertions.html\\n[1061]: https://github.com/infection/infection/pull/1061\\n\n\n## Context\\nWhen deciding on a naming scheme for unit test methods, the following criteria were most important to us:\\n* **Readability** - The test method names should read like English sentences, with clear word boundaries\\n* **Flexibility** - We should be able to follow different patterns, like \"Given ... When .. Then\", but also like \"Does X\".\\n\n## Decision\nWe use `snake_case` for method names in unit tests. We adapt the coding style settings to ignore the deviation from our usual `camelCase` convention.\\nIf it makes sense, we use sentences containing the words `given`, `when` and `then`. To give each section a clear boundary, when we use `given` or `when`, then we also use `then`.\\nGood:\\ntest_given_first_time_visitor_then_return_main_banner\\nBad:\\ntest_given_first_time_visitor_return_main_banner\\nWe keep in mind that the sentences always refer to the system-under-test (SUT) and don't unnecessarily repeat its class name.\\n\n\n## Problem\\nThe previous unit testing framework started as hack to have a bit more\\nthan simple asserts. It is not easy to use (needs explicit enumeration\\nof all test cases) and lacks really important features (e.g. output of\\nthe assertion that failed).\\n\n## Decision\n- Keep C framework for C tests and ABI tests\\n- Google Unit testing framework `gtest` with code downloaded by CMake for\\nsystems where no source is packaged (Debian Wheezy, Arch Linux,\\nFedora,...) for C++ tests\\n- [Script Testing](script_testing.md)\\n\n\n## Context\\n<!--- What is the issue that we're seeing that is motivating this decision or change? -->\\nOne of the benefits of writing our infrastructure in a fully fledged programming language is the ability to write reusable components which can be tested. This library defines those components, both in the form of constructs and patterns (see [001-constructs-and-patterns](./001-constructs-and-patterns.md) for more details).\\nThere are two main strategies that can be used to unit test these components.\\nSnapshot tests synthesise a given stack and compare the output to a previous version that has been verified by the user and checked into the repository. If there are any changes, the test fails and the user is displayed the diff to either fix or update the stored snapshot.\\n[+] Quick and easy to write\\n[+] Pick up any changes to the output cloudformation (particularly useful for unintended side effects)\\n[-] A change to the component may cause many unrelated tests to also fail\\n[-] When testing different permutations of a component, a number of snapshots will be created, each of which contains a certain amount of information which is irrelevant to the particular test. This adds some extra effort to understand what is relevant to the test which you're looking at\\n[-] Snapshots are easy to update which, especially when multiple are affected by a change, makes it easy to accidentally update them incorrectly. Further to this, as the snapshot (which is essentially the assertion) is stored in a different file, it's not immediately obvious if the assertions are valid\\nDirect Assertions use the different assertions provided by the test framework to test specific functionality. For example, asserting that a property exists or that an array contains a particular value.\\n[+] Each test only contains the relevant assertions for the test, making it easier to understand the consequence of settings certain props\\n[+] Changes will only fail tests that cover that particular area\\n[-] More complex and time consuming to write\\n\n## Decision\n<!-- What is the change that we're proposing and/or doing? -->\\nUse direct assertions for constructs and snapshots for patterns (and stacks)\\n_This decision is a recommendation for the general approach. There may be some cases where using a different approach is more applicable for a given test._\\n\n\n## Context\\n* We want to write programmer tests to support a TDD workflow.\\n* We want to be able to mock out functions.\\n\n## Decision\n* We will use Midje to test our code.\\n* Despite it's heavy macro design, it allows you to write expressive code and easily makes mocks\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n### Context\\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\\n```php\\nfunction test_something(): void {\\n// ...\\ntry {\\n// the statement that fail\\n$this->fail();\\n} catch (Exception $e) {\\n// ...\\n}\\n}\\n```\\nOr:\\n```php\\nfunction test_something(): void {\\n// ...\\n$this->expectException($exception)\\n// the statement that fail\\n}\\n```\\n### Decision\\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https://thephp.cc/company/consultants/sebastian-bergmann\\n[phpunit-exception-best-practices]: https://thephp.cc/news/2016/02/questioning-phpunit-best-practices\\n[1090]: https://github.com/infection/infection/pull/1061\\n\n## Decision\n", "response": "## Decision\n\nThe decision is to adopt the `expectException*()` API for test cases that involve code expected to fail with exceptions. This approach makes for more readable and concise tests by leveraging the PHPUnit API purpose-built for this use case. While existing code using try-catch blocks with a `fail()` method can remain, it's recommended to update them to conform to the `expectException*()` pattern where feasible. New contributions to the codebase, however, should follow this practice to maintain consistency and clarity in test case expectations. This decision aligns with best practices suggested by recognized experts in the field.", "matched_ids": [66, 852, 1304, 1192, 1387], "time": "0:00:05.564374", "input_tokens": 1860, "output_tokens": 120}
{"context": "## Context\\nWhen developing new algorithms and features for PACE-related software,\\ndevelopers often create useful demo/visualisation scripts for their own use.\\nThese scripts could be useful or interesting for other developers, and are\\nimportant for reproducibility or justifying design decisions. They should be\\nstored somewhere in version control so that they can be easily accessed by any\\ndevelopers and referred to later. However, they are not intended for general\\nuse and will not be actively maintained or tested. There are 2 main options:\\n* Store them in a `dev_scripts` directory in each separate project repository\\n* Store them in a `scripts` directory in `pace-developers`\\nIf they're in the `dev_scripts` directory for each project repository:\\n+ All in one place\\n+ Scripts will be close to the code they are used for\\n- Scripts may not work with the version of the code they are distributed with\\n- It's unclear where scripts that use more than one project would go\\n- Despite the folder being called `dev_scripts` people might expect the scripts\\nto actually work as they're in the main project repository\\nIf they're in a `scripts` directory in `pace-developers`:\\n+ They can be kept close to the decision-making developer documentation that\\nthey support\\n+ A version can be specified for any project dependencies\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\ndxw have approved an RFC for following the pattern of Scripts To Rule Them\\nAll[1].\\nThis repository should include reference and document this decision.\\n[1]\\nhttps://github.com/dxw/tech-team-rfcs/blob/main/rfc-023-use-scripts-to-rule-them-all.md\\n\n## Decision\nBy default we will follow the Scripts To Rule Them All pattern for common tasks\\nin this template.\\n\n\n## Context\\nWhilst the core framework of PACE is the Horace (and Herbert) projects,\\nPACE involves many other codes, including [Euphonic](https://github.com/pace-neutrons/Euphonic),\\n[Brille](https://github.com/brille/brille) and [SpinW](https://spinw.org).\\nThe way in which these programs interact with each other is presently unclear.\\nFurthermore the programs are written in several different languages.\\nIn this ADR we propose some guiding principles in the design of the interfaces\\nbetween the programs or components which make up PACE.\\nThis was discussed at a meeting whose\\n[minutes are here.](https://stfc365.sharepoint.com/:w:/r/sites/PACEProject/Shared%20Documents/Meetings/PACE-General/pace_syntax_meeting1_minutes.docx))\\n\n## Decision\nThe guiding principle for PACE-supported programs is **commonality**.\\nThat is, interacting with the different programs of PACE should be **seamless**\\nto the user, and the user interface presented to them should be **uniform**.\\nThe uniform interface means that function names and how users specify parameters\\nor arguments to functions in Matlab and Python flavours of PACE should align\\nas closely as possible.\\n\n\n## Context\\nWe should have a consistent directory structure for Terraform from the beginning.\\nSome initial requirements:\\n- We want to separate code from data, so in the future we can open source the code without disclosing our implementation details,\\nmaybe keeping the data in a private Github repository\\n- We want to be able to encrypt sensitive data in the repository: we want to support sensitive data encryption as part of the same\\nprocess, without having to manage secrets in a different repository, with different scripts, etc.\\n- We want to create Terraform modules to reuse code\\n- We want to separate Terraform code into different projects, each one managing an infrastructure tier or application stack. This\\nis especially important to separate resources between GOV.UK applications.\\n\n## Decision\nThe initial solution presents three directories: data, modules and projects:\\n- The data directory contains a subdirectory per Terraform project, to store variable values that can be customised per environment.\\n- The data directory also contains \\_secrets files with sensitive data encrypted with 'sops'\\n- The modules directory contains a subdirectory per Terraform provider\\n- The projects directory contains the Terraform stacks/tiers\\n- Projects will be named such that:\\n+ Those to deploy common infrastructure will have a `govuk` prefix (e.g. networking, DNS zones)\\n+ Those to deploy applications (or groups of applications) will have an `app` prefix (e.g. frontend, puppetmaster).\\n```\\n\u251c\u2500\u2500 data\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 base\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 my-application\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 common.tfvars\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 integration.tfvars\\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 integration_secrets.json\\n\u251c\u2500\u2500 modules\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 aws\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 route53_zone\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500  ...\\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 mysql_database_instance\\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 network\\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 ...\\n\u2514\u2500\u2500 projects\\n\u251c\u2500\u2500 base\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 integration.backend\\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main.tf\\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 variables.tf\\n\u2514\u2500\u2500 my-application\\n\u251c\u2500\u2500 integration.backend\\n\u251c\u2500\u2500 main.tf\\n\u2514\u2500\u2500 variables.tf\\n```\\n\n\n## Context\\nGitHub Pages support two [publishing sources](https://help.github.com/en/github/working-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site) for project documentation:\\n- the `docs` folder on the `master` branch\\n- the root folder of the `gh-pages` branch\\na third option is to store the built project documentation in\\n- documentation GitHub project.\\nIf the built documentation is stored on the `master` branch:\\n- the codebase includes built artifacts\\n- any release tag needs to be created on the commit after that from which the build was executed\\n- requires the CI server to make commits to the `master` branch\\n- it's straightforward to manage document builds from branches other than `master`\\nWhere the build artifacts on a separate branch:\\n- harder to compare source with the built documentation and manage the build artifacts from branches,\\n- cleanly separates build artefacts from source code\\nStoring built documentations in a separate GitHub repository:\\n- offers clear separation between source and built documentation\\n- carries an overhead or managing another repository\\n- documentation will served at a URL that doesn't match the source: e.g. `pace-neutrons.github.io/horace-docs`\\n\n## Decision\nBuilt documentation will be stored on the `gh-pages` branch.\\n\n\n## Context\\nWhen multiple demo instances exist in parallel, we need to avoid to step on each others toes. The scope is\\n- the manuela-gitops repository, where component deployment and configuration takes place as part of a demo run\\n- the manuela-dev repository, where coding changes take place as part of a demo run\\nOptions:\\n1. **BRANCHES** inside the existing sa-mw-dach repos, for each manuela demo env there would be a branch (e.g. name based on a convention).\\nPros:\\n- Easiest to set up since it's literally just cloning the central repos.\\n- only need to adjust branch names for new demo instance (however the number of required adjustments is likely the same)\\nCons:\\n- need to coordinate the creation/assignment of branches\\n- all demoers need write access to the repos\\n- Danger of cluttering the repo with \"abandoned\" branches\\n2. **FORKS**: The \"owner\" of the new installation forks all repos in GitHub or creates new repositories from scratch. The changing remote URL needs to be adjusted when setting up the demo. Preferably, there would be somewhere one central configuration (GIT_BASE_URL) defining from which fork to pull from.\\nPros:\\n- demo instances can be set up without any coordination\\n- No conflicts when branches are used for e.g. production staging approvals\\nCons:\\n- need to adjust\\n- pipeline configs (pointing to forks of manuela-dev/manuela-gitops)\\n- argoCD applications (poingint to forks of manuela-gitops)\\n3. **DEDICATED GIT**: We could deploy a dedicated GIT(LAB?) to a namespace, place the manuela-gitops +  manuela-dev repos there. As long as we can use only cluster-internal URLs these will not change across instances.\\nPros:\\n- Consistent URLs at least in a single-cluster scenario, no need to adapt anything\\nCons:\\n- Will require similar adaptations as **FORKS** in a multi-cluster scenario, since we will need to use external URLs for cross-cluster communication\\n\n## Decision\nUse **FORKS**: this requires least coordination, access rights or additional components.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWhen developing new algorithms and features for PACE-related software,\\ndevelopers often create useful demo/visualisation scripts for their own use.\\nThese scripts could be useful or interesting for other developers, and are\\nimportant for reproducibility or justifying design decisions. They should be\\nstored somewhere in version control so that they can be easily accessed by any\\ndevelopers and referred to later. However, they are not intended for general\\nuse and will not be actively maintained or tested. There are 2 main options:\\n* Store them in a `dev_scripts` directory in each separate project repository\\n* Store them in a `scripts` directory in `pace-developers`\\nIf they're in the `dev_scripts` directory for each project repository:\\n+ All in one place\\n+ Scripts will be close to the code they are used for\\n- Scripts may not work with the version of the code they are distributed with\\n- It's unclear where scripts that use more than one project would go\\n- Despite the folder being called `dev_scripts` people might expect the scripts\\nto actually work as they're in the main project repository\\nIf they're in a `scripts` directory in `pace-developers`:\\n+ They can be kept close to the decision-making developer documentation that\\nthey support\\n+ A version can be specified for any project dependencies\\n\n## Decision\n", "response": "## Decision\nStore the scripts in a `scripts` directory in `pace-developers`. This approach allows the scripts to be centrally located where they can be easily accessed and maintained in alignment with the developer documentation they support. It also enables specifying project dependency versions, which can help mitigate compatibility issues. Additionally, this location clearly communicates that these scripts are for developer use and not intended for general use, avoiding potential confusion for end users regarding script functionality.", "matched_ids": [4178, 5178, 4054, 5175, 2971], "time": "0:00:03.925231", "input_tokens": 2269, "output_tokens": 90}
{"context": "**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\\nfor all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https://en.cppreference.com/w/cpp/io/ios_base/iostate\\n[bigtable-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/bigtable\\n[gcs-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/storage\\n[survey-link]: https://isocpp.org/blog/2018/03/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https://github.com/googleapis/google-cloud-cpp/issues/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n\n## Decision\nwill not test binary distributions of our code. And we will not host configs\\nfor binary distributions, since that would involve hosting files which we do\\nnot test. However, we will do our best to make our code easily packageable by\\nothers in a wide variety of formats. For more context, see\\n[#333](https://github.com/googleapis/google-cloud-cpp/issues/333).\\n**Consequences**: This decision will shield from endorsing any particular\\nbinary package management system. Other individuals who want to build and\\nmaintain a binary distribution of our code may do so without our permission or\\nknowledge. If they need small and reasonable tweaks from us to support their\\nbinary distributions, they may file an issue and we'll do our best to\\naccommodate them within reason.\\n\n\n**Context**: Compiling google-cloud-cpp from source is not always possible nor\\ndesired by all users. Some users may want to install a binary package that\\ncontains our public headers and pre-compiled libraries so that they can link\\nagainst our library. There is no single standard binary package manager in C++,\\ninstead there are a variety of others that users might want to use (e.g., dpkg,\\nrpm, vcpkg).\\n**Decision**: We will not directly support nor provide any binary packages. We\\nwill not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https://github.com/googleapis/google-cloud-cpp/issues/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n\n## Decision\nwill not test binary distributions of our code. And we will not host configs for\\nbinary distributions, since that would involve hosting files which we do not\\ntest. However, we will do our best to make our code easily packageable by others\\nin a wide variety of formats. For more context, see\\n[#333](https://github.com/googleapis/google-cloud-cpp/issues/333).\\n**Consequences**: This decision will shield from endorsing any particular binary\\npackage management system. Other individuals who want to build and maintain a\\nbinary distribution of our code may do so without our permission or knowledge.\\nIf they need small and reasonable tweaks from us to support their binary\\ndistributions, they may file an issue and we'll do our best to accommodate them\\nwithin reason.\\n\n\n**Context**: most APIs in the GCS library have a number of optional parameters,\\nfor example, the API can use `ifMetagenerationMatch` to apply an operation only\\nif the metadata generation matches a given number. The question arose of how to\\nrepresent these parameters, as properties that we modify in the client or\\nobject, or as per-request parameters in the function used to access the API.\\nThat is, we had two proposals, one where the application would write code like\\nthis:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\nvs. code like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\n// Create a new bucket handle that applies the given optional parameters to\\n// all requests.\\nauto bucket = b.ApplyModifiers(\\nstorage::IfMetagenerationMatch(7), UserProject(\"my-project\"))\\nbucket.ApiName(foo, bar);\\n}\\n```\\n**Decision**: The parameters are passed as variadic arguments into any function\\nthat needs them. That is, all APIs look like this:\\n```C++\\nclass Bucket /* or Object as applicable */ { public:\\ntemplate <typename... Parameters>\\nReturnType ApiName(\\nFixedArgument1 a1, FixedArgument2 a2,\\nParameters&&... p);\\n```\\nand are used like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\n**Consequences**: The advantages of this approach include:\\n- It is easier to use parameters in an API, it does not require to create a new\\nbucket or object or client handle just for changing the parameters in one\\nrequest.\\nThe downsides include:\\n- All APIs become templates, we should be careful not to create massive header\\nfiles that are slow to compile.\\n- It is harder to overload APIs.\\n- It is not clear how other optional parameters of the APIs, such as timeouts,\\nfit with this structure.\\n\n## Decision\nthat needs them. That is, all APIs look like this:\\n```C++\\nclass Bucket /* or Object as applicable */ { public:\\ntemplate <typename... Parameters>\\nReturnType ApiName(\\nFixedArgument1 a1, FixedArgument2 a2,\\nParameters&&... p);\\n```\\nand are used like this:\\n```C++\\nvoid AppCode(Bucket b, FixedArgument1 foo, FixedArgument2 bar) {\\nb.ApiName(\\nfoo, bar, storage::IfMetagenerationMatch(7), UserProject(\"my-project\"));\\n```\\n**Consequences**: The advantages of this approach include:\\n- It is easier to use parameters in an API, it does not require to create a new\\nbucket or object or client handle just for changing the parameters in one\\nrequest.\\nThe downsides include:\\n- All APIs become templates, we should be careful not to create massive header\\nfiles that are slow to compile.\\n- It is harder to overload APIs.\\n- It is not clear how other optional parameters of the APIs, such as timeouts,\\nfit with this structure.\\n\n\n**Context**: operations that change state in the server may complete with an\\nindeterminate status code. For example: an error in the 500 range may be\\nproduced by a middle-tier server after the operation was started by the backend.\\nRe-issuing the request may not work as expected for the application developer.\\n**Decision**: by default the library will retry all requests, including\\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n\n## Decision\nnon-idempotent ones. The library will allow application developers to override\\nthe definition of what operations are automatically retried.\\n**Consequences**: most operations become easier to use for application\\ndevelopers. In very rare cases the operation will result in double uploads, or\\nin a new generation of the object or metadata being created. In even more rare\\ncases the operation may fail, for example, an operation to create an object with\\n`IfGenerationMatch(0)` would fail on the second attempt.\\n\n\n## Context\\nThe issue motivatina# [short title of solved problem and solution]\\nWhat is the issue that we're seeing that is motivating this decision or change?\\n* Status: [proposed | rejected | accepted | deprecated | ... | superseded by [ADR-0005](0005-example.md)] <!-- optional -->\\n* Deciders: [list everyone involved in the decision] <!-- optional -->\\n* Date: [YYYY-MM-DD when the decision was last updated] <!-- optional -->\\nTechnical Story: [description | ticket/issue URL] <!-- optional -->\\n## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\n\n## Decision\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | ... | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, ...]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, ...]\\n* ...\\nWhat is the change that we're proposing and/or doing?\\nThe change that we're proposing or have agreed to implement.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\\nfor all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https://en.cppreference.com/w/cpp/io/ios_base/iostate\\n[bigtable-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/bigtable\\n[gcs-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/storage\\n[survey-link]: https://isocpp.org/blog/2018/03/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n\n## Decision\n", "response": "## Decision\nNone of our APIs will throw exceptions to indicate errors. Instead, our APIs will typically report errors to callers by returning a `Status` or `StatusOr<T>` object, unless the library we're using has another non-throwing way to report errors (e.g., [badbit][badbit-link] in the standard I/O library).\n\n**Consequences**: This decision will result in a single set of APIs and a consistent vocabulary for all users, whether or not they choose to compile with exceptions. This decision does not prevent callers from using exceptions in their own code. A downside of this decision is that our APIs will not be natural or idiomatic for the [50+%][survey-link] of users who might prefer exceptions for error reporting. Changing existing APIs from throwing exceptions to returning `StatusOr<T>` is a breaking change. As of this writing, this project has a [Google Cloud Storage][gcs-link] component that is at the Alpha quality level, and a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality level. We plan to immediately change the API for Google Cloud Storage. We have no immediate plans to change the API for Cloud Bigtable. We will communicate a timeline to change this API in a separate document.\n\n[badbit-link]: https://en.cppreference.com/w/cpp/io/ios_base/iostate  \n[bigtable-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/bigtable  \n[gcs-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/storage  \n[survey-link]: https://isocpp.org/blog/2018/03/results-summary-cpp-foundation-developer-survey-lite-2018-02  ", "matched_ids": [5061, 2462, 2469, 2466, 4800], "time": "0:00:12.115608", "input_tokens": 3299, "output_tokens": 359}
{"context": "## Context and Problem Statement\\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nGradle builds written in Kotlin are less esoteric comparing to Groovy ones.\\nIntellij IDEA has great support for Kotlin based builds.\\n\n## Decision\nWe will adopt Kotlin as a Gradle build's definition language.\\n\n\n## Context and Problem Statement\\nWhich build tool should be used?\\n\n## Decision\nChosen option: \"Maven\", because\\n* None of Gradle's customizability and the overhead in setup that comes with that is required.\\n* The structure the comes with Maven makes the build files easier to understand compared to ANT.\\n\n\n## Context\\nModern software needs to be easy to build.\\nFor this reason most software uses a build system to specify how the software is build and which dependencies it needs to work.\\nPopular build systems in the Java world are Ant, Maven and Gradle.\\nAnt is pretty flexible but lacks dependency management and is also very rarely used these days.\\nMaven is more rigid than Ant but supports dependency management and is still widely used.\\nGradle is the newest build system. It can be programmed in Groovy/Kotlin and is the most flexible build system.\\nGradle is also the only way to build for the Android platform.\\nGradle enjoys widespread usage and the authors are most proficient in it.\\n\n## Decision\nWe will use Gradle as build system.\\n\n\n## Problem\\nThe compilation variants of plugins blow up the number of plugins.\\nAdditionally there is the concept of default storage + resolver that is\\nneeded for bootstrapping plugins.\\n\n## Decision\nRejected: keep default plugins as-is\\n\n\n## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n\n## Decision\nWe will try to modularize the software and will use multiple projects to achieve this goal.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\\n\n## Decision\n", "response": "## Decision\nTo optimize build times, we will undertake the following measures:\n1. **Evaluate and Disable Unused Plugins:** Conduct an audit of all enabled plugins and remove any that are not essential to the current build process.\n2. **Refactor the Use of gradle.kts:** While Kotlin DSL provides better IDE support and type safety, we will evaluate and selectively refactor to Groovy DSL for build scripts that are identified as causing significant delays, subject to retaining the necessary functionality.\n3. **Optimize Module Configuration:** Investigate the feasibility of consolidating modules where possible or utilizing build optimizations like parallel builds and configuration avoidance techniques.\n4. **Ensure Efficient Build Caching:** Reassess and configure the build cache to ensure that cached results are reused effectively across modules.\nBy implementing these strategies, we aim to significantly reduce the overall build time.", "matched_ids": [1463, 4306, 2633, 1279, 2626], "time": "0:00:06.508414", "input_tokens": 574, "output_tokens": 171}
{"context": "## Context and Problem Statement\\nIn order to simplify the tracking consent build and deploy process and\\nmake integrating with tracking consent less surprising, should\\nthe configuration of the GTM container used by tracking consent be via\\ndata attributes rather than separate bundles?\\n## Decision Drivers\\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context and Problem Statement\\nTracking consent reads the PLAY_LANG cookie to determine whether to render the banner in Welsh. PLAY_LANG is the cookie\\nused by the Play Framework to persist a user's language preferences. It will be set to 'cy'\\nwhen a user has selected Welsh using the language toggle in MDTP services using the Play Framework.\\nTeams are increasingly setting PLAY_LANG to HttpOnly in an attempt to get green ZAP tests, even though there are no\\nknown security concerns around keeping PLAY_LANG as a normal cookie. Setting a cookie to\\nHttpOnly makes it unreadable within the client-side Javascript code that renders the tracking consent banner. The result\\nof this is that the banner will not be translated into Welsh for these services.\\nA related issue is that PLAY_LANG is not set for classic services written in Java, which means a Welsh version of the banner is not\\ncurrently available for classic services.\\nIt is worth noting that the only other known instance of our reading PLAY_LANG using Javascript is in the assets-frontend\\n[timeout dialog](https://github.com/hmrc/assets-frontend/blob/97c638289e23bee255ac30724a8572c6efa96817/assets/patterns/help-users-when-we-time-them-out-of-a-service/timeoutDialog.js#L14) timeout dialog. All the new govuk-frontend and hmrc-frontend components use data attributes instead.\\nShould we remove the reading of PLAY_LANG in tracking consent and accept a data-language attribute instead?\\n## Decision Drivers\\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\n\n## Decision\n* The need to support classic services\\n* The time-sensitive nature of this issue. The need to deploy quickly before too many services have integrated, to\\navoid services having to upgrade a second time.\\n* The preference for avoiding further changes to tracking consent. This is a non-breaking change.\\n* The preference for not advising teams to add exemptions to ZAP tests across MDTP\\n* The preference for consistency in language settings across our frontend components\\nChosen option: \"Re-work\" because we need to act now and in the medium term we are not in a position to uncouple services'\\ndependency on PLAY_LANG nor add a global exemption for PLAY_LANG into Zap tests. We also agreed that our frontend\\ncomponents should be consistent in their treatment of language until such time as we are able to provide an\\nalternative approach that works for all components.\\n### Positive Consequences\\n* Classic services or services using other non-Scala or non-Play frameworks can get Welsh translations for the banner\\n* Services do not need to set any HttpOnly exemptions in their ZAP tests, which may mask other legitimate warnings/errors\\n* Language setting is consistent with other components e.g. hmrcTimeoutDialog, hmrcReportATechnicalProblem etc\\n### Negative Consequences\\n* We will need to make a small change to tracking consent and communicate this change to teams\\n* Teams will need to upgrade to the latest version of play-ui/play-frontend-hmrc to get language translations\\n* Teams not using the tracking consent helpers will need to add a data-attribute for the language\\n\n\n## Context and Problem Statement\\nTracking consent stores cookie consent information in a userConsent cookie as URL-encoded JSON. This information is available\\nto frontend microservices in Javascript running in a user's browser and in the request headers passed to the Scala\\nPlay controllers running on the server-side.\\nBefore frontend services can interpret the cookie, they need to decode and parse it using a Scala JSON library. They also need\\nto apply some business logic to ensure in cases of uncertainty, the user is, by default, assumed to have rejected cookies.\\nFor Javascript code, we propose to make the existing UserPreferences object available in the window object. This will mean\\nservices integrating with tracking consent will have a method available to them for parsing the cookie using the latest\\nbusiness logic. Additionally, they will have a method for subscribing and being notified about changes to cookie preferences in real-time.\\nFor Scala code, the situation is more problematic. Services will only have the raw userConsent cookie itself passed down in the `Cookie` header.\\nTo avoid teams having to copy and paste substantial quantities of Scala boilerplate replicating the existing Javascript logic\\nacross services, should we look to centralise this logic so it can be more easily maintained across the platform and if so how?\\nIdeally we would like a Scala controller to be able to do something like:\\n```scala\\nprivate val themeCookie = Cookie(name = \"theme\", value = \"dark\", maxAge = Some(365.days.toSeconds.toInt))\\nprivate def allowedCookies(implicit request: RequestHeader) =\\nif (userPreferences.preferences.settings) Seq(themeCookie) else Seq.empty\\nval termsAndConditions: Action[AnyContent] = Action { implicit request =>\\nOk(termsAndConditionsPage()).withCookies(allowedCookies: _*)\\n}\\n```\\n## Decision Drivers\\n* the need to keep repetitious boilerplate to a minimum, with changes needed of the order of a single line code change.\\n* the need to centralise the maintenance of the cookie reading logic, so it is easier to roll out changes\\n* the importance of having loosely coupled microservices and avoidance of unnecessarily introducing additional hard\\nnetworked dependencies into microservices\\n* the need for simplicity and for the fewest number of moving parts\\n* the preference for not introducing a requirement for microservices to add additional libraries\\n\n## Decision\n* the need to keep repetitious boilerplate to a minimum, with changes needed of the order of a single line code change.\\n* the need to centralise the maintenance of the cookie reading logic, so it is easier to roll out changes\\n* the importance of having loosely coupled microservices and avoidance of unnecessarily introducing additional hard\\nnetworked dependencies into microservices\\n* the need for simplicity and for the fewest number of moving parts\\n* the preference for not introducing a requirement for microservices to add additional libraries\\nChosen option 3: because it meets most if not all the above criteria.\\n### Positive Consequences\\n* Services can determine acceptance or rejection of non-essential cookies in a single line of code.\\n* Changes to the cookie parsing logic can be rolled out relatively easily by #team-plat-ui\\n* Consuming services do not rely on any additional networked dependencies\\n* No additional libraries would be needed (assuming the micro-library is added to play-frontend-hmrc or bootstrap-play)\\n### Negative Consequences\\n* Any version change to the userConsent cookie has the potential to break functionality relying on non-essential cookies\\nin consuming services. We may need to look at introducing the ability to have the concept of minor versions or only increment the version for breaking\\nchanges e.g. removal or change to the meaning of a cookie type.\\n\n\n## Context and Problem Statement\\nNearly all services running on MDTP need to include a link to their landing page in the\\nGOV.UK header. Exceptionally, frontend microservices may either need to omit the service name\\nor be able to configure it based on the path. For example, in the case of shared frontend\\nmicroservices (contact-frontend, accessibility-statement-frontend) or microservices that host\\nmore than one public-facing service (view-external-guidance-frontend, gform-frontend).\\nWhen integrating with play-frontend-hmrc we want to\\nminimise boilerplate for service teams while providing flexibility to cope with edge\\ncases.\\nServices using the govukLayout component in play-frontend-govuk and not overriding the `headerBlock`\\nparameter, specify the service name in the\\n`service.name` message in `conf/messages`, providing any Welsh translation in `conf/messages.cy`.\\nBased on a Github search, most services overriding the headerBlock with, for example, `hmrcHeader`\\nor `govukHeader` are passing the service name from `messages('service.name')`. However, this\\nmechanism is awkward for services not requiring a service name \u2013 they have to override with a blank message \u2013\\nand services needing more than one are unable to use this functionality. This pattern\\nassumes a 1:1 correspondence between frontend microservices and public-facing\\ntax services.\\nShould we continue to support this pattern to reduce boilerplate for the majority of frontend\\nservices, insist on explicitly passing the service name or support some combination of the above?\\n## Decision Drivers\\n* Minimising of boilerplate and lines of code needed in frontend microservices\\n* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with\\npublic-facing services.\\n* The need to keep things simple and unsurprising\\n\n## Decision\n* Minimising of boilerplate and lines of code needed in frontend microservices\\n* Providing flexibility for frontend microservices that do not have a 1:1 correspondence with\\npublic-facing services.\\n* The need to keep things simple and unsurprising\\nChosen option 2 because reducing boilerplate for the majority of services is important, we should support\\nmore unusual services and not dictate an architecture that assumes a 1:1 correspondence\\nbetween frontend microservices and public-facing services.\\n\n\n## Context\\nThe issue motivatina# [short title of solved problem and solution]\\nWhat is the issue that we're seeing that is motivating this decision or change?\\n* Status: [proposed | rejected | accepted | deprecated | ... | superseded by [ADR-0005](0005-example.md)] <!-- optional -->\\n* Deciders: [list everyone involved in the decision] <!-- optional -->\\n* Date: [YYYY-MM-DD when the decision was last updated] <!-- optional -->\\nTechnical Story: [description | ticket/issue URL] <!-- optional -->\\n## Context and Problem Statement\\n[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]\\n## Decision Drivers <!-- optional -->\\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\n\n## Decision\n* [driver 1, e.g., a force, facing concern, ...]\\n* [driver 2, e.g., a force, facing concern, ...]\\n* ... <!-- numbers of drivers can vary -->\\nChosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | ... | comes out best (see below)].\\n### Positive Consequences <!-- optional -->\\n* [e.g., improvement of quality attribute satisfaction, follow-up decisions required, ...]\\n* ...\\n### Negative Consequences <!-- optional -->\\n* [e.g., compromising quality attribute, follow-up decisions required, ...]\\n* ...\\nWhat is the change that we're proposing and/or doing?\\nThe change that we're proposing or have agreed to implement.\\n\n\n## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n- onboarding a fresh team has consumed a lot of time and we want to improve this for the future as this service moves from a build phase into the support phase.\\n- 2 ways to start and run the application were documented, depending on your preference you would use the ones you were most comfortable with. This led to docker documentation that fell behind and didn't work out of the box\\n- when supported by the dxw support team, a different developer will be on hand each week to fix issues. In order to effectively and confidently apply fixes we want to standardise and test the set up process, removing as much manual process as possible and allowing the developer to address problems quickly.\\n- in ADR 26 we discussed moving from AWS ECS (containers) to GOV.UK PaaS (no containers). Whilst we no longer use containers in production and there aren't the same advantages to parity we would normally fine, we believe there is still good value to using it to make consistent development environments.\\n\n## Decision\nWe are going to use Docker in development exclusively for the frontend and the API.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\nIn order to simplify the tracking consent build and deploy process and\\nmake integrating with tracking consent less surprising, should\\nthe configuration of the GTM container used by tracking consent be via\\ndata attributes rather than separate bundles?\\n## Decision Drivers\\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\n\n## Decision\n", "response": "## Decision\n* The need to keep things simple for service developers\n* The need to improve the operability of tracking consent\n\nChosen option: \"Use data attributes for GTM container configuration,\" because it reduces complexity and potential confusion for developers, aligns with front-end standards, and improves the maintainability of the tracking consent integration by centralizing configuration.\n\n### Positive Consequences\n* Simplified integration process for service developers, reducing the learning curve and potential for errors.\n* Improvements in operability by centralizing configuration, which facilitates easier updates and maintenance.\n* Increased consistency in configuration management across different services using tracking consent.\n\n### Negative Consequences\n* Initial effort required to transition existing implementations from separate bundles to data attributes.\n* Potential need for additional documentation to guide developers in transitioning to and using the new configuration approach.", "matched_ids": [3526, 3523, 559, 4800, 2057], "time": "0:00:04.515362", "input_tokens": 2865, "output_tokens": 162}
{"context": "## Context\\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nES2016 introduced native support for the concept of modules. These are scoped files that expose some public functions. Modules are a way of organizing and sharing code.\\n\n## Decision\nWe will use ES2016 modules to organize and share code. More information can be found here: http://exploringjs.com/es6/ch_modules.html#sec_modules-in-javascript\\n\n\n## Context\\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\\n\n## Decision\nWe will use [Standard.rb](https://github.com/testdouble/standard) and run the standard.rb rake task to lint the code as part of the test suite.\\n\n\n## Context\\nWe need to make sure our code is written in a standard style for clarity, consistency across a project and to avoid back and forth between developers about code style.\\n\n## Decision\nWe will use [Standard.rb](https://github.com/testdouble/standard) and run the standard.rb rake task to lint the code as part of the test suite.\\n\n\n## Context\\nState handling in React becomes very complicated when you have too many states\\n\n## Decision\nWe implement Redux to handle state\\n\n\n## Context\\nWe must allow developers to use new JavaScript syntax and features without excluding older execution environments. Code must be automatically checked against common development mistakes and optimised for download and execution.\\n\n## Decision\nWe acknowledge that there is a plethora of Node.js tooling options available, however we have decided to use the [Google Closure Compiler]. It is developed and maintained by Google and is used for high traffic, complex and global applications such as Gmail and Google Maps. By adopting this tool we leverage decade of research and engineering in that field.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\\n\n## Decision\n", "response": "## Decision\nWe will adopt the use of arrow functions in our JavaScript codebase where applicable. Arrow functions provide a concise syntax, improve readability by reducing boilerplate, and have lexical scoping of the 'this' keyword, which helps prevent common errors. More information can be found in the MDN documentation: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Functions/Arrow_functions.", "matched_ids": [4295, 3386, 2382, 1854, 2687], "time": "0:00:02.743193", "input_tokens": 495, "output_tokens": 83}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n", "response": "## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: [Documenting Architecture Decisions](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).", "matched_ids": [4699, 4428, 4408, 3119, 1436], "time": "0:00:02.724438", "input_tokens": 410, "output_tokens": 48}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n", "response": "## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n", "matched_ids": [4699, 4428, 4408, 3119, 1436], "time": "0:00:03.132517", "input_tokens": 410, "output_tokens": 42}
{"context": "## Context\\nI need to implement two special pages, the privacy policy page and the about me page.\\nThey can't be in the map, I don't want them there because the user would\\nbe forced to read them when scanning the spiral.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nRaster Foundry's web presence needs a landing page for customer acquisition and a blog for curating content relevant to the project's objectives. Given that we are still early in the product development process, there is a desire to keep solutions to the landing page and blog simple, but also flexible so that they can easily be iterated on by all parties involved.\\n\n## Decision\nBuilding upon the experience gained by maintaining products like Cicero, HunchLab, and OpenTreeMap, the following decisions have been made regarding Raster Foundry\u2019s domain layout:\\n- Canonical domain is `www.rasterfoundry.com`\\n- Web application resides on `app.rasterfoundry.com`\\n- Blog resides on `blog.rasterfoundry.com`\\nAs far as the platforms that drive the landing page and blog, the former will be a static website built using [Jekyll](https://jekyllrb.com/) and hosted via [GitHub Pages](https://pages.github.com/) (with support from Amazon CloudFront for HTTPS) and the latter will be based on [Medium](https://medium.com/).\\n\n\n## Context\\nA way to document and track my design choices is needed, for myself in order to see how project architecture is changing.\\n\n## Decision\nI will use ADR to document any important architectural decisions I make.\\nADRs will be checked into the repository as numbered md files in the folder docs/architecture/adr\\nI will follow the template described [here](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions)\\nAll Records will contain, Title, Context, Decision, Status and Consequences\\nIf a decision is changed or overruled we don't delete the record, but change the status accordingly (to superseded or deprecated).\\nIf a decision is superseded or deprecated we should add a link to the new decision. In the format Superseded by [link]\\n\n\n## Context\\nThe POC assumes generation of the table of contents based on the content presented on the page. Data need to be fetched from somewhere, and then presented on the view in some way. Data model won't be simple data structure, as it can be multi-leveled list, which might also contain some behavior, to make it easier to prepare for rendering. It's not going to be simple CRUD application, even if model will not contain much behavior.\\n\n## Decision\nUse ports and adapters architecture style.\\n\n\n## Context\\nCertain elements on the Talk To Frank site need to be loaded regardless of the user entry point,\\ne.g. the current user story involves adding a warning message bar that appears on all pages.\\nThe CMS needs to be updated to support the addition of site wide 'settings'.\\n\n## Decision\nA new 'site settings' content model will be created.  A 'Sitewide' item will then be created to contain all sitewide settings.\\nThis would then allow for the following (future) setup,\\nContent items of type 'site settings'\\n- Sitewide\\n- Homepage - any settings that should override the sitewide values on the homepage.\\n- Drugs A-Z - any settings that should override the sitewide values on the Drugs A-Z page.\\nReferences:\\n- https://www.contentful.com/r/knowledgebase/dynamic-microcopy/\\n- https://www.contentfulcommunity.com/t/is-it-common-to-have-a-settings-content-model/924/2\\n\n\n## Context\\nEvery environment the application is available in, be that development, review,\\nstaging or public are openly available to anybody. There is no access control\\nat all.  This isn't suitable for a site that could be misinterpreted as the\\n'real' version either by people or by search engines and other bots. There\\nneeds to be some form of hurdle to overcome in order to prompt people to\\nconsider whether this is the correct site. There should also be a barrier to\\nprevent access by bots.\\n\n## Decision\nWe have decided to use basic authentication on the service in all environments\\napart from the public facing one. This will only be applicable to the\\nenvironments hosted within the Rancher environment. The username and password\\nwill not be secret and will be included within the `README` of the application.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nI need to implement two special pages, the privacy policy page and the about me page.\\nThey can't be in the map, I don't want them there because the user would\\nbe forced to read them when scanning the spiral.\\n\n## Decision\n", "response": "## Decision\n\nThe privacy policy page and the about me page will be implemented as standalone static pages. These pages will be accessible via the main navigation menu so that users can find them easily without being forced to interact with them during their automated navigation through the site. Additionally, these pages will not be included in the site's content map or sitemap used for scans or any sequential automation processes. This ensures that users have control over when they engage with these sections, and can choose to access them at their convenience. Furthermore, these static pages will have clear links in the website's footer section to provide consistent accessibility from all other pages.", "matched_ids": [1692, 706, 1883, 1894, 2207], "time": "0:00:04.333908", "input_tokens": 1065, "output_tokens": 124}
{"context": "## Context\\nThe current request builder APIs are not able to handle some odata requests like:\\n- query navigation properties `GET /People('scottketchum')/Friends`\\n- getting \"raw value\" of a property `/People('scottketchum')/$value`\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nIn the request builder you can currently call methods like `filter` and `top` multiple times.\\nIn this document we discuss possibilities to unify this the behavior when this is done.\\n\n## Decision\nWe decided to use option `A` and `B` depending on the methods.\\nFor the OData related options we will use option `A`.\\nFor configuration related builder methods we will use the verbs `add` or `set` to make clear if it is option `A` or `B`.\\nThe relevant builder methods (the one containing arguments) are listed in the table below including the decision on the option:\\n| class                           | methods                                                                                                                                                                              |\\n| ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| request-builder-base            | withCustomHeader ( refactor to addCustomHeaders B)<br>withCustomQueryParameters (refactor to addCustomQueryParameters B)<br>withCustomServicePath (A rename to setCustomServicePath) |\\n| get-all-request-builder-base    | select (A) <br>orderBy (A) <br>top (A) <br>skip (A)                                                                                                                                  |\\n| get-all-request-builder-v2/v4   | filter (A)<br>expand(v4)                                                                                                                                                             |\\n| get-by-key-request-builder-base | select (A refactoring move to base)                                                                                                                                                  |\\n| get-by-key-request-builder-v4   | expand(A)                                                                                                                                                                            |\\n| delete-request-builder-v2/v4    | setVersionIdentifier (A)                                                                                                                                                             |\\n| update-request-builder-base     | requiredFields (refactor setRequiredFields A)<br>ignoredFields (refactor setRequiredFields A)<br>withCustomVersionIdentifier (refactor align with setVersionIdentifier A)            |\\n\n\n## Context\\nWe want to allow partial or merge updates of things with a single request.\\n\n## Decision\nA merge request\\n* uses HTTP `PATCH` method.\\n* has payload in _JSON merge patch_ format defined in [RFC-7396](https://tools.ietf.org/html/rfc7396).\\n* has the request header `content-type` set to `application/merge-patch+json`.\\n\n\n## Context\\nAs we want to provide bi-directional capabilities for bindings to allow for cases such as getting a blob from a storage account,\\nAn API change is needed to account for the requested type of operation.\\n\n## Decision\n### Naming\\nIt was decided to keep the bindings name as is. Alternative proposals were included changing bindings to connectors, but a strong case couldn't be made in favor of connectors to justify the breaking change it would cause.\\n### Types\\nIt was decided to keep the same YAML format for both input bindings and bi-directional bindings as it is today.\\nAfter careful inspection, splitting to two types (for example, trigger bindings and bindings) would incur significant maintanace overhead for the app operator and\\nDid not provide meaningful value.\\nIn addition, there was no feedback from community or prospecive users that input bindings and output bindings were confusing in any way.\\n### API structure\\nIt was decided that the API url will be kept as: `http://localhost:<port>/v1.0/bindings/<name>`.\\nThe verb for the HTTP API will remain POST/PUT, and the type of operation will be part of a versioned, structured schema for bindings.\\nThis is not a breaking change.\\n### Schema and versioning\\nIn accordance with our decision to work towards enterprise versioning, it was accepted that schemas will include a `version` field in\\nThe payload to specify which version of given component needs to be used that corresponds to the given payload.\\nIn addition, an extra field will be added to denote the type of operation that binding supports, for example: `get`, `list`, `create` etc.\\nBindings components will provide the means for the Dapr runtime to query for their supported capabilities and return a validaton error if the operation type is not supported.\\n\n\n## Context\\n* We are building a web app that has one official url that uses query params\\n* The url is /search?q=search-term\\n* We believe that future endpoints will be added, or more complicated search requests\\n\n## Decision\n* We will use Bidi to parse the routes.\\n\n\n## Context\\nOur application should handle 2 types of requests - reading and writing. </br>\\nFor now, it looks like:</br>\\n- for reading, we need data model in relational form to return data in tabular/flattened way (tables, lists, dictionaries).\\n- for writing, we need to have a graph of objects to perform more sophisticated work like validations, business rules checks, calculations.\\n\n## Decision\nWe applied the CQRS architectural style/pattern for each business module. Each module will have a separate model for reading and writing. For now, it will be the simplest CQRS implementation when the read model is immediate consistent. This kind of separation is useful even in simple modules like User Access.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nThe current request builder APIs are not able to handle some odata requests like:\\n- query navigation properties `GET /People('scottketchum')/Friends`\\n- getting \"raw value\" of a property `/People('scottketchum')/$value`\\n\n## Decision\n", "response": "## Decision\nTo accommodate the necessary OData requests, we have decided to extend the request builder APIs with two new features:\n1. **Navigation Property Querying:** Introduce a new method `navigateTo(propertyName)` which will allow querying of navigation properties directly. This method will append the specified navigation property to the request path, enabling requests like `GET /People('scottketchum')/Friends`.\n   \n2. **Raw Value Retrieval:** Implement a method called `getRawValue()` that can be appended to a request for retrieving the raw value of a property. This method will facilitate requests such as `/People('scottketchum')/$value` by appending `/$value` to the query path, allowing access to the raw data directly.\n\nBoth methods will ensure that the syntax and semantics align with current OData standards, providing seamless and natural query formation for developers using the API. Additionally, thorough documentation will be included to cover these new functionalities and provide examples for developers.", "matched_ids": [3620, 779, 75, 1384, 888], "time": "0:00:06.098199", "input_tokens": 1257, "output_tokens": 201}
{"context": "## Context\\nFrequency Capping allows Content Managers to limit the number of\\nimpressions or interactions users have with content. Is a widely\\navailable tool in Publishing Platforms.\\nIt's usually developed on the server side where the system can decide\\nhow many times to serve the content to the requesting users which we\\ncall \"Global Frequency Capping\". Additionally the system may be able\\nto limit the number of impressions per user which we call \"Local\" or\\n\"User Frequency Capping\".\\nFor example a Content Piece can be set to 1,000,000 Global Impressions\\nand 1 Impression per User, thus indirectly driving 1,000,000 different\\nusers to this Content.\\nThis functionality has been lacking from the Snippet Service due to\\ntechnical limitations imposed by the way metrics were collected and\\ncontent selection was handled on the client side. The latest\\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\\nPipeline unblock this capability. [0]\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nData Engineers are building performance dashboards for Snippets using the\\nmetrics we collect from Firefox Telemetry. Telemetry pings include only basic\\ninformation about the Snippet, like Snippet ID.\\nFor better to understand and more complete dashboards, we want to enhance the\\nTelemetry received information with more Snippet metadata, like campaign\\ninformation, included URL, main message used and others.\\nTo achieve this we will export the metadata from the Snippets Service in a\\nmachine readable format and make the file available in a Cloud Storage Provider.\\nThen Data Engineers will import the metadata and combine them Telemetry data in\\nunified dashboards.\\nGitHub Issue: [#887](https://github.com/mozmeao/snippets-service/issues/887)\\n\n## Decision\n- Export in CSV format.\\n- Create a cron job to export and upload resulting file to S3.\\n- The job will run daily, on early morning UTC hours.\\n- The job will be monitored using Dead Man's Snitch and report to the usual\\nnotification channels that project developers follow.\\n\n\n## Context\\nOur current method for implementing multi-variant tests involves frequent, often non-trivial code changes to our most high traffic download pages. Prioritizing and running concurrent experiments on such pages is also often complex, increasing the risk of accidental breakage and making longer-term changes harder to roll out. Our current tool, [Traffic Cop](https://github.com/mozmeao/trafficcop/), also requires significant custom code to accomodate these types of situations. Accurately measuring and reporting on the outcome of experiments is also a time consuming step of the process for our data science team, often requiring custom instrumentation and analysis.\\nWe would like to make our end-to-end experimentation process faster, with increased capacity, whilst also minimizing the performance impact and volume of code churn related to experiments running on our most important web pages.\\n\n## Decision\nWe will use [Cloudflare Workers](https://www.cloudflare.com/en-gb/products/cloudflare-workers/) to redirect a small percentage of traffic to standalone, experimental versions of our download pages. The worker code will live in the [www-workers](https://github.com/mozmeao/www-workers) repository. We will implement a ([vetted and approved](https://bugzilla.mozilla.org/show_bug.cgi?id=1565012)) third-party experimentation tool called [Convert](https://www.convert.com/) for use on those experimental pages.\\n\n\n## Context\\nWe need to capture information about user behaviors and task completion on the website. We should use previously approved systems for capturing this information. This excludes services that capture and track web analytics in external systems, e.g. Google Analytics.\\n\n## Decision\nWe will use New Relic to capture and track web analytics. New Relic provides [browser monitoring](https://docs.newrelic.com/docs/browser/) that can capture essential metrics such as page views, and offers both [agent and SPA API](https://docs.newrelic.com/docs/browser/new-relic-browser/browser-agent-spa-api/) and [APM](https://developer.newrelic.com/collect-data/custom-attributes) methods for capturing custom data.\\n\n\n## Context\\nWe have an established aim of rendering content in Dotcom Rendering. This will dramatically improve the developer experience (measured in time to new features, time on maintenance).\\nAMP is a good candidate for early migration to the new platform:\\n- It is simpler than \u2018full\u2019 content types\\n- It is a discrete piece of functionality\\n- It receives a large volume of traffic, which will help validate the performance of our new platform\\n- It receives relatively little development outside of the Dotcom team, reducing risk during the migration period\\nThere are also some downsides. In particular:\\n- It is relatively \u2018static\u2019 (rarely developed on) so the developer experience benefits are likely less than for some non-AMP content, such as articles or liveblogs\\nIt is quite possible though that Google will weight AMP more heavily going forward, pushing us to develop the platform more than we have done so in the past.\\n\n## Decision\nTo implement AMP using dotcom rendering.\\n\n\n## Context\\nRendering apps track pageviews and events to help us measure the performance\\nof GOV.UK.\\nStatic uses an [abstraction layer](https://github.com/alphagov/govuk_frontend_toolkit/blob/master/docs/analytics.md) in GOV.UK Frontend Toolkit, which reports the data to Google Analytics (but the underlying provider could be changed).\\nFor analysing search performance, we want to be able to reliably track clickthrough rates for search result links.\\n[Enhanced E-Commerce](https://developers.google.com/analytics/devguides/collection/analyticsjs/enhanced-ecommerce) is a plugin for google analytics that can track impressions as well as clicks. We model each search result page as a \"product list\", and each content item as a \"product\".\\n\n## Decision\nWe added a plugin for E-Commerce to Static, so we could try using it\\nto measure search performance.\\nWe didn't add an abstraction layer to GOV.UK Frontend Toolkit, because we don't\\nknow if there is a wider need for it, and we wanted to validate our use case with GOV.UK data. The code could be moved to the Frontend Toolkit at a later date if necessary.\\nWe're sending the search query as a custom dimension. This is a workaround: without the custom dimension, Enhanced-Ecommerce\\ncan infer it from the path, but only for impression data. We couldn't get it to set the query for the click data.\\nFor each \"product\" we send the id, position, and list ('site search result'). To avoid unnecessary data transfer when users are interacting with the page, we use Query Time Imports to add\\nmetadata to products later.\\nWe use publishing platform `content_id` as the unique id where available, and use the path as a fallback.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nFrequency Capping allows Content Managers to limit the number of\\nimpressions or interactions users have with content. Is a widely\\navailable tool in Publishing Platforms.\\nIt's usually developed on the server side where the system can decide\\nhow many times to serve the content to the requesting users which we\\ncall \"Global Frequency Capping\". Additionally the system may be able\\nto limit the number of impressions per user which we call \"Local\" or\\n\"User Frequency Capping\".\\nFor example a Content Piece can be set to 1,000,000 Global Impressions\\nand 1 Impression per User, thus indirectly driving 1,000,000 different\\nusers to this Content.\\nThis functionality has been lacking from the Snippet Service due to\\ntechnical limitations imposed by the way metrics were collected and\\ncontent selection was handled on the client side. The latest\\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\\nPipeline unblock this capability. [0]\\n\n## Decision\n", "response": "## Decision\n\n- Implement Global Frequency Capping and User Frequency Capping on the server side of the Snippet Service.\n- Utilize the latest capabilities offered by the Firefox Messaging Center and Firefox Telemetry Pipeline for accurate tracking of impressions and interactions.\n- Develop a new service component that manages frequency capping logic and user state, leveraging existing telemetry data.\n- Ensure that the solution is scalable to handle the specified global cap (e.g., 1,000,000 impressions).\n- Design the system to track and update user-specific impression data efficiently, maintaining minimal latency.\n- Implement a caching layer to minimize the impact on performance and ensure real-time capabilities for impression tracking.\n- Provide an administrative interface for content managers to configure and monitor frequency capping settings for each content piece.\n- Develop reporting tools to provide insights into the performance and reach of frequency-capped content, utilizing the enhanced metrics collected.", "matched_ids": [3230, 3997, 1165, 2656, 1361], "time": "0:00:06.667728", "input_tokens": 1717, "output_tokens": 177}
{"context": "## Context\\nThe application is about finding services closest to the search point. All\\nservices have a co-ordinate in lat/lon format. Currently the application\\nrequests either an out-code or a postcode as the search point. The submitted\\npostcode needs to be resolved to a lat/lon that can be used to query against.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nThe application has been using https://postcodes.io/. Postcodes.io is available\\nfor free and does not require any relationship between consumer and supplier.\\nOne of the drawbacks to this is that although support is available when using\\nthe free tier, there is no provision for any specific support relationship.\\nMore formal relationships are available but this is through the paid product\\nhttps://ideal-postcodes.co.uk/.  Given the number of requests and the fact the\\nfree tier was supplying the information required, it had previously been\\ndecided there was no need to migrate to a paid product. However, the strategic\\ndirection has been set such that high value APIs, usable across many products\\nshould be supplied in-house. To that end, there is now an in-house postcode\\nsearch available.\\n\n## Decision\nThe decision is to replace the use of postcodes.io with the in-house postcode\\nlookup, as per the strategic direction.\\n\n\n## Context\\nThe move to Azure search has introduced the need to calculate the distance\\nbetween the search point and each result item. Previously, when using\\nElasticsearch, the distance was returned within the query response. Azure\\nsearch does not have this capability, it is currently a\\n[feature request](https://feedback.azure.com/forums/263029-azure-search/suggestions/17760211-support-geo-distance-in-select-result).\\n\n## Decision\nThe decision is to calculate the distance between the search point and each\\nresult item within the consuming application i.e. the web app. The calculation\\nfor\\n[great-circle distance](https://en.wikipedia.org/wiki/Great-circle_distance)\\nis well known and available in numerous languages.\\n\n\n## Context\\nThe primary (only) consuming application for this API needs to show both open\\nand nearby services on separate pages (and more of them). Previously the\\napplication had shown a mix of open and nearby services within a\\nsingle page.\\nHaving the API so closely aligned to the needs of the consumer is not ideal.\\nThere is scope to increase the flexibility of the API along with increasing the\\nease with which it can be used both by the current and future consumers.\\n\n## Decision\nThe decision is to add a new endpoint i.e. `/open` alongside the current\\n`/nearby` endpoint. The former endpoint will return only services that are open\\nwhere the latter will be refactored to return only services that are nearby\\nregardless of their opening state.\\n\n\n## Context\\nFollowing user research a more comprehensive text search engine was needed in order to be able to\\nuse more complex searches (and/or), full phrase search, stemming as well as geolocation searches.\\n\n## Decision\nA review of ElasticSearch and Funnelback was undertaken [here](https://docs.google.com/document/d/10xK0on4gzrB1ImZHk3Z_PoCr7V_aUpiieuLdQDg4OwQ/edit)\\nand [ElasticSearch](https://www.elastic.co/products/elasticsearch) was chosen as a candidate due to the fact that it was\\nopen source and had comprehensive documentation with an established community around it.\\n(Funnelback)[https://www.funnelback.com/] was discounted due to it being closed-source and for\\nit's lack of community support.\\nA  Docker image of GP data ([profiles-db-elastic](https://hub.docker.com/r/nhsuk/profiles-db-elastic/))\\nrunning an ElasticSearch instance has been created for use by this application and possibly by other applications.\\n\n\n## Context\\n* We are building a web app that has one official url that uses query params\\n* The url is /search?q=search-term\\n* We believe that future endpoints will be added, or more complicated search requests\\n\n## Decision\n* We will use Bidi to parse the routes.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nThe application is about finding services closest to the search point. All\\nservices have a co-ordinate in lat/lon format. Currently the application\\nrequests either an out-code or a postcode as the search point. The submitted\\npostcode needs to be resolved to a lat/lon that can be used to query against.\\n\n## Decision\n", "response": "The decision is to implement a geocoding service that converts postcodes into latitude and longitude coordinates. This will involve integrating a third-party geocoding API such as Google Maps Geocoding API or leveraging an in-house service if available. The resolved coordinates will then be used to query the database for services closest to the search point. This ensures a streamlined process for retrieving and displaying location-based services to users.", "matched_ids": [4276, 4274, 482, 3472, 1384], "time": "0:00:04.629852", "input_tokens": 1016, "output_tokens": 82}
{"context": "## Context or problem to solve\\n* How should git histories look like?\\n* What format should we use to write commit messages?\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nThe point of the tool is to enforce good commit hygiene, which is easy to compromise while trying to just get good code out. We should pick a standard that seems like it's popular or otherwise conventional, and not overly strict for flexibility.\\n\n## Decision\nUse [Conventional Commits v1.0.0-beta.4]. It's similar enough to [Semantic Commits] and its inspiration [Angular Commits], but loose enough and working on being an independent standard.\\n\n\n## Context\\nThe value of tidy and [semi-linear commit history](https://fangpenlin.com/images/2013-09-30-keep-a-readable-git-history/source_tree_new_branch_rebase_merge.png) is often overlooked in many Git-based\\nprojects. This is unfortunate since non-linear git commit history might be a [horrible mess](https://tugberkugurlu.blob.core.windows.net/bloggyimages/d773c1fe-4db8-4d2f-a994-c60f3f8cb6f0.png) that\\ndoes not provide any useful information. We want to use as simple as possible git workflow that promotes and ensures a semi-linear history.\\n> * **Semi-linear** commit history usually refers to a history that uses merge commits (git \"no-fast-forward\" merge option) to clearly denote which commits are meant to be together and represent a\\n>   coherent whole.\\n> * **Linear** commit history usually refers to completely flat history (git default \"fast-forward\") where it is impossible to tell at first glance which commits belong together.\\nWhen working on individual features, related git commits can be organized either as \"work log\" or as a \"recipe\". When working in a team, it is crucial that team members and/or reviewers can easily\\ncomprehend what is going on in a particular feature. For this reason, we prefer features to be organized as \"recipes\".\\n> * **Work log** style of organizing feature commits refers to the style without any organization. Commits are added solely as they are developed through time.\\n> * **Recipe** style of organizing feature commits refers to the style where commits have a sensible organization where peer developers can clearly see and learn how the feature is created. This\\n>   style requires some additional work as its primary goal is communication, instead of just implementing a feature.\\nVery often, in bigger teams, common git workflows have a problem of broken continuous integration builds. We want to embrace and use as simple as possible workflow that resolves that problem.\\nChosen git workflow should seamlessly support release versioning and, if needed, related work on release branches.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n## Decision\n**We will use a [stable mainline branching model for Git](https://www.bitsnbites.eu/a-stable-mainline-branching-model-for-git/).** It\\n[supports semi-linear Git history](https://www.bitsnbites.eu/a-tidy-linear-git-history/) and helps to resolve the problem of broken continuous integration builds.\\n**We will, however, introduce several tweaks to the \"stable mainline branching model\":**\\n* We will use the following naming pattern for feature branches: **`feature_<name>`** instead of `feature/name`.\\n* We will not use new branches when remote feature branches need rebasing. Instead, we will just inform all collaborators that rebasing is pending for a feature branch. It is important\\nto communicate with collaborators **before** rebasing and force-pushing.\\n* We will not create a release branch for each release. In general, we will just tag a release. However, we will create a release branch when a particular release needs fixing.\\nWe will organize our [feature commits as recipes](https://www.bitsnbites.eu/git-history-work-log-vs-recipe/) because we want to promote the team's learning and communication.\\nWe will also use specific commit message format as described in [ADR-0014 - Commit Message Format](./0014-commit-message-format.md)\\n\n\n## Context\\nUsing commit messages without any structure looks convenient for developers as they do not have to think about messages too much. Unfortunately, that freedom and lack of thinking can impose some\\nadditional burden on long-term project maintenance.\\nQuite often, we can find incomprehensible commit messages that do not communicate anything useful. Hopefully, imposing some lightweight rules and guidance will help developers create commit messages\\nthat are helpful for their colleagues.\\nIn addition, with unstructured commit messages, there is much less opportunity to introduce any tools on top of commit history. For example, we would like to employ an automated changelog generator\\nbased on extracting some semantical meaning from commits, but this will not work if commit messages lack any structure. Without the commit message structure, we can just dump the commit log in the\\nchangelog, which does not make the changelog more helpful than looking at the history of commits in the first place.\\n### Architectural Context\\n* System (`klokwrk-project`)\\n\n## Decision\n**We will use a customized [conventional commits](https://www.conventionalcommits.org/en/v1.0.0/) format for writing commit messages.**\\nConventional commits format is nice and short and defines the simple structure that is easy to learn and follow. Here is basic structure of our customized conventional commits format:\\n<type>(optional <scope>): <description> {optional <metadata>}\\nOur customization:\\n- defines additional message types as an extension of [types defined by the Angular team](https://github.com/angular/angular/blob/22b96b9/CONTRIBUTING.md#-commit-message-guidelines)\\n- allows adding additional metadata in the message title if useful and appropriate (see details section for more info)\\n- requires format compliance only for messages of \"significant\" commits (see details section for more info)\\n### Decision details\\nDetails about the decision are given mainly as a set of strong recommendations (strongly recommended) and rules enforced by tooling (rule). In our case, the tooling is implemented as git commit hooks.\\nEvery contributor should install git hooks provided in this repository. That can be done with following command (executed from the project root):\\ngit config core.hooksPath support/git/hooks\\nThere might be cases when implemented rules are not appropriate and should be updated or removed or just temporarily ignored. In such scenarios, hooks can be skipped with git's `--no-verify` option.\\nWhile describing details, following terms are used as described:\\n- *commit message title*: refers to the first line of a commit message\\n- *commit message description*: refers to the part of the title describing a commit with human-readable message. In conventional commits specification that part is called `description`.\\n#### General guidance and rules for all commit messages\\n- (strongly recommended) - avoid trivial commit messages titles or descriptions\\n- (strongly recommended) - use imperative mood in title or description (add instead of adding or added, update instead of updating or updated etc.) as you are spelling out a command\\n- (rule) - message title or description must start with the uppercase letter <br/>\\n<br/>\\nThe main reason is a desire for better readability as we want easily spot the beginning message description or title. There are some arguments for using the lowercase like \"message titles are not\\nsentences\". While this is true, we prefer to have better readability than comply with some vague constraints.<br/>\\n<br/>\\n- (rule) - message title or description should not end with common punctuation characters: `.!?`\\n- (strongly recommended) - message title or description should not be comprised of multiple sentences\\n- (rule) - message title should not be longer than 120 characters. Use the message body if more space for description is needed<br/>\\n<br/>\\nActually, there is a common convention that we should not use more than 69 characters in the message title. It looks like the main reason for it is that GitHub truncates anything above 69 chars\\nfrom message titles. Having such a tight constraint seems unreasonable today, and the apparent shortcomings of any tool shouldn't restrict us, even if the tool is GitHub.<br/>\\n<br/>\\n- (strongly recommended) - commit message title or description should describe \"what\" (and sometimes \"why\"), instead of \"how\"<br/>\\n<br/>\\nFor describing \"why\", the message body is more appropriate as we have more space there. If needed, the message body may contain \"how\" too, but it should be clearly separated (at least with a blank\\nline) from \"what\" and \"why\".<br/>\\n<br/>\\n- (recommended) - commit message title should provide optional scope (from conventional commit specification) if applicable\\n- if commit refers to multiple scopes, scopes should be separated with `/` character\\n- if commit refers to the work which influences the whole project, the scope should be `project` or it can be left out\\n- the scope should be a single word in lowercase<br/>\\n<br/>\\n- (strongly recommended) - message body must be separated from message title with a single blank line\\n- (option) - message body can contain additional blank lines\\n- (recommended) - message body should not use lines longer than 150 characters\\n- (strongly recommended) - include relevant references to issues or pull request to the metadata section of message title<br/>\\n<br/>\\nExample: `feat(some-module): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}`<br/>\\n<br/>\\n- (option) - include relevant feature/bug ticket links in message footer according to conventional commits guidelines<br/>\\n<br/>\\nFooter is separated from body with a single blank line.\\n#### Guidance and rules for \"normal\" commits to the main development branch\\n- (rule) - all commits to the main development branch must have a message title in customized conventional commit format\\n#### Guidance and rules for merge commits to the main development branch\\nWhen used with [semi-linear commit history](./0007-git-workflow-with-linear-history.md), merge commits are the primary carriers of completed work units. As such, they are the most interesting for\\ncreating a changelog.\\nBefore merging, merge commits must be rebased against main development branch, and merging must be executed with no-fast-forward option (`--no-ff`).\\n- (rule) - merge commits must have 'merge' metadata (`{m}`) present at the end of the title <br/>\\n<br/>\\nThat way, merge commits can be easily distinguished on GitHub and in the changelog.\\n- (option) - merge commit metadata can carry additional information related to the issues and PRs references like in the following example\\nfeat(klokwrk-tool-gradle-source-repack): Adding a new feature {m, fixes i#123, pr#13, related to i#333, resolves i#444}\\nHere, `i#123` is a reference to the issue, while `pr#12` is a reference to the pull request. Additional metadata are not controlled or enforced by git hooks.\\n#### Guidance and rules for normal commits to the feature branches\\n- (option) - normal commits don't have to follow custom conventional commits format for message title\\n- (strongly recommended) - normal commits should use conventional commits format when contained change is significant enough on its own to be placed in the changelog\\nWhen all useful changelog entries are contained in normal commits of a feature branch, we can do two different things depending on the situation:\\n- use merge commit with type of `notype`. Such merge commit will be ignored when creating a changelog.\\n- merge a branch with fast-forward option (no merge commit will be present)\\nPreferably, use `notype` merge commits, as they are still useful for clear separation of related work.\\n#### Types for conventional commits format\\n- common (angular)\\n- `feat` or `feature` - a new feature\\n- `fix` - a bug fix\\n- `docs` - documentation only changes\\n- `style` - changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)\\n- `test` - adding missing tests or correcting existing tests\\n- `build` - changes that affect the build system or external dependencies\\n- `ci` - changes to our CI configuration files and scripts\\n- `refactor` - a code change that neither fixes a bug nor adds a feature\\n- `perf` - a code change that improves performance\\n- `chore` - routine task\\n- custom\\n- `enhance` or `enhancement` - improvements to the existing features\\n- `deps` - dependencies updates (use instead of `build` when commit only updates dependencies)<br/>\\n<br/>\\nThere are two main scenarios when upgrading dependencies, a simple version bump and the more involved upgrade requiring resolving various issues like compilation errors, API upgrades, etc.<br/>\\n<br/>\\nSimple version bumps should be contained in a single individual commit with a description message starting with the word \"Bump\". For example: `deps: Bump Micronaut to 2.5.2 version`.<br/>\\n<br/>\\nMore complicated upgrades should be organized as feature branches where each non-conventional commit resolves a single step in the process. When finished, the feature branch should be merged\\ninto the main development branch with a description starting with the word \"Upgrade\". For example: `deps: Upgrade Spring Boot to 2.5.0 version {m}`.<br/>\\n<br/>\\n- `task` - same meaning as `chore`. Prefer using `task`.\\n- `article` - use instead of `docs` when changes are related only to articles\\n- `misc` - anything that does not fit into previous categories\\n- `notype` - only for merge commits in situations where contained plain commits carries all relevant types and merge commit doesn't add anything useful. It is ignored in changelog.<br/>\\n<br/>\\nTypical example is when we are updating number of dependencies through commits in a feature branch. In this situation each feature branch commit should contain updates to a single dependency\\nand communicate the update through its own commit message in conventional commit format. When all updates in the branch are tested, we can merge them in the main branch. However, that merge\\nshould be ignored by changelog as it does not communicate anything new.\\n\n\n## Context\\nWe need a strategy for how to git and name branches\\n\n## Decision\nTasks can be found in [Projects](https://github.com/kits-ab/LogLady/projects/1).\\nAssign one to yourself then create branch from develop and name it like this: feature/whetever-you-are-doing-#corresponding-task-ID\\nExample: `feature/branchName-#5`\\n**Note:** _feature_ is an example, it could be:\\n- feature\\n- hotfix\\n- release\\nCommit messages should start with task ID.\\nExample: `#5 update README. Explained naming.`\\nPush when done. Make pull request where you add task ID to beginning of comment.\\n\n\n## Context\\nWe use git and GitHub in different ways, it\u2019s emergency to teach team members with the git basics and style guide we will use.\\n\n## Decision\n### git basics\\n1. setup ssh keys ([https://github.com/settings/keys][1]) or GUI;\\n2. clone repo into local system: `git clone git@github.com:huifenqi/django-project-skeleton.git`;\\n3. files store in three stages:\\n![][image-1]\\n1. `Working Directory`: holds the actual files;\\n2. `Index`: staging area which store your changed files;\\n3. `HEAD`: points to the last commit you've made.\\n4. `Working Directory` -\\> `Index`: `git add <filename>` or `git add *`;\\n5. `Index` -\\> `HEAD`: `git commit -m \"Commit message\"`;\\n6. push updates to Github: `git push origin master`;\\n7. create branch: `git checkout -b feature/x`\\n![][image-2]\\n8. push branch to Github and others can see: `git push origin <branch_name>`;\\n9. sync local with Github: `git pull`;\\n10. make a new tag: `git tag <tag_name>`;\\n11. show history: `git log`;\\n12. show changes: `git diff <previous_commit_hash>`;\\n13. others: `git status`, `git branch`, `git tag`, etc.\\n### git style guide\\n#### Branches\\n* Choose short and descriptive names: [https://github.com/agis-/git-style-guide#branches][2];\\n* Use dashes to separate words.\\n#### Commits\\n* Each commit should be a single logical change. Don't make several logical changes in one commit;\\n#### Messages\\n* when writing a commit message, think about what you would need to know if you run across the commit in a year from now.\\n### Refs\\n* [http://rogerdudler.github.io/git-guide/index.html][3]\\n* [https://confluence.atlassian.com/bitbucketserver/basic-git-commands-776639767.html][4]\\n* [https://github.com/agis-/git-style-guide][5]\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context or problem to solve\\n* How should git histories look like?\\n* What format should we use to write commit messages?\\n\n## Decision\n", "response": "## Decision\n\n**For Git Histories: We will use a semi-linear commit history model.**  \nA semi-linear history model will allow us to maintain a clear, organized, and easily readable history. We will achieve this by using the no-fast-forward merge option and ensuring that each feature or bug fix is developed on a separate branch. This helps team members understand the context of the changes and allows for easy identification of the complete set of changes related to particular features or fixes.\n\nKey practices:\n- Use feature branches for developing features or fixing bugs.\n- Regularly rebase feature branches on the main development branch to minimize merge conflicts.\n- Ensure all merges to the main branch are done using the --no-ff option to maintain a meteorological log of changes.\n\n**For Commit Messages: We will adhere to the Conventional Commits specification.**  \nThe Conventional Commits specification provides a simple yet flexible convention for creating an explicit commit history; this makes scaling to larger projects easy.\n\nCommit message structure:\n- `<type>(optional <scope>): <description>`\n- Descriptive examples:\n  - `fix(parser): handle unexpected input`\n  - `feat(ui): add new login modal`\n  - `docs: update README with new installation instructions`\n\nAdditional guidelines:\n- Use imperative mood in the description (e.g., \"fix\" instead of \"fixed\" or \"fixes\").\n- Limit the subject line to 72 characters.\n- Separate subject from body with a blank line if a detailed body is required.\n- Provide context, motivation, and any relevant information in the body.\n- Relate commits to issues by including issue identifiers in commit messages, e.g., `feat(account): integrate payment API #234`.\n- Use specific types like `feat`, `fix`, `docs`, `style`, `refactor`, `test`, `chore`, with custom extensions where necessary.\n\nBy adhering to these standards for both Git histories and commit messages, we aim for improved readability, traceability, and enhanced collaboration within the team.", "matched_ids": [969, 4899, 4890, 1851, 2424], "time": "0:00:11.210814", "input_tokens": 3845, "output_tokens": 406}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as [described by Michael\\nNygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\\n\n\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe need to record the architectural decisions made on this project.\\n\n## Decision\n", "response": "## Decision\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\\n", "matched_ids": [1539, 2576, 867, 463, 4823], "time": "0:00:02.735455", "input_tokens": 411, "output_tokens": 42}
{"context": "## Context\\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe need to choose main programming language for this project.\\n\n## Decision\nI'm now fluent in writing Python and JavaScript. Both languages are widely used\\nthese days and may attract developers to join this project.\\nPython is more mature with more stable libs for writing server side code and\\ntesting. JS may have better libraries for writing GraphQL APIs because\\nGraphQL originated in JS ecosystem.\\nI'm doing this in free time and for fun. I want to focus on project's features\\nand not to waste a time with poorly designed language lacking mature and\\nstable libraries. So for obvious reasons I'm choosing Python.\\n\n\n## Context\\nAn application has to written in some programming languge.\\n\n## Decision\nThe following languages were considered for this application:\\n* Java\\n* Python\\n* Ruby\\nThe initial inclination was to use either Java or Ruby, based solely on\\ndeveloper experience.\\nAfter discussions with Ben Wallberg, it was decided to use Python because:\\n* The application is fairly simple, an unlikely to reach a complexity where\\nJava's more explicit typing and compile-time verification is needed.\\n* We have a number of developers familiar with Python, providing a broader\\nbase from which to draw support.\\n* While we are content with using Ruby (specifically Ruby on Rails) for website\\nprojects, this is a command-line application. We have fewer developers\\nfamiliar with Ruby.\\n\n\n## Context\\nAs it is monolith, only one language (or platform) must be selected for implementation.\\n\n## Decision\nI decided to use:\\n- .NET Core platform - it is new generation multi-platform, fully supported by Microsoft and open-source community, optimized and designed to replace old .NET Framework\\n- C# language - most popuplar language in .NET ecosystem, I have 12 years commercial experience\\n- F# will not be used, I don't have commercial experience with it\\n\n\n## Context\\nIn terms of the application implementation, the programming language and run time environment are important decisions. Significant factors include the general availability of developers, the level of support available in the community, and resources for learning, developing, and maintaining an application in that language.\\n\n## Decision\nJavaScript is, at least according to the Stack Overflow Developer Surveys, far and away a very popular choice of programming language. This is due in large part to the proliferation of web browsers, which natively run JavaScript, but also can be credited to Node.js, which makes writing backend systems almost as easy as writing the frontend client in the browser.\\nApplications running on Node are sufficiently fast, especially compared to Python or Ruby, and JavaScript is a small and predictable language when compared to PHP. There are multiple OIDC and SAML libraries for Node to choose from. In general the Node ecosystem is huge, so finding libraries with permissive licenses is very easy. Deploying to a variety of systems is well supported.\\nAs such, this application will be written in **JavaScript** and the run time environment will be **Node.js**.\\n\n\n## Context\\nI need to decide in which language I implement the frontend.\\n\n## Decision\nI use Javascript.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\\n\n## Decision\n", "response": "## Decision\nAfter evaluating several languages based on factors such as developer availability, library support, performance, and ease of integration, we have decided to choose **Python** as our base programming language. \n\nThe reasons for selecting Python are as follows:\n- **Developer Familiarity**: Python is widely taught and used in educational settings, resulting in a large pool of developers from which to recruit.\n- **Rich Ecosystem**: Python has a rich ecosystem of libraries and frameworks, such as Django and Flask for web development, and Pandas and NumPy for data analysis, which will help us rapidly build and scale functionalities.\n- **Versatility**: Its versatility allows Python to be utilized across different domains such as web development, data science, automation, and more.\n- **Community Support**: There is excellent community support, making it easier to troubleshoot and find solutions to potential problems.\n\nOther languages can be considered on a per-need basis, provided there is substantial justification such as specific library or performance requirements that align better with another language's strengths.", "matched_ids": [472, 3510, 886, 2024, 3715], "time": "0:00:03.882252", "input_tokens": 802, "output_tokens": 210}
{"context": "## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nWe are currently using Docker as our container runtime.\\nThere are needs for public docker images:\\n* so that master builds can be easily pulled and tested in the local development environments\\n* so that images can be easily shared between different teams\\nThere are needs for digitally verifying the provenance of images:\\n* so that production systems can trust that an image has not been tampered with\\n* so that production systems can authenticate the origin of a build\\nThere are needs for vulnerability scanning:\\n* so that production systems can warn or prevent exploitable software running in production\\nThe docker ecosystem provides tooling that can help us meet these needs:\\n* [Docker Content Trust] (Notary) can be used to sign images and prove provenance\\n* [Docker Registries][Docker Registry] can expose images publicly\\n* Scanning tools like [Clair] can periodically or at pull/push time perform CVE scanning.\\nUnfortunately AWS [ECR] does not currently support public images or [Docker Content Trust], and there is no managed solution to image scanning from AWS as yet.\\n### Option 1: Wrap AWS ECR\\nWe could write/manage a proxy to allow exposing [ECR] publicly and integrate the missing features.\\n* Potentially fragile implementation tied to the underlying AWS services\\n* Another thing to have to maintain\\n* Might offer ability to remove functionality as AWS support more features in future\\n### Option 2: Use an external SaaS offering\\nUse a SaaS service like [Quay] which offers most of these features.\\n* Additional configuration for cluster to pull from external source\\n* Reduces ability to automate provisioning (requires additional credential management)\\n* Procurement\\n### Option 3: Self-hosted Docker Tools in cluster\\nWe could deploy Docker Distribution, Notary & Clair into the cluster backed by a managed storage backend like S3\\n* Well integrated with the platform\\n* Would work for local GSP instance\\n\n## Decision\nWe will run a self hosted set of Docker tools\\n\n\n## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we should standardize on a base Docker Image for Java-based services that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use.\\nWhile there are a few popular choices and de facto standards such as OpenJDK, we need to consider the support and legal implications of that choice, particularly since we could be considered as \u2018distributing\u2019 all layers in that image, including the OS and Java binaries.\\nWe could potentially workaround being the distributors of sensitive layers using techniques like init containers, composite containers, or foreign layers.\\n\n## Decision\nWe will start from the official CentOS 7 Docker image and add the Oracle server JRE binaries for the first iteration of the Alfresco base Java image.\\nThis allows us to start from a popular OS and Java runtime which are already in the supported ACS and APS stacks and QuickStart AMIs, and closer to the most popular customer OS, RHEL, than others.  We also have precedence in distributing the Oracle JRE in our current installer.\\nThe CentOS image also contains very few known vulnerabilities which we can easily patch at the moment.\\nWhile Alpine is an attractive choice due to its simplicity, it may be more difficult to debug and support, and the majority of our components are not yet architected to take advantage of the smaller size and lower resource consumption.\\nWhile distribution workarounds mentioned above were considered, it was felt that they are not common practice and are likely to produce more problems than they solve.\\nThe summary [![report of the comparison](https://img.shields.io/badge/report%20of%20the%20comparison-PRIVATE-red.svg)](https://ts.alfresco.com/share/s/bqDcnHWpSrSGybJhMxf93A) contains more details.\\nThe base image Dockerfile is [here](https://github.com/Alfresco/alfresco-docker-base-java/blob/master/Dockerfile).\\n\n\n## Context\\nOur CI/CD pipeline uses containers (Docker) as does our hosting platform (GOVUK\\nPaaS), we need a way to store built images from our pipeline so our hosting\\nplatform can access and deploy them.\\nDocker hub is one solution offered by the maker of Docker itself.\\n\n## Decision\nUse [Docker hub](https://hub.docker.com/) to store built deployment container\\nimages to facilitate continuous delivery.\\nHost the built container images on the dxw Docker hub account.\\n\n\n## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to decide on a Helm Chart Repository that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Helm Charts. When we looked for a solution, we considered that this is going to be a public-facing customer repository. We also took into consideration the need for a custom domain for it.\\nThe criterias we looked for when we investigated each option are: if it uses AWS and Quay, if it is a PaaS/Cloud Storage solution, if the project is mature enough for our use case, if it is a personal project or if it is maintained by a company and if it requires IT support (if it does can it be avoided by altering the pipeline).  We considered that authentication and SSL is not needed.\\nWe had several options that we considered. The full list of pros and cons for each option can be found [here](https://issues.alfresco.com/jira/secure/attachment/97743/DEPLOY-150%20Helm%20Chart%20Repos.xlsx). More comments on the investigation we have done can be found [here](https://issues.alfresco.com/jira/browse/DEPLOY-150).\\n\n## Decision\nWe will use Github Pages to store the Helm Charts. The reasons why we elected this solution are: it is a cloud storage solution, it is mature project that has been actively mantained, doesn't require IT support and it offers an easy, well-known pipeline.\\n\n\n## Context\\nOne of the main advantages of Docker images is their immutability. This means once an image has been tested and verified there is a high confidence it will work as intended in other environments as it doesn't change.\\nAlfresco allows the core product to the enhanced via external modules in the form of [AMPs](https://docs.alfresco.com/5.2/concepts/dev-extensions-packaging-techniques-amps.html) or [simple JARs](https://docs.alfresco.com/5.2/concepts/dev-extensions-packaging-techniques-jar-files.html).\\nThis results in two big problems, how do we release containers with every combination of AMP available and how do customers apply their own extensions?\\nWe have three options; apply extensions at build time (thus retaining the immutability advantage), apply extensions at runtime using a mechanism that doesn't change the contents of the container or apply extensions as the container initializes (breaking immutability).\\nApplying extensions at build time means we will be forcing customers to build their own images depending on which official and custom extensions they require.\\nHowever, there are a number of disadvantages of applying extensions at runtime; The immutability advantage is lost (the WAR file is changed), an extension could fail to apply and prevent the container from starting, it has the potential to introduce performance issues as the AMP or JAR has to be fetched and applied and finally it opens a potential security hole as code can be applied to a container via a simple environment variable.\\nWe investigated potential approaches to keep the main container immutable by using volumes and initContainers but this increases the complexity of the solution and doesn't resolve all the issues outlined above.\\n\n## Decision\nWe will proceed with the build time option as there are a number of disadvantages to applying extensions at runtime, customers are already used to applying their own extensions and they're having to learn a new deployment mechanism anyway.\\nWe will release a small number of images with and without common AMPs applied.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\\n\n## Decision\n", "response": "## Decision\nWe will adopt a managed Docker registry service, specifically Amazon Elastic Container Registry (ECR). This decision is based on several key factors:\n\n1. **Managed Service**: Amazon ECR is a fully managed container image registry that handles infrastructure scaling, maintenance, and upgrades, reducing the burden on our limited IT resources.\n\n2. **Access Control**: It provides robust integration with AWS Identity and Access Management (IAM), enabling us to define fine-grained access control policies.\n\n3. **Security Features**: ECR includes image vulnerability scanning, ensuring that images are scanned for security issues, which addresses one of our primary concerns.\n\n4. **Scalability and Performance**: As an AWS service, ECR offers high scalability and global availability, ensuring consistent performance irrespective of our team's geographical distribution.\n\n5. **Usability and Integration**: ECR integrates seamlessly with AWS, including existing AWS services that we're likely using, such as AWS CodeBuild, CodePipeline, and Elastic Kubernetes Service (EKS).\n\nBy utilizing Amazon ECR, we align with our objective of minimizing additional workload for IT while meeting our requirements for access control, security, and scalability.", "matched_ids": [3917, 3603, 2392, 3602, 3604], "time": "0:00:04.948182", "input_tokens": 2015, "output_tokens": 234}
{"context": "## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context and Problem Statement\\n* The current API implementations is spread across multiple modules (such as webapp, config, db and query) without a formal definition of the inputs/outputs. This induces inconsistencies and we foresee technical difficulties to maintain and evole the service.\\n* We would like to use an interface description language to define the API between the various component (such as between the webapi and webclients, worker and database).\\n* How do we define and maintain the interfaces?\\n\n## Decision\nChosen option: \"Protobuf\", because it comes out best (see below).\\n### Positive Consequences\\n- We need to start defining interfaces outside of their implementations.\\n- We need to use a simpler HTTP api (e.g. the path for the action function name and the body for its input), instead of encoding the inputs with a mix of path, querystring and body components.\\nFor example, use `POST /get_projects BODY { \"index\": \"name\" }` instead of `GET /projects?index=name`.\\n### Negative Consequences\\n- The team needs to learn a new language.\\n- Follow-up decisions required for the actual implementation.\\n\n\n## Context\\nOffer an API without a good documentation is not a choice. Document this API with text documents is not efficient, and as the code changes more and more effort are necessary to keep it up-to-date. It's necessary to document APIs in a simply and efficient way, preferably in a way that developers can test it.\\n\n## Decision\nI decided to use Swagger as a documentation tool for the APIs, as it is a de facto standard. I choosed springfox-swagger2 and springfox-swagger-ui because of its smooth integration with Spring Boot.\\n\n\n## Context\\nThe client and the service provider will have to communicate using some API.\\nWe need to decide how the requests and responses will look like.\\n\n## Decision\nWe will use swagger to document the API between the client and the service-provider. This will form part of the documentation of a strawman that we send to our users.\\n\n\n## Context\\nA consistent and well-defined document specification is required so that we may develop an API contract\\n\n## Decision\nOur structure closely follows the [JSON-API](https://jsonapi.org/format/#document-structure) document structure\\n### Document Root Level\\nAt the root level is always a JSON object.\\nThis **MUST** contain at least one of the following root-level members:\\n* data: the document's \"primary data\" resource object\\n* A single resource object is represented by a JSON object\\n* A collection or resource objects is represented by an array of objects\\n* errors: an array of error objects\\n#### Single resource object\\n```json\\n{\\n\"data\": {},\\n\"meta\": {}\\n}\\n```\\n#### Collection of resource objects\\n```json\\n{\\n\"data\": [\\n{...},\\n{...}\\n],\\n\"errors\": [],\\n\"meta\": {},\\n\"links\": {}\\n}\\n```\\nJSON-API states\\n> The members data and errors MUST NOT coexist in the same document.\\n`opg-data` standard is that if a data resource OBJECT is returned, then there **MUST be no** error member.\\nHowever there ARE certain circumstances where an array of errors **MAY** be returned alongside a data resource COLLECTION. See [0007-error-handling-and-status-codes.md#errors-in-20x](0007-error-handling-and-status-codes.md#errors-in-20x)\\nThe root JSON object **MAY** also contain the following root-level members:\\n* meta: a meta object that contains non-standard meta-information\\n* links: a links object related to the primary data (typically used for pagination links if the data returned is a collection)\\n### The Resource Object\\nSee [JSON-AP](https://jsonapi.org/format/#document-resource-objects)\\nNamely:\\n* As a minimum, every resource object **MUST** contain:\\n* an id member\\n* a type member. The values of the id and type members **MUST** be strings. For consistency and the avoidance of confusion, types **MUST** use PLURAL. eg \"articles\", \"people\"\\n* an array of attributes (even if empty)\\n* IF the \"data\" top level member is present, then there **MUST** be a `links` array, containing as it's minimum, a `self` member with a URL which **MUST** be callable at the API\\n* A resource object's data attributes is presented in an array named \"attributes\"\\n* A resource object's links is presented in an object named \"links\"\\n* A resource object's relationships is presented in an array named \"relationships\"\\n* A resource object's attributes and its relationships are collectively called its \"fields\"\\n```json\\n{\\n\"data\": [\\n{\\n\"type\": \"articles\",\\n\"id\": \"1\",\\n\"attributes\": {\\n\"title\": \"My First Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https://api.example.com/articles/1\",\\n\"next\": \"https://api.example.com/articles/2\"\\n},\\n\"relationships\": {\\n...\\n}\\n},\\n{\\n\"type\": \"articles\",\\n\"id\": \"2\",\\n\"attributes\": {\\n\"title\": \"Second Article\",\\n\"description\": \"...\"\\n},\\n\"links\": {\\n\"self\": \"https://api.example.com/articles/2\",\\n\"prev\": \"https://api.example.com/articles/1\"\\n},\\n\"relationships\": {\\n...\\n}\\n}\\n],\\n\"links\": {},\\n\"meta\": {}\\n}\\n```\\n#### Relationships\\nRelationships will be defined in a separate ADR\\n### Meta\\nSee [JSON-API](https://jsonapi.org/format/#document-meta)\\n### Errors\\nErrors responses will be defined in a separate ADR\\n\n\n## Context\\nWe need to define the API specifications of the services we're going to implement.\\n\n## Decision\nWe use the [OpenAPI 2.0](https://swagger.io/specification/) specification (aka Swagger spec) as standard for our REST API definitions.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n\n## Decision\n", "response": "## Decision\nWe will use the [OpenAPI 3.0](https://swagger.io/specification/) specification as it provides both human and machine-readable documentation, facilitating the automatic generation of client services. \n\n### Positive Consequences\n- Provides a comprehensive and widely accepted standard for describing REST APIs.\n- Enables the use of tools such as Swagger UI to generate human-readable documentation and Swagger Codegen or OpenAPI Generator for automatic client code generation.\n- Streamlines both the development and maintenance of client services by reducing manual effort.\n\n### Negative Consequences\n- Requires learning and adhering to the OpenAPI specification format.\n- Initial setup might take time to define the API contracts accurately.\n\n### Rationale\nThe OpenAPI 3.0 specification supports the requirements for both documentation readability and client auto-generation, providing a productive balance between effort and output quality.", "matched_ids": [4644, 3069, 4439, 2181, 1180], "time": "0:00:04.736804", "input_tokens": 1512, "output_tokens": 169}
{"context": "## Context and Problem Statement\\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\\nand was only noticed some days after when the Project Manager needed to\\nexport the data. Upon investigation, it was discovered that:\\n* Error logging was inactive on the application. (Now fixed)\\n* We can\u2019t debug using the responses our system returned to PayPal as\\nwe don\u2019t have access to the IPN log.\\nThis led to a situation where we couldn't get the information required\\nto debug the error. It was suggested we queue all incoming requests from\\nPayPal on our own system for processing by our system.\\n## Decision Drivers\\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\n1. We have a fixed deadline\\n1. We will have our first users week beginning 7 June\u00a02021\\n1. For our Delius integration, we rely on an intricate set of reference data\\n1. We are uncertain if there are any issues with the reference data for all possible combinations of offenders\\n1. We rely on these integrations to set up \"container\" data in nDelius. If they fail, we need to recover before providers book appointments\\nWe want to ensure when updates fail, we have a way to resume that retries the interaction.\\nHowever, **we do not have retry mechanisms in place yet**.\\nWe could add an admin task/script to retry a certain event (we use Spring Application Events that don't\\ncreate side effects, so we could retrigger those).\\nWe feel this would create an overhead that would be too much administrative burden together with\\nthe anticipated noise of most users starting the service at the same time.\\n\n## Decision\nDue to the uncertainties and lack of convenient retry mechanism,\\nwe favour **consistency over availabilty** in the short term.\\n\n\n## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n\n## Decision\nUse Rollbar to collect and manage our application errors.\\n\n\n## Context\\nWe need a mechanism to capture and manage application errors. Without an integration our debugging options are to access a console on live environments and try to replicate (something we want to minimise) or by looking through information provided by logs.\\nWe have used Rollbar for a few years now and we have not reviewed this decision since. It is currently being used for 14 applications.\\nFor some projects we use their technical tooling of choice to aid in the transition to business as usual. Due to this we will have a handful of projects using Sentry and possible others.\\nSometimes Rollbar environment names don't match the Rails environment. Dalmatian-<project> and paas-<project> both exist. There also exists both permutations for the same project as we transition. We have used ROLLBAR_ENV to manage this before so making it explicit will hopefully make it clearer how it can be changed.\\n\n## Decision\nUse Rollbar to collect and manage our application errors.\\n\n\n## Context\\nWe adopted the `PublicActivity` gem (as documented in [0019](https://github.com/UKGovernmentBEIS/beis-report-official-development-assistance/blob/731b91f5773c4306f919433af81976d29c5feef1/doc/architecture/decisions/0019-use-public-activity-gem.md)), but the information we record has never been used, or displayed to the user.\\nNow we're looking at surfacing information about changes to end users it's becoming increasingly clear that is `PublicActivity` is not fit for our needs, and would be easier to build our own approach to logging what changed were made by what user.\\n\n## Decision\nWe have decided to introduce a `HistoricalEvent` model to record changes to models, starting with `Activity`s.\\nOnce we have applied `HistoricalEvents` to all the models we need to track, we can remove all the calls to `PublicActivity` and remove the gem entirely.\\n\n\n## Context and Problem Statement\\nA solution for isolating third-party payment widgets from the rest of account\\nmanagement can include building a separate web app on its own dedicated domain.\\nAs a side effect in building that app, we have the opportunity to choose\\ntechnologies for building it that don't necessarily follow the rest of FxA.\\n## Decision Drivers\\n* Opportunity for a fresh start with tech stack without rewriting\\n* Security in dealing with payment transactions\\n* Developer ergonomics\\n* Code quality & testing\\n* Subscription services deadlines\\n\n## Decision\n* Opportunity for a fresh start with tech stack without rewriting\\n* Security in dealing with payment transactions\\n* Developer ergonomics\\n* Code quality & testing\\n* Subscription services deadlines\\nChosen option: \"React, Redux, Typescript\", because\\n* Chance to start with a fresh stack\\n* More vibrant ecosystem\\n* Better tooling & developer ergonomics\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context and Problem Statement\\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\\nand was only noticed some days after when the Project Manager needed to\\nexport the data. Upon investigation, it was discovered that:\\n* Error logging was inactive on the application. (Now fixed)\\n* We can\u2019t debug using the responses our system returned to PayPal as\\nwe don\u2019t have access to the IPN log.\\nThis led to a situation where we couldn't get the information required\\nto debug the error. It was suggested we queue all incoming requests from\\nPayPal on our own system for processing by our system.\\n## Decision Drivers\\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\n\n## Decision\n", "response": "## Decision\nImplement a queuing mechanism to store all incoming PayPal IPN requests on our system before processing them. \n\nRationale:\n- **Transparency**: By queuing incoming requests, we can maintain a complete record of all IPNs received, enabling better debugging and troubleshooting when errors occur, as we would have a history of all events.\n- **Automation**: This queue allows the system to automatically retry processing of previously failed IPNs once the error is fixed, minimizing manual intervention and reducing the risk of missing any important notifications.\n- This approach ensures that we are more resilient to failures and can handle PayPal IPN requests without the need for them to be refired, leading to more reliable fundraising operations.", "matched_ids": [1799, 3388, 2387, 2403, 372], "time": "0:00:04.956092", "input_tokens": 1488, "output_tokens": 144}
{"context": "## Context\\nWe need to deploy the Broker somewhere.\\nThe Broker implements the service broker API to generate per-application syslog\\ndrain URLs (Adapter URLs).\\nThe Adapter is written in Go.\\nThe Broker is written in Go.\\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\\n", "fewshot": "You are an expert software architect who is tasked with making decisions for Architectural Decision Records (ADRs). You will be given a context and you need to provide a decision. Here are some examples:\n\n## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n\n## Decision\nThe GOV.UK PaaS team decided that they were happy to take ownership of the broker, because it requires knowledge of the platform to maintain, and they maintain all the other brokers on the platform.\\n\n\n## Context\\nWe need a way to deploy our application to AWS.\\n\n## Decision\nWe will use [Terraform] to deploy our application. More specifically,\\nwe will use [Chalice] to create the build artifact for the source\\ncode (.zip) and then use [Terraform] to manage the infrastructure\\nresources.\\n\ud83d\udc49 **Note:** Ideally, we would publish the build artifacts to S3 before\\ndeployment. That way the deployment process could happen independently\\nof the build process.\\n\ud83d\udc49 **Note:** To begin with, we'll just convert the JSON data created\\nby [Chalice] to [HCL] and add any other resources needed. Going forward,\\nwe will use modules to create reusable components.\\n\n\n## Context\\nWe want to provide a reliable method of shipping logs from applications (on\\nGOV.UK Paas) to Splunk so they can take advantage of the log storage, analytics\\nand protective monitoring provided by the GDS Cyber Security team.\\nGDS Cyber Security maintain an [AWS Kinesis][kinesis] based log shipping stream\\nthat accepts log events in the [AWS CloudWatch Logs][cloudwatch] format and\\nqueues them for delivery to Splunk.\\nGOV.UK PaaS supports forwarding log events from an application's stdout and\\nstderr streams in [syslog format][syslog] via [syslog drains][drains].\\nComponents such as [Fluentd][fluentd] are available that process and forwarding\\nlogs from various sources to various targets, but their configuration can\\nunweildly and hard to test.\\nGOV.UK PaaS Tenants can run [sidecar][sidecar] containers to handle custom log\\nshipping or instument their applications with [logging libraries that support\\nmultiple transports][winston], but this brings extra complexity and doesn't\\nmake use of supported logging infrastructure already provided by GOV.UK PaaS.\\n\n## Decision\nWe will build an \"adapter\" application to deploy alongside the CSLS logging\\npipeline that accepts requests in the \"syslog over http\" format exported by the\\nGOV.UK PaaS syslog drain system, translates them into the CloudWatch Logs\\nformat and forwards them on to the CSLS Kinesis stream.\\n\n\n## Context\\nWe need to provide tenants with the ability to provision databases for use in\\ntheir applications. Our first iteration of this will be using RDS.\\nWe investigated some implementations of a service broker which supported RDS\\n- [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\\n- [18F](https://github.com/18F/rds-service-broker)\\n\n## Decision\nWe will use the [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\\nrds broker. As this is not a supported product, we will fork this and maintain\\nthis and implement new features ourselves.\\n\n\n## Context\\nWe need a way of deploying our services to our various hosted environments (dev, staging, prod, etc.). We would\\nprefer these deployments to be automated with minimal to zero human interaction.\\n\n## Decision\nThe DIT infrastructure already has a lot of tooling and infrastructure around deployments so we will utilise this. We\\nwill use Jenkins to automatically deploy from dedicated git branches - these will be:\\n- development\\n- staging\\n- uat\\nWe will have 5 Jenkins jobs in total - these will be:\\n- trade-access-program-backoffice\\n- trade-access-program-frontend\\n- trade-access-program-polling-dev\\n- trade-access-program-polling-staging\\n- trade-access-program-polling-uat\\nThe role of the \"polling\" jobs are to watch a related git branch for any commit changes. Once a change is detected then\\nit will trigger the `trade-access-program-backoffice` and `trade-access-program-frontend` jobs with a set of\\nenvironment parameters triggering the deployment to one of our three environments.\\nThis allows us to simply merge or push to one of the three dedicated git branches above and a full automated deployment\\nwill occur for that environment.\\n\n\nMake sure to give decisions that are similar to the ones above.\nNow provide a decision according to the context given below:\n## Context\\nWe need to deploy the Broker somewhere.\\nThe Broker implements the service broker API to generate per-application syslog\\ndrain URLs (Adapter URLs).\\nThe Adapter is written in Go.\\nThe Broker is written in Go.\\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\\n\n## Decision\n", "response": "## Decision\nWe will deploy the Broker as a standalone application on AWS Fargate. AWS Fargate provides a serverless compute engine for containers, which allows us to manage and deploy the Broker container without additional overhead of provisioning or managing EC2 instances. \n\nThis deployment strategy aligns well with the existing infrastructure, as our Adapter runs on AWS Lambda, ensuring the Broker can easily interact with the Adapter through AWS services. \n\nWe will set up a continuous integration and deployment pipeline that mirrors our Adapter pipeline, ensuring that any changes to the Broker are automatically built, tested, and deployed to AWS Fargate. This approach optimizes resource usage and allows us to scale the Broker effortlessly according to demand.", "matched_ids": [3291, 4358, 3290, 186, 5013], "time": "0:00:04.130236", "input_tokens": 1247, "output_tokens": 140}
