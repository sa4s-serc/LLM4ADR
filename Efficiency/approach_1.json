{"context": "## Context and Problem Statement\\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\\n## Decision Drivers <!-- optional -->\\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\n", "response": "## Decision\n\nWe will use optimistic locking based on entity versions/timestamps for concurrency control.  This approach strikes a balance between performance and implementation complexity. Each entity will have a version field (either a numeric version or a timestamp).  When an entity is updated, the version is checked against the version in the database. If the versions match, the update proceeds, and the version is incremented/updated. If the versions don't match, a concurrency exception is thrown, signaling to the user that their data is stale and they need to refresh and retry their operation.\n", "time": "0:00:04.887409", "input_tokens": 115, "output_tokens": 116}
{"context": "## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https://www.postgresql.org/docs/9.5/static/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https://www.postgresql.org/docs/9.5/static/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n", "response": "## Decision\n\nImplement a permissions model based on database schemas. Each application binding will receive its own dedicated schema within the shared RDS database instance.  The application will be granted full ownership and control over all objects (tables, functions, sequences, etc.) within its designated schema. This approach avoids the complexities of shared table ownership and provides isolation between different applications, preventing unintended modifications.  Migrations will operate solely within the application's own schema, eliminating conflicts.  This also provides a clearer path for future multi-tenancy models and simplifies permission management.\n", "time": "0:00:03.097897", "input_tokens": 509, "output_tokens": 111}
{"context": "## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n", "response": "## Decision\n\nA Docker container running PostgreSQL will be integrated into the Service Tests workflow.  This container will be automatically started before the tests and stopped after the tests are complete.  We will leverage Docker Compose to manage the lifecycle of the PostgreSQL container.\n", "time": "0:00:04.291088", "input_tokens": 61, "output_tokens": 51}
{"context": "## Context\\nWe need a system to manage the lifecycle of a grant application.\\n", "response": "## Decision\n\nWe will develop a custom web application to manage the grant application lifecycle.  This application will handle all stages from initial submission through review, award, reporting, and closeout.\n", "time": "0:00:01.997717", "input_tokens": 44, "output_tokens": 39}
{"context": "## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n", "response": "## Decision\n\nAdopt the `ProofOperator` interface for handling Merkle proofs in `abci.ResponseQuery`.  This replaces the current use of raw `[]byte` and provides a more flexible and extensible approach to proof construction and verification.  This will allow for easier handling of multilayer proofs and custom proof formats by delegating proof-specific logic to implementations of the `ProofOperator` interface.\n", "time": "0:00:02.589755", "input_tokens": 128, "output_tokens": 81}
{"context": "## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n", "response": "## Decision\n\nWe will utilize Azure Active Directory (Azure AD) as the identity provider for the frontend application, enabling single sign-on (SSO) for both backstage/SEPA users and integration with Dynamics 365 and other Azure Cloud Services. This leverages SEPA's existing Active Directory infrastructure and promotes a unified identity management system across the platform.\n", "time": "0:00:02.268752", "input_tokens": 122, "output_tokens": 74}
{"context": "## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n", "response": "## Decision\n\nWe will adopt a multi-layered approach to code clarity and understanding, encompassing the following:\n\n1. **Code Level:**  Enforce coding style guidelines using a linter and formatter (e.g., ESLint, Prettier, Black) to ensure consistency and readability.  Encourage meaningful variable and function names, and promote the use of comments to explain complex logic.  Functions and methods should be kept concise and focused on a single purpose.\n\n2. **Module/Component Level:**  Modules and components should be designed with clear interfaces and well-defined responsibilities.  Documentation should be provided using docstrings or a dedicated documentation generator (e.g., JSDoc, Sphinx) to outline the purpose, usage, and dependencies of each module/component.\n\n3. **Application Level:**  Architecture diagrams and design documents will be created and maintained to provide a high-level overview of the application's structure and functionality.  These documents should be easily accessible to all developers.  README files should be included in each application's repository, providing clear instructions on how to build, run, and test the application.\n\n4. **System/Group of Applications Level:**  A service catalog or similar documentation will be maintained to outline the different applications within the system, their interactions, and their dependencies.  This will provide a comprehensive view of the entire system and how each application fits into the bigger picture.\n\n5. **Ongoing Education and Review:** Regular code reviews will be conducted to ensure adherence to these guidelines and to facilitate knowledge sharing among team members.  Training sessions or workshops may be organized to introduce new tools and best practices for code clarity.\n\nThis multi-layered approach will help ensure that code is understandable at all levels, promoting maintainability, collaboration, and reducing the cost of onboarding new developers.\n", "time": "0:00:09.826833", "input_tokens": 88, "output_tokens": 370}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "response": "## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture important architectural decisions along with their context, consequences, and status.  These ADRs will be stored in a dedicated `docs/adr` directory within the project's version control system (e.g., Git) using Markdown format.  The ADRs will follow a consistent template (e.g., MADR).  This will allow us to track the evolution of the architecture and understand the rationale behind specific choices.\n", "time": "0:00:56.310733", "input_tokens": 43, "output_tokens": 104}
{"context": "## Context\\nThe rig defined at [Bookit Infrastructure](https://github.com/buildit/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\\nThe only way to capture that is via branches which can be hard to discover.\\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\\n", "response": "## Decision\n\nCreate a \"rig-definitions\" repository to manage and version different rig configurations. This repository will house a collection of reusable, composable modules representing various infrastructure components (e.g., compute, database, networking).  Specific rig instances, like the Bookit Infrastructure rig, will be defined as configurations that assemble these modules.  Versioning will be managed through tags and releases, allowing easy instantiation of rigs with specific feature sets.  A \"latest\" tag will always point to the most up-to-date recommended configuration for a standard AWS Bare Metal Rig.  Documentation within the repository will clearly outline available modules and how to compose them into new rig configurations. This approach replaces branching with modularity and provides a discoverable, centralized source of truth for all rig definitions.\n", "time": "0:00:11.003452", "input_tokens": 193, "output_tokens": 159}
{"context": "## Context\\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\\nPHP [appears to be on a downwards trend as a language](https://pypl.github.io/PYPL.html?country=GB),\\nespecially in contrast with Python. It's likely it will become increasingly difficult\\nto find good PHP developers in future.\\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\\nlanguages of similar pedigree, such as Python.\\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\\nA mix of contractors and developers working on the code base over several years has\\nresulted in a mix of styles and approaches. While we have already cleared out a lot\\nof unused and/or broken code, there is likely to be more we haven't found yet.\\nWe are also lagging behind the latest Design System guidelines, as our application was one\\nof the first to go live, before the current iteration of the Design System existed.\\nThis means that any changes to design have to be done piecemeal and manually: we can't\\nsimply import the newest version of the design system and have everything magically update.\\nThis combination of factors means that the code base can be difficult to work with:\\nresistant to change and easy to break.\\n", "response": "## Decision\n\nWe will incrementally migrate the existing PHP application to Python.  This will be done in a phased approach, prioritizing features and modules based on business value and technical debt.  We will not attempt a \"big bang\" rewrite.\n", "time": "0:00:01.908844", "input_tokens": 335, "output_tokens": 50}
{"context": "## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n", "response": "## Decision\n\nWe will use the MouseEvent API for capturing mouse events and the TouchEvent API for capturing touch events.  While this introduces some code complexity compared to a single PointerEvents implementation, it ensures compatibility across our target browsers, including Safari and iOS, while still allowing access to the necessary movement data for mouse interactions. We will maintain separate event handlers for mouse and touch interactions.  Since mobile is a secondary concern, we will prioritize optimization of the MouseEvent implementation.\n", "time": "0:00:04.212530", "input_tokens": 130, "output_tokens": 96}
{"context": "## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n", "response": "## Decision\n\nWe will **not** couple Dapr CLI versions with Dapr runtime versions.  The CLI will be versioned and released independently of the runtime.\n", "time": "0:00:01.880140", "input_tokens": 55, "output_tokens": 34}
{"context": "## Problem\\nMost of the credentials/information of the environment (like a service address) are only available as soon the application is deployed.\\n", "response": "## Decision\n\nWe will leverage environment variables to manage environment-specific configuration.  Sensitive information, like credentials, will be stored in a secrets management service and accessed at runtime.  Non-sensitive configuration will be injected via environment variables directly.\n", "time": "0:00:03.595522", "input_tokens": 56, "output_tokens": 49}
{"context": "## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno / minimal costs.\\n", "response": "## Decision\n\nUse Auth0 as the identity and access management solution for the BFI's IIIF Universal Viewer auditing platform.  Auth0 provides a robust, cloud-based platform with a generous free tier that covers the anticipated user base and required features for this project.  Its ease of integration and comprehensive documentation further contribute to minimizing initial development time and ongoing maintenance effort.\n", "time": "0:00:04.095251", "input_tokens": 128, "output_tokens": 76}
{"context": "## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n", "response": "## Decision\n\nUse AWS Aurora MySQL for the persistence layer.  This provides a managed, high-availability, performant SQL database that leverages a familiar open-source technology.  While cost is a concern, the added performance and availability benefits of Aurora MySQL outweigh the marginal cost increase over standard MySQL on RDS, especially considering the reduced operational overhead.  PostgreSQL on Aurora is a possible future consideration once it is fully released.\n", "time": "0:00:18.331454", "input_tokens": 268, "output_tokens": 87}
{"context": "## Context\\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\\n", "response": "## Decision\n\nAll data will be written on-chain first.  Once the on-chain transaction is confirmed, the data (or its hash) will be asynchronously replicated to off-chain storage. This ensures that any references to the data on-chain will always point to valid, committed data.  Off-chain storage will serve primarily as a performant cache and archival solution, with on-chain storage as the source of truth.\n", "time": "0:00:19.464575", "input_tokens": 84, "output_tokens": 89}
{"context": "## Problem\\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\\n(as they are designed to do so). Limitations of other storage plugins are\\ne.g., that not every structure of configuration is allowed.\\nSome of these limitations were documented `infos/status`, others were not.\\n", "response": "## Decision\n\nWe will introduce a new Elektra API function `keySetIsRepresentable(keySet, plugin)`. This function will determine whether a given KeySet can be stored by a specific storage plugin.  This will allow us to perform checks before attempting to save a KeySet, preventing data loss and providing more informative error messages to the user. We will update all existing storage plugins to correctly implement this new API function, reflecting their specific limitations.  Furthermore, the documentation of each storage plugin will be updated to explicitly list all limitations that affect representability.  The now obsolete `infos/status` will be removed.\n", "time": "0:00:03.597773", "input_tokens": 96, "output_tokens": 127}
{"context": "## Context\\n[Command-query separation](https://martinfowler.com/bliki/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n", "response": "## Decision\n\nWe will adopt command-query separation throughout the software.  All methods will be classified as either a command (modifying state) or a query (returning data without side effects).  This will improve the predictability and maintainability of the codebase by clearly delineating methods that modify state from those that don't.  This aligns with the existing terminology used within the `menu-generation` application.\n", "time": "0:00:02.678172", "input_tokens": 145, "output_tokens": 85}
{"context": "## Context\\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\\n", "response": "## Decision\n\nEmbed a version string and a UTC timestamp into the application.  This information will be accessible at runtime and displayed in a consistent location, such as:\n\n*   A dedicated `/version` endpoint that returns this information in JSON format.\n*   Included in the response headers of every request (e.g., `X-Application-Version` and `X-Application-Timestamp`).\n*   In the footer or a dedicated section of any web UI.\n*   Logged on application startup.\n\n\nThe version string should follow a clear and consistent format (e.g., semantic versioning). The timestamp should be the UTC time at the moment the application was built.\n", "time": "0:00:30.334689", "input_tokens": 83, "output_tokens": 140}
{"context": "## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n", "response": "## Decision\n\nThe software will be developed as a multi-module (Gradle) project. While a single, monolithic project might offer simpler initial integration, the long-term benefits of a modular architecture outweigh the short-term convenience.  This decision prioritizes code reusability, maintainability, and enforced API boundaries, which are crucial for long-term project health and potential future expansion beyond the current scope. This structure will necessitate more upfront effort in defining module interactions but will ultimately lead to a more robust and maintainable codebase.\n", "time": "0:00:03.184935", "input_tokens": 95, "output_tokens": 108}
{"context": "## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n", "response": "## Decision\n\nWe will implement a generalized \"Relationship\" model that can represent both the parent-child relationship and the family member-head of household relationship.  This model will have the following key attributes:\n\n* **Source Individual:**  A reference to the individual initiating the relationship (e.g., the mother in a parent-child relationship, the head of household in a family relationship).\n* **Target Individual:** A reference to the individual the relationship is directed towards (e.g., the child, a family member).\n* **Relationship Type:** An enumeration specifying the type of relationship (e.g., \"Parent-Child,\" \"Family Member,\" \"Spouse,\" etc.). This allows for filtering and specific logic based on relationship type.\n* **Start Date:** The date the relationship began (e.g., birth date of the child, date the family member joined the household).\n* **End Date (optional):** The date the relationship ended (e.g., in cases of adoption or family member leaving the household).\n* **Active (boolean):** Indicates if the relationship is currently active.  This provides an alternative to using the End Date for managing relationship status.\n\n\nThis generic structure will allow us to easily switch between children during delivery and PNC visits by querying for all \"Parent-Child\" relationships associated with the mother. Similarly, we can retrieve all members of a family by querying for all \"Family Member\" relationships associated with the head of household.  This approach avoids creating two separate models and maintains a consistent way of representing relationships within the system.  Future relationship types can be easily added by extending the `Relationship Type` enumeration.\n", "time": "0:00:08.142077", "input_tokens": 128, "output_tokens": 338}
{"context": "## Context\\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\\n", "response": "## Decision\n\nWe will use Auth0 as our authentication provider for Link Platform Administrators.  Auth0 provides a robust, secure, and scalable solution that supports various authentication methods, including username/password, social logins, and multi-factor authentication (MFA).  It also offers a user-friendly management dashboard for administrator user management.\n", "time": "0:00:04.391493", "input_tokens": 64, "output_tokens": 69}
{"context": "## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n", "response": "## Decision\n\nMigrate the service broker from the `alphagov/tech-ops` repository and the Tech Ops multi-tenant Concourse to a dedicated repository and pipeline owned and managed by the RE Autom8 team. This will provide the team with greater autonomy and control over the development, deployment, and maintenance of the broker. It also isolates the service from unrelated Tech Ops concerns, simplifying management and reducing the risk of unintended side effects from changes in the shared environment.  This new repository will be configured to deploy to GOV.UK PaaS in both the Ireland and London regions.\n", "time": "0:00:03.475934", "input_tokens": 119, "output_tokens": 117}
{"context": "## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n", "response": "## Decision\nWe will adopt a \"wait and see\" approach, gathering more data before making a significant architectural change.  This means we will continue with the current architecture for the time being, but prioritize implementing robust monitoring and analytics to better understand the actual performance bottlenecks and user behavior. This data will inform future architectural decisions and allow us to make more targeted and effective changes when necessary.\n", "time": "0:00:06.593865", "input_tokens": 48, "output_tokens": 78}
{"context": "## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n", "response": "## Decision\n\nUtilize Kafkarator to integrate applications with a managed Kafka instance hosted on Aiven. This decision supports the migration from on-premise infrastructure to the cloud and reduces operational overhead associated with maintaining a Kafka cluster.  It also promotes infrastructure-as-code principles by allowing for codified Kafka configuration.\n", "time": "0:00:02.272612", "input_tokens": 98, "output_tokens": 64}
{"context": "## Context and Problem Statement\\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\\n", "response": "## Decision\n\nWe will utilize the Newslabs PSTT service via its documented API using a client library specifically designed for it, if available. If no dedicated client library exists, we will use a suitable general-purpose HTTP client library to interact with the API. We will manage authentication as prescribed by Newslabs PSTT, likely via API keys or OAuth.\n", "time": "0:00:02.621719", "input_tokens": 53, "output_tokens": 72}
{"context": "## Context\\nPIMS requires a database to store all property information.\\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\\nAdditionally it must be supported by Entity Framework Core 3.1.\\n", "response": "## Decision\n\nUse PostgreSQL 12 running in a Linux Docker container on OpenShift.  This satisfies the relational data needs, supports required constraints, is compatible with the deployment environment, and is supported by Entity Framework Core 3.1.\n", "time": "0:00:08.475988", "input_tokens": 76, "output_tokens": 50}
{"context": "## Context and Problem Statement\\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\\ninterfaces\" - possibly leading to the impression that there is such a thing as\\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\\nthe only question is who manages this thread-safety. Interfaces which are not\\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\\nhidden in the generated code and therefore not obvious to the casual reader.\\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\\nmanaging their own locking internally. This ADR proposes that uniffi forces\\ncomponent authors to explicitly manage that locking in all cases - or to put\\nthis in Rust terms, that all structs supported by uniffi must already be\\n`Send+Sync`\\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\\nany misunderstandings that might be caused by using the somewhat broad and\\ngeneric \"Threadsafe\".\\n## Decision Drivers\\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https://github.com/mozilla-mobile/fenix/issues/17086)\\n(with more details available in [this JIRA ticket](https://jira.mozilla.com/browse/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https://github.com/mozilla/uniffi-rs/issues/417) or to\\n[pass interface references over the FFI](https://github.com/mozilla/uniffi-rs/issues/419).\\n", "response": "## Decision\n\nUniffi will only support components which expose structs that are `Send + Sync`. The `[Threadsafe]` attribute (and the underlying support for non-`Send + Sync` types) will be removed from uniffi.  All components will be required to manage their own internal synchronization and ensure that any exposed types satisfy the `Send` and `Sync` traits.  This change explicitly shifts the responsibility of thread safety to the component author, eliminating the hidden mutex and its associated overhead and potential for unexpected blocking.\n", "time": "0:00:03.293520", "input_tokens": 535, "output_tokens": 108}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "response": "## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation for capturing significant architectural decisions, along with their context, consequences, and rationale.  We will store ADRs as Markdown files in a dedicated `/docs/adr` directory within the project repository.  Each ADR will adhere to a consistent template (e.g., MADR).  New ADRs will be created for each significant architectural decision and reviewed as part of the standard pull request process.\n", "time": "0:00:03.477617", "input_tokens": 43, "output_tokens": 96}
{"context": "## Context and Problem Statement\\nWe found that without a standardised format our javascript files ended up with different\\nformats in different files or even multiple formats in the same file.  We also found that\\nour IDEs had different configurations which meant that using an autoformat tool would give\\ndifferent results when each of us do it.\\n## Decision Drivers\\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\n", "response": "## Decision\n\nWe will enforce a standard code style using Prettier, a code formatter.  We will integrate Prettier into our development workflow to automatically format code on save or commit. We will configure Prettier with a shared configuration file (e.g., `.prettierrc`) to ensure consistent formatting across all files and developers' environments.  We will also add Prettier to our CI pipeline to prevent code with formatting errors from being merged.  We will encourage the team to adopt a consistent IDE configuration (recommended extensions, etc.) where practical, however this will not be enforced and Prettier will be our source of truth for formatting.\n", "time": "0:00:03.776337", "input_tokens": 165, "output_tokens": 131}
{"context": "## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n", "response": "## Decision\n\nIntroduce named constants for the ordinal parameter values of operations with multiple modes.  These constants will replace the direct use of integer values in the codebase, improving readability and maintainability.\n", "time": "0:00:13.895463", "input_tokens": 59, "output_tokens": 40}
{"context": "## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n", "response": "## Decision\n\nWe will adopt TypeScript for all new Alfa code.  This decision addresses the concerns around code complexity, API contract enforcement, and the prevalence of `undefined`/`null` related bugs. TypeScript offers a robust type system that allows for static analysis and compile-time error checking, significantly reducing the risk of runtime errors.  Its transpilation capabilities ensure continued compatibility with browsers, allowing us to maintain support for browser-based tools like our Chrome extension.  This approach allows us to leverage the benefits of a strongly typed language while still meeting the requirement of browser compatibility.\n", "time": "0:00:03.296429", "input_tokens": 220, "output_tokens": 117}
{"context": "## Context and Problem Statement\\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\\nWhich query syntax should be used for the abstract queries?\\nWhich features should the syntax support?\\n", "response": "## Decision\n\nWe will use a subset of the Lucene query syntax for the abstract search queries. This subset will include the following features:\n\n* **Basic keyword search:**  Searching for individual words or phrases enclosed in double quotes.\n* **Boolean operators:** `AND`, `OR`, and `NOT` to combine search terms.  Parentheses `()` can be used to group expressions.\n* **Field-specific search:** Searching within specific fields using the syntax `field:value`. The available fields will be determined by the underlying libraries and exposed in the user interface.\n* **Wildcard search:** Using `*` (matches any sequence of characters) and `?` (matches any single character) for partial matching.\n* **Range queries:** Specifying ranges for numerical or date fields using square brackets (inclusive) and curly braces (exclusive), e.g., `year:[2020 TO 2023]` or `price:{10 TO 100}`.\n* **Proximity search:** Searching for terms near each other using the tilde operator `~`, e.g., `\"artificial intelligence\"~5` (finds \"artificial\" and \"intelligence\" within 5 words of each other).\n\n\nWe chose Lucene syntax because it's widely used and understood, has well-documented parsing libraries available (in various languages), and offers a good balance of power and complexity.  We deliberately omit some of Lucene's more advanced features (e.g., fuzzy searches, regular expressions) initially to keep the query language simple and predictable for our users. We can consider adding these features in future iterations based on user feedback and requirements.  A key advantage of choosing a well-established syntax like Lucene's is the potential for future extensibility.\n\nThis decision allows us to create a consistent search experience across different libraries while leveraging existing parsing infrastructure. The defined feature set balances the need for expressive queries with the goal of maintaining a user-friendly interface.\n", "time": "0:00:09.340898", "input_tokens": 111, "output_tokens": 409}
{"context": "## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\\nGovUK services have a \"trial mode\", and a \"live mode\".\\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\\n", "response": "## Decision\n\nKeycloak in each TDR environment will be configured to use the corresponding GovUK Notify service.  A separate GovUK Notify service will be created for each of trial and live modes within each TDR environment.  This results in separate GovUK Notify services for `Dev`, `Test`, `Staging`, `Preprod`, `Prod` (all using trial mode initially), and parallel services for `Prod-Live` when live functionality is required.  This approach provides isolation between environments and allows for controlled testing and promotion to live with GovUK Notify.  It also allows the non-production environments to operate within the constraints of trial mode while providing the option to switch production to live mode when required, lifting the associated restrictions.\n", "time": "0:00:17.280518", "input_tokens": 168, "output_tokens": 150}
{"context": "## Context\\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\\n", "response": "## Decision\n\nWe will implement Prometheus as our primary monitoring solution for the TTA Smart Hub, complemented by Grafana for visualization and alerting.  This decision is driven by Prometheus's proven effectiveness in monitoring microservice architectures, its open-source nature, its pull-based model which simplifies deployment, and its robust time-series database. Grafana's integration with Prometheus provides a user-friendly interface for creating dashboards and alerts, enabling quick identification and resolution of issues.\n", "time": "0:00:31.806271", "input_tokens": 46, "output_tokens": 96}
{"context": "## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https://stylelint.io/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n", "response": "## Decision\n\nWe will use Stylelint for linting our CSS and SASS code.  We will integrate it into our development workflow, both as a pre-commit hook and as part of our CI/CD pipeline.  A configuration file will be created and maintained to enforce our chosen style guide and best practices.\n", "time": "0:00:02.271903", "input_tokens": 105, "output_tokens": 65}
{"context": "## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n", "response": "## Decision\n\nPermissions will be determined by combining author-granted permissions and ACLs.  If a user has permission granted by *either* the author or an applicable ACL, they will have access. In other words, the effective permissions will be the *union* of author-granted permissions and ACL-granted permissions.  This approach prioritizes access and ensures that users granted access through one mechanism aren't inadvertently blocked by the other.\n", "time": "0:00:02.887158", "input_tokens": 104, "output_tokens": 89}
{"context": "## Context\\nThe Data Migrate gem has caused us a number of issues in the past, it runs\\nsilently as part of a deploy, and this can result in surprising errors\\nduring a deploy. We've also had issues with the gem itself - most recently\\na bug in a new version causing strange errors in deployment.\\n", "response": "## Decision\n\nWe will remove the Data Migrate gem from our deployment process and replace it with a manual migration process. This process will involve generating SQL migration scripts and running them manually before deployment.  This gives us greater control and visibility over the data migration process and reduces the risk of unexpected errors during deployments.  It also decouples data migrations from the application deployment itself, allowing us to handle them independently and with more scrutiny.\n", "time": "0:00:06.777543", "input_tokens": 97, "output_tokens": 87}
{"context": "## Context\\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\\n", "response": "## Decision\n\nWe will utilize the system's `DownloadManager` for handling background downloads. This leverages a robust, platform-optimized solution that manages HTTP interactions, retries, and persistence across connectivity changes and system reboots.  Client requests will specify the URI and destination file, and the application will register a broadcast receiver to monitor download progress, completion, and failures.  This approach simplifies our implementation, reduces maintenance overhead, and provides a consistent user experience aligned with platform standards.\n", "time": "0:00:02.891239", "input_tokens": 126, "output_tokens": 98}
{"context": "## Context\\nWe are doing a rewrite.\\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\\n", "response": "## Decision\n\nWe will adopt an actor model with a single, unified state tree managed by a central actor within each agent.  This central actor will be responsible for handling all incoming messages and updating the state tree accordingly.  All other modules will interact with the state indirectly by sending messages to the central actor, requesting data or state changes.  This approach enforces clear boundaries between modules, reduces implicit dependencies, and simplifies reasoning about state changes, especially within the context of a distributed p2p network.\n", "time": "0:00:04.315674", "input_tokens": 118, "output_tokens": 102}
{"context": "## Problem\\nCurrently multiple warnings are saved in an elektra non-conforming array\\nnotation which is limited to 100 entries. The notation of `#00` is against\\nthe design [decision made](array.md).\\n", "response": "## Decision\nMigrate the storage of warnings to a format compliant with Elektra's array specifications.  This will involve removing the current `#00` style notation and implementing a proper Elektra array using the `/#` syntax, which does not have the 100-entry limitation.\n", "time": "0:00:02.477293", "input_tokens": 79, "output_tokens": 62}
{"context": "## Context\\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\\nThere is currently no formal recommendation for a tool of choice in the technical guidance https://github.com/DFE-Digital/technical-guidance.\\nWe want a tool that provides an open and modern security standard.\\n", "response": "## Decision\n\nUse DfE Sign-in (DSI) for authentication, despite the preference for a simpler passwordless solution. This decision aligns with DfE governance requirements and ensures compliance with established security standards.  While a passwordless approach might offer a better user experience for school buying professionals, utilizing DSI provides a standardized and vetted authentication mechanism across DfE services.  Further investigation will be needed to determine the specific integration approach and how to best handle the additional user information provided by DSI, given it is not strictly required for the service's functionality.\n", "time": "0:00:03.241718", "input_tokens": 155, "output_tokens": 115}
{"context": "## Context and Problem Statement\\nHow to guarantee reproducibility of Jupyter Notebooks?\\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\\nNotebooks should be treated as component/service that use their own dependencies, therefore when storing notebooks,\\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\\n## Decision Drivers <!-- optional -->\\n* user prospective\\n* reproducibility\\n* traecability\\n", "response": "## Decision\n\nStore each Jupyter Notebook with a dedicated `requirements.txt` file containing all direct and transitive dependencies.  This file will be generated automatically upon saving the notebook using a tool like `pipreqsnb` or a similar dependency management solution specifically designed for Jupyter Notebooks.  When building a container image for running the notebook or sharing the notebook with others, this `requirements.txt` file will be included, ensuring a consistent and reproducible execution environment.\n", "time": "0:00:02.838321", "input_tokens": 164, "output_tokens": 92}
{"context": "## Context\\nWe need to record the architectural decisions that we make as we develop Ockam.\\n", "response": "## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents that capture the context, decision, and consequences of significant architectural choices.  These ADRs will be stored as Markdown files in a dedicated directory within the Ockam repository.\n", "time": "0:00:13.330701", "input_tokens": 48, "output_tokens": 52}
{"context": "## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n", "response": "## Decision\nWe will adopt a \"wait-and-see\" approach and not implement any specific solution at this time.  We will continue to monitor the situation and gather more data on the actual impact of the issue before investing development resources. This allows us to avoid premature optimization and potentially unnecessary complexity.  If the issue becomes significant enough to warrant action, we will revisit this decision and explore alternative solutions with more information at hand.\n", "time": "0:00:02.778234", "input_tokens": 48, "output_tokens": 88}
{"context": "## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https://apps.nrs.gov.bc.ca/int/jira/browse/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n", "response": "## Decision\n\nIITD Architecture officially recommends migrating all IITD-hosted Java applications to OpenJDK 11 (or later LTS releases) as the strategic direction forward.  This decision is driven by Oracle's shift to a subscription-based licensing model for Oracle Java SE and the need for continued security updates and support. While Oracle JDK and OpenJDK are largely interchangeable, utilizing OpenJDK mitigates licensing costs and aligns with the community-driven development model.  This applies to all IITD server-side and client-side Java applications.  Migration efforts should be prioritized to ensure timely completion and minimize security risks associated with remaining on older, unsupported Java versions.\n", "time": "0:00:07.089938", "input_tokens": 442, "output_tokens": 134}
{"context": "## **Context**\\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\\nWe will look at two options for validation:\\n- **Data Annotations**\\nThis involves \"annotating\" each class model with specific validation, such as\\n- `[Required(ErrorMessage = \"This field is required\")]`\\n- `[MaxLength(20)]`\\nThere are a number of issues with this approach:\\n- Validation is scattered throughout the codebase as attributes on data model classes\\n- Testing is not straightforward\\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\\n- Does not allow for conditional validation\\n- **Fluent Validation**\\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\\n- Lives outside of the data model classes\\n- Very easy to test validation in isolation\\n- Error messaging can be externalised using dependency injection\\n- Allows for chaining of validators and conditional validation\\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\\n", "response": "## **Decision**\n\nWe will use FluentValidation for input validation within our APIs. While Data Annotations offer a simpler initial setup, FluentValidation provides greater flexibility, maintainability, and testability in the long run.  The ability to centralize validation logic, externalize error messages, and implement complex conditional validation outweighs the minor overhead of implementing FluentValidation. This approach aligns better with our goals of creating a robust and easily maintainable API.\n", "time": "0:00:04.934247", "input_tokens": 320, "output_tokens": 91}
{"context": "## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n", "response": "## Decision\n\nIntroduce a distributed task management system. This system will enable centralized management and orchestration of tasks across the entire James cluster.  This will involve the following changes:\n\n1. **Centralized Task Queue:** Implement a shared, distributed queue for all tasks submitted to the cluster.  This queue will be accessible by all James nodes.  A suitable technology for this could be a message queue like RabbitMQ or Kafka, or a distributed cache like Redis.\n\n2. **Distributed Task Execution:** Allow tasks to be executed on any node within the cluster, not just the node on which they were submitted. This will enable load balancing and better resource utilization.\n\n3. **Task Metadata Storage:** Store metadata about each task (e.g., status, start time, end time, assigned node) in a centralized data store. This data store will enable tracking and monitoring of tasks across the cluster. This could be implemented with a database like PostgreSQL or a distributed key-value store like etcd.\n\n4. **API for Task Management:** Implement a cluster-wide API for interacting with the distributed task management system. This API will provide functionalities such as submitting tasks, querying task status, and retrieving task results.\n\n5. **Task Assignment Mechanism:**  Implement a mechanism for assigning tasks from the centralized queue to individual nodes in the cluster. This mechanism could be based on factors like node load, available resources, and task requirements.  Consider strategies like round-robin, least connections, or a dedicated scheduler.\n\n6. **Graceful Handling of Node Failures:** Implement mechanisms to handle node failures gracefully. For example, if a node executing a task fails, the task should be re-queued and assigned to another available node. The chosen distributed queue technology should ideally provide at-least-once delivery guarantees.\n", "time": "0:00:08.620898", "input_tokens": 130, "output_tokens": 366}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "response": "## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture significant architectural decisions along with their context, consequences, and status.  These ADRs will be stored as Markdown files in a dedicated `docs/architecture/decisions` directory within the project's version control system.  This approach allows us to maintain a clear history of architectural choices and facilitate communication among team members.\n", "time": "0:00:30.318220", "input_tokens": 43, "output_tokens": 82}
{"context": "## Context\\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\\nsandboxed environment. A Docker image has been created to help with this but this still\\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\\nto run Bitcoin miners and other compute-stealing stuff.\\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\\nbe found in the [notes on Isolation](https://github.com/martinklepsch/cljdoc/blob/72da65055ab94942f33fb63b29b732e81b559508/doc/isolation.md)\\n", "response": "## Decision\n\nWe will utilize a continuous integration (CI) service, specifically GitHub Actions, to execute untrusted Clojure code within a controlled environment.  This approach avoids the operational overhead and security risks associated with managing our own Docker infrastructure or using serverless computing platforms like AWS Lambda, while still providing a degree of isolation and resource limits. We will leverage GitHub Actions' built-in security features and configure jobs with strict resource constraints (CPU time, memory, network access) to mitigate the risk of abuse, such as cryptocurrency mining.\n", "time": "0:02:45.078793", "input_tokens": 211, "output_tokens": 108}
{"context": "## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\\nAs part of this, considerations were made around:\\n* Ease of use for creating new sites\\n* Ability to share components\\n* Ability to segregate content for specific channels (sites)\\n* Ability to share content up and down the stack where needed and appropriate\\n* Permissions model required to support this model\\n", "response": "## Decision\n\nWe will leverage Bloomreach's native content management capabilities and \"Channel\" concept for managing multiple sites (including Deenary and Speciality sites) both at MVP and for future expansion.  This decision prioritizes ease of use, component reusability, and streamlined content management within a single platform.  External document management system integration is deferred for now due to added complexity and potential performance overhead.  This approach allows us to leverage Bloomreach's built-in features for content sharing, segregation, and permissions management, simplifying development and maintenance while facilitating a consistent experience across all sites.\n", "time": "0:00:11.920380", "input_tokens": 174, "output_tokens": 121}
{"context": "## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https://symfony.com/doc/current/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n", "response": "## Decision\n\nWe will write our own validation logic and place it within the domain layer.  This encapsulates validation rules with the business logic they pertain to and keeps the domain logic independent of external libraries or framework concerns.  Simple validators will be implemented directly within the domain entities or value objects where appropriate.  More complex validation rules that involve multiple domain objects will be implemented as separate validator classes within the domain layer.\n", "time": "0:00:12.715320", "input_tokens": 262, "output_tokens": 84}
{"context": "## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n", "response": "## Decision\n\nWe will implement a generic approach to retrieve change metrics based on task information. This will involve the following:\n\n1. **No direct integration with any specific task tracker:** Instead of integrating directly with a task tracker API (e.g., Jira, Trello, etc.), we will rely on user-provided task data. This ensures flexibility and avoids dependencies on specific task tracker platforms.\n\n2. **CSV input for task data:** Users will provide task data via a CSV file.  This file must contain a pre-defined set of mandatory fields (e.g., Task ID, Task Type, optionally Story Points, etc.) allowing categorization and analysis.  We will define a clear specification for the CSV format, including required and optional fields.\n\n3. **Code Churn Analysis via Git Log:**  We will leverage `git log` to gather change metrics (lines added, lines deleted, files changed, etc.). The `git log` command will be used with parameters that allow filtering commits based on the task IDs found in the CSV input.  We will assume that commit messages include the task IDs, possibly using a specific pattern like `#TASK-123` or `TASK-123`.  The tool will offer options to configure this pattern.\n\n4. **Output in a standardized format:**  The generated metrics will be output in a consistent format, such as JSON or CSV, suitable for further processing or visualization.  This output will include aggregated metrics by task type (e.g., total lines of code changed for all bug fixes, average story points per feature request, etc.).\n\nThis approach provides a balance between simplicity, flexibility, and avoids making assumptions about the specific task tracker used. It allows users to export task data from their chosen tracker and use our tool to gain insights into the changes related to those tasks.\n", "time": "0:00:18.045694", "input_tokens": 111, "output_tokens": 377}
{"context": "## Context\\nOur service-operator allows service teams to provision various AWS services by\\ndeclaratively defining resources and submitting them via the kubernetes api.\\nSome of these resources require IAM to authorise how the provisioned service\\ncan be used. The types of actions that can be performed on.\\n#### Example\\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\\n```\\n---\\napiVersion: storage.govsvc.uk/v1beta1\\nkind: S3Bucket\\nmetadata:\\nname: s3-bucket-sample\\nspec:\\naws:\\nLifecycleRules:\\n- Expiration: 90days\\nVersioning:\\nEnabled: true\\n```\\nIn order to access a provisioned bucket via the the AWS SDK users will require\\nan IAM role/policy that allows access.\\nWe want things like bucket ACL, versioning configuration and lifecycle policy\\nto be defined declaratively via the resource manifest (see example above), and continuously managed\\nby the service operator.\\nWe want users of the provisioned bucket to be able to read back all\\nconfiguration, and be able to fully utilise the specific bucket for reading,\\nwriting and managing their objects within the provisioned bucket, but we want\\nto avoid giving permissions to users that could cause conflicts with the\\nproperties that are managed by the service operator's reconcile loop.\\nFor example, given the example manifest above, we would like to avoid giving\\npermissions that would allow a user to alter the Expiration LifeCycleRules,\\nsince any changes the user made would be periodically overwritten by the\\nservice operator's reconciliation.\\n", "response": "## Decision\n\nWe will use a combination of IAM roles and policies to manage access to provisioned AWS resources.  Specifically:\n\n1. **Service Operator Role:** The service operator will utilize an IAM role with permissions sufficient to create, modify, and delete all supported AWS resources defined in the CRDs.  This role will be used during the reconciliation loop to ensure the desired state is maintained.\n\n2. **Provisioned Resource Role (per resource):** For each provisioned resource (e.g., S3 bucket), a dedicated IAM role will be created.  This role will grant data plane access to the provisioned resource, but *explicitly deny* permissions that could interfere with the service operator's management.\n\n    * **S3 Example:** The provisioned resource role for an S3 bucket would grant permissions for `s3:GetObject`, `s3:PutObject`, `s3:DeleteObject`, `s3:ListBucket`, `s3:GetBucket*`  etc., allowing users full access to object management within the bucket.  Crucially, it would *deny* permissions like `s3:PutBucketLifecycleConfiguration`, `s3:PutBucketAcl` which could conflict with the service operator's configuration.\n\n3. **User Access:**  Users will be granted access to the *provisioned resource role*, not the service operator role. This provides them with the necessary permissions to utilize the resource while preventing them from making changes that the service operator manages.  This could be achieved by various means appropriate to the platform, such as Kubernetes service accounts referencing the provisioned resource role, or direct association of IAM users/groups to the role.\n\n\nThis approach allows for a clear separation of concerns, enabling users to fully interact with their provisioned resources while preventing conflicts with the automated management performed by the service operator.  It also allows for fine-grained control over user permissions on a per-resource basis, increasing security and reducing the risk of unintended changes.\n", "time": "0:00:11.445413", "input_tokens": 378, "output_tokens": 407}
{"context": "## Context\\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\\n", "response": "## Decision\n\nWe will implement autocomplete functionality using a third-party library, specifically `react-autosuggest` for front-end components and a dedicated autocomplete service backed by an Elasticsearch instance for data retrieval and suggestions.  This combination allows for flexible data indexing and querying while providing a robust and performant user experience.\n", "time": "0:00:06.936626", "input_tokens": 53, "output_tokens": 64}
{"context": "## Context and Problem Statement\\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\\n## Decision Drivers <!-- optional -->\\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible/when if needed.\\n", "response": "## Decision\n\nUse a message bus architecture for communication between internal components of the Transcript Editor.  Each component will subscribe to specific message types and publish messages when relevant events occur.  This will decouple the components and allow for flexible extension and integration in the future.  A message schema will be defined to ensure consistency and facilitate validation.\n", "time": "0:00:24.703280", "input_tokens": 99, "output_tokens": 68}
{"context": "## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state/model.\\n", "response": "## Decision\n\nWe will adopt `react-testing-library` for integration tests to facilitate interactions with UI elements like toggling tabs and window groups. However, instead of directly asserting the rendered DOM, we will prioritize verifying the underlying state/model changes triggered by these interactions.  We will only use `react-testing-library` to locate elements to interact with (e.g., clicking a button).  We will then check that the expected changes have occurred in the application state or model as a result of this interaction. This approach reduces the brittleness of our tests by decoupling them from implementation details of the view layer and focusing on the core functionality.  We acknowledge the risk of missing potential view rendering errors, and will rely on comprehensive unit tests for the view components and careful manual testing to mitigate this risk.\n", "time": "0:00:04.626624", "input_tokens": 142, "output_tokens": 167}
{"context": "## Context\\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\\nAdditionally Geocoder can be used to verify addresses that are manually entered.\\n- [Geocoder](https://www2.gov.bc.ca/gov/content/data/geographic-data-services/location-services/geocoder)\\n- [Data BC](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service)\\n- [API Swagger](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service/resource/40d6411e-ab98-4df9-a24e-67f81c45f6fa/view/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\\n- [Developer API Keys](https://github.com/bcgov/gwa/wiki/Developer-Guide#developer-api-keys)\\n- API Host = `https://geocoder.api.gov.bc.ca`\\n", "response": "## Decision\n\nWe will integrate the Data BC Geocoder API into the inventory system to improve the accuracy and user experience of entering property location data.  This integration will allow users to:\n\n1. **Retrieve Latitude/Longitude based on Address:** Users will be able to enter a property address and automatically populate the latitude and longitude fields using the Geocoder API. This will be the preferred method for entering location data.\n\n2. **Verify Manually Entered Addresses:** If a user chooses to manually enter latitude/longitude, the system will use the Geocoder API to perform a reverse geocode lookup to verify the address and alert the user of any discrepancies.  This will help prevent incorrect or inconsistent address data from being entered.\n\nWe will utilize the provided API documentation and acquire the necessary developer API keys to access the service. The API host (`https://geocoder.api.gov.bc.ca`) will be configured within the application.  We will also implement appropriate error handling and fallback mechanisms in case the Geocoder API is unavailable.\n", "time": "0:00:06.878830", "input_tokens": 337, "output_tokens": 209}
{"context": "## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n", "response": "## Decision\nTo revert to extracting the `Grant-wise expenditure` section from Koshvani as the original issues preventing its extraction have been resolved.  The previous workaround of extracting the `DDO-wise expenditure` section will be discontinued.\n", "time": "0:00:05.348660", "input_tokens": 112, "output_tokens": 50}
{"context": "## Context and Problem Statement\\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\\n```\\n<xs:element name=\"take\" type=\"xs:integer\">\\n```\\nor with attributes as:\\n```\\n<xs:element name=\"take\">\\n<xs:complexType>\\n<xs:attribute name=\"value\"/>\\n</xs:complexType>\\n</xs:element>\\n```\\n## Decision Drivers\\n* consistency\\n* expressiveness\\n", "response": "## Decision\n\nOperations will be defined as elements with attributes.  While defining operations as simple elements is simpler, using attributes provides more flexibility and consistency with how other constructs are represented within FDML. This approach also aligns better with potential future extensions and allows for richer metadata to be associated with operations.\n", "time": "0:00:18.505726", "input_tokens": 145, "output_tokens": 60}
{"context": "Context\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https://github.com/openedx/event-routing-backends/blob/f430d4cf58bdab01e42fcc944241898606873d82/event_routing_backends/processors/mixins/base_transformer.py#L139\\n", "response": "Decision\n--------\nA centralized accessor method will be implemented within the `event-routing-backends` application to retrieve data from Open edX events.  This method will be the only mechanism permitted for accessing event data within base transformers and specific event transformers (e.g., xAPI, Caliper).  This accessor will enforce key presence based on the target specification (xAPI or Caliper), throwing an exception for missing required keys and logging a warning and returning `None` for missing optional keys.  Future work will investigate the feasibility and benefit of incorporating fuzzy matching to further reduce the risk of PII leakage.\n", "time": "0:00:04.473851", "input_tokens": 371, "output_tokens": 125}
{"context": "# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https://www.gov.uk/service-manual/making-software/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop / HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n", "response": "## Decision\n\nWe will use MongoDB as the persistent data store for the Performance Platform.  This choice aligns with GDS experience and supports the platform's need to ingest and query unstructured data. This approach is suitable for the anticipated small data volume and our role as an aggregator, not the canonical source, relaxing requirements for ACID properties and intricate considerations around the CAP theorem.  This decision is made with the understanding that it will be revisited as the platform scales and user needs evolve.\n", "time": "0:00:03.099332", "input_tokens": 440, "output_tokens": 98}
{"context": "## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https://www.w3.org/WAI/WCAG21/Understanding/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https://github.com/hmrc/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n", "response": "## Decision\n\nImplement a new endpoint on the existing `bootstrap-play` service which returns the remaining session time. This endpoint will be excluded from session management and thus will not refresh the session expiry time. The `hmrcTimeoutDialog` will be updated to periodically call this endpoint (e.g., every minute) to get the accurate remaining session time.  This approach leverages the existing `bootstrap-play` service which already handles session management, thereby minimizing code changes for service teams and avoiding the need for new routing rules or configurations.  Furthermore, this centralizes session-related logic and avoids duplication of knowledge. Finally, by not requiring interaction with other services, we maintain loose coupling and avoid introducing single points of failure.\n", "time": "0:00:04.014899", "input_tokens": 393, "output_tokens": 148}
{"context": "## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n", "response": "## Decision\n\nWe will adhere to established coding standards and style guides for the chosen programming language(s) and framework(s) of this project. This includes utilizing a linter and formatter to enforce these standards automatically.  We will select specific style guides and linters during the project setup phase and document them clearly in the project's contributing guidelines.\n", "time": "0:00:20.116178", "input_tokens": 73, "output_tokens": 72}
{"context": "## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n", "response": "## Decision\n\nWe will use Airflow to manage asynchronous tasks and workflows. While it introduces a new dependency (Celery) and requires managing the scheduler and a cache, its maturity, stability, excellent UI, and fit within our execution model outweigh these drawbacks.  Furthermore, its Python integration aligns well with our existing codebase.  This decision supersedes the selection made in this ADR and is documented in `ADR-0019`.\n", "time": "0:00:02.958629", "input_tokens": 376, "output_tokens": 90}
{"context": "## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n", "response": "## Decision\n\nUse a graph database (e.g., Neo4j, Amazon Neptune) as the persistence layer for SKOS taxonomies.  This leverages the native RDF support of graph databases, simplifying data storage and retrieval and eliminating the need for custom serialization/deserialization logic. This decision prioritizes ease of implementation and maintainability, accepting potential trade-offs in other areas like vendor lock-in.\n", "time": "0:00:02.530286", "input_tokens": 127, "output_tokens": 84}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "response": "## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents that capture important architectural decisions along with their context and consequences.  These ADRs will be stored as Markdown files in a dedicated `/adr` directory at the root of the project's repository.  We will use a sequential numbering scheme for ADR filenames (e.g., `0001-use-markdown-for-adrs.md`).  A template will be used for consistency and completeness.\n", "time": "0:00:02.983779", "input_tokens": 43, "output_tokens": 100}
{"context": "## Context\\nThere are lots of different tasks that need processed in order to get the prototype kit up and running. Tasks such as; installing dependencies, moving files from dependencies into the app file structure, and most importantly - running the application.\\n", "response": "## Decision\n\nWe will create a build script using a task runner like `npm scripts` (if using Node.js) or `make` (for a more general approach).  This script will automate the execution of the necessary tasks for setting up and running the prototype kit. This includes installing dependencies, copying required files, and starting the application.  Using a task runner provides a centralized and repeatable process for these actions, simplifying development and ensuring consistency across different environments.\n", "time": "0:00:02.826018", "input_tokens": 76, "output_tokens": 95}
{"context": "## Context\\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short\\nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\\nto not overload a server.\\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\\nallows, among others:\\n- Delaying retries upon MX delivery failure to a remote site.\\n- Throttling, which could be helpful for not being considered a spammer.\\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait\\ndelays, purging the queue, etc.\\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.\\nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need\\nto interact with all its James servers, which is not friendly in a distributed setup.\\nDistributed James relies on the following third party softwares (among other):\\n- **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be\\nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\\n- **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\\n- **ObjectStorage** (Swift or S3) holds byte content.\\n", "response": "## Decision\n\nWe will implement a new distributed Mail Queue leveraging a message broker for queuing and an object storage for mail content.  RabbitMQ will be used as the message broker, and the existing object storage (Swift or S3) will store the mail content.  To address the limitations of RabbitMQ for administrative operations like `browse`, `getSize`, and `arbitrary mail removal`, we will introduce a dedicated metadata store.  This metadata store will maintain essential information about each queued mail item, including its location in object storage, allowing efficient implementation of these operations.  Cassandra will *not* be used for the mail queue metadata due to its unsuitability for queue-like structures.  Instead, a more appropriate data store, such as a relational database (e.g., PostgreSQL) or a key-value store optimized for fast lookups, will be evaluated and selected based on performance and operational requirements. This approach combines the strengths of RabbitMQ for queuing with a more suitable solution for metadata management and existing object storage for content, providing both performance and administrative flexibility.\n", "time": "0:00:06.166093", "input_tokens": 341, "output_tokens": 216}
{"context": "## Context\\nThe main data source of the API catalogue website was originally a collection of\\nmarkdown files. The build process uses the Middleman static site generator\\nconfigured by the [Tech Docs Gem](https://github.com/alphagov/tech-docs-gem)\\n('TDG').\\nThe TDG provides additional functionality including search, sidebar\\nnavigation ('Table of Contents'), the layout, and styling.\\nThe TDG is not necessarily a good fit for the API catalogue because the project\\nisn't purely documentation, and our data source is now a CSV.\\nIn particular it is difficult to override templates inherited from the gem, to\\nadjust the layout on a particular page or add page-specific JavaScript for\\nexample.\\nUsing TDG to render the Table of Contents is slow for our site because\\nby design every page is re-rendered multiple times to pull out the headings\\n(adding over a minute to build times).\\nThe TDG also requires specific dependency versions. These version\\nrestrictions prevent us being in control of version upgrades which are necessary\\nto remain on support versions and receive security patches.\\n", "response": "## Decision\n\nMigrate away from the Tech Docs Gem (TDG) for generating the API catalogue website.  While TDG provides useful features like search, navigation, and styling, its limitations outweigh its benefits in our specific context. The shift to a CSV data source, the difficulty in customizing templates and adding page-specific functionality, the performance overhead of TOC generation, and the constraints on dependency versions make TDG a suboptimal choice.  We will explore alternative static site generators or frameworks that offer greater flexibility, performance, and control over dependencies, while still providing the necessary features for an API catalogue.\n", "time": "0:00:03.612981", "input_tokens": 265, "output_tokens": 120}
{"context": "## Context and Problem Statement\\nThe organization has several UI apps and libraries and of them have different React versions, causing issues whenever we want to consume them. To remove these problems, and to keep every app updated, we need to move to React 17 in every UI app and lib, specially in the UI repository that contains most of our shared UI components.\\nUpdating the UI repository to the latest version of React implies updating `react-semantic-ui` to its latest version, ending up in [a major change that removed the `Responsive` component](https://github.com/Semantic-Org/Semantic-UI-React/pull/4008), a widely used component dedicated to conditionally rendering different components based on their display. Removing this component will cause a breaking change in our current [UI library](https://github.com/decentraland/ui) and will imply everyone to get on board of this breaking change, but a different strategy can be chosen by keeping the `Responsive` component by copying it from the library until everyone gets on board with an alternative.\\nWe need to provide, alongside this update, an alternative library to the `Responsive` component, providing a similar or a better API for rendering components according to device sizes.\\n- The `@artsy/fresnel` works by using a ContextProvider component that wraps the whole application, coupling the media query solution to this library.\\n- Doesn't have hooks support.\\n#### Second alternative (react-semantic-ui)\\n##### Advantages\\n- The libary doesn't require a provider or something previously set in an application to use it (non-coupling dependency).\\n- Provides hooks and component solutions for rendering components with different media queries, providing a versatile that allows us to render different components or part of the components by using the hooks.\\n##### Disadvantages\\n- Bad SSR support.\\n", "response": "## Decision\n\nWe will upgrade all UI apps and libraries to React 17 and the latest version of `react-semantic-ui`.  Recognizing the breaking change introduced by the removal of the `Responsive` component, we will adopt the following strategy:\n\n1. **Fork and Copy `Responsive` Component:**  We will fork `react-semantic-ui` and copy the `Responsive` component into our UI library. This temporary measure ensures backward compatibility and allows teams to gradually adapt to the change without immediate disruption.\n\n2. **Introduce `react-responsive` as the Long-Term Solution:**  We will adopt `react-responsive` as the official replacement for the `Responsive` component.  Its non-coupling design, hooks support, and component-based approach offer the most versatile and least disruptive long-term solution. While SSR support is a noted disadvantage, we will prioritize its other benefits and address potential SSR issues as they arise.\n\n3. **Deprecation Period and Migration Guide:** We will establish a reasonable deprecation period for the copied `Responsive` component.  During this time, we will provide clear documentation and a migration guide to assist teams in transitioning to `react-responsive`.  After the deprecation period, the copied `Responsive` component will be removed from our UI library.\n", "time": "0:00:06.456630", "input_tokens": 412, "output_tokens": 268}
{"context": "## Context\\nEach commercial agreement will ask the buyer questions in order to ascertain what outcomes and supplier offers are applicable to the buyer. Some of these questions will be the same across agreements. Many will have common patterns and data, even if they differ in detail.\\nIn the future we will want to more automatically match buyers\u2019 articulations of need so as to work out which products CCS can offer to meet their need.\\nThe buyer needs data needs to be stored during a buying journey, but final agreement details, when the buyer agrees a call-off for instance, will need to be stored in the agreement records.\\n", "response": "## Decision\n\nWe will create a \"Buyer Requirements\" data structure that will be associated with each commercial agreement during the buyer's journey. This structure will store the buyer's answers to the qualification questions. It will be designed to accommodate both common questions and agreement-specific questions.  The data structure will support future enhancements to allow for automated matching of buyer needs to supplier offerings.  While the Buyer Requirements data will be collected and maintained during the buying journey, final, legally binding details agreed upon at the point of call-off will be stored separately within the \"Agreement Records\" data store. This separation ensures a clear distinction between the exploratory phase of the buyer journey and the finalized contractual obligations.  We will explore and document the relationship between these two data structures (Buyer Requirements and Agreement Records) as a separate decision record.\n", "time": "0:00:04.318557", "input_tokens": 152, "output_tokens": 167}
{"context": "## Context\\nTravis has some limitations about what stages you can use to create a pipleine.  We still desire to have a hosted/PaaS CI/CD solution\\n", "response": "## Decision\n\nMigrate from Travis CI to GitHub Actions for our CI/CD pipeline.\n", "time": "0:00:04.497609", "input_tokens": 64, "output_tokens": 18}
{"context": "## Context\\nThere are a number of ways an API could allow clients to upload files to S3, the popular ones:\\n- Allow the API to accept Base 64 encoded files in a JSON POST request and subsequently send this blob to S3\\n- Allow the API to accept multipart form uploads, compile the parts on the server then send the file to S3\\n- Use the S3 Presigned URL functionality, which allows the client to act as the IAM which created the URL for a single operation, and upload the file directly to S3 themselves\\n", "response": "## Decision\n\nWe will use S3 Presigned URLs for file uploads.  This approach offloads the file transfer process directly to the client and S3, bypassing our API servers entirely. This minimizes server load, reduces bandwidth costs, and improves upload speeds, especially for larger files.  It also simplifies our API implementation as we no longer need to handle file data streaming or storage on the server.\n", "time": "0:00:05.959618", "input_tokens": 144, "output_tokens": 81}
{"context": "## Context\\nSmarthub project consist of multiple packages which are the part of Smarthub SDK. Complex dependency graph forces us to update dependent packages manually every time dependency has changed.\\n", "response": "## Decision\n\nWe will implement a centralized dependency management system using a dedicated tool.  This tool will automatically update dependent packages within the Smarthub SDK whenever a dependency changes. We will evaluate and select a suitable tool (e.g., a monorepo tool like Nx, Lerna, or Rush, or a package manager with advanced dependency management features) based on our specific needs and project structure. This approach will automate the update process, reducing manual effort and the risk of inconsistencies across packages.\n", "time": "0:00:40.242480", "input_tokens": 65, "output_tokens": 101}
{"context": "## Context\\nDrop FreeTFT, TFTA tokens from SDK\\n", "response": "## Decision\n\nRemove FreeTFT and TFTA token functionality from the SDK. This includes any related classes, methods, and dependencies.  The SDK will no longer support interactions with these tokens.\n", "time": "0:00:01.764064", "input_tokens": 41, "output_tokens": 39}
{"context": "## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\nThe original author of go-bindata delete their account and then the repo was\\nrecreated under a different owner. The dependency has shifted around enough that\\nwe have lost faith in the intention of the maintainers\\n[more details here](https://twitter.com/francesc/status/961249107020001280?lang=en)\\nAlso, some of the development use cases around go-bindata (like what is bundled into the code\\nduring a test run or final build) made it hard to reason about.\\n", "response": "## Decision\n\nWe will replace `go-bindata` with `embed`, the built-in Go functionality introduced in Go 1.16. This addresses the maintainability concerns around `go-bindata` and simplifies our build process by leveraging native tooling.  We believe `embed` provides equivalent functionality for our needs and its integration with the standard library offers greater stability and predictable behavior.\n", "time": "0:00:07.644049", "input_tokens": 174, "output_tokens": 81}
{"context": "## Context\\nThe Code Style Guide is a set of conventions on how to write the source code. It covers many areas, including the use of `camelCase` vs `PascalCase` for variable identifiers, whether or not to use a semicolon as a statement terminator, or the use of tabs or spaces for indentation.\\nIt is obvious that an enforced, unified code style in a project is beneficial. Not only does it improve the readability, but it also saves you from a lot of noise while looking at diffs, caused by unadjusted whitespaces, different break line settings, and other issues. It also ends discussions around the style itself. Once applied, everyone can write code the way they want, and let the formatter do the work. In the end, it saves time and energy.\\n[Prettier](https://prettier.io) is one of the solutions dedicated to code formatting. It does not enforce such code quality rules as the use of globally scoped variables or naming. It only enforces the formatting rules. It can be used as a plugin for selected IDEs, as a pre-commit `git` hook, or as a standalone CLI tool. No matter which option you choose, it produces the same output given its configuration.\\nIt was chosen because of its simplicity, ease of configuration, small amount of available options, and support for JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON, all of which are used in Kyma projects.\\n", "response": "## Decision\n\nWe will adopt Prettier as the code formatter for all projects.  This will enforce consistent code style across our JavaScript, TypeScript, GraphQL, CSS, SCSS, and JSON files, improving readability and reducing noise in code reviews. Prettier will be integrated into our development workflow via IDE plugins, pre-commit hooks, and CI pipelines to ensure consistent formatting before code is merged.  We chose Prettier due to its simplicity, ease of configuration, limited options (reducing configuration debates), and wide language support covering all languages used in Kyma projects.\n", "time": "0:00:03.257332", "input_tokens": 326, "output_tokens": 114}
{"context": "## Context\\nWe have to make a decision on the branching strategy for development.\\n", "response": "## Decision\n\nWe will adopt a Gitflow branching strategy.  This involves using the following branches:\n\n* **main:** This branch will represent the production-ready code.  All releases will be tagged from this branch.\n* **develop:** This branch will represent the latest development changes integrated and tested.  It will be the primary integration branch for feature branches.\n* **feature/[feature_name]:**  Feature branches will be created off of `develop` and merged back into `develop` once complete. This allows isolated development of individual features.\n* **release/[release_version]:** Release branches will be created off of `develop` when preparing for a release.  This allows for final testing and minor bug fixes specific to the release without disrupting ongoing development on `develop`.  Once released, the release branch is merged into both `main` and `develop`.\n* **hotfix/[hotfix_name]:** Hotfix branches will be created off of `main` to address critical bugs in production.  Once the fix is complete, the hotfix branch is merged into both `main` and `develop`.\n\nThis strategy provides a clear structure for managing different stages of development and allows for parallel development and stable releases.\n", "time": "0:00:09.659487", "input_tokens": 44, "output_tokens": 253}
{"context": "## Context\\nWe need to avoid side effects on configuration loading and prevent the need to fully configure the settings to run a subset of tests in projects using `konfetti`.\\n", "response": "## Decision\n\nAdopt a layered configuration approach in `konfetti` to separate base configurations from overrides and enable partial configuration loading.  This will involve introducing separate configuration objects for default values, environment-specific settings, and test-specific overrides. The system will prioritize loading test-specific configurations when running tests, falling back to environment-specific and then default values only when necessary. This layered approach will allow tests to define only the configurations they require without modifying or needing access to the complete application configuration.\n", "time": "0:00:02.991917", "input_tokens": 64, "output_tokens": 101}
{"context": "## Context\\nDriver interfaces were previously defined in the global namespace, and associated types were defined as `struct`s in the global namespace with a generic name like `tof`:\\n```\\nstruct tof\\n{\\nusing distance_t = uint16_t;\\nusing cb_t = stdext::inplace_function<void(distance_t)>;\\nstatic const distance_t INVALID_RANGE = UINT16_MAX;\\nenum class mode\\n{\\ndefaultRange = 0,\\nshortRange,\\nmedRange,\\nlongRange,\\n};\\n}''\\n```\\nInterface classes would inherit from these structs:\\n```\\nclass TimeOfFlight : public embvm::DriverBase, public embvm::tof\\n```\\nAnd the effect was similar to namespacing (`embvm::tof::mode`):\\n```\\nvirtual embvm::tof::mode mode(embvm::tof::mode m) = 0;\\n```\\nEssentially, we are recreating a feature that `namespace` already provides and complicating our inheritance chains.\\n", "response": "## Decision\n\nRefactor driver interfaces to use namespaces instead of global structs.  Specifically, each driver type (e.g., `tof`) will have its own namespace (e.g., `namespace drivers::tof`).  Associated types and enums will be defined within these namespaces. Interface classes will no longer inherit from these structs but will instead use the namespace directly for type definitions and enum class access.  This will simplify the inheritance hierarchy and improve code clarity by leveraging the standard namespacing mechanism.\n", "time": "0:00:10.070737", "input_tokens": 263, "output_tokens": 102}
{"context": "### Context\\nWhen executing code that is expected to fail in a test case, there is two ways to do this:\\n```php\\nfunction test_something(): void {\\n// ...\\ntry {\\n// the statement that fail\\n$this->fail();\\n} catch (Exception $e) {\\n// ...\\n}\\n}\\n```\\nOr:\\n```php\\nfunction test_something(): void {\\n// ...\\n$this->expectException($exception)\\n// the statement that fail\\n}\\n```\\n### Decision\\nAs recommended by [Sebastian Bergmann][sebastian-bergmann] in\\n[this article][phpunit-exception-best-practices], since in both cases a PHPUnit specific API is\\nnecessary, the decision taken is to leverage the `expectException*()` API when possible.\\nA pull request to fix this practice in the whole codebase may be done but has not been made\\nmandatory. New pull requests though should stick to this practice.\\n### Status\\nAccepted ([#1090][1090])\\n[sebastian-bergmann]: https://thephp.cc/company/consultants/sebastian-bergmann\\n[phpunit-exception-best-practices]: https://thephp.cc/news/2016/02/questioning-phpunit-best-practices\\n[1090]: https://github.com/infection/infection/pull/1061\\n", "response": "### Decision\nUse PHPUnit's `expectException*()` API whenever possible for asserting exceptions in tests, instead of using `try-catch` blocks with `$this->fail()`. While both approaches rely on PHPUnit APIs, `expectException*()` is the recommended practice according to Sebastian Bergmann.  A full codebase sweep to enforce this is encouraged but not mandatory at this time.  All new pull requests should adhere to this practice.\n", "time": "0:00:03.184727", "input_tokens": 348, "output_tokens": 90}
{"context": "## Context\\nWhen developing new algorithms and features for PACE-related software,\\ndevelopers often create useful demo/visualisation scripts for their own use.\\nThese scripts could be useful or interesting for other developers, and are\\nimportant for reproducibility or justifying design decisions. They should be\\nstored somewhere in version control so that they can be easily accessed by any\\ndevelopers and referred to later. However, they are not intended for general\\nuse and will not be actively maintained or tested. There are 2 main options:\\n* Store them in a `dev_scripts` directory in each separate project repository\\n* Store them in a `scripts` directory in `pace-developers`\\nIf they're in the `dev_scripts` directory for each project repository:\\n+ All in one place\\n+ Scripts will be close to the code they are used for\\n- Scripts may not work with the version of the code they are distributed with\\n- It's unclear where scripts that use more than one project would go\\n- Despite the folder being called `dev_scripts` people might expect the scripts\\nto actually work as they're in the main project repository\\nIf they're in a `scripts` directory in `pace-developers`:\\n+ They can be kept close to the decision-making developer documentation that\\nthey support\\n+ A version can be specified for any project dependencies\\n", "response": "## Decision\n\nWe will store developer scripts in a dedicated `scripts` directory within the `pace-developers` repository.  This central location offers better discoverability and allows scripts to specify project dependency versions explicitly, addressing concerns about compatibility. While keeping scripts close to relevant project code might seem appealing, the drawbacks of potential confusion with production code, unclear ownership of multi-project scripts, and compatibility issues outweigh this benefit. This approach also aligns well with the supporting role these scripts play in developer documentation and decision-making processes also housed within the `pace-developers` repository.\n", "time": "0:00:03.296116", "input_tokens": 327, "output_tokens": 115}
{"context": "**Context**: We know there will be users of these C++ libraries who want to use\\nC++ exceptions as well as those who are not able to. Our C++ libraries must work\\nfor all of our users, regardless of their ability to use exceptions.\\n**Decision**: None of our APIs will throw exceptions to indicate errors.\\nInstead, our APIs will typically report errors to callers by returning a\\n`Status` or `StatusOr<T>` object, unless the library we're using has another\\nnon-throwing way to report errors (e.g., [badbit][badbit-link] in the standard\\nI/O library).\\n**Consequences**: This decision will result in a single set of APIs and a\\nconsistent vocabulary for all users, whether or not they choose to compile with\\nexceptions. This decision does not prevent callers from using exceptions in\\ntheir own code.\\nA downside of this decision is that our APIs will not be natural or idiomatic\\nfor the [50+%][survey-link] of users who might prefer exceptions for error\\nreporting.\\nChanging existing APIs from throwing exceptions to returning `StatusOr<T>` is a\\nbreaking change. As of this writing (Jan 2019), this project has a\\n[Google Cloud Storage][gcs-link] component that is at the Alpha quality level,\\nand a [Google Cloud Bigtable][bigtable-link] that is already at the Beta quality\\nlevel. We plan to immediately change the API for Google Cloud Storage. We have\\nno immediate plans to change the API for Cloud Bigtable. We will communicate a\\ntimeline to change this API in a separate document.\\n[badbit-link]: https://en.cppreference.com/w/cpp/io/ios_base/iostate\\n[bigtable-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/bigtable\\n[gcs-link]: https://github.com/googleapis/google-cloud-cpp/tree/main/google/cloud/storage\\n[survey-link]: https://isocpp.org/blog/2018/03/results-summary-cpp-foundation-developer-survey-lite-2018-02\\n", "response": "## Decision\n\nOur C++ libraries will not throw exceptions to indicate errors.  Errors will be reported using `Status` or `StatusOr<T>` return values, or through existing non-throwing mechanisms provided by the underlying libraries we depend on (e.g., `badbit` for I/O operations).\n", "time": "0:00:06.692067", "input_tokens": 522, "output_tokens": 65}
{"context": "## Context and Problem Statement\\nNumber of module count has been increased our build times. Enabling unused plugins and using **gradle.kts** is causing to longer build times.\\n", "response": "## Decision\n\nWe will optimize build times by disabling unused plugins, migrating from **gradle.kts** (Kotlin DSL) to **build.gradle** (Groovy DSL), and investigating module dependencies to identify potential areas for decoupling or restructuring.  We will prioritize the switch to Groovy DSL due to the currently observed performance overhead of Kotlin DSL, and address plugin usage and module restructuring in subsequent steps.  This decision aims to improve developer productivity and reduce CI/CD pipeline durations.\n", "time": "0:00:32.210889", "input_tokens": 62, "output_tokens": 97}
{"context": "## Context and Problem Statement\\nIn order to simplify the tracking consent build and deploy process and\\nmake integrating with tracking consent less surprising, should\\nthe configuration of the GTM container used by tracking consent be via\\ndata attributes rather than separate bundles?\\n## Decision Drivers\\n* The need to keep things simple for service developers\\n* The need to improve the operability of tracking consent\\n", "response": "## Decision\n\nWe will configure the GTM container used by tracking consent via data attributes instead of separate bundles.  This approach prioritizes simplicity for service developers by centralizing configuration and reducing the number of moving parts. It also contributes to improved operability by making the configuration more readily accessible and understandable within the existing codebase, simplifying debugging and maintenance.\n", "time": "0:00:18.102102", "input_tokens": 109, "output_tokens": 71}
{"context": "## Context\\nArrow functions are a much more natural way to reduce visual noise in most contexts in JavaScript.\\n", "response": "## Decision\nWe will adopt arrow functions as the standard for defining anonymous functions and callbacks in our JavaScript codebase, except where their usage introduces ambiguity or conflicts with established patterns within specific libraries or frameworks.\n", "time": "0:03:07.741078", "input_tokens": 49, "output_tokens": 41}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "response": "## Decision\n\nWe will use Architectural Decision Records (ADRs) as the mechanism for documenting significant architectural decisions.  These records will be stored as Markdown files in a dedicated `/docs/adr` directory within the project's version control system.  We will follow the MADR (Markdown Architectural Decision Records) template.\n", "time": "0:00:11.316414", "input_tokens": 43, "output_tokens": 65}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "response": "## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight text files stored in a dedicated `docs/adr` directory within the project's version control system (e.g., Git).  Each decision will be documented in a separate Markdown file following a consistent template (e.g., MADR).  This allows us to track the decisions, their rationale, and their consequences in a transparent and auditable way.\n", "time": "0:00:19.937437", "input_tokens": 43, "output_tokens": 89}
{"context": "## Context\\nI need to implement two special pages, the privacy policy page and the about me page.\\nThey can't be in the map, I don't want them there because the user would\\nbe forced to read them when scanning the spiral.\\n", "response": "## Decision\n\nThe privacy policy and about me pages will be implemented as separate routes outside the main application map.  They will be accessible through dedicated links in the header/footer or a dedicated menu, rather than being integrated into the main navigational flow represented by the spiral.\n", "time": "0:00:52.435269", "input_tokens": 82, "output_tokens": 55}
{"context": "## Context\\nThe current request builder APIs are not able to handle some odata requests like:\\n- query navigation properties `GET /People('scottketchum')/Friends`\\n- getting \"raw value\" of a property `/People('scottketchum')/$value`\\n", "response": "## Decision\n\nWe will extend the existing request builder APIs to support OData features like querying navigation properties and retrieving raw values. This will involve adding new methods and parameters to the existing builder classes to accommodate the specific syntax and semantics of these OData operations.  We will prioritize support for the identified use cases (querying navigation properties like `/People('scottketchum')/Friends` and retrieving raw values like `/People('scottketchum')/$value`) and ensure backward compatibility with existing API usage.\n", "time": "0:00:03.479192", "input_tokens": 88, "output_tokens": 104}
{"context": "## Context\\nFrequency Capping allows Content Managers to limit the number of\\nimpressions or interactions users have with content. Is a widely\\navailable tool in Publishing Platforms.\\nIt's usually developed on the server side where the system can decide\\nhow many times to serve the content to the requesting users which we\\ncall \"Global Frequency Capping\". Additionally the system may be able\\nto limit the number of impressions per user which we call \"Local\" or\\n\"User Frequency Capping\".\\nFor example a Content Piece can be set to 1,000,000 Global Impressions\\nand 1 Impression per User, thus indirectly driving 1,000,000 different\\nusers to this Content.\\nThis functionality has been lacking from the Snippet Service due to\\ntechnical limitations imposed by the way metrics were collected and\\ncontent selection was handled on the client side. The latest\\ndevelopments in Firefox Messaging Center and the Firefox Telemetry\\nPipeline unblock this capability. [0]\\n", "response": "## Decision\n\nWe will implement both Global and User Frequency Capping within the Snippet Service.  Leveraging the new capabilities of Firefox Messaging Center and the Firefox Telemetry Pipeline, the Snippet Service will now track impressions and enforce frequency limits server-side. This allows for more accurate and consistent capping compared to the previous client-side approach.  Global capping will limit the total number of impressions across all users, while User capping will limit the number of impressions per individual user.\n", "time": "0:00:04.486005", "input_tokens": 246, "output_tokens": 97}
{"context": "## Context\\nThe application is about finding services closest to the search point. All\\nservices have a co-ordinate in lat/lon format. Currently the application\\nrequests either an out-code or a postcode as the search point. The submitted\\npostcode needs to be resolved to a lat/lon that can be used to query against.\\n", "response": "## Decision\n\nUse a third-party geocoding API to resolve postcodes and outcodes to latitude/longitude coordinates.  This will avoid the need to maintain our own postcode database and ensure we have up-to-date and accurate location data. We will evaluate several providers (e.g., Google Maps Geocoding API, OpenCage Geocoder, etc.) based on accuracy, cost, performance, and terms of service.\n", "time": "0:00:02.790676", "input_tokens": 99, "output_tokens": 87}
{"context": "## Context or problem to solve\\n* How should git histories look like?\\n* What format should we use to write commit messages?\\n", "response": "## Decision\n\nWe will adopt the Conventional Commits specification (https://www.conventionalcommits.org/) for all commit messages.  This provides a structured format that allows for automated tooling around changelog generation, release versioning, and other development workflows.  Git histories should, therefore, reflect a clear and consistent structure of these conventional commits.\n", "time": "0:00:34.108333", "input_tokens": 55, "output_tokens": 69}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "response": "## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documentation for capturing significant architectural decisions along with their context and consequences.  Each decision will be recorded in a separate Markdown file following a consistent template (e.g., MADR).  These files will be stored in a dedicated `docs/adr` directory within the project's version control system.\n", "time": "0:00:17.291343", "input_tokens": 43, "output_tokens": 76}
{"context": "## Context\\nWe need to have a base programming language as our first choice to implement the various functionalities. Other languages may be used as needed with proper justification.\\n", "response": "## Decision\n\nWe will use Python as the primary programming language for this project.\n", "time": "0:02:20.470124", "input_tokens": 60, "output_tokens": 17}
{"context": "## Context\\nAs part of our efforts around containerized deployment of the Alfresco Digital Business Platform we need to standardize on a Docker Image Registry that Alfresco engineering teams, other internal groups in the organization, and customers and partners can use to publish and consume Docker images.  We can describe those as 'internal' and 'protected' tiers.\\nThe Nexus3 implementation in use at the time of writing does not meet our requirements around access control, security scanning, scalability and global performance, usability, or maintainability.\\nOur IT resources are currently stretched very thin and we should avoid adding another system for them to deploy and maintain if possible.\\n", "response": "## Decision\n\nWe will adopt Amazon Elastic Container Registry (ECR) as our Docker Image Registry for both internal and protected tiers.  This leverages existing AWS infrastructure and tooling, minimizing new administrative overhead for our IT team. ECR provides the required access control mechanisms, integrates well with other AWS services for security scanning (e.g., ECR vulnerability scanning, AWS Security Hub), offers high scalability and global performance through its integration with CloudFront, and provides a relatively simple and user-friendly interface.  Further, utilizing ECR aligns with our broader cloud strategy and avoids the operational burden of managing a separate registry solution like Nexus3.\n", "time": "0:00:03.483114", "input_tokens": 157, "output_tokens": 126}
{"context": "## Context and Problem Statement\\nThe API has to be documented in human and machine readable form. If the API is documented in machine readable form, the automatic generation of client services is possible.\\n## Decision Drivers\\n* readable API documentation\\n* effort of manually creating client services\\n", "response": "## Decision\n\nWe will use OpenAPI 3.0 to document the API.  This standard provides both human-readable documentation (e.g., via Swagger UI) and a machine-readable specification that enables automated client SDK generation.\n", "time": "0:00:01.995411", "input_tokens": 86, "output_tokens": 48}
{"context": "## Context and Problem Statement\\nThe PayPal IPNs on Fundraising Frontend started to fail after a deployment\\nand was only noticed some days after when the Project Manager needed to\\nexport the data. Upon investigation, it was discovered that:\\n* Error logging was inactive on the application. (Now fixed)\\n* We can\u2019t debug using the responses our system returned to PayPal as\\nwe don\u2019t have access to the IPN log.\\nThis led to a situation where we couldn't get the information required\\nto debug the error. It was suggested we queue all incoming requests from\\nPayPal on our own system for processing by our system.\\n## Decision Drivers\\n* **Transparency**: If our system fails we would have a stored queue to\\nuse for debugging.\\n* **Automation**: The IPNs wouldn't need to be fired again once an error\\nbecomes fixed as our system would resume processing the queue.\\n", "response": "## Decision\n\nImplement a queueing system for incoming PayPal IPNs.  All incoming IPNs will be immediately placed onto the queue, and a separate worker process will consume the queue and process the IPNs.  This will allow us to retain a record of all incoming IPNs regardless of whether our processing logic succeeds or fails.\n", "time": "0:00:14.320271", "input_tokens": 221, "output_tokens": 67}
{"context": "## Context\\nWe need to deploy the Broker somewhere.\\nThe Broker implements the service broker API to generate per-application syslog\\ndrain URLs (Adapter URLs).\\nThe Adapter is written in Go.\\nThe Broker is written in Go.\\nThe Adapter runs as a lambda in AWS alongside the CSLS infrastructure.\\nWe have a pipeline to continuously build, test, deploy the Adapter to lambda.\\n", "response": "## Decision\n\nWe will deploy the Broker as a separate Lambda function in AWS, alongside the Adapter and CSLS infrastructure. This leverages our existing Go build/test/deploy pipeline and keeps the related components co-located for simplified management and reduced latency.\n", "time": "0:00:05.225656", "input_tokens": 110, "output_tokens": 52}
