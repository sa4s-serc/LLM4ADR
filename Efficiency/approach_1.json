{"context": "## Context and Problem Statement\\nAs any modern system, Volley Management faces a problem of concurrent changes to data and we need to support such scenario.\\nWe explicitly do not consider an option to go without concurrency checks - time will tell if it is a good decision).\\n## Decision Drivers <!-- optional -->\\n* Performance - decision should support high throughput scenarios\\n* Maintainability - amount of code needed to write should be minimized\\n", "response": "## Decision\n\nWe will use optimistic locking for concurrency control.  This approach checks for conflicts by comparing a version number or timestamp at the time of update with the value when the data was originally retrieved. If the values match, the update proceeds.  If not, the update is rejected, signaling a concurrency conflict.\n", "time": "0:00:03.145097"}
{"context": "## Context\\nWe use RDS for tenant databases. Tenants can bind more than one application to a database instance created via the services console. Database migrations were broken when a binding was removed and re-added, or when another bound application other than the one that created the database tables attempted to modify them.\\nPreviously the RDS broker PostgreSQL engine copied the MySQL engine and granted all rights on the database to the newly created user. In PostgreSQL this will give the user rights to create tables, but because it has a more finely-grained permission model than MySQL this does not give the user rights on existing tables, or tables newly created by other users.\\nOnly the owner of a table can alter/drop it, and you cannot grant this permission to other users. Users who are the owners of tables cannot be removed until the table ownership is changed.\\nWe attempted to work around the PostgreSQL permissions system in the following ways:\\n* Using [`ALTER DEFAULT PRIVILEGES`](https://www.postgresql.org/docs/9.5/static/sql-alterdefaultprivileges.html) on every newly created user to `GRANT ALL ON PUBLIC` - this means that every user can `SELECT`, `INSERT`, and `DELETE`, but because only the table owner can `ALTER` or `DROP` this will not allow other bound users to run migrations. This is also limited to the `PUBLIC` (default) schema, so would fail to work for any applications that have custom schemas.\\n* Making the group the owner of the `PUBLIC` schema. This allowed members of the group to `DROP` tables within the schema, but still did not allow them to `ALTER` these tables.\\n* Creating a \"parent\" group that is given ownership of all tables, sequences, and functions. New users are granted rights on this group and thus can carry out all owner actions on group-owned items. A [DDL event trigger](https://www.postgresql.org/docs/9.5/static/event-trigger-definition.html) using the built-in `REASSIGN OWNED BY` command handed over ownership of all resources owned by the current user to the parent `$DBNAME_owner` group. This worked perfectly on all the PostgreSQL databases we tested with, however RDS does not allow use of event triggers so it was unworkable in production.\\n", "response": "## Decision\n\nImplement a permissions model in PostgreSQL based on schemas.  Each application binding will be assigned its own schema within the shared tenant database.  The application will be granted full ownership and control privileges within its designated schema. This allows complete isolation between application bindings and avoids the permission conflicts encountered with shared table ownership.  Migrations will operate solely within the application's own schema.\n\nThis approach avoids relying on features unavailable in RDS (event triggers) and provides a cleaner, more manageable permission structure compared to attempting to manipulate group ownership or default privileges in the `public` schema.\n", "time": "0:00:03.150399"}
{"context": "## Context and Problem Statement\\nRunning Service Tests requires PostgreSQL. To ensure an instance is always up and running a container has to be manually launched or started automaticially.\\n", "response": "## Decision\n\nA Docker container running PostgreSQL will be integrated into the Service Tests workflow.  This container will be automatically started before the tests run and stopped after the tests complete.\n", "time": "0:00:01.412403"}
{"context": "## Context\\nWe need a system to manage the lifecycle of a grant application.\\n", "response": "## Decision\n\nWe will develop a custom web application to manage the grant application lifecycle. This application will handle all stages, from initial submission through review, award, and closeout.\n", "time": "0:00:01.451078"}
{"context": "## Context\\nWe are using raw `[]byte` for merkle proofs in `abci.ResponseQuery`. It makes hard to handle multilayer merkle proofs and general cases. Here, new interface `ProofOperator` is defined. The users can defines their own Merkle proof format and layer them easily.\\nGoals:\\n- Layer Merkle proofs without decoding/reencoding\\n- Provide general way to chain proofs\\n- Make the proof format extensible, allowing thirdparty proof types\\n", "response": "## Decision\n\nWe will adopt the `ProofOperator` interface for handling Merkle proofs in `abci.ResponseQuery`. This interface will allow users to define custom proof formats and combine multiple proofs (potentially of different types) through a layered approach.  The existing raw `[]byte` approach will be deprecated in favor of this more flexible and extensible solution.\n", "time": "0:00:02.338535"}
{"context": "## Context\\nAzure Cloud Services and Microsoft Dynamics 365 have been proposed to support the CCP case & contact management, therefore, to maximise vendor reuse\\n& interoperability, Azure's identity management system will be used to provide authentication for the frontend application (back stage/SEPA users).\\n## Decision Drivers\\nSEPA uses Active Directory for it's existing internal account management so providing one unified single sign on login would improve security and usability.\\n", "response": "## Decision\n\nWe will utilize Azure Active Directory (Azure AD) as the identity provider for the frontend application authenticating both backstage/SEPA users and users accessing the application through Microsoft Dynamics 365. This leverages Azure's existing integration with Dynamics 365 and provides a single sign-on (SSO) experience for SEPA users through their existing Active Directory accounts.\n", "time": "0:00:02.361405"}
{"context": "## Context\\nAs it's hard to understand code, it is crucial that anybody can easily\\nunderstand the code you're working on. This applies to all levels of code, not\\nonly the code itself, but groups of code, complete applications and groups of\\napplications.\\n", "response": "## Decision\n\nWe will adopt a multi-layered approach to code clarity, encompassing the following strategies:\n\n1. **Code-Level Clarity:** Enforce coding style guides and best practices using linters and automated formatters.  Promote meaningful variable and function names, concise functions with single responsibilities, and comprehensive inline documentation where necessary. Encourage the use of static typing where applicable.\n\n2. **Module/Component-Level Clarity:**  Organize code into well-defined modules and components with clear interfaces and responsibilities.  Utilize design patterns judiciously to improve structure and predictability.  Provide clear documentation for each module outlining its purpose, usage, and dependencies.\n\n3. **Application-Level Clarity:** Employ architectural diagrams and documentation to illustrate the high-level structure and interactions within the application.  Use consistent naming conventions across the application.  Implement robust logging and monitoring to facilitate debugging and understanding of application behavior.\n\n4. **System-Level Clarity:**  Document the relationships and interactions between different applications within the system. Utilize service discovery and API documentation tools to improve discoverability and understanding of system components.  Employ system-level monitoring and tracing to understand the overall flow of data and interactions.\n\n5. **Ongoing Maintenance:**  Regularly review code and documentation to ensure clarity and accuracy. Encourage peer reviews and knowledge sharing sessions to promote collective code ownership and understanding.\n\n\nThis approach aims to ensure code clarity at all levels, enabling any developer to quickly understand and contribute to the codebase.\n", "time": "0:00:06.854834"}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "response": "## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture significant architectural decisions along with their context, rationale, and consequences.  These ADRs will be stored as Markdown files in a dedicated `/docs/architecture/decisions` directory within the project's version control system.  The ADRs will follow a consistent template (e.g., MADR or a similar structure) to ensure readability and maintainability.\n", "time": "0:00:02.510936"}
{"context": "## Context\\nThe rig defined at [Bookit Infrastructure](https://github.com/buildit/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.\\nWhilst it's rather generic as it is, it is specific to Bookit's needs.\\nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).\\nThe only way to capture that is via branches which can be hard to discover.\\nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\\n", "response": "## Decision\n\nWe will create a new, centralized repository for managing AWS Bare Metal Rig configurations. This repository will use a modular approach to define rig components, allowing for easy customization and combination of features.  Instead of using branches for variations (e.g., ECS EC2 vs. ECS Fargate), we will leverage configuration files (e.g., YAML, JSON) and potentially templating tools (e.g., Helm, Terraform modules) to define different profiles.  This new repository will represent the canonical \"upstream\" for AWS Bare Metal Rig definitions, facilitating the creation of new rigs based on the latest best practices and features.  Specific project instances, like Bookit Infrastructure, can then consume and extend these configurations as needed.  This allows Bookit to maintain project-specific customizations while still benefiting from upgrades and improvements to the core rig definition.\n", "time": "0:00:04.402504"}
{"context": "## Context\\nWe have inherited a relatively large and complex legacy code base, mostly written in PHP.\\nPHP [appears to be on a downwards trend as a language](https://pypl.github.io/PYPL.html?country=GB),\\nespecially in contrast with Python. It's likely it will become increasingly difficult\\nto find good PHP developers in future.\\nAnecdotally, PHP is not seen as a desirable language for developers to work with. It doesn't\\nhave the cool factor of newer languages like golang; nor the clean syntax and API of\\nlanguages of similar pedigree, such as Python.\\nOur code base is also showing its age somewhat. Some of the libraries are starting to rot.\\nA mix of contractors and developers working on the code base over several years has\\nresulted in a mix of styles and approaches. While we have already cleared out a lot\\nof unused and/or broken code, there is likely to be more we haven't found yet.\\nWe are also lagging behind the latest Design System guidelines, as our application was one\\nof the first to go live, before the current iteration of the Design System existed.\\nThis means that any changes to design have to be done piecemeal and manually: we can't\\nsimply import the newest version of the design system and have everything magically update.\\nThis combination of factors means that the code base can be difficult to work with:\\nresistant to change and easy to break.\\n", "response": "## Decision\n\nWe will incrementally migrate the existing PHP application to Python.  This will be achieved through a combination of strangling the legacy application with new Python services and rewriting existing functionality in Python when changes are required.  We will not attempt a \"big bang\" rewrite.  This approach allows us to:\n\n* **Mitigate risk:** By migrating piece by piece, we can test and deploy smaller changes, reducing the chances of large-scale failures.\n* **Maintain functionality:** The application can continue to function while parts are being migrated.\n* **Improve developer experience:**  Introducing Python will make the codebase more attractive to current and future developers, and potentially improve development speed and code quality.\n* **Address technical debt iteratively:**  Rewriting sections in Python allows us to refactor and improve the codebase as we go, addressing design system inconsistencies and removing outdated libraries.\n* **Spread the cost:**  The migration can be budgeted over time rather than requiring a large upfront investment.\n\nWe will prioritize migrating functionality based on a combination of business value, development cost, and the degree of technical debt in the existing code.  New features will be implemented in Python, while existing features will be migrated as opportunities arise (e.g., during bug fixes, feature enhancements, or design system updates).  A clear roadmap and migration plan will be developed and maintained throughout the process.\n", "time": "0:00:06.575081"}
{"context": "## Context\\n- Originally we wanted to use just PointerEvents API for capturing events because the API covers mouse and pointer inputs.\\n- Unfortunately Safari and iOS have very limited support. Safari does currently have an experimental API for it but the movementX and movementY properties are always 0.\\n- MovementX/Y is a clean browser only method for determining distance without having to track previous coordinates.\\n- Mobile is a secondary concern for this app, but pointer events on desktop Safari is also unsupported :(\\n", "response": "## Decision\n\nWe will use the MouseEvents API for capturing mouse events alongside TouchEvents API for capturing touch events.  While this introduces some redundancy in event handling logic, it ensures compatibility across all target browsers and devices, including Safari and iOS. We will implement a thin abstraction layer over both APIs to unify event data and simplify application logic.  This approach prioritizes broad compatibility over the conciseness offered by the PointerEvents API, acknowledging the importance of supporting Safari and providing a consistent user experience across platforms. The added complexity of handling two separate APIs is deemed acceptable given the limitations of PointerEvents support.\n", "time": "0:00:03.183428"}
{"context": "## Context\\nAs we formally establish Dapr component version, we need to decide if we want to couple CLI versions with runtime versions.\\n", "response": "## Decision\n\nWe will **not** couple Dapr CLI versions with Dapr runtime versions.  Instead, the CLI will be designed to be forward and backward compatible with a reasonable range of runtime versions.  This allows for independent evolution and release cycles for both the CLI and the runtime, providing greater flexibility and agility.  Specific compatibility ranges will be documented and tested.\n", "time": "0:00:02.293351"}
{"context": "## Problem\\nMost of the credentials/information of the environment (like a service address) are only available as soon the application is deployed.\\n", "response": "## Decision\n\nWe will utilize environment variables to manage environment-specific configuration.  These variables will be injected into the application at runtime during the deployment process.  No sensitive information will be hardcoded within the application's source code or configuration files.\n", "time": "0:00:01.802077"}
{"context": "## Context and Problem Statement\\nIn order to deliver BFI's IIIF Universal Viewer auditing platform, an\\nidentity and access management solution must be provisioned which\\nsupports user creation and registration, user management, and\\nauthentication and authorisation.\\n## Decision Drivers\\n* Ease of initial deployment and configuration of the solution.\\n* Ongoing hosting and maintenance costs of the solution.\\n* Availability of core features to satisfy the project requirements with\\nno / minimal costs.\\n", "response": "## Decision\n\nUse Auth0 as the identity and access management solution.  Auth0 provides a robust, cloud-based platform with a generous free tier that covers the anticipated user base and usage patterns of the BFI's IIIF Universal Viewer auditing platform. Its features directly address the project's requirements for user creation, registration, management, authentication, and authorization.  Furthermore, Auth0's ease of integration and comprehensive documentation minimize initial setup complexity and reduce long-term maintenance overhead.\n", "time": "0:00:02.766477"}
{"context": "## Context\\nBookit needs a persistence mechanism.  There are many to choose from that fit an application's needs.  Currently, we believe a SQL/RDBMS approach fits better than NoSQL.  There's not a lot of context to add to that, just a quick poll of the engineers when we kicked off the project.  With that in mind, we wanted something hosted/PaaS.\\nGiven we're in AWS, RDS is an obvious choice.  We don't currently have a preference for DB vendor/implementation, but are drawn to open source and free.  MySql and PostgreSql fit that criteria.\\nFurther, AWS RDS has their own MySql implementation which provides much better performance and up to the minute backups with no degredation for fractions of a penny/hr more than the standard MySql over RDS.  And while Bookit's usage might not warrant the need for higher performance, there is always a need for high availability and Aurora provides that in a very hands off way.  There is also an Aurora implentation for PostgreSql but at the time of this decision, that is in Preview so we decided to skip it.\\n", "response": "## Decision\n\nUse AWS Aurora MySQL for the persistence layer. This leverages a managed, cloud-native database service, providing high availability, performance, and automated backups. We selected the MySQL-compatible Aurora offering due to its mature feature set and the availability of a performant AWS-specific implementation.  While PostgreSQL on RDS was considered, the Aurora PostgreSQL offering being in preview at the time of this decision led us to choose the more stable and readily available Aurora MySQL.\n", "time": "0:00:02.869771"}
{"context": "## Context\\nStoring data in hybrid storage (off-chain and on-chain) presented problems with data inconsistency when some data was written off-chain but it's on-chain reference hasn't been committed yet. This would cause some data being corrupted under certain conditions.\\n", "response": "## Decision\n\nStore all data on-chain until a robust and efficient solution for hybrid storage is implemented and thoroughly tested. This prioritizes data consistency and integrity over potential cost savings offered by hybrid storage.  We will revisit the hybrid storage approach in the future when suitable solutions for managing distributed transactions and ensuring data consistency become available and mature.  This could involve techniques like optimistic updates with robust rollback mechanisms, two-phase commit protocols adapted for blockchain environments, or dedicated chain-aware middleware.\n", "time": "0:00:02.729574"}
{"context": "## Problem\\nOnly plugins like `dump` and `quickdump` are able to represent any KeySet\\n(as they are designed to do so). Limitations of other storage plugins are\\ne.g., that not every structure of configuration is allowed.\\nSome of these limitations were documented `infos/status`, others were not.\\n", "response": "## Decision\nWe will introduce a new plugin called `keyset` specifically designed to store arbitrary KeySets. This plugin will serve as a canonical representation of any KeySet and will be used as a reference point for other plugins.  We will update the documentation of all plugins, clearly specifying their limitations in terms of the KeySet structures they can handle. The `infos/status` output will be extended to include a structured representation of these limitations for programmatic access.  Existing plugins (`dump`, `quickdump`) will be refactored, if necessary, to ensure compatibility with the `keyset` plugin\u2019s output.\n", "time": "0:00:03.270113"}
{"context": "## Context\\n[Command-query separation](https://martinfowler.com/bliki/CommandQuerySeparation.html) states that every method should\\neither be a command that performs an action, or a query that returns data to the caller, but not both.\\nAdopting command-query separation makes a clear separation of methods that change state from those that don't. So we can\\nuse queries with much more confidence, and only be careful with commands orchestration.\\nCommands and queries terminology is already used in the `menu-generation` application.\\n", "response": "## Decision\n\nWe will adopt command-query separation throughout the codebase.  All new methods will be written as either a command or a query.  Existing methods will be refactored to conform to this principle as they are touched during normal development and maintenance. We will prioritize refactoring methods related to critical user flows or areas with high complexity.  A small allowance will be made for trivial getter methods (e.g., simple property accessors) where strict adherence adds significant boilerplate without substantial benefit.\n", "time": "0:00:02.877023"}
{"context": "## Context\\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\\n", "response": "## Decision\n\nEmbed version and UTC datetime information into application logs.  Specifically, include the application's version string (e.g., git commit hash, release tag) and the current UTC timestamp in each log entry.  This should apply to all logging levels.\n", "time": "0:00:01.841923"}
{"context": "## Context\\nThe software could be developed in one big (Gradle) project.\\nThis would make integration easier.\\nAt the same time this would make re-use of the code outside of this project harder.\\nOne big project would probably lead to worse code since there would not be the need to have defined API boundaries.\\n", "response": "## Decision\n\nThe software will be developed as a set of smaller, independent (Gradle) modules/libraries with well-defined APIs.  While this adds some initial overhead in setting up inter-module dependencies and managing versioning, it prioritizes code reusability, maintainability, and enforces cleaner architectural boundaries. The increased effort for integration testing within the main application will be mitigated through robust module-level testing and clear API documentation.  This approach also allows for easier onboarding of new developers and facilitates parallel development efforts across different modules.\n", "time": "0:00:16.301077"}
{"context": "## Context\\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.\\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.\\nWe need the modeling of a relationship to be a generic structure that can support both these use cases.\\n", "response": "## Decision\n\nWe will implement a generalized \"Relationship\" model that can represent both the parent-child relationship and the family member-head of household relationship.  This model will be flexible enough to accommodate future relationship types as needed. The relationship will be represented by a joining table with columns to store the two individuals related, the type of relationship (e.g., \"parent-child\", \"family-head\"), and potentially other relevant attributes like start/end dates (for example, if a child is adopted out of the family). This approach avoids redundant relationship-specific logic and tables while providing a consistent framework for managing various relationships.  Navigation between mother and children during delivery and PNC visits will be facilitated through UI affordances leveraging this underlying relationship model.\n", "time": "0:00:05.996997"}
{"context": "## Context\\nWe need a some way to authenticate and manage Link Platform Adminstrators.  Administrators will need to log in to their Link Instances to manage data and configuration.\\n", "response": "## Decision\n\nWe will use Auth0 as our authentication and authorization provider for Link Platform Administrators.  Auth0 provides a robust, secure, and scalable solution that integrates well with our existing technology stack.  It offers features such as multi-factor authentication, role-based access control, and a customizable user interface. This allows us to delegate the complexities of identity management to a trusted third party while retaining granular control over access permissions within the Link Platform.\n", "time": "0:00:02.558907"}
{"context": "## Context\\nThe RE Autom8 team originally wrote and maintained this broker, in collaboration with Cyber Security. It is configured as a service broker in the Ireland and London regions of GOV.UK PaaS, and enabled for a few GDS-internal tenants. The code lives in a subdirectory of the `alphagov/tech-ops` repository, and the pipeline which builds and deploys it lives in the Tech Ops multi-tenant Concourse.\\n", "response": "## Decision\n\nMigrate the service broker out of the `alphagov/tech-ops` repository and into its own standalone repository.  This will provide clearer ownership and responsibility for the RE Autom8 team, simplify the deployment pipeline by decoupling it from the broader Tech Ops pipeline, and allow for more focused development and maintenance.  Furthermore, the broker will be deployed to its own dedicated Concourse pipeline to give RE Autom8 full control over its deployment lifecycle.\n", "time": "0:00:02.819647"}
{"context": "## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n", "response": "## Decision\nWe will adopt a \"wait-and-see\" approach and not make any significant architectural changes at this time. We will continue to monitor the situation and gather more data on usage patterns and performance bottlenecks.  If the anticipated issues materialize, we will revisit this decision within the next [Timeframe, e.g., three months/sprint].\n", "time": "0:00:02.265523"}
{"context": "## Context\\nNAV currently has Kafka running on-premise, but have plans to move everything to the cloud.\\nKafkarator will create an integration between applications and Kafka, removing some manual work and putting more configuration in code.\\nThe plan is hosted Kafka from Aiven, eliminating the need for maintaining our own Kafka cluster on-premise.\\n", "response": "## Decision\n\nMigrate the existing on-premise Kafka instance to a hosted Aiven Kafka instance and integrate Kafkarator to manage application integrations with Kafka.  This will eliminate the operational overhead of maintaining our own Kafka cluster and streamline the application integration process by codifying configuration.\n", "time": "0:00:01.839157"}
{"context": "## Context and Problem Statement\\nWe want to connect to Newslabs' shared STT service (named Newslabs PSTT).\\n", "response": "## Decision\n\nWe will integrate with Newslabs PSTT using their provided API (assuming one exists, otherwise specify how the connection will be made e.g., gRPC, message queue etc.)  and the recommended client library if available. We will investigate any specific authentication and authorization requirements.  A fallback mechanism, such as a local STT engine or alternative provider, will be considered to handle potential outages or performance issues with the shared service.\n", "time": "0:00:02.589126"}
{"context": "## Context\\nPIMS requires a database to store all property information.\\nThe data is relational, requiring constraints and must run within a Linux docker container on OpenShift.\\nAdditionally it must be supported by Entity Framework Core 3.1.\\n", "response": "## Decision\n\nUse PostgreSQL 12 running in a Linux Docker container deployed on OpenShift.  PostgreSQL is a robust, open-source relational database that supports the required constraints and is compatible with Entity Framework Core 3.1.  Using Docker allows for consistent deployment across environments and aligns with the existing OpenShift infrastructure.  Version 12 is chosen for its maturity and proven stability at the time of this decision.\n", "time": "0:00:02.699089"}
{"context": "## Context and Problem Statement\\n[ADR-0003](0003-threadsafe-interfaces.md) introduced support for \"thread-safe\\ninterfaces\" - possibly leading to the impression that there is such a thing as\\nnon-threadsafe interfaces and confusion about exactly what the attribute means.\\nHowever, the entire concept of non-threadsafe interfaces is a misconception -\\nthe Rust compiler insists that everything wrapped by uniffi is thread-safe -\\nthe only question is who manages this thread-safety. Interfaces which are not\\nmarked as thread-safe cause uniffi to wrap the interface in a mutex which is\\nhidden in the generated code and therefore not obvious to the casual reader.\\nThe `[Threadsafe]` marker acts as a way for the component author to opt out of\\nthe overhead and blocking behaviour of this mutex, at the cost of opting in to\\nmanaging their own locking internally. This ADR proposes that uniffi forces\\ncomponent authors to explicitly manage that locking in all cases - or to put\\nthis in Rust terms, that all structs supported by uniffi must already be\\n`Send+Sync`\\nNote that this ADR will hence-forth use the term `Send+Sync` instead of\\n\"Threadsafe\" because it more accurately describes the actual intent and avoids\\nany misunderstandings that might be caused by using the somewhat broad and\\ngeneric \"Threadsafe\".\\n## Decision Drivers\\n* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make\\nthem `Send+Sync`. We consider this a \"foot-gun\" as it may lead to accidentally\\nhaving method calls unexpectedly block for long periods, such as\\n[this Fenix bug](https://github.com/mozilla-mobile/fenix/issues/17086)\\n(with more details available in [this JIRA ticket](https://jira.mozilla.com/browse/SDK-157)).\\n* Supporting such structs will hinder uniffi growing in directions that we've\\nfound are desired in practice, such as allowing structs to use [alternative\\nmethod receivers](https://github.com/mozilla/uniffi-rs/issues/417) or to\\n[pass interface references over the FFI](https://github.com/mozilla/uniffi-rs/issues/419).\\n", "response": "## Decision\n\nUniffi will no longer support non-`Send+Sync` structs.  All structs passed across the FFI *must* be `Send+Sync`.  The `[Threadsafe]` attribute will be removed from the IDL.  This means component authors are explicitly responsible for ensuring thread safety of their components, typically by using appropriate internal synchronization primitives within their Rust implementation.  This change eliminates the hidden mutex wrapping previously performed by Uniffi and enforces a consistent and predictable threading model.\n", "time": "0:00:03.257485"}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "response": "## Decision\n\nWe will use Architectural Decision Records (ADRs) to document significant architectural decisions.  Each decision will be recorded in a separate Markdown file following a lightweight template (e.g., MADR).  These files will be stored in a dedicated `docs/adr` directory within the project's version control system.  This approach allows us to track the rationale behind decisions, facilitate communication and knowledge sharing, and provide a historical record of architectural evolution.\n", "time": "0:00:02.643311"}
{"context": "## Context and Problem Statement\\nWe found that without a standardised format our javascript files ended up with different\\nformats in different files or even multiple formats in the same file.  We also found that\\nour IDEs had different configurations which meant that using an autoformat tool would give\\ndifferent results when each of us do it.\\n## Decision Drivers\\n* We wanted to spend less time doing manual formatting\\n* We wanted to spend less time undoing autoformatting which had been applied to unchanged lines\\n* We wanted to see easily which lines had actually changed when reviewing PRs\\n* We wanted to avoid discussions about individual's preferences for particular\\n", "response": "## Decision\n\nWe will enforce a standard code style using Prettier and ESLint.  Prettier will be used to automatically format the code, ensuring consistency across all files. ESLint will be used alongside Prettier to enforce coding style rules and identify potential issues. We will configure our IDEs to automatically apply Prettier formatting on save.  We will also integrate Prettier and ESLint into our CI/CD pipeline to enforce the standard format before code is merged.  Finally, we will document the chosen configuration and encourage all team members to adopt the same IDE settings to maximize consistency.\n", "time": "0:00:03.187469"}
{"context": "## Context\\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\\nmakes usage and documentation easier.\\n", "response": "## Decision\n\nIntroduce named constants for the ordinal parameter values of operations with multiple modes. These constants should be clearly defined and documented within the codebase (e.g., using enums or constant variables).  The ordinal parameter itself will remain, but documentation and usage examples should primarily refer to the named constants for clarity.\n", "time": "0:00:02.443781"}
{"context": "## Context\\nHaving decided on [ADR 2](adr-002.md), we foresee that the Alfa code base will be both significantly larger and more complex than the code base of our proprietary engine. This is due to the fact that we will have to implement a great deal of APIs that we have previously relied on the browser implementations of. Coupled with the fact that the most common type of bug we have encountered in the past has been stray `undefined` or `null` values and APIs receiving incorrect parameters, plain JavaScript, even if covered by tests, is simply not an option moving forward. We will either need tooling that can sanity check our JavaScript or move to a language with a proper type system that can enforce API contracts.\\nHowever, given that browsers are still part of the equation, Alfa must be able to also run in a browser. This way, we ensure that we can implement tools such as our Chrome extension based on Alfa.\\n", "response": "## Decision\n\nWe will use TypeScript to develop the Alfa code base.  This decision addresses the concerns about code complexity, API contract enforcement, and the prevalence of `null`/`undefined` errors by leveraging TypeScript's static type system.  This approach allows for compile-time checks and improved code maintainability as the project scales.  The requirement for browser compatibility is also met, as TypeScript compiles to JavaScript, ensuring that Alfa can continue to function within a browser environment for tools like the Chrome extension.\n", "time": "0:00:02.888952"}
{"context": "## Context and Problem Statement\\nAll libraries use their own query syntax for advanced search options. To increase usability, users should be able to formulate their (abstract) search queries in a query syntax that can be mapped to the library specific search queries. To achieve this, the query has to be parsed into an AST.\\nWhich query syntax should be used for the abstract queries?\\nWhich features should the syntax support?\\n", "response": "## Decision\n\nWe will use a custom query syntax based on a simplified form of boolean logic with support for field-specific searches, phrase searches, and wildcard searches.  This syntax will be parsed into an Abstract Syntax Tree (AST) for further processing and translation to library-specific queries.\n\nThe chosen syntax will support the following features:\n\n* **Field-Specific Search:** `field:value` (e.g., `title:physics`, `author:\"Stephen Hawking\"`)\n* **Phrase Search:** `\"exact phrase\"` (e.g., `\"quantum mechanics\"`)\n* **Wildcard Search:** `*` (e.g., `astro*`, `title:astro*`)  Wildcards can be used at the beginning, end, or within a term.\n* **Boolean Operators:** `AND`, `OR`, `NOT` (e.g., `title:physics AND author:\"Stephen Hawking\"`, `astro* OR cosmology NOT \"black holes\"`)\n* **Parentheses for Grouping:** `()` (e.g., `(astro* OR cosmology) NOT \"black holes\"`)\n* **Case-Insensitive Search:** Searches should be case-insensitive by default.\n\nThis syntax balances expressiveness and ease of use, catering to both basic and advanced search needs.  The custom nature allows for flexibility and future extensibility as needed.  Parsing into an AST allows for efficient processing and transformation into various target query languages used by different libraries.\n", "time": "0:00:06.924771"}
{"context": "## Context\\nGovUK Notify is used as the email provider for Keycloak: [0012 Keycloak Email Provider](0012-keycloak-email-provider.md)\\nIt was decided to use a separate GovUK Notify service for each TDR environment as GovUK Notify does not have the concept of environments: [0014 GovUK Notify Multi-environment Configuration](0014-govuk-notify-multi-environment-configuration.md)\\nGovUK services have a \"trial mode\", and a \"live mode\".\\nThe trial mode has limits placed on who can receive emails, and the number of emails that can be sent.\\n", "response": "## Decision\n\nUse the GovUK Notify trial mode API keys for the development, integration, staging and UAT environments. Use the live mode API keys for the pre-production and production environments.  This allows for unrestricted testing in lower environments while maintaining the safeguards and deliverability of the live mode for pre-production and production.\n", "time": "0:00:02.132797"}
{"context": "## Context\\nTTA Smart Hub requires a continuous monitoring solution to ensure uptime and error resolution.\\n", "response": "## Decision\n\nWe will implement Prometheus as our primary monitoring solution, supplemented by Grafana for visualization and alerting.\n", "time": "0:00:01.188871"}
{"context": "## Context\\nWe want to enforce consistency in our code, and catch as many errors\\nautomatically as we are able to. Linting the code is good practice to achieve\\nthese aims. [Stylelint](https://stylelint.io/) is one of the more popular CSS\\nlinters with support for SASS, and is easily configurable for our purposes.\\n", "response": "## Decision\n\nWe will use Stylelint to enforce coding style and best practices for our CSS and SASS code.  We will integrate Stylelint into our development workflow, including pre-commit hooks and CI/CD pipelines, to ensure consistent code quality.  A shared Stylelint configuration file will be created and maintained to define the specific rules and conventions for the project.\n", "time": "0:00:02.234928"}
{"context": "## Context\\nPermissions on works and collections can come from two sources: 1) the person who authored the resource, such as the\\ndepositor or the proxy depositor; and 2) access controls (ACLs) that grant permissions based on user or group identity.\\nWhen determining who has access to a given resource, both these sources may need to be consulted.\\n", "response": "## Decision\n\nPermissions for works and collections will be determined by a combination of author-granted permissions and Access Control Lists (ACLs).  The system will first check for author-granted permissions. If no explicit permissions are granted by the author or if the requesting user is not covered by author-granted permissions, the system will then evaluate applicable ACLs.  If neither author-granted permissions nor ACLs grant access, the request will be denied.  This approach ensures that authorial intent is prioritized while still allowing for flexible administrative control via ACLs.\n", "time": "0:00:03.142853"}
{"context": "## Context\\nThe Data Migrate gem has caused us a number of issues in the past, it runs\\nsilently as part of a deploy, and this can result in surprising errors\\nduring a deploy. We've also had issues with the gem itself - most recently\\na bug in a new version causing strange errors in deployment.\\n", "response": "## Decision\n\nWe will remove the `DataMigrate` gem from our deployment process and replace it with a dedicated migration task managed through a separate process. This new process will be explicitly triggered and will not be part of the standard deployment pipeline.  This will give us greater control over data migrations, allow for more thorough testing and validation before execution, and provide clearer visibility into the migration status.\n", "time": "0:00:02.439180"}
{"context": "## Context\\nWe have some ways to perform downloading stable in the background, but system exposed a service called `DownloadManager`.\\nClient may request that a URI be downloaded to a particular destination file. The download manager will conduct the\\ndownload in the background, taking care of HTTP interactions and retrying downloads after failures or across connectivity changes and system reboot.\\nApps that request downloads through this API can register a broadcast receiver to handle when the download is progress, failure, completed.\\n", "response": "## Decision\n\nWe will utilize the system's `DownloadManager` for handling background downloads. This leverages a robust, platform-optimized solution that manages network interactions, retries, and persistence across system events. Client requests for downloads will be routed through this service, specifying the URI and desired destination file.  We will register a broadcast receiver to monitor download progress, completion, and failures, allowing us to update the user interface and handle any necessary post-download processing.  This approach minimizes development effort and relies on a well-tested component for a core application feature.\n", "time": "0:00:03.432757"}
{"context": "## Context\\nWe are doing a rewrite.\\nHolochain Go code shows many implicit dependencies between different modules and stateful objects. In conjunction with the complexity of a p2p network of agents, this leads to a level of overall complexity that feels too much to manage. A clean and fitting architecture for this Rust rebuild is needed.\\nHaving a single global state within the agent feels appropriate and even balancing the distributed nature of the network of agents.\\n", "response": "## Decision\n\nWe will adopt an Entity Component System (ECS) architecture for the Rust rewrite. This approach addresses the complexity concerns stemming from implicit dependencies and statefulness in the existing Holochain Go codebase.  The ECS model enforces a clear separation of concerns by decoupling data (Components), logic (Systems), and entities (identifiers).  This allows for greater modularity, testability, and maintainability.\n\nA single global `World` object will manage all entities, components, and systems within each agent.  This `World` will represent the single source of truth for the agent's internal state while still allowing for the distributed nature of the Holochain network. Systems will operate on components within the `World`, providing the necessary logic for agent behavior and interaction with the network.\n", "time": "0:00:04.035403"}
{"context": "## Problem\\nCurrently multiple warnings are saved in an elektra non-conforming array\\nnotation which is limited to 100 entries. The notation of `#00` is against\\nthe design [decision made](array.md).\\n", "response": "## Decision\nMigrate all warnings to a new, conforming Elektra array notation and remove the limitation of 100 entries.  The new notation will adhere to the established array design decision documented in `array.md`.\n", "time": "0:00:01.765340"}
{"context": "## Context\\nThe service needs a way to authenticate trusted school buying professionals and to restrict the majority of access to the public.\\nWe believe a simpler password-less authentication mechanism would be all that's required. This service does not need any of the school and user information held within DfE Sign-in (DSI). DfE governance has reviewed our concern and decided this service should use DSI.\\nThere is currently no formal recommendation for a tool of choice in the technical guidance https://github.com/DFE-Digital/technical-guidance.\\nWe want a tool that provides an open and modern security standard.\\n", "response": "## Decision\n\nUse DfE Sign-in (DSI) for authentication, despite the preference for a simpler passwordless solution. This decision aligns with DfE governance requirements and avoids potential future integration issues.  While it introduces complexity beyond the initial needs, it adheres to organizational security policies and ensures consistency across DfE services. We will adopt the current best practice recommended for integrating with DSI, as outlined in official documentation or communication channels, even if a specific tool isn't formally recommended in the technical guidance yet. We will prioritize solutions that adhere to open and modern security standards (e.g., OAuth 2.0, OpenID Connect) within the DSI framework.\n", "time": "0:00:03.601185"}
{"context": "## Context and Problem Statement\\nHow to guarantee reproducibility of Jupyter Notebooks?\\nIn order to allow any user to re run the notebook with similar behaviour, it's important that each notebook is shipped with dependencies requirements\\nthat include direct and transitive dependencies. This would also enforce and support security, reproducibility, traecability.\\nNotebooks should be treated as component/service that use their own dependencies, therefore when storing notebooks,\\nthey should be stored with dependencies so that an image can be built to run them or they can be shared and reused by others.\\n## Decision Drivers <!-- optional -->\\n* user prospective\\n* reproducibility\\n* traecability\\n", "response": "## Decision\n\nStore each Jupyter Notebook with a `requirements.txt` file that lists all direct dependencies required to run the notebook.  Utilize a tool like `pipreqs` or `pigar` to automatically generate the `requirements.txt` file, ensuring that both direct and transitive dependencies are captured.  This `requirements.txt` file will be version-controlled alongside the notebook itself.  When a notebook needs to be executed, a dedicated environment will be created (e.g., using a Docker container, virtual environment, or conda environment) using the associated `requirements.txt` file. This ensures that the notebook execution environment is consistent and reproducible regardless of the user's local environment.\n", "time": "0:00:03.796027"}
{"context": "## Context\\nWe need to record the architectural decisions that we make as we develop Ockam.\\n", "response": "## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight text files stored in the Ockam repository to document significant architectural decisions.  These ADRs will follow the MADR template.\n", "time": "0:00:01.600719"}
{"context": "## Context\\nThe issue motivating this decision, and any context that influences or constrains the decision.\\n", "response": "## Decision\n\nWe will adopt a \"wait and see\" approach, deferring any major architectural decisions until we have gathered more information.  This is due to the lack of clear requirements and constraints outlined in the provided context.  Attempting to make architectural decisions without sufficient understanding of the problem space could lead to premature optimization and potentially costly rework later.\n", "time": "0:00:03.687127"}
{"context": "## Context\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n* Status: proposed\\n* Deciders: Licence Change\\n* Date: 2020-08-27\\nTechnical Story: [description | <https://apps.nrs.gov.bc.ca/int/jira/browse/ARCH-62]>\\n## Context and Problem Statement\\nThis is a response from IITD Architecture to the IITD Infrastructure Team, and others, for an Official Statement on OpenJDK positioning.\\n## Decision Drivers\\n* An official Information Innovation & Technology Division (IITD) statement is needed on the direction forward for target versions of Java/JDKs for all IITD hosted java applications.  Oracle has changed its licensing model, and will now be using a subscription model for charging for the use of its Oracle Java SE product releases.\\n* Java SE 8 is the end of the legacy versioning and release cadence model; Java 9 was the new beginning.\\n* Oracle extended the public updates of its Oracle JDK to January 2019 for commercial production use, and at least the end of 2020 for individual desktop use.\\n* Oracle JDK should be, for the most part, interchangeable with Oracle\u2019s OpenJDK builds for those who want to move up to the next releases, as they did for Java 6->7 and Java 7->8.\\n* Java 10 is the suggested release by Oracle\\n* Oracle has posted more information on the roadmap for the Java Client (including Applets and Web Start).\\n* Oracle plans to discontinue contributions to the JDK 8 Updates project until January 2019.\\n* For security reasons, IITD Architecture encourages upgrade/migration of all java applications to at least JDK 11.\\n* Scope - IITD all server and client side applications owned by IITD that run Java.\\n", "response": "## Decision\n\nIITD will standardize on OpenJDK 11 (or later LTS release) as the supported Java Development Kit (JDK) for all IITD hosted Java applications, both server-side and client-side.  This decision is driven by Oracle's shift to a subscription-based licensing model for Oracle JDK and the need for a secure and cost-effective long-term solution.  While Oracle JDK and OpenJDK are largely interchangeable, migrating to OpenJDK eliminates licensing costs and aligns with the community-driven development model.  This decision encourages immediate migration to at least JDK 11 for security reasons,  and establishes a policy of using OpenJDK for future Java development and deployments within IITD.  Applications currently running on older Java versions (e.g., Java 8) should be upgraded as soon as feasible.  This policy applies to all server and client-side applications owned by IITD that utilize Java.\n", "time": "0:00:04.844845"}
{"context": "## **Context**\\nValidation is an important function within APIs, to ensure that data that is submitted via API calls is properly checked to ensure it meets the requirements set by the business.\\nWe will look at two options for validation:\\n- **Data Annotations**\\nThis involves \"annotating\" each class model with specific validation, such as\\n- `[Required(ErrorMessage = \"This field is required\")]`\\n- `[MaxLength(20)]`\\nThere are a number of issues with this approach:\\n- Validation is scattered throughout the codebase as attributes on data model classes\\n- Testing is not straightforward\\n- Error messages are part of the compiled code and it is not possible to decouple this, e.g. to allow for customisable error messages\\n- Does not allow for conditional validation\\n- **Fluent Validation**\\nFluent Validation solves a number of the issues that DataAnnotation cannot be solved by Data Annotations. It:\\n- Is easy to configure with minimal, unintrusive setup in `Startup.cs`\\n- Lives outside of the data model classes\\n- Very easy to test validation in isolation\\n- Error messaging can be externalised using dependency injection\\n- Allows for chaining of validators and conditional validation\\n- Has a lot of built-in validation already (*if **x** exists, then **y** must also exist*)\\n", "response": "## **Decision**\n\nWe will use FluentValidation for our API request validation. While Data Annotations offer a simpler initial setup, FluentValidation provides greater flexibility, testability, and maintainability in the long run.  The ability to centralize validation logic, externalize error messages, and implement complex conditional validation outweighs the initial simplicity of Data Annotations.  This decision aligns with our goals of creating a robust, maintainable, and testable API.\n", "time": "0:00:03.282492"}
{"context": "## Context\\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\\nTasks are scheduled and ran on the same node they are scheduled.\\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\\n", "response": "## Decision\n\nIntroduce a distributed task management system.  This system will allow centralized management and distributed execution of tasks across the James cluster.  This will involve a central coordinator responsible for task assignment and tracking, and worker nodes capable of executing tasks dispatched by the coordinator.  The current `TaskManager` will be refactored to integrate with this distributed system, acting as both a client to submit tasks to the coordinator and a worker to execute tasks assigned by the coordinator. This will enable cluster-wide task visibility and control.\n", "time": "0:00:02.950994"}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "response": "## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture significant architectural decisions along with their context and consequences.  These ADRs will be stored as Markdown files in a dedicated `docs/adr` directory within the project's version control system.  Each decision will be assigned a unique, incrementing ID and follow a consistent template (e.g., MADR).\n", "time": "0:00:02.358712"}
{"context": "## Context\\nAnalyzing untrusted Clojure code means loading it which should only be done in some kind of\\nsandboxed environment. A Docker image has been created to help with this but this still\\nrequires us to run and monitor job execution. Bad actors could still trigger many builds\\nto run Bitcoin miners and other compute-stealing stuff.\\nAlternatives to running Docker ourselves are AWS Lambda (probably similar compute-stealing\\nissues) and \"hacking\" a continous integration service to do the job for us. More detail can\\nbe found in the [notes on Isolation](https://github.com/martinklepsch/cljdoc/blob/72da65055ab94942f33fb63b29b732e81b559508/doc/isolation.md)\\n", "response": "## Decision\n\nWe will utilize a continuous integration (CI) service, specifically GitHub Actions, to execute untrusted Clojure code within a controlled environment.  This approach avoids the operational overhead and security risks associated with managing our own Docker infrastructure or using serverless compute platforms like AWS Lambda, which are susceptible to compute-stealing attacks. GitHub Actions provides built-in resource limits and security measures that mitigate these risks.  The CI environment will be configured to use the existing Docker image for consistent execution.  We acknowledge this approach involves \"hacking\" the CI system to a degree, as it is not its intended purpose, but believe the benefits in terms of security and maintainability outweigh the drawbacks.  We will investigate and address any potential abuse limitations imposed by GitHub Actions.\n", "time": "0:00:04.009808"}
{"context": "## Context\\nWe sought to determine whether to deliver our document management capabilities using the content management platform natively or through the integration of an external document management platform.\\nWe sought to determine whether Bloomreach's 'Channel' concept would be suitable for managing the various sites required to be brought onto the platform both at MVP and in the future, such as Deenary and Speciality sights.\\nAs part of this, considerations were made around:\\n* Ease of use for creating new sites\\n* Ability to share components\\n* Ability to segregate content for specific channels (sites)\\n* Ability to share content up and down the stack where needed and appropriate\\n* Permissions model required to support this model\\n", "response": "## Decision\n\nWe will leverage Bloomreach's native content management capabilities and \"Channel\" concept for managing multiple sites (including Deenary and Speciality sites) both at MVP launch and for future expansion.  This decision prioritizes ease of development and maintenance by avoiding the complexity of integrating and managing a separate document management system.  We believe Bloomreach's features adequately address our core requirements for content segregation, sharing, and permissions management within a multi-site architecture.\n", "time": "0:00:03.037902"}
{"context": "## Context\\nWhen the team for the re-write of the Fundraising formed in 2016, we discovered that team members had different approaches to do validation:\\n* Use an established library, like [Symfony Validation](https://symfony.com/doc/current/validation.html).\\n* Write our own validation logic.\\nThe arguments in favor of writing our own logic were:\\n* We don't want to bind our domain layer to a concrete validation library implementation.\\n* The individual validations - checking for required fields in most cases - are so simple that using an external library would make the validation more complicated.\\n* We don't know the \"maintenance cycles\" of the library, either we need to constantly update or the library is not maintained properly.\\n* Every developer would have to learn the API of the external library.\\nAt the start of the project we did not know where we should put the validation logic:\\n* At the framework/presentation layer, forcing us to create valid, fully formed domain objects as input for use cases.\\n* At the use case layer, making validation part of the use case.\\n", "response": "## Decision\n\nWe will write our own validation logic and place it within the Application/Use Case layer.  This allows us to keep validation decoupled from the framework/presentation layer and integral to the core application logic.  While this might lead to some duplicated validation rules across use cases, it provides flexibility and avoids a hard dependency on an external library.  The simplicity of the current validation needs outweighs the benefits of using a library at this time. We acknowledge the potential for duplicated code and will address it through refactoring and shared helper functions/classes if it becomes a significant issue.\n", "time": "0:00:03.316807"}
{"context": "## Context and Problem Statement\\nAs a user, I want to get Changes metrics related to tasks defined\\nin a task tracker. A simple usecase example is to get insight of the\\nratio of changes related to Feature Requests vs Bug fixing.\\n## Decision Drivers\\n* Simple implementation\\n* No assumption about the tasks tracker\\n* Support of a set of generic fields related to a task\\n", "response": "## Decision\n\nWe will implement a solution based on regular expressions applied to commit messages to extract task IDs.  These task IDs will then be used to query the task tracker API (assuming it exists) for additional information like task type (Feature, Bug, etc.). A configuration file will allow users to define the regular expressions corresponding to their task tracker, and to map the task tracker fields to generic fields (e.g., \"type\", \"priority\"). If no task tracker API is available, the user can provide a CSV file containing the task IDs and their associated metadata.  This allows for flexibility and avoids hardcoding specific task tracker integrations.\n", "time": "0:00:03.776868"}
{"context": "## Context\\nOur service-operator allows service teams to provision various AWS services by\\ndeclaratively defining resources and submitting them via the kubernetes api.\\nSome of these resources require IAM to authorise how the provisioned service\\ncan be used. The types of actions that can be performed on.\\n#### Example\\nThe service operator allows provisioning of S3 buckets and bucket configuration such as:\\n```\\n---\\napiVersion: storage.govsvc.uk/v1beta1\\nkind: S3Bucket\\nmetadata:\\nname: s3-bucket-sample\\nspec:\\naws:\\nLifecycleRules:\\n- Expiration: 90days\\nVersioning:\\nEnabled: true\\n```\\nIn order to access a provisioned bucket via the the AWS SDK users will require\\nan IAM role/policy that allows access.\\nWe want things like bucket ACL, versioning configuration and lifecycle policy\\nto be defined declaratively via the resource manifest (see example above), and continuously managed\\nby the service operator.\\nWe want users of the provisioned bucket to be able to read back all\\nconfiguration, and be able to fully utilise the specific bucket for reading,\\nwriting and managing their objects within the provisioned bucket, but we want\\nto avoid giving permissions to users that could cause conflicts with the\\nproperties that are managed by the service operator's reconcile loop.\\nFor example, given the example manifest above, we would like to avoid giving\\npermissions that would allow a user to alter the Expiration LifeCycleRules,\\nsince any changes the user made would be periodically overwritten by the\\nservice operator's reconciliation.\\n", "response": "## Decision\n\nWe will use a combination of IAM roles and policies to manage access to provisioned AWS resources.  The service operator will assume an IAM role with sufficient permissions to create, update, and delete resources according to the user-provided specifications.  Separately, users will be granted an IAM role with a policy specifically crafted to allow full utilization of the *contents* of the provisioned resources while restricting actions that could interfere with the service operator's management of the resource itself.\n\nSpecifically, for S3 buckets, the user's IAM role will be granted permissions for `s3:GetObject`, `s3:PutObject`, `s3:DeleteObject`, `s3:ListBucket`, `s3:GetBucketLocation`, `s3:GetBucketAcl`, `s3:GetBucketVersioning`, `s3:GetLifecycleConfiguration` and other object-level operations.  Crucially, permissions to modify bucket-level settings such as `s3:PutLifecycleConfiguration`, `s3:PutBucketAcl`, `s3:PutBucketVersioning` will be *explicitly denied*. This allows users to fully manage the *data* within the bucket but prevents them from changing settings that the service operator manages.  The service operator's IAM role will include these restricted permissions.\n\nThis approach ensures a clear separation of concerns: the service operator manages the resource's configuration, while users interact with the data within the provisioned resource.  This prevents conflicts and ensures predictable behavior.  The specifics of the permissions granted to users will depend on the AWS service being provisioned and will be detailed in the service operator\u2019s documentation.\n", "time": "0:00:07.850206"}
{"context": "## Context\\nIt is necessary to provide autocomplete functionality to make certain fields quicker to enter by suggesting potential results to the user.\\n", "response": "## Decision\n\nWe will implement autocomplete functionality using a third-party library, specifically [Name of chosen library, e.g., `react-autosuggest` for React, `autocomplete.js` for vanilla JS,  `Typeahead.js` for Bootstrap].  This library was selected due to [Reasons for selecting this library, e.g., its robust feature set, active community support, good performance with large datasets, ease of integration with our existing tech stack].  We will integrate this library with our existing [Backend data source/API endpoint, e.g., REST API, GraphQL endpoint] to fetch suggestions dynamically as the user types.\n", "time": "0:00:03.931417"}
{"context": "## Context and Problem Statement\\nDeciding how to have the internal components of the Transcript Editor communicate with each other.\\n## Decision Drivers <!-- optional -->\\n* Simple and straightforward way to reason around passing data between components\\n* Extensible anticipating use cases when using the component \"in the wild\" and having internal info accessible/when if needed.\\n", "response": "## Decision\n\nUse a message bus architecture for communication between internal components of the Transcript Editor.  Each component will subscribe to specific message types and publish messages as needed.  A central message broker will handle routing messages between publishers and subscribers.  This will be implemented using a lightweight message bus library appropriate for the chosen tech stack (e.g., EventEmitter, mitt, RxJS).  Messages will be structured as plain JavaScript objects with clearly defined properties.\n", "time": "0:00:02.794139"}
{"context": "## Context\\nWe want to avoid testing implementation details in our integration tests.\\nWe want to use `react-testing-library` which makes it easier to make assertions on the rendered DOM rather than assert implementation details. But mostly because it enable us to find and trigger click events on different UI elements.\\ne.g. toggling the visibility of different tabs and window groups.\\nBut when it comes to asserting the rendered DOM, in most cases we trust the view will render the model properly.\\nIt could be more sensible to only verify the state/model.\\n", "response": "## Decision\n\nWe will use `react-testing-library` for integration tests to interact with the UI (e.g., clicking elements, toggling visibility) as it simplifies simulating user interactions.  However, we will primarily focus our assertions on verifying the underlying state/model changes resulting from those interactions rather than extensively testing the rendered DOM itself.  We will assume that the view correctly reflects the model unless specific visual aspects are critical to the test case (e.g., verifying conditional rendering based on state).  This approach balances testing user flows with avoiding overly brittle tests tied to implementation details of the view.\n", "time": "0:00:03.458398"}
{"context": "## Context\\nPresently when submitting new properties or editing existing properties within inventory the only way to set the latitude and longitude values is manually.\\nIdeally the inventory would use GIS location values that are pulled from Data BC (better source of truth).\\nProviding a way through the property address to pull valid GIS coordinates from Data BC Geocoder would improve the data and the user experience.\\nAdditionally Geocoder can be used to verify addresses that are manually entered.\\n- [Geocoder](https://www2.gov.bc.ca/gov/content/data/geographic-data-services/location-services/geocoder)\\n- [Data BC](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service)\\n- [API Swagger](https://catalogue.data.gov.bc.ca/dataset/bc-address-geocoder-web-service/resource/40d6411e-ab98-4df9-a24e-67f81c45f6fa/view/1d3c42fc-53dc-4aab-ae3b-f4d056cb00e0)\\n- [Developer API Keys](https://github.com/bcgov/gwa/wiki/Developer-Guide#developer-api-keys)\\n- API Host = `https://geocoder.api.gov.bc.ca`\\n", "response": "## Decision\n\nThe system will integrate with the Data BC Geocoder API to improve the accuracy and user experience of entering property location data.  Specifically:\n\n1. **Address Autocomplete/Suggestion:** As a user types an address, the system will query the Geocoder API's suggest endpoint to provide real-time address suggestions. This will help standardize address formatting and reduce input errors.\n2. **Geocoding on Address Submission:**  When a new property is added or an existing property's address is modified (either via autocomplete selection or manual entry), the system will call the Geocoder API to retrieve the corresponding latitude and longitude. These values will then populate the respective fields in the inventory system.  This will prioritize the Geocoder data as the source of truth for location information.\n3. **Address Verification:** The Geocoder API will also be used to verify the validity of manually entered addresses.  If the API cannot find a match or returns a low confidence score, the user will be notified and prompted to review and correct the address.  This will prevent incorrect or incomplete address data from being stored in the system.\n4. **API Key Management:**  A dedicated API key will be obtained through the process documented on the [Developer API Keys](https://github.com/bcgov/gwa/wiki/Developer-Guide#developer-api-keys) page and securely stored within the application's configuration.  Regular audits of API usage will be conducted.\n\nThis integration will improve data quality, reduce manual data entry, and enhance the overall user experience.\n", "time": "0:00:07.452878"}
{"context": "## Context and Problem Statement\\nWhile extracting the `Grant-wise expenditure` a lot of site crashes were encountered. A decision was taken earlier to extract the `DDO-wise expenditure` section from Koshvani as both section contain the same information in different heirarchies.\\n## Decision Drivers\\nThe challenges around site crashes have been addressed and the `Grant-wise expenditure` section can now be extracted.\\n", "response": "## Decision\n\nRevert to extracting the Grant-wise expenditure section from Koshvani.  The previous workaround of extracting DDO-wise expenditure is no longer necessary.\n", "time": "0:00:01.667214"}
{"context": "## Context and Problem Statement\\nOperations can be defined in the FDML as elements and attributes or simply as elements.  For example the operation take can be described as:\\n```\\n<xs:element name=\"take\" type=\"xs:integer\">\\n```\\nor with attributes as:\\n```\\n<xs:element name=\"take\">\\n<xs:complexType>\\n<xs:attribute name=\"value\"/>\\n</xs:complexType>\\n</xs:element>\\n```\\n## Decision Drivers\\n* consistency\\n* expressiveness\\n", "response": "## Decision\n\nOperations will be defined as elements with attributes.  This allows for greater expressiveness as attributes can be used to specify additional information about the operation. For example, an attribute could be used to indicate the units of the value being passed to the operation.  Using attributes also allows for easier extension of the FDML in the future by adding more attributes to capture additional information about the operation, while elements will be reserved for nesting and structure. This approach maintains a clear separation and promotes better organization within the FDML definition.\n", "time": "0:00:03.016896"}
{"context": "Context\\n-------\\n``Event-routing-backends`` transforms and emits edx events that may contain PII which is not meant to be shared with learning record consumers. New xAPI and Caliper transformers are expected to be added in ``Event-routing-backends`` and therefore, a mechanism needs to be put in place to reduce chances of PII leakage via these transformers.\\nDecision\\n--------\\n#. An accessor `method`_ will be developed to get values from open edx events for a specified key. Information inside the open edx event will only be accessed using this accessor method in base transformers and in event transformers.\\n#. If a key, required by the specification (xAPI or Caliper), is not found in open edx event, this method will throw an exception.\\n#. If a key, deemed optional by the specification (xAPI or Caliper), is not found in open edx event, this method will log this instance and return None.\\n#. The efficacy of fuzzy comparison between keys in transformed events and potential PII keys will be evaluated in future and incorporated if found useful.\\nBenefits\\n---------\\n#. Only the instances where this accessor is called, will need to be reviewed during code review, in order to check for potential PII leakage.\\n.. _method: https://github.com/openedx/event-routing-backends/blob/f430d4cf58bdab01e42fcc944241898606873d82/event_routing_backends/processors/mixins/base_transformer.py#L139\\n", "response": "Decision\n-------\nA centralized accessor method will be implemented within the `event-routing-backends` application to retrieve data from Open edX events.  This method will be mandatory for all xAPI and Caliper event transformers, enforcing a controlled access pattern for event data.  This accessor will throw an exception if a required key is missing and log a warning if an optional key is missing. Future investigation will explore fuzzy matching of keys to further mitigate the risk of PII leakage.\n", "time": "0:00:03.029494"}
{"context": "# Context\\nThe way the Government Digital Service (GDS) [makes technology choices is\\ndescribed in the service manual](https://www.gov.uk/service-manual/making-software/choosing-technology). We are selecting which technology will to use to provide\\npersistence for the Performance Platform.\\nGDS has experience in running MongoDB and MySQL in production.\\nWe envisage the Performance Platform as taking in unstructured data from a\\nvariety of data sources (spreadsheets, analytics, logs, other databases and\\napplications) and allowing people to collect this data in a single place. This\\nshould enable service managers to:\\n- make comparisons\\n- see how well their service is performing\\n- see how the performance changes over time, as they iterate the service\\nSo we want a persistent data store that will store unstructured data, and\\nallow us to apply a structure either by post-processing the data, or at query\\ntime.\\nThe volume of the data that we are envisaging at this stage is pretty small.\\nWe will be building a small thing to start; as we learn more about the\\nuser needs and problem space, then we will revisit this decision. Since the\\nvolume is small, it does not seem likely that we need Hadoop / HDFS or\\nCassandra.\\nWe are not the canonical source of this data. We are an aggregator; the\\ncanonical source remains the data sources which will be providing feeds or\\npushing the data into the Performance Platform.\\nBecause of this position, we do not need ACID properties for this data, nor\\nneed worry about the CAP theorem in any detail.\\n# Decision\\nWe will use MongoDB. We are comfortable operating it in production,\\nit will allow unstructured data (in the form of JSON documents) and we can\\napply structure at query time.\\n# Status\\nAccepted.\\n# Consequences\\nUse MongoDB with an appropriate replica-set configuration.\\n", "response": "## Decision\n\nUse MongoDB for the persistent data store of the Performance Platform.\n", "time": "0:00:01.287516"}
{"context": "## Context and Problem Statement\\nTax users are sometimes, without warning, unnecessarily signed out when accessing\\nMDTP services using multiple browser tabs or windows. This is a violation of [WCAG 2.1\\nsuccess criterion 2.1.1 (Timing adjustable)](https://www.w3.org/WAI/WCAG21/Understanding/timing-adjustable.html).\\nThis problem is a consequence of the fact that a user's session can be refreshed via user activity in any\\ntab or window belonging to the same browser profile. However, the Javascript-powered [hmrcTimeoutDialog](https://github.com/hmrc/play-frontend-hmrc#warning-users-before-timing-them-out)\\ncurrently has no way of knowing about this activity following initial page load.\\nSolving this issue requires providing the timeout dialog component with knowledge of the actual time\\nremaining on the user's active session via an endpoint that is itself excluded from\\nsession management.\\nHow can we achieve this cost-effectively while minimising impact for service teams, limiting duplication of\\nknowledge and avoiding introducing additional coupling between frontend microservices?\\n## Decision Drivers\\n* The need to minimise code changes for service teams other than a library upgrade.\\n* The avoidance of requiring service teams to add any additional routing rules.\\n* The avoidance of requiring service teams to add any additional configuration.\\n* The need to limit duplication of knowledge between SessionTimeoutFilter in bootstrap-play and\\nany other library or service.\\n* The preference for maintaining loose coupling of frontend services and avoiding adding single points\\nof failure between frontend microservices.\\n* The need for the endpoint used to interrogate the session to not itself affect the session.\\n", "response": "## Decision\n\nImplement a new endpoint on the existing `bootstrap-play` application that returns the remaining session time.  This endpoint will be excluded from session management, preventing it from refreshing the session timer.  The `hmrcTimeoutDialog` will be updated to periodically poll this endpoint and update its displayed remaining time accordingly. This leverages existing infrastructure and minimizes the impact on service teams, requiring only a library upgrade and no code or configuration changes.\n", "time": "0:00:02.799562"}
{"context": "## Context\\nYou never think of everything. Sticking to standards is a very good\\nthing to prevent you from doing things that can go bad. Those also\\nhelps making the code be more readable and structured.\\n", "response": "## Decision\n\nWe will adhere to established coding style guidelines and industry best practices for the chosen programming language(s) and frameworks. This will include using a linter to enforce consistent code style and prevent common errors. We will also adopt a standard project structure and follow established design patterns where applicable.  Specific standards and tools will be documented in the project's contribution guidelines.\n", "time": "0:00:02.322567"}
{"context": "## Context\\nThis ADR has been superseded by `ADR-0019`.\\nSome actions and features of Raster Foundry require a way to manage asynchronous tasks and workflows.\\nFor instance, user uploads of imagery or tools may start workflows in an ad hoc manner, while in\\ncontrast imports of imagery from NASA or partners may need to happen on a schedule. The nature of\\nthese tasks could vary from bash scripts and python functions to spark jobs and ECS tasks.\\nThe ideal tool will provide some means to monitor task progress, retry on some failures, and\\nnotify personnel if necessary. There are a few options of tools we can use: celery, SWF, Luigi, and Airflow.\\nAzavea has experience working with both celery and SWF; however, due to our past experience with these\\ntools it seemed prudent to explore additional options as well.\\n| Workflow Tool   | Pros | Cons |\\n|-----------------|------|------|\\n| Celery          | Familiar, written in python, flexible | Provides poor primitives for workflows, many open issues, difficult to monitor workflows |\\n| SWF (botoflow)) | Familiar, now written in python, maintaining state is not our responsibility (HA by default), great primitives for workflows and tasks | Difficult to monitor, relatively immature tools and projects, not many others using it |\\n| Luigi           | Mature, seems to be stable, written in python | Unfamiliar execution model, primarily designed for scheduled, recurring task |\\n| Airflow         | Mature, stable, fits into our execution model, written in python, excellent UI | Requires celery (for what we want to do)), requires managing the scheduler and a cache |\\n", "response": "## Decision\n\nWe will use Airflow to manage asynchronous tasks and workflows. While it introduces a dependency on Celery and requires managing the scheduler and a cache, its maturity, stability, excellent UI, and compatibility with our execution model outweigh these drawbacks.  Airflow's superior workflow management capabilities and monitoring tools address the shortcomings experienced with Celery and SWF.  Although Luigi offered maturity and stability, its unfamiliar execution model and focus on scheduled tasks make it less suitable for our requirements which include both ad-hoc and scheduled tasks.\n", "time": "0:00:02.920565"}
{"context": "## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n", "response": "## Decision\n\nUse a graph database as the persistence layer for SKOS taxonomies.  This leverages the native RDF support offered by many graph databases, simplifying data storage and retrieval compared to relational databases.\n", "time": "0:00:01.795244"}
{"context": "## Context and Problem Statement\\nPersistence and retrieval of SKOS taxonomies require a storage layer that supports storing rich, free-form linked data.\\nSuch a data model could be represented in a traditional RDBMS, however, doing so would require a specialized serialization and deserialization implementation whereas graph databases can typically store RDF natively.\\n## Decision Drivers <!-- optional -->\\n* High availability/Fault tolerance\\n* Learning curve\\n* Maintenance overhead\\n* Vendor lock-in\\n", "response": "## Decision\n\nUse a Graph Database (specifically Neo4j) as the persistence layer for SKOS taxonomies.  This decision leverages Neo4j's native support for RDF triples and Cypher query language, simplifying data management and retrieval compared to a relational database approach.  While other graph databases could be considered, Neo4j is chosen for its maturity, large community, and robust tooling.\n", "time": "0:00:04.393919"}
{"context": "## Context\\nWe need to record the architectural decisions made on this project.\\n", "response": "## Decision\n\nWe will use Architectural Decision Records (ADRs) as lightweight documents to capture significant architectural decisions along with their context, consequences, and status.  These ADRs will be stored as Markdown files in a dedicated `/docs/adr` directory within the project's version control system.  This will allow us to track the evolution of the architecture and provide rationale for the current state.\n", "time": "0:00:02.446327"}
